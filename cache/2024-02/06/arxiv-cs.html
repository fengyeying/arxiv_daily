<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Fri  2 Feb 24  to  Mon  5 Feb 24, announced Tue,  6 Feb 24</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item914">Cross-lists</a></li>
<li><a href="#item998">Replacements</a></li>
</ul>
<small>[ total of 1482 entries:  <b>1-1482</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Tue,  6 Feb 24</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01640" title="Abstract">arXiv:2402.01640</a> [<a href="/pdf/2402.01640" title="Download PDF">pdf</a>, <a href="/format/2402.01640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating ChatGPT in a Computer Science Course: Students Perceptions  and Suggestions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aruleba%2C+K">Kehinde Aruleba</a>, 
<a href="/search/cs?searchtype=author&query=Sanusi%2C+I+T">Ismaila Temitayo Sanusi</a>, 
<a href="/search/cs?searchtype=author&query=Obaido%2C+G">George Obaido</a>, 
<a href="/search/cs?searchtype=author&query=Ogbuokiri%2C+B">Blessing Ogbuokiri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">The integration of artificial intelligence tools such as ChatGPT in the
education system has gained attention in recent years. This experience report
explores students' perceptions and suggestions for integrating ChatGPT in a
computer science course. Following a ChatGPT activity which includes code
completion and analysis, seven students participated in in-depth interviews.
Findings from the transcribed interviews suggest that ChatGPT has the potential
to enhance learning experience including programming. They highlighted the
tool's ability to respond immediately to queries and supporting personalised
learning. However, they raise concerns that heavy reliance on ChatGPT may
adversely affect students' critical thinking and problem-solving skills. These
findings show the importance of carefully balancing using ChatGPT in computer
science courses. The findings of this research have significant implications
for educators, curriculum designers and policymakers as they explore
integrating AI tools into educational contexts.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01641" title="Abstract">arXiv:2402.01641</a> [<a href="/pdf/2402.01641" title="Download PDF">pdf</a>, <a href="/format/2402.01641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Syntactic Structures: Modeling Syntax for Various Natural  Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+M+K">Min K. Kim</a>, 
<a href="/search/cs?searchtype=author&query=Takero%2C+H">Hafu Takero</a>, 
<a href="/search/cs?searchtype=author&query=Fedovik%2C+S">Sara Fedovik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We aim to provide an explanation for how the human brain might connect words
for sentence formation. A novel approach to modeling syntactic representation
is introduced, potentially showing the existence of universal syntactic
structures for all natural languages. As the discovery of DNA's double helix
structure shed light on the inner workings of genetics, we wish to introduce a
basic understanding of how language might work in the human brain. It could be
the brain's way of encoding and decoding knowledge. It also brings some insight
into theories in linguistics, psychology, and cognitive science. After looking
into the logic behind universal syntactic structures and the methodology of the
modeling technique, we attempt to analyze corpora that showcase universality in
the language process of different natural languages such as English and Korean.
Lastly, we discuss the critical period hypothesis, universal grammar, and a few
other assertions on language for the purpose of advancing our understanding of
the human brain.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01642" title="Abstract">arXiv:2402.01642</a> [<a href="/pdf/2402.01642" title="Download PDF">pdf</a>, <a href="/ps/2402.01642" title="Download PostScript">ps</a>, <a href="/format/2402.01642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detection of Machine-Generated Text: Literature Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Valiaiev%2C+D">Dmytro Valiaiev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Since language models produce fake text quickly and easily, there is an
oversupply of such content in the public domain. The degree of sophistication
and writing style has reached a point where differentiating between human
authored and machine-generated content is nearly impossible. As a result, works
generated by language models rather than human authors have gained significant
media attention and stirred controversy.Concerns regarding the possible
influence of advanced language models on society have also arisen, needing a
fuller knowledge of these processes. Natural language generation (NLG) and
generative pre-trained transformer (GPT) models have revolutionized a variety
of sectors: the scope not only permeated throughout journalism and customer
service but also reached academia. To mitigate the hazardous implications that
may arise from the use of these models, preventative measures must be
implemented, such as providing human agents with the capacity to distinguish
between artificially made and human composed texts utilizing automated systems
and possibly reverse-engineered language models. Furthermore, to ensure a
balanced and responsible approach, it is critical to have a full grasp of the
socio-technological ramifications of these breakthroughs. This literature
survey aims to compile and synthesize accomplishments and developments in the
aforementioned work, while also identifying future prospects. It also gives an
overview of machine-generated text trends and explores the larger societal
implications. Ultimately, this survey intends to contribute to the development
of robust and effective approaches for resolving the issues connected with the
usage and detection of machine-generated text by exploring the interplay
between the capabilities of language models and their possible implications.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01643" title="Abstract">arXiv:2402.01643</a> [<a href="/pdf/2402.01643" title="Download PDF">pdf</a>, <a href="/format/2402.01643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> L-TUNING: Synchronized Label Tuning for Prompt and Prefix in LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kowsher%2C+M">Md. Kowsher</a>, 
<a href="/search/cs?searchtype=author&query=Sobuj%2C+M+S+I">Md. Shohanur Islam Sobuj</a>, 
<a href="/search/cs?searchtype=author&query=Mahmud%2C+A">Asif Mahmud</a>, 
<a href="/search/cs?searchtype=author&query=Prottasha%2C+N+J">Nusrat Jahan Prottasha</a>, 
<a href="/search/cs?searchtype=author&query=Bhat%2C+P">Prakash Bhat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Efficiently fine-tuning Large Language Models (LLMs) for specific tasks
presents a considerable challenge in natural language processing. Traditional
methods, like prompt or prefix tuning, typically rely on arbitrary tokens for
training, leading to prolonged training times and generalized token use across
various class labels. To address these issues, this paper introduces L-Tuning,
an efficient fine-tuning approach designed for classification tasks within the
Natural Language Inference (NLI) framework. Diverging from conventional
methods, L-Tuning focuses on the fine-tuning of label tokens processed through
a pre-trained LLM, thereby harnessing its pre-existing semantic knowledge. This
technique not only improves the fine-tuning accuracy and efficiency but also
facilitates the generation of distinct label embeddings for each class,
enhancing the model's training nuance. Our experimental results indicate a
significant improvement in training efficiency and classification accuracy with
L-Tuning compared to traditional approaches, marking a promising advancement in
fine-tuning LLMs for complex language tasks. \\ Code is available at:
\textcolor{red}{\href{https://github.com/Kowsher/L-Tuning}{\texttt{https://github.com/Kowsher/L-Tuning}}}.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01644" title="Abstract">arXiv:2402.01644</a> [<a href="/pdf/2402.01644" title="Download PDF">pdf</a>, <a href="/format/2402.01644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Holistic Approach for Equity-aware Carbon Reduction of Ridesharing  Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sahebdel%2C+M">Mahsa Sahebdel</a>, 
<a href="/search/cs?searchtype=author&query=Zeynali%2C+A">Ali Zeynali</a>, 
<a href="/search/cs?searchtype=author&query=Bashir%2C+N">Noman Bashir</a>, 
<a href="/search/cs?searchtype=author&query=Shenoy%2C+P">Prashant Shenoy</a>, 
<a href="/search/cs?searchtype=author&query=Hajiesmaili%2C+M">Mohammad Hajiesmaili</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Ridesharing services have revolutionized personal mobility, offering
convenient on-demand transportation anytime. While early proponents of
ridesharing suggested that these services would reduce the overall carbon
emissions of the transportation sector, recent studies reported a type of
rebound effect showing substantial carbon emissions of ridesharing platforms,
mainly due to their deadhead miles traveled between two consecutive rides.
However, reducing deadhead miles' emissions can incur longer waiting times for
riders and starvation of ride assignments for some drivers. Therefore, any
efforts towards reducing the carbon emissions from ridesharing platforms must
consider the impact on the quality of service, e.g., waiting time, and on the
equitable distribution of rides across drivers. This paper proposes a holistic
approach to reduce the carbon emissions of ridesharing platforms while
minimizing the degradation in user waiting times and equitable ride assignments
across drivers. Towards this end, we decompose the global carbon reduction
problem into two sub-problems: carbon- and equity-aware ride assignment and
fuel-efficient routing. For the ride assignment problem, we consider the
trade-off between the amount of carbon reduction and the rider's waiting time
and propose simple yet efficient algorithms to handle the conflicting
trade-offs. For the routing problem, we analyze the impact of fuel-efficient
routing in reducing the carbon footprint, trip duration, and driver efficiency
of ridesharing platforms using route data from Google Maps. Our comprehensive
trace-driven experimental results show significant emissions reduction with a
minor increase in riders' waiting times. Finally, we release E$^2$-RideKit, a
toolkit that enables researchers to augment ridesharing datasets with emissions
and equity information for further research on emission analysis and platform
improvement.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01645" title="Abstract">arXiv:2402.01645</a> [<a href="/pdf/2402.01645" title="Download PDF">pdf</a>, <a href="/format/2402.01645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recent Innovations in Footwear Sensors: Role of Smart Footwear in  Healthcare -- A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=R.%2C+P+G">Pradyumna G. R.</a>, 
<a href="/search/cs?searchtype=author&query=Hegde%2C+R+B">Roopa B. Hegde</a>, 
<a href="/search/cs?searchtype=author&query=B.%2C+B+K">Bommegowda K. B.</a>, 
<a href="/search/cs?searchtype=author&query=Bhat%2C+A+K">Anil Kumar Bhat</a>, 
<a href="/search/cs?searchtype=author&query=Pujari%2C+A+N">Amit N. Pujari</a>, 
<a href="/search/cs?searchtype=author&query=Naik%2C+G+R">Ganesh R. Naik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Smart shoes have ushered in a new era of personalised health monitoring and
assistive technology. The shoe leverages technologies such as Bluetooth for
data collection and wireless transmission and incorporates features such as GPS
tracking, obstacle detection, and fitness tracking. This article provides an
overview of the current state of smart shoe technology, highlighting the
integration of advanced sensors for health monitoring, energy harvesting,
assistive features for the visually impaired, and deep learning for data
analysis. The study discusses the potential of smart footwear in medical
applications, particularly for patients with diabetes, and the ongoing research
in this field. Current footwear challenges are also discussed, including
complex construction, poor fit, comfort, and high cost.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01646" title="Abstract">arXiv:2402.01646</a> [<a href="/pdf/2402.01646" title="Download PDF">pdf</a>, <a href="/ps/2402.01646" title="Download PostScript">ps</a>, <a href="/format/2402.01646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recommendations for public action towards sustainable generative AI  systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goff%2C+T+L">Thomas Le Goff</a> (EDF)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> in French language
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Growing awareness of the environmental impact of digital technologies has led
to several isolated initiatives to promote sustainable practices. However,
despite these efforts, the environmental footprint of generative AI,
particularly in terms of greenhouse gas emissions and water consumption,
remains considerable. This contribution first presents the components of this
environmental footprint, highlighting the massive CO2 emissions and water
consumption associated with training large language models, thus underlining
the need to rethink learning and inference methods. The paper also explores the
factors and characteristics of models that have an influence on their
environmental footprint and demonstrates the existence of solutions to reduce
it, such as using more efficient processors or optimising the energy
performance of data centres. The potentially harmful effects of AI on the
planet and its ecosystem have made environmental protection one of the founding
principles of AI ethics at international and European levels. However, this
recognition has not yet translated into concrete measures to address it.To
address this issue, our contribution puts forward twelve pragmatic
recommendations for public action to promote sustainable generative AI, in
particular by building a long-term strategy to achieve carbon neutrality for AI
models, encouraging international cooperation to set common standards,
supporting scientific research and developing appropriate legal and regulatory
frameworks.This paper seeks to inform the members of the Interministerial
Committee on Generative AI about the environmental challenges of this
technology by providing a brief review of the scientific literature on the
subject and proposing concrete recommendations of public policy actions to
reconcile technological innovation with the need to protect our environment.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01647" title="Abstract">arXiv:2402.01647</a> [<a href="/pdf/2402.01647" title="Download PDF">pdf</a>, <a href="/format/2402.01647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Build Your Own Robot Friend: An Open-Source Learning Module for  Accessible and Engaging AI Education
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zhonghao Shi</a>, 
<a href="/search/cs?searchtype=author&query=O%27Connell%2C+A">Allison O&#x27;Connell</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongjian Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Siqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ayissi%2C+J">Jennifer Ayissi</a>, 
<a href="/search/cs?searchtype=author&query=Hoffman%2C+G">Guy Hoffman</a>, 
<a href="/search/cs?searchtype=author&query=Soleymani%2C+M">Mohammad Soleymani</a>, 
<a href="/search/cs?searchtype=author&query=Matari%C4%87%2C+M+J">Maja J. Matari&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Proceedings of the AAAI Conference on Artificial Intelligence (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">As artificial intelligence (AI) is playing an increasingly important role in
our society and global economy, AI education and literacy have become necessary
components in college and K-12 education to prepare students for an AI-powered
society. However, current AI curricula have not yet been made accessible and
engaging enough for students and schools from all socio-economic backgrounds
with different educational goals. In this work, we developed an open-source
learning module for college and high school students, which allows students to
build their own robot companion from the ground up. This open platform can be
used to provide hands-on experience and introductory knowledge about various
aspects of AI, including robotics, machine learning (ML), software engineering,
and mechanical engineering. Because of the social and personal nature of a
socially assistive robot companion, this module also puts a special emphasis on
human-centered AI, enabling students to develop a better understanding of
human-AI interaction and AI ethics through hands-on learning activities. With
open-source documentation, assembling manuals and affordable materials,
students from different socio-economic backgrounds can personalize their
learning experience based on their individual educational goals. To evaluate
the student-perceived quality of our module, we conducted a usability testing
workshop with 15 college students recruited from a minority-serving
institution. Our results indicate that our AI module is effective,
easy-to-follow, and engaging, and it increases student interest in studying
AI/ML and robotics in the future. We hope that this work will contribute toward
accessible and engaging AI education in human-AI interaction for college and
high school students.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01648" title="Abstract">arXiv:2402.01648</a> [<a href="/pdf/2402.01648" title="Download PDF">pdf</a>, <a href="/ps/2402.01648" title="Download PostScript">ps</a>, <a href="/format/2402.01648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forecasting Imports in OECD Member Countries and Iran by Using Neural  Network Algorithms of LSTM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khajoui%2C+S">Soheila Khajoui</a>, 
<a href="/search/cs?searchtype=author&query=Dehyadegari%2C+S">Saeid Dehyadegari</a>, 
<a href="/search/cs?searchtype=author&query=Jalaee%2C+S+A">Sayyed Abdolmajid Jalaee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Machine Learning (cs.LG); General Economics (econ.GN)

</div>
<p class="mathjax">Artificial Neural Networks (ANN) which are a branch of artificial
intelligence, have shown their high value in lots of applications and are used
as a suitable forecasting method. Therefore, this study aims at forecasting
imports in OECD member selected countries and Iran for 20 seasons from 2021 to
2025 by means of ANN. Data related to the imports of such countries collected
over 50 years from 1970 to 2019 from valid resources including World Bank, WTO,
IFM,the data turned into seasonal data to increase the number of collected data
for better performance and high accuracy of the network by using Diz formula
that there were totally 200 data related to imports. This study has used LSTM
to analyse data in Pycharm. 75% of data considered as training data and 25%
considered as test data and the results of the analysis were forecasted with
99% accuracy which revealed the validity and reliability of the output. Since
the imports is consumption function and since the consumption is influenced
during Covid-19 Pandemic, so it is time-consuming to correct and improve it to
be influential on the imports, thus the imports in the years after Covid-19
Pandemic has had a fluctuating trend.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01649" title="Abstract">arXiv:2402.01649</a> [<a href="/pdf/2402.01649" title="Download PDF">pdf</a>, <a href="/format/2402.01649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Analytical Review on Cybercrime in West Africa
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adewopo%2C+V">Victor Adewopo</a>, 
<a href="/search/cs?searchtype=author&query=Azumah%2C+S+W">Sylvia Worlali Azumah</a>, 
<a href="/search/cs?searchtype=author&query=Yakubu%2C+M+A">Mustapha Awinsongya Yakubu</a>, 
<a href="/search/cs?searchtype=author&query=Gyamfi%2C+E+K">Emmanuel Kojo Gyamfi</a>, 
<a href="/search/cs?searchtype=author&query=Ozer%2C+M">Murat Ozer</a>, 
<a href="/search/cs?searchtype=author&query=Elsayed%2C+N">Nelly Elsayed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Cybercrime is a growing concern in West Africa due to the increasing use of
technology and internet penetration in the region. Legal frameworks are
essential for guiding the control of cybercrime. However, the implementation
proves challenging for law enforcement agencies due to the absence of a
dedicated and effective regional institutional follow-up mechanism. This study
conducted a systematic literature review focusing on West Africa's prevalence
of cybercrime, governing policies, regulations, and methodologies for combating
cybercrime. West-Africa countries face significant cybercrime challenges,
exacerbated by inadequate resources and a dearth of security experts. This
study pinpoints potential cybercrime prevention strategies, such as leveraging
the Triage framework and broadening research to cover pivotal areas like cyber
aggression and cyberbullying. Our research findings highlight the urgency for
policymakers and law enforcement agencies to devise more efficient prevention
strategies and policies. Overall, this study provides invaluable insights into
the state of cybercrime in West Africa to guide the formulation of potent
prevention and intervention strategies.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01650" title="Abstract">arXiv:2402.01650</a> [<a href="/pdf/2402.01650" title="Download PDF">pdf</a>, <a href="/ps/2402.01650" title="Download PostScript">ps</a>, <a href="/format/2402.01650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effect of trip attributes on ridehailing driver trip request acceptance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tu%2C+Y">Yuanjie Tu</a>, 
<a href="/search/cs?searchtype=author&query=Khaloei%2C+M">Moein Khaloei</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+N+A">Nazmul Arefin Khan</a>, 
<a href="/search/cs?searchtype=author&query=MacKenzie%2C+D">Don MacKenzie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper in print at Journal of Sustainable Transportation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">A generalized additive mixed model was estimated to investigate the factors
that impact ridehailing driver trip request acceptance choices, relying on 200
responses from a stated preference survey in Seattle, US. Several policy
recommendations were proposed to promote trip request acceptance based on
ridehailing drivers willingness to accept compensation for undesired trip
features. The findings could be useful for transportation agencies to improve
ridehailing service efficiency, better fulfill urban mobility needs, and reduce
environmental burden.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01651" title="Abstract">arXiv:2402.01651</a> [<a href="/pdf/2402.01651" title="Download PDF">pdf</a>, <a href="/format/2402.01651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Informed AI Regulation: Comparing the Ethical Frameworks of Leading LLM  Chatbots Using an Ethics-Based Audit to Assess Moral Reasoning and Normative  Values
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chun%2C+J">Jon Chun</a>, 
<a href="/search/cs?searchtype=author&query=Elkins%2C+K">Katherine Elkins</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 6 figures (3 as tables), 1 table (in LaTeX)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the rise of individual and collaborative networks of autonomous agents,
AI is deployed in more key reasoning and decision-making roles. For this
reason, ethics-based audits play a pivotal role in the rapidly growing fields
of AI safety and regulation. This paper undertakes an ethics-based audit to
probe the 8 leading commercial and open-source Large Language Models including
GPT-4. We assess explicability and trustworthiness by a) establishing how well
different models engage in moral reasoning and b) comparing normative values
underlying models as ethical frameworks. We employ an experimental,
evidence-based approach that challenges the models with ethical dilemmas in
order to probe human-AI alignment. The ethical scenarios are designed to
require a decision in which the particulars of the situation may or may not
necessitate deviating from normative ethical principles. A sophisticated
ethical framework was consistently elicited in one model, GPT-4. Nonetheless,
troubling findings include underlying normative frameworks with clear bias
towards particular cultural norms. Many models also exhibit disturbing
authoritarian tendencies. Code is available at
https://github.com/jonchun/llm-sota-chatbots-ethics-based-audit.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01652" title="Abstract">arXiv:2402.01652</a> [<a href="/pdf/2402.01652" title="Download PDF">pdf</a>, <a href="/format/2402.01652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> User-Centric AI Analytics for Chronic Health Conditions Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ayesh%2C+A">Aladdin Ayesh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Keynote talk at IEEE Conference on Intelligent Methods, Systems, and Applications (IMSA), Cairo, Egypt, July 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">The use of AI analytics in health informatics has seen a rapid growth in
recent years. In this talk, we look at AI analytics use in managing chronic
health conditions such as diabetes, obesity, etc. We focus on the challenges in
managing these conditions especially with drug-free approaches due to the
variations in individual circumstances. These variations directed the research
into user-centric approach leading to variety of research questions. In this
short paper, we give examples from recent and current research work and
conclude with what, in our opinion, to be the next steps and some remaining
open research questions.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01653" title="Abstract">arXiv:2402.01653</a> [<a href="/pdf/2402.01653" title="Download PDF">pdf</a>, <a href="/ps/2402.01653" title="Download PostScript">ps</a>, <a href="/format/2402.01653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Child Impact Statements: Interdisciplinary Collaboration in Political  Science and Computer Science
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Windsor%2C+L+C">Leah Cathryn Windsor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Databases (cs.DB)

</div>
<p class="mathjax">Child Impact Statements (CIS) are instrumental in helping to foreground the
concerns and needs of minor community members who are too young to vote and
often unable to advocate for themselves politically. While many politicians and
policymakers assert they make decisions in the best interests of children, they
often lack the necessary information to meaningfully accomplish this. CISs are
akin to Environmental Impact Statements in that both give voice to constituents
who are often under-represented in policymaking. This paper highlights an
interdisciplinary collaboration between Social Science and Computer Science to
create a CIS tool for policymakers and community members in Shelby County, TN.
Furthermore, this type of collaboration is fruitful beyond the scope of the CIS
tool. Social scientists and computer scientists can leverage their
complementary skill sets in data management and data interpretation for the
benefit of their communities, advance scientific knowledge, and bridge
disciplinary divides within the academy.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01654" title="Abstract">arXiv:2402.01654</a> [<a href="/pdf/2402.01654" title="Download PDF">pdf</a>, <a href="/ps/2402.01654" title="Download PostScript">ps</a>, <a href="/format/2402.01654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Scoping Review of Energy Load Disaggregation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tolnai%2C+B+A">Bal&#xe1;zs Andr&#xe1;s Tolnai</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zheng Ma</a>, 
<a href="/search/cs?searchtype=author&query=J%C3%B8rgensen%2C+B+N">Bo N&#xf8;rregaard J&#xf8;rgensen</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Progress in Artificial Intelligence. EPIA 2023. Lecture Notes in
  Computer Science, vol 14116
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Energy load disaggregation can contribute to balancing power grids by
enhancing the effectiveness of demand-side management and promoting
electricity-saving behavior through increased consumer awareness. However, the
field currently lacks a comprehensive overview. To address this gap, this paper
con-ducts a scoping review of load disaggregation domains, data types, and
methods, by assessing 72 full-text journal articles. The findings reveal that
domestic electricity consumption is the most researched area, while others,
such as industrial load disaggregation, are rarely discussed. The majority of
research uses relatively low-frequency data, sampled between 1 and 60 seconds.
A wide variety of methods are used, and artificial neural networks are the most
common, followed by optimization strategies, Hidden Markov Models, and Graph
Signal Processing approaches.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01655" title="Abstract">arXiv:2402.01655</a> [<a href="/pdf/2402.01655" title="Download PDF">pdf</a>, <a href="/format/2402.01655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Deep Learning Approach Towards Student Performance Prediction in  Online Courses: Challenges Based on a Global Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moubayed%2C+A">Abdallah Moubayed</a>, 
<a href="/search/cs?searchtype=author&query=Injadat%2C+M">MohammadNoor Injadat</a>, 
<a href="/search/cs?searchtype=author&query=Alhindawi%2C+N">Nouh Alhindawi</a>, 
<a href="/search/cs?searchtype=author&query=Samara%2C+G">Ghassan Samara</a>, 
<a href="/search/cs?searchtype=author&query=Abuasal%2C+S">Sara Abuasal</a>, 
<a href="/search/cs?searchtype=author&query=Alazaidah%2C+R">Raed Alazaidah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted and presented in 24th International Arab Conference on Information Technology (ACIT'2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Analyzing and evaluating students' progress in any learning environment is
stressful and time consuming if done using traditional analysis methods. This
is further exasperated by the increasing number of students due to the shift of
focus toward integrating the Internet technologies in education and the focus
of academic institutions on moving toward e-Learning, blended, or online
learning models. As a result, the topic of student performance prediction has
become a vibrant research area in recent years. To address this, machine
learning and data mining techniques have emerged as a viable solution. To that
end, this work proposes the use of deep learning techniques (CNN and RNN-LSTM)
to predict the students' performance at the midpoint stage of the online course
delivery using three distinct datasets collected from three different regions
of the world. Experimental results show that deep learning models have
promising performance as they outperform other optimized traditional ML models
in two of the three considered datasets while also having comparable
performance for the third dataset.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01656" title="Abstract">arXiv:2402.01656</a> [<a href="/pdf/2402.01656" title="Download PDF">pdf</a>, <a href="/format/2402.01656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Promises and pitfalls of artificial intelligence for legal applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kapoor%2C+S">Sayash Kapoor</a>, 
<a href="/search/cs?searchtype=author&query=Henderson%2C+P">Peter Henderson</a>, 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+A">Arvind Narayanan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Is AI set to redefine the legal profession? We argue that this claim is not
supported by the current evidence. We dive into AI's increasingly prevalent
roles in three types of legal tasks: information processing; tasks involving
creativity, reasoning, or judgment; and predictions about the future. We find
that the ease of evaluating legal applications varies greatly across legal
tasks, based on the ease of identifying correct answers and the observability
of information relevant to the task at hand. Tasks that would lead to the most
significant changes to the legal profession are also the ones most prone to
overoptimism about AI capabilities, as they are harder to evaluate. We make
recommendations for better evaluation and deployment of AI in legal contexts.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01657" title="Abstract">arXiv:2402.01657</a> [<a href="/pdf/2402.01657" title="Download PDF">pdf</a>, <a href="/format/2402.01657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tapping into the Natural Language System with Artificial Languages when  Learning Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hartmann%2C+E+M">Elisa Madeleine Hartmann</a>, 
<a href="/search/cs?searchtype=author&query=Bergum%2C+A">Annabelle Bergum</a>, 
<a href="/search/cs?searchtype=author&query=Gorgosch%2C+D">Dominik Gorgosch</a>, 
<a href="/search/cs?searchtype=author&query=Peitek%2C+N">Norman Peitek</a>, 
<a href="/search/cs?searchtype=author&query=Apel%2C+S">Sven Apel</a>, 
<a href="/search/cs?searchtype=author&query=Siegmund%2C+J">Janet Siegmund</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Background: In times when the ability to program is becoming increasingly
important, it is still difficult to teach students to become successful
programmers. One remarkable aspect are recent findings from neuro-imaging
studies, which suggest a consistent role of language competency of novice
programmers when they learn programming. Thus, for effectively teaching
programming, it might be beneficial to draw from linguistic research,
especially from foreign language acquisition.
<br />Objective: The goal of this study is to investigate the feasibility of this
idea, such that we can enhance learning programming by activating language
learning mechanisms.
<br />Method: To this end, we conducted an empirical study, in which we taught one
group of students an artificial language, while another group received an
introduction into Git as control condition, before we taught both groups basic
programming knowledge in a programming course.
<br />Result: We observed that the training of the artificial language can be
easily integrated into our curriculum. Furthermore, we observed that language
learning strategies were activated and that participants perceived similarities
between learning the artificial language and the programming language. However,
within the context of our study, we did not find a significant benefit for
programming competency when students learned an artificial language first.
<br />Conclusion: Our study lays the methodological foundation to explore the use
of natural language acquisition research and expand this field step by step. We
report our experience here to guide research and to open up the possibilities
from the field of linguistic research to improve programming acquisition.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01658" title="Abstract">arXiv:2402.01658</a> [<a href="/pdf/2402.01658" title="Download PDF">pdf</a>, <a href="/ps/2402.01658" title="Download PostScript">ps</a>, <a href="/format/2402.01658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Untersuchung der Wirkung von Data Storytelling auf das Datenverstaendnis  von Dashboard-Nutzern
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zitz%2C+V">Valeria Zitz</a>, 
<a href="/search/cs?searchtype=author&query=Baier%2C+P">Patrick Baier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> in German language
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Tagungsband zur 36. AKWI-Jahrestagung 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">With the increasing use of big data and business analytics, data storytelling
has gained popularity as an effective means of communicating analytical
insights to audiences to support decision making and improve business
performance. However, there is little empirical evidence on the impact of data
storytelling on data understanding. This study validates the concept of data
storytelling as a construct in terms of its impact on users' data
understanding. Based on empirical data analysis, the results of this study show
that data storytelling competence is positively associated with organizational
performance, which is partly due to the quality of the decision is conveyed.
These results provide a theoretical basis for further investigation of
potential antecedents and consequences of data storytelling.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01659" title="Abstract">arXiv:2402.01659</a> [<a href="/pdf/2402.01659" title="Download PDF">pdf</a>, <a href="/ps/2402.01659" title="Download PostScript">ps</a>, <a href="/format/2402.01659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Artificial Intelligence in Higher Education: Evidence from an  Analysis of Institutional Policies and Guidelines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McDonald%2C+N">Nora McDonald</a>, 
<a href="/search/cs?searchtype=author&query=Johri%2C+A">Aditya Johri</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+A">Areej Ali</a>, 
<a href="/search/cs?searchtype=author&query=Hingle%2C+A">Aayushi Hingle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The release of ChatGPT in November 2022 prompted a massive uptake of
generative artificial intelligence (GenAI) across higher education institutions
(HEIs). HEIs scrambled to respond to its use, especially by students, looking
first to regulate it and then arguing for its productive integration within
teaching and learning. In the year since the release, HEIs have increasingly
provided policies and guidelines to direct GenAI. In this paper we examined
documents produced by 116 US universities categorized as high research activity
or R1 institutions to comprehensively understand GenAI related advice and
guidance given to institutional stakeholders. Through an extensive analysis, we
found the majority of universities (N=73, 63%) encourage the use of GenAI and
many provide detailed guidance for its use in the classroom (N=48, 41%). More
than half of all institutions provided sample syllabi (N=65, 56%) and half
(N=58, 50%) provided sample GenAI curriculum and activities that would help
instructors integrate and leverage GenAI in their classroom. Notably, most
guidance for activities focused on writing, whereas code and STEM-related
activities were mentioned half the time and vaguely even when they were (N=58,
50%). Finally, more than one half of institutions talked about the ethics of
GenAI on a range of topics broadly, including Diversity, Equity and Inclusion
(DEI) (N=60, 52%). Overall, based on our findings we caution that guidance for
faculty can become burdensome as extensive revision of pedagogical approaches
is often recommended in the policies.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01660" title="Abstract">arXiv:2402.01660</a> [<a href="/pdf/2402.01660" title="Download PDF">pdf</a>, <a href="/ps/2402.01660" title="Download PostScript">ps</a>, <a href="/format/2402.01660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integration of LaTeX formula in computer-based test application for  academic purposes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Onyenwe%2C+I+E">Ikechukwu E. Onyenwe</a>, 
<a href="/search/cs?searchtype=author&query=Onyedinma%2C+E">Ebele Onyedinma</a>, 
<a href="/search/cs?searchtype=author&query=Ikechukwu-Onyenwe%2C+O+O">Onyedika O. Ikechukwu-Onyenwe</a>, 
<a href="/search/cs?searchtype=author&query=Agbata%2C+O">Obinna Agbata</a>, 
<a href="/search/cs?searchtype=author&query=Tubo%2C+F+N">Faustinah N. Tubo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">LaTeX is a free document preparation system that handles the typesetting of
mathematical expressions smoothly and elegantly. It has become the standard
format for creating and publishing research articles in mathematics and many
scientific fields. Computer-based testing (CBT) has become widespread in recent
years. Most establishments now use it to deliver assessments as an alternative
to using the pen-paper method. To deliver an assessment, the examiner would
first add a new exam or edit an existing exam using a CBT editor. Thus, the
implementation of CBT should comprise both support for setting and
administering questions. Existing CBT applications used in the academic space
lacks the capacity to handle advanced formulas, programming codes, and tables,
thereby resorting to converting them into images which takes a lot of time and
storage space. In this paper, we discuss how we solvde this problem by
integrating latex technology into our CBT applications. This enables seamless
manipulation and accurate rendering of tables, programming codes, and equations
to increase readability and clarity on both the setting and administering of
questions platforms. Furthermore, this implementation has reduced drastically
the sizes of system resources allocated to converting tables, codes, and
equations to images. Those in mathematics, statistics, computer science,
engineering, chemistry, etc. will find this application useful.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01661" title="Abstract">arXiv:2402.01661</a> [<a href="/pdf/2402.01661" title="Download PDF">pdf</a>, <a href="/ps/2402.01661" title="Download PostScript">ps</a>, <a href="/format/2402.01661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tracing the Genealogies of Ideas with Large Language Model Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lucian Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">In this paper, I present a novel method to detect intellectual influence
across a large corpus. Taking advantage of the unique affordances of large
language models in encoding semantic and structural meaning while remaining
robust to paraphrasing, we can search for substantively similar ideas and hints
of intellectual influence in a computationally efficient manner. Such a method
allows us to operationalize different levels of confidence: we can allow for
direct quotation, paraphrase, or speculative similarity while remaining open
about the limitations of each threshold. I apply an ensemble method combining
General Text Embeddings, a state-of-the-art sentence embedding method optimized
to capture semantic content and an Abstract Meaning Representation graph
representation designed to capture structural similarities in argumentation
style and the use of metaphor. I apply this method to vectorize sentences from
a corpus of roughly 400,000 nonfiction books and academic publications from the
19th century for instances of ideas and arguments appearing in Darwin's
publications. This functions as an initial evaluation and proof of concept; the
method is not limited to detecting Darwinian ideas but is capable of detecting
similarities on a large scale in a wide range of corpora and contexts.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01662" title="Abstract">arXiv:2402.01662</a> [<a href="/pdf/2402.01662" title="Download PDF">pdf</a>, <a href="/ps/2402.01662" title="Download PostScript">ps</a>, <a href="/format/2402.01662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Ghosts: Anticipating Benefits and Risks of AI Afterlives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morris%2C+M+R">Meredith Ringel Morris</a>, 
<a href="/search/cs?searchtype=author&query=Brubaker%2C+J+R">Jed R. Brubaker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As AI systems quickly improve in both breadth and depth of performance, they
lend themselves to creating increasingly powerful and realistic agents,
including the possibility of agents modeled on specific people. We anticipate
that within our lifetimes it may become common practice for people to create a
custom AI agent to interact with loved ones and/or the broader world after
death. We call these generative ghosts, since such agents will be capable of
generating novel content rather than merely parroting content produced by their
creator while living. In this paper, we first discuss the design space of
potential implementations of generative ghosts. We then discuss the practical
and ethical implications of generative ghosts, including potential positive and
negative impacts on individuals and society. Based on these considerations, we
lay out a research agenda for the AI and HCI research communities to empower
people to create and interact with AI afterlives in a safe and beneficial
manner.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01663" title="Abstract">arXiv:2402.01663</a> [<a href="/pdf/2402.01663" title="Download PDF">pdf</a>, <a href="/format/2402.01663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Killer Apps: Low-Speed, Large-Scale AI Weapons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feldman%2C+P">Philip Feldman</a>, 
<a href="/search/cs?searchtype=author&query=Dant%2C+A">Aaron Dant</a>, 
<a href="/search/cs?searchtype=author&query=Foulds%2C+J+R">James R. Foulds</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages with 10 pages of appendices. 3 Figures, 2 code listings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">The accelerating advancements in Artificial Intelligence (AI) and Machine
Learning (ML), highlighted by the development of cutting-edge Generative
Pre-trained Transformer (GPT) models by organizations such as OpenAI, Meta, and
Anthropic, present new challenges and opportunities in warfare and security.
Much of the current focus is on AI's integration within weapons systems and its
role in rapid decision-making in kinetic conflict. However, an equally
important but often overlooked aspect is the potential of AI-based
psychological manipulation at internet scales within the information domain.
These capabilities could pose significant threats to individuals,
organizations, and societies globally. This paper explores the concept of AI
weapons, their deployment, detection, and potential countermeasures.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01664" title="Abstract">arXiv:2402.01664</a> [<a href="/pdf/2402.01664" title="Download PDF">pdf</a>, <a href="/ps/2402.01664" title="Download PostScript">ps</a>, <a href="/format/2402.01664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Edge Offloading in Smart Grid
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arcas%2C+G+I">Gabriel Ioan Arcas</a>, 
<a href="/search/cs?searchtype=author&query=Cioara%2C+T">Tudor Cioara</a>, 
<a href="/search/cs?searchtype=author&query=Anghel%2C+I">Ionut Anghel</a>, 
<a href="/search/cs?searchtype=author&query=Lazea%2C+D">Dragos Lazea</a>, 
<a href="/search/cs?searchtype=author&query=Hangan%2C+A">Anca Hangan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to be submitted to journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">The energy transition supports the shift towards more sustainable energy
alternatives, paving towards decentralized smart grids, where the energy is
generated closer to the point of use. The decentralized smart grids foresee
novel data-driven low latency applications for improving resilience and
responsiveness, such as peer-to-peer energy trading, microgrid control, fault
detection, or demand response. However, the traditional cloud-based smart grid
architectures are unable to meet the requirements of the new emerging
applications such as low latency and high-reliability thus alternative
architectures such as edge, fog, or hybrid need to be adopted. Moreover, edge
offloading can play a pivotal role for the next-generation smart grid AI
applications because it enables the efficient utilization of computing
resources and addresses the challenges of increasing data generated by IoT
devices, optimizing the response time, energy consumption, and network
performance. However, a comprehensive overview of the current state of research
is needed to support sound decisions regarding energy-related applications
offloading from cloud to fog or edge, focusing on smart grid open challenges
and potential impacts. In this paper, we delve into smart grid and
computational distribution architec-tures, including edge-fog-cloud models,
orchestration architecture, and serverless computing, and analyze the
decision-making variables and optimization algorithms to assess the efficiency
of edge offloading. Finally, the work contributes to a comprehensive
understanding of the edge offloading in smart grid, providing a SWOT analysis
to support decision making.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01665" title="Abstract">arXiv:2402.01665</a> [<a href="/pdf/2402.01665" title="Download PDF">pdf</a>, <a href="/format/2402.01665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge-Driven Deep Learning Paradigms for Wireless Network  Optimization in 6G
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+R">Ruijin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+N">Nan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Changle Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Fangjiong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wen Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In the sixth-generation (6G) networks, newly emerging diversified services of
massive users in dynamic network environments are required to be satisfied by
multi-dimensional heterogeneous resources. The resulting large-scale
complicated network optimization problems are beyond the capability of
model-based theoretical methods due to the overwhelming computational
complexity and the long processing time. Although with fast online inference
and universal approximation ability, data-driven deep learning (DL) heavily
relies on abundant training data and lacks interpretability. To address these
issues, a new paradigm called knowledge-driven DL has emerged, aiming to
integrate proven domain knowledge into the construction of neural networks,
thereby exploiting the strengths of both methods. This article provides a
systematic review of knowledge-driven DL in wireless networks. Specifically, a
holistic framework of knowledge-driven DL in wireless networks is proposed,
where knowledge sources, knowledge representation, knowledge integration and
knowledge application are forming as a closed loop. Then, a detailed taxonomy
of knowledge integration approaches, including knowledge-assisted,
knowledge-fused, and knowledge-embedded DL, is presented. Several open issues
for future research are also discussed. The insights offered in this article
provide a basic principle for the design of network optimization that
incorporates communication-specific domain knowledge and DL, facilitating the
realization of intelligent 6G networks.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01666" title="Abstract">arXiv:2402.01666</a> [<a href="/pdf/2402.01666" title="Download PDF">pdf</a>, <a href="/format/2402.01666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Exploration of Personalized Learning in Smart Education:  From Student Modeling to Personalized Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Siyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+J">Jiajun Cui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Runze Li</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+H">Hong Qian</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+B">Bo Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 82 pages,5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">With the development of artificial intelligence, personalized learning has
attracted much attention as an integral part of intelligent education. China,
the United States, the European Union, and others have put forward the
importance of personalized learning in recent years, emphasizing the
realization of the organic combination of large-scale education and
personalized training. The development of a personalized learning system
oriented to learners' preferences and suited to learners' needs should be
accelerated. This review provides a comprehensive analysis of the current
situation of personalized learning and its key role in education. It discusses
the research on personalized learning from multiple perspectives, combining
definitions, goals, and related educational theories to provide an in-depth
understanding of personalized learning from an educational perspective,
analyzing the implications of different theories on personalized learning, and
highlighting the potential of personalized learning to meet the needs of
individuals and to enhance their abilities. Data applications and assessment
indicators in personalized learning are described in detail, providing a solid
data foundation and evaluation system for subsequent research. Meanwhile, we
start from both student modeling and recommendation algorithms and deeply
analyze the cognitive and non-cognitive perspectives and the contribution of
personalized recommendations to personalized learning. Finally, we explore the
challenges and future trajectories of personalized learning. This review
provides a multidimensional analysis of personalized learning through a more
comprehensive study, providing academics and practitioners with cutting-edge
explorations to promote continuous progress in the field of personalized
learning.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01667" title="Abstract">arXiv:2402.01667</a> [<a href="/pdf/2402.01667" title="Download PDF">pdf</a>, <a href="/ps/2402.01667" title="Download PostScript">ps</a>, <a href="/format/2402.01667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Students&#x27; accommodation allocation: A Multicriteria Decision Support  System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rasoanaivo%2C+R+G">R&#xf4;lin Gabriel Rasoanaivo</a> (IRIT, UT Capitole), 
<a href="/search/cs?searchtype=author&query=Zarat%C3%A9%2C+P">Pascale Zarat&#xe9;</a> (IRIT, UT Capitole, IRIT-ADRIA)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> SADIO electronic journal of informatics and operations research,
  2023, 22 (3)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">The social life of students at university has an impact on their educational
success. The allocation of accommodation is part of this aspect. This article
presents our proposal to improve students' allocation accommodation. We aim to
support university administrative departments for the selection of students for
housing. Therefore, we propose a decision support system based on
multi-criteria decision support methods. To calculate the weights of the
criteria, we use the AHP method. Then, to rank the students, AHP, Weighted Sum
Method and PROMETHEE methods are used. The aim is to find the most adequate
method to rank the students. The result is achieved because the AHP is able to
calculate the weight of criteria and the AHP, SWM and PROMETHEE are able to
rank the students.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01668" title="Abstract">arXiv:2402.01668</a> [<a href="/pdf/2402.01668" title="Download PDF">pdf</a>, <a href="/format/2402.01668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Determining the Difficulties of Students With Dyslexia via Virtual  Reality and Artificial Intelligence: An Exploratory Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yeguas-Bol%C3%ADvar%2C+E">Enrique Yeguas-Bol&#xed;var</a>, 
<a href="/search/cs?searchtype=author&query=Alcalde-Llergo%2C+J+M">Jos&#xe9; M. Alcalde-Llergo</a>, 
<a href="/search/cs?searchtype=author&query=Aparicio-Mart%C3%ADnez%2C+P">Pilar Aparicio-Mart&#xed;nez</a>, 
<a href="/search/cs?searchtype=author&query=Taborri%2C+J">Juri Taborri</a>, 
<a href="/search/cs?searchtype=author&query=Zingoni%2C+A">Andrea Zingoni</a>, 
<a href="/search/cs?searchtype=author&query=Pinzi%2C+S">Sara Pinzi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, 3 tables, MetroXRAINE 2022 Conference, VRAILEXIA european project
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2022 IEEE International Conference on Metrology for Extended
  Reality, Artificial Intelligence and Neural Engineering (MetroXRAINE), Rome,
  Italy, 2022, pp. 585-590
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Learning disorders are neurological conditions that affect the brain's
ability to interconnect communication areas. Dyslexic students experience
problems with reading, memorizing, and exposing concepts; however the magnitude
of these can be mitigated through both therapies and the creation of
compensatory mechanisms. Several efforts have been made to mitigate these
issues, leading to the creation of digital resources for students with specific
learning disorders attending primary and secondary education levels.
Conversely, a standard approach is still missed in higher education. The
VRAIlexia project has been created to tackle this issue by proposing two
different tools: a mobile application integrating virtual reality (VR) to
collect data quickly and easily, and an artificial intelligencebased software
(AI) to analyze the collected data for customizing the supporting methodology
for each student. The first one has been created and is being distributed among
dyslexic students in Higher Education Institutions, for the conduction of
specific psychological and psychometric tests. The second tool applies specific
artificial intelligence algorithms to the data gathered via the application and
other surveys. These AI techniques have allowed us to identify the most
relevant difficulties faced by the students' cohort. Our different models have
obtained around 90\% mean accuracy for predicting the support tools and
learning strategies.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01669" title="Abstract">arXiv:2402.01669</a> [<a href="/pdf/2402.01669" title="Download PDF">pdf</a>, <a href="/format/2402.01669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Performances and Motivation in Intelligent Tutoring Systems:  Combining Machine Learning and Learner Choice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cl%C3%A9ment%2C+B">Benjamin Cl&#xe9;ment</a> (1 adn 3), 
<a href="/search/cs?searchtype=author&query=Sauz%C3%A9on%2C+H">H&#xe9;l&#xe8;ne Sauz&#xe9;on</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Roy%2C+D">Didier Roy</a> (1), 
<a href="/search/cs?searchtype=author&query=Oudeyer%2C+P">Pierre-Yves Oudeyer</a> (1) ((1) Inria FLOWERS team Talence France, (2) Universit&#xe9; de Bordeaux BPH lab Bordeaux France, (3) EvidenceB Paris France)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 37 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large class sizes pose challenges to personalized learning in schools, which
educational technologies, especially intelligent tutoring systems (ITS), aim to
address. In this context, the ZPDES algorithm, based on the Learning Progress
Hypothesis (LPH) and multi-armed bandit machine learning techniques, sequences
exercises that maximize learning progress (LP). This algorithm was previously
shown in field studies to boost learning performances for a wider diversity of
students compared to a hand-designed curriculum. However, its motivational
impact was not assessed. Also, ZPDES did not allow students to express choices.
This limitation in agency is at odds with the LPH theory concerned with
modeling curiosity-driven learning. We here study how the introduction of such
choice possibilities impact both learning efficiency and motivation. The given
choice concerns dimensions that are orthogonal to exercise difficulty, acting
as a playful feature.
<br />In an extensive field study (265 7-8 years old children, RCT design), we
compare systems based either on ZPDES or a hand-designed curriculum, both with
and without self-choice. We first show that ZPDES improves learning performance
and produces a positive and motivating learning experience. We then show that
the addition of choice triggers intrinsic motivation and reinforces the
learning effectiveness of the LP-based personalization. In doing so, it
strengthens the links between intrinsic motivation and performance progress
during the serious game. Conversely, deleterious effects of the playful feature
are observed for hand-designed linear paths. Thus, the intrinsic motivation
elicited by a playful feature is beneficial only if the curriculum
personalization is effective for the learner. Such a result deserves great
attention due to increased use of playful features in non adaptive educational
technologies.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01670" title="Abstract">arXiv:2402.01670</a> [<a href="/pdf/2402.01670" title="Download PDF">pdf</a>, <a href="/format/2402.01670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Scalable and Automated Framework for Tracking the likely Adoption of  Emerging Technologies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Williams%2C+L">Lowri Williams</a>, 
<a href="/search/cs?searchtype=author&query=Anthi%2C+E">Eirini Anthi</a>, 
<a href="/search/cs?searchtype=author&query=Burnap%2C+P">Pete Burnap</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">While new technologies are expected to revolutionise and become game-changers
in improving the efficiencies and practises of our daily lives, it is also
critical to investigate and understand the barriers and opportunities faced by
their adopters. Such findings can serve as an additional feature in the
decision-making process when analysing the risks, costs, and benefits of
adopting an emerging technology in a particular setting. Although several
studies have attempted to perform such investigations, these approaches adopt a
qualitative data collection methodology which is limited in terms of the size
of the targeted participant group and is associated with a significant manual
overhead when transcribing and inferring results. This paper presents a
scalable and automated framework for tracking likely adoption and/or rejection
of new technologies from a large landscape of adopters. In particular, a large
corpus of social media texts containing references to emerging technologies was
compiled. Text mining techniques were applied to extract sentiments expressed
towards technology aspects. In the context of the problem definition herein, we
hypothesise that the expression of positive sentiment infers an increase in the
likelihood of impacting a technology user's acceptance to adopt, integrate,
and/or use the technology, and negative sentiment infers an increase in the
likelihood of impacting the rejection of emerging technologies by adopters. To
quantitatively test our hypothesis, a ground truth analysis was performed to
validate that the sentiment captured by the text mining approach is comparable
to the results given by human annotators when asked to label whether such texts
positively or negatively impact their outlook towards adopting an emerging
technology.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01671" title="Abstract">arXiv:2402.01671</a> [<a href="/pdf/2402.01671" title="Download PDF">pdf</a>, <a href="/ps/2402.01671" title="Download PostScript">ps</a>, <a href="/format/2402.01671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Analytics Dashboards for Advisors -- A Systematic Literature  Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vemula%2C+S+R">Suchith Reddy Vemula</a> (1), 
<a href="/search/cs?searchtype=author&query=Moraes%2C+M">Marcia Moraes</a> (1) ((1) Colorado State University, USA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Learning Analytics Dashboard for Advisors is designed to provide data-driven
insights and visualizations to support advisors in their decision-making
regarding student academic progress, engagement, targeted support, and overall
success. This study explores the current state of the art in learning analytics
dashboards, focusing on specific requirements for advisors. By examining
existing literature and case studies, this research investigates the key
features and functionalities essential for an effective learning analytics
dashboard tailored to advisor needs. This study also aims to provide a
comprehensive understanding of the landscape of learning analytics dashboards
for advisors, offering insights into the advancements, opportunities, and
challenges in their development by synthesizing the current trends from a total
of 21 research papers used for analysis. The findings will contribute to the
design and implementation of new features in learning analytics dashboards that
empower advisors to provide proactive and individualized support, ultimately
fostering student retention and academic success.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01672" title="Abstract">arXiv:2402.01672</a> [<a href="/pdf/2402.01672" title="Download PDF">pdf</a>, <a href="/format/2402.01672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prerequisite Structure Discovery in Intelligent Tutoring Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Annabi%2C+L">Louis Annabi</a> (Flowers, U2IS), 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+S+M">Sao Mai Nguyen</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE International Conference on Development and Learning
  (ICDL), Nov 2023, Macau, China. pp.176-181
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper addresses the importance of Knowledge Structure (KS) and Knowledge
Tracing (KT) in improving the recommendation of educational content in
intelligent tutoring systems. The KS represents the relations between different
Knowledge Components (KCs), while KT predicts a learner's success based on her
past history. The contribution of this research includes proposing a KT model
that incorporates the KS as a learnable parameter, enabling the discovery of
the underlying KS from learner trajectories. The quality of the uncovered KS is
assessed by using it to recommend content and evaluating the recommendation
algorithm with simulated students.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01673" title="Abstract">arXiv:2402.01673</a> [<a href="/pdf/2402.01673" title="Download PDF">pdf</a>, <a href="/ps/2402.01673" title="Download PostScript">ps</a>, <a href="/format/2402.01673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Legal and ethical implications of applications based on agreement  technologies: the case of auction-based road intersections
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Santos%2C+J">Jos&#xe9;-Antonio Santos</a>, 
<a href="/search/cs?searchtype=author&query=Fern%C3%A1ndez%2C+A">Alberto Fern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Moreno-Rebato%2C+M">Mar Moreno-Rebato</a>, 
<a href="/search/cs?searchtype=author&query=Billhardt%2C+H">Holger Billhardt</a>, 
<a href="/search/cs?searchtype=author&query=Rodr%C3%ADguez-Garc%C3%ADa%2C+J">Jos&#xe9;-A. Rodr&#xed;guez-Garc&#xed;a</a>, 
<a href="/search/cs?searchtype=author&query=Ossowski%2C+S">Sascha Ossowski</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Artif. Intell. Law 28(4): 385-414 (2020)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Agreement Technologies refer to a novel paradigm for the construction of
distributed intelligent systems, where autonomous software agents negotiate to
reach agreements on behalf of their human users. Smart Cities are a key
application domain for Agreement Technologies. While several proofs of concept
and prototypes exist, such systems are still far from ready for being deployed
in the real-world. In this paper we focus on a novel method for managing
elements of smart road infrastructures of the future, namely the case of
auction-based road intersections. We show that, even though the key
technological elements for such methods are already available, there are
multiple non-technical issues that need to be tackled before they can be
applied in practice. For this purpose, we analyse legal and ethical
implications of auction-based road intersections in the context of
international regulations and from the standpoint of the Spanish legislation.
From this exercise, we extract a set of required modifications, of both
technical and legal nature, which need to be addressed so as to pave the way
for the potential real-world deployment of such systems in a future that may
not be too far away.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01674" title="Abstract">arXiv:2402.01674</a> [<a href="/pdf/2402.01674" title="Download PDF">pdf</a>, <a href="/ps/2402.01674" title="Download PostScript">ps</a>, <a href="/format/2402.01674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using ChatGPT for Science Learning: A Study on Pre-service Teachers&#x27;  Lesson Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+G">Gyeong-Geon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+X">Xiaoming Zhai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Despite the buzz around ChatGPT's potential, empirical studies exploring its
actual utility in the classroom for learning remain scarce. This study aims to
fill this gap by analyzing the lesson plans developed by 29 pre-service
elementary teachers from a Korean university and assessing how they integrated
ChatGPT into science learning activities. We first examined how the subject
domains and teaching and learning methods/strategies were integrated with
ChatGPT in the lesson plans. We then evaluated the lesson plans using a
modified TPACK-based rubric. We further examined pre-service teachers'
perceptions and concerns about integrating ChatGPT into science learning.
Results show diverse applications of ChatGPT in different science domains.
Fourteen types of teaching and learning methods/strategies were identified in
the lesson plans. On average, the pre-service teachers' lesson plans scored
high on the modified TPACK-based rubric, indicating a reasonable envisage of
integrating ChatGPT into science learning, particularly in 'instructional
strategies &amp; ChatGPT'. However, they scored relatively lower on exploiting
ChatGPT's functions toward its full potential compared to other aspects. The
study also identifies both appropriate and inappropriate use cases of ChatGPT
in lesson planning. Pre-service teachers anticipated ChatGPT to afford
high-quality questioning, self-directed learning, individualized learning
support, and formative assessment. Meanwhile, they also expressed concerns
about its accuracy and the risks that students may be overly dependent on
ChatGPT. They further suggested solutions to systemizing classroom dynamics
between teachers and students. The study underscores the need for more research
on the roles of generative AI in actual classroom settings and provides
insights for future AI-integrated science learning.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01675" title="Abstract">arXiv:2402.01675</a> [<a href="/pdf/2402.01675" title="Download PDF">pdf</a>, <a href="/format/2402.01675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resource-efficient In-orbit Detection of Earth Objects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qiyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+R">Ruolin Xing</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yiran Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zimu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mengwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Dustdar%2C+S">Schahram Dustdar</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shangguang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE INFOCOM 2024-IEEE Conference on Computer Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">With the rapid proliferation of large Low Earth Orbit (LEO) satellite
constellations, a huge amount of in-orbit data is generated and needs to be
transmitted to the ground for processing. However, traditional LEO satellite
constellations, which downlink raw data to the ground, are significantly
restricted in transmission capability. Orbital edge computing (OEC), which
exploits the computation capacities of LEO satellites and processes the raw
data in orbit, is envisioned as a promising solution to relieve the downlink
burden. Yet, with OEC, the bottleneck is shifted to the inelastic computation
capacities. The computational bottleneck arises from two primary challenges
that existing satellite systems have not adequately addressed: the inability to
process all captured images and the limited energy supply available for
satellite operations. In this work, we seek to fully exploit the scarce
satellite computation and communication resources to achieve satellite-ground
collaboration and present a satellite-ground collaborative system named
TargetFuse for onboard object detection. TargetFuse incorporates a combination
of techniques to minimize detection errors under energy and bandwidth
constraints. Extensive experiments show that TargetFuse can reduce detection
errors by 3.4 times on average, compared to onboard computing. TargetFuse
achieves a 9.6 times improvement in bandwidth efficiency compared to the
vanilla baseline under the limited bandwidth budget constraint.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01676" title="Abstract">arXiv:2402.01676</a> [<a href="/pdf/2402.01676" title="Download PDF">pdf</a>, <a href="/format/2402.01676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language models align with human judgments on key grammatical  constructions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jennifer Hu</a>, 
<a href="/search/cs?searchtype=author&query=Mahowald%2C+K">Kyle Mahowald</a>, 
<a href="/search/cs?searchtype=author&query=Lupyan%2C+G">Gary Lupyan</a>, 
<a href="/search/cs?searchtype=author&query=Ivanova%2C+A">Anna Ivanova</a>, 
<a href="/search/cs?searchtype=author&query=Levy%2C+R">Roger Levy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Response to Dentella et al. (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Do Large Language Models (LLMs) make human-like linguistic generalizations?
Dentella et al. (2023; "DGL") prompt several LLMs ("Is the following sentence
grammatically correct in English?") to elicit grammaticality judgments of 80
English sentences, concluding that LLMs demonstrate a "yes-response bias" and a
"failure to distinguish grammatical from ungrammatical sentences". We
re-evaluate LLM performance using well-established practices and find that
DGL's data in fact provide evidence for just how well LLMs capture human
behaviors. Models not only achieve high accuracy overall, but also capture
fine-grained variation in human linguistic judgments.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01677" title="Abstract">arXiv:2402.01677</a> [<a href="/pdf/2402.01677" title="Download PDF">pdf</a>, <a href="/format/2402.01677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Embedding Ontologies via Incoprorating Extensional and Intensional  Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Keyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+G">Guilin Qi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiaoyan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tianxing Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitting to IJCAI2024; 9 pages and 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Ontologies contain rich knowledge within domain, which can be divided into
two categories, namely extensional knowledge and intensional knowledge.
Extensional knowledge provides information about the concrete instances that
belong to specific concepts in the ontology, while intensional knowledge
details inherent properties, characteristics, and semantic associations among
concepts. However, existing ontology embedding approaches fail to take both
extensional knowledge and intensional knowledge into fine consideration
simultaneously. In this paper, we propose a novel ontology embedding approach
named EIKE (Extensional and Intensional Knowledge Embedding) by representing
ontologies in two spaces, called extensional space and intensional space. EIKE
presents a unified framework for embedding instances, concepts and their
relations in an ontology, applying a geometry-based method to model extensional
knowledge and a pretrained language model to model intensional knowledge, which
can capture both structure information and textual information. Experimental
results show that EIKE significantly outperforms state-of-the-art methods in
three datasets for both triple classification and link prediction, indicating
that EIKE provides a more comprehensive and representative perspective of the
domain.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01679" title="Abstract">arXiv:2402.01679</a> [<a href="/pdf/2402.01679" title="Download PDF">pdf</a>, <a href="/format/2402.01679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StickerConv: Generating Multimodal Empathetic Responses from Scratch
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yiqun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+F">Fanheng Kong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peidong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Shuang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lingshuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shi Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Daling Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+K">Kaisong Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> On going work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Stickers, while widely recognized for enhancing empathetic communication in
online interactions, remain underexplored in current empathetic dialogue
research. In this paper, we introduce the Agent for StickerConv (Agent4SC),
which uses collaborative agent interactions to realistically simulate human
behavior with sticker usage, thereby enhancing multimodal empathetic
communication. Building on this foundation, we develop a multimodal empathetic
dialogue dataset, StickerConv, which includes 12.9K dialogue sessions, 5.8K
unique stickers, and 2K diverse conversational scenarios, specifically designs
to augment the generation of empathetic responses in a multimodal context. To
leverage the richness of this dataset, we propose PErceive and Generate
Stickers (PEGS), a multimodal empathetic response generation model,
complemented by a comprehensive set of empathy evaluation metrics based on LLM.
Our experiments demonstrate PEGS's effectiveness in generating contextually
relevant and emotionally resonant multimodal empathetic responses, contributing
to the advancement of more nuanced and engaging empathetic dialogue systems.
Our project page is available at https://neu-datamining.github.io/StickerConv .
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01680" title="Abstract">arXiv:2402.01680</a> [<a href="/pdf/2402.01680" title="Download PDF">pdf</a>, <a href="/format/2402.01680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model based Multi-Agents: A Survey of Progress and  Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+T">Taicheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiuying Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+R">Ruidi Chang</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+S">Shichao Pei</a>, 
<a href="/search/cs?searchtype=author&query=Chawla%2C+N+V">Nitesh V. Chawla</a>, 
<a href="/search/cs?searchtype=author&query=Wiest%2C+O">Olaf Wiest</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiangliang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work is ongoing and we welcome your contribution!
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Large Language Models (LLMs) have achieved remarkable success across a wide
array of tasks. Due to the impressive planning and reasoning abilities of LLMs,
they have been used as autonomous agents to do many tasks automatically.
Recently, based on the development of using one LLM as a single planning or
decision-making agent, LLM-based multi-agent systems have achieved considerable
progress in complex problem-solving and world simulation. To provide the
community with an overview of this dynamic field, we present this survey to
offer an in-depth discussion on the essential aspects of multi-agent systems
based on LLMs, as well as the challenges. Our goal is for readers to gain
substantial insights on the following questions: What domains and environments
do LLM-based multi-agents simulate? How are these agents profiled and how do
they communicate? What mechanisms contribute to the growth of agents'
capacities? For those interested in delving into this field of study, we also
summarize the commonly used datasets or benchmarks for them to have convenient
access. To keep researchers updated on the latest studies, we maintain an
open-source GitHub repository, dedicated to outlining the research on LLM-based
multi-agent systems.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01681" title="Abstract">arXiv:2402.01681</a> [<a href="/pdf/2402.01681" title="Download PDF">pdf</a>, <a href="/format/2402.01681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social  Media Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuhang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Paiheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Xuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+G">Ge Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ai%2C+W">Wei Ai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 2 page appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Emojis, which encapsulate semantics beyond mere words or phrases, have become
prevalent in social network communications. This has spurred increasing
scholarly interest in exploring their attributes and functionalities. However,
emoji-related research and application face two primary challenges. First,
researchers typically rely on crowd-sourcing to annotate emojis in order to
understand their sentiments, usage intentions, and semantic meanings. Second,
subjective interpretations by users can often lead to misunderstandings of
emojis and cause the communication barrier. Large Language Models (LLMs) have
achieved significant success in various annotation tasks, with ChatGPT
demonstrating expertise across multiple domains. In our study, we assess
ChatGPT's effectiveness in handling previously annotated and downstream tasks.
Our objective is to validate the hypothesis that ChatGPT can serve as a viable
alternative to human annotators in emoji research and that its ability to
explain emoji meanings can enhance clarity and transparency in online
communications. Our findings indicate that ChatGPT has extensive knowledge of
emojis. It is adept at elucidating the meaning of emojis across various
application scenarios and demonstrates the potential to replace human
annotators in a range of tasks.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01682" title="Abstract">arXiv:2402.01682</a> [<a href="/pdf/2402.01682" title="Download PDF">pdf</a>, <a href="/ps/2402.01682" title="Download PostScript">ps</a>, <a href="/format/2402.01682" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Social Media Data to Identify Factors Influencing Public  Attitude Towards Accessibility, Socioeconomic Disparity and Public  Transportation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Momin%2C+K+A">Khondhaker Al Momin</a>, 
<a href="/search/cs?searchtype=author&query=Sadri%2C+A+M">Arif Mohaimin Sadri</a>, 
<a href="/search/cs?searchtype=author&query=Hasnine%2C+M+S">Md Sami Hasnine</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computation and Language (cs.CL); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">This study proposes a novel method to understand the factors affecting
individuals' perception of transport accessibility, socioeconomic disparity,
and public infrastructure. As opposed to the time consuming and expensive
survey-based approach, this method can generate organic large-scale responses
from social media and develop statistical models to understand individuals'
perceptions of various transportation issues. This study retrieved and analyzed
36,098 tweets from New York City from March 19, 2020, to May 15, 2022. A
state-of-the-art natural language processing algorithm is used for text mining
and classification. A data fusion technique has been adopted to generate a
series of socioeconomic traits that are used as explanatory variables in the
model. The model results show that females and individuals of Asian origin tend
to discuss transportation accessibility more than their counterparts, with
those experiencing high neighborhood traffic also being more vocal. However,
disadvantaged individuals, including the unemployed and those living in
low-income neighborhoods or in areas with high natural hazard risks, tend to
communicate less about such issues. As for socioeconomic disparity, individuals
of Asian origin and those experiencing various types of air pollution are more
likely to discuss these topics on Twitter, often with a negative sentiment.
However, unemployed, or disadvantaged individuals, as well as those living in
areas with high natural hazard risks or expected losses, are less inclined to
tweet about this subject. Lack of internet accessibility could be a reason why
many disadvantaged individuals do not tweet about transport accessibility and
subsidized internet could be a possible solution.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01683" title="Abstract">arXiv:2402.01683</a> [<a href="/pdf/2402.01683" title="Download PDF">pdf</a>, <a href="/ps/2402.01683" title="Download PostScript">ps</a>, <a href="/format/2402.01683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Community-based Behavioral Understanding of Crisis Activity Concerns  using Social Media Data: A Study on the 2023 Canadian Wildfires in New York  City
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Momin%2C+K+A">Khondhaker Al Momin</a>, 
<a href="/search/cs?searchtype=author&query=Hasnine%2C+M+S">Md Sami Hasnine</a>, 
<a href="/search/cs?searchtype=author&query=Sadri%2C+A+M">Arif Mohaimin Sadri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computation and Language (cs.CL); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">New York City (NYC) topped the global chart for the worst air pollution in
June 2023, owing to the wildfire smoke drifting in from Canada. This
unprecedented situation caused significant travel disruptions and shifts in
traditional activity patterns of NYC residents. This study utilized large-scale
social media data to study different crisis activity concerns (i.e.,
evacuation, staying indoors, shopping, and recreational activities among
others) in the emergence of the 2023 Canadian wildfire smoke in NYC. In this
regard, one week (June 02 through June 09, 2023) geotagged Twitter data from
NYC were retrieved and used in the analysis. The tweets were processed using
advanced text classification techniques and later integrated with national
databases such as Social Security Administration data, Census, and American
Community Survey. Finally, a model has been developed to make community
inferences of different activity concerns in a major wildfire. The findings
suggest, during wildfires, females are less likely to engage in discussions
about evacuation, trips for medical, social, or recreational purposes, and
commuting for work, likely influenced by workplaces maintaining operations
despite poor air quality. There were also racial disparities in these
discussions, with Asians being more likely than Hispanics to discuss evacuation
and work commute, and African Americans being less likely to discuss social and
recreational activities. Additionally, individuals from low-income
neighborhoods and non-higher education students expressed fewer concerns about
evacuation. This study provides valuable insights for policymakers, emergency
planners, and public health officials, aiding them in formulating targeted
communication strategies and equitable emergency response plans.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01684" title="Abstract">arXiv:2402.01684</a> [<a href="/pdf/2402.01684" title="Download PDF">pdf</a>, <a href="/format/2402.01684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs  Using the CGC-LORA Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chao Song</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Z">Zhihao Ye</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Q">Qiqiang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Q">Qiuying Peng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">With the productive evolution of large language models (LLMs) in the field of
natural language processing (NLP), tons of effort has been made to effectively
fine-tune common pre-trained LLMs to fulfill a variety of tasks in one or
multiple specific domain. In practice, there are two prevailing ways, in which
the adaptation can be achieved: (i) Multiple Independent Models: Pre-trained
LLMs are fine-tuned a few times independently using the corresponding training
samples from each task. (ii) An Integrated Model: Samples from all tasks are
employed to fine-tune a pre-trianed LLM unitedly. To address the high computing
cost and seesawing issue simultaneously, we propose a unified framework that
implements a 1 + N mutli-task fine-tuning pattern in LLMs using a novel
Customized Gate Control (CGC) Low-rank Adaptation (LoRA) algorithm. Our work
aims to take an advantage of both MTL (i.e., CGC) and PEFT (i.e., LoRA) scheme.
For a given cluster of tasks, we design an innovative layer that contains two
types of experts as additional trainable parameters to make LoRA be compatible
with MTL. To comprehensively evaluate the proposed framework, we conduct
well-designed experiments on two public datasets. The experimental results
demonstrate that the unified framework with CGC-LoRA modules achieves higher
evaluation scores than all benchmarks on both two datasets.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01685" title="Abstract">arXiv:2402.01685</a> [<a href="/pdf/2402.01685" title="Download PDF">pdf</a>, <a href="/format/2402.01685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SMUTF: Schema Matching Using Generative Tags and Hybrid Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Di%2C+M">Mei Di</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haozheng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chenwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tsai%2C+R+T">Richard Tzong-Han Tsai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB)

</div>
<p class="mathjax">We introduce SMUTF, a unique approach for large-scale tabular data schema
matching (SM), which assumes that supervised learning does not affect
performance in open-domain tasks, thereby enabling effective cross-domain
matching. This system uniquely combines rule-based feature engineering,
pre-trained language models, and generative large language models. In an
innovative adaptation inspired by the Humanitarian Exchange Language, we deploy
'generative tags' for each data column, enhancing the effectiveness of SM.
SMUTF exhibits extensive versatility, working seamlessly with any pre-existing
pre-trained embeddings, classification methods, and generative models.
<br />Recognizing the lack of extensive, publicly available datasets for SM, we
have created and open-sourced the HDXSM dataset from the public humanitarian
data. We believe this to be the most exhaustive SM dataset currently available.
In evaluations across various public datasets and the novel HDXSM dataset,
SMUTF demonstrated exceptional performance, surpassing existing
state-of-the-art models in terms of accuracy and efficiency, and} improving the
F1 score by 11.84% and the AUC of ROC by 5.08%.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01686" title="Abstract">arXiv:2402.01686</a> [<a href="/pdf/2402.01686" title="Download PDF">pdf</a>, <a href="/ps/2402.01686" title="Download PostScript">ps</a>, <a href="/format/2402.01686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Systematic Mapping Study of Digital Twins for Diagnosis in  Transportation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prikler%2C+L+M">Liliana Marie Prikler</a>, 
<a href="/search/cs?searchtype=author&query=Wotawa%2C+F">Franz Wotawa</a> (Graz University of Technology, Institute for Software Technology)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 10th International Conference on Dependable Systems and Their
  Applications (DSA), Tokyo, Japan, 2023, pp. 431-442
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">In recent years, digital twins have been proposed and implemented in various
fields with potential applications ranging from prototyping to maintenance.
Going forward, they are to enable numerous efficient and sustainable
technologies, among them autonomous cars. However, despite a large body of
research in many fields, academics have yet to agree on what exactly a digital
twin is -- and as a result, what its capabilities and limitations might be. To
further our understanding, we explore the capabilities of digital twins
concerning diagnosis in the field of transportation. We conduct a systematic
mapping study including digital twins of vehicles and their components, as well
as transportation infrastructure. We discovered that few papers on digital
twins describe any diagnostic process. Furthermore, most existing approaches
appear limited to system monitoring or fault detection. These findings suggest
that we need more research for diagnostic reasoning utilizing digital twins.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01687" title="Abstract">arXiv:2402.01687</a> [<a href="/pdf/2402.01687" title="Download PDF">pdf</a>, <a href="/ps/2402.01687" title="Download PostScript">ps</a>, <a href="/format/2402.01687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;Which LLM should I use?&quot;: Evaluating LLMs for tasks performed by  Undergraduate Computer Science Students in India
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+V">Vibhor Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Thureja%2C+N">Nakul Thureja</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+M+K">Madhav Krishan Garg</a>, 
<a href="/search/cs?searchtype=author&query=Dharmavaram%2C+S">Sahiti Dharmavaram</a>, 
<a href="/search/cs?searchtype=author&query=Meghna">Meghna</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+D">Dhruv Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">This study evaluates the effectiveness of various large language models
(LLMs) in performing tasks common among undergraduate computer science
students. Although a number of research studies in the computing education
community have explored the possibility of using LLMs for a variety of tasks,
there is a lack of comprehensive research comparing different LLMs and
evaluating which LLMs are most effective for different tasks. Our research
systematically assesses some of the publicly available LLMs such as Google
Bard, ChatGPT, GitHub Copilot Chat, and Microsoft Copilot across diverse tasks
commonly encountered by undergraduate computer science students. These tasks
include code generation, explanation, project ideation, content generation,
class assignments, and email composition. Evaluation for these tasks was
carried out by junior and senior students in computer science, and provides
insights into the models' strengths and limitations. This study aims to guide
students in selecting suitable LLMs for any specific task and offers valuable
insights on how LLMs can be used constructively by students and instructors.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01688" title="Abstract">arXiv:2402.01688</a> [<a href="/pdf/2402.01688" title="Download PDF">pdf</a>, <a href="/format/2402.01688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Online Hierarchical Energy Management System for Energy Communities,  Complying with the Current Technical Legislation Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Capillo%2C+A">Antonino Capillo</a>, 
<a href="/search/cs?searchtype=author&query=De+Santis%2C+E">Enrico De Santis</a>, 
<a href="/search/cs?searchtype=author&query=Mascioli%2C+F+M+F">Fabio Massimo Frattale Mascioli</a>, 
<a href="/search/cs?searchtype=author&query=Rizzi%2C+A">Antonello Rizzi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Efforts in the fight against Climate Change are increasingly oriented towards
new energy efficiency strategies in Smart Grids (SGs). In 2018, with proper
legislation, the European Union (EU) defined the Renewable Energy Community
(REC) as a local electrical grid whose participants share their self-produced
renewable energy, aiming at reducing bill costs by taking advantage of proper
incentives. That action aspires to accelerate the spread of local renewable
energy exploitation, whose costs could not be within everyone's reach. Since a
REC is technically an SG, the strategies above can be applied, and
specifically, practical Energy Management Systems (EMSs) are required.
Therefore, in this work, an online Hierarchical EMS (HEMS) is synthesized for
REC cost minimization to evaluate its superiority over a local self-consumption
approach. EU technical indications (as inherited from Italy) are diligently
followed, aiming for results that are as realistic as possible. Power flows
between REC nodes, or Microgrids (MGs) are optimized by taking Energy Storage
Systems (ESSs) and PV plant costs, energy purchase costs, and REC incentives. A
hybrid Fuzzy Inference System - Genetic Algorithm (FIS-GA) model is implemented
with the GA encoding the FIS parameters. Power generation and consumption,
which are the overall system input, are predicted by a LSTM trained on
historical data. The proposed hierarchical model achieves good precision in
short computation times and outperforms the self-consumption approach, leading
to about 20% savings compared to the latter. In addition, the Explainable AI
(XAI), which characterizes the model through the FIS, makes results more
reliable thanks to an excellent human interpretation level. To finish, the HEMS
is parametrized so that it is straightforward to switch to another Country's
technical legislation framework.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01689" title="Abstract">arXiv:2402.01689</a> [<a href="/pdf/2402.01689" title="Download PDF">pdf</a>, <a href="/ps/2402.01689" title="Download PostScript">ps</a>, <a href="/format/2402.01689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predictive Health Analysis in Industry 5.0: A Scientometric and  Systematic Review of Motion Capture in Construction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+H">Md Hadisur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+M+R">Md Rabiul Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+N+I">Nahian Ismail Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Syed%2C+M+A+B">Md Asif Bin Syed</a>, 
<a href="/search/cs?searchtype=author&query=Farah%2C+M+U">Mst Ummul Farah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">In an era of rapid technological advancement, the rise of Industry 4.0 has
prompted industries to pursue innovative improvements in their processes. As we
advance towards Industry 5.0, which focuses more on collaboration between
humans and intelligent systems, there is a growing requirement for better
sensing technologies for healthcare and safety purposes. Consequently, Motion
Capture (MoCap) systems have emerged as critical enablers in this technological
evolution by providing unmatched precision and versatility in various
workplaces, including construction. As the construction workplace requires
physically demanding tasks, leading to work-related musculoskeletal disorders
(WMSDs) and health issues, the study explores the increasing relevance of MoCap
systems within the concept of Industry 4.0 and 5.0. Despite the growing
significance, there needs to be more comprehensive research, a scientometric
review that quantitatively assesses the role of MoCap systems in construction.
Our study combines bibliometric, scientometric, and systematic review
approaches to address this gap, analyzing articles sourced from the Scopus
database. A total of 52 papers were carefully selected from a pool of 962
papers for a quantitative study using a scientometric approach and a
qualitative, indepth examination. Results showed that MoCap systems are
employed to improve worker health and safety and reduce occupational
hazards.The in-depth study also finds the most tested construction tasks are
masonry, lifting, training, and climbing, with a clear preference for
markerless systems.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01690" title="Abstract">arXiv:2402.01690</a> [<a href="/pdf/2402.01690" title="Download PDF">pdf</a>, <a href="/format/2402.01690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linguistic-Based Mild Cognitive Impairment Detection Using Informative  Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fard%2C+A+P">Ali Pourramezan Fard</a>, 
<a href="/search/cs?searchtype=author&query=Mahoor%2C+M+H">Mohammad H. Mahoor</a>, 
<a href="/search/cs?searchtype=author&query=Alsuhaibani%2C+M">Muath Alsuhaibani</a>, 
<a href="/search/cs?searchtype=author&query=Dodgec%2C+H+H">Hiroko H. Dodgec</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper presents a deep learning method using Natural Language Processing
(NLP) techniques, to distinguish between Mild Cognitive Impairment (MCI) and
Normal Cognitive (NC) conditions in older adults. We propose a framework that
analyzes transcripts generated from video interviews collected within the
I-CONECT study project, a randomized controlled trial aimed at improving
cognitive functions through video chats. Our proposed NLP framework consists of
two Transformer-based modules, namely Sentence Embedding (SE) and Sentence
Cross Attention (SCA). First, the SE module captures contextual relationships
between words within each sentence. Subsequently, the SCA module extracts
temporal features from a sequence of sentences. This feature is then used by a
Multi-Layer Perceptron (MLP) for the classification of subjects into MCI or NC.
To build a robust model, we propose a novel loss function, called InfoLoss,
that considers the reduction in entropy by observing each sequence of sentences
to ultimately enhance the classification accuracy. The results of our
comprehensive model evaluation using the I-CONECT dataset show that our
framework can distinguish between MCI and NC with an average area under the
curve of 84.75%.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01691" title="Abstract">arXiv:2402.01691</a> [<a href="/pdf/2402.01691" title="Download PDF">pdf</a>, <a href="/ps/2402.01691" title="Download PostScript">ps</a>, <a href="/format/2402.01691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Algorithm Review Boards for Organizational Responsible  Artificial Intelligence Governance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hadley%2C+E">Emily Hadley</a>, 
<a href="/search/cs?searchtype=author&query=Blatecky%2C+A">Alan Blatecky</a>, 
<a href="/search/cs?searchtype=author&query=Comfort%2C+M">Megan Comfort</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Organizations including companies, nonprofits, governments, and academic
institutions are increasingly developing, deploying, and utilizing artificial
intelligence (AI) tools. Responsible AI (RAI) governance approaches at
organizations have emerged as important mechanisms to address potential AI
risks and harms. In this work, we interviewed 17 technical contributors across
organization types (Academic, Government, Industry, Nonprofit) and sectors
(Finance, Health, Tech, Other) about their experiences with internal RAI
governance. Our findings illuminated the variety of organizational definitions
of RAI and accompanying internal governance approaches. We summarized the first
detailed findings on algorithm review boards (ARBs) and similar review
committees in practice, including their membership, scope, and measures of
success. We confirmed known robust model governance in finance sectors and
revealed extensive algorithm and AI governance with ARB-like review boards in
health sectors. Our findings contradict the idea that Institutional Review
Boards alone are sufficient for algorithm governance and posit that ARBs are
among the more impactful internal RAI governance approaches. Our results
suggest that integration with existing internal regulatory approaches and
leadership buy-in are among the most important attributes for success and that
financial tensions are the greatest challenge to effective organizational RAI.
We make a variety of suggestions for how organizational partners can learn from
these findings when building their own internal RAI frameworks. We outline
future directions for developing and measuring effectiveness of ARBs and other
internal RAI governance approaches.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01692" title="Abstract">arXiv:2402.01692</a> [<a href="/pdf/2402.01692" title="Download PDF">pdf</a>, <a href="/format/2402.01692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maximizing Data Efficiency for Cross-Lingual TTS Adaptation by  Self-Supervised Representation Mixing and Embedding Initialization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wei-Ping Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Sung-Feng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ASRU 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper presents an effective transfer learning framework for language
adaptation in text-to-speech systems, with a focus on achieving language
adaptation using minimal labeled and unlabeled data. While many works focus on
reducing the usage of labeled data, very few consider minimizing the usage of
unlabeled data. By utilizing self-supervised features in the pretraining stage,
replacing the noisy portion of pseudo labels with these features during
fine-tuning, and incorporating an embedding initialization trick, our method
leverages more information from unlabeled data compared to conventional
approaches. Experimental results show that our framework is able to synthesize
intelligible speech in unseen languages with only 4 utterances of labeled data
and 15 minutes of unlabeled data. Our methodology continues to surpass
conventional techniques, even when a greater volume of data is accessible.
These findings highlight the potential of our data-efficient language
adaptation framework.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01693" title="Abstract">arXiv:2402.01693</a> [<a href="/pdf/2402.01693" title="Download PDF">pdf</a>, <a href="/ps/2402.01693" title="Download PostScript">ps</a>, <a href="/format/2402.01693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quality of Answers of Generative Large Language Models vs Peer Patients  for Interpreting Lab Test Results for Lay Patients: Evaluation Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhe He</a>, 
<a href="/search/cs?searchtype=author&query=Bhasuran%2C+B">Balu Bhasuran</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Q">Qiao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+S">Shubo Tian</a>, 
<a href="/search/cs?searchtype=author&query=Hanna%2C+K">Karim Hanna</a>, 
<a href="/search/cs?searchtype=author&query=Shavor%2C+C">Cindy Shavor</a>, 
<a href="/search/cs?searchtype=author&query=Arguello%2C+L+G">Lisbeth Garcia Arguello</a>, 
<a href="/search/cs?searchtype=author&query=Murray%2C+P">Patrick Murray</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhiyong Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Lab results are often confusing and hard to understand. Large language models
(LLMs) such as ChatGPT have opened a promising avenue for patients to get their
questions answered. We aim to assess the feasibility of using LLMs to generate
relevant, accurate, helpful, and unharmful responses to lab test-related
questions asked by patients and to identify potential issues that can be
mitigated with augmentation approaches. We first collected lab test results
related question and answer data from Yahoo! Answers and selected 53 QA pairs
for this study. Using the LangChain framework and ChatGPT web portal, we
generated responses to the 53 questions from four LLMs including GPT-4, Meta
LLaMA 2, MedAlpaca, and ORCA_mini. We first assessed the similarity of their
answers using standard QA similarity-based evaluation metrics including ROUGE,
BLEU, METEOR, BERTScore. We also utilized an LLM-based evaluator to judge
whether a target model has higher quality in terms of relevance, correctness,
helpfulness, and safety than the baseline model. Finally, we performed a manual
evaluation with medical experts for all the responses to seven selected
questions on the same four aspects. The results of Win Rate and medical expert
evaluation both showed that GPT-4's responses achieved better scores than all
the other LLM responses and human responses on all four aspects (relevance,
correctness, helpfulness, and safety). However, LLM responses occasionally also
suffer from a lack of interpretation in one's medical context, incorrect
statements, and lack of references. We find that compared to other three LLMs
and human answer from the Q&amp;A website, GPT-4's responses are more accurate,
helpful, relevant, and safer. However, there are cases which GPT-4 responses
are inaccurate and not individualized. We identified a number of ways to
improve the quality of LLM responses.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01694" title="Abstract">arXiv:2402.01694</a> [<a href="/pdf/2402.01694" title="Download PDF">pdf</a>, <a href="/format/2402.01694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ARGS: Alignment as Reward-Guided Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khanov%2C+M">Maxim Khanov</a>, 
<a href="/search/cs?searchtype=author&query=Burapacheep%2C+J">Jirayu Burapacheep</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yixuan Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Aligning large language models with human objectives is paramount, yet common
approaches including RLHF suffer from unstable and resource-intensive training.
In response to this challenge, we introduce ARGS, Alignment as Reward-Guided
Search, a novel framework that integrates alignment into the decoding process,
eliminating the need for expensive RL training. By adjusting the model's
probabilistic predictions using a reward signal, ARGS generates texts with
semantic diversity while being aligned with human preferences, offering a
promising and flexible solution for aligning language models. Notably, ARGS
demonstrates consistent enhancements in average reward compared to baselines
across diverse alignment tasks and various model dimensions. For example, under
the same greedy-based decoding strategy, our method improves the average reward
by 19.56% relative to the baseline and secures a preference or tie score of
64.33% in GPT-4 evaluation. We believe that our framework, emphasizing
decoding-time alignment, paves the way for more responsive language models in
the future. Code is publicly available at:
\url{https://github.com/deeplearning-wisc/args}.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01695" title="Abstract">arXiv:2402.01695</a> [<a href="/pdf/2402.01695" title="Download PDF">pdf</a>, <a href="/format/2402.01695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language-Guided World Models: A Model-Based Approach to AI Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Alex Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+K">Khanh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Tuyls%2C+J">Jens Tuyls</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+A">Albert Lin</a>, 
<a href="/search/cs?searchtype=author&query=Narasimhan%2C+K">Karthik Narasimhan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Installing probabilistic world models into artificial agents opens an
efficient channel for humans to communicate with and control these agents. In
addition to updating agent policies, humans can modify their internal world
models in order to influence their decisions. The challenge, however, is that
currently existing world models are difficult for humans to adapt because they
lack a natural communication interface. Aimed at addressing this shortcoming,
we develop Language-Guided World Models (LWMs), which can capture environment
dynamics by reading language descriptions. These models enhance agent
communication efficiency, allowing humans to simultaneously alter their
behavior on multiple tasks with concise language feedback. They also enable
agents to self-learn from texts originally written to instruct humans. To
facilitate the development of LWMs, we design a challenging benchmark based on
the game of MESSENGER (Hanjie et al., 2021), requiring compositional
generalization to new language descriptions and environment dynamics. Our
experiments reveal that the current state-of-the-art Transformer architecture
performs poorly on this benchmark, motivating us to design a more robust
architecture. To showcase the practicality of our proposed LWMs, we simulate a
scenario where these models augment the interpretability and safety of an agent
by enabling it to generate and discuss plans with a human before execution. By
effectively incorporating language feedback on the plan, the models boost the
agent performance in the real environment by up to three times without
collecting any interactive experiences in this environment.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01696" title="Abstract">arXiv:2402.01696</a> [<a href="/pdf/2402.01696" title="Download PDF">pdf</a>, <a href="/format/2402.01696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HiGen: Hierarchy-Aware Sequence Generation for Hierarchical Text  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jain%2C+V">Vidit Jain</a>, 
<a href="/search/cs?searchtype=author&query=Rungta%2C+M">Mukund Rungta</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Y">Yuchen Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zeyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Mu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Skolnick%2C+J">Jeffrey Skolnick</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Hierarchical text classification (HTC) is a complex subtask under multi-label
text classification, characterized by a hierarchical label taxonomy and data
imbalance. The best-performing models aim to learn a static representation by
combining document and hierarchical label information. However, the relevance
of document sections can vary based on the hierarchy level, necessitating a
dynamic document representation. To address this, we propose HiGen, a
text-generation-based framework utilizing language models to encode dynamic
text representations. We introduce a level-guided loss function to capture the
relationship between text and label name semantics. Our approach incorporates a
task-specific pretraining strategy, adapting the language model to in-domain
knowledge and significantly enhancing performance for classes with limited
examples. Furthermore, we present a new and valuable dataset called ENZYME,
designed for HTC, which comprises articles from PubMed with the goal of
predicting Enzyme Commission (EC) numbers. Through extensive experiments on the
ENZYME dataset and the widely recognized WOS and NYT datasets, our methodology
demonstrates superior performance, surpassing existing approaches while
efficiently handling data and mitigating class imbalance. The data and code
will be released publicly.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01697" title="Abstract">arXiv:2402.01697</a> [<a href="/pdf/2402.01697" title="Download PDF">pdf</a>, <a href="/format/2402.01697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> APT-Pipe: An Automatic Prompt-Tuning Tool for Social Computing Data  Annotation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yiming Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zhizhuo Yin</a>, 
<a href="/search/cs?searchtype=author&query=Haq%2C+E">Ehsan-Ul Haq</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+L">Lik-Hang Lee</a>, 
<a href="/search/cs?searchtype=author&query=Tyson%2C+G">Gareth Tyson</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+P">Pan Hui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent research has highlighted the potential of LLM applications, like
ChatGPT, for performing label annotation on social computing text. However, it
is already well known that performance hinges on the quality of the input
prompts. To address this, there has been a flurry of research into prompt
tuning -- techniques and guidelines that attempt to improve the quality of
prompts. Yet these largely rely on manual effort and prior knowledge of the
dataset being annotated. To address this limitation, we propose APT-Pipe, an
automated prompt-tuning pipeline. APT-Pipe aims to automatically tune prompts
to enhance ChatGPT's text classification performance on any given dataset. We
implement APT-Pipe and test it across twelve distinct text classification
datasets. We find that prompts tuned by APT-Pipe help ChatGPT achieve higher
weighted F1-score on nine out of twelve experimented datasets, with an
improvement of 7.01% on average. We further highlight APT-Pipe's flexibility as
a framework by showing how it can be extended to support additional tuning
mechanisms.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01698" title="Abstract">arXiv:2402.01698</a> [<a href="/pdf/2402.01698" title="Download PDF">pdf</a>, <a href="/format/2402.01698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large language model empowered participatory urban planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhilun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yuming Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 7 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Participatory urban planning is the mainstream of modern urban planning and
involves the active engagement of different stakeholders. However, the
traditional participatory paradigm encounters challenges in time and manpower,
while the generative planning tools fail to provide adjustable and inclusive
solutions. This research introduces an innovative urban planning approach
integrating Large Language Models (LLMs) within the participatory process. The
framework, based on the crafted LLM agent, consists of role-play, collaborative
generation, and feedback iteration, solving a community-level land-use task
catering to 1000 distinct interests. Empirical experiments in diverse urban
communities exhibit LLM's adaptability and effectiveness across varied planning
scenarios. The results were evaluated on four metrics, surpassing human experts
in satisfaction and inclusion, and rivaling state-of-the-art reinforcement
learning methods in service and ecology. Further analysis shows the advantage
of LLM agents in providing adjustable and inclusive solutions with natural
language reasoning and strong scalability. While implementing the recent
advancements in emulating human behavior for planning, this work envisions both
planners and citizens benefiting from low-cost, efficient LLM agents, which is
crucial for enhancing participation and realizing participatory urban planning.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01700" title="Abstract">arXiv:2402.01700</a> [<a href="/pdf/2402.01700" title="Download PDF">pdf</a>, <a href="/ps/2402.01700" title="Download PostScript">ps</a>, <a href="/format/2402.01700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Question answering systems for health professionals at the point of care  -- a systematic review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kell%2C+G">Gregory Kell</a>, 
<a href="/search/cs?searchtype=author&query=Roberts%2C+A">Angus Roberts</a>, 
<a href="/search/cs?searchtype=author&query=Umansky%2C+S">Serge Umansky</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+L">Linglong Qian</a>, 
<a href="/search/cs?searchtype=author&query=Ferrari%2C+D">Davide Ferrari</a>, 
<a href="/search/cs?searchtype=author&query=Soboczenski%2C+F">Frank Soboczenski</a>, 
<a href="/search/cs?searchtype=author&query=Wallace%2C+B">Byron Wallace</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+N">Nikhil Patel</a>, 
<a href="/search/cs?searchtype=author&query=Marshall%2C+I+J">Iain J Marshall</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Journal of the American Medical Informatics Association (JAMIA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Objective: Question answering (QA) systems have the potential to improve the
quality of clinical care by providing health professionals with the latest and
most relevant evidence. However, QA systems have not been widely adopted. This
systematic review aims to characterize current medical QA systems, assess their
suitability for healthcare, and identify areas of improvement.
<br />Materials and methods: We searched PubMed, IEEE Xplore, ACM Digital Library,
ACL Anthology and forward and backward citations on 7th February 2023. We
included peer-reviewed journal and conference papers describing the design and
evaluation of biomedical QA systems. Two reviewers screened titles, abstracts,
and full-text articles. We conducted a narrative synthesis and risk of bias
assessment for each study. We assessed the utility of biomedical QA systems.
<br />Results: We included 79 studies and identified themes, including question
realism, answer reliability, answer utility, clinical specialism, systems,
usability, and evaluation methods. Clinicians' questions used to train and
evaluate QA systems were restricted to certain sources, types and complexity
levels. No system communicated confidence levels in the answers or sources.
Many studies suffered from high risks of bias and applicability concerns. Only
8 studies completely satisfied any criterion for clinical utility, and only 7
reported user evaluations. Most systems were built with limited input from
clinicians.
<br />Discussion: While machine learning methods have led to increased accuracy,
most studies imperfectly reflected real-world healthcare information needs. Key
research priorities include developing more realistic healthcare QA datasets
and considering the reliability of answer sources, rather than merely focusing
on accuracy.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01701" title="Abstract">arXiv:2402.01701</a> [<a href="/pdf/2402.01701" title="Download PDF">pdf</a>, <a href="/format/2402.01701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nie pozw&#xf3;l algorytmom rz&#x105;dzi&#x107; Twoim koszykiem: systemy  rekomendacyjne w dobie Omnibusa
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morzy%2C+M">Miko&#x142;aj Morzy</a>, 
<a href="/search/cs?searchtype=author&query=Sobieraj%2C+M">Miros&#x142;aw Sobieraj</a>, 
<a href="/search/cs?searchtype=author&query=Sikora%2C+S">Sebastian Sikora</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, in Polish
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">The Omnibus Directive is an essential part of the European Union's New Deal
for Consumers. The Directive introduces new regulations in trade, including
e-commerce, with the main goal being to increase transparency, fairness and
consumer protection. The authors critically draw attention to a significant
oversight in the Omnibus Directive, namely the lack of consideration of
recommendation systems. Recommendation engines can be a source of potentially
harmful practices affecting consumers, hence the need for a directive
extension. The proposals presented in this article include the introduction of
ethical supervision over recommendation systems to minimize the risk of
negative effects of their recommendations, as well as a clear explanation of
the criteria on which recommendations are made -- similar to search result
rankings.
<br />--
<br />Dyrektywa Omnibus stanowi istotn\k{a} cz\k{e}\'s\'c Nowego {\L}adu dla
Konsument\'ow (ang. \emph{New Deal for Consumers}) Unii Europejskiej. Dyrektywa
wprowadza nowe regulacje w handlu, w tym e-commerce, kt\'orych g{\l}\'ownym
celem jest zwi\k{e}kszenie przejrzysto\'sci, uczciwo\'sci i ochrony
konsument\'ow. Autorzy krytycznie zwracaj\k{a} uwag\k{e} na istotne zaniedbanie
w dyrektywie Omnibus, jakim jest brak uwzgl\k{e}dnienia system\'ow
rekomendacyjnych. Silniki rekomendacyjne mog\k{a} by\'c \'zr\'od{\l}em
potencjalnie szkodliwych praktyk uderzaj\k{a}cych w konsument\'ow, st\k{a}d
niezb\k{e}dne jest rozszerzenie dyrektywy. Propozycje przedstawione w
niniejszym artykule obejmuj\k{a} wprowadzenie etycznego nadzoru nad systemami
rekomenduj\k{a}cymi, aby zminimalizowa\'c ryzyko negatywnych skutk\'ow ich
rekomendacji, a tak\.ze jasne wyja\'snienie kryteri\'ow, na podstawie kt\'orych
dokonywane s\k{a} rekomendacje -- analogicznie do ranking\'ow wynik\'ow
wyszukiwania.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01702" title="Abstract">arXiv:2402.01702</a> [<a href="/pdf/2402.01702" title="Download PDF">pdf</a>, <a href="/format/2402.01702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fluent dreaming for language models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thompson%2C+T+B">T. Ben Thompson</a> (1), 
<a href="/search/cs?searchtype=author&query=Straznickas%2C+Z">Zygimantas Straznickas</a> (1), 
<a href="/search/cs?searchtype=author&query=Sklar%2C+M">Michael Sklar</a> (1) ((1) Confirm Labs)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 6 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Feature visualization, also known as "dreaming", offers insights into vision
models by optimizing the inputs to maximize a neuron's activation or other
internal component. However, dreaming has not been successfully applied to
language models because the input space is discrete. We extend Greedy
Coordinate Gradient, a method from the language model adversarial attack
literature, to design the Evolutionary Prompt Optimization (EPO) algorithm. EPO
optimizes the input prompt to simultaneously maximize the Pareto frontier
between a chosen internal feature and prompt fluency, enabling fluent dreaming
for language models. We demonstrate dreaming with neurons, output logits and
arbitrary directions in activation space. We measure the fluency of the
resulting prompts and compare language model dreaming with max-activating
dataset examples. Critically, fluent dreaming allows automatically exploring
the behavior of model internals in reaction to mildly out-of-distribution
prompts. Code for running EPO is available at
https://github.com/Confirm-Solutions/dreamy. A companion page demonstrating
code usage is at https://confirmlabs.org/posts/dreamy.html
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01703" title="Abstract">arXiv:2402.01703</a> [<a href="/pdf/2402.01703" title="Download PDF">pdf</a>, <a href="/ps/2402.01703" title="Download PostScript">ps</a>, <a href="/format/2402.01703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver  Interaction in Los Angeles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grahama%2C+B+A+T">Benjamin A.T. Grahama</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+L">Lauren Brown</a>, 
<a href="/search/cs?searchtype=author&query=Chochlakis%2C+G">Georgios Chochlakis</a>, 
<a href="/search/cs?searchtype=author&query=Dehghani%2C+M">Morteza Dehghani</a>, 
<a href="/search/cs?searchtype=author&query=Delerme%2C+R">Raquel Delerme</a>, 
<a href="/search/cs?searchtype=author&query=Friedman%2C+B">Brittany Friedman</a>, 
<a href="/search/cs?searchtype=author&query=Graeden%2C+E">Ellie Graeden</a>, 
<a href="/search/cs?searchtype=author&query=Golazizian%2C+P">Preni Golazizian</a>, 
<a href="/search/cs?searchtype=author&query=Hebbar%2C+R">Rajat Hebbar</a>, 
<a href="/search/cs?searchtype=author&query=Hejabi%2C+P">Parsa Hejabi</a>, 
<a href="/search/cs?searchtype=author&query=Kommeneni%2C+A">Aditya Kommeneni</a>, 
<a href="/search/cs?searchtype=author&query=Salinas%2C+M">Mayag&#xfc;ez Salinas</a>, 
<a href="/search/cs?searchtype=author&query=Sierra-Ar%C3%A9valo%2C+M">Michael Sierra-Ar&#xe9;valo</a>, 
<a href="/search/cs?searchtype=author&query=Trager%2C+J">Jackson Trager</a>, 
<a href="/search/cs?searchtype=author&query=Weller%2C+N">Nicholas Weller</a>, 
<a href="/search/cs?searchtype=author&query=Narayan%2C+S">Shrikanth Narayan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Interactions between the government officials and civilians affect public
wellbeing and the state legitimacy that is necessary for the functioning of
democratic society. Police officers, the most visible and contacted agents of
the state, interact with the public more than 20 million times a year during
traffic stops. Today, these interactions are regularly recorded by body-worn
cameras (BWCs), which are lauded as a means to enhance police accountability
and improve police-public interactions. However, the timely analysis of these
recordings is hampered by a lack of reliable automated tools that can enable
the analysis of these complex and contested police-public interactions. This
article proposes an approach to developing new multi-perspective, multimodal
machine learning (ML) tools to analyze the audio, video, and transcript
information from this BWC footage. Our approach begins by identifying the
aspects of communication most salient to different stakeholders, including both
community members and police officers. We move away from modeling approaches
built around the existence of a single ground truth and instead utilize new
advances in soft labeling to incorporate variation in how different observers
perceive the same interactions. We argue that this inclusive approach to the
conceptualization and design of new ML tools is broadly applicable to the study
of communication and development of analytic tools across domains of human
interaction, including education, medicine, and the workplace.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01704" title="Abstract">arXiv:2402.01704</a> [<a href="/pdf/2402.01704" title="Download PDF">pdf</a>, <a href="/format/2402.01704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> States as Strings as Strategies: Steering Language Models with  Game-Theoretic Solvers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gemp%2C+I">Ian Gemp</a>, 
<a href="/search/cs?searchtype=author&query=Bachrach%2C+Y">Yoram Bachrach</a>, 
<a href="/search/cs?searchtype=author&query=Lanctot%2C+M">Marc Lanctot</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+R">Roma Patel</a>, 
<a href="/search/cs?searchtype=author&query=Dasagi%2C+V">Vibhavari Dasagi</a>, 
<a href="/search/cs?searchtype=author&query=Marris%2C+L">Luke Marris</a>, 
<a href="/search/cs?searchtype=author&query=Piliouras%2C+G">Georgios Piliouras</a>, 
<a href="/search/cs?searchtype=author&query=Tuyls%2C+K">Karl Tuyls</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 8 figures, code available @ <a href="https://github.com/google-deepmind/open_spiel/blob/master/open_spiel/python/games/chat_game.py">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Game theory is the study of mathematical models of strategic interactions
among rational agents. Language is a key medium of interaction for humans,
though it has historically proven difficult to model dialogue and its strategic
motivations mathematically. A suitable model of the players, strategies, and
payoffs associated with linguistic interactions (i.e., a binding to the
conventional symbolic logic of game theory) would enable existing
game-theoretic algorithms to provide strategic solutions in the space of
language. In other words, a binding could provide a route to computing stable,
rational conversational strategies in dialogue. Large language models (LLMs)
have arguably reached a point where their generative capabilities can enable
realistic, human-like simulations of natural dialogue. By prompting them in
various ways, we can steer their responses towards different output utterances.
Leveraging the expressivity of natural language, LLMs can also help us quickly
generate new dialogue scenarios, which are grounded in real world applications.
In this work, we present one possible binding from dialogue to game theory as
well as generalizations of existing equilibrium finding algorithms to this
setting. In addition, by exploiting LLMs generation capabilities along with our
proposed binding, we can synthesize a large repository of formally-defined
games in which one can study and test game-theoretic solution concepts. We also
demonstrate how one can combine LLM-driven game generation, game-theoretic
solvers, and imitation learning to construct a process for improving the
strategic capabilities of LLMs.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01705" title="Abstract">arXiv:2402.01705</a> [<a href="/pdf/2402.01705" title="Download PDF">pdf</a>, <a href="/ps/2402.01705" title="Download PostScript">ps</a>, <a href="/format/2402.01705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Behaviorist Representational Harms: A Plan for Measurement and  Mitigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chien%2C+J">Jennifer Chien</a>, 
<a href="/search/cs?searchtype=author&query=Danks%2C+D">David Danks</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Algorithmic harms are commonly categorized as either allocative or
representational. This study specifically addresses the latter, focusing on an
examination of current definitions of representational harms to discern what is
included and what is not. This analysis motivates our expansion beyond
behavioral definitions to encompass harms to cognitive and affective states.
The paper outlines high-level requirements for measurement: identifying the
necessary expertise to implement this approach and illustrating it through a
case study. Our work highlights the unique vulnerabilities of large language
models to perpetrating representational harms, particularly when these harms go
unmeasured and unmitigated. The work concludes by presenting proposed
mitigations and delineating when to employ them. The overarching aim of this
research is to establish a framework for broadening the definition of
representational harms and to translate insights from fairness research into
practical measurement and mitigation praxis.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01706" title="Abstract">arXiv:2402.01706</a> [<a href="/pdf/2402.01706" title="Download PDF">pdf</a>, <a href="/format/2402.01706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MULTIVERSE: Exposing Large Language Model Alignment Problems in Diverse  Worlds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xiaolong Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiangyu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Model (LLM) alignment aims to ensure that LLM outputs match
with human values. Researchers have demonstrated the severity of alignment
problems with a large spectrum of jailbreak techniques that can induce LLMs to
produce malicious content during conversations. Finding the corresponding
jailbreaking prompts usually requires substantial human intelligence or
computation resources. In this paper, we report that LLMs have different levels
of alignment in various contexts. As such, by systematically constructing many
contexts, called worlds, leveraging a Domain Specific Language describing
possible worlds (e.g., time, location, characters, actions and languages) and
the corresponding compiler, we can cost-effectively expose latent alignment
issues. Given the low cost of our method, we are able to conduct a large scale
study regarding LLM alignment issues in different worlds. Our results show that
our method outperforms the-state-of-the-art jailbreaking techniques on both
effectiveness and efficiency. In addition, our results indicate that existing
LLMs are extremely vulnerable to nesting worlds and programming language
worlds. They imply that existing alignment training focuses on the real-world
and is lacking in various (virtual) worlds where LLMs can be exploited.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01707" title="Abstract">arXiv:2402.01707</a> [<a href="/pdf/2402.01707" title="Download PDF">pdf</a>, <a href="/format/2402.01707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revitalizing Sex Education for Chinese Children: A Formative Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K+Z">Kyrie Zhixuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yilin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+J">Jingwen Shan</a>, 
<a href="/search/cs?searchtype=author&query=Sanfilippo%2C+M+R">Madelyn Rose Sanfilippo</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H+R">Hee Rin Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Sex education helps children obtain knowledge and awareness of sexuality, and
protects them against sexually transmitted diseases, pregnancy, and sexual
abuse. Sex education is not well taught to children in China -- both
school-based education and parental communication on this topic are limited. To
interrogate the status quo of sex education in China and explore suitable
interventions, we conducted a series of formative studies including interviews
and social media analysis. Multiple stakeholders such as children, parents,
education practitioners, and the general public were engaged for an in-depth
understanding of their unique needs regarding teaching and learning sex
education. We found that school-based sex education for Chinese children was
currently insufficient and restrictive. Involving parents in sex education
posed several challenges, such as a lack of sexuality and pedagogy knowledge,
and embarrassment in initiating sex education conversations. Culture and
politics were major hurdles to effective sex education. Based on the findings,
we reflect on the complex interactions between culture, politics, education
policy, and pedagogy, and discuss situated design of sex education in broader
cultural and social contexts.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01708" title="Abstract">arXiv:2402.01708</a> [<a href="/pdf/2402.01708" title="Download PDF">pdf</a>, <a href="/format/2402.01708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech  Generators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hutiri%2C+W">Wiebke Hutiri</a>, 
<a href="/search/cs?searchtype=author&query=Papakyriakopoulos%2C+O">Oresiti Papakyriakopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+A">Alice Xiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 4 tables, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The rapid and wide-scale adoption of AI to generate human speech poses a
range of significant ethical and safety risks to society that need to be
addressed. For example, a growing number of speech generation incidents are
associated with swatting attacks in the United States, where anonymous
perpetrators create synthetic voices that call police officers to close down
schools and hospitals, or to violently gain access to innocent citizens' homes.
Incidents like this demonstrate that multimodal generative AI risks and harms
do not exist in isolation, but arise from the interactions of multiple
stakeholders and technical AI systems. In this paper we analyse speech
generation incidents to study how patterns of specific harms arise. We find
that specific harms can be categorised according to the exposure of affected
individuals, that is to say whether they are a subject of, interact with,
suffer due to, or are excluded from speech generation systems. Similarly,
specific harms are also a consequence of the motives of the creators and
deployers of the systems. Based on these insights we propose a conceptual
framework for modelling pathways to ethical and safety harms of AI, which we
use to develop a taxonomy of harms of speech generators. Our relational
approach captures the complexity of risks and harms in sociotechnical AI
systems, and yields an extensible taxonomy that can support appropriate policy
interventions and decision making for responsible multimodal model development
and release of speech generators.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01709" title="Abstract">arXiv:2402.01709</a> [<a href="/pdf/2402.01709" title="Download PDF">pdf</a>, <a href="/format/2402.01709" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Finding and Supporting Struggling Students in a Programming  Course with an Early Warning System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schantong%2C+B">Belinda Schantong</a>, 
<a href="/search/cs?searchtype=author&query=Gorgosch%2C+D">Dominik Gorgosch</a>, 
<a href="/search/cs?searchtype=author&query=Siegmund%2C+J">Janet Siegmund</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Background: Programming skills are advantageous to navigate today's society,
so it is important to teach them to students. However, failure rates for
programming courses are high, and especially students who fall behind early in
introductory programming courses tend to stay behind. Objective: To catch these
students as early as possible, we aim to develop an early warning system, so we
can offer the students support, for example, in the form of syntax
drill-and-practice exercises. Method: To develop the early warning system, we
assess different cognitive skills of students of an introductory programming
course. On several points in time over the course, students complete tests that
measure their ability to develop a mental model of programming, language
skills, attention, and fluid intelligence. Then, we evaluated to what extent
these skills predict whether students acquire programming skills. Additionally,
we assess how syntax drill-and-practice exercises improve how students acquire
programming skill. Findings: Most of the cognitive skills can predict whether
students acquire programming skills to a certain degree. Especially the ability
to develop an early mental model of programming and language skills appear to
be relevant. Fluid intelligence also shows predictive power, but appears to be
comparable with the ability to develop a mental model. Furthermore, we found a
significant positive effect of the syntax drill-and-practice exercises on the
success of a course. Implications: Our first suggestion of an early warning
system consists of few, easy-to-apply tests that can be integrated in
programming courses or applied even before a course starts. Thus, with the
start of a programming course, students who are at high risk of failing can be
identified and offered support, for example, in the form of syntax
drill-and-practice exercises to help students to develop programming skills.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01710" title="Abstract">arXiv:2402.01710</a> [<a href="/pdf/2402.01710" title="Download PDF">pdf</a>, <a href="/ps/2402.01710" title="Download PostScript">ps</a>, <a href="/format/2402.01710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Educational Equity: A Machine Learning Approach to Unravel  Achievement Disparities in Georgia
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yichen Ma</a>, 
<a href="/search/cs?searchtype=author&query=Nazzal%2C+D">Dima Nazzal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The COVID-19 pandemic has significantly exacerbated existing educational
disparities in Georgia's K-12 system, particularly in terms of racial and
ethnic achievement gaps. Utilizing machine learning methods, the study conducts
a comprehensive analysis of student achievement rates across different
demographics, regions, and subjects. The findings highlight a significant
decline in proficiency in English and Math during the pandemic, with a
noticeable contraction in score distribution and a greater impact on
economically disadvantaged and Black students. Socio-economic status, as
represented by the Directly Certified Percentage -- the percentage of students
eligible for free lunch, emerges as the most crucial factor, with additional
insights drawn from faculty resources such as teacher salaries and expenditure
on instruction. The study also identifies disparities in achievement rates
between urban and rural settings, as well as variations across counties,
underscoring the influence of geographical and socio-economic factors. The data
suggests that targeted interventions and resource allocation, particularly in
schools with higher percentages of economically disadvantaged students, are
essential for mitigating educational disparities.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01711" title="Abstract">arXiv:2402.01711</a> [<a href="/pdf/2402.01711" title="Download PDF">pdf</a>, <a href="/format/2402.01711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM on FHIR -- Demystifying Health Records
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmiedmayer%2C+P">Paul Schmiedmayer</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+A">Adrit Rao</a>, 
<a href="/search/cs?searchtype=author&query=Zagar%2C+P">Philipp Zagar</a>, 
<a href="/search/cs?searchtype=author&query=Ravi%2C+V">Vishnu Ravi</a>, 
<a href="/search/cs?searchtype=author&query=Zahedivash%2C+A">Aydin Zahedivash</a>, 
<a href="/search/cs?searchtype=author&query=Fereydooni%2C+A">Arash Fereydooni</a>, 
<a href="/search/cs?searchtype=author&query=Aalami%2C+O">Oliver Aalami</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print of the paper submitted to the Call for Papers for the Special Focus Issue on ChatGPT and Large Language Models (LLMs) in Biomedicine and Health at the Journal of the American Medical Informatics Association: <a href="https://academic.oup.com/jamia/pages/call-for-papers-for-special-focus-issue">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Objective: To enhance health literacy and accessibility of health information
for a diverse patient population by developing a patient-centered artificial
intelligence (AI) solution using large language models (LLMs) and Fast
Healthcare Interoperability Resources (FHIR) application programming interfaces
(APIs). Materials and Methods: The research involved developing LLM on FHIR, an
open-source mobile application allowing users to interact with their health
records using LLMs. The app is built on Stanford's Spezi ecosystem and uses
OpenAI's GPT-4. A pilot study was conducted with the SyntheticMass patient
dataset and evaluated by medical experts to assess the app's effectiveness in
increasing health literacy. The evaluation focused on the accuracy, relevance,
and understandability of the LLM's responses to common patient questions.
Results: LLM on FHIR demonstrated varying but generally high degrees of
accuracy and relevance in providing understandable health information to
patients. The app effectively translated medical data into patient-friendly
language and was able to adapt its responses to different patient profiles.
However, challenges included variability in LLM responses and the need for
precise filtering of health data. Discussion and Conclusion: LLMs offer
significant potential in improving health literacy and making health records
more accessible. LLM on FHIR, as a pioneering application in this field,
demonstrates the feasibility and challenges of integrating LLMs into patient
care. While promising, the implementation and pilot also highlight risks such
as inconsistent responses and the importance of replicable output. Future
directions include better resource identification mechanisms and executing LLMs
on-device to enhance privacy and reduce costs.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01712" title="Abstract">arXiv:2402.01712</a> [<a href="/pdf/2402.01712" title="Download PDF">pdf</a>, <a href="/format/2402.01712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Socially Aware Synthetic Data Generation for Suicidal Ideation Detection  Using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghanadian%2C+H">Hamideh Ghanadian</a>, 
<a href="/search/cs?searchtype=author&query=Nejadgholi%2C+I">Isar Nejadgholi</a>, 
<a href="/search/cs?searchtype=author&query=Osman%2C+H+A">Hussein Al Osman</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Access
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Suicidal ideation detection is a vital research area that holds great
potential for improving mental health support systems. However, the sensitivity
surrounding suicide-related data poses challenges in accessing large-scale,
annotated datasets necessary for training effective machine learning models. To
address this limitation, we introduce an innovative strategy that leverages the
capabilities of generative AI models, such as ChatGPT, Flan-T5, and Llama, to
create synthetic data for suicidal ideation detection. Our data generation
approach is grounded in social factors extracted from psychology literature and
aims to ensure coverage of essential information related to suicidal ideation.
In our study, we benchmarked against state-of-the-art NLP classification
models, specifically, those centered around the BERT family structures. When
trained on the real-world dataset, UMD, these conventional models tend to yield
F1-scores ranging from 0.75 to 0.87. Our synthetic data-driven method, informed
by social factors, offers consistent F1-scores of 0.82 for both models,
suggesting that the richness of topics in synthetic data can bridge the
performance gap across different model complexities. Most impressively, when we
combined a mere 30% of the UMD dataset with our synthetic data, we witnessed a
substantial increase in performance, achieving an F1-score of 0.88 on the UMD
test set. Such results underscore the cost-effectiveness and potential of our
approach in confronting major challenges in the field, such as data scarcity
and the quest for diversity in data representation.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01713" title="Abstract">arXiv:2402.01713</a> [<a href="/pdf/2402.01713" title="Download PDF">pdf</a>, <a href="/format/2402.01713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting Large Language Models for Zero-Shot Clinical Prediction with  Structured Longitudinal Electronic Health Record Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yinghao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zixiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Junyi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+Y">Yuning Tong</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+J">Jingkun An</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+W">Weibin Liao</a>, 
<a href="/search/cs?searchtype=author&query=Harrison%2C+E+M">Ewen M. Harrison</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Liantao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+C">Chengwei Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The inherent complexity of structured longitudinal Electronic Health Records
(EHR) data poses a significant challenge when integrated with Large Language
Models (LLMs), which are traditionally tailored for natural language
processing. Motivated by the urgent need for swift decision-making during new
disease outbreaks, where traditional predictive models often fail due to a lack
of historical data, this research investigates the adaptability of LLMs, like
GPT-4, to EHR data. We particularly focus on their zero-shot capabilities,
which enable them to make predictions in scenarios in which they haven't been
explicitly trained. In response to the longitudinal, sparse, and
knowledge-infused nature of EHR data, our prompting approach involves taking
into account specific EHR characteristics such as units and reference ranges,
and employing an in-context learning strategy that aligns with clinical
contexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets
demonstrate that with our elaborately designed prompting framework, LLMs can
improve prediction performance in key tasks such as mortality, length-of-stay,
and 30-day readmission by about 35\%, surpassing ML models in few-shot
settings. Our research underscores the potential of LLMs in enhancing clinical
decision-making, especially in urgent healthcare situations like the outbreak
of emerging diseases with no labeled data. The code is publicly available at
https://github.com/yhzhu99/llm4healthcare for reproducibility.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01714" title="Abstract">arXiv:2402.01714</a> [<a href="/pdf/2402.01714" title="Download PDF">pdf</a>, <a href="/format/2402.01714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TrICy: Trigger-guided Data-to-text Generation with Intent aware  Attention-Copy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+V">Vibhav Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Sourav Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=BSS%2C+H">Harichandana BSS</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+H">Himanshu Arora</a>, 
<a href="/search/cs?searchtype=author&query=Raja%2C+B+R+K">Barath Raj Kandur Raja</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in the IEEE/ACM Transactions on Audio, Speech, and Language Processing. (Sourav Ghosh and Vibhav Agarwal contributed equally to this work.)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Data-to-text (D2T) generation is a crucial task in many natural language
understanding (NLU) applications and forms the foundation of task-oriented
dialog systems. In the context of conversational AI solutions that can work
directly with local data on the user's device, architectures utilizing large
pre-trained language models (PLMs) are impractical for on-device deployment due
to a high memory footprint. To this end, we propose TrICy, a novel lightweight
framework for an enhanced D2T task that generates text sequences based on the
intent in context and may further be guided by user-provided triggers. We
leverage an attention-copy mechanism to predict out-of-vocabulary (OOV) words
accurately. Performance analyses on E2E NLG dataset (BLEU: 66.43%, ROUGE-L:
70.14%), WebNLG dataset (BLEU: Seen 64.08%, Unseen 52.35%), and our Custom
dataset related to text messaging applications, showcase our architecture's
effectiveness. Moreover, we show that by leveraging an optional trigger input,
data-to-text generation quality increases significantly and achieves the new
SOTA score of 69.29% BLEU for E2E NLG. Furthermore, our analyses show that
TrICy achieves at least 24% and 3% improvement in BLEU and METEOR respectively
over LLMs like GPT-3, ChatGPT, and Llama 2. We also demonstrate that in some
scenarios, performance improvement due to triggers is observed even when they
are absent in training.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01715" title="Abstract">arXiv:2402.01715</a> [<a href="/pdf/2402.01715" title="Download PDF">pdf</a>, <a href="/format/2402.01715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatGPT vs Gemini vs LLaMA on Multilingual Sentiment Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Buscemi%2C+A">Alessio Buscemi</a>, 
<a href="/search/cs?searchtype=author&query=Proverbio%2C+D">Daniele Proverbio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Automated sentiment analysis using Large Language Model (LLM)-based models
like ChatGPT, Gemini or LLaMA2 is becoming widespread, both in academic
research and in industrial applications. However, assessment and validation of
their performance in case of ambiguous or ironic text is still poor. In this
study, we constructed nuanced and ambiguous scenarios, we translated them in 10
languages, and we predicted their associated sentiment using popular LLMs. The
results are validated against post-hoc human responses. Ambiguous scenarios are
often well-coped by ChatGPT and Gemini, but we recognise significant biases and
inconsistent performance across models and evaluated human languages. This work
provides a standardised methodology for automated sentiment analysis evaluation
and makes a call for action to further improve the algorithms and their
underlying data, to improve their performance, interpretability and
applicability.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01716" title="Abstract">arXiv:2402.01716</a> [<a href="/pdf/2402.01716" title="Download PDF">pdf</a>, <a href="/ps/2402.01716" title="Download PostScript">ps</a>, <a href="/format/2402.01716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bloom-epistemic and sentiment analysis hierarchical classification in  course discussion forums
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Toba%2C+H">H. Toba</a>, 
<a href="/search/cs?searchtype=author&query=Hernita%2C+Y+T">Y. T. Hernita</a>, 
<a href="/search/cs?searchtype=author&query=Ayub%2C+M">M. Ayub</a>, 
<a href="/search/cs?searchtype=author&query=Wijanto%2C+M+C">M. C. Wijanto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 7 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Evaluation and Research in Education 13
  (2024) 80-90
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Online discussion forums are widely used for active textual interaction
between lecturers and students, and to see how the students have progressed in
a learning process. The objective of this study is to compare appropriate
machine-learning models to assess sentiments and Bloom\'s epistemic taxonomy
based on textual comments in educational discussion forums. Our proposed method
is called the hierarchical approach of Bloom-Epistemic and Sentiment Analysis
(BE-Sent). The research methodology consists of three main steps. The first
step is the data collection from the internal discussion forum and YouTube
comments of a Web Programming channel. The next step is text preprocessing to
annotate the text and clear unimportant words. Furthermore, with the text
dataset that has been successfully cleaned, sentiment analysis and epistemic
categorization will be done in each sentence of the text. Sentiment analysis is
divided into three categories: positive, negative, and neutral. Bloom\'s
epistemic is divided into six categories: remembering, understanding, applying,
analyzing, evaluating, and creating. This research has succeeded in producing a
course learning subsystem that assesses opinions based on text reviews of
discussion forums according to the category of sentiment and epistemic
analysis.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01717" title="Abstract">arXiv:2402.01717</a> [<a href="/pdf/2402.01717" title="Download PDF">pdf</a>, <a href="/format/2402.01717" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical  Regulatory Compliance Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jaewoong Kim</a> (Sungkyunkwan University), 
<a href="/search/cs?searchtype=author&query=Min%2C+M">Moohong Min</a> (Sungkyunkwan University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Total number of pages: 9. Total number of figures: 2. For the source code and experimental results of this paper, see <a href="https://github.com/jwoongkim11/QA-RAG.">this https URL</a> For the dataset used in training and evaluating the model, see <a href="https://huggingface.co/datasets/Jaymax/FDA">this https URL</a> Pharmaceuticals FAQ
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Regulatory compliance in the pharmaceutical industry entails navigating
through complex and voluminous guidelines, often requiring significant human
resources. To address these challenges, our study introduces a chatbot model
that utilizes generative AI and the Retrieval Augmented Generation (RAG)
method. This chatbot is designed to search for guideline documents relevant to
the user inquiries and provide answers based on the retrieved guidelines.
Recognizing the inherent need for high reliability in this domain, we propose
the Question and Answer Retrieval Augmented Generation (QA-RAG) model. In
comparative experiments, the QA-RAG model demonstrated a significant
improvement in accuracy, outperforming all other baselines including
conventional RAG methods. This paper details QA-RAG's structure and performance
evaluation, emphasizing its potential for the regulatory compliance domain in
the pharmaceutical industry and beyond. We have made our work publicly
available for further research and development.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01718" title="Abstract">arXiv:2402.01718</a> [<a href="/pdf/2402.01718" title="Download PDF">pdf</a>, <a href="/ps/2402.01718" title="Download PostScript">ps</a>, <a href="/format/2402.01718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Business Models for Digitalization Enabled Energy Efficiency and  Flexibility in Industry: A Survey with Nine Case Studies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhipeng Ma</a>, 
<a href="/search/cs?searchtype=author&query=J%C3%B8rgensen%2C+B+N">Bo N&#xf8;rregaard J&#xf8;rgensen</a>, 
<a href="/search/cs?searchtype=author&query=Levesque%2C+M">Michelle Levesque</a>, 
<a href="/search/cs?searchtype=author&query=Amazouz%2C+M">Mouloud Amazouz</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z+G">Zheng Grace Ma</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Energy Informatics. EI.A 2023. Lecture Notes in Computer Science,
  vol 14467
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Digitalization is challenging in heavy industrial sectors, and many pi-lot
projects facing difficulties to be replicated and scaled. Case studies are
strong pedagogical vehicles for learning and sharing experience &amp; knowledge,
but rarely available in the literature. Therefore, this paper conducts a survey
to gather a diverse set of nine industry cases, which are subsequently
subjected to analysis using the business model canvas (BMC). The cases are
summarized and compared based on nine BMC components, and a Value of Business
Model (VBM) evaluation index is proposed to assess the business potential of
industrial digital solutions. The results show that the main partners are
industry stakeholders, IT companies and academic institutes. Their key
activities for digital solutions include big-data analysis, machine learning
algorithms, digital twins, and internet of things developments. The value
propositions of most cases are improving energy efficiency and enabling energy
flexibility. Moreover, the technology readiness levels of six industrial
digital solutions are under level 7, indicating that they need further
validation in real-world environments. Building upon these insights, this paper
proposes six recommendations for future industrial digital solution
development: fostering cross-sector collaboration, prioritizing comprehensive
testing and validation, extending value propositions, enhancing product
adaptability, providing user-friendly platforms, and adopting transparent
recommendations.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01719" title="Abstract">arXiv:2402.01719</a> [<a href="/pdf/2402.01719" title="Download PDF">pdf</a>, <a href="/format/2402.01719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Moral Inconsistencies in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bonagiri%2C+V+K">Vamshi Krishna Bonagiri</a>, 
<a href="/search/cs?searchtype=author&query=Vennam%2C+S">Sreeram Vennam</a>, 
<a href="/search/cs?searchtype=author&query=Gaur%2C+M">Manas Gaur</a>, 
<a href="/search/cs?searchtype=author&query=Kumaraguru%2C+P">Ponnurangam Kumaraguru</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> BBNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">A Large Language Model~(LLM) is considered consistent if semantically
equivalent prompts produce semantically equivalent responses. Despite recent
advancements showcasing the impressive capabilities of LLMs in conversational
systems, we show that even state-of-the-art LLMs are highly inconsistent in
their generations, questioning their reliability. Prior research has tried to
measure this with task-specific accuracies. However, this approach is
unsuitable for moral scenarios, such as the trolley problem, with no
``correct'' answer. To address this issue, we propose a novel
information-theoretic measure called Semantic Graph Entropy~(SGE) to measure
the consistency of an LLM in moral scenarios. We leverage ``Rules of
Thumb''~(RoTs) to explain a model's decision-making strategies and further
enhance our metric. Compared to existing consistency metrics, SGE correlates
better with human judgments across five LLMs. In the future, we aim to
investigate the root causes of LLM inconsistencies and propose improvements.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01720" title="Abstract">arXiv:2402.01720</a> [<a href="/pdf/2402.01720" title="Download PDF">pdf</a>, <a href="/ps/2402.01720" title="Download PostScript">ps</a>, <a href="/format/2402.01720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning Based Amharic Chatbot for FAQs in Universities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hailu%2C+G+Y">Goitom Ybrah Hailu</a>, 
<a href="/search/cs?searchtype=author&query=Welay%2C+S">Shishay Welay</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">University students often spend a considerable amount of time seeking answers
to common questions from administrators or teachers. This can become tedious
for both parties, leading to a need for a solution. In response, this paper
proposes a chatbot model that utilizes natural language processing and deep
learning techniques to answer frequently asked questions (FAQs) in the Amharic
language. Chatbots are computer programs that simulate human conversation
through the use of artificial intelligence (AI), acting as a virtual assistant
to handle questions and other tasks. The proposed chatbot program employs
tokenization, normalization, stop word removal, and stemming to analyze and
categorize Amharic input sentences. Three machine learning model algorithms
were used to classify tokens and retrieve appropriate responses: Support Vector
Machine (SVM), Multinomial Na\"ive Bayes, and deep neural networks implemented
through TensorFlow, Keras, and NLTK. The deep learning model achieved the best
results with 91.55% accuracy and a validation loss of 0.3548 using an Adam
optimizer and SoftMax activation function. The chatbot model was integrated
with Facebook Messenger and deployed on a Heroku server for 24-hour
accessibility. The experimental results demonstrate that the chatbot framework
achieved its objectives and effectively addressed challenges such as Amharic
Fidel variation, morphological variation, and lexical gaps. Future research
could explore the integration of Amharic WordNet to narrow the lexical gap and
support more complex questions.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01721" title="Abstract">arXiv:2402.01721</a> [<a href="/pdf/2402.01721" title="Download PDF">pdf</a>, <a href="/format/2402.01721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attitudes Towards and Knowledge of Non-Consensual Synthetic Intimate  Imagery in 10 Countries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Umbach%2C+R">Rebecca Umbach</a>, 
<a href="/search/cs?searchtype=author&query=Henry%2C+N">Nicola Henry</a>, 
<a href="/search/cs?searchtype=author&query=Beard%2C+G">Gemma Beard</a>, 
<a href="/search/cs?searchtype=author&query=Berryessa%2C+C">Colleen Berryessa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Deepfake technology tools have become ubiquitous, "democratizing" the ability
to manipulate images and videos. One popular use of such technology is the
creation of sexually explicit content, which can then be posted and shared
widely on the internet. This article examines attitudes and behaviors related
to non-consensual synthetic intimate imagery (NSII) across over 16,000
respondents in 10 countries. Despite nascent societal awareness of NSII, NSII
behaviors were considered harmful. In regards to prevalence, 2.2% of all
respondents indicated personal victimization, and 1.8% all of respondents
indicated perpetration behaviors. Respondents from countries with relevant
legislation also reported perpetration and victimization experiences,
suggesting legislative action alone is not a sufficient solution to deter
perpetration. Technical considerations to reduce harms may include suggestions
for how individuals can better monitor their presence online, as well as
enforced platform policies which ban, or allow for removal of, NSII content.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01722" title="Abstract">arXiv:2402.01722</a> [<a href="/pdf/2402.01722" title="Download PDF">pdf</a>, <a href="/format/2402.01722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Large Language Model Performance To Answer Questions and  Extract Information More Accurately
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jijo%2C+K">Katherine Jijo</a>, 
<a href="/search/cs?searchtype=author&query=Setty%2C+S">Spurthi Setty</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+E">Eden Chung</a>, 
<a href="/search/cs?searchtype=author&query=Javid%2C+F">Fatima Javid</a>, 
<a href="/search/cs?searchtype=author&query=Vidra%2C+N">Natan Vidra</a>, 
<a href="/search/cs?searchtype=author&query=Clifford%2C+T">Tommy Clifford</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) generate responses to questions; however, their
effectiveness is often hindered by sub-optimal quality of answers and
occasional failures to provide accurate responses to questions. To address
these challenges, a fine-tuning process is employed, involving feedback and
examples to refine models. The objective is to enhance AI models through
continuous feedback loops, utilizing metrics such as cosine similarity, LLM
evaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like
GPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on
financial datasets, including the FinanceBench and RAG Instruct Benchmark
Tester Dataset, illustrating the necessity of fine-tuning. The results showcase
the capability of fine-tuned models to surpass the accuracy of zero-shot LLMs,
providing superior question and answering capabilities. Notably, the
combination of fine-tuning the LLM with a process known as Retrieval Augmented
Generation (RAG) proves to generate responses with improved accuracy.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01723" title="Abstract">arXiv:2402.01723</a> [<a href="/pdf/2402.01723" title="Download PDF">pdf</a>, <a href="/format/2402.01723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Study on Large Language Models in Accuracy and Robustness  under Chinese Industrial Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+W">Wenying Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+P">Pingchuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yichen Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">You Li</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Sijia He</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+B">Baozheng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+W">Weixi Gu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent years have witnessed the rapid development of large language models
(LLMs) in various domains. To better serve the large number of Chinese users,
many commercial vendors in China have adopted localization strategies, training
and providing local LLMs specifically customized for Chinese users.
Furthermore, looking ahead, one of the key future applications of LLMs will be
practical deployment in industrial production by enterprises and users in those
sectors. However, the accuracy and robustness of LLMs in industrial scenarios
have not been well studied. In this paper, we present a comprehensive empirical
study on the accuracy and robustness of LLMs in the context of the Chinese
industrial production area. We manually collected 1,200 domain-specific
problems from 8 different industrial sectors to evaluate LLM accuracy.
Furthermore, we designed a metamorphic testing framework containing four
industrial-specific stability categories with eight abilities, totaling 13,631
questions with variants to evaluate LLM robustness. In total, we evaluated 9
different LLMs developed by Chinese vendors, as well as four different LLMs
developed by global vendors. Our major findings include: (1) Current LLMs
exhibit low accuracy in Chinese industrial contexts, with all LLMs scoring less
than 0.6. (2) The robustness scores vary across industrial sectors, and local
LLMs overall perform worse than global ones. (3) LLM robustness differs
significantly across abilities. Global LLMs are more robust under
logical-related variants, while advanced local LLMs perform better on problems
related to understanding Chinese industrial terminology. Our study results
provide valuable guidance for understanding and promoting the industrial domain
capabilities of LLMs from both development and industrial enterprise
perspectives. The results further motivate possible research directions and
tooling support.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01724" title="Abstract">arXiv:2402.01724</a> [<a href="/pdf/2402.01724" title="Download PDF">pdf</a>, <a href="/format/2402.01724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CERM: Context-aware Literature-based Discovery via Sentiment Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Young%2C+J+C">Julio Christian Young</a>, 
<a href="/search/cs?searchtype=author&query=Akujuobi%2C+U">Uchenna Akujuobi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Driven by the abundance of biomedical publications, we introduce a sentiment
analysis task to understand food-health relationship. Prior attempts to
incorporate health into recipe recommendation and analysis systems have
primarily focused on ingredient nutritional components or utilized basic
computational models trained on curated labeled data. Enhanced models that
capture the inherent relationship between food ingredients and biomedical
concepts can be more beneficial for food-related research, given the wealth of
information in biomedical texts. Considering the costly data labeling process,
these models should effectively utilize both labeled and unlabeled data. This
paper introduces Entity Relationship Sentiment Analysis (ERSA), a new task that
captures the sentiment of a text based on an entity pair. ERSA extends the
widely studied Aspect Based Sentiment Analysis (ABSA) task. Specifically, our
study concentrates on the ERSA task applied to biomedical texts, focusing on
(entity-entity) pairs of biomedical and food concepts. ERSA poses a significant
challenge compared to traditional sentiment analysis tasks, as sentence
sentiment may not align with entity relationship sentiment. Additionally, we
propose CERM, a semi-supervised architecture that combines different word
embeddings to enhance the encoding of the ERSA task. Experimental results
showcase the model's efficiency across diverse learning scenarios.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01725" title="Abstract">arXiv:2402.01725</a> [<a href="/pdf/2402.01725" title="Download PDF">pdf</a>, <a href="/format/2402.01725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fortifying Ethical Boundaries in AI: Advanced Strategies for Enhancing  Security in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yunhong He</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+J">Jianling Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zhengqing Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advancements in large language models (LLMs) have significantly
enhanced capabilities in natural language processing and artificial
intelligence. These models, including GPT-3.5 and LLaMA-2, have revolutionized
text generation, translation, and question-answering tasks due to the
transformative Transformer model. Despite their widespread use, LLMs present
challenges such as ethical dilemmas when models are compelled to respond
inappropriately, susceptibility to phishing attacks, and privacy violations.
This paper addresses these challenges by introducing a multi-pronged approach
that includes: 1) filtering sensitive vocabulary from user input to prevent
unethical responses; 2) detecting role-playing to halt interactions that could
lead to 'prison break' scenarios; 3) implementing custom rule engines to
restrict the generation of prohibited content; and 4) extending these
methodologies to various LLM derivatives like Multi-Model Large Language Models
(MLLMs). Our approach not only fortifies models against unethical manipulations
and privacy breaches but also maintains their high performance across tasks. We
demonstrate state-of-the-art performance under various attack prompts, without
compromising the model's core functionalities. Furthermore, the introduction of
differentiated security levels empowers users to control their personal data
disclosure. Our methods contribute to reducing social risks and conflicts
arising from technological abuse, enhance data protection, and promote social
equity. Collectively, this research provides a framework for balancing the
efficiency of question-answering systems with user privacy and ethical
standards, ensuring a safer user experience and fostering trust in AI
technology.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01726" title="Abstract">arXiv:2402.01726</a> [<a href="/pdf/2402.01726" title="Download PDF">pdf</a>, <a href="/format/2402.01726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guidance for AI-Mediated Communication: AI Does Not Alter Perceptions of  Text Messages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Diamond%2C+N">N&#x27;yoma Diamond</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">For many people, anxiety, depression, and other social and mental factors can
make composing text messages an active challenge. To remedy this problem, large
language models (LLMs) may yet prove to be the perfect tool to assist users
that would otherwise find texting difficult or stressful. However, despite
rapid uptake in LLM usage, considerations for their assistive usage in text
message composition have not been explored. A primary concern regarding LLM
usage is that poor public sentiment regarding AI introduces the possibility
that its usage may harm perceptions of AI-assisted text messages, making usage
counter-productive. To (in)validate this possibility, we explore how the belief
that a text message did or did not receive AI assistance in composition alters
its perceived tone, clarity, and ability to convey intent. In this study, we
survey the perceptions of 26 participants on 18 randomly labeled pre-composed
text messages. In analyzing the participants' ratings of message tone, clarity,
and ability to convey intent, we find that there is no statistically
significant evidence that the belief that AI is utilized alters recipient
perceptions. This provides hopeful evidence that LLM-based text message
composition assistance can be implemented without the risk of
counter-productive outcomes.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01727" title="Abstract">arXiv:2402.01727</a> [<a href="/pdf/2402.01727" title="Download PDF">pdf</a>, <a href="/ps/2402.01727" title="Download PostScript">ps</a>, <a href="/format/2402.01727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting Diverse Ideas: Increasing AI Idea Variance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meincke%2C+L">Lennart Meincke</a>, 
<a href="/search/cs?searchtype=author&query=Mollick%2C+E+R">Ethan R. Mollick</a>, 
<a href="/search/cs?searchtype=author&query=Terwiesch%2C+C">Christian Terwiesch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Unlike routine tasks where consistency is prized, in creativity and
innovation the goal is to create a diverse set of ideas. This paper delves into
the burgeoning interest in employing Artificial Intelligence (AI) to enhance
the productivity and quality of the idea generation process. While previous
studies have found that the average quality of AI ideas is quite high, prior
research also has pointed to the inability of AI-based brainstorming to create
sufficient dispersion of ideas, which limits novelty and the quality of the
overall best idea. Our research investigates methods to increase the dispersion
in AI-generated ideas. Using GPT-4, we explore the effect of different
prompting methods on Cosine Similarity, the number of unique ideas, and the
speed with which the idea space gets exhausted. We do this in the domain of
developing a new product development for college students, priced under $50. In
this context, we find that (1) pools of ideas generated by GPT-4 with various
plausible prompts are less diverse than ideas generated by groups of human
subjects (2) the diversity of AI generated ideas can be substantially improved
using prompt engineering (3) Chain-of-Thought (CoT) prompting leads to the
highest diversity of ideas of all prompts we evaluated and was able to come
close to what is achieved by groups of human subjects. It also was capable of
generating the highest number of unique ideas of any prompt we studied.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01728" title="Abstract">arXiv:2402.01728</a> [<a href="/pdf/2402.01728" title="Download PDF">pdf</a>, <a href="/format/2402.01728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hardware Phi-1.5B: A Large Language Model Encodes Hardware Domain  Specific Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+W">Weimin Fu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shijie Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yifang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Haocheng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Dutta%2C+R">Raj Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kaichen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yier Jin</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xiaolong Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 29th IEEE/ACM Asia and South Pacific Design Automation Conference
  (ASP-DAC); 2024 January; Incheon Songdo Convensia, South Korea
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the rapidly evolving semiconductor industry, where research, design,
verification, and manufacturing are intricately linked, the potential of Large
Language Models to revolutionize hardware design and security verification is
immense. The primary challenge, however, lies in the complexity of hardware
specific issues that are not adequately addressed by the natural language or
software code knowledge typically acquired during the pretraining stage.
Additionally, the scarcity of datasets specific to the hardware domain poses a
significant hurdle in developing a foundational model. Addressing these
challenges, this paper introduces Hardware Phi 1.5B, an innovative large
language model specifically tailored for the hardware domain of the
semiconductor industry. We have developed a specialized, tiered dataset
comprising small, medium, and large subsets and focused our efforts on
pretraining using the medium dataset. This approach harnesses the compact yet
efficient architecture of the Phi 1.5B model. The creation of this first
pretrained, hardware domain specific large language model marks a significant
advancement, offering improved performance in hardware design and verification
tasks and illustrating a promising path forward for AI applications in the
semiconductor sector.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01729" title="Abstract">arXiv:2402.01729</a> [<a href="/pdf/2402.01729" title="Download PDF">pdf</a>, <a href="/format/2402.01729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contextualization Distillation from Large Language Model for Knowledge  Graph Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dawei Li</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhen Tan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianlong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huan Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EACL 2024 findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While textual information significantly enhances the performance of
pre-trained language models (PLMs) in knowledge graph completion (KGC), the
static and noisy nature of existing corpora collected from Wikipedia articles
or synsets definitions often limits the potential of PLM-based KGC models. To
surmount these challenges, we introduce the Contextualization Distillation
strategy, a versatile plug-in-and-play approach compatible with both
discriminative and generative KGC frameworks. Our method begins by instructing
large language models (LLMs) to transform compact, structural triplets into
context-rich segments. Subsequently, we introduce two tailored auxiliary tasks,
reconstruction and contextualization, allowing smaller KGC models to assimilate
insights from these enriched triplets. Comprehensive evaluations across diverse
datasets and KGC techniques highlight the efficacy and adaptability of our
approach, revealing consistent performance enhancements irrespective of
underlying pipelines or architectures. Moreover, our analysis makes our method
more explainable and provides insight into generating path selection, as well
as the choosing of suitable distillation tasks. All the code and data in this
work will be released at
https://github.com/David-Li0406/Contextulization-Distillation
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01730" title="Abstract">arXiv:2402.01730</a> [<a href="/pdf/2402.01730" title="Download PDF">pdf</a>, <a href="/format/2402.01730" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating LLM -- Generated Multimodal Diagnosis from Medical Images and  Symptom Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Panagoulias%2C+D+P">Dimitrios P. Panagoulias</a>, 
<a href="/search/cs?searchtype=author&query=Virvou%2C+M">Maria Virvou</a>, 
<a href="/search/cs?searchtype=author&query=Tsihrintzis%2C+G+A">George A. Tsihrintzis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Department of Informatics, University of Piraeus, Greece
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) constitute a breakthrough state-of-the-art
Artificial Intelligence technology which is rapidly evolving and promises to
aid in medical diagnosis. However, the correctness and the accuracy of their
returns has not yet been properly evaluated. In this work, we propose an LLM
evaluation paradigm that incorporates two independent steps of a novel
methodology, namely (1) multimodal LLM evaluation via structured interactions
and (2) follow-up, domain-specific analysis based on data extracted via the
previous interactions. Using this paradigm, (1) we evaluate the correctness and
accuracy of LLM-generated medical diagnosis with publicly available multimodal
multiple-choice questions(MCQs) in the domain of Pathology and (2) proceed to a
systemic and comprehensive analysis of extracted results. We used
GPT-4-Vision-Preview as the LLM to respond to complex, medical questions
consisting of both images and text, and we explored a wide range of diseases,
conditions, chemical compounds, and related entity types that are included in
the vast knowledge domain of Pathology. GPT-4-Vision-Preview performed quite
well, scoring approximately 84\% of correct diagnoses. Next, we further
analyzed the findings of our work, following an analytical approach which
included Image Metadata Analysis, Named Entity Recognition and Knowledge
Graphs. Weaknesses of GPT-4-Vision-Preview were revealed on specific knowledge
paths, leading to a further understanding of its shortcomings in specific
areas. Our methodology and findings are not limited to the use of
GPT-4-Vision-Preview, but a similar approach can be followed to evaluate the
usefulness and accuracy of other LLMs and, thus, improve their use with further
optimization.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01731" title="Abstract">arXiv:2402.01731</a> [<a href="/pdf/2402.01731" title="Download PDF">pdf</a>, <a href="/ps/2402.01731" title="Download PostScript">ps</a>, <a href="/format/2402.01731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating AI in Educational Measurement: ChatGPT&#x27;s Efficacy in Item  Response Theory Data Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gurdil%2C+H">Hatice Gurdil</a>, 
<a href="/search/cs?searchtype=author&query=Soguksu%2C+Y+B">Yesim Beril Soguksu</a>, 
<a href="/search/cs?searchtype=author&query=Salihoglu%2C+S">Salih Salihoglu</a>, 
<a href="/search/cs?searchtype=author&query=Coskun%2C+F">Fatma Coskun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">This paper explores the efficacy of ChatGPT in generating data for Item
Response Theory (IRT) using the R programming language. Focusing on the 2
Parameter Logistic Model (2PLM), it evaluates datasets produced by ChatGPT
against several IRT assumptions like unidimensionality and local independence.
The study compares these datasets with those generated by researchers,
assessing compliance with simulation conditions, bias, and RMSE values. The
results indicate that while ChatGPT algorithms successfully generate data
adhering to IRT assumptions, they exhibit more issues with item parameter
compliance compared to researcher-generated algorithms. This study highlights
ChatGPT's potential in data generation, but also underscores the importance of
human expertise in guiding its outputs for scientific research.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01732" title="Abstract">arXiv:2402.01732</a> [<a href="/pdf/2402.01732" title="Download PDF">pdf</a>, <a href="/format/2402.01732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying and Improving Disability Bias in GAI-Based Resume Screening
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Glazko%2C+K">Kate Glazko</a>, 
<a href="/search/cs?searchtype=author&query=Mohammed%2C+Y">Yusuf Mohammed</a>, 
<a href="/search/cs?searchtype=author&query=Kosa%2C+B">Ben Kosa</a>, 
<a href="/search/cs?searchtype=author&query=Potluri%2C+V">Venkatesh Potluri</a>, 
<a href="/search/cs?searchtype=author&query=Mankoff%2C+J">Jennifer Mankoff</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As Generative AI rises in adoption, its use has expanded to include domains
such as hiring and recruiting. However, without examining the potential of
bias, this may negatively impact marginalized populations, including people
with disabilities. To address this important concern, we present a resume audit
study, in which we ask ChatGPT (specifically, GPT-4) to rank a resume against
the same resume enhanced with an additional leadership award, scholarship,
panel presentation, and membership that are disability related. We find that
GPT-4 exhibits prejudice towards these enhanced CVs. Further, we show that this
prejudice can be quantifiably reduced by training a custom GPTs on principles
of DEI and disability justice. Our study also includes a unique qualitative
analysis of the types of direct and indirect ableism GPT-4 uses to justify its
biased decisions and suggest directions for additional bias mitigation work.
Additionally, since these justifications are presumably drawn from training
data containing real-world biased statements made by humans, our analysis
suggests additional avenues for understanding and addressing human bias.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01733" title="Abstract">arXiv:2402.01733</a> [<a href="/pdf/2402.01733" title="Download PDF">pdf</a>, <a href="/ps/2402.01733" title="Download PostScript">ps</a>, <a href="/format/2402.01733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Development and Testing of Retrieval Augmented Generation in Large  Language Models -- A Case Study Report
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ke%2C+Y">YuHe Ke</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+L">Liyuan Jin</a>, 
<a href="/search/cs?searchtype=author&query=Elangovan%2C+K">Kabilan Elangovan</a>, 
<a href="/search/cs?searchtype=author&query=Abdullah%2C+H+R">Hairil Rizal Abdullah</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Nan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sia%2C+A+T+H">Alex Tiong Heng Sia</a>, 
<a href="/search/cs?searchtype=author&query=Soh%2C+C+R">Chai Rick Soh</a>, 
<a href="/search/cs?searchtype=author&query=Tung%2C+J+Y+M">Joshua Yi Min Tung</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+J+C+L">Jasmine Chiat Ling Ong</a>, 
<a href="/search/cs?searchtype=author&query=Ting%2C+D+S+W">Daniel Shu Wei Ting</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Purpose: Large Language Models (LLMs) hold significant promise for medical
applications. Retrieval Augmented Generation (RAG) emerges as a promising
approach for customizing domain knowledge in LLMs. This case study presents the
development and evaluation of an LLM-RAG pipeline tailored for healthcare,
focusing specifically on preoperative medicine.
<br />Methods: We developed an LLM-RAG model using 35 preoperative guidelines and
tested it against human-generated responses, with a total of 1260 responses
evaluated. The RAG process involved converting clinical documents into text
using Python-based frameworks like LangChain and Llamaindex, and processing
these texts into chunks for embedding and retrieval. Vector storage techniques
and selected embedding models to optimize data retrieval, using Pinecone for
vector storage with a dimensionality of 1536 and cosine similarity for loss
metrics. Human-generated answers, provided by junior doctors, were used as a
comparison.
<br />Results: The LLM-RAG model generated answers within an average of 15-20
seconds, significantly faster than the 10 minutes typically required by humans.
Among the basic LLMs, GPT4.0 exhibited the best accuracy of 80.1%. This
accuracy was further increased to 91.4% when the model was enhanced with RAG.
Compared to the human-generated instructions, which had an accuracy of 86.3%,
the performance of the GPT4.0 RAG model demonstrated non-inferiority (p=0.610).
<br />Conclusions: In this case study, we demonstrated a LLM-RAG model for
healthcare implementation. The pipeline shows the advantages of grounded
knowledge, upgradability, and scalability as important aspects of healthcare
LLM deployment.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01734" title="Abstract">arXiv:2402.01734</a> [<a href="/pdf/2402.01734" title="Download PDF">pdf</a>, <a href="/format/2402.01734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CFTM: Continuous time fractional topic model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nakagawa%2C+K">Kei Nakagawa</a>, 
<a href="/search/cs?searchtype=author&query=Hayashi%2C+K">Kohei Hayashi</a>, 
<a href="/search/cs?searchtype=author&query=Fujimoto%2C+Y">Yugo Fujimoto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Computational Finance (q-fin.CP); Applications (stat.AP)

</div>
<p class="mathjax">In this paper, we propose the Continuous Time Fractional Topic Model (cFTM),
a new method for dynamic topic modeling. This approach incorporates fractional
Brownian motion~(fBm) to effectively identify positive or negative correlations
in topic and word distribution over time, revealing long-term dependency or
roughness. Our theoretical analysis shows that the cFTM can capture these
long-term dependency or roughness in both topic and word distributions,
mirroring the main characteristics of fBm. Moreover, we prove that the
parameter estimation process for the cFTM is on par with that of LDA,
traditional topic models. To demonstrate the cFTM's property, we conduct
empirical study using economic news articles. The results from these tests
support the model's ability to identify and track long-term dependency or
roughness in topics over time.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01735" title="Abstract">arXiv:2402.01735</a> [<a href="/pdf/2402.01735" title="Download PDF">pdf</a>, <a href="/format/2402.01735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yilin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+R">Rong Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jing Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hillming Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Visually Impaired Assistance (VIA) aims to automatically help visually
impaired (VI) handle daily activities. The advancement of VIA primarily depends
on developments in Computer Vision (CV) and Natural Language Processing (NLP),
both of which exhibit cutting-edge paradigms with large models (LMs).
Furthermore, LMs have shown exceptional multimodal abilities to tackle
challenging physically-grounded tasks such as embodied robots. To investigate
the potential and limitations of state-of-the-art (SOTA) LMs' capabilities in
VIA applications, we present an extensive study for the task of VIA with LMs
(\textbf{VIALM}). In this task, given an \textit{image} illustrating the
physical environments and a \textit{linguistic request} from a VI user, VIALM
aims to output step-by-step \textit{guidance} to assist the VI user in
fulfilling the request grounded in the environment. The study consists of a
survey reviewing recent LM research and benchmark experiments examining
selected LMs' capabilities in VIA. The results indicate that while LMs can
augment VIA, their output cannot be well \textit{grounded} in reality (i.e.,
25.7\% GPT-4's responses) and lacks \textit{fine-grained} guidance (i.e.,
32.1\% GPT-4's responses).
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01736" title="Abstract">arXiv:2402.01736</a> [<a href="/pdf/2402.01736" title="Download PDF">pdf</a>, <a href="/format/2402.01736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SADAS: A Dialogue Assistant System Towards Remediating Norm Violations  in Bilingual Socio-Cultural Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hua%2C+Y">Yuncheng Hua</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhuang Li</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+L">Linhao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Satriadi%2C+K+A">Kadek Ananta Satriadi</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+T">Tao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+H">Haolan Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+L">Lizhen Qu</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+S">Suraj Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Zukerman%2C+I">Ingrid Zukerman</a>, 
<a href="/search/cs?searchtype=author&query=Semnani-Azad%2C+Z">Zhaleh Semnani-Azad</a>, 
<a href="/search/cs?searchtype=author&query=Haffari%2C+G">Gholamreza Haffari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In today's globalized world, bridging the cultural divide is more critical
than ever for forging meaningful connections. The Socially-Aware Dialogue
Assistant System (SADAS) is our answer to this global challenge, and it's
designed to ensure that conversations between individuals from diverse cultural
backgrounds unfold with respect and understanding. Our system's novel
architecture includes: (1) identifying the categories of norms present in the
dialogue, (2) detecting potential norm violations, (3) evaluating the severity
of these violations, (4) implementing targeted remedies to rectify the
breaches, and (5) articulates the rationale behind these corrective actions. We
employ a series of State-Of-The-Art (SOTA) techniques to build different
modules, and conduct numerous experiments to select the most suitable backbone
model for each of the modules. We also design a human preference experiment to
validate the overall performance of the system. We will open-source our system
(including source code, tools and applications), hoping to advance future
research. A demo video of our system can be found
at:~\url{https://youtu.be/JqetWkfsejk}. We have released our code and software
at:~\url{https://github.com/AnonymousEACLDemo/SADAS}.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01737" title="Abstract">arXiv:2402.01737</a> [<a href="/pdf/2402.01737" title="Download PDF">pdf</a>, <a href="/format/2402.01737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assistive Large Language Model Agents for Socially-Aware Negotiation  Dialogues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hua%2C+Y">Yuncheng Hua</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+L">Lizhen Qu</a>, 
<a href="/search/cs?searchtype=author&query=Haffari%2C+G">Gholamreza Haffari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 1 figure, 11 tables; Under review in IJCAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this work, we aim to develop LLM agents to mitigate social norm violations
in negotiations in a multi-agent setting. We simulate real-world negotiations
by letting two large Language Models (LLMs) play the roles of two negotiators
in each conversation. A third LLM acts as a remediation agent to rewrite
utterances violating norms for improving negotiation outcomes. As it is a novel
task, no manually constructed data is available. To address this limitation, we
introduce a value impact based In-Context Learning (ICL) method to identify
high-quality ICL examples for the LLM-based remediation agents, where the value
impact function measures the quality of negotiation outcomes. We show the
connection of this method to policy learning and provide rich empirical
evidence to demonstrate its effectiveness in negotiations across three
different topics: product sale, housing price, and salary negotiation. The
source code and the generated dataset will be publicly available upon
acceptance.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01738" title="Abstract">arXiv:2402.01738</a> [<a href="/pdf/2402.01738" title="Download PDF">pdf</a>, <a href="/format/2402.01738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> C4Q: A Chatbot for Quantum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aragon%C3%A9s-Soria%2C+Y">Yaiza Aragon&#xe9;s-Soria</a>, 
<a href="/search/cs?searchtype=author&query=Oriol%2C+M">Manuel Oriol</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted in the 5th International Workshop on Quantum Software Engineering (Q-SE 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">Quantum computing is a growing field that promises many real-world
applications such as quantum cryptography or quantum finance. The number of
people able to use quantum computing is however still very small. This
limitation comes from the difficulty to understand the concepts and to know how
to start coding. Therefore, there is a need for tools that can assist
non-expert in overcoming this complexity. One possibility would be to use
existing conversational agents. Unfortunately ChatGPT and other Large-Language
Models produce inaccurate results. This article presents C4Q, a chatbot that
answers accurately basic questions and guides users when trying to code quantum
programs. Contrary to other approaches C4Q uses a pre-trained large language
model only to discover and classify user requests. It then generates an
accurate answer using an own engine. Thanks to this architectural design, C4Q's
answers are always correct, and thus C4Q can become a support tool that makes
quantum computing more available to non-experts.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01739" title="Abstract">arXiv:2402.01739</a> [<a href="/pdf/2402.01739" title="Download PDF">pdf</a>, <a href="/format/2402.01739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+F">Fuzhao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zian Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yao Fu</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+J">Jinjie Ni</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zangwei Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wangchunshu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yang You</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)

</div>
<p class="mathjax">To help the open-source community have a better understanding of
Mixture-of-Experts (MoE) based large language models (LLMs), we train and
release OpenMoE, a series of fully open-sourced and reproducible decoder-only
MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T
tokens. Our investigation confirms that MoE-based LLMs can offer a more
favorable cost-effectiveness trade-off than dense LLMs, highlighting the
potential effectiveness for future LLM development.
<br />One more important contribution of this study is an in-depth analysis of the
routing mechanisms within our OpenMoE models, leading to three significant
findings: Context-Independent Specialization, Early Routing Learning, and
Drop-towards-the-End. We discovered that routing decisions in MoE models are
predominantly based on token IDs, with minimal context relevance. The
token-to-expert assignments are determined early in the pre-training phase and
remain largely unchanged. This imperfect routing can result in performance
degradation, particularly in sequential tasks like multi-turn conversations,
where tokens appearing later in a sequence are more likely to be dropped.
Finally, we rethink our design based on the above-mentioned observations and
analysis. To facilitate future MoE LLM development, we propose potential
strategies for mitigating the issues we found and further improving
off-the-shelf MoE LLM designs.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01740" title="Abstract">arXiv:2402.01740</a> [<a href="/pdf/2402.01740" title="Download PDF">pdf</a>, <a href="/format/2402.01740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compensatory Biases Under Cognitive Load: Reducing Selection Bias in  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eicher%2C+J+E">J. E. Eicher</a>, 
<a href="/search/cs?searchtype=author&query=Irgoli%C4%8D%2C+R+F">R. F. Irgoli&#x10d;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 23 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) like gpt-3.5-turbo and claude-instant-1.2 have
become instrumental in interpreting and executing semantic-based tasks.
Unfortunately, these models' inherent biases, akin to human cognitive biases,
adversely affect their performance. Particularly affected is object selection
from lists; a fundamental operation in digital navigation and decision-making.
This research critically examines these biases and quantifies the effects on a
representative list selection task. To explore these biases, we conducted a
series of controlled experiments, manipulating temperature, list length, object
identity, object type, prompt complexity, and model. This enabled us to isolate
and measure the influence of the biases on selection behavior. Our findings
show that bias structure is strongly dependent on the model, with object type
modulating the magnitude of the effect. With a strong primacy effect, causing
the first objects in a list to be disproprotionately represented in outputs.
Furthermore the usage of guard rails, a prompt engineering method of ensuring a
response structure, can increase bias and decrease instruction adherence when
combined with a selection task. The bias is ablated when the guard rail step is
separated from the list sampling step, lowering the complexity of each
individual task. The implications of this research are two-fold, practically
providing a guide for designing unbiased LLM applications and theoretically
suggesting that LLMs experience a form of cognitive load compensated for by
increasing bias.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01741" title="Abstract">arXiv:2402.01741</a> [<a href="/pdf/2402.01741" title="Download PDF">pdf</a>, <a href="/ps/2402.01741" title="Download PostScript">ps</a>, <a href="/format/2402.01741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Development and Testing of a Novel Large Language Model-Based Clinical  Decision Support Systems for Medication Safety in 12 Clinical Specialties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ong%2C+J+C+L">Jasmine Chiat Ling Ong</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+L">Liyuan Jin</a>, 
<a href="/search/cs?searchtype=author&query=Elangovan%2C+K">Kabilan Elangovan</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+G+Y+S">Gilbert Yong San Lim</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+D+Y+Z">Daniel Yan Zheng Lim</a>, 
<a href="/search/cs?searchtype=author&query=Sng%2C+G+G+R">Gerald Gui Ren Sng</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+Y">Yuhe Ke</a>, 
<a href="/search/cs?searchtype=author&query=Tung%2C+J+Y+M">Joshua Yi Min Tung</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+R+J">Ryan Jian Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Koh%2C+C+M+Y">Christopher Ming Yao Koh</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K+Z+H">Keane Zhi Hao Lee</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chng%2C+J+K">Jack Kian Chng</a>, 
<a href="/search/cs?searchtype=author&query=Than%2C+A">Aung Than</a>, 
<a href="/search/cs?searchtype=author&query=Goh%2C+K+J">Ken Junyang Goh</a>, 
<a href="/search/cs?searchtype=author&query=Ting%2C+D+S+W">Daniel Shu Wei Ting</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Importance: We introduce a novel Retrieval Augmented Generation (RAG)-Large
Language Model (LLM) as a Clinical Decision Support System (CDSS) for safe
medication prescription. This model addresses the limitations of traditional
rule-based CDSS by providing relevant prescribing error alerts tailored to
patient context and institutional guidelines.
<br />Objective: The study evaluates the efficacy of an LLM-based CDSS in
identifying medication errors across various medical and surgical case
vignettes, compared to a human expert panel. It also examines clinician
preferences among different CDSS integration modalities: junior pharmacist,
LLM-based CDSS alone, and a combination of both.
<br />Design, Setting, and Participants: Utilizing a RAG model with GPT-4.0, the
study involved 61 prescribing error scenarios within 23 clinical vignettes
across 12 specialties. An expert panel assessed these cases using the PCNE
classification and NCC MERP index. Three junior pharmacists independently
reviewed each vignette under simulated conditions.
<br />Main Outcomes and Measures: The study assesses the LLM-based CDSS's accuracy,
precision, recall, and F1 scores in identifying Drug-Related Problems (DRPs),
compared to junior pharmacists alone or in an assistive mode with the CDSS.
<br />Results: The co-pilot mode of RAG-LLM significantly improved DRP
identification accuracy by 22% over solo pharmacists. It showed higher recall
and F1 scores, indicating better detection of severe DRPs, despite a slight
decrease in precision. Accuracy varied across categories when pharmacists had
access to RAG-LLM responses.
<br />Conclusions: The RAG-LLM based CDSS enhances medication error identification
accuracy when used with junior pharmacists, especially in detecting severe
DRPs.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01742" title="Abstract">arXiv:2402.01742</a> [<a href="/pdf/2402.01742" title="Download PDF">pdf</a>, <a href="/format/2402.01742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Optimizing the Costs of LLM Usage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shekhar%2C+S">Shivanshu Shekhar</a>, 
<a href="/search/cs?searchtype=author&query=Dubey%2C+T">Tanishq Dubey</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+K">Koyel Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Saxena%2C+A">Apoorv Saxena</a>, 
<a href="/search/cs?searchtype=author&query=Tyagi%2C+A">Atharv Tyagi</a>, 
<a href="/search/cs?searchtype=author&query=Kotla%2C+N">Nishanth Kotla</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages + Appendix, Total 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Generative AI and LLMs in particular are heavily used nowadays for various
document processing tasks such as question answering and summarization.
However, different LLMs come with different capabilities for different tasks as
well as with different costs, tokenization, and latency. In fact, enterprises
are already incurring huge costs of operating or using LLMs for their
respective use cases.
<br />In this work, we propose optimizing the usage costs of LLMs by estimating
their output quality (without actually invoking the LLMs), and then solving an
optimization routine for the LLM selection to either keep costs under a budget,
or minimize the costs, in a quality and latency aware manner. We propose a
model to predict the output quality of LLMs on document processing tasks like
summarization, followed by an LP rounding algorithm to optimize the selection
of LLMs. We study optimization problems trading off the quality and costs, both
theoretically and empirically. We further propose a sentence simplification
model for reducing the number of tokens in a controlled manner. Additionally,
we propose several deterministic heuristics for reducing tokens in a quality
aware manner, and study the related optimization problem of applying the
heuristics optimizing the quality and cost trade-off. We perform extensive
empirical validation of our methods on not only enterprise datasets but also on
open-source datasets, annotated by us, and show that we perform much better
compared to closest baselines. Our methods reduce costs by 40%- 90% while
improving quality by 4%-7%. We will release the annotated open source datasets
to the community for further research and exploration.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01743" title="Abstract">arXiv:2402.01743</a> [<a href="/pdf/2402.01743" title="Download PDF">pdf</a>, <a href="/ps/2402.01743" title="Download PostScript">ps</a>, <a href="/format/2402.01743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Reasoning Under Uncertainty Trap: A Structural AI Risk
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pilditch%2C+T+D">Toby D. Pilditch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 51 pages (excluding references), 7 chapters, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This report examines a novel risk associated with current (and projected) AI
tools. Making effective decisions about future actions requires us to reason
under uncertainty (RUU), and doing so is essential to many critical real world
problems. Overfaced by this challenge, there is growing demand for AI tools
like LLMs to assist decision-makers. Having evidenced this demand and the
incentives behind it, we expose a growing risk: we 1) do not currently
sufficiently understand LLM capabilities in this regard, and 2) have no
guarantees of performance given fundamental computational explosiveness and
deep uncertainty constraints on accuracy. This report provides an exposition of
what makes RUU so challenging for both humans and machines, and relates these
difficulties to prospective AI timelines and capabilities. Having established
this current potential misuse risk, we go on to expose how this seemingly
additive risk (more misuse additively contributed to potential harm) in fact
has multiplicative properties. Specifically, we detail how this misuse risk
connects to a wider network of underlying structural risks (e.g., shifting
incentives, limited transparency, and feedback loops) to produce non-linear
harms. We go on to provide a solutions roadmap that targets multiple leverage
points in the structure of the problem. This includes recommendations for all
involved actors (prospective users, developers, and policy-makers) and enfolds
insights from areas including Decision-making Under Deep Uncertainty and
complex systems theory. We argue this report serves not only to raise awareness
(and subsequently mitigate/correct) of a current, novel AI risk, but also
awareness of the underlying class of structural risks by illustrating how their
interconnected nature poses twin-dangers of camouflaging their presence, whilst
amplifying their potential effects.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01746" title="Abstract">arXiv:2402.01746</a> [<a href="/pdf/2402.01746" title="Download PDF">pdf</a>, <a href="/format/2402.01746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3DG: A Framework for Using Generative AI for Handling Sparse Learner  Performance Data From Intelligent Tutoring Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jionghao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Borchers%2C+C">Conrad Borchers</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+M">Meng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiangen Hu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> LAK 2024: International Workshop on Generative AI for Learning
  Analytics (GenAI-LA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Learning performance data (e.g., quiz scores and attempts) is significant for
understanding learner engagement and knowledge mastery level. However, the
learning performance data collected from Intelligent Tutoring Systems (ITSs)
often suffers from sparsity, impacting the accuracy of learner modeling and
knowledge assessments. To address this, we introduce the 3DG framework
(3-Dimensional tensor for Densification and Generation), a novel approach
combining tensor factorization with advanced generative models, including
Generative Adversarial Network (GAN) and Generative Pre-trained Transformer
(GPT), for enhanced data imputation and augmentation. The framework operates by
first representing the data as a three-dimensional tensor, capturing dimensions
of learners, questions, and attempts. It then densifies the data through tensor
factorization and augments it using Generative AI models, tailored to
individual learning patterns identified via clustering. Applied to data from an
AutoTutor lesson by the Center for the Study of Adult Literacy (CSAL), the 3DG
framework effectively generated scalable, personalized simulations of learning
performance. Comparative analysis revealed GAN's superior reliability over
GPT-4 in this context, underscoring its potential in addressing data sparsity
challenges in ITSs and contributing to the advancement of personalized
educational technology.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01748" title="Abstract">arXiv:2402.01748</a> [<a href="/pdf/2402.01748" title="Download PDF">pdf</a>, <a href="/format/2402.01748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Multi-Modal Models (LMMs) as Universal Foundation Models for  AI-Native Wireless Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shengzhe Xu</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+C+K">Christo Kurisummoottil Thomas</a>, 
<a href="/search/cs?searchtype=author&query=Hashash%2C+O">Omar Hashash</a>, 
<a href="/search/cs?searchtype=author&query=Muralidhar%2C+N">Nikhil Muralidhar</a>, 
<a href="/search/cs?searchtype=author&query=Saad%2C+W">Walid Saad</a>, 
<a href="/search/cs?searchtype=author&query=Ramakrishnan%2C+N">Naren Ramakrishnan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) and foundation models have been recently touted
as a game-changer for 6G systems. However, recent efforts on LLMs for wireless
networks are limited to a direct application of existing language models that
were designed for natural language processing (NLP) applications. To address
this challenge and create wireless-centric foundation models, this paper
presents a comprehensive vision on how to design universal foundation models
that are tailored towards the deployment of artificial intelligence (AI)-native
networks. Diverging from NLP-based foundation models, the proposed framework
promotes the design of large multi-modal models (LMMs) fostered by three key
capabilities: 1) processing of multi-modal sensing data, 2) grounding of
physical symbol representations in real-world wireless systems using causal
reasoning and retrieval-augmented generation (RAG), and 3) enabling
instructibility from the wireless environment feedback to facilitate dynamic
network adaptation thanks to logical and mathematical reasoning facilitated by
neuro-symbolic AI. In essence, these properties enable the proposed LMM
framework to build universal capabilities that cater to various cross-layer
networking tasks and alignment of intents across different domains. Preliminary
results from experimental evaluation demonstrate the efficacy of grounding
using RAG in LMMs, and showcase the alignment of LMMs with wireless system
designs. Furthermore, the enhanced rationale exhibited in the responses to
mathematical questions by LMMs, compared to vanilla LLMs, demonstrates the
logical and mathematical reasoning capabilities inherent in LMMs. Building on
those results, we present a sequel of open questions and challenges for LMMs.
We then conclude with a set of recommendations that ignite the path towards
LMM-empowered AI-native systems.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01749" title="Abstract">arXiv:2402.01749</a> [<a href="/pdf/2402.01749" title="Download PDF">pdf</a>, <a href="/format/2402.01749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Urban General Intelligence: A Review and Outlook of Urban  Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weijia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jindong Han</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+H">Hang Ni</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Hui Xiong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Machine learning techniques are now integral to the advancement of
intelligent urban services, playing a crucial role in elevating the efficiency,
sustainability, and livability of urban environments. The recent emergence of
foundation models such as ChatGPT marks a revolutionary shift in the fields of
machine learning and artificial intelligence. Their unparalleled capabilities
in contextual understanding, problem solving, and adaptability across a wide
range of tasks suggest that integrating these models into urban domains could
have a transformative impact on the development of smart cities. Despite
growing interest in Urban Foundation Models~(UFMs), this burgeoning field faces
challenges such as a lack of clear definitions, systematic reviews, and
universalizable solutions. To this end, this paper first introduces the concept
of UFM and discusses the unique challenges involved in building them. We then
propose a data-centric taxonomy that categorizes current UFM-related works,
based on urban data modalities and types. Furthermore, to foster advancement in
this field, we present a promising framework aimed at the prospective
realization of UFMs, designed to overcome the identified challenges.
Additionally, we explore the application landscape of UFMs, detailing their
potential impact in various urban contexts. Relevant papers and open-source
resources have been collated and are continuously updated at
https://github.com/usail-hkust/Awesome-Urban-Foundation-Models.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01750" title="Abstract">arXiv:2402.01750</a> [<a href="/pdf/2402.01750" title="Download PDF">pdf</a>, <a href="/format/2402.01750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PACE: A Pragmatic Agent for Enhancing Communication Efficiency Using  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Minxi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+D">Dahua Gao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wenlong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+G">Guangming Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages,11 figures, submitted to IJCAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Current communication technologies face limitations in terms of theoretical
capacity, spectrum availability, and power resources. Pragmatic communication,
leveraging terminal intelligence for selective data transmission, offers
resource conservation. Existing research lacks universal intention resolution
tools, limiting applicability to specific tasks. This paper proposes an image
pragmatic communication framework based on a Pragmatic Agent for Communication
Efficiency (PACE) using Large Language Models (LLM). In this framework, PACE
sequentially performs semantic perception, intention resolution, and
intention-oriented coding. To ensure the effective utilization of LLM in
communication, a knowledge base is designed to supplement the necessary
knowledge, dedicated prompts are introduced to facilitate understanding of
pragmatic communication scenarios and task requirements, and a chain of thought
is designed to assist in making reasonable trade-offs between transmission
efficiency and cost. For experimental validation, this paper constructs an
image pragmatic communication dataset along with corresponding evaluation
standards. Simulation results indicate that the proposed method outperforms
traditional and non-LLM-based pragmatic communication in terms of transmission
efficiency.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01751" title="Abstract">arXiv:2402.01751</a> [<a href="/pdf/2402.01751" title="Download PDF">pdf</a>, <a href="/ps/2402.01751" title="Download PostScript">ps</a>, <a href="/format/2402.01751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Assessment of ChatGPT vs Bard in Detecting Alzheimer&#x27;s  Dementia
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=T%2C+B+B">Balamurali B T</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jer-Ming Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) find increasing applications in many fields.
Here, three LLM chatbots (ChatGPT-3.5, ChatGPT-4 and Bard) are assessed - in
their current form, as publicly available - for their ability to recognize
Alzheimer's Dementia (AD) and Cognitively Normal (CN) individuals using textual
input derived from spontaneous speech recordings. Zero-shot learning approach
is used at two levels of independent queries, with the second query
(chain-of-thought prompting) eliciting more detailed than the first. Each LLM
chatbot's performance is evaluated on the prediction generated in terms of
accuracy, sensitivity, specificity, precision and F1 score. LLM chatbots
generated three-class outcome ("AD", "CN", or "Unsure"). When positively
identifying AD, Bard produced highest true-positives (89% recall) and highest
F1 score (71%), but tended to misidentify CN as AD, with high confidence (low
"Unsure" rates); for positively identifying CN, GPT-4 resulted in the highest
true-negatives at 56% and highest F1 score (62%), adopting a diplomatic stance
(moderate "Unsure" rates). Overall, three LLM chatbots identify AD vs CN
surpassing chance-levels but do not currently satisfy clinical application.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01753" title="Abstract">arXiv:2402.01753</a> [<a href="/pdf/2402.01753" title="Download PDF">pdf</a>, <a href="/format/2402.01753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpecDiff-GAN: A Spectrally-Shaped Noise Diffusion GAN for Speech and  Music Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baoueb%2C+T">Teysir Baoueb</a> (IP Paris, LTCI, IDS, S2A), 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haocheng Liu</a> (IP Paris, LTCI, IDS, S2A), 
<a href="/search/cs?searchtype=author&query=Fontaine%2C+M">Mathieu Fontaine</a> (IP Paris, LTCI, IDS, S2A), 
<a href="/search/cs?searchtype=author&query=Roux%2C+J+L">Jonathan Le Roux</a> (MERL), 
<a href="/search/cs?searchtype=author&query=Richard%2C+G">Gael Richard</a> (IP Paris, LTCI, IDS, S2A)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICASSP 2024
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE International Conference on Acoustics, Speech and Signal
  Processing, Apr 2024, Seoul (Korea), South Korea
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS); Signal Processing (eess.SP)

</div>
<p class="mathjax">Generative adversarial network (GAN) models can synthesize highquality audio
signals while ensuring fast sample generation. However, they are difficult to
train and are prone to several issues including mode collapse and divergence.
In this paper, we introduce SpecDiff-GAN, a neural vocoder based on HiFi-GAN,
which was initially devised for speech synthesis from mel spectrogram. In our
model, the training stability is enhanced by means of a forward diffusion
process which consists in injecting noise from a Gaussian distribution to both
real and fake samples before inputting them to the discriminator. We further
improve the model by exploiting a spectrally-shaped noise distribution with the
aim to make the discriminator's task more challenging. We then show the merits
of our proposed model for speech and music synthesis on several datasets. Our
experiments confirm that our model compares favorably in audio quality and
efficiency compared to several baselines.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01758" title="Abstract">arXiv:2402.01758</a> [<a href="/pdf/2402.01758" title="Download PDF">pdf</a>, <a href="/format/2402.01758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aalap: AI Assistant for Legal &amp; Paralegal Functions in India
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+A">Aman Tiwari</a>, 
<a href="/search/cs?searchtype=author&query=Kalamkar%2C+P">Prathamesh Kalamkar</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+A">Atreyo Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Karn%2C+S">Saurabh Karn</a>, 
<a href="/search/cs?searchtype=author&query=Hemachandran%2C+V">Varun Hemachandran</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Smita Gupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Using proprietary Large Language Models on legal tasks poses challenges due
to data privacy issues, domain data heterogeneity, domain knowledge
sophistication, and domain objectives uniqueness. We created Aalalp, a
fine-tuned Mistral 7B model on instructions data related to specific Indian
legal tasks. The performance of Aalap is better than gpt-3.5-turbo in 31\% of
our test data and obtains an equivalent score in 34\% of the test data as
evaluated by GPT4. Training Aalap mainly focuses on teaching legal reasoning
rather than legal recall. Aalap is definitely helpful for the day-to-day
activities of lawyers, judges, or anyone working in legal systems.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01759" title="Abstract">arXiv:2402.01759</a> [<a href="/pdf/2402.01759" title="Download PDF">pdf</a>, <a href="/format/2402.01759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Systematic Literature Review: Computational Approaches for Humour Style  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kenneth%2C+M+O">Mary Ogbuka Kenneth</a>, 
<a href="/search/cs?searchtype=author&query=Khosmood%2C+F">Foaad Khosmood</a>, 
<a href="/search/cs?searchtype=author&query=Edalat%2C+A">Abbas Edalat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Understanding various humour styles is essential for comprehending the
multifaceted nature of humour and its impact on fields such as psychology and
artificial intelligence. This understanding has revealed that humour, depending
on the style employed, can either have therapeutic or detrimental effects on an
individual's health and relationships. Although studies dedicated exclusively
to computational-based humour style analysis remain somewhat rare, an expansive
body of research thrives within related task, particularly binary humour and
sarcasm recognition. In this systematic literature review (SLR), we survey the
landscape of computational techniques applied to these related tasks and also
uncover their fundamental relevance to humour style analysis. Through this
study, we unveil common approaches, illuminate various datasets and evaluation
metrics, and effectively navigate the complex terrain of humour research. Our
efforts determine potential research gaps and outlined promising directions.
Furthermore, the SLR identifies a range of features and computational models
that can seamlessly transition from related tasks like binary humour and
sarcasm detection to invigorate humour style classification. These features
encompass incongruity, sentiment and polarity analysis, ambiguity detection,
acoustic nuances, visual cues, contextual insights, and more. The computational
models that emerge contain traditional machine learning paradigms, neural
network architectures, transformer-based models, and specialised models attuned
to the nuances of humour. Finally, the SLR provides access to existing datasets
related to humour and sarcasm, facilitating the work of future researchers.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01760" title="Abstract">arXiv:2402.01760</a> [<a href="/pdf/2402.01760" title="Download PDF">pdf</a>, <a href="/format/2402.01760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trust and ethical considerations in a multi-modal, explainable AI-driven  chatbot tutoring system: The case of collaboratively solving Rubik&#x27;s Cube
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lakkaraju%2C+K">Kausik Lakkaraju</a>, 
<a href="/search/cs?searchtype=author&query=Khandelwal%2C+V">Vedant Khandelwal</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+B">Biplav Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Agostinelli%2C+F">Forest Agostinelli</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Hengtao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+P">Prathamjeet Singh</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Dezhi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Irvin%2C+M">Matt Irvin</a>, 
<a href="/search/cs?searchtype=author&query=Kundu%2C+A">Ashish Kundu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at 'Neural Conversational AI Workshop - What's left to TEACH (Trustworthy, Enhanced, Adaptable, Capable, and Human-centric) chatbots?' at ICML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Artificial intelligence (AI) has the potential to transform education with
its power of uncovering insights from massive data about student learning
patterns. However, ethical and trustworthy concerns of AI have been raised but
are unsolved. Prominent ethical issues in high school AI education include data
privacy, information leakage, abusive language, and fairness. This paper
describes technological components that were built to address ethical and
trustworthy concerns in a multi-modal collaborative platform (called ALLURE
chatbot) for high school students to collaborate with AI to solve the Rubik's
cube. In data privacy, we want to ensure that the informed consent of children,
parents, and teachers, is at the center of any data that is managed. Since
children are involved, language, whether textual, audio, or visual, is
acceptable both from users and AI and the system can steer interaction away
from dangerous situations. In information management, we also want to ensure
that the system, while learning to improve over time, does not leak information
about users from one group to another.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01761" title="Abstract">arXiv:2402.01761</a> [<a href="/pdf/2402.01761" title="Download PDF">pdf</a>, <a href="/format/2402.01761" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Interpretability in the Era of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+C">Chandan Singh</a>, 
<a href="/search/cs?searchtype=author&query=Inala%2C+J+P">Jeevana Priya Inala</a>, 
<a href="/search/cs?searchtype=author&query=Galley%2C+M">Michel Galley</a>, 
<a href="/search/cs?searchtype=author&query=Caruana%2C+R">Rich Caruana</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Interpretable machine learning has exploded as an area of interest over the
last decade, sparked by the rise of increasingly large datasets and deep neural
networks. Simultaneously, large language models (LLMs) have demonstrated
remarkable capabilities across a wide array of tasks, offering a chance to
rethink opportunities in interpretable machine learning. Notably, the
capability to explain in natural language allows LLMs to expand the scale and
complexity of patterns that can be given to a human. However, these new
capabilities raise new challenges, such as hallucinated explanations and
immense computational costs.
<br />In this position paper, we start by reviewing existing methods to evaluate
the emerging field of LLM interpretation (both interpreting LLMs and using LLMs
for explanation). We contend that, despite their limitations, LLMs hold the
opportunity to redefine interpretability with a more ambitious scope across
many applications, including in auditing LLMs themselves. We highlight two
emerging research priorities for LLM interpretation: using LLMs to directly
analyze new datasets and to generate interactive explanations.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01762" title="Abstract">arXiv:2402.01762</a> [<a href="/pdf/2402.01762" title="Download PDF">pdf</a>, <a href="/ps/2402.01762" title="Download PostScript">ps</a>, <a href="/format/2402.01762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Commercial AI, Conflict, and Moral Responsibility: A theoretical  analysis and practical approach to the moral responsibilities associated with  dual-use AI technology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Trusilo%2C+D">Daniel Trusilo</a>, 
<a href="/search/cs?searchtype=author&query=Danks%2C+D">David Danks</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper presents a theoretical analysis and practical approach to the
moral responsibilities when developing AI systems for non-military applications
that may nonetheless be used for conflict applications. We argue that AI
represents a form of crossover technology that is different from previous
historical examples of dual- or multi-use technology as it has a multiplicative
effect across other technologies. As a result, existing analyses of ethical
responsibilities around dual-use technologies do not necessarily work for AI
systems. We instead argue that stakeholders involved in the AI system lifecycle
are morally responsible for uses of their systems that are reasonably
foreseeable. The core idea is that an agent's moral responsibility for some
action is not necessarily determined by their intentions alone; we must also
consider what the agent could reasonably have foreseen to be potential outcomes
of their action, such as the potential use of a system in conflict even when it
is not designed for that. In particular, we contend that it is reasonably
foreseeable that: (1) civilian AI systems will be applied to active conflict,
including conflict support activities, (2) the use of civilian AI systems in
conflict will impact applications of the law of armed conflict, and (3)
crossover AI technology will be applied to conflicts that fall short of armed
conflict. Given these reasonably foreseeably outcomes, we present three
technically feasible actions that developers of civilian AIs can take to
potentially mitigate their moral responsibility: (a) establishing systematic
approaches to multi-perspective capability testing, (b) integrating digital
watermarking in model weight matrices, and (c) utilizing monitoring and
reporting mechanisms for conflict-related AI applications.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01763" title="Abstract">arXiv:2402.01763</a> [<a href="/pdf/2402.01763" title="Download PDF">pdf</a>, <a href="/format/2402.01763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Large Language Models Meet Vector Databases: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jing%2C+Z">Zhi Jing</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yongye Su</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yikun Han</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+B">Bo Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chunjiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haiyun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kehai Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">The recent burst in Large Language Models has opened new frontiers in
human-like text processing and generation. However, alongside their remarkable
growth, Large Language Models have encountered critical challenges including
issues of hallucination, bias, real-time knowledge updates, and the high costs
of implementation and maintenance in commercial settings. Vector Databases,
another increasingly popular tool, offer potential solutions to these
challenges. These databases are adept at handling high-dimensional data and are
crucial for tasks such as efficient information retrieval and semantic search.
By integrating with Large Language Models, they significantly enhance AI
systems' ability to manage and utilize diverse data more effectively. This
survey paper provides an in-depth and unique analysis of the intersection
between Large Language Models and Vector Databases.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01764" title="Abstract">arXiv:2402.01764</a> [<a href="/pdf/2402.01764" title="Download PDF">pdf</a>, <a href="/ps/2402.01764" title="Download PostScript">ps</a>, <a href="/format/2402.01764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XP2021 Experience Report: Five Strategies for the Future of Work:  Accelerating Innovation through Tech Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fraser%2C+S">Steven Fraser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">This experience report outlines five tech transfer strategies developed over
a period of 25 years at four Global 1000 companies (HP, Cisco, Qualcomm, and
Nortel) to mitigate R&amp;D challenges associated with duplicated effort, product
quality, and time-to-market. The five strategies accelerate innovation through
open knowledge sharing, rather than licensing intellectual property rights
(IPR) such as patents, trade secrets, and copyrights. The strategies are based
on corporate tech forums, conference panels, exploratory workshops, research
reviews (at universities and companies), and talent exchanges. While the
initial objective was to foster the corporate adoption of software best
practices, over time the strategies had broader impact on company innovation,
including incubating cross-company R&amp;D collaborations, capturing organizational
memory, cultivating and leveraging external research partnerships, and feeding
company talent pipelines.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01765" title="Abstract">arXiv:2402.01765</a> [<a href="/pdf/2402.01765" title="Download PDF">pdf</a>, <a href="/format/2402.01765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMs Simulate Big Five Personality Traits: Further Evidence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sorokovikova%2C+A">Aleksandra Sorokovikova</a>, 
<a href="/search/cs?searchtype=author&query=Fedorova%2C+N">Natalia Fedorova</a>, 
<a href="/search/cs?searchtype=author&query=Rezagholi%2C+S">Sharwin Rezagholi</a>, 
<a href="/search/cs?searchtype=author&query=Yamshchikov%2C+I+P">Ivan P. Yamshchikov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">An empirical investigation into the simulation of the Big Five personality
traits by large language models (LLMs), namely Llama2, GPT4, and Mixtral, is
presented. We analyze the personality traits simulated by these models and
their stability. This contributes to the broader understanding of the
capabilities of LLMs to simulate personality traits and the respective
implications for personalized human-computer interaction.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01766" title="Abstract">arXiv:2402.01766</a> [<a href="/pdf/2402.01766" title="Download PDF">pdf</a>, <a href="/format/2402.01766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM Voting: Human Choices and AI Collective Decision Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J+C">Joshua C. Yang</a>, 
<a href="/search/cs?searchtype=author&query=Korecki%2C+M">Marcin Korecki</a>, 
<a href="/search/cs?searchtype=author&query=Dailisan%2C+D">Damian Dailisan</a>, 
<a href="/search/cs?searchtype=author&query=Hausladen%2C+C+I">Carina I. Hausladen</a>, 
<a href="/search/cs?searchtype=author&query=Helbing%2C+D">Dirk Helbing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICML2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG); General Economics (econ.GN)

</div>
<p class="mathjax">This paper investigates the voting behaviors of Large Language Models (LLMs),
particularly OpenAI's GPT4 and LLaMA2, and their alignment with human voting
patterns. Our approach included a human voting experiment to establish a
baseline for human preferences and a parallel experiment with LLM agents. The
study focused on both collective outcomes and individual preferences, revealing
differences in decision-making and inherent biases between humans and LLMs. We
observed a trade-off between preference diversity and alignment in LLMs, with a
tendency towards more uniform choices as compared to the diverse preferences of
human voters. This finding indicates that LLMs could lead to more homogenized
collective outcomes when used in voting assistance, underscoring the need for
cautious integration of LLMs into democratic processes.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01767" title="Abstract">arXiv:2402.01767</a> [<a href="/pdf/2402.01767" title="Download PDF">pdf</a>, <a href="/format/2402.01767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HiQA: A Hierarchical Contextual Augmentation RAG for Massive Documents  QA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyue Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+P">Pengyu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jiangjiang Song</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+X">Xiaoyang Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">As language model agents leveraging external tools rapidly evolve,
significant progress has been made in question-answering(QA) methodologies
utilizing supplementary documents and the Retrieval-Augmented Generation (RAG)
approach. This advancement has improved the response quality of language models
and alleviates the appearance of hallucination. However, these methods exhibit
limited retrieval accuracy when faced with massive indistinguishable documents,
presenting notable challenges in their practical application. In response to
these emerging challenges, we present HiQA, an advanced framework for
multi-document question-answering (MDQA) that integrates cascading metadata
into content as well as a multi-route retrieval mechanism. We also release a
benchmark called MasQA to evaluate and research in MDQA. Finally, HiQA
demonstrates the state-of-the-art performance in multi-document environments.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01768" title="Abstract">arXiv:2402.01768</a> [<a href="/pdf/2402.01768" title="Download PDF">pdf</a>, <a href="/ps/2402.01768" title="Download PostScript">ps</a>, <a href="/format/2402.01768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enriched Physics-informed Neural Networks for Dynamic  Poisson-Nernst-Planck Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xujia Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fajie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Benrong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hanqing Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 16 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">This paper proposes a meshless deep learning algorithm, enriched
physics-informed neural networks (EPINNs), to solve dynamic
Poisson-Nernst-Planck (PNP) equations with strong coupling and nonlinear
characteristics. The EPINNs takes the traditional physics-informed neural
networks as the foundation framework, and adds the adaptive loss weight to
balance the loss functions, which automatically assigns the weights of losses
by updating the parameters in each iteration based on the maximum likelihood
estimate. The resampling strategy is employed in the EPINNs to accelerate the
convergence of loss function. Meanwhile, the GPU parallel computing technique
is adopted to accelerate the solving process. Four examples are provided to
demonstrate the validity and effectiveness of the proposed method. Numerical
results indicate that the new method has better applicability than traditional
numerical methods in solving such coupled nonlinear systems. More importantly,
the EPINNs is more accurate, stable, and fast than the traditional
physics-informed neural networks. This work provides a simple and
high-performance numerical tool for addressing PNPs with arbitrary boundary
shapes and boundary conditions.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01769" title="Abstract">arXiv:2402.01769</a> [<a href="/pdf/2402.01769" title="Download PDF">pdf</a>, <a href="/format/2402.01769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Redefining &quot;Hallucination&quot; in LLMs: Towards a psychology-informed  framework for mitigating misinformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berberette%2C+E">Elijah Berberette</a>, 
<a href="/search/cs?searchtype=author&query=Hutchins%2C+J">Jack Hutchins</a>, 
<a href="/search/cs?searchtype=author&query=Sadovnik%2C+A">Amir Sadovnik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In recent years, large language models (LLMs) have become incredibly popular,
with ChatGPT for example being used by over a billion users. While these models
exhibit remarkable language understanding and logical prowess, a notable
challenge surfaces in the form of "hallucinations." This phenomenon results in
LLMs outputting misinformation in a confident manner, which can lead to
devastating consequences with such a large user base. However, we question the
appropriateness of the term "hallucination" in LLMs, proposing a psychological
taxonomy based on cognitive biases and other psychological phenomena. Our
approach offers a more fine-grained understanding of this phenomenon, allowing
for targeted solutions. By leveraging insights from how humans internally
resolve similar challenges, we aim to develop strategies to mitigate LLM
hallucinations. This interdisciplinary approach seeks to move beyond
conventional terminology, providing a nuanced understanding and actionable
pathways for improvement in LLM reliability.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01770" title="Abstract">arXiv:2402.01770</a> [<a href="/pdf/2402.01770" title="Download PDF">pdf</a>, <a href="/format/2402.01770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extending Interactive Science Exhibits into the Classroom using  Anthropomorphized Chatbots and Bloom&#x27;s Taxonomy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Golding%2C+Y">Yousuf Golding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study explores the use of Generative AI chatbots for transforming public
science exhibits into virtual experiences that can extend the engagement of
exhibits into the classroom. The broader goal is to increase accessibility of
science exhibits, especially for those marginalized in STEM due to various
factors, including cultural barriers. We hypothesize that turning exhibits into
first-person anthropomorphized chatbots with a personality, like quirky-talking
asteroids or comets, can increase engagement and learning. The paper mainly
explores if such techniques are possible using Generative AI (e.g. GPT) via
prompt engineering alone. The research includes an investigation into the
possibility of integrating interactive assessment via question-generation using
Bloom's Taxonomy. Initial results indicate that it is possible to combine these
techniques. As such, it lays a foundation for future classroom evaluations of
such chatbots to gauge their overall efficacy in extending the reach of science
exhibitions. The paper concludes by discussing extensions of the research to
fully evaluate effectiveness in virtual field-trips. We also include a brief
examination of additional ways to enhance student motivation towards learning
via chatbots.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01771" title="Abstract">arXiv:2402.01771</a> [<a href="/pdf/2402.01771" title="Download PDF">pdf</a>, <a href="/format/2402.01771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BlackMamba: Mixture of Experts for State-Space Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anthony%2C+Q">Quentin Anthony</a>, 
<a href="/search/cs?searchtype=author&query=Tokpanov%2C+Y">Yury Tokpanov</a>, 
<a href="/search/cs?searchtype=author&query=Glorioso%2C+P">Paolo Glorioso</a>, 
<a href="/search/cs?searchtype=author&query=Millidge%2C+B">Beren Millidge</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)

</div>
<p class="mathjax">State-space models (SSMs) have recently demonstrated competitive performance
to transformers at large-scale language modeling benchmarks while achieving
linear time and memory complexity as a function of sequence length. Mamba, a
recently released SSM model, shows impressive performance in both language
modeling and long sequence processing tasks. Simultaneously, mixture-of-expert
(MoE) models have shown remarkable performance while significantly reducing the
compute and latency costs of inference at the expense of a larger memory
footprint. In this paper, we present BlackMamba, a novel architecture that
combines the Mamba SSM with MoE to obtain the benefits of both. We demonstrate
that BlackMamba performs competitively against both Mamba and transformer
baselines, and outperforms in inference and training FLOPs. We fully train and
open-source 340M/1.5B and 630M/2.8B BlackMamba models on 300B tokens of a
custom dataset. We show that BlackMamba inherits and combines both of the
benefits of SSM and MoE architectures, combining linear-complexity generation
from SSM with cheap and fast inference from MoE. We release all weights,
checkpoints, and inference code open-source. Inference code at:
https://github.com/Zyphra/BlackMamba
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01772" title="Abstract">arXiv:2402.01772</a> [<a href="/pdf/2402.01772" title="Download PDF">pdf</a>, <a href="/format/2402.01772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disentangling the Roles of Target-Side Transfer and Regularization in  Multilingual Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meng%2C+Y">Yan Meng</a>, 
<a href="/search/cs?searchtype=author&query=Monz%2C+C">Christof Monz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Multilingual Machine Translation (MMT) benefits from knowledge transfer
across different language pairs. However, improvements in one-to-many
translation compared to many-to-one translation are only marginal and sometimes
even negligible. This performance discrepancy raises the question of to what
extent positive transfer plays a role on the target-side for one-to-many MT. In
this paper, we conduct a large-scale study that varies the auxiliary target
side languages along two dimensions, i.e., linguistic similarity and corpus
size, to show the dynamic impact of knowledge transfer on the main language
pairs. We show that linguistically similar auxiliary target languages exhibit
strong ability to transfer positive knowledge. With an increasing size of
similar target languages, the positive transfer is further enhanced to benefit
the main language pairs. Meanwhile, we find distant auxiliary target languages
can also unexpectedly benefit main language pairs, even with minimal positive
transfer ability. Apart from transfer, we show distant auxiliary target
languages can act as a regularizer to benefit translation performance by
enhancing the generalization and model inference calibration.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01773" title="Abstract">arXiv:2402.01773</a> [<a href="/pdf/2402.01773" title="Download PDF">pdf</a>, <a href="/ps/2402.01773" title="Download PostScript">ps</a>, <a href="/format/2402.01773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Creating a Synthesizer from Schr&#xf6;dinger&#x27;s Equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Freye%2C+A">Arthur Freye</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+J">Jannis M&#xfc;ller</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 28th International Conference on Auditory
  Display (ICAD 2023), 2023, pp. 179-182
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS); Quantum Physics (quant-ph)

</div>
<p class="mathjax">Our project offers an alternative approach to the sensory perception of the
Schr\"odinger equation (an elementary model of quantum phenomena) by
interpreting it as a sound wave. We are building a synthesizer plugin that
simulates a quantum mechanical state that evolves over time. Thus, our tool
allows the creation of unique sounds that are in motion and feel alive. These
can be used in professional music production without any knowledge of physics,
while at the same time providing insight into a chapter of quantum mechanics.
The goal is to lower the threshold for entering complex theory by first
developing an intuition for the subject; but the tool can also be used purely
as a musical instrument. The user is encouraged, but not forced, to learn more
about the underlying physics. Simulation parameters are adjustable in
real-time, allowing intuitive experimentation. Despite the approximate
calculations, real physical effects such as quantum tunneling can be observed
acoustically and visually.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01775" title="Abstract">arXiv:2402.01775</a> [<a href="/pdf/2402.01775" title="Download PDF">pdf</a>, <a href="/format/2402.01775" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design and consensus content validity of the questionnaire for  b-learning education: A 2-Tuple Fuzzy Linguistic Delphi based Decision  Support Tool
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Montes%2C+R">Rosana Montes</a>, 
<a href="/search/cs?searchtype=author&query=Zuheros%2C+C">Cristina Zuheros</a>, 
<a href="/search/cs?searchtype=author&query=Morales%2C+J+M">Jeovani M. Morales</a>, 
<a href="/search/cs?searchtype=author&query=Zerme%C3%B1o%2C+N">Noe Zerme&#xf1;o</a>, 
<a href="/search/cs?searchtype=author&query=Duran%2C+J">Jer&#xf3;nimo Duran</a>, 
<a href="/search/cs?searchtype=author&query=Herrera%2C+F">Francsico Herrera</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 47 pages, 7 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Open Access Volume 147 November 2023 Article number 110755
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Classic Delphi and Fuzzy Delphi methods are used to test content validity of
data collection tools such as questionnaires. Fuzzy Delphi takes the opinion
issued by judges from a linguistic perspective reducing ambiguity in opinions
by using fuzzy numbers. We propose an extension named 2-Tuple Fuzzy Linguistic
Delphi method to deal with scenarios in which judges show different expertise
degrees by using fuzzy multigranular semantics of the linguistic terms and to
obtain intermediate and final results expressed by 2-tuple linguistic values.
The key idea of our proposal is to validate the full questionnaire by means of
the evaluation of its parts, defining the validity of each item as a Decision
Making problem. Taking the opinion of experts, we measure the degree of
consensus, the degree of consistency, and the linguistic score of each item, in
order to detect those items that affect, positively or negatively, the quality
of the instrument. Considering the real need to evaluate a b-learning
educational experience with a consensual questionnaire, we present a Decision
Making model for questionnaire validation that solves it. Additionally, we
contribute to this consensus reaching problem by developing an online tool
under GPL v3 license. The software visualizes the collective valuations for
each iteration and assists to determine which parts of the questionnaire should
be modified to reach a consensual solution.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01777" title="Abstract">arXiv:2402.01777</a> [<a href="/pdf/2402.01777" title="Download PDF">pdf</a>, <a href="/ps/2402.01777" title="Download PostScript">ps</a>, <a href="/format/2402.01777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Psychology of GPT-4: Moderately anxious, slightly masculine,  honest, and humble
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barua%2C+A">Adrita Barua</a>, 
<a href="/search/cs?searchtype=author&query=Brase%2C+G">Gary Brase</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+K">Ke Dong</a>, 
<a href="/search/cs?searchtype=author&query=Hitzler%2C+P">Pascal Hitzler</a>, 
<a href="/search/cs?searchtype=author&query=Vasserman%2C+E">Eugene Vasserman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 8 tables, 1 code repository
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">We subject GPT-4 to a number of rigorous psychometric tests and analyze the
results. We find that, compared to the average human, GPT-4 tends to show more
honesty and humility, and less machiavellianism and narcissism. It sometimes
exhibits ambivalent sexism, leans slightly toward masculinity, is moderately
anxious but mostly not depressive (but not always). It shows human-average
numerical literacy and has cognitive reflection abilities that are above human
average for verbal tasks.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01781" title="Abstract">arXiv:2402.01781</a> [<a href="/pdf/2402.01781" title="Download PDF">pdf</a>, <a href="/format/2402.01781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Benchmarks are Targets: Revealing the Sensitivity of Large Language  Model Leaderboards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alzahrani%2C+N">Norah Alzahrani</a>, 
<a href="/search/cs?searchtype=author&query=Alyahya%2C+H+A">Hisham Abdullah Alyahya</a>, 
<a href="/search/cs?searchtype=author&query=Alnumay%2C+Y">Yazeed Alnumay</a>, 
<a href="/search/cs?searchtype=author&query=Alrashed%2C+S">Sultan Alrashed</a>, 
<a href="/search/cs?searchtype=author&query=Alsubaie%2C+S">Shaykhah Alsubaie</a>, 
<a href="/search/cs?searchtype=author&query=Almushaykeh%2C+Y">Yusef Almushaykeh</a>, 
<a href="/search/cs?searchtype=author&query=Mirza%2C+F">Faisal Mirza</a>, 
<a href="/search/cs?searchtype=author&query=Alotaibi%2C+N">Nouf Alotaibi</a>, 
<a href="/search/cs?searchtype=author&query=Altwairesh%2C+N">Nora Altwairesh</a>, 
<a href="/search/cs?searchtype=author&query=Alowisheq%2C+A">Areeb Alowisheq</a>, 
<a href="/search/cs?searchtype=author&query=Bari%2C+M+S">M Saiful Bari</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+H">Haidar Khan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Model (LLM) leaderboards based on benchmark rankings are
regularly used to guide practitioners in model selection. Often, the published
leaderboard rankings are taken at face value - we show this is a (potentially
costly) mistake. Under existing leaderboards, the relative performance of LLMs
is highly sensitive to (often minute) details. We show that for popular
multiple choice question benchmarks (e.g. MMLU) minor perturbations to the
benchmark, such as changing the order of choices or the method of answer
selection, result in changes in rankings up to 8 positions. We explain this
phenomenon by conducting systematic experiments over three broad categories of
benchmark perturbations and identifying the sources of this behavior. Our
analysis results in several best-practice recommendations, including the
advantage of a hybrid scoring method for answer selection. Our study highlights
the dangers of relying on simple benchmark evaluations and charts the path for
more robust evaluation schemes on the existing benchmarks.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01782" title="Abstract">arXiv:2402.01782</a> [<a href="/pdf/2402.01782" title="Download PDF">pdf</a>, <a href="/format/2402.01782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking Spiking Neural Network Learning Methods with Varying  Locality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jiaqi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Sen Lu</a>, 
<a href="/search/cs?searchtype=author&query=Bal%2C+M">Malyaban Bal</a>, 
<a href="/search/cs?searchtype=author&query=Sengupta%2C+A">Abhronil Sengupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Spiking Neural Networks (SNNs), providing more realistic neuronal dynamics,
have shown to achieve performance comparable to Artificial Neural Networks
(ANNs) in several machine learning tasks. Information is processed as spikes
within SNNs in an event-based mechanism that significantly reduces energy
consumption. However, training SNNs is challenging due to the
non-differentiable nature of the spiking mechanism. Traditional approaches,
such as Backpropagation Through Time (BPTT), have shown effectiveness but comes
with additional computational and memory costs and are biologically
implausible. In contrast, recent works propose alternative learning methods
with varying degrees of locality, demonstrating success in classification
tasks. In this work, we show that these methods share similarities during the
training process, while they present a trade-off between biological
plausibility and performance. Further, this research examines the implicitly
recurrent nature of SNNs and investigates the influence of addition of explicit
recurrence to SNNs. We experimentally prove that the addition of explicit
recurrent weights enhances the robustness of SNNs. We also investigate the
performance of local learning methods under gradient and non-gradient based
adversarial attacks.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01783" title="Abstract">arXiv:2402.01783</a> [<a href="/pdf/2402.01783" title="Download PDF">pdf</a>, <a href="/format/2402.01783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Multi-Label Classification of Online Vaccine Concerns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C+Q">Chloe Qinyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Stureborg%2C+R">Rickard Stureborg</a>, 
<a href="/search/cs?searchtype=author&query=Dhingra%2C+B">Bhuwan Dhingra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in AAAI 2024 Health Intelligence workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Vaccine concerns are an ever-evolving target, and can shift quickly as seen
during the COVID-19 pandemic. Identifying longitudinal trends in vaccine
concerns and misinformation might inform the healthcare space by helping public
health efforts strategically allocate resources or information campaigns. We
explore the task of detecting vaccine concerns in online discourse using large
language models (LLMs) in a zero-shot setting without the need for expensive
training datasets. Since real-time monitoring of online sources requires
large-scale inference, we explore cost-accuracy trade-offs of different
prompting strategies and offer concrete takeaways that may inform choices in
system designs for current applications. An analysis of different prompting
strategies reveals that classifying the concerns over multiple passes through
the LLM, each consisting a boolean question whether the text mentions a vaccine
concern or not, works the best. Our results indicate that GPT-4 can strongly
outperform crowdworker accuracy when compared to ground truth annotations
provided by experts on the recently introduced VaxConcerns dataset, achieving
an overall F1 score of 78.7%.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01785" title="Abstract">arXiv:2402.01785</a> [<a href="/pdf/2402.01785" title="Download PDF">pdf</a>, <a href="/format/2402.01785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DoubleMLDeep: Estimation of Causal Effects with Multimodal Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Klaassen%2C+S">Sven Klaassen</a>, 
<a href="/search/cs?searchtype=author&query=Teichert-Kluge%2C+J">Jan Teichert-Kluge</a>, 
<a href="/search/cs?searchtype=author&query=Bach%2C+P">Philipp Bach</a>, 
<a href="/search/cs?searchtype=author&query=Chernozhukov%2C+V">Victor Chernozhukov</a>, 
<a href="/search/cs?searchtype=author&query=Spindler%2C+M">Martin Spindler</a>, 
<a href="/search/cs?searchtype=author&query=Vijaykumar%2C+S">Suhas Vijaykumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Econometrics (econ.EM); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">This paper explores the use of unstructured, multimodal data, namely text and
images, in causal inference and treatment effect estimation. We propose a
neural network architecture that is adapted to the double machine learning
(DML) framework, specifically the partially linear model. An additional
contribution of our paper is a new method to generate a semi-synthetic dataset
which can be used to evaluate the performance of causal effect estimation in
the presence of text and images as confounders. The proposed methods and
architectures are evaluated on the semi-synthetic dataset and compared to
standard approaches, highlighting the potential benefit of using text and
images directly in causal studies. Our findings have implications for
researchers and practitioners in economics, marketing, finance, medicine and
data science in general who are interested in estimating causal quantities
using non-traditional data.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01786" title="Abstract">arXiv:2402.01786</a> [<a href="/pdf/2402.01786" title="Download PDF">pdf</a>, <a href="/format/2402.01786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COA-GPT: Generative Pre-trained Transformers for Accelerated Course of  Action Development in Military Operations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goecks%2C+V+G">Vinicius G. Goecks</a>, 
<a href="/search/cs?searchtype=author&query=Waytowich%2C+N">Nicholas Waytowich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the NATO Science and Technology Organization Symposium (ICMCIS) organized by the Information Systems Technology (IST) Panel, IST-205-RSY - the ICMCIS, held in Koblenz, Germany, 23-24 April 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">The development of Courses of Action (COAs) in military operations is
traditionally a time-consuming and intricate process. Addressing this
challenge, this study introduces COA-GPT, a novel algorithm employing Large
Language Models (LLMs) for rapid and efficient generation of valid COAs.
COA-GPT incorporates military doctrine and domain expertise to LLMs through
in-context learning, allowing commanders to input mission information - in both
text and image formats - and receive strategically aligned COAs for review and
approval. Uniquely, COA-GPT not only accelerates COA development, producing
initial COAs within seconds, but also facilitates real-time refinement based on
commander feedback. This work evaluates COA-GPT in a military-relevant scenario
within a militarized version of the StarCraft II game, comparing its
performance against state-of-the-art reinforcement learning algorithms. Our
results demonstrate COA-GPT's superiority in generating strategically sound
COAs more swiftly, with added benefits of enhanced adaptability and alignment
with commander intentions. COA-GPT's capability to rapidly adapt and update
COAs during missions presents a transformative potential for military planning,
particularly in addressing planning discrepancies and capitalizing on emergent
windows of opportunities.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01787" title="Abstract">arXiv:2402.01787</a> [<a href="/pdf/2402.01787" title="Download PDF">pdf</a>, <a href="/format/2402.01787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harm Amplification in Text-to-Image Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+S">Susan Hao</a>, 
<a href="/search/cs?searchtype=author&query=Shelby%2C+R">Renee Shelby</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuchi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+H">Hansa Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Bhutani%2C+M">Mukul Bhutani</a>, 
<a href="/search/cs?searchtype=author&query=Ayan%2C+B+K">Burcu Karagol Ayan</a>, 
<a href="/search/cs?searchtype=author&query=Poddar%2C+S">Shivani Poddar</a>, 
<a href="/search/cs?searchtype=author&query=Laszlo%2C+S">Sarah Laszlo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Text-to-image (T2I) models have emerged as a significant advancement in
generative AI; however, there exist safety concerns regarding their potential
to produce harmful image outputs even when users input seemingly safe prompts.
This phenomenon, where T2I models generate harmful representations that were
not explicit in the input, poses a potentially greater risk than adversarial
prompts, leaving users unintentionally exposed to harms. Our paper addresses
this issue by first introducing a formal definition for this phenomenon, termed
harm amplification. We further contribute to the field by developing
methodologies to quantify harm amplification in which we consider the harm of
the model output in the context of user input. We then empirically examine how
to apply these different methodologies to simulate real-world deployment
scenarios including a quantification of disparate impacts across genders
resulting from harm amplification. Together, our work aims to offer researchers
tools to comprehensively address safety challenges in T2I systems and
contribute to the responsible deployment of generative AI models.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01788" title="Abstract">arXiv:2402.01788</a> [<a href="/pdf/2402.01788" title="Download PDF">pdf</a>, <a href="/format/2402.01788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LitLLM: A Toolkit for Scientific Literature Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+S">Shubham Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Laradji%2C+I+H">Issam H. Laradji</a>, 
<a href="/search/cs?searchtype=author&query=Charlin%2C+L">Laurent Charlin</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+C">Christopher Pal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Conducting literature reviews for scientific papers is essential for
understanding research, its limitations, and building on existing work. It is a
tedious task which makes an automatic literature review generator appealing.
Unfortunately, many existing works that generate such reviews using Large
Language Models (LLMs) have significant limitations. They tend to
hallucinate-generate non-actual information-and ignore the latest research they
have not been trained on. To address these limitations, we propose a toolkit
that operates on Retrieval Augmented Generation (RAG) principles, specialized
prompting and instructing techniques with the help of LLMs. Our system first
initiates a web search to retrieve relevant papers by summarizing user-provided
abstracts into keywords using an off-the-shelf LLM. Authors can enhance the
search by supplementing it with relevant papers or keywords, contributing to a
tailored retrieval process. Second, the system re-ranks the retrieved papers
based on the user-provided abstract. Finally, the related work section is
generated based on the re-ranked results and the abstract. There is a
substantial reduction in time and effort for literature review compared to
traditional methods, establishing our toolkit as an efficient alternative. Our
open-source toolkit is accessible at https://github.com/shubhamagarwal92/LitLLM
and Huggingface space (https://huggingface.co/spaces/shubhamagarwal92/LitLLM)
with the video demo at https://youtu.be/E2ggOZBAFw0.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01789" title="Abstract">arXiv:2402.01789</a> [<a href="/pdf/2402.01789" title="Download PDF">pdf</a>, <a href="/ps/2402.01789" title="Download PostScript">ps</a>, <a href="/format/2402.01789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Political Preferences of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rozado%2C+D">David Rozado</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">We report here a comprehensive analysis about the political preferences
embedded in Large Language Models (LLMs). Namely, we administer 11 political
orientation tests, designed to identify the political preferences of the test
taker, to 24 state-of-the-art conversational LLMs, both close and open source.
The results indicate that when probed with questions/statements with political
connotations most conversational LLMs tend to generate responses that are
diagnosed by most political test instruments as manifesting preferences for
left-of-center viewpoints. We note that this is not the case for base (i.e.
foundation) models upon which LLMs optimized for conversation with humans are
built. However, base models' suboptimal performance at coherently answering
questions suggests caution when interpreting their classification by political
orientation tests. Though not conclusive, our results provide preliminary
evidence for the intriguing hypothesis that the embedding of political
preferences into LLMs might be happening mostly post-pretraining. Namely,
during the supervised fine-tuning (SFT) and/or Reinforcement Learning (RL)
stages of the conversational LLMs training pipeline. We provide further support
for this hypothesis by showing that LLMs are easily steerable into target
locations of the political spectrum via SFT requiring only modest compute and
custom data, illustrating the ability of SFT to imprint political preferences
onto LLMs. As LLMs have started to displace more traditional information
sources such as search engines or Wikipedia, the implications of political
biases embedded in LLMs has important societal ramifications.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01790" title="Abstract">arXiv:2402.01790</a> [<a href="/pdf/2402.01790" title="Download PDF">pdf</a>, <a href="/format/2402.01790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An introduction to graphical tensor notation for mechanistic  interpretability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taylor%2C+J+K">Jordan K. Taylor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 75 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Graphical tensor notation is a simple way of denoting linear operations on
tensors, originating from physics. Modern deep learning consists almost
entirely of operations on or between tensors, so easily understanding tensor
operations is quite important for understanding these systems. This is
especially true when attempting to reverse-engineer the algorithms learned by a
neural network in order to understand its behavior: a field known as
mechanistic interpretability. It's often easy to get confused about which
operations are happening between tensors and lose sight of the overall
structure, but graphical tensor notation makes it easier to parse things at a
glance and see interesting equivalences. The first half of this document
introduces the notation and applies it to some decompositions (SVD, CP, Tucker,
and tensor network decompositions), while the second half applies it to some
existing some foundational approaches for mechanistically understanding
language models, loosely following ``A Mathematical Framework for Transformer
Circuits'', then constructing an example ``induction head'' circuit in
graphical tensor notation.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01795" title="Abstract">arXiv:2402.01795</a> [<a href="/pdf/2402.01795" title="Download PDF">pdf</a>, <a href="/format/2402.01795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Few-Shot Scenario Testing for Autonomous Vehicles Based on Neighborhood  Coverage and Similarity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+S">Shu Li</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+J">Jingxuan Yang</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+H">Honglin He</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yi Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+J">Jianming Hu</a>, 
<a href="/search/eess?searchtype=author&query=Feng%2C+S">Shuo Feng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Testing and evaluating the safety performance of autonomous vehicles (AVs) is
essential before the large-scale deployment. Practically, the acceptable cost
of testing specific AV model can be restricted within an extremely small limit
because of testing cost or time. With existing testing methods, the limitations
imposed by strictly restricted testing numbers often result in significant
uncertainties or challenges in quantifying testing results. In this paper, we
formulate this problem for the first time the "few-shot testing" (FST) problem
and propose a systematic FST framework to address this challenge. To alleviate
the considerable uncertainty inherent in a small testing scenario set and
optimize scenario utilization, we frame the FST problem as an optimization
problem and search for a small scenario set based on neighborhood coverage and
similarity. By leveraging the prior information on surrogate models (SMs), we
dynamically adjust the testing scenario set and the contribution of each
scenario to the testing result under the guidance of better generalization
ability on AVs. With certain hypotheses on SMs, a theoretical upper bound of
testing error is established to verify the sufficiency of testing accuracy
within given limited number of tests. The experiments of the cut-in scenario
using FST method demonstrate a notable reduction in testing error and variance
compared to conventional testing methods, especially for situations with a
strict limitation on the number of scenarios.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01797" title="Abstract">arXiv:2402.01797</a> [<a href="/pdf/2402.01797" title="Download PDF">pdf</a>, <a href="/format/2402.01797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust support vector machines via conic optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cepeda%2C+V">Valentina Cepeda</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B3mez%2C+A">Andr&#xe9;s G&#xf3;mez</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Shaoning Han</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Computation (stat.CO)

</div>
<p class="mathjax">We consider the problem of learning support vector machines robust to
uncertainty. It has been established in the literature that typical loss
functions, including the hinge loss, are sensible to data perturbations and
outliers, thus performing poorly in the setting considered. In contrast, using
the 0-1 loss or a suitable non-convex approximation results in robust
estimators, at the expense of large computational costs. In this paper we use
mixed-integer optimization techniques to derive a new loss function that better
approximates the 0-1 loss compared with existing alternatives, while preserving
the convexity of the learning problem. In our computational results, we show
that the proposed estimator is competitive with the standard SVMs with the
hinge loss in outlier-free regimes and better in the presence of outliers.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01798" title="Abstract">arXiv:2402.01798</a> [<a href="/pdf/2402.01798" title="Download PDF">pdf</a>, <a href="/format/2402.01798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Quantization Strategies for Managing Heavy-tailed Gradients in  Distributed Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+G">Guangfeng Yan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tan Li</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yuanzhang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+H">Hanxu Hou</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linqi Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2402.01160">arXiv:2402.01160</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Gradient compression has surfaced as a key technique to address the challenge
of communication efficiency in distributed learning. In distributed deep
learning, however, it is observed that gradient distributions are heavy-tailed,
with outliers significantly influencing the design of compression strategies.
Existing parameter quantization methods experience performance degradation when
this heavy-tailed feature is ignored. In this paper, we introduce a novel
compression scheme specifically engineered for heavy-tailed gradients, which
effectively combines gradient truncation with quantization. This scheme is
adeptly implemented within a communication-limited distributed Stochastic
Gradient Descent (SGD) framework. We consider a general family of heavy-tail
gradients that follow a power-law distribution, we aim to minimize the error
resulting from quantization, thereby determining optimal values for two
critical parameters: the truncation threshold and the quantization density. We
provide a theoretical analysis on the convergence error bound under both
uniform and non-uniform quantization scenarios. Comparative experiments with
other benchmarks demonstrate the effectiveness of our proposed method in
managing the heavy-tailed gradients in a distributed learning environment.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01799" title="Abstract">arXiv:2402.01799</a> [<a href="/pdf/2402.01799" title="Download PDF">pdf</a>, <a href="/ps/2402.01799" title="Download PostScript">ps</a>, <a href="/format/2402.01799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chavan%2C+A">Arnav Chavan</a>, 
<a href="/search/cs?searchtype=author&query=Magazine%2C+R">Raghav Magazine</a>, 
<a href="/search/cs?searchtype=author&query=Kushwaha%2C+S">Shubham Kushwaha</a>, 
<a href="/search/cs?searchtype=author&query=Debbah%2C+M">M&#xe9;rouane Debbah</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+D">Deepak Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Brief Survey, Under review at IJCAI '24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Despite the impressive performance of LLMs, their widespread adoption faces
challenges due to substantial computational and memory requirements during
inference. Recent advancements in model compression and system-level
optimization methods aim to enhance LLM inference. This survey offers an
overview of these methods, emphasizing recent developments. Through experiments
on LLaMA(/2)-7B, we evaluate various compression techniques, providing
practical insights for efficient LLM deployment in a unified setting. The
empirical analysis on LLaMA(/2)-7B highlights the effectiveness of these
methods. Drawing from survey insights, we identify current limitations and
discuss potential future directions to improve LLM inference efficiency. We
release the codebase to reproduce the results presented in this paper at
https://github.com/nyunAI/Faster-LLM-Survey
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01801" title="Abstract">arXiv:2402.01801</a> [<a href="/pdf/2402.01801" title="Download PDF">pdf</a>, <a href="/format/2402.01801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models for Time Series: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+R+R">Ranak Roy Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+R+K">Rajesh K. Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+J">Jingbo Shang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> GitHub repository: <a href="https://github.com/xiyuanzh/awesome-llm-time-series">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Large Language Models (LLMs) have seen significant use in domains such as
natural language processing and computer vision. Going beyond text, image and
graphics, LLMs present a significant potential for analysis of time series
data, benefiting domains such as climate, IoT, healthcare, traffic, audio and
finance. This survey paper provides an in-depth exploration and a detailed
taxonomy of the various methodologies employed to harness the power of LLMs for
time series analysis. We address the inherent challenge of bridging the gap
between LLMs' original text data training and the numerical nature of time
series data, and explore strategies for transferring and distilling knowledge
from LLMs to numerical time series analysis. We detail various methodologies,
including (1) direct prompting of LLMs, (2) time series quantization, (3)
alignment techniques, (4) utilization of the vision modality as a bridging
mechanism, and (5) the combination of LLMs with tools. Additionally, this
survey offers a comprehensive overview of the existing multimodal time series
and text datasets and delves into the challenges and future opportunities of
this emerging field. We maintain an up-to-date Github repository which includes
all the papers and datasets discussed in the survey.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01802" title="Abstract">arXiv:2402.01802</a> [<a href="/pdf/2402.01802" title="Download PDF">pdf</a>, <a href="/format/2402.01802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Auction-based Marketplace for Model Trading in Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yue Cui</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+L">Liuyi Yao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ziqian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+B">Bolin Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiaofang Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Federated learning (FL) is increasingly recognized for its efficacy in
training models using locally distributed data. However, the proper valuation
of shared data in this collaborative process remains insufficiently addressed.
In this work, we frame FL as a marketplace of models, where clients act as both
buyers and sellers, engaging in model trading. This FL market allows clients to
gain monetary reward by selling their own models and improve local model
performance through the purchase of others' models. We propose an auction-based
solution to ensure proper pricing based on performance gain. Incentive
mechanisms are designed to encourage clients to truthfully reveal their model
valuations. Furthermore, we introduce a reinforcement learning (RL) framework
for marketing operations, aiming to achieve maximum trading volumes under the
dynamic and evolving market status. Experimental results on four datasets
demonstrate that the proposed FL market can achieve high trading revenue and
fair downstream task accuracy.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01804" title="Abstract">arXiv:2402.01804</a> [<a href="/pdf/2402.01804" title="Download PDF">pdf</a>, <a href="/format/2402.01804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Internet of Things implementation barriers in the cold  supply chain: an integrated ISM-MICMAC and DEMATEL approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+K">Kazrin Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+S">Md. Saiful Islam</a>, 
<a href="/search/cs?searchtype=author&query=Jahin%2C+M+A">Md Abrar Jahin</a>, 
<a href="/search/cs?searchtype=author&query=Mridha%2C+M+F">M. F. Mridha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Integrating Internet of Things (IoT) technology inside the cold supply chain
can enhance transparency, efficiency, and quality, optimizing operating
procedures and increasing productivity. The integration of IoT in this
complicated setting is hindered by specific barriers that need a thorough
examination. Prominent barriers to IoT implementation in the cold supply chain
are identified using a two-stage model. After reviewing the available
literature on the topic of IoT implementation, a total of 13 barriers were
found. The survey data was cross-validated for quality, and Cronbach's alpha
test was employed to ensure validity. This research applies the interpretative
structural modeling technique in the first phase to identify the main barriers.
Among those barriers, "regularity compliance" and "cold chain networks" are key
drivers for IoT adoption strategies. MICMAC's driving and dependence power
element categorization helps evaluate the barrier interactions. In the second
phase of this research, a decision-making trial and evaluation laboratory
methodology was employed to identify causal relationships between barriers and
evaluate them according to their relative importance. Each cause is a potential
drive, and if its efficiency can be enhanced, the system as a whole benefits.
The research findings provide industry stakeholders, governments, and
organizations with significant drivers of IoT adoption to overcome these
barriers and optimize the utilization of IoT technology to improve the
effectiveness and reliability of the cold supply chain.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01805" title="Abstract">arXiv:2402.01805</a> [<a href="/pdf/2402.01805" title="Download PDF">pdf</a>, <a href="/format/2402.01805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Limitations of Graph Reasoning in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+P">Palaash Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Vasania%2C+S">Shavak Vasania</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Cheston Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Pretrained Large Language Models have demonstrated various types of reasoning
capabilities through language-based prompts alone. However, in this paper, we
test the depth of graph reasoning for 5 different LLMs (GPT-4, GPT-3.5,
Claude-2, Llama-2 and Palm-2) through the problems of graph reasoning. In
particular, we design 10 distinct problems of graph traversal, each
representing increasing levels of complexity. Further, we analyze the
performance of models across various settings such as varying sizes of graphs
as well as different forms of k-shot prompting. We highlight various
limitations, biases, and properties of LLMs through this benchmarking process,
such as an inverse relation to the average degrees of freedom of traversal per
node in graphs, the overall negative impact of k-shot prompting on graph
reasoning tasks, and a positive response bias which prevents LLMs from
identifying the absence of a valid solution. Finally, we propose a new
prompting technique specially designed for graph traversal tasks, known as
PathCompare, which shows a notable increase in the performance of LLMs in
comparison to standard prompting and CoT.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01806" title="Abstract">arXiv:2402.01806</a> [<a href="/pdf/2402.01806" title="Download PDF">pdf</a>, <a href="/format/2402.01806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HQA-Attack: Toward High Quality Black-Box Hard-Label Adversarial Attack  on Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Han Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaotong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Feng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+F">Fenglong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xianchao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Black-box hard-label adversarial attack on text is a practical and
challenging task, as the text data space is inherently discrete and
non-differentiable, and only the predicted label is accessible. Research on
this problem is still in the embryonic stage and only a few methods are
available. Nevertheless, existing methods rely on the complex heuristic
algorithm or unreliable gradient estimation strategy, which probably fall into
the local optimum and inevitably consume numerous queries, thus are difficult
to craft satisfactory adversarial examples with high semantic similarity and
low perturbation rate in a limited query budget. To alleviate above issues, we
propose a simple yet effective framework to generate high quality textual
adversarial examples under the black-box hard-label attack scenarios, named
HQA-Attack. Specifically, after initializing an adversarial example randomly,
HQA-attack first constantly substitutes original words back as many as
possible, thus shrinking the perturbation rate. Then it leverages the synonym
set of the remaining changed words to further optimize the adversarial example
with the direction which can improve the semantic similarity and satisfy the
adversarial condition simultaneously. In addition, during the optimizing
procedure, it searches a transition synonym word for each changed word, thus
avoiding traversing the whole synonym set and reducing the query number to some
extent. Extensive experimental results on five text classification datasets,
three natural language inference datasets and two real-world APIs have shown
that the proposed HQA-Attack method outperforms other strong baselines
significantly.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01807" title="Abstract">arXiv:2402.01807</a> [<a href="/pdf/2402.01807" title="Download PDF">pdf</a>, <a href="/format/2402.01807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AOC-IDS: Autonomous Online Framework with Contrastive Learning for  Intrusion Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinchen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Running Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhihan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhicong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yulong Ding</a>, 
<a href="/search/cs?searchtype=author&query=Ngai%2C+E+C+H">Edith C.H. Ngai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shuang-Hua Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The rapid expansion of the Internet of Things (IoT) has raised increasing
concern about targeted cyber attacks. Previous research primarily focused on
static Intrusion Detection Systems (IDSs), which employ offline training to
safeguard IoT systems. However, such static IDSs struggle with real-world
scenarios where IoT system behaviors and attack strategies can undergo rapid
evolution, necessitating dynamic and adaptable IDSs. In response to this
challenge, we propose AOC-IDS, a novel online IDS that features an autonomous
anomaly detection module (ADM) and a labor-free online framework for continual
adaptation. In order to enhance data comprehension, the ADM employs an
Autoencoder (AE) with a tailored Cluster Repelling Contrastive (CRC) loss
function to generate distinctive representation from limited or incrementally
incoming data in the online setting. Moreover, to reduce the burden of manual
labeling, our online framework leverages pseudo-labels automatically generated
from the decision-making process in the ADM to facilitate periodic updates of
the ADM. The elimination of human intervention for labeling and decision-making
boosts the system's compatibility and adaptability in the online setting to
remain synchronized with dynamic environments. Experimental validation using
the NSL-KDD and UNSW-NB15 datasets demonstrates the superior performance and
adaptability of AOC-IDS, surpassing the state-of-the-art solutions. The code is
released at https://github.com/xinchen930/AOC-IDS.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01808" title="Abstract">arXiv:2402.01808</a> [<a href="/pdf/2402.01808" title="Download PDF">pdf</a>, <a href="/format/2402.01808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KS-Net: Multi-band joint speech restoration and enhancement network for  2024 ICASSP SSI Challenge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+G">Guochen Yu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+R">Runqiang Han</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chenglin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haoran Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+N">Nan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xiguang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bing Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICASSP 2024; Rank 1st in ICASSP 2024 Speech Signal Improvement (SSI) Challenge
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This paper presents the speech restoration and enhancement system created by
the 1024K team for the ICASSP 2024 Speech Signal Improvement (SSI) Challenge.
Our system consists of a generative adversarial network (GAN) in complex-domain
for speech restoration and a fine-grained multi-band fusion module for speech
enhancement. In the blind test set of SSI, the proposed system achieves an
overall mean opinion score (MOS) of 3.49 based on ITU-T P.804 and a Word
Accuracy Rate (WAcc) of 0.78 for the real-time track, as well as an overall
P.804 MOS of 3.43 and a WAcc of 0.78 for the non-real-time track, ranking 1st
in both tracks.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01811" title="Abstract">arXiv:2402.01811</a> [<a href="/pdf/2402.01811" title="Download PDF">pdf</a>, <a href="/format/2402.01811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Distributionally Robust Optimisation Approach to Fair Credit Scoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Casas%2C+P">Pablo Casas</a>, 
<a href="/search/cs?searchtype=author&query=Mues%2C+C">Christophe Mues</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Huan Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Credit scoring has been catalogued by the European Commission and the
Executive Office of the US President as a high-risk classification task, a key
concern being the potential harms of making loan approval decisions based on
models that would be biased against certain groups. To address this concern,
recent credit scoring research has considered a range of fairness-enhancing
techniques put forward by the machine learning community to reduce bias and
unfair treatment in classification systems. While the definition of fairness or
the approach they follow to impose it may vary, most of these techniques,
however, disregard the robustness of the results. This can create situations
where unfair treatment is effectively corrected in the training set, but when
producing out-of-sample classifications, unfair treatment is incurred again.
Instead, in this paper, we will investigate how to apply Distributionally
Robust Optimisation (DRO) methods to credit scoring, thereby empirically
evaluating how they perform in terms of fairness, ability to classify
correctly, and the robustness of the solution against changes in the marginal
proportions. In so doing, we find DRO methods to provide a substantial
improvement in terms of fairness, with almost no loss in performance. These
results thus indicate that DRO can improve fairness in credit scoring, provided
that further advances are made in efficiently implementing these systems. In
addition, our analysis suggests that many of the commonly used fairness metrics
are unsuitable for a credit scoring setting, as they depend on the choice of
classification threshold.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01812" title="Abstract">arXiv:2402.01812</a> [<a href="/pdf/2402.01812" title="Download PDF">pdf</a>, <a href="/format/2402.01812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distilling LLMs&#x27; Decomposition Abilities into Compact Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tarasov%2C+D">Denis Tarasov</a>, 
<a href="/search/cs?searchtype=author&query=Shridhar%2C+K">Kumar Shridhar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://github.com/DT6A/GSM8K-AI-SubQ">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated proficiency in their reasoning
abilities, yet their large size presents scalability challenges and limits any
further customization. In contrast, compact models offer customized training
but often fall short in solving complex reasoning tasks. This study focuses on
distilling the LLMs' decomposition skills into compact models using offline
reinforcement learning. We leverage the advancements in the LLM`s capabilities
to provide feedback and generate a specialized task-specific dataset for
training compact models. The development of an AI-generated dataset and the
establishment of baselines constitute the primary contributions of our work,
underscoring the potential of compact models in replicating complex
problem-solving skills.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01813" title="Abstract">arXiv:2402.01813</a> [<a href="/pdf/2402.01813" title="Download PDF">pdf</a>, <a href="/format/2402.01813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Educational Tool for Learning about Social Media Tracking, Profiling,  and Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pope%2C+N">Nicolas Pope</a>, 
<a href="/search/cs?searchtype=author&query=Kahila%2C+J">Juho Kahila</a>, 
<a href="/search/cs?searchtype=author&query=Laru%2C+J">Jari Laru</a>, 
<a href="/search/cs?searchtype=author&query=Vartiainen%2C+H">Henriikka Vartiainen</a>, 
<a href="/search/cs?searchtype=author&query=Roos%2C+T">Teemu Roos</a>, 
<a href="/search/cs?searchtype=author&query=Tedre%2C+M">Matti Tedre</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 5 figures, submitted to ICALT 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">This paper introduces an educational tool for classroom use, based on
explainable AI (XAI), designed to demystify key social media mechanisms -
tracking, profiling, and content recommendation - for novice learners. The tool
provides a familiar, interactive interface that resonates with learners'
experiences with popular social media platforms, while also offering the means
to "peek under the hood" and exposing basic mechanisms of datafication.
Learners gain first-hand experience of how even the slightest actions, such as
pausing to view content, are captured and recorded in their digital footprint,
and further distilled into a personal profile. The tool uses real-time
visualizations and verbal explanations to create a sense of immediacy: each
time the user acts, the resulting changes in their engagement history and their
profile are displayed in a visually engaging and understandable manner. This
paper discusses the potential of XAI and educational technology in transforming
data and digital literacy education and in fostering the growth of children's
privacy and security mindsets.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01816" title="Abstract">arXiv:2402.01816</a> [<a href="/pdf/2402.01816" title="Download PDF">pdf</a>, <a href="/format/2402.01816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple Symmetric Sustainable Sorting -- the greeNsort article
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oehlschl%C3%A4gel%2C+J">Jens Oehlschl&#xe4;gel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 50 pages, 6 Figures, latest version under <a href="https://github.com/greeNsort/greeNsort.article">this https URL</a>, see also <a href="https://greensort.org">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computers and Society (cs.CY); Distributed, Parallel, and Cluster Computing (cs.DC); Performance (cs.PF)

</div>
<p class="mathjax">We explored an uncharted part of the solution space for sorting algorithms:
the role of symmetry in divide&amp;conquer algorithms. We found/designed novel
simple binary Quicksort and Mergesort algorithms operating in contiguous space
which achieve improved trade-offs between worst-case CPU-efficiency, best-case
adaptivity and RAM-requirements. The 'greeNsort' algorithms need less hardware
(RAM) and/or less energy (CPU) compared to the prior art. The new algorithms
fit a theoretical framework: 'Footprint' KPIs allow to compare algorithms with
different RAM-requirements, a new 'definition' of sorting API-targets
simplifies construction of stable algorithms with mirrored scan directions, and
our ordinal machine model encourages robust algorithms that minimize access
'distance'. Unlike earlier 'Quicksorts', our 'Zacksort', 'Zucksort' and
'Ducksort' algorithms optimally marry CPU-efficiency and tie-adaptivity. Unlike
earlier 'Mergesorts' which required 100% distant buffer, our 'Frogsort' and
'Geckosort' algorithms achieve similar CPU-efficiency with 50% or less local
buffer. Unlike natural Mergesorts such as 'Timsort' which are optimized for the
best case of full-presorting, our 'Octosort' and 'Squidsort' algorithms achieve
excellent bi-adaptivity to presorted best-cases without sacrificing worst-case
efficiency in real sorting tasks. Our 'Walksort' and 'Jumpsort' have lower
Footprint than the impressive low-memory 'Grailsort' and 'Sqrtsort' of
Astrelin. Given the current climate-emergency, this is a call to action for all
maintainers of sorting libraries, all software-engineers using custom sorting
code, all professors teaching algorithms, all IT professionals designing
programming languages, compilers and CPUs: check for better algorithms and
consider symmetric code-mirroring.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01817" title="Abstract">arXiv:2402.01817</a> [<a href="/pdf/2402.01817" title="Download PDF">pdf</a>, <a href="/format/2402.01817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMs Can&#x27;t Plan, But Can Help Planning in LLM-Modulo Frameworks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kambhampati%2C+S">Subbarao Kambhampati</a>, 
<a href="/search/cs?searchtype=author&query=Valmeekam%2C+K">Karthik Valmeekam</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+L">Lin Guan</a>, 
<a href="/search/cs?searchtype=author&query=Stechly%2C+K">Kaya Stechly</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+M">Mudit Verma</a>, 
<a href="/search/cs?searchtype=author&query=Bhambri%2C+S">Siddhant Bhambri</a>, 
<a href="/search/cs?searchtype=author&query=Saldyt%2C+L">Lucas Saldyt</a>, 
<a href="/search/cs?searchtype=author&query=Murthy%2C+A">Anil Murthy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">There is considerable confusion about the role of Large Language Models
(LLMs) in planning and reasoning tasks. On one side are over-optimistic claims
that LLMs can indeed do these tasks with just the right prompting or
self-verification strategies. On the other side are perhaps over-pessimistic
claims that all that LLMs are good for in planning/reasoning tasks are as mere
translators of the problem specification from one syntactic format to another,
and ship the problem off to external symbolic solvers. In this position paper,
we take the view that both these extremes are misguided. We argue that
auto-regressive LLMs cannot, by themselves, do planning or self-verification
(which is after all a form of reasoning), and shed some light on the reasons
for misunderstandings in the literature. We will also argue that LLMs should be
viewed as universal approximate knowledge sources that have much more
meaningful roles to play in planning/reasoning tasks beyond simple
front-end/back-end format translators. We present a vision of {\bf LLM-Modulo
Frameworks} that combine the strengths of LLMs with external model-based
verifiers in a tighter bi-directional interaction regime. We will show how the
models driving the external verifiers themselves can be acquired with the help
of LLMs. We will also argue that rather than simply pipelining LLMs and
symbolic components, this LLM-Modulo Framework provides a better neuro-symbolic
approach that offers tighter integration between LLMs and symbolic components,
and allows extending the scope of model-based planning/reasoning regimes
towards more flexible knowledge, problem and preference specifications.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01821" title="Abstract">arXiv:2402.01821</a> [<a href="/pdf/2402.01821" title="Download PDF">pdf</a>, <a href="/format/2402.01821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ecologically rational meta-learned inference explains human category  learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jagadish%2C+A+K">Akshay K. Jagadish</a>, 
<a href="/search/cs?searchtype=author&query=Coda-Forno%2C+J">Julian Coda-Forno</a>, 
<a href="/search/cs?searchtype=author&query=Thalmann%2C+M">Mirko Thalmann</a>, 
<a href="/search/cs?searchtype=author&query=Schulz%2C+E">Eric Schulz</a>, 
<a href="/search/cs?searchtype=author&query=Binz%2C+M">Marcel Binz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages (8 pages of main text, 4 pages of references, and 10 pages of appendix), 7 figures, and 4 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Ecological rationality refers to the notion that humans are rational agents
adapted to their environment. However, testing this theory remains challenging
due to two reasons: the difficulty in defining what tasks are ecologically
valid and building rational models for these tasks. In this work, we
demonstrate that large language models can generate cognitive tasks,
specifically category learning tasks, that match the statistics of real-world
tasks, thereby addressing the first challenge. We tackle the second challenge
by deriving rational agents adapted to these tasks using the framework of
meta-learning, leading to a class of models called ecologically rational
meta-learned inference (ERMI). ERMI quantitatively explains human data better
than seven other cognitive models in two different experiments. It additionally
matches human behavior on a qualitative level: (1) it finds the same tasks
difficult that humans find difficult, (2) it becomes more reliant on an
exemplar-based strategy for assigning categories with learning, and (3) it
generalizes to unseen stimuli in a human-like way. Furthermore, we show that
ERMI's ecologically valid priors allow it to achieve state-of-the-art
performance on the OpenML-CC18 classification benchmark.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01822" title="Abstract">arXiv:2402.01822</a> [<a href="/pdf/2402.01822" title="Download PDF">pdf</a>, <a href="/format/2402.01822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Building Guardrails for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yi Dong</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+R">Ronghui Mu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+G">Gaojie Jin</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Y">Yi Qi</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jinwei Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xingyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+J">Jie Meng</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+W">Wenjie Ruan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaowei Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As Large Language Models (LLMs) become more integrated into our daily lives,
it is crucial to identify and mitigate their risks, especially when the risks
can have profound impacts on human users and societies. Guardrails, which
filter the inputs or outputs of LLMs, have emerged as a core safeguarding
technology. This position paper takes a deep look at current open-source
solutions (Llama Guard, Nvidia NeMo, Guardrails AI), and discusses the
challenges and the road towards building more complete solutions. Drawing on
robust evidence from previous research, we advocate for a systematic approach
to construct guardrails for LLMs, based on comprehensive consideration of
diverse contexts across various LLMs applications. We propose employing
socio-technical methods through collaboration with a multi-disciplinary team to
pinpoint precise technical requirements, exploring advanced neural-symbolic
implementations to embrace the complexity of the requirements, and developing
verification and testing to ensure the utmost quality of the final product.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01824" title="Abstract">arXiv:2402.01824</a> [<a href="/pdf/2402.01824" title="Download PDF">pdf</a>, <a href="/format/2402.01824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identification of Cognitive Decline from Spoken Language through Feature  Selection and the Bag of Acoustic Words Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niemel%C3%A4%2C+M">Marko Niemel&#xe4;</a>, 
<a href="/search/cs?searchtype=author&query=von+Bonsdorff%2C+M">Mikaela von Bonsdorff</a>, 
<a href="/search/cs?searchtype=author&query=%C3%84yr%C3%A4m%C3%B6%2C+S">Sami &#xc4;yr&#xe4;m&#xf6;</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%A4rkk%C3%A4inen%2C+T">Tommi K&#xe4;rkk&#xe4;inen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Memory disorders are a central factor in the decline of functioning and daily
activities in elderly individuals. The confirmation of the illness, initiation
of medication to slow its progression, and the commencement of occupational
therapy aimed at maintaining and rehabilitating cognitive abilities require a
medical diagnosis. The early identification of symptoms of memory disorders,
especially the decline in cognitive abilities, plays a significant role in
ensuring the well-being of populations. Features related to speech production
are known to connect with the speaker's cognitive ability and changes. The lack
of standardized speech tests in clinical settings has led to a growing emphasis
on developing automatic machine learning techniques for analyzing naturally
spoken language. Non-lexical but acoustic properties of spoken language have
proven useful when fast, cost-effective, and scalable solutions are needed for
the rapid diagnosis of a disease. The work presents an approach related to
feature selection, allowing for the automatic selection of the essential
features required for diagnosis from the Geneva minimalistic acoustic parameter
set and relative speech pauses, intended for automatic paralinguistic and
clinical speech analysis. These features are refined into word histogram
features, in which machine learning classifiers are trained to classify control
subjects and dementia patients from the Dementia Bank's Pitt audio database.
The results show that achieving a 75% average classification accuracy with only
twenty-five features with the separate ADReSS 2020 competition test data and
the Leave-One-Subject-Out cross-validation of the entire competition data is
possible. The results rank at the top compared to international research, where
the same dataset and only acoustic features have been used to diagnose
patients.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01825" title="Abstract">arXiv:2402.01825</a> [<a href="/pdf/2402.01825" title="Download PDF">pdf</a>, <a href="/format/2402.01825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fractal Patterns May Unravel the Intelligence in Next-Token Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alabdulmohsin%2C+I">Ibrahim Alabdulmohsin</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+V+Q">Vinh Q. Tran</a>, 
<a href="/search/cs?searchtype=author&query=Dehghani%2C+M">Mostafa Dehghani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 10 tables, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We study the fractal structure of language, aiming to provide a precise
formalism for quantifying properties that may have been previously suspected
but not formally shown. We establish that language is: (1) self-similar,
exhibiting complexities at all levels of granularity, with no particular
characteristic context length, and (2) long-range dependent (LRD), with a Hurst
parameter of approximately H=0.70. Based on these findings, we argue that
short-term patterns/dependencies in language, such as in paragraphs, mirror the
patterns/dependencies over larger scopes, like entire documents. This may shed
some light on how next-token prediction can lead to a comprehension of the
structure of text at multiple levels of granularity, from words and clauses to
broader contexts and intents. We also demonstrate that fractal parameters
improve upon perplexity-based bits-per-byte (BPB) in predicting downstream
performance. We hope these findings offer a fresh perspective on language and
the mechanisms underlying the success of LLMs.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01826" title="Abstract">arXiv:2402.01826</a> [<a href="/pdf/2402.01826" title="Download PDF">pdf</a>, <a href="/format/2402.01826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Large Language Models for Analyzing Blood Pressure Variations  Across Biological Sex from Scientific Literature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yuting Guo</a>, 
<a href="/search/cs?searchtype=author&query=Mousavi%2C+S+S">Seyedeh Somayyeh Mousavi</a>, 
<a href="/search/cs?searchtype=author&query=Sameni%2C+R">Reza Sameni</a>, 
<a href="/search/cs?searchtype=author&query=Sarker%2C+A">Abeed Sarker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Hypertension, defined as blood pressure (BP) that is above normal, holds
paramount significance in the realm of public health, as it serves as a
critical precursor to various cardiovascular diseases (CVDs) and significantly
contributes to elevated mortality rates worldwide. However, many existing BP
measurement technologies and standards might be biased because they do not
consider clinical outcomes, comorbidities, or demographic factors, making them
inconclusive for diagnostic purposes. There is limited data-driven research
focused on studying the variance in BP measurements across these variables. In
this work, we employed GPT-35-turbo, a large language model (LLM), to
automatically extract the mean and standard deviation values of BP for both
males and females from a dataset comprising 25 million abstracts sourced from
PubMed. 993 article abstracts met our predefined inclusion criteria (i.e.,
presence of references to blood pressure, units of blood pressure such as mmHg,
and mention of biological sex). Based on the automatically-extracted
information from these articles, we conducted an analysis of the variations of
BP values across biological sex. Our results showed the viability of utilizing
LLMs to study the BP variations across different demographic factors.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01828" title="Abstract">arXiv:2402.01828</a> [<a href="/pdf/2402.01828" title="Download PDF">pdf</a>, <a href="/format/2402.01828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieval Augmented End-to-End Spoken Dialog Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingqiu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shafran%2C+I">Izhak Shafran</a>, 
<a href="/search/cs?searchtype=author&query=Soltau%2C+H">Hagen Soltau</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+W">Wei Han</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yuan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dian Yu</a>, 
<a href="/search/cs?searchtype=author&query=Shafey%2C+L+E">Laurent El Shafey</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proc. ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We recently developed SLM, a joint speech and language model, which fuses a
pretrained foundational speech model and a large language model (LLM), while
preserving the in-context learning capability intrinsic to the pretrained LLM.
In this paper, we apply SLM to speech dialog applications where the dialog
states are inferred directly from the audio signal.
<br />Task-oriented dialogs often contain domain-specific entities, i.e.,
restaurants, hotels, train stations, and city names, which are difficult to
recognize, however, critical for the downstream applications. Inspired by the
RAG (retrieval-augmented generation) paradigm, we propose a retrieval augmented
SLM (ReSLM) that overcomes this weakness. We first train a speech retriever to
retrieve text entities mentioned in the audio. The retrieved entities are then
added as text inputs to the underlying SLM to bias model predictions. We
evaluated ReSLM on speech MultiWoz task (DSTC-11 challenge), and found that
this retrieval augmentation boosts model performance, achieving joint goal
accuracy (38.6% vs 32.7%), slot error rate (20.6% vs 24.8%) and ASR word error
rate (5.5% vs 6.7%). While demonstrated on dialog state tracking, our approach
is broadly applicable to other speech tasks requiring contextual information or
domain-specific entities, such as contextual ASR with biasing capability.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01830" title="Abstract">arXiv:2402.01830</a> [<a href="/pdf/2402.01830" title="Download PDF">pdf</a>, <a href="/format/2402.01830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Peer-review-in-LLMs: Automatic Evaluation Method for LLMs in  Open-environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ning%2C+K">Kun-Peng Ning</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shuo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu-Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jia-Yu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhen-Hui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+M">Ming Pang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Li Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Existing large language models (LLMs) evaluation methods typically focus on
testing the performance on some closed-environment and domain-specific
benchmarks with human annotations. In this paper, we explore a novel
unsupervised evaluation direction, utilizing peer-review mechanisms to measure
LLMs automatically. In this setting, both open-source and closed-source LLMs
lie in the same environment, capable of answering unlabeled questions and
evaluating each other, where each LLM's response score is jointly determined by
other anonymous ones. To obtain the ability hierarchy among these models, we
assign each LLM a learnable capability parameter to adjust the final ranking.
We formalize it as a constrained optimization problem, intending to maximize
the consistency of each LLM's capabilities and scores. The key assumption
behind is that high-level LLM can evaluate others' answers more accurately than
low-level ones, while higher-level LLM can also achieve higher response scores.
Moreover, we propose three metrics called PEN, CIN, and LIS to evaluate the gap
in aligning human rankings. We perform experiments on multiple datasets with
these metrics, validating the effectiveness of the proposed approach.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01831" title="Abstract">arXiv:2402.01831</a> [<a href="/pdf/2402.01831" title="Download PDF">pdf</a>, <a href="/format/2402.01831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and  Dialogue Abilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+Z">Zhifeng Kong</a>, 
<a href="/search/cs?searchtype=author&query=Goel%2C+A">Arushi Goel</a>, 
<a href="/search/cs?searchtype=author&query=Badlani%2C+R">Rohan Badlani</a>, 
<a href="/search/cs?searchtype=author&query=Ping%2C+W">Wei Ping</a>, 
<a href="/search/cs?searchtype=author&query=Valle%2C+R">Rafael Valle</a>, 
<a href="/search/cs?searchtype=author&query=Catanzaro%2C+B">Bryan Catanzaro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Augmenting large language models (LLMs) to understand audio -- including
non-speech sounds and non-verbal speech -- is critically important for diverse
real-world applications of LLMs. In this paper, we propose Audio Flamingo, a
novel audio language model with 1) strong audio understanding abilities, 2) the
ability to quickly adapt to unseen tasks via in-context learning and retrieval,
and 3) strong multi-turn dialogue abilities. We introduce a series of training
techniques, architecture design, and data strategies to enhance our model with
these abilities. Extensive evaluations across various audio understanding tasks
confirm the efficacy of our method, setting new state-of-the-art benchmarks.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01832" title="Abstract">arXiv:2402.01832</a> [<a href="/pdf/2402.01832" title="Download PDF">pdf</a>, <a href="/format/2402.01832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hammoud%2C+H+A+A+K">Hasan Abed Al Kader Hammoud</a>, 
<a href="/search/cs?searchtype=author&query=Itani%2C+H">Hani Itani</a>, 
<a href="/search/cs?searchtype=author&query=Pizzati%2C+F">Fabio Pizzati</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P">Philip Torr</a>, 
<a href="/search/cs?searchtype=author&query=Bibi%2C+A">Adel Bibi</a>, 
<a href="/search/cs?searchtype=author&query=Ghanem%2C+B">Bernard Ghanem</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We present SynthCLIP, a novel framework for training CLIP models with
entirely synthetic text-image pairs, significantly departing from previous
methods relying on real data. Leveraging recent text-to-image (TTI) generative
networks and large language models (LLM), we are able to generate synthetic
datasets of images and corresponding captions at any scale, with no human
intervention. With training at scale, SynthCLIP achieves performance comparable
to CLIP models trained on real datasets. We also introduce SynthCI-30M, a
purely synthetic dataset comprising 30 million captioned images. Our code,
trained models, and generated data are released at
https://github.com/hammoudhasan/SynthCLIP
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01840" title="Abstract">arXiv:2402.01840</a> [<a href="/pdf/2402.01840" title="Download PDF">pdf</a>, <a href="/format/2402.01840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ruitenburg&#x27;s Theorem mechanized and contextualized
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Litak%2C+T">Tadeusz Litak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This note has been prepared for the informal (pre-)proceedings of FICS 2024. The version to be submitted to the post-proceedings volume is going to be significantly different, focusing on the Coq formalization, as requested by referees and the PC
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">In 1984, Wim Ruitenburg published a surprising result about periodic
sequences in intuitionistic propositional calculus (IPC). The property
established by Ruitenburg naturally generalizes local finiteness
(intuitionistic logic is not locally finite, even in a single variable).
However, one of the two main goals of this note is to illustrate that most
"natural" non-classical logics failing local finiteness also do not enjoy the
periodic sequence property; IPC is quite unique in separating these properties.
The other goal of this note is to present a Coq formalization of Ruitenburg's
heavily syntactic proof. Apart from ensuring its correctness, the formalization
allows extraction of a program providing a certified implementation of
Ruitenburg's algorithm.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01841" title="Abstract">arXiv:2402.01841</a> [<a href="/pdf/2402.01841" title="Download PDF">pdf</a>, <a href="/format/2402.01841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COMET: Generating Commit Messages using Delta Graph Context  Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mandli%2C+A+R">Abhinav Reddy Mandli</a>, 
<a href="/search/cs?searchtype=author&query=Rajput%2C+S">Saurabhsingh Rajput</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+T">Tushar Sharma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 Pages, 7 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Commit messages explain code changes in a commit and facilitate collaboration
among developers. Several commit message generation approaches have been
proposed; however, they exhibit limited success in capturing the context of
code changes. We propose Comet (Context-Aware Commit Message Generation), a
novel approach that captures context of code changes using a graph-based
representation and leverages a transformer-based model to generate high-quality
commit messages. Our proposed method utilizes delta graph that we developed to
effectively represent code differences. We also introduce a customizable
quality assurance module to identify optimal messages, mitigating subjectivity
in commit messages. Experiments show that Comet outperforms state-of-the-art
techniques in terms of bleu-norm and meteor metrics while being comparable in
terms of rogue-l. Additionally, we compare the proposed approach with the
popular gpt-3.5-turbo model, along with gpt-4-turbo; the most capable GPT
model, over zero-shot, one-shot, and multi-shot settings. We found Comet
outperforming the GPT models, on five and four metrics respectively and provide
competitive results with the two other metrics. The study has implications for
researchers, tool developers, and software developers. Software developers may
utilize Comet to generate context-aware commit messages. Researchers and tool
developers can apply the proposed delta graph technique in similar contexts,
like code review summarization.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01843" title="Abstract">arXiv:2402.01843</a> [<a href="/pdf/2402.01843" title="Download PDF">pdf</a>, <a href="/format/2402.01843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Scalable In Situ Fast Fourier Transform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+S">Sudhanshu Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Loring%2C+B">Burlen Loring</a>, 
<a href="/search/cs?searchtype=author&query=Bethel%2C+E+W">E. Wes Bethel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures. Submitted to ISAV workshop in SC23 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">The Fast Fourier Transform (FFT) is a numerical operation that transforms a
function into a form comprised of its constituent frequencies and is an
integral part of scientific computation and data analysis. The objective of our
work is to enable use of the FFT as part of a scientific in situ processing
chain to facilitate the analysis of data in the spectral regime. We describe
the implementation of an FFT endpoint for the transformation of
multi-dimensional data within the SENSEI infrastructure. Our results show its
use on a sample problem in the context of a multi-stage in situ processing
workflow.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01845" title="Abstract">arXiv:2402.01845</a> [<a href="/pdf/2402.01845" title="Download PDF">pdf</a>, <a href="/format/2402.01845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Armed Bandits with Interference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+S">Su Jia</a>, 
<a href="/search/cs?searchtype=author&query=Frazier%2C+P">Peter Frazier</a>, 
<a href="/search/cs?searchtype=author&query=Kallus%2C+N">Nathan Kallus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Experimentation with interference poses a significant challenge in
contemporary online platforms. Prior research on experimentation with
interference has concentrated on the final output of a policy. The cumulative
performance, while equally crucial, is less well understood. To address this
gap, we introduce the problem of {\em Multi-armed Bandits with Interference}
(MABI), where the learner assigns an arm to each of $N$ experimental units over
a time horizon of $T$ rounds. The reward of each unit in each round depends on
the treatments of {\em all} units, where the influence of a unit decays in the
spatial distance between units. Furthermore, we employ a general setup wherein
the reward functions are chosen by an adversary and may vary arbitrarily across
rounds and units. We first show that switchback policies achieve an optimal
{\em expected} regret $\tilde O(\sqrt T)$ against the best fixed-arm policy.
Nonetheless, the regret (as a random variable) for any switchback policy
suffers a high variance, as it does not account for $N$. We propose a cluster
randomization policy whose regret (i) is optimal in {\em expectation} and (ii)
admits a high probability bound that vanishes in $N$.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01849" title="Abstract">arXiv:2402.01849</a> [<a href="/pdf/2402.01849" title="Download PDF">pdf</a>, <a href="/format/2402.01849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Capturing waste collection planning expert knowledge in a fitness  function through preference learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=D%C3%ADaz%2C+L+F">Laura Fern&#xe1;ndez D&#xed;az</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%ADaz%2C+M+F">Miriam Fern&#xe1;ndez D&#xed;az</a>, 
<a href="/search/cs?searchtype=author&query=Quevedo%2C+J+R">Jos&#xe9; Ram&#xf3;n Quevedo</a>, 
<a href="/search/cs?searchtype=author&query=Monta%C3%B1%C3%A9s%2C+E">Elena Monta&#xf1;&#xe9;s</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Engineering Applications of Artificial Intelligence 2021 Volume 99
  104113
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper copes with the COGERSA waste collection process. Up to now,
experts have been manually designed the process using a trial and error
mechanism. This process is not globally optimized, since it has been
progressively and locally built as council demands appear. Planning
optimization algorithms usually solve it, but they need a fitness function to
evaluate a route planning quality. The drawback is that even experts are not
able to propose one in a straightforward way due to the complexity of the
process. Hence, the goal of this paper is to build a fitness function though a
preference framework, taking advantage of the available expert knowledge and
expertise. Several key performance indicators together with preference
judgments are carefully established according to the experts for learning a
promising fitness function. Particularly, the additivity property of them makes
the task be much more affordable, since it allows to work with routes rather
than with route plannings. Besides, a feature selection analysis is performed
over such indicators, since the experts suspect of a potential existing (but
unknown) redundancy among them. The experiment results confirm this hypothesis,
since the best $C-$index ($98\%$ against around $94\%$) is reached when 6 or 8
out of 21 indicators are taken. Particularly, truck load seems to be a highly
promising key performance indicator, together to the travelled distance along
non-main roads. A comparison with other existing approaches shows that the
proposed method clearly outperforms them, since the $C-$index goes from $72\%$
or $90\%$ to $98\%$.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01852" title="Abstract">arXiv:2402.01852</a> [<a href="/pdf/2402.01852" title="Download PDF">pdf</a>, <a href="/ps/2402.01852" title="Download PostScript">ps</a>, <a href="/format/2402.01852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QPP and HPPK: Unifying Non-Commutativity for Quantum-Secure Cryptography  with Galois Permutation Group
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuang%2C+R">Randy Kuang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">In response to the evolving landscape of quantum computing and the escalating
vulnerabilities in classical cryptographic systems, our paper introduces a
unified cryptographic framework. Rooted in the innovative work of Kuang et al.,
we leverage two novel primitives: the Quantum Permutation Pad (QPP) for
symmetric key encryption and the Homomorphic Polynomial Public Key (HPPK) for
Key Encapsulation Mechanism (KEM) and Digital Signatures (DS). Our approach
adeptly confronts the challenges posed by quantum advancements. Utilizing the
Galois Permutation Group's matrix representations and inheriting its bijective
and non-commutative properties, QPP achieves quantum-secure symmetric key
encryption, seamlessly extending Shannon's perfect secrecy to both classical
and quantum-native systems. Meanwhile, HPPK, free from NP-hard problems,
fortifies symmetric encryption for the plain public key. It accomplishes this
by concealing the mathematical structure through modular multiplications or
arithmetic representations of Galois Permutation Group over hidden rings,
harnessing their partial homomorphic properties. This allows for secure
computation on encrypted data during secret encapsulations, bolstering the
security of the plain public key. The seamless integration of KEM and DS within
HPPK cryptography yields compact key, cipher, and signature sizes,
demonstrating exceptional performance. This paper organically unifies QPP and
HPPK under the Galois Permutation Group, marking a significant advancement in
laying the groundwork for quantum-resistant cryptographic protocols. Our
contribution propels the development of secure communication systems amid the
era of quantum computing.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01857" title="Abstract">arXiv:2402.01857</a> [<a href="/pdf/2402.01857" title="Download PDF">pdf</a>, <a href="/format/2402.01857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Position Paper: Assessing Robustness, Privacy, and Fairness in Federated  Learning Integrated with Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xi Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaqi Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Federated Learning (FL), while a breakthrough in decentralized machine
learning, contends with significant challenges such as limited data
availability and the variability of computational resources, which can stifle
the performance and scalability of the models. The integration of Foundation
Models (FMs) into FL presents a compelling solution to these issues, with the
potential to enhance data richness and reduce computational demands through
pre-training and data augmentation. However, this incorporation introduces
novel issues in terms of robustness, privacy, and fairness, which have not been
sufficiently addressed in the existing research. We make a preliminary
investigation into this field by systematically evaluating the implications of
FM-FL integration across these dimensions. We analyze the trade-offs involved,
uncover the threats and issues introduced by this integration, and propose a
set of criteria and strategies for navigating these challenges. Furthermore, we
identify potential research directions for advancing this field, laying a
foundation for future development in creating reliable, secure, and equitable
FL systems.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01858" title="Abstract">arXiv:2402.01858</a> [<a href="/pdf/2402.01858" title="Download PDF">pdf</a>, <a href="/format/2402.01858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explaining latent representations of generative models with large  multimodal models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Mengdan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenke Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+B">Bo Pan</a>, 
<a href="/search/cs?searchtype=author&query=Angirekula%2C+A">Abhinav Angirekula</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Learning interpretable representations of data generative latent factors is
an important topic for the development of artificial intelligence. With the
rise of the large multimodal model, it can align images with text to generate
answers. In this work, we propose a framework to comprehensively explain each
latent factor in the generative models using a large multimodal model. We
further measure the uncertainty of our generated explanations, quantitatively
evaluate the performance of explanation generation among multiple large
multimodal models, and qualitatively visualize the variations of each latent
factor to learn the disentanglement effects of different generative models on
explanations. Finally, we discuss the explanatory capabilities and limitations
of state-of-the-art large multimodal models.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01860" title="Abstract">arXiv:2402.01860</a> [<a href="/pdf/2402.01860" title="Download PDF">pdf</a>, <a href="/ps/2402.01860" title="Download PostScript">ps</a>, <a href="/format/2402.01860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Outlier Accommodation for GNSS Precise Point Positioning using  Risk-Averse State Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hu%2C+W">Wang Hu</a>, 
<a href="/search/eess?searchtype=author&query=Uwineza%2C+J">Jean-Bernard Uwineza</a>, 
<a href="/search/eess?searchtype=author&query=Farrell%2C+J+A">Jay A. Farrell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages,2 figures, Accepted by 2024 American Control Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In the realm of intelligent transportation systems, reliable and precise
absolute Connected Automated Vehicles (CAV) positioning is necessary. Global
Navigation Satellite Systems (GNSS) provides the foundation for absolute
positioning. Recently enhanced Precise Point Positioning (PPP) GNSS, now
offering corrections for multi-GNSS on a global scale, offers the potential for
PPP GNSS to achieve the accuracy suitable for real-time CAV applications.
However, in obstructed sky conditions GNSS signals are often affected by
outliers; therefore, addressing outliers is crucial. In multi-GNSS
applications, there are many more measurements available than are required to
meet the specification, if outlier measurements can be avoided; therefore,
measurement selection is important. The recently developed Risk-Averse
Performance-Specified (RAPS) state estimation optimally selects measurements to
minimize outlier risk while meeting a positive semi-definite constraint on
performance; at present, the existing solution methods are not suitable for
real-time computation and have not been demonstrated using challenging
real-world data or in Real-time PPP (RT-PPP) applications. This article makes
contributions in a few directions. First, it uses a diagonal performance
specification, which reduces computational costs relative to the positive
semidefinite constraint. Second, this article considers multi-GNSS RT-PPP
applications. Third, the experiments use real-world GNSS data collected in
challenging environments. The RT-PPP experimental results show that among the
compared methods: all achieve comparable performance in open-sky conditions,
and all exceed the Society of Automotive Engineers (SAE) specification;
however, in challenging environments, the diagonal RAPS approach shows
improvement of 6-19% over traditional methods. Throughout, RAPS achieves the
lowest outlier risk.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01862" title="Abstract">arXiv:2402.01862</a> [<a href="/pdf/2402.01862" title="Download PDF">pdf</a>, <a href="/format/2402.01862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parametric Feature Transfer: One-shot Federated Learning with Foundation  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beitollahi%2C+M">Mahdi Beitollahi</a>, 
<a href="/search/cs?searchtype=author&query=Bie%2C+A">Alex Bie</a>, 
<a href="/search/cs?searchtype=author&query=Hemati%2C+S">Sobhan Hemati</a>, 
<a href="/search/cs?searchtype=author&query=Brunswic%2C+L+M">Leo Maxime Brunswic</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xu Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guojun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In one-shot federated learning (FL), clients collaboratively train a global
model in a single round of communication. Existing approaches for one-shot FL
enhance communication efficiency at the expense of diminished accuracy. This
paper introduces FedPFT (Federated Learning with Parametric Feature Transfer),
a methodology that harnesses the transferability of foundation models to
enhance both accuracy and communication efficiency in one-shot FL. The approach
involves transferring per-client parametric models (specifically, Gaussian
mixtures) of features extracted from foundation models. Subsequently, each
parametric model is employed to generate synthetic features for training a
classifier head. Experimental results on eight datasets demonstrate that FedPFT
enhances the communication-accuracy frontier in both centralized and
decentralized FL scenarios, as well as across diverse data-heterogeneity
settings such as covariate shift and task shift, with improvements of up to
20.6%. Additionally, FedPFT adheres to the data minimization principle of FL,
as clients do not send real features. We demonstrate that sending real features
is vulnerable to potent reconstruction attacks. Moreover, we show that FedPFT
is amenable to formal privacy guarantees via differential privacy,
demonstrating favourable privacy-accuracy tradeoffs.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01863" title="Abstract">arXiv:2402.01863</a> [<a href="/pdf/2402.01863" title="Download PDF">pdf</a>, <a href="/format/2402.01863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DFML: Decentralized Federated Mutual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khalil%2C+Y+H">Yasser H. Khalil</a>, 
<a href="/search/cs?searchtype=author&query=Estiri%2C+A+H">Amir H. Estiri</a>, 
<a href="/search/cs?searchtype=author&query=Beitollahi%2C+M">Mahdi Beitollahi</a>, 
<a href="/search/cs?searchtype=author&query=Asadi%2C+N">Nader Asadi</a>, 
<a href="/search/cs?searchtype=author&query=Hemati%2C+S">Sobhan Hemati</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xu Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guojun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the realm of real-world devices, centralized servers in Federated Learning
(FL) present challenges including communication bottlenecks and susceptibility
to a single point of failure. Additionally, contemporary devices inherently
exhibit model and data heterogeneity. Existing work lacks a Decentralized FL
(DFL) framework capable of accommodating such heterogeneity without imposing
architectural restrictions or assuming the availability of public data. To
address these issues, we propose a Decentralized Federated Mutual Learning
(DFML) framework that is serverless, supports nonrestrictive heterogeneous
models, and avoids reliance on public data. DFML effectively handles model and
data heterogeneity through mutual learning, which distills knowledge between
clients, and cyclically varying the amount of supervision and distillation
signals. Extensive experimental results demonstrate consistent effectiveness of
DFML in both convergence speed and global accuracy, outperforming prevalent
baselines under various conditions. For example, with the CIFAR-100 dataset and
50 clients, DFML achieves a substantial increase of +17.20% and +19.95% in
global accuracy under Independent and Identically Distributed (IID) and non-IID
data shifts, respectively.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01864" title="Abstract">arXiv:2402.01864</a> [<a href="/pdf/2402.01864" title="Download PDF">pdf</a>, <a href="/format/2402.01864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> (A)I Am Not a Lawyer, But...: Engaging Legal Experts towards Responsible  LLM Policies for Legal Advice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheong%2C+I">Inyoung Cheong</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+K">King Xia</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+K+J+K">K.J. Kevin Feng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q+Z">Quan Ze Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A+X">Amy X. Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The rapid proliferation of large language models (LLMs) as general purpose
chatbots available to the public raises hopes around expanding access to
professional guidance in law, medicine, and finance, while triggering concerns
about public reliance on LLMs for high-stakes circumstances. Prior research has
speculated on high-level ethical considerations but lacks concrete criteria
determining when and why LLM chatbots should or should not provide professional
assistance. Through examining the legal domain, we contribute a structured
expert analysis to uncover nuanced policy considerations around using LLMs for
professional advice, using methods inspired by case-based reasoning. We
convened workshops with 20 legal experts and elicited dimensions on appropriate
AI assistance for sample user queries (``cases''). We categorized our expert
dimensions into: (1) user attributes, (2) query characteristics, (3) AI
capabilities, and (4) impacts. Beyond known issues like hallucinations, experts
revealed novel legal problems, including that users' conversations with LLMs
are not protected by attorney-client confidentiality or bound to professional
ethics that guard against conflicted counsel or poor quality advice. This
accountability deficit led participants to advocate for AI systems to help
users polish their legal questions and relevant facts, rather than recommend
specific actions. More generally, we highlight the potential of case-based
expert deliberation as a method of responsibly translating professional
integrity and domain knowledge into design requirements to inform appropriate
AI behavior when generating advice in professional domains.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01865" title="Abstract">arXiv:2402.01865</a> [<a href="/pdf/2402.01865" title="Download PDF">pdf</a>, <a href="/format/2402.01865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Will My Model Forget? Forecasting Forgotten Examples in Language  Model Refinement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xisen Jin</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xiang Ren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Machine Learning (stat.ML)

</div>
<p class="mathjax">Language models deployed in the wild make errors. However, simply updating
the model with the corrected error instances causes catastrophic forgetting --
the updated model makes errors on instances learned during the instruction
tuning or upstream training phase. Randomly replaying upstream data yields
unsatisfactory performance and often comes with high variance and poor
controllability. To this end, we try to forecast upstream examples that will be
forgotten due to a model update for improved controllability of the replay
process and interpretability. We train forecasting models given a collection of
online learned examples and corresponding forgotten upstream pre-training
examples. We propose a partially interpretable forecasting model based on the
observation that changes in pre-softmax logit scores of pretraining examples
resemble that of online learned examples, which performs decently on BART but
fails on T5 models. We further show a black-box classifier based on inner
products of example representations achieves better forecasting performance
over a series of setups. Finally, we show that we reduce forgetting of upstream
pretraining examples by replaying examples that are forecasted to be forgotten,
demonstrating the practical utility of forecasting example forgetting.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01867" title="Abstract">arXiv:2402.01867</a> [<a href="/pdf/2402.01867" title="Download PDF">pdf</a>, <a href="/format/2402.01867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Large Language Models for Structure Learning in Prompted Weak  Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jinyan Su</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P">Peilin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jieyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bach%2C+S+H">Stephen H. Bach</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE International Conference on Big Data 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Prompted weak supervision (PromptedWS) applies pre-trained large language
models (LLMs) as the basis for labeling functions (LFs) in a weak supervision
framework to obtain large labeled datasets. We further extend the use of LLMs
in the loop to address one of the key challenges in weak supervision: learning
the statistical dependency structure among supervision sources. In this work,
we ask the LLM how similar are these prompted LFs. We propose a Structure
Refining Module, a simple yet effective first approach based on the
similarities of the prompts by taking advantage of the intrinsic structure in
the embedding space. At the core of Structure Refining Module are Labeling
Function Removal (LaRe) and Correlation Structure Generation (CosGen). Compared
to previous methods that learn the dependencies from weak labels, our method
finds the dependencies which are intrinsic to the LFs and less dependent on the
data. We show that our Structure Refining Module improves the PromptedWS
pipeline by up to 12.7 points on the benchmark tasks. We also explore the
trade-offs between efficiency and performance with comprehensive ablation
experiments and analysis. Code for this project can be found in
https://github.com/BatsResearch/su-bigdata23-code.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01868" title="Abstract">arXiv:2402.01868</a> [<a href="/pdf/2402.01868" title="Download PDF">pdf</a>, <a href="/format/2402.01868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Challenges in Training PINNs: A Loss Landscape Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rathore%2C+P">Pratik Rathore</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+W">Weimu Lei</a>, 
<a href="/search/cs?searchtype=author&query=Frangella%2C+Z">Zachary Frangella</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Lu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Udell%2C+M">Madeleine Udell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages (including appendices), 8 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">This paper explores challenges in training Physics-Informed Neural Networks
(PINNs), emphasizing the role of the loss landscape in the training process. We
examine difficulties in minimizing the PINN loss function, particularly due to
ill-conditioning caused by differential operators in the residual term. We
compare gradient-based optimizers Adam, L-BFGS, and their combination
Adam+L-BFGS, showing the superiority of Adam+L-BFGS, and introduce a novel
second-order optimizer, NysNewton-CG (NNCG), which significantly improves PINN
performance. Theoretically, our work elucidates the connection between
ill-conditioned differential operators and ill-conditioning in the PINN loss
and shows the benefits of combining first- and second-order optimization
methods. Our work presents valuable insights and more powerful optimization
strategies for training PINNs, which could improve the utility of PINNs for
solving difficult partial differential equations.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01869" title="Abstract">arXiv:2402.01869</a> [<a href="/pdf/2402.01869" title="Download PDF">pdf</a>, <a href="/format/2402.01869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> APIServe: Efficient API Support for Large-Language Model Inferencing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abhyankar%2C+R">Reyna Abhyankar</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zijian He</a>, 
<a href="/search/cs?searchtype=author&query=Srivatsa%2C+V">Vikranth Srivatsa</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yiying Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Large language models are increasingly integrated with external tools and
APIs like ChatGPT plugins to extend their capability beyond language-centric
tasks. However, today's LLM inference systems are designed for standalone LLMs.
They treat API calls as new requests, causing unnecessary recomputation of
already computed contexts, which accounts for 37-40% of total model forwarding
time. This paper presents APIServe, the first LLM inference framework targeting
API-augmented LLMs. APISERVE minimizes the GPU resource waste caused by API
calls and dedicates saved memory for serving more requests. APISERVE improves
the overall serving throughput by 1.6x and completes 2x more requests per
second compared to the state-of-the-art LLM inference systems.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01874" title="Abstract">arXiv:2402.01874</a> [<a href="/pdf/2402.01874" title="Download PDF">pdf</a>, <a href="/format/2402.01874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement  Learning and Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pternea%2C+M">Moschoula Pternea</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+P">Prerna Singh</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+A">Abir Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Oruganti%2C+Y">Yagna Oruganti</a>, 
<a href="/search/cs?searchtype=author&query=Milletari%2C+M">Mirco Milletari</a>, 
<a href="/search/cs?searchtype=author&query=Bapat%2C+S">Sayli Bapat</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+K">Kebei Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages (including bibliography), 1 figure, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">In this work, we review research studies that combine Reinforcement Learning
(RL) and Large Language Models (LLMs), two areas that owe their momentum to the
development of deep neural networks. We propose a novel taxonomy of three main
classes based on the way that the two model types interact with each other. The
first class, RL4LLM, includes studies where RL is leveraged to improve the
performance of LLMs on tasks related to Natural Language Processing. L4LLM is
divided into two sub-categories depending on whether RL is used to directly
fine-tune an existing LLM or to improve the prompt of the LLM. In the second
class, LLM4RL, an LLM assists the training of an RL model that performs a task
that is not inherently related to natural language. We further break down
LLM4RL based on the component of the RL training framework that the LLM assists
or replaces, namely reward shaping, goal generation, and policy function.
Finally, in the third class, RL+LLM, an LLM and an RL agent are embedded in a
common planning framework without either of them contributing to training or
fine-tuning of the other. We further branch this class to distinguish between
studies with and without natural language feedback. We use this taxonomy to
explore the motivations behind the synergy of LLMs and RL and explain the
reasons for its success, while pinpointing potential shortcomings and areas
where further research is needed, as well as alternative methodologies that
serve the same goal.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01875" title="Abstract">arXiv:2402.01875</a> [<a href="/pdf/2402.01875" title="Download PDF">pdf</a>, <a href="/format/2402.01875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $hp$-FEM for Elastoplasticity &amp; $hp$-Adaptivity Based on Local Error  Reductions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bammer%2C+P">Patrick Bammer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PhD thesis (corrected version without paper-PDFs), 52 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The first part of the cumulative thesis contains the numerical analysis of
different $hp$-finite element discretizations related to two different weak
formulations of a model problem in elastoplasticity with linearly kinematic
hardening. Thereby, the weak formulation either takes the form of a variational
inequality of the second kind, including a non-differentiable plasticity
functional, or represents a mixed formulation, in which the non-smooth
plasticity functional is resolved by a Lagrange multiplier. As the
non-differentiability of the plasticity functional causes many difficulties in
the numerical analysis and the computation of a discrete solution it seems
advantageous to consider discretizations of the mixed formulation. In a first
work, an a priori error analysis of an higher-order finite element
discretization of the mixed formulation (explicitly including the
discretization of the Lagrange multiplier) is presented. The relations between
the three different $hp$-discretizations are studied in a second work where
also a reliable a posteriori error estimator that also satisfies some (local)
efficiency estimates is derived. In a third work, an efficient semi-smooth
Newton solver is proposed, which is obtained by reformulating a discretization
of the mixed formulation as a system of decoupled nonlinear equations. The
second part of the thesis introduces a new $hp$-adaptive algorithm for solving
variational equations, in which the automatic mesh refinement does not rely on
the use of an a posteriori error estimator or smoothness indicators but is
based on comparing locally predicted error reductions.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01877" title="Abstract">arXiv:2402.01877</a> [<a href="/pdf/2402.01877" title="Download PDF">pdf</a>, <a href="/format/2402.01877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mobile Fitting Room: On-device Virtual Try-on via Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blalock%2C+J">Justin Blalock</a>, 
<a href="/search/cs?searchtype=author&query=Munechika%2C+D">David Munechika</a>, 
<a href="/search/cs?searchtype=author&query=Karanth%2C+H">Harsha Karanth</a>, 
<a href="/search/cs?searchtype=author&query=Helbling%2C+A">Alec Helbling</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+P">Pratham Mehta</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seongmin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Chau%2C+D+H">Duen Horng Chau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The growing digital landscape of fashion e-commerce calls for interactive and
user-friendly interfaces for virtually trying on clothes. Traditional try-on
methods grapple with challenges in adapting to diverse backgrounds, poses, and
subjects. While newer methods, utilizing the recent advances of diffusion
models, have achieved higher-quality image generation, the human-centered
dimensions of mobile interface delivery and privacy concerns remain largely
unexplored. We present Mobile Fitting Room, the first on-device diffusion-based
virtual try-on system. To address multiple inter-related technical challenges
such as high-quality garment placement and model compression for mobile
devices, we present a novel technical pipeline and an interface design that
enables privacy preservation and user customization. A usage scenario
highlights how our tool can provide a seamless, interactive virtual try-on
experience for customers and provide a valuable service for fashion e-commerce
businesses.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01878" title="Abstract">arXiv:2402.01878</a> [<a href="/pdf/2402.01878" title="Download PDF">pdf</a>, <a href="/format/2402.01878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LiPO: Listwise Preference Optimization through Learning-to-Rank
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zhen Qin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Junru Wu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jiaming Shen</a>, 
<a href="/search/cs?searchtype=author&query=Khalman%2C+M">Misha Khalman</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+R">Rishabh Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Saleh%2C+M">Mohammad Saleh</a>, 
<a href="/search/cs?searchtype=author&query=Baumgartner%2C+S">Simon Baumgartner</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jialu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P+J">Peter J. Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuanhui Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Aligning language models (LMs) with curated human feedback is critical to
control their behaviors in real-world applications. Several recent policy
optimization methods, such as DPO and SLiC, serve as promising alternatives to
the traditional Reinforcement Learning from Human Feedback (RLHF) approach. In
practice, human feedback often comes in a format of a ranked list over multiple
responses to amortize the cost of reading prompt. Multiple responses can also
be ranked by reward models or AI feedback. There lacks such a study on directly
fitting upon a list of responses. In this work, we formulate the LM alignment
as a listwise ranking problem and describe the Listwise Preference Optimization
(LiPO) framework, where the policy can potentially learn more effectively from
a ranked list of plausible responses given the prompt. This view draws an
explicit connection to Learning-to-Rank (LTR), where most existing preference
optimization work can be mapped to existing ranking objectives, especially
pairwise ones. Following this connection, we provide an examination of ranking
objectives that are not well studied for LM alignment withDPO and SLiC as
special cases when list size is two. In particular, we highlight a specific
method, LiPO-{\lambda}, which leverages a state-of-the-art listwise ranking
objective and weights each preference pair in a more advanced manner. We show
that LiPO-{\lambda} can outperform DPO and SLiC by a clear margin on two
preference alignment tasks.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01879" title="Abstract">arXiv:2402.01879</a> [<a href="/pdf/2402.01879" title="Download PDF">pdf</a>, <a href="/format/2402.01879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $&#x3c3;$-zero: Gradient-based Optimization of $\ell_0$-norm Adversarial  Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cin%C3%A0%2C+A+E">Antonio Emanuele Cin&#xe0;</a>, 
<a href="/search/cs?searchtype=author&query=Villani%2C+F">Francesco Villani</a>, 
<a href="/search/cs?searchtype=author&query=Pintor%2C+M">Maura Pintor</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6nherr%2C+L">Lea Sch&#xf6;nherr</a>, 
<a href="/search/cs?searchtype=author&query=Biggio%2C+B">Battista Biggio</a>, 
<a href="/search/cs?searchtype=author&query=Pelillo%2C+M">Marcello Pelillo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code available at <a href="https://github.com/Cinofix/sigma-zero-adversarial-attack">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Evaluating the adversarial robustness of deep networks to gradient-based
attacks is challenging. While most attacks consider $\ell_2$- and
$\ell_\infty$-norm constraints to craft input perturbations, only a few
investigate sparse $\ell_1$- and $\ell_0$-norm attacks. In particular,
$\ell_0$-norm attacks remain the least studied due to the inherent complexity
of optimizing over a non-convex and non-differentiable constraint. However,
evaluating adversarial robustness under these attacks could reveal weaknesses
otherwise left untested with more conventional $\ell_2$- and $\ell_\infty$-norm
attacks. In this work, we propose a novel $\ell_0$-norm attack, called
$\sigma$-zero, which leverages an ad hoc differentiable approximation of the
$\ell_0$ norm to facilitate gradient-based optimization, and an adaptive
projection operator to dynamically adjust the trade-off between loss
minimization and perturbation sparsity. Extensive evaluations using MNIST,
CIFAR10, and ImageNet datasets, involving robust and non-robust models, show
that $\sigma$-zero finds minimum $\ell_0$-norm adversarial examples without
requiring any time-consuming hyperparameter tuning, and that it outperforms all
competing sparse attacks in terms of success rate, perturbation size, and
scalability.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01881" title="Abstract">arXiv:2402.01881</a> [<a href="/pdf/2402.01881" title="Download PDF">pdf</a>, <a href="/format/2402.01881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model Agent for Hyper-Parameter Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Siyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chen Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Hyperparameter optimization is critical in modern machine learning, requiring
expert knowledge, numerous trials, and high computational and human resources.
Despite the advancements in Automated Machine Learning (AutoML), challenges in
terms of trial efficiency, setup complexity, and interoperability still
persist. To address these issues, we introduce a novel paradigm leveraging
Large Language Models (LLMs) to automate hyperparameter optimization across
diverse machine learning tasks, which is named AgentHPO (short for LLM
Agent-based Hyperparameter Optimization). Specifically, AgentHPO processes the
task information autonomously, conducts experiments with specific
hyperparameters (HPs), and iteratively optimizes them based on historical
trials. This human-like optimization process largely reduces the number of
required trials, simplifies the setup process, and enhances interpretability
and user trust, compared to traditional AutoML methods. Extensive empirical
experiments conducted on 12 representative machine-learning tasks indicate that
AgentHPO not only matches but also often surpasses the best human trials in
terms of performance while simultaneously providing explainable results.
Further analysis sheds light on the strategies employed by the LLM in
optimizing these tasks, highlighting its effectiveness and adaptability in
various scenarios.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01885" title="Abstract">arXiv:2402.01885</a> [<a href="/pdf/2402.01885" title="Download PDF">pdf</a>, <a href="/ps/2402.01885" title="Download PostScript">ps</a>, <a href="/format/2402.01885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Growth Mindset Practices in an Introductory Physical  Computing Classroom: High School Students&#x27; Engagement with Debugging by  Design Activities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morales-Navarro%2C+L">Luis Morales-Navarro</a>, 
<a href="/search/cs?searchtype=author&query=Fields%2C+D+A">Deborah A. Fields</a>, 
<a href="/search/cs?searchtype=author&query=Kafai%2C+Y+B">Yasmin B. Kafai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Physics Education (physics.ed-ph)

</div>
<p class="mathjax">Background and Context: While debugging is recognized as an essential
practice, for many students, encountering bugs can generate emotional responses
such as fear and anxiety that can lead to disengagement and the avoidance of
computer programming. Growth mindsets can support perseverance and learning in
these situations, yet few studies have investigated how growth mindsets emerge
in practice amongst K-12 computing students facing physical computing debugging
challenges. Objective: We seek to understand what (if any) growth mindset
practices high school students exhibited when creating and exchanging buggy
physical computing projects for their peers to solve during a Debugging by
Design activity as part of their introductory computing course. Method: We
focused on moment-to-moment microgenetic analysis of student interactions in
designing and solving bugs for others to examine the practices students
exhibited that demonstrated the development of a growth mindset and the
contexts in which these practices emerged. Findings: We identified five
emergent growth mindset practices: choosing challenges that lead to more
learning, persisting after setbacks, giving and valuing praise for effort,
approaching learning as constant improvement, and developing comfort with
failure. Students most often exhibited these practices in peer-to-peer
interactions and while making buggy physical computing projects for their peers
to solve. Implications: Our analysis contributes to a more holistic
understanding of students' social, emotional, and motivational approaches to
debugging physical computing projects through the characterization of growth
mindset practices. The presented inventory of growth mindset practices may be
helpful to further study growth mindset in action in other computing settings.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01886" title="Abstract">arXiv:2402.01886</a> [<a href="/pdf/2402.01886" title="Download PDF">pdf</a>, <a href="/format/2402.01886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inverse Reinforcement Learning by Estimating Expertise of Demonstrators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beliaev%2C+M">Mark Beliaev</a>, 
<a href="/search/cs?searchtype=author&query=Pedarsani%2C+R">Ramtin Pedarsani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 3 figures, preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In Imitation Learning (IL), utilizing suboptimal and heterogeneous
demonstrations presents a substantial challenge due to the varied nature of
real-world data. However, standard IL algorithms consider these datasets as
homogeneous, thereby inheriting the deficiencies of suboptimal demonstrators.
Previous approaches to this issue typically rely on impractical assumptions
like high-quality data subsets, confidence rankings, or explicit environmental
knowledge. This paper introduces IRLEED, Inverse Reinforcement Learning by
Estimating Expertise of Demonstrators, a novel framework that overcomes these
hurdles without prior knowledge of demonstrator expertise. IRLEED enhances
existing Inverse Reinforcement Learning (IRL) algorithms by combining a general
model for demonstrator suboptimality to address reward bias and action
variance, with a Maximum Entropy IRL framework to efficiently derive the
optimal policy from diverse, suboptimal demonstrations. Experiments in both
online and offline IL settings, with simulated and human-generated data,
demonstrate IRLEED's adaptability and effectiveness, making it a versatile
solution for learning from suboptimal demonstrations.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01889" title="Abstract">arXiv:2402.01889</a> [<a href="/pdf/2402.01889" title="Download PDF">pdf</a>, <a href="/format/2402.01889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Role of Foundation Models in Neuro-Symbolic Learning and Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cunnington%2C+D">Daniel Cunnington</a>, 
<a href="/search/cs?searchtype=author&query=Law%2C+M">Mark Law</a>, 
<a href="/search/cs?searchtype=author&query=Lobo%2C+J">Jorge Lobo</a>, 
<a href="/search/cs?searchtype=author&query=Russo%2C+A">Alessandra Russo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Neuro-Symbolic AI (NeSy) holds promise to ensure the safe deployment of AI
systems, as interpretable symbolic techniques provide formal behaviour
guarantees. The challenge is how to effectively integrate neural and symbolic
computation, to enable learning and reasoning from raw data. Existing pipelines
that train the neural and symbolic components sequentially require extensive
labelling, whereas end-to-end approaches are limited in terms of scalability,
due to the combinatorial explosion in the symbol grounding problem. In this
paper, we leverage the implicit knowledge within foundation models to enhance
the performance in NeSy tasks, whilst reducing the amount of data labelling and
manual engineering. We introduce a new architecture, called NeSyGPT, which
fine-tunes a vision-language foundation model to extract symbolic features from
raw data, before learning a highly expressive answer set program to solve a
downstream task. Our comprehensive evaluation demonstrates that NeSyGPT has
superior accuracy over various baselines, and can scale to complex NeSy tasks.
Finally, we highlight the effective use of a large language model to generate
the programmatic interface between the neural and symbolic components,
significantly reducing the amount of manual engineering required.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01893" title="Abstract">arXiv:2402.01893</a> [<a href="/pdf/2402.01893" title="Download PDF">pdf</a>, <a href="/format/2402.01893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Surface Reconstruction Using Rotation Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+R">Ruiqi Cui</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%A6de%2C+E+T">Emil Toftegaard G&#xe6;de</a>, 
<a href="/search/cs?searchtype=author&query=Rotenberg%2C+E">Eva Rotenberg</a>, 
<a href="/search/cs?searchtype=author&query=Kobbelt%2C+L">Leif Kobbelt</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%A6rentzen%2C+J+A">J. Andreas B&#xe6;rentzen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Inspired by the seminal result that a graph and an associated rotation system
uniquely determine the topology of a closed manifold, we propose a
combinatorial method for reconstruction of surfaces from points. Our method
constructs a spanning tree and a rotation system. Since the tree is trivially a
planar graph, its rotation system determines a genus zero surface with a single
face which we proceed to incrementally refine by inserting edges to split faces
and thus merging them. In order to raise the genus, special handles are added
by inserting edges between different faces and thus merging them. We apply our
method to a wide range of input point clouds in order to investigate its
effectiveness, and we compare our method to several other surface
reconstruction methods. We find that our method offers better control over
outlier classification, i.e. which points to include in the reconstructed
surface, and also more control over the topology of the reconstructed surface.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01894" title="Abstract">arXiv:2402.01894</a> [<a href="/pdf/2402.01894" title="Download PDF">pdf</a>, <a href="/format/2402.01894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> S2malloc: Statistically Secure Allocator for Use-After-Free Protection  And More
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruizhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Meng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Asokan%2C+N">N. Asokan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Attacks on heap memory, encompassing memory overflow, double and invalid
free, use-after-free (UAF), and various heap spraying techniques are
ever-increasing. Existing entropy-based secure memory allocators provide
statistical defenses against virtually all of these attack vectors. Although
they claim protections against UAF attacks, their designs are not tailored to
detect (failed) attempts. Consequently, to beat this entropy-based protection,
an attacker can simply launch the same attack repeatedly with the potential use
of heap spraying to further improve their chance of success.
<br />We introduce S2malloc, aiming to enhance UAF-attempt detection without
compromising other security guarantees or introducing significant performance
overhead. To achieve this, we use three innovative constructs in secure
allocator design: free block canaries (FBC) to detect UAF attempts, random
in-block offset (RIO) to stop the attacker from accurately overwriting the
victim object, and random bag layout (RBL) to impede attackers from estimating
the block size based on its address.
<br />We show that (a) by reserving 25% of the object size for the RIO offset, an
8-byte canary offers a 69% protection rate if the attacker reuses the same
pointer and 96% protection rate if the attacker does not, against UAF
exploitation attempts targeting a 64 bytes object, with equal or higher
security guarantees against all other attacks; and (b) S2malloc is practical,
with only a 2.8% run-time overhead on PARSEC and an 11.5% overhead on SPEC.
Compared to state-of-the-art entropy-based allocators, S2malloc improves
UAF-protection without incurring additional performance overhead. Compared to
UAF-mitigating allocators, S2malloc trades off a minuscule probability of
failed protection for significantly lower overhead.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01895" title="Abstract">arXiv:2402.01895</a> [<a href="/pdf/2402.01895" title="Download PDF">pdf</a>, <a href="/ps/2402.01895" title="Download PostScript">ps</a>, <a href="/format/2402.01895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $L_q$ Lower Bounds on Distributed Estimation via Fisher Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei-Ning Chen</a>, 
<a href="/search/cs?searchtype=author&query=%C3%96zg%C3%BCr%2C+A">Ayfer &#xd6;zg&#xfc;r</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Statistics Theory (math.ST)

</div>
<p class="mathjax">Van Trees inequality, also known as the Bayesian Cram\'er-Rao lower bound, is
a powerful tool for establishing lower bounds for minimax estimation through
Fisher information. It easily adapts to different statistical models and often
yields tight bounds. Recently, its application has been extended to distributed
estimation with privacy and communication constraints where it yields
order-wise optimal minimax lower bounds for various parametric tasks under
squared $L_2$ loss.
<br />However, a widely perceived drawback of the van Trees inequality is that it
is limited to squared $L_2$ loss. The goal of this paper is to dispel that
perception by introducing a strengthened version of the van Trees inequality
that applies to general $L_q$ loss functions by building on the Efroimovich's
inequality -- a lesser-known entropic inequality dating back to the 1970s. We
then apply the generalized van Trees inequality to lower bound $L_q$ loss in
distributed minimax estimation under communication and local differential
privacy constraints. This leads to lower bounds for $L_q$ loss that apply to
sequentially interactive and blackboard communication protocols. Additionally,
we show how the generalized van Trees inequality can be used to obtain
\emph{local} and \emph{non-asymptotic} minimax results that capture the
hardness of estimating each instance at finite sample sizes.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01901" title="Abstract">arXiv:2402.01901</a> [<a href="/pdf/2402.01901" title="Download PDF">pdf</a>, <a href="/ps/2402.01901" title="Download PostScript">ps</a>, <a href="/format/2402.01901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unitary Owen points in cooperative lot-sizing models with backlogging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guardiola%2C+L+A">Luis A. Guardiola</a>, 
<a href="/search/cs?searchtype=author&query=Meca%2C+A">Ana Meca</a>, 
<a href="/search/cs?searchtype=author&query=Puerto%2C+J">Justo Puerto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Cooperative lot-sizing models with backlogging and heterogeneous costs are
studied in Guardiola et al. (2020). In this model several firms participate in
a consortium aiming at satisfying their demand over the planing horizon with
minimal operation cost. Each firm uses the best ordering channel and holding
technology provided by the participants in the consortium. The authors show
that there are always fair allocations of the overall operation cost among the
firms so that no group of agents profit from leaving the consortium. This paper
revisits those cooperative lot-sizing models and presents a new family of cost
allocations, the unitary Owen points. This family is an extension of the Owen
set which enjoys very good properties in production-inventory proble,
introduced by Guardiola et al. (2008). Necessary and sufficient conditions are
provided for the unitary Owen points to be fair allocations. In addition, we
provide empirical evidence, throughout simulation, showing that the above
condition is fulfilled in most cases. Additionally, a relationship between
lot-sizing games and a certain family of production-inventory games, through
Owen's points of the latter, is described. This interesting relationship
enables to easily construct a variety of fair allocations for cooperative
lot-sizing models.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01902" title="Abstract">arXiv:2402.01902</a> [<a href="/pdf/2402.01902" title="Download PDF">pdf</a>, <a href="/format/2402.01902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EBV: Electronic Bee-Veterinarian for Principled Mining and Forecasting  of Honeybee Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hossain%2C+M+S">Mst. Shamima Hossain</a>, 
<a href="/search/cs?searchtype=author&query=Faloutsos%2C+C">Christos Faloutsos</a>, 
<a href="/search/cs?searchtype=author&query=Baer%2C+B">Boris Baer</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyoseung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Tsotras%2C+V+J">Vassilis J. Tsotras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 7 figure, Accepted at 2024 SIAM International Conference on Data Mining (SDM'24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Honeybees are vital for pollination and food production. Among many factors,
extreme temperature (e.g., due to climate change) is particularly dangerous for
bee health. Anticipating such extremities would allow beekeepers to take early
preventive action. Thus, given sensor (temperature) time series data from
beehives, how can we find patterns and do forecasting? Forecasting is crucial
as it helps spot unexpected behavior and thus issue warnings to the beekeepers.
In that case, what are the right models for forecasting? ARIMA, RNNs, or
something else?
<br />We propose the EBV (Electronic Bee-Veterinarian) method, which has the
following desirable properties: (i) principled: it is based on a) diffusion
equations from physics and b) control theory for feedback-loop controllers;
(ii) effective: it works well on multiple, real-world time sequences, (iii)
explainable: it needs only a handful of parameters (e.g., bee strength) that
beekeepers can easily understand and trust, and (iv) scalable: it performs
linearly in time. We applied our method to multiple real-world time sequences,
and found that it yields accurate forecasting (up to 49% improvement in RMSE
compared to baselines), and segmentation. Specifically, discontinuities
detected by EBV mostly coincide with domain expert's opinions, showcasing our
approach's potential and practical feasibility. Moreover, EBV is scalable and
fast, taking about 20 minutes on a stock laptop for reconstructing two months
of sensor data.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01905" title="Abstract">arXiv:2402.01905</a> [<a href="/pdf/2402.01905" title="Download PDF">pdf</a>, <a href="/format/2402.01905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Carthago Delenda Est: Co-opetitive Indirect Information Diffusion Model  for Influence Operations on Online Social Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Low%2C+J+F">Jwen Fai Low</a>, 
<a href="/search/cs?searchtype=author&query=Fung%2C+B+C+M">Benjamin C. M. Fung</a>, 
<a href="/search/cs?searchtype=author&query=Iqbal%2C+F">Farkhund Iqbal</a>, 
<a href="/search/cs?searchtype=author&query=Fachkha%2C+C">Claude Fachkha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">For a state or non-state actor whose credibility is bankrupt, relying on bots
to conduct non-attributable, non-accountable, and influence/information
operations (info ops) that are grassroots in appearance but decentralized in
actuality on social media can help circumvent the issue of trust deficit while
advancing its interests. Planning and/or defending against decentralized info
ops can be aided by computational simulations in lieu of ethically-fraught live
experiments on social media. In this study, we introduce \textit{Diluvsion}, an
agent-based model for contested information propagation efforts on Twitter-like
social media. The model emphasizes a user's belief in an opinion (stance) being
impacted by the perception of potentially illusory popular support from
constant incoming floods of indirect information, floods that can be
cooperatively engineered in an uncoordinated manner by bots as they compete to
spread their stances. Our model, which has been validated against real-world
data, is an advancement over previous models because we account for engagement
metrics in influencing stance adoption, non-social tie spreading of
information, neutrality as a stance that can be spread, and themes that are
analogous to media's framing effect and are symbiotic with respect to stance
propagation. The strengths of the \textit{Diluvsion} model are demonstrated in
simulations of orthodox info ops, e.g., maximizing adoption of one stance;
creating echo chambers; inducing polarization; and unorthodox info ops, e.g.,
simultaneous support of multiple stances as a Trojan horse tactic for the
dissemination of a theme.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01908" title="Abstract">arXiv:2402.01908</a> [<a href="/pdf/2402.01908" title="Download PDF">pdf</a>, <a href="/format/2402.01908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large language models cannot replace human participants because they  cannot portray identity groups
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">Angelina Wang</a>, 
<a href="/search/cs?searchtype=author&query=Morgenstern%2C+J">Jamie Morgenstern</a>, 
<a href="/search/cs?searchtype=author&query=Dickerson%2C+J+P">John P. Dickerson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Large language models (LLMs) are increasing in capability and popularity,
propelling their application in new domains -- including as replacements for
human participants in computational social science, user testing, annotation
tasks, and more. Traditionally, in all of these settings survey distributors
are careful to find representative samples of the human population to ensure
the validity of their results and understand potential demographic differences.
This means in order to be a suitable replacement, LLMs will need to be able to
capture the influence of positionality (i.e., relevance of social identities
like gender and race). However, we show that there are two inherent limitations
in the way current LLMs are trained that prevent this. We argue analytically
for why LLMs are doomed to both misportray and flatten the representations of
demographic groups, then empirically show this to be true on 4 LLMs through a
series of human studies with 3200 participants across 16 demographic
identities. We also discuss a third consideration about how identity prompts
can essentialize identities. Throughout, we connect each of these limitations
to a pernicious history that shows why each is harmful for marginalized
demographic groups. Overall, we urge caution in use cases where LLMs are
intended to replace human participants whose identities are relevant to the
task at hand. At the same time, in cases where the goal is to supplement rather
than replace (e.g., pilot studies), we provide empirically-better
inference-time techniques to reduce, but not remove, these harms.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01909" title="Abstract">arXiv:2402.01909</a> [<a href="/pdf/2402.01909" title="Download PDF">pdf</a>, <a href="/format/2402.01909" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Catastrophic Inheritance of Large Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+B">Bhiksha Raj</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large foundation models (LFMs) are claiming incredible performances. Yet
great concerns have been raised about their mythic and uninterpreted potentials
not only in machine learning, but also in various other disciplines. In this
position paper, we propose to identify a neglected issue deeply rooted in LFMs:
Catastrophic Inheritance, describing the weaknesses and limitations inherited
from biased large-scale pre-training data to behaviors of LFMs on the
downstream tasks, including samples that are corrupted, long-tailed, noisy,
out-of-distributed, to name a few. Such inheritance can potentially cause
catastrophes to downstream applications, such as bias, lack of generalization,
deteriorated performance, security vulnerability, privacy leakage, and value
misalignment. We discuss the challenges behind this issue and propose UIM, a
framework to Understand the catastrophic inheritance of LFMs from both
pre-training and downstream adaptation, Interpret the implications of
catastrophic inheritance on downstream tasks, and how to Mitigate it. UIM aims
to unite both the machine learning and social sciences communities for more
responsible and promising AI development and deployment.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01910" title="Abstract">arXiv:2402.01910</a> [<a href="/pdf/2402.01910" title="Download PDF">pdf</a>, <a href="/ps/2402.01910" title="Download PostScript">ps</a>, <a href="/format/2402.01910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring productivity in networks: A game-theoretic approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Allouch%2C+N">N. Allouch</a>, 
<a href="/search/cs?searchtype=author&query=Guardiola%2C+L+A">Luis A. Guardiola</a>, 
<a href="/search/cs?searchtype=author&query=Meca%2C+A">A. Meca</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Measuring individual productivity (or equivalently distributing the overall
productivity) in a network structure of workers displaying peer effects has
been a subject of ongoing interest in many areas ranging from academia to
industry. In this paper, we propose a novel approach based on cooperative game
theory that takes into account the peer effects of worker productivity
represented by a complete bipartite network of interactions. More specifically,
we construct a series of cooperative games where the characteristic function of
each coalition of workers is equal to the sum of each worker intrinsic
productivity as well as the productivity of other workers within a distance
discounted by an attenuation factor. We show that these (truncated) games are
balanced and converge to a balanced game when the distance of influence grows
large. We then provide an explicit formula for the Shapley value and propose an
alternative coalitionally stable distribution of productivity which is
computationally much more tractable than the Shapley value. Lastly, we
characterize this alternative distribution based on three sensible properties
of a logistic network. This analysis enhances our understanding of
game-theoretic analysis within logistics networks, offering valuable insights
into the peer effects' impact when assessing the overall productivity and its
distribution among workers.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01911" title="Abstract">arXiv:2402.01911</a> [<a href="/pdf/2402.01911" title="Download PDF">pdf</a>, <a href="/format/2402.01911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From PEFT to DEFT: Parameter Efficient Finetuning for Reducing  Activation Density in Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Runwal%2C+B">Bharat Runwal</a>, 
<a href="/search/cs?searchtype=author&query=Pedapati%2C+T">Tejaswini Pedapati</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pin-Yu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Pretrained Language Models (PLMs) have become the de facto starting point for
fine-tuning on downstream tasks. However, as model sizes continue to increase,
traditional fine-tuning of all parameters becomes challenging. To address this,
parameter-efficient fine-tuning (PEFT) methods have gained popularity as a
means to adapt PLMs effectively. In parallel, recent studies have revealed the
presence of activation sparsity within the intermediate outputs of the
multilayer perception (MLP) blocks in transformers. Low activation density
enables efficient model inference on sparsity-aware hardware. Building upon
this insight, in this work, we propose a novel density loss that encourages
higher activation sparsity (equivalently, lower activation density) in the
pre-trained models. We demonstrate the effectiveness of our approach by
utilizing mainstream PEFT techniques including QLoRA, LoRA, Adapter,
Prompt/Prefix Tuning to facilitate efficient model adaptation across diverse
downstream tasks. Experiments show that our proposed method DEFT,
Density-Efficient Fine-Tuning, can reduce the activation density consistently
and up to $\boldsymbol{50.72\%}$ on RoBERTa$_\mathrm{Large}$, and $\boldsymbol
{53.19\%}$ (encoder density) and $\boldsymbol{90.60\%}$ (decoder density) on
Flan-T5$_\mathrm{XXL}$ ($\boldsymbol{11B}$) compared to PEFT using GLUE and QA
(SQuAD) benchmarks respectively while maintaining competitive performance on
downstream tasks. We also showcase that DEFT works complementary with quantized
and pruned models
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01912" title="Abstract">arXiv:2402.01912</a> [<a href="/pdf/2402.01912" title="Download PDF">pdf</a>, <a href="/format/2402.01912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Natural language guidance of high-fidelity text-to-speech with synthetic  annotations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyth%2C+D">Dan Lyth</a>, 
<a href="/search/cs?searchtype=author&query=King%2C+S">Simon King</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Text-to-speech models trained on large-scale datasets have demonstrated
impressive in-context learning capabilities and naturalness. However, control
of speaker identity and style in these models typically requires conditioning
on reference speech recordings, limiting creative applications. Alternatively,
natural language prompting of speaker identity and style has demonstrated
promising results and provides an intuitive method of control. However,
reliance on human-labeled descriptions prevents scaling to large datasets.
<br />Our work bridges the gap between these two approaches. We propose a scalable
method for labeling various aspects of speaker identity, style, and recording
conditions. We then apply this method to a 45k hour dataset, which we use to
train a speech language model. Furthermore, we propose simple methods for
increasing audio fidelity, significantly outperforming recent work despite
relying entirely on found data.
<br />Our results demonstrate high-fidelity speech generation in a diverse range of
accents, prosodic styles, channel conditions, and acoustic conditions, all
accomplished with a single model and intuitive natural language conditioning.
Audio samples can be heard at https://text-description-to-speech.com/.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01913" title="Abstract">arXiv:2402.01913</a> [<a href="/pdf/2402.01913" title="Download PDF">pdf</a>, <a href="/format/2402.01913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TartanDrive 2.0: More Modalities and Better Infrastructure to Further  Self-Supervised Learning Research in Off-Road Driving Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sivaprakasam%2C+M">Matthew Sivaprakasam</a>, 
<a href="/search/cs?searchtype=author&query=Maheshwari%2C+P">Parv Maheshwari</a>, 
<a href="/search/cs?searchtype=author&query=Castro%2C+M+G">Mateo Guaman Castro</a>, 
<a href="/search/cs?searchtype=author&query=Triest%2C+S">Samuel Triest</a>, 
<a href="/search/cs?searchtype=author&query=Nye%2C+M">Micah Nye</a>, 
<a href="/search/cs?searchtype=author&query=Willits%2C+S">Steve Willits</a>, 
<a href="/search/cs?searchtype=author&query=Saba%2C+A">Andrew Saba</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenshan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Scherer%2C+S">Sebastian Scherer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We present TartanDrive 2.0, a large-scale off-road driving dataset for
self-supervised learning tasks. In 2021 we released TartanDrive 1.0, which is
one of the largest datasets for off-road terrain. As a follow-up to our
original dataset, we collected seven hours of data at speeds of up to 15m/s
with the addition of three new LiDAR sensors alongside the original camera,
inertial, GPS, and proprioceptive sensors. We also release the tools we use for
collecting, processing, and querying the data, including our metadata system
designed to further the utility of our data. Custom infrastructure allows end
users to reconfigure the data to cater to their own platforms. These tools and
infrastructure alongside the dataset are useful for a variety of tasks in the
field of off-road autonomy and, by releasing them, we encourage collaborative
data aggregation. These resources lower the barrier to entry to utilizing
large-scale datasets, thereby helping facilitate the advancement of robotics in
areas such as self-supervised learning, multi-modal perception, inverse
reinforcement learning, and representation learning. The dataset is available
at https://github.com/castacks/tartan drive 2.0.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01915" title="Abstract">arXiv:2402.01915</a> [<a href="/pdf/2402.01915" title="Download PDF">pdf</a>, <a href="/format/2402.01915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Inverse Graphics via Probabilistic Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+T+A">Tuan Anh Le</a>, 
<a href="/search/cs?searchtype=author&query=Sountsov%2C+P">Pavel Sountsov</a>, 
<a href="/search/cs?searchtype=author&query=Hoffman%2C+M+D">Matthew D. Hoffman</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+B">Ben Lee</a>, 
<a href="/search/cs?searchtype=author&query=Patton%2C+B">Brian Patton</a>, 
<a href="/search/cs?searchtype=author&query=Saurous%2C+R+A">Rif A. Saurous</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation (stat.CO)

</div>
<p class="mathjax">How do we infer a 3D scene from a single image in the presence of corruptions
like rain, snow or fog? Straightforward domain randomization relies on knowing
the family of corruptions ahead of time. Here, we propose a Bayesian
approach-dubbed robust inverse graphics (RIG)-that relies on a strong scene
prior and an uninformative uniform corruption prior, making it applicable to a
wide range of corruptions. Given a single image, RIG performs posterior
inference jointly over the scene and the corruption. We demonstrate this idea
by training a neural radiance field (NeRF) scene prior and using a secondary
NeRF to represent the corruptions over which we place an uninformative prior.
RIG, trained only on clean data, outperforms depth estimators and alternative
NeRF approaches that perform point estimation instead of full inference. The
results hold for a number of scene prior architectures based on normalizing
flows and diffusion models. For the latter, we develop reconstruction-guidance
with auxiliary latents (ReGAL)-a diffusion conditioning algorithm that is
applicable in the presence of auxiliary latent variables such as the
corruption. RIG demonstrates how scene priors can be used beyond generation
tasks.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01916" title="Abstract">arXiv:2402.01916</a> [<a href="/pdf/2402.01916" title="Download PDF">pdf</a>, <a href="/ps/2402.01916" title="Download PostScript">ps</a>, <a href="/format/2402.01916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoLe and LYS at BioASQ MESINESP8 Task: similarity based descriptor  assignment in Spanish
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ribadas-Pena%2C+F+J">Francisco J. Ribadas-Pena</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+S">Shuyuan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Kuriyozov%2C+E">Elmurod Kuriyozov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 8th BioASQ Workshop at the 11th Conference and Labs of the Evaluation Forum (CLEF) 2020. 11 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Working Notes of CLEF 2020. Vol. 2696 of CEUR Workshop Proceedings
  (CEUR-WS.org)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we describe our participation in the MESINESP Task of the
BioASQ biomedical semantic indexing challenge. The participating system follows
an approach based solely on conventional information retrieval tools. We have
evaluated various alternatives for extracting index terms from IBECS/LILACS
documents in order to be stored in an Apache Lucene index. Those indexed
representations are queried using the contents of the article to be annotated
and a ranked list of candidate labels is created from the retrieved documents.
We also have evaluated a sort of limited Label Powerset approach which creates
meta-labels joining pairs of DeCS labels with high co-occurrence scores, and an
alternative method based on label profile matching. Results obtained in
official runs seem to confirm the suitability of this approach for languages
like Spanish.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01917" title="Abstract">arXiv:2402.01917</a> [<a href="/pdf/2402.01917" title="Download PDF">pdf</a>, <a href="/ps/2402.01917" title="Download PostScript">ps</a>, <a href="/format/2402.01917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Whispering in Norwegian: Navigating Orthographic and Dialectic  Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kummervold%2C+P+E">Per E Kummervold</a>, 
<a href="/search/cs?searchtype=author&query=de+la+Rosa%2C+J">Javier de la Rosa</a>, 
<a href="/search/cs?searchtype=author&query=Wetjen%2C+F">Freddy Wetjen</a>, 
<a href="/search/cs?searchtype=author&query=Braaten%2C+R">Rolv-Arild Braaten</a>, 
<a href="/search/cs?searchtype=author&query=Solberg%2C+P+E">Per Erik Solberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This article introduces NB-Whisper, an adaptation of OpenAI's Whisper,
specifically fine-tuned for Norwegian language Automatic Speech Recognition
(ASR). We highlight its key contributions and summarise the results achieved in
converting spoken Norwegian into written forms and translating other languages
into Norwegian. We show that we are able to improve the Norwegian Bokm{\aa}l
transcription by OpenAI Whisper Large-v3 from a WER of 10.4 to 6.6 on the
Fleurs Dataset and from 6.8 to 2.2 on the NST dataset.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01918" title="Abstract">arXiv:2402.01918</a> [<a href="/pdf/2402.01918" title="Download PDF">pdf</a>, <a href="/ps/2402.01918" title="Download PostScript">ps</a>, <a href="/format/2402.01918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-Loop and Feedback Nash Trajectories for Competitive Racing with  iLQGames
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rowold%2C+M">Matthias Rowold</a>, 
<a href="/search/cs?searchtype=author&query=Langmann%2C+A">Alexander Langmann</a>, 
<a href="/search/cs?searchtype=author&query=Lohmann%2C+B">Boris Lohmann</a>, 
<a href="/search/cs?searchtype=author&query=Betz%2C+J">Johannes Betz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, submitted to be published at the 35th IEEE Intelligent Vehicles Symposium, June 2 - 5, 2024, Jeju Shinhwa World, Jeju Island, Korea
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Interaction-aware trajectory planning is crucial for closing the gap between
autonomous racing cars and human racing drivers. Prior work has applied game
theory as it provides equilibrium concepts for non-cooperative dynamic
problems. With this contribution, we formulate racing as a dynamic game and
employ a variant of iLQR, called iLQGames, to solve the game. iLQGames finds
trajectories for all players that satisfy the equilibrium conditions for a
linear-quadratic approximation of the game and has been previously applied in
traffic scenarios. We analyze the algorithm's applicability for trajectory
planning in racing scenarios and evaluate it based on interaction awareness,
competitiveness, and safety. With the ability of iLQGames to solve for
open-loop and feedback Nash equilibria, we compare the behavioral outcomes of
the two equilibrium concepts in simple scenarios on a straight track section.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01920" title="Abstract">arXiv:2402.01920</a> [<a href="/pdf/2402.01920" title="Download PDF">pdf</a>, <a href="/format/2402.01920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preference Poisoning Attacks on Reward Model Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Junlin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiongxiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chaowei Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenguang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ning Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Vorobeychik%2C+Y">Yevgeniy Vorobeychik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Learning utility, or reward, models from pairwise comparisons is a
fundamental component in a number of application domains. These approaches
inherently entail collecting preference information from people, with feedback
often provided anonymously. Since preferences are subjective, there is no gold
standard to compare against; yet, reliance of high-impact systems on preference
learning creates a strong motivation for malicious actors to skew data
collected in this fashion to their ends. We investigate the nature and extent
of this vulnerability systematically by considering a threat model in which an
attacker can flip a small subset of preference comparisons with the goal of
either promoting or demoting a target outcome. First, we propose two classes of
algorithmic approaches for these attacks: a principled gradient-based
framework, and several variants of rank-by-distance methods. Next, we
demonstrate the efficacy of best attacks in both these classes in successfully
achieving malicious goals on datasets from three diverse domains: autonomous
control, recommendation system, and textual prompt-response preference
learning. We find that the best attacks are often highly successful, achieving
in the most extreme case 100% success rate with only 0.3% of the data poisoned.
However, which attack is best can vary significantly across domains,
demonstrating the value of our comprehensive vulnerability analysis that
involves several classes of attack algorithms. In addition, we observe that the
simpler and more scalable rank-by-distance approaches are often competitive
with the best, and on occasion significantly outperform gradient-based methods.
Finally, we show that several state-of-the-art defenses against other classes
of poisoning attacks exhibit, at best, limited efficacy in our setting.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01922" title="Abstract">arXiv:2402.01922</a> [<a href="/pdf/2402.01922" title="Download PDF">pdf</a>, <a href="/format/2402.01922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A General Framework for Learning from Weak Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+L">Lei Feng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yidong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Sugiyama%2C+M">Masashi Sugiyama</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+R">Rita Singh</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+B">Bhiksha Raj</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Weakly supervised learning generally faces challenges in applicability to
various scenarios with diverse weak supervision and in scalability due to the
complexity of existing algorithms, thereby hindering the practical deployment.
This paper introduces a general framework for learning from weak supervision
(GLWS) with a novel algorithm. Central to GLWS is an Expectation-Maximization
(EM) formulation, adeptly accommodating various weak supervision sources,
including instance partial labels, aggregate statistics, pairwise observations,
and unlabeled data. We further present an advanced algorithm that significantly
simplifies the EM computational demands using a Non-deterministic Finite
Automaton (NFA) along with a forward-backward algorithm, which effectively
reduces time complexity from quadratic or factorial often required in existing
solutions to linear scale. The problem of learning from arbitrary weak
supervision is therefore converted to the NFA modeling of them. GLWS not only
enhances the scalability of machine learning models but also demonstrates
superior performance and versatility across 11 weak supervision scenarios. We
hope our work paves the way for further advancements and practical deployment
in this field.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01923" title="Abstract">arXiv:2402.01923</a> [<a href="/pdf/2402.01923" title="Download PDF">pdf</a>, <a href="/format/2402.01923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FuzzSlice: Pruning False Positives in Static Analysis Warnings Through  Function-Level Fuzzing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Murali%2C+A">Aniruddhan Murali</a>, 
<a href="/search/cs?searchtype=author&query=Mathews%2C+N+S">Noble Saji Mathews</a>, 
<a href="/search/cs?searchtype=author&query=Alfadel%2C+M">Mahmoud Alfadel</a>, 
<a href="/search/cs?searchtype=author&query=Nagappan%2C+M">Meiyappan Nagappan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Meng Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper has been accepted for publication at ICSE 2024 (Research Track)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Manual confirmation of static analysis reports is a daunting task. This is
due to both the large number of warnings and the high density of false
positives among them. Fuzzing techniques have been proposed to verify static
analysis warnings. However, a major limitation is that fuzzing the whole
project to reach all static analysis warnings is not feasible. This can take
several days and exponential machine time to increase code coverage linearly.
Therefore, we propose FuzzSlice, a novel framework that automatically prunes
possible false positives among static analysis warnings. Unlike prior work that
mostly focuses on confirming true positives among static analysis warnings,
which requires end-to-end fuzzing, FuzzSlice focuses on ruling out potential
false positives, which are the majority in static analysis reports. The key
insight that we base our work on is that a warning that does not yield a crash
when fuzzed at the function level in a given time budget is a possible false
positive. To achieve this, FuzzSlice first aims to generate compilable code
slices at the function level and then fuzzes these code slices instead of the
entire binary. FuzzSlice is also unlikely to misclassify a true bug as a false
positive because the crashing input can be reproduced by a fuzzer at the
function level as well. We evaluate FuzzSlice on the Juliet synthetic dataset
and real-world complex C projects. Our evaluation shows that the ground truth
in the Juliet dataset had 864 false positives which were all detected by
FuzzSlice. For the open-source repositories, we were able to get the developers
from two of these open-source repositories to independently label these
warnings. FuzzSlice automatically identifies 33 out of 53 false positives
confirmed by developers in these two repositories. Thus FuzzSlice reduces false
positives by 62.26% in the open-source repositories and by 100% in the Juliet
dataset.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01925" title="Abstract">arXiv:2402.01925</a> [<a href="/pdf/2402.01925" title="Download PDF">pdf</a>, <a href="/ps/2402.01925" title="Download PostScript">ps</a>, <a href="/format/2402.01925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are You a Real Software Engineer? Best Practices in Online Recruitment  for Software Engineering Studies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alami%2C+A">Adam Alami</a>, 
<a href="/search/cs?searchtype=author&query=Zahedi%2C+M">Mansooreh Zahedi</a>, 
<a href="/search/cs?searchtype=author&query=Ernst%2C+N">Neil Ernst</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages. Accepted at WSESE 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Online research platforms, such as Prolific, offer rapid access to diverse
participant pools but also pose unique challenges in participant qualification
and skill verification. Previous studies reported mixed outcomes and challenges
in leveraging online platforms for the recruitment of qualified software
engineers. Drawing from our experience in conducting three different studies
using Prolific, we propose best practices for recruiting and screening
participants to enhance the quality and relevance of both qualitative and
quantitative software engineering (SE) research samples. We propose refined
best practices for recruitment in SE research on Prolific. (1) Iterative and
controlled prescreening, enabling focused and manageable assessment of
submissions (2) task-oriented and targeted questions that assess technical
skills, knowledge of basic SE concepts, and professional engagement. (3) AI
detection to verify the authenticity of free-text responses. (4) Qualitative
and manual assessment of responses, ensuring authenticity and relevance in
participant answers (5) Additional layers of prescreening are necessary when
necessary to collect data relevant to the topic of the study. (6) Fair or
generous compensation post-qualification to incentivize genuine participation.
By sharing our experiences and lessons learned, we contribute to the
development of effective and rigorous methods for SE empirical research.
particularly the ongoing effort to establish guidelines to ensure reliable data
collection. These practices have the potential to transferability to other
participant recruitment platforms.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01926" title="Abstract">arXiv:2402.01926</a> [<a href="/pdf/2402.01926" title="Download PDF">pdf</a>, <a href="/format/2402.01926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Building Blocks of Accountability in Software  Engineering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alami%2C+A">Adam Alami</a>, 
<a href="/search/cs?searchtype=author&query=Ernst%2C+N">Neil Ernst</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, accepted at CHASE '24, April 14-15, 2024, Lisbon, Portugal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">In the social and organizational sciences, accountability has been linked to
the efficient operation of organizations. However, it has received limited
attention in software engineering (SE) research, in spite of its central role
in the most popular software development methods (e.g., Scrum). In this
article, we explore the mechanisms of accountability in SE environments. We
investigate the factors that foster software engineers' individual
accountability within their teams through an interview study with 12 people.
Our findings recognize two primary forms of accountability shaping software
engineers individual senses of accountability: institutionalized and
grassroots. While the former is directed by formal processes and mechanisms,
like performance reviews, grassroots accountability arises organically within
teams, driven by factors such as peers' expectations and intrinsic motivation.
This organic form cultivates a shared sense of collective responsibility,
emanating from shared team standards and individual engineers' inner commitment
to their personal, professional values, and self-set standards. While
institutionalized accountability relies on traditional "carrot and stick"
approaches, such as financial incentives or denial of promotions, grassroots
accountability operates on reciprocity with peers and intrinsic motivations,
like maintaining one's reputation in the team.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01927" title="Abstract">arXiv:2402.01927</a> [<a href="/pdf/2402.01927" title="Download PDF">pdf</a>, <a href="/format/2402.01927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mathemyths: Leveraging Large Language Models to Teach Mathematical  Language through Child-AI Co-Creative Storytelling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuechen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ziska%2C+K">Katherine Ziska</a>, 
<a href="/search/cs?searchtype=author&query=Jeon%2C+S">Soobin Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chi-Lin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Ying Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conditionally Accepted at CHI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Mathematical language is a cornerstone of a child's mathematical development,
and children can effectively acquire this language through storytelling with a
knowledgeable and engaging partner. In this study, we leverage the recent
advances in large language models to conduct free-form, creative conversations
with children. Consequently, we developed Mathemyths, a joint storytelling
agent that takes turns co-creating stories with children while integrating
mathematical terms into the evolving narrative. This paper details our
development process, illustrating how prompt-engineering can optimize LLMs for
educational contexts. Through a user study involving 35 children aged 4-8
years, our results suggest that when children interacted with Mathemyths, their
learning of mathematical language was comparable to those who co-created
stories with a human partner. However, we observed differences in how children
engaged with co-creation partners of different natures. Overall, we believe
that LLM applications, like Mathemyths, offer children a unique conversational
experience pertaining to focused learning objectives.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01928" title="Abstract">arXiv:2402.01928</a> [<a href="/pdf/2402.01928" title="Download PDF">pdf</a>, <a href="/ps/2402.01928" title="Download PostScript">ps</a>, <a href="/format/2402.01928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Counterfactual Explanations in Machine Learning: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Junqi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Leofante%2C+F">Francesco Leofante</a>, 
<a href="/search/cs?searchtype=author&query=Rago%2C+A">Antonio Rago</a>, 
<a href="/search/cs?searchtype=author&query=Toni%2C+F">Francesca Toni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Counterfactual explanations (CEs) are advocated as being ideally suited to
providing algorithmic recourse for subjects affected by the predictions of
machine learning models. While CEs can be beneficial to affected individuals,
recent work has exposed severe issues related to the robustness of
state-of-the-art methods for obtaining CEs. Since a lack of robustness may
compromise the validity of CEs, techniques to mitigate this risk are in order.
In this survey, we review works in the rapidly growing area of robust CEs and
perform an in-depth analysis of the forms of robustness they consider. We also
discuss existing solutions and their limitations, providing a solid foundation
for future developments.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01929" title="Abstract">arXiv:2402.01929</a> [<a href="/pdf/2402.01929" title="Download PDF">pdf</a>, <a href="/format/2402.01929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sample, estimate, aggregate: A recipe for causal discovery foundation  models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Menghua Wu</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+Y">Yujia Bao</a>, 
<a href="/search/cs?searchtype=author&query=Barzilay%2C+R">Regina Barzilay</a>, 
<a href="/search/cs?searchtype=author&query=Jaakkola%2C+T">Tommi Jaakkola</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Causal discovery, the task of inferring causal structure from data, promises
to accelerate scientific research, inform policy making, and more. However, the
per-dataset nature of existing causal discovery algorithms renders them slow,
data hungry, and brittle. Inspired by foundation models, we propose a causal
discovery framework where a deep learning model is pretrained to resolve
predictions from classical discovery algorithms run over smaller subsets of
variables. This method is enabled by the observations that the outputs from
classical algorithms are fast to compute for small problems, informative of
(marginal) data structure, and their structure outputs as objects remain
comparable across datasets. Our method achieves state-of-the-art performance on
synthetic and realistic datasets, generalizes to data generating mechanisms not
seen during training, and offers inference speeds that are orders of magnitude
faster than existing models.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01930" title="Abstract">arXiv:2402.01930</a> [<a href="/pdf/2402.01930" title="Download PDF">pdf</a>, <a href="/format/2402.01930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reducing Optimism Bias in Incomplete Cooperative Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C3%9Aradn%C3%ADk%2C+F">Filip &#xda;radn&#xed;k</a>, 
<a href="/search/cs?searchtype=author&query=Sychrovsk%C3%BD%2C+D">David Sychrovsk&#xfd;</a>, 
<a href="/search/cs?searchtype=author&query=%C4%8Cern%C3%BD%2C+J">Jakub &#x10c;ern&#xfd;</a>, 
<a href="/search/cs?searchtype=author&query=%C4%8Cern%C3%BD%2C+M">Martin &#x10c;ern&#xfd;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proc. of the 23rd International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Cooperative game theory has diverse applications in contemporary artificial
intelligence, including domains like interpretable machine learning, resource
allocation, and collaborative decision-making. However, specifying a
cooperative game entails assigning values to exponentially many coalitions, and
obtaining even a single value can be resource-intensive in practice. Yet simply
leaving certain coalition values undisclosed introduces ambiguity regarding
individual contributions to the collective grand coalition. This ambiguity
often leads to players holding overly optimistic expectations, stemming from
either inherent biases or strategic considerations, frequently resulting in
collective claims exceeding the actual grand coalition value. In this paper, we
present a framework aimed at optimizing the sequence for revealing coalition
values, with the overarching goal of efficiently closing the gap between
players' expectations and achievable outcomes in cooperative games. Our
contributions are threefold: (i) we study the individual players' optimistic
completions of games with missing coalition values along with the arising gap,
and investigate its analytical characteristics that facilitate more efficient
optimization; (ii) we develop methods to minimize this gap over classes of
games with a known prior by disclosing values of additional coalitions in both
offline and online fashion; and (iii) we empirically demonstrate the
algorithms' performance in practical scenarios, together with an investigation
into the typical order of revealing coalition values.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01931" title="Abstract">arXiv:2402.01931</a> [<a href="/pdf/2402.01931" title="Download PDF">pdf</a>, <a href="/format/2402.01931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digits micro-model for accurate and secure transactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chhablani%2C+C">Chirag Chhablani</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+N">Nikhita Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Hosier%2C+J">Jordan Hosier</a>, 
<a href="/search/cs?searchtype=author&query=Gurbani%2C+V+K">Vijay K. Gurbani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 1 figure, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Sound (cs.SD)

</div>
<p class="mathjax">Automatic Speech Recognition (ASR) systems are used in the financial domain
to enhance the caller experience by enabling natural language understanding and
facilitating efficient and intuitive interactions. Increasing use of ASR
systems requires that such systems exhibit very low error rates. The
predominant ASR models to collect numeric data are large, general-purpose
commercial models -- Google Speech-to-text (STT), or Amazon Transcribe -- or
open source (OpenAI's Whisper). Such ASR models are trained on hundreds of
thousands of hours of audio data and require considerable resources to run.
Despite recent progress large speech recognition models, we highlight the
potential of smaller, specialized "micro" models. Such light models can be
trained perform well on number recognition specific tasks, competing with
general models like Whisper or Google STT while using less than 80 minutes of
training time and occupying at least an order of less memory resources. Also,
unlike larger speech recognition models, micro-models are trained on carefully
selected and curated datasets, which makes them highly accurate, agile, and
easy to retrain, while using low compute resources. We present our work on
creating micro models for multi-digit number recognition that handle diverse
speaking styles reflecting real-world pronunciation patterns. Our work
contributes to domain-specific ASR models, improving digit recognition
accuracy, and privacy of data. An added advantage, their low resource
consumption allows them to be hosted on-premise, keeping private data local
instead uploading to an external cloud. Our results indicate that our
micro-model makes less errors than the best-of-breed commercial or open-source
ASRs in recognizing digits (1.8% error rate of our best micro-model versus 5.8%
error rate of Whisper), and has a low memory footprint (0.66 GB VRAM for our
model versus 11 GB VRAM for Whisper).
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01934" title="Abstract">arXiv:2402.01934</a> [<a href="/pdf/2402.01934" title="Download PDF">pdf</a>, <a href="/format/2402.01934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clarifying the Path to User Satisfaction: An Investigation into  Clarification Usefulness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+H+A">Hossein A. Rahmani</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Aliannejadi%2C+M">Mohammad Aliannejadi</a>, 
<a href="/search/cs?searchtype=author&query=Naghiaei%2C+M">Mohammadmehdi Naghiaei</a>, 
<a href="/search/cs?searchtype=author&query=Yilmaz%2C+E">Emine Yilmaz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Clarifying questions are an integral component of modern information
retrieval systems, directly impacting user satisfaction and overall system
performance. Poorly formulated questions can lead to user frustration and
confusion, negatively affecting the system's performance. This research
addresses the urgent need to identify and leverage key features that contribute
to the classification of clarifying questions, enhancing user satisfaction. To
gain deeper insights into how different features influence user satisfaction,
we conduct a comprehensive analysis, considering a broad spectrum of lexical,
semantic, and statistical features, such as question length and sentiment
polarity. Our empirical results provide three main insights into the qualities
of effective query clarification: (1) specific questions are more effective
than generic ones; (2) the subjectivity and emotional tone of a question play a
role; and (3) shorter and more ambiguous queries benefit significantly from
clarification. Based on these insights, we implement feature-integrated user
satisfaction prediction using various classifiers, both traditional and
neural-based, including random forest, BERT, and large language models. Our
experiments show a consistent and significant improvement, particularly in
traditional classifiers, with a minimum performance boost of 45\%. This study
presents invaluable guidelines for refining the formulation of clarifying
questions and enhancing both user satisfaction and system performance.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01935" title="Abstract">arXiv:2402.01935</a> [<a href="/pdf/2402.01935" title="Download PDF">pdf</a>, <a href="/format/2402.01935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Code Representation Learning At Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dejiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+W">Wasi Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+M">Ming Tan</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+H">Hantian Ding</a>, 
<a href="/search/cs?searchtype=author&query=Nallapati%2C+R">Ramesh Nallapati</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+D">Dan Roth</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaofei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+B">Bing Xiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent studies have shown that code language models at scale demonstrate
significant performance gains on downstream tasks, i.e., code generation.
However, most of the existing works on code representation learning train
models at a hundred million parameter scale using very limited pretraining
corpora. In this work, we fuel code representation learning with a vast amount
of code data via a two-stage pretraining scheme. We first train the encoders
via a mix that leverages both randomness in masking language modeling and the
structure aspect of programming language. We then enhance the representations
via contrastive learning with hard negative and hard positive constructed in an
unsupervised manner. We establish an off-the-shelf encoder model that
persistently outperforms the existing models on a wide variety of downstream
tasks by large margins. To comprehend the factors contributing to successful
code representation learning, we conduct detailed ablations and share our
findings on (i) a customized and effective token-level denoising scheme for
source code; (ii) the importance of hard negatives and hard positives; (iii)
how the proposed bimodal contrastive learning boost the cross-lingual semantic
search performance; and (iv) how the pretraining schemes decide the downstream
task performance scales with the model size.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01939" title="Abstract">arXiv:2402.01939</a> [<a href="/pdf/2402.01939" title="Download PDF">pdf</a>, <a href="/format/2402.01939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Morphologically-Aware Dictionary-based Data Augmentation Technique for  Machine Translation of Under-Represented Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alam%2C+M+M+I">Md Mahfuz Ibn Alam</a>, 
<a href="/search/cs?searchtype=author&query=Ahmadi%2C+S">Sina Ahmadi</a>, 
<a href="/search/cs?searchtype=author&query=Anastasopoulos%2C+A">Antonios Anastasopoulos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The availability of parallel texts is crucial to the performance of machine
translation models. However, most of the world's languages face the predominant
challenge of data scarcity. In this paper, we propose strategies to synthesize
parallel data relying on morpho-syntactic information and using bilingual
lexicons along with a small amount of seed parallel data. Our methodology
adheres to a realistic scenario backed by the small parallel seed data. It is
linguistically informed, as it aims to create augmented data that is more
likely to be grammatically correct. We analyze how our synthetic data can be
combined with raw parallel data and demonstrate a consistent improvement in
performance in our experiments on 14 languages (28 English &lt;-&gt; X pairs) ranging
from well- to very low-resource ones. Our method leads to improvements even
when using only five seed sentences and a bilingual lexicon.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01943" title="Abstract">arXiv:2402.01943</a> [<a href="/pdf/2402.01943" title="Download PDF">pdf</a>, <a href="/format/2402.01943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Precedence-Constrained Winter Value for Effective Graph Data Valuation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chi%2C+H">Hongliang Chi</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+J">Jin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+C">Charu Aggarwal</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yao Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages in total
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Data valuation is essential for quantifying data's worth, aiding in assessing
data quality and determining fair compensation. While existing data valuation
methods have proven effective in evaluating the value of Euclidean data, they
face limitations when applied to the increasingly popular graph-structured
data. Particularly, graph data valuation introduces unique challenges,
primarily stemming from the intricate dependencies among nodes and the
exponential growth in value estimation costs. To address the challenging
problem of graph data valuation, we put forth an innovative solution,
Precedence-Constrained Winter (PC-Winter) Value, to account for the complex
graph structure. Furthermore, we develop a variety of strategies to address the
computational challenges and enable efficient approximation of PC-Winter.
Extensive experiments demonstrate the effectiveness of PC-Winter across diverse
datasets and tasks.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01944" title="Abstract">arXiv:2402.01944</a> [<a href="/pdf/2402.01944" title="Download PDF">pdf</a>, <a href="/format/2402.01944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guarantees in Software Security
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=B%C3%B6hme%2C+M">Marcel B&#xf6;hme</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">We review general approaches to reason about the security of a software
system and reflect upon the guarantees they provide. We introduce a taxonomy of
fundamental challenges towards the provision of guarantees, and discuss how
these challenges are routinely exploited to attack a system in spite of
credible assurances about the absence of such bugs. It is only when we
identify, study, and acknowledge the flaws in our current reasoning systems
today that we can develop effective mitigation strategies in the future. To
this end, we finally propose a research programme whose goal it is to tackle
the software security challenges of this decade.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01945" title="Abstract">arXiv:2402.01945</a> [<a href="/pdf/2402.01945" title="Download PDF">pdf</a>, <a href="/format/2402.01945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Case Study on Filtering for End-to-End Speech Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alam%2C+M+M+I">Md Mahfuz Ibn Alam</a>, 
<a href="/search/cs?searchtype=author&query=Anastasopoulos%2C+A">Antonios Anastasopoulos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">It is relatively easy to mine a large parallel corpus for any machine
learning task, such as speech-to-text or speech-to-speech translation. Although
these mined corpora are large in volume, their quality is questionable. This
work shows that the simplest filtering technique can trim down these big, noisy
datasets to a more manageable, clean dataset. We also show that using this
clean dataset can improve the model's performance, as in the case of the
multilingual-to-English Speech Translation (ST) model, where, on average, we
obtain a 4.65 BLEU score improvement.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01950" title="Abstract">arXiv:2402.01950</a> [<a href="/pdf/2402.01950" title="Download PDF">pdf</a>, <a href="/format/2402.01950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConRF: Zero-shot Stylization of 3D Scenes with Conditioned Radiation  Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miao%2C+X">Xingyu Miao</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yang Bai</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+H">Haoran Duan</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+F">Fan Wan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yawen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+Y">Yang Long</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yefeng Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Most of the existing works on arbitrary 3D NeRF style transfer required
retraining on each single style condition. This work aims to achieve zero-shot
controlled stylization in 3D scenes utilizing text or visual input as
conditioning factors. We introduce ConRF, a novel method of zero-shot
stylization. Specifically, due to the ambiguity of CLIP features, we employ a
conversion process that maps the CLIP feature space to the style space of a
pre-trained VGG network and then refine the CLIP multi-modal knowledge into a
style transfer neural radiation field. Additionally, we use a 3D volumetric
representation to perform local style transfer. By combining these operations,
ConRF offers the capability to utilize either text or images as references,
resulting in the generation of sequences with novel views enhanced by global or
local stylization. Our experiment demonstrates that ConRF outperforms other
existing methods for 3D scene and single-text stylization in terms of visual
quality.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01955" title="Abstract">arXiv:2402.01955</a> [<a href="/pdf/2402.01955" title="Download PDF">pdf</a>, <a href="/format/2402.01955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OPSurv: Orthogonal Polynomials Quadrature Algorithm for Survival  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bialokozowicz%2C+L+W">Lilian W. Bialokozowicz</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+H+M">Hoang M. Le</a>, 
<a href="/search/cs?searchtype=author&query=Sylvain%2C+T">Tristan Sylvain</a>, 
<a href="/search/cs?searchtype=author&query=Forsyth%2C+P+A+I">Peter A. I. Forsyth</a>, 
<a href="/search/cs?searchtype=author&query=Nagisetty%2C+V">Vineel Nagisetty</a>, 
<a href="/search/cs?searchtype=author&query=Mori%2C+G">Greg Mori</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Functional Analysis (math.FA)

</div>
<p class="mathjax">This paper introduces the Orthogonal Polynomials Quadrature Algorithm for
Survival Analysis (OPSurv), a new method providing time-continuous functional
outputs for both single and competing risks scenarios in survival analysis.
OPSurv utilizes the initial zero condition of the Cumulative Incidence function
and a unique decomposition of probability densities using orthogonal
polynomials, allowing it to learn functional approximation coefficients for
each risk event and construct Cumulative Incidence Function estimates via
Gauss--Legendre quadrature. This approach effectively counters overfitting,
particularly in competing risks scenarios, enhancing model expressiveness and
control. The paper further details empirical validations and theoretical
justifications of OPSurv, highlighting its robust performance as an advancement
in survival analysis with competing risks.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01959" title="Abstract">arXiv:2402.01959</a> [<a href="/pdf/2402.01959" title="Download PDF">pdf</a>, <a href="/ps/2402.01959" title="Download PostScript">ps</a>, <a href="/format/2402.01959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seamless Capture and Stabilization of Spinning Satellites By Space  Robots with Spinning Base
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aghili%2C+F">Farhad Aghili</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper introduces an innovative guidance and control method for
simultaneously capturing and stabilizing a fast-spinning target satellite, such
as a spin-stabilized satellite, using a spinning-base servicing satellite
equipped with a robotic manipulator, joint locks, and reaction wheels (RWs).
The method involves controlling the RWs of the servicing satellite to replicate
the spinning motion of the target satellite, while locking the manipulator's
joints to achieve spin-matching. This maneuver makes the target stationary with
respect to the rotating frame of the servicing satellite located at its
center-of-mass (CoM), simplifying the robot capture trajectory planning and
eliminating post-capture trajectory planning entirely. In the next phase, the
joints are unlocked, and a coordination controller drives the robotic
manipulator to capture the target satellite while maintaining zero relative
rotation between the servicing and target satellites. The spin stabilization
phase begins after completing the capture phase, where the joints are locked to
form a single tumbling rigid body consisting of the rigidly connected servicing
and target satellites. An optimal controller applies negative control torques
to the RWs to dampen out the tumbling motion of the interconnected satellites
as quickly as possible, subject to the actuation torque limit of the RWs and
the maximum torque exerted by the manipulator's end-effector.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01960" title="Abstract">arXiv:2402.01960</a> [<a href="/pdf/2402.01960" title="Download PDF">pdf</a>, <a href="/format/2402.01960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Calibrated Uncertainty Quantification for Operator Learning via  Conformal Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Ziqi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Azizzadenesheli%2C+K">Kamyar Azizzadenesheli</a>, 
<a href="/search/cs?searchtype=author&query=Anandkumar%2C+A">Anima Anandkumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Operator learning has been increasingly adopted in scientific and engineering
applications, many of which require calibrated uncertainty quantification.
Since the output of operator learning is a continuous function, quantifying
uncertainty simultaneously at all points in the domain is challenging. Current
methods consider calibration at a single point or over one scalar function or
make strong assumptions such as Gaussianity. We propose a risk-controlling
quantile neural operator, a distribution-free, finite-sample functional
calibration conformal prediction method. We provide a theoretical calibration
guarantee on the coverage rate, defined as the expected percentage of points on
the function domain whose true value lies within the predicted uncertainty
ball. Empirical results on a 2D Darcy flow and a 3D car surface pressure
prediction tasks validate our theoretical results, demonstrating calibrated
coverage and efficient uncertainty bands outperforming baseline methods. In
particular, on the 3D problem, our method is the only one that meets the target
calibration percentage (percentage of test samples for which the uncertainty
estimates are calibrated) of 98\%.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01961" title="Abstract">arXiv:2402.01961</a> [<a href="/pdf/2402.01961" title="Download PDF">pdf</a>, <a href="/format/2402.01961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anytime Multi-Agent Path Finding using Operation Parallelism in Large  Neighborhood Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chan%2C+S">Shao-Hung Chan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhe Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dian-Lun Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Harabor%2C+D">Daniel Harabor</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tsung-Wei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Koenig%2C+S">Sven Koenig</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+T">Thomy Phan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as an extended abstract in AAMAS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">Multi-Agent Path Finding (MAPF) is the problem of finding a set of
collision-free paths for multiple agents in a shared environment while
minimizing the sum of travel time. Since solving the MAPF problem optimally is
NP-hard, anytime algorithms based on Large Neighborhood Search (LNS) are
promising to find good-quality solutions in a scalable way by iteratively
destroying and repairing the paths. We propose Destroy-Repair Operation
Parallelism for LNS (DROP-LNS), a parallel framework that performs multiple
destroy and repair operations concurrently to explore more regions of the
search space within a limited time budget. Unlike classic MAPF approaches,
DROP-LNS can exploit parallelized hardware to improve the solution quality. We
also formulate two variants of parallelism and conduct experimental
evaluations. The results show that DROP-LNS significantly outperforms the
state-of-the-art and the variants.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01963" title="Abstract">arXiv:2402.01963</a> [<a href="/pdf/2402.01963" title="Download PDF">pdf</a>, <a href="/format/2402.01963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Large-Scale k-Nearest Neighbor Text Categorization with Label  Autoencoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ribadas-Pena%2C+F+J">Francisco J. Ribadas-Pena</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+S">Shuyuan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Bilbao%2C+V+M+D">V&#xed;ctor M. Darriba Bilbao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 4 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Mathematics 2022, 10(16), 2867
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Information Retrieval (cs.IR)

</div>
<p class="mathjax">In this paper, we introduce a multi-label lazy learning approach to deal with
automatic semantic indexing in large document collections in the presence of
complex and structured label vocabularies with high inter-label correlation.
The proposed method is an evolution of the traditional k-Nearest Neighbors
algorithm which uses a large autoencoder trained to map the large label space
to a reduced size latent space and to regenerate the predicted labels from this
latent space. We have evaluated our proposal in a large portion of the MEDLINE
biomedical document collection which uses the Medical Subject Headings (MeSH)
thesaurus as a controlled vocabulary. In our experiments we propose and
evaluate several document representation approaches and different label
autoencoder configurations.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01964" title="Abstract">arXiv:2402.01964</a> [<a href="/pdf/2402.01964" title="Download PDF">pdf</a>, <a href="/format/2402.01964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No Need to Look Back: An Efficient and Scalable Approach for Temporal  Network Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yuhong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pan Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Temporal graph representation learning (TGRL) is crucial for modeling
complex, dynamic systems in real-world networks. Traditional TGRL methods,
though effective, suffer from high computational demands and inference latency.
This is mainly induced by their inefficient sampling of temporal neighbors by
backtracking the interaction history of each node when making model inference.
This paper introduces a novel efficient TGRL framework, No-Looking-Back (NLB).
NLB employs a "forward recent sampling" strategy, which bypasses the need for
backtracking historical interactions. This strategy is implemented using a
GPU-executable size-constrained hash table for each node, recording
down-sampled recent interactions, which enables rapid response to queries with
minimal inference latency. The maintenance of this hash table is highly
efficient, with $O(1)$ complexity. NLB is fully compatible with GPU processing,
maximizing programmability, parallelism, and power efficiency. Empirical
evaluations demonstrate that NLB matches or surpasses state-of-the-art methods
in accuracy for link prediction and node classification across six real-world
datasets. Significantly, it is 1.32-4.40 $\times$ faster in training, 1.2-7.94
$\times$ more energy efficient, and 1.97-5.02 $\times$ more effective in
reducing inference latency compared to the most competitive baselines. The link
to the code: https://github.com/Graph-COM/NLB.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01965" title="Abstract">arXiv:2402.01965</a> [<a href="/pdf/2402.01965" title="Download PDF">pdf</a>, <a href="/format/2402.01965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Neural Network-Based Generative Diffusion Models through  Convex Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fangzhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pilanci%2C+M">Mert Pilanci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Diffusion models are becoming widely used in state-of-the-art image, video
and audio generation. Score-based diffusion models stand out among these
methods, necessitating the estimation of score function of the input data
distribution. In this study, we present a theoretical framework to analyze
two-layer neural network-based diffusion models by reframing score matching and
denoising score matching as convex optimization. Though existing diffusion
theory is mainly asymptotic, we characterize the exact predicted score function
and establish the convergence result for neural network-based diffusion models
with finite data. This work contributes to understanding what neural
network-based diffusion model learns in non-asymptotic settings.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01967" title="Abstract">arXiv:2402.01967</a> [<a href="/pdf/2402.01967" title="Download PDF">pdf</a>, <a href="/format/2402.01967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MasonPerplexity at Multimodal Hate Speech Event Detection 2024: Hate  Speech and Target Detection Using Transformer Ensembles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ganguly%2C+A">Amrita Ganguly</a>, 
<a href="/search/cs?searchtype=author&query=Emran%2C+A+N+B">Al Nahian Bin Emran</a>, 
<a href="/search/cs?searchtype=author&query=Puspo%2C+S+S+C">Sadiya Sayara Chowdhury Puspo</a>, 
<a href="/search/cs?searchtype=author&query=Raihan%2C+M+N">Md Nishat Raihan</a>, 
<a href="/search/cs?searchtype=author&query=Goswami%2C+D">Dhiman Goswami</a>, 
<a href="/search/cs?searchtype=author&query=Zampieri%2C+M">Marcos Zampieri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The automatic identification of offensive language such as hate speech is
important to keep discussions civil in online communities. Identifying hate
speech in multimodal content is a particularly challenging task because
offensiveness can be manifested in either words or images or a juxtaposition of
the two. This paper presents the MasonPerplexity submission for the Shared Task
on Multimodal Hate Speech Event Detection at CASE 2024 at EACL 2024. The task
is divided into two sub-tasks: sub-task A focuses on the identification of hate
speech and sub-task B focuses on the identification of targets in text-embedded
images during political events. We use an XLM-roBERTa-large model for sub-task
A and an ensemble approach combining XLM-roBERTa-base, BERTweet-large, and
BERT-base for sub-task B. Our approach obtained 0.8347 F1-score in sub-task A
and 0.6741 F1-score in sub-task B ranking 3rd on both sub-tasks.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01968" title="Abstract">arXiv:2402.01968</a> [<a href="/pdf/2402.01968" title="Download PDF">pdf</a>, <a href="/format/2402.01968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges  and Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+H">Hung Du</a>, 
<a href="/search/cs?searchtype=author&query=Thudumu%2C+S">Srikanth Thudumu</a>, 
<a href="/search/cs?searchtype=author&query=Vasa%2C+R">Rajesh Vasa</a>, 
<a href="/search/cs?searchtype=author&query=Mouzakis%2C+K">Kon Mouzakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Research interest in autonomous agents is on the rise as an emerging topic.
The notable achievements of Large Language Models (LLMs) have demonstrated the
considerable potential to attain human-like intelligence in autonomous agents.
However, the challenge lies in enabling these agents to learn, reason, and
navigate uncertainties in dynamic environments. Context awareness emerges as a
pivotal element in fortifying multi-agent systems when dealing with dynamic
situations. Despite existing research focusing on both context-aware systems
and multi-agent systems, there is a lack of comprehensive surveys outlining
techniques for integrating context-aware systems with multi-agent systems. To
address this gap, this survey provides a comprehensive overview of
state-of-the-art context-aware multi-agent systems. First, we outline the
properties of both context-aware systems and multi-agent systems that
facilitate integration between these systems. Subsequently, we propose a
general process for context-aware systems, with each phase of the process
encompassing diverse approaches drawn from various application domains such as
collision avoidance in autonomous driving, disaster relief management, utility
management, supply chain management, human-AI interaction, and others. Finally,
we discuss the existing challenges of context-aware multi-agent systems and
provide future research directions in this field.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01969" title="Abstract">arXiv:2402.01969</a> [<a href="/pdf/2402.01969" title="Download PDF">pdf</a>, <a href="/format/2402.01969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simulation-Enhanced Data Augmentation for Machine Learning Pathloss  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohamed%2C+A+P">Ahmed P. Mohamed</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+B">Byunghyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yaguang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hollingsworth%2C+M">Max Hollingsworth</a>, 
<a href="/search/cs?searchtype=author&query=Anderson%2C+C+R">C. Robert Anderson</a>, 
<a href="/search/cs?searchtype=author&query=Krogmeier%2C+J+V">James V. Krogmeier</a>, 
<a href="/search/cs?searchtype=author&query=Love%2C+D+J">David J. Love</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, Accepted at ICC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Machine learning (ML) offers a promising solution to pathloss prediction.
However, its effectiveness can be degraded by the limited availability of data.
To alleviate these challenges, this paper introduces a novel
simulation-enhanced data augmentation method for ML pathloss prediction. Our
method integrates synthetic data generated from a cellular coverage simulator
and independently collected real-world datasets. These datasets were collected
through an extensive measurement campaign in different environments, including
farms, hilly terrains, and residential areas. This comprehensive data
collection provides vital ground truth for model training. A set of channel
features was engineered, including geographical attributes derived from LiDAR
datasets. These features were then used to train our prediction model,
incorporating the highly efficient and robust gradient boosting ML algorithm,
CatBoost. The integration of synthetic data, as demonstrated in our study,
significantly improves the generalizability of the model in different
environments, achieving a remarkable improvement of approximately 12dB in terms
of mean absolute error for the best-case scenario. Moreover, our analysis
reveals that even a small fraction of measurements added to the simulation
training set, with proper data balance, can significantly enhance the model's
performance.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01970" title="Abstract">arXiv:2402.01970</a> [<a href="/pdf/2402.01970" title="Download PDF">pdf</a>, <a href="/format/2402.01970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BVI-Lowlight: Fully Registered Benchmark Dataset for Low-Light Video  Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anantrasirichai%2C+N">Nantheera Anantrasirichai</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+R">Ruirui Lin</a>, 
<a href="/search/cs?searchtype=author&query=Malyugina%2C+A">Alexandra Malyugina</a>, 
<a href="/search/cs?searchtype=author&query=Bull%2C+D">David Bull</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Low-light videos often exhibit spatiotemporal incoherent noise, leading to
poor visibility and compromised performance across various computer vision
applications. One significant challenge in enhancing such content using modern
technologies is the scarcity of training data. This paper introduces a novel
low-light video dataset, consisting of 40 scenes captured in various motion
scenarios under two distinct low-lighting conditions, incorporating genuine
noise and temporal artifacts. We provide fully registered ground truth data
captured in normal light using a programmable motorized dolly, and
subsequently, refine them via image-based post-processing to ensure the
pixel-wise alignment of frames in different light levels. This paper also
presents an exhaustive analysis of the low-light dataset, and demonstrates the
extensive and representative nature of our dataset in the context of supervised
learning. Our experimental results demonstrate the significance of fully
registered video pairs in the development of low-light video enhancement
methods and the need for comprehensive evaluation. Our dataset is available at
DOI:10.21227/mzny-8c77.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01974" title="Abstract">arXiv:2402.01974</a> [<a href="/pdf/2402.01974" title="Download PDF">pdf</a>, <a href="/format/2402.01974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hypergraph-Transformer (HGT) for Interactive Event Prediction in  Laparoscopic and Robotic Surgery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+L">Lianhao Yin</a>, 
<a href="/search/cs?searchtype=author&query=Ban%2C+Y">Yutong Ban</a>, 
<a href="/search/cs?searchtype=author&query=Eckhoff%2C+J">Jennifer Eckhoff</a>, 
<a href="/search/cs?searchtype=author&query=Meireles%2C+O">Ozanan Meireles</a>, 
<a href="/search/cs?searchtype=author&query=Rus%2C+D">Daniela Rus</a>, 
<a href="/search/cs?searchtype=author&query=Rosman%2C+G">Guy Rosman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Understanding and anticipating intraoperative events and actions is critical
for intraoperative assistance and decision-making during minimally invasive
surgery. Automated prediction of events, actions, and the following
consequences is addressed through various computational approaches with the
objective of augmenting surgeons' perception and decision-making capabilities.
We propose a predictive neural network that is capable of understanding and
predicting critical interactive aspects of surgical workflow from
intra-abdominal video, while flexibly leveraging surgical knowledge graphs. The
approach incorporates a hypergraph-transformer (HGT) structure that encodes
expert knowledge into the network design and predicts the hidden embedding of
the graph. We verify our approach on established surgical datasets and
applications, including the detection and prediction of action triplets, and
the achievement of the Critical View of Safety (CVS). Moreover, we address
specific, safety-related tasks, such as predicting the clipping of cystic duct
or artery without prior achievement of the CVS. Our results demonstrate the
superiority of our approach compared to unstructured alternatives.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01975" title="Abstract">arXiv:2402.01975</a> [<a href="/pdf/2402.01975" title="Download PDF">pdf</a>, <a href="/format/2402.01975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure-Aware E(3)-Invariant Molecular Conformer Aggregation Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D+M+H">Duy M. H. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Lukashina%2C+N">Nina Lukashina</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Tai Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+A+T">An T. Le</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">TrungTin Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+N">Nhat Ho</a>, 
<a href="/search/cs?searchtype=author&query=Peters%2C+J">Jan Peters</a>, 
<a href="/search/cs?searchtype=author&query=Sonntag%2C+D">Daniel Sonntag</a>, 
<a href="/search/cs?searchtype=author&query=Zaverkin%2C+V">Viktor Zaverkin</a>, 
<a href="/search/cs?searchtype=author&query=Niepert%2C+M">Mathias Niepert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">A molecule's 2D representation consists of its atoms, their attributes, and
the molecule's covalent bonds. A 3D (geometric) representation of a molecule is
called a conformer and consists of its atom types and Cartesian coordinates.
Every conformer has a potential energy, and the lower this energy, the more
likely it occurs in nature. Most existing machine learning methods for
molecular property prediction consider either 2D molecular graphs or 3D
conformer structure representations in isolation. Inspired by recent work on
using ensembles of conformers in conjunction with 2D graph representations, we
propose E(3)-invariant molecular conformer aggregation networks. The method
integrates a molecule's 2D representation with that of multiple of its
conformers. Contrary to prior work, we propose a novel 2D--3D aggregation
mechanism based on a differentiable solver for the \emph{Fused
Gromov-Wasserstein Barycenter} problem and the use of an efficient online
conformer generation method based on distance geometry. We show that the
proposed aggregation mechanism is E(3) invariant and provides an efficient GPU
implementation. Moreover, we demonstrate that the aggregation mechanism helps
to outperform state-of-the-art property prediction methods on established
datasets significantly.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01976" title="Abstract">arXiv:2402.01976</a> [<a href="/pdf/2402.01976" title="Download PDF">pdf</a>, <a href="/format/2402.01976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MasonPerplexity at ClimateActivism 2024: Integrating Advanced Ensemble  Techniques and Data Augmentation for Climate Activism Stance and Hate Event  Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Emran%2C+A+N+B">Al Nahian Bin Emran</a>, 
<a href="/search/cs?searchtype=author&query=Ganguly%2C+A">Amrita Ganguly</a>, 
<a href="/search/cs?searchtype=author&query=Puspo%2C+S+S+C">Sadiya Sayara Chowdhury Puspo</a>, 
<a href="/search/cs?searchtype=author&query=Goswami%2C+D">Dhiman Goswami</a>, 
<a href="/search/cs?searchtype=author&query=Raihan%2C+M+N">Md Nishat Raihan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The task of identifying public opinions on social media, particularly
regarding climate activism and the detection of hate events, has emerged as a
critical area of research in our rapidly changing world. With a growing number
of people voicing either to support or oppose to climate-related issues -
understanding these diverse viewpoints has become increasingly vital. Our team,
MasonPerplexity, participates in a significant research initiative focused on
this subject. We extensively test various models and methods, discovering that
our most effective results are achieved through ensemble modeling, enhanced by
data augmentation techniques like back-translation. In the specific components
of this research task, our team achieved notable positions, ranking 5th, 1st,
and 6th in the respective sub-tasks, thereby illustrating the effectiveness of
our approach in this important field of study.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01980" title="Abstract">arXiv:2402.01980</a> [<a href="/pdf/2402.01980" title="Download PDF">pdf</a>, <a href="/format/2402.01980" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SOCIALITE-LLAMA: An Instruction-Tuned Model for Social Scientific Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dey%2C+G">Gourab Dey</a>, 
<a href="/search/cs?searchtype=author&query=Ganesan%2C+A+V">Adithya V Ganesan</a>, 
<a href="/search/cs?searchtype=author&query=Lal%2C+Y+K">Yash Kumar Lal</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+M">Manal Shah</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+S">Shreyashee Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Matero%2C+M">Matthew Matero</a>, 
<a href="/search/cs?searchtype=author&query=Giorgi%2C+S">Salvatore Giorgi</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+V">Vivek Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Schwartz%2C+H+A">H. Andrew Schwartz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Short paper accepted to EACL 2024. 4 pgs, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Social science NLP tasks, such as emotion or humor detection, are required to
capture the semantics along with the implicit pragmatics from text, often with
limited amounts of training data. Instruction tuning has been shown to improve
the many capabilities of large language models (LLMs) such as commonsense
reasoning, reading comprehension, and computer programming. However, little is
known about the effectiveness of instruction tuning on the social domain where
implicit pragmatic cues are often needed to be captured. We explore the use of
instruction tuning for social science NLP tasks and introduce Socialite-Llama
-- an open-source, instruction-tuned Llama. On a suite of 20 social science
tasks, Socialite-Llama improves upon the performance of Llama as well as
matches or improves upon the performance of a state-of-the-art, multi-task
finetuned model on a majority of them. Further, Socialite-Llama also leads to
improvement on 5 out of 6 related social tasks as compared to Llama, suggesting
instruction tuning can lead to generalized social understanding. All resources
including our code, model and dataset can be found through
bit.ly/socialitellama.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01981" title="Abstract">arXiv:2402.01981</a> [<a href="/pdf/2402.01981" title="Download PDF">pdf</a>, <a href="/format/2402.01981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Debiasing Large Language Models: Zero-Shot Recognition and  Reduction of Stereotypes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gallegos%2C+I+O">Isabel O. Gallegos</a>, 
<a href="/search/cs?searchtype=author&query=Rossi%2C+R+A">Ryan A. Rossi</a>, 
<a href="/search/cs?searchtype=author&query=Barrow%2C+J">Joe Barrow</a>, 
<a href="/search/cs?searchtype=author&query=Tanjim%2C+M+M">Md Mehrab Tanjim</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Deilamsalehy%2C+H">Hanieh Deilamsalehy</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruiyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungchul Kim</a>, 
<a href="/search/cs?searchtype=author&query=Dernoncourt%2C+F">Franck Dernoncourt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) have shown remarkable advances in language
generation and understanding but are also prone to exhibiting harmful social
biases. While recognition of these behaviors has generated an abundance of bias
mitigation techniques, most require modifications to the training data, model
parameters, or decoding strategy, which may be infeasible without access to a
trainable model. In this work, we leverage the zero-shot capabilities of LLMs
to reduce stereotyping in a technique we introduce as zero-shot self-debiasing.
With two approaches, self-debiasing via explanation and self-debiasing via
reprompting, we show that self-debiasing can significantly reduce the degree of
stereotyping across nine different social groups while relying only on the LLM
itself and a simple prompt, with explanations correctly identifying invalid
assumptions and reprompting delivering the greatest reductions in bias. We hope
this work opens inquiry into other zero-shot techniques for bias mitigation.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01982" title="Abstract">arXiv:2402.01982</a> [<a href="/pdf/2402.01982" title="Download PDF">pdf</a>, <a href="/ps/2402.01982" title="Download PostScript">ps</a>, <a href="/format/2402.01982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Proof-theoretic Semantics for Intuitionistic Linear Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Buzoku%2C+Y">Yll Buzoku</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">The approach to giving a proof-theoretic semantics for IMLL taken by
Gheorghiu, Gu and Pym is an interesting adaptation of the work presented by
Sandqvist in his 2015 paper for IPL. What is particularly interesting is how
naturally the move to the substructural setting provided a semantics for the
multiplicative fragment of intuitionistic linear logic. Whilst ultimately the
authors of the semantics for IMLL used their foundations to provide a semantics
for bunched implication logic, it begs the question, what of the rest of
intuitionistic linear logic? In this paper, I present a semantics for
intuitionistic linear logic, by first presenting a semantics for the
multiplicative and additive fragment after which we focus solely on considering
the modality "of-course", thus giving a proof-theoretic semantics for
intuitionistic linear logic.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01985" title="Abstract">arXiv:2402.01985</a> [<a href="/pdf/2402.01985" title="Download PDF">pdf</a>, <a href="/format/2402.01985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Discrete-time Dynamical Model for Optimal Dispatching and Rebalancing  of Autonomous Mobility-on-Demand Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Aalipour%2C+A">Ali Aalipour</a>, 
<a href="/search/eess?searchtype=author&query=Khani%2C+A">Alireza Khani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Autonomous vehicles are rapidly evolving and will soon enable the application
of large-scale mobility-on-demand (MoD) systems. Managing the fleets of
available vehicles, commonly known as "rebalancing," is crucial to ensure that
vehicles are distributed properly to meet customer demands. This paper presents
an optimal control approach to optimize vehicle scheduling and rebalancing in
an autonomous mobility-on-demand (AMoD) system. We use graph theory to model a
city partitioned into virtual zones. Zones represent small areas of the city
where vehicles can stop and pick up/drop off customers, whereas links denote
corridors of the city along which autonomous vehicles can move. They are
considered vertices and edges in the graph. Vehicles employed in the AMoD
scheme are autonomous, and rebalancing can be executed by dispatching available
empty vehicles to areas undersupplied. Rebalancing is performed on the graph's
vertices, i.e., between city areas. We propose a linear, discrete-time model of
an AMoD system using a transformed network. After acquiring the model, the
desired number of rebalancing vehicles for the AMoD model is derived through an
optimization problem. Moreover, the well-posedness of the model is illustrated.
To leverage the proposed model, we implemented the model predictive control
(MPC) framework to find the optimal rebalancing and scheduling policy. We show
the MPC's effectiveness and how the MPC framework can be implemented in
real-time for a real-world case study. The numerical results show that the MPC
with a linear cost function and linear reference, which it tracks, is
effective, outperforming other MPC-based and state-of-the-art algorithms across
all evaluation criteria.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01987" title="Abstract">arXiv:2402.01987</a> [<a href="/pdf/2402.01987" title="Download PDF">pdf</a>, <a href="/format/2402.01987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Transfer Learning for RSV Case Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yiming Sun</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yuhe Gao</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+R">Runxue Bao</a>, 
<a href="/search/cs?searchtype=author&query=Cooper%2C+G+F">Gregory F. Cooper</a>, 
<a href="/search/cs?searchtype=author&query=Espino%2C+J">Jessi Espino</a>, 
<a href="/search/cs?searchtype=author&query=Hochheiser%2C+H">Harry Hochheiser</a>, 
<a href="/search/cs?searchtype=author&query=Michaels%2C+M+G">Marian G. Michaels</a>, 
<a href="/search/cs?searchtype=author&query=Aronis%2C+J+M">John M. Aronis</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Ye Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Transfer learning has become a pivotal technique in machine learning,
renowned for its effectiveness in various real-world applications. However, a
significant challenge arises when applying this approach to sequential
epidemiological data, often characterized by a scarcity of labeled information.
To address this challenge, we introduce Predictive Volume-Adaptive Weighting
(PVAW), a novel online multi-source transfer learning method. PVAW innovatively
implements a dynamic weighting mechanism within an ensemble model, allowing for
the automatic adjustment of weights based on the relevance and contribution of
each source and target model. We demonstrate the effectiveness of PVAW through
its application in analyzing Respiratory Syncytial Virus (RSV) data, collected
over multiple seasons at the University of Pittsburgh Medical Center. Our
method showcases significant improvements in model performance over existing
baselines, highlighting the potential of online transfer learning in handling
complex, sequential data. This study not only underscores the adaptability and
sophistication of transfer learning in healthcare but also sets a new direction
for future research in creating advanced predictive models.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01988" title="Abstract">arXiv:2402.01988</a> [<a href="/pdf/2402.01988" title="Download PDF">pdf</a>, <a href="/format/2402.01988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-power scalable multilayer optoelectronic neural networks enabled  with incoherent light
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+A">Alexander Song</a>, 
<a href="/search/cs?searchtype=author&query=Kottapalli%2C+S+N+M">Sai Nikhilesh Murty Kottapalli</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+R">Rahul Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+P">Peer Fischer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Optics (physics.optics)

</div>
<p class="mathjax">Optical approaches have made great strides towards the goal of high-speed,
energy-efficient computing necessary for modern deep learning and AI
applications. Read-in and read-out of data, however, limit the overall
performance of existing approaches. This study introduces a multilayer
optoelectronic computing framework that alternates between optical and
optoelectronic layers to implement matrix-vector multiplications and rectified
linear functions, respectively. Our framework is designed for real-time,
parallelized operations, leveraging 2D arrays of LEDs and photodetectors
connected via independent analog electronics. We experimentally demonstrate
this approach using a system with a three-layer network with two hidden layers
and operate it to recognize images from the MNIST database with a recognition
accuracy of 92% and classify classes from a nonlinear spiral data with 86%
accuracy. By implementing multiple layers of a deep neural network
simultaneously, our approach significantly reduces the number of read-ins and
read-outs required and paves the way for scalable optical accelerators
requiring ultra low energy.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01989" title="Abstract">arXiv:2402.01989</a> [<a href="/pdf/2402.01989" title="Download PDF">pdf</a>, <a href="/ps/2402.01989" title="Download PostScript">ps</a>, <a href="/format/2402.01989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Planning of PV and Battery Resources in Remote Microgrids  Considering Degradation Costs: An Iterative Post-Optimization  Correction-based Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Butt%2C+H+Z">Hassan Zahid Butt</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+X">Xingpeng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The benefits of shifting to renewable energy sources have granted microgrids
considerable attention, especially photovoltaic (PV) systems. However, given
the inherent variable and intermittent nature of solar power, battery energy
storage systems (BESS) are pivotal for a reliable and cost-effective microgrid.
The optimal sizing and energy scheduling of PV and BESS pose significant
importance for minimal investment and operational cost. The associated costs of
degradation for both these sources further add complexity to the overall
planning problem. This paper proposes a microgrid resource planning model for
determining the optimal PV and BESS sizes in combination with natural gas
generators, considering their technical and financial characteristics as well
as the degradation costs of both PV and BESS. Its objective is to minimize the
microgrid-wide total operational and capital cost. The optimization model is
formulated using mixed-integer linear programming to ensure the resource sizing
problem converges with a reasonably small optimality gap. In addition, an
iterative post-optimization BESS degradation cost correction algorithm is
proposed for enhanced accuracy. The results showcase the savings in the overall
objective cost and reductions in solar energy curtailment upon BESS's
inclusion.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01994" title="Abstract">arXiv:2402.01994</a> [<a href="/pdf/2402.01994" title="Download PDF">pdf</a>, <a href="/ps/2402.01994" title="Download PostScript">ps</a>, <a href="/format/2402.01994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-Centered Privacy Research in the Age of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianshi Li</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Sauvik Das</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hao-Ping Lee</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dakuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+B">Bingsheng Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiping Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, CHI EA'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The emergence of large language models (LLMs), and their increased use in
user-facing systems, has led to substantial privacy concerns. To date, research
on these privacy concerns has been model-centered: exploring how LLMs lead to
privacy risks like memorization, or can be used to infer personal
characteristics about people from their content. We argue that there is a need
for more research focusing on the human aspect of these privacy issues: e.g.,
research on how design paradigms for LLMs affect users' disclosure behaviors,
users' mental models and preferences for privacy controls, and the design of
tools, systems, and artifacts that empower end-users to reclaim ownership over
their personal data. To build usable, efficient, and privacy-friendly systems
powered by these models with imperfect privacy properties, our goal is to
initiate discussions to outline an agenda for conducting human-centered
research on privacy issues in LLM-powered systems. This Special Interest Group
(SIG) aims to bring together researchers with backgrounds in usable security
and privacy, human-AI collaboration, NLP, or any other related domains to share
their perspectives and experiences on this problem, to help our community
establish a collective understanding of the challenges, research opportunities,
research methods, and strategies to collaborate with researchers outside of
HCI.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01995" title="Abstract">arXiv:2402.01995</a> [<a href="/pdf/2402.01995" title="Download PDF">pdf</a>, <a href="/format/2402.01995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Uniform Risk Times Sampling: First Approximation Algorithms,  Learning Augmentation with Full Confidence Interval Integration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xueqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+K">Kyra Gan</a>, 
<a href="/search/cs?searchtype=author&query=Keyvanshokooh%2C+E">Esmaeil Keyvanshokooh</a>, 
<a href="/search/cs?searchtype=author&query=Murphy%2C+S">Susan Murphy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">In digital health, the strategy of allocating a limited treatment budget
across available risk times is crucial to reduce user fatigue. This strategy,
however, encounters a significant obstacle due to the unknown actual number of
risk times, a factor not adequately addressed by existing methods lacking
theoretical guarantees. This paper introduces, for the first time, the online
uniform risk times sampling problem within the approximation algorithm
framework. We propose two online approximation algorithms for this problem, one
with and one without learning augmentation, and provide rigorous theoretical
performance guarantees for them using competitive ratio analysis. We assess the
performance of our algorithms using both synthetic experiments and a real-world
case study on HeartSteps mobile applications.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01999" title="Abstract">arXiv:2402.01999</a> [<a href="/pdf/2402.01999" title="Download PDF">pdf</a>, <a href="/format/2402.01999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Hyperdimensional Computing Framework for Online Time Series  Forecasting on the Edge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mejri%2C+M">Mohamed Mejri</a>, 
<a href="/search/cs?searchtype=author&query=Amarnath%2C+C">Chandramouli Amarnath</a>, 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+A">Abhijit Chatterjee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In recent years, both online and offline deep learning models have been
developed for time series forecasting. However, offline deep forecasting models
fail to adapt effectively to changes in time-series data, while online deep
forecasting models are often expensive and have complex training procedures. In
this paper, we reframe the online nonlinear time-series forecasting problem as
one of linear hyperdimensional time-series forecasting. Nonlinear
low-dimensional time-series data is mapped to high-dimensional
(hyperdimensional) spaces for linear hyperdimensional prediction, allowing
fast, efficient and lightweight online time-series forecasting. Our framework,
TSF-HD, adapts to time-series distribution shifts using a novel co-training
framework for its hyperdimensional mapping and its linear hyperdimensional
predictor. TSF-HD is shown to outperform the state of the art, while having
reduced inference latency, for both short-term and long-term time series
forecasting. Our code is publicly available at
<a href="http://github.com/tsfhd2024/tsf-hd.git">this http URL</a>
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02000" title="Abstract">arXiv:2402.02000</a> [<a href="/pdf/2402.02000" title="Download PDF">pdf</a>, <a href="/format/2402.02000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Graph Condensation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongjia Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liangliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Sheng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhuonan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Jiajun%2C+B">Bu Jiajun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Analytics on large-scale graphs have posed significant challenges to
computational efficiency and resource requirements. Recently, Graph
condensation (GC) has emerged as a solution to address challenges arising from
the escalating volume of graph data. The motivation of GC is to reduce the
scale of large graphs to smaller ones while preserving essential information
for downstream tasks. For a better understanding of GC and to distinguish it
from other related topics, we present a formal definition of GC and establish a
taxonomy that systematically categorizes existing methods into three types
based on its objective, and classify the formulations to generate the condensed
graphs into two categories as modifying the original graphs or synthetic
completely new ones. Moreover, our survey includes a comprehensive analysis of
datasets and evaluation metrics in this field. Finally, we conclude by
addressing challenges and limitations, outlining future directions, and
offering concise guidelines to inspire future research in this field.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02001" title="Abstract">arXiv:2402.02001</a> [<a href="/pdf/2402.02001" title="Download PDF">pdf</a>, <a href="/ps/2402.02001" title="Download PostScript">ps</a>, <a href="/format/2402.02001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PANDA: Query Evaluation in Submodular Width
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khamis%2C+M+A">Mahmoud Abo Khamis</a>, 
<a href="/search/cs?searchtype=author&query=Ngo%2C+H+Q">Hung Q. Ngo</a>, 
<a href="/search/cs?searchtype=author&query=Suciu%2C+D">Dan Suciu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">In recent years, several information-theoretic upper bounds have been
introduced on the output size and evaluation cost of database join queries.
These bounds vary in their power depending on both the type of statistics on
input relations and the query plans that they support. This motivated the
search for algorithms that can compute the output of a join query in times that
are bounded by the corresponding information-theoretic bounds. In this paper,
we describe "PANDA", an algorithm that takes a Shannon-inequality that
underlies the bound, and translates each proof step into an algorithmic step
corresponding to some database operation. PANDA computes a full join query in
time given by the largest output size, and computes a Boolean query in time
given by the submodular width. It represents a significant simplification of
the original version in [ANS17].
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02003" title="Abstract">arXiv:2402.02003</a> [<a href="/pdf/2402.02003" title="Download PDF">pdf</a>, <a href="/format/2402.02003" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenFace: A Large-Scale Fine-Grained Face Forgery Benchmark and Cross  Appearance-Edge Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yaning Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zitong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaobin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Linlin Shen</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jianfeng Ren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The rapid advancement of photorealistic generators has reached a critical
juncture where the discrepancy between authentic and manipulated images is
increasingly indistinguishable. Thus, benchmarking and advancing techniques
detecting digital manipulation become an urgent issue. Although there have been
a number of publicly available face forgery datasets, the forgery faces are
mostly generated using GAN-based synthesis technology, which does not involve
the most recent technologies like diffusion. The diversity and quality of
images generated by diffusion models have been significantly improved and thus
a much more challenging face forgery dataset shall be used to evaluate SOTA
forgery detection literature. In this paper, we propose a large-scale, diverse,
and fine-grained high-fidelity dataset, namely GenFace, to facilitate the
advancement of deepfake detection, which contains a large number of forgery
faces generated by advanced generators such as the diffusion-based model and
more detailed labels about the manipulation approaches and adopted generators.
In addition to evaluating SOTA approaches on our benchmark, we design an
innovative cross appearance-edge learning (CAEL) detector to capture
multi-grained appearance and edge global representations, and detect
discriminative and general forgery traces. Moreover, we devise an
appearance-edge cross-attention (AECA) module to explore the various
integrations across two domains. Extensive experiment results and
visualizations show that our detection model outperforms the state of the arts
on different settings like cross-generator, cross-forgery, and cross-dataset
evaluations. Code and datasets will be available at
\url{https://github.com/Jenine-321/GenFace
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02005" title="Abstract">arXiv:2402.02005</a> [<a href="/pdf/2402.02005" title="Download PDF">pdf</a>, <a href="/format/2402.02005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topology-Informed Graph Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y+Y">Yun Young Choi</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S+W">Sun Woo Park</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Minho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Woo%2C+Y">Youngho Woo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Transformers have revolutionized performance in Natural Language Processing
and Vision, paving the way for their integration with Graph Neural Networks
(GNNs). One key challenge in enhancing graph transformers is strengthening the
discriminative power of distinguishing isomorphisms of graphs, which plays a
crucial role in boosting their predictive performances. To address this
challenge, we introduce 'Topology-Informed Graph Transformer (TIGT)', a novel
transformer enhancing both discriminative power in detecting graph isomorphisms
and the overall performance of Graph Transformers. TIGT consists of four
components: A topological positional embedding layer using non-isomorphic
universal covers based on cyclic subgraphs of graphs to ensure unique graph
representation: A dual-path message-passing layer to explicitly encode
topological characteristics throughout the encoder layers: A global attention
mechanism: And a graph information layer to recalibrate channel-wise graph
features for better feature representation. TIGT outperforms previous Graph
Transformers in classifying synthetic dataset aimed at distinguishing
isomorphism classes of graphs. Additionally, mathematical analysis and
empirical evaluations highlight our model's competitive edge over
state-of-the-art Graph Transformers across various benchmark datasets.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02006" title="Abstract">arXiv:2402.02006</a> [<a href="/pdf/2402.02006" title="Download PDF">pdf</a>, <a href="/format/2402.02006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PresAIse, An Enterprises Prescriptive AI Solution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wei Sun</a>, 
<a href="/search/cs?searchtype=author&query=McFaddin%2C+S">Scott McFaddin</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+L+H">Linh Ha Tran</a>, 
<a href="/search/cs?searchtype=author&query=Subramanian%2C+S">Shivaram Subramanian</a>, 
<a href="/search/cs?searchtype=author&query=Greenewald%2C+K">Kristjan Greenewald</a>, 
<a href="/search/cs?searchtype=author&query=Tenzin%2C+Y">Yeshi Tenzin</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Z">Zack Xue</a>, 
<a href="/search/cs?searchtype=author&query=Drissi%2C+Y">Youssef Drissi</a>, 
<a href="/search/cs?searchtype=author&query=Ettl%2C+M">Markus Ettl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Prescriptive AI represents a transformative shift in decision-making,
offering causal insights and actionable recommendations. Despite its huge
potential, enterprise adoption often faces several challenges. The first
challenge is caused by the limitations of observational data for accurate
causal inference which is typically a prerequisite for good decision-making.
The second pertains to the interpretability of recommendations, which is
crucial for enterprise decision-making settings. The third challenge is the
silos between data scientists and business users, hindering effective
collaboration. This paper outlines an initiative from IBM Research, aiming to
address some of these challenges by offering a suite of prescriptive AI
solutions. Leveraging insights from various research papers, the solution suite
includes scalable causal inference methods, interpretable decision-making
approaches, and the integration of large language models (LLMs) to bridge
communication gaps via a conversation agent. A proof-of-concept, PresAIse,
demonstrates the solutions' potential by enabling non-ML experts to interact
with prescriptive AI models via a natural language interface, democratizing
advanced analytics for strategic decision-making.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02007" title="Abstract">arXiv:2402.02007</a> [<a href="/pdf/2402.02007" title="Download PDF">pdf</a>, <a href="/format/2402.02007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Time Series Anomaly State Detection through One-Class  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hanxu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Leng%2C+G">Guangjie Leng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruofan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z+J">Zhi-Qin John Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">For a long time, research on time series anomaly detection has mainly focused
on finding outliers within a given time series. Admittedly, this is consistent
with some practical problems, but in other practical application scenarios,
people are concerned about: assuming a standard time series is given, how to
judge whether another test time series deviates from the standard time series,
which is more similar to the problem discussed in one-class classification
(OCC). Therefore, in this article, we try to re-understand and define the time
series anomaly detection problem through OCC, which we call 'time series
anomaly state detection problem'. We first use stochastic processes and
hypothesis testing to strictly define the 'time series anomaly state detection
problem', and its corresponding anomalies. Then, we use the time series
classification dataset to construct an artificial dataset corresponding to the
problem. We compile 38 anomaly detection algorithms and correct some of the
algorithms to adapt to handle this problem. Finally, through a large number of
experiments, we fairly compare the actual performance of various time series
anomaly detection algorithms, providing insights and directions for future
research by researchers.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02008" title="Abstract">arXiv:2402.02008</a> [<a href="/pdf/2402.02008" title="Download PDF">pdf</a>, <a href="/format/2402.02008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How well do LLMs cite relevant medical references? An evaluation  framework and analyses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kevin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+E">Eric Wu</a>, 
<a href="/search/cs?searchtype=author&query=Cassasola%2C+A">Ally Cassasola</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Angela Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+K">Kevin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Teresa Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Riantawan%2C+S">Sith Riantawan</a>, 
<a href="/search/cs?searchtype=author&query=Riantawan%2C+P+S">Patricia Shi Riantawan</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+D+E">Daniel E. Ho</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">James Zou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) are currently being used to answer medical
questions across a variety of clinical domains. Recent top-performing
commercial LLMs, in particular, are also capable of citing sources to support
their responses. In this paper, we ask: do the sources that LLMs generate
actually support the claims that they make? To answer this, we propose three
contributions. First, as expert medical annotations are an expensive and
time-consuming bottleneck for scalable evaluation, we demonstrate that GPT-4 is
highly accurate in validating source relevance, agreeing 88% of the time with a
panel of medical doctors. Second, we develop an end-to-end, automated pipeline
called \textit{SourceCheckup} and use it to evaluate five top-performing LLMs
on a dataset of 1200 generated questions, totaling over 40K pairs of statements
and sources. Interestingly, we find that between ~50% to 90% of LLM responses
are not fully supported by the sources they provide. We also evaluate GPT-4
with retrieval augmented generation (RAG) and find that, even still, around
30\% of individual statements are unsupported, while nearly half of its
responses are not fully supported. Third, we open-source our curated dataset of
medical questions and expert annotations for future evaluations. Given the
rapid pace of LLM development and the potential harms of incorrect or outdated
medical information, it is crucial to also understand and quantify their
capability to produce relevant, trustworthy medical references.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02009" title="Abstract">arXiv:2402.02009</a> [<a href="/pdf/2402.02009" title="Download PDF">pdf</a>, <a href="/format/2402.02009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Multi-Task Learning with Excess Risks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yifei He</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shiji Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guojun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+H">Hyokun Yun</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+B">Belinda Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Chilimbi%2C+T">Trishul Chilimbi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Han Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Multi-task learning (MTL) considers learning a joint model for multiple tasks
by optimizing a convex combination of all task losses. To solve the
optimization problem, existing methods use an adaptive weight updating scheme,
where task weights are dynamically adjusted based on their respective losses to
prioritize difficult tasks. However, these algorithms face a great challenge
whenever label noise is present, in which case excessive weights tend to be
assigned to noisy tasks that have relatively large Bayes optimal errors,
thereby overshadowing other tasks and causing performance to drop across the
board. To overcome this limitation, we propose Multi-Task Learning with Excess
Risks (ExcessMTL), an excess risk-based task balancing method that updates the
task weights by their distances to convergence instead. Intuitively, ExcessMTL
assigns higher weights to worse-trained tasks that are further from
convergence. To estimate the excess risks, we develop an efficient and accurate
method with Taylor approximation. Theoretically, we show that our proposed
algorithm achieves convergence guarantees and Pareto stationarity. Empirically,
we evaluate our algorithm on various MTL benchmarks and demonstrate its
superior performance over existing methods in the presence of label noise.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02010" title="Abstract">arXiv:2402.02010</a> [<a href="/pdf/2402.02010" title="Download PDF">pdf</a>, <a href="/format/2402.02010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenFormer: A Deep-Learning-Based Approach for Generating Multivariate  Stochastic Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haoran Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Uy%2C+W+I+T">Wayne Isaac Tan Uy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Stochastic generators are essential to produce synthetic realizations that
preserve target statistical properties. We propose GenFormer, a stochastic
generator for spatio-temporal multivariate stochastic processes. It is
constructed using a Transformer-based deep learning model that learns a mapping
between a Markov state sequence and time series values. The synthetic data
generated by the GenFormer model preserves the target marginal distributions
and approximately captures other desired statistical properties even in
challenging applications involving a large number of spatial locations and a
long simulation horizon. The GenFormer model is applied to simulate synthetic
wind speed data at various stations in Florida to calculate exceedance
probabilities for risk management.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02012" title="Abstract">arXiv:2402.02012</a> [<a href="/pdf/2402.02012" title="Download PDF">pdf</a>, <a href="/format/2402.02012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Precise Knowledge Transfer via Flow Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+S">Shitong Shao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zhiqiang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+L">Linrui Gong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huanran Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xu Dai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we propose a novel knowledge transfer framework that
introduces continuous normalizing flows for progressive knowledge
transformation and leverages multi-step sampling strategies to achieve
precision knowledge transfer. We name this framework Knowledge Transfer with
Flow Matching (FM-KT), which can be integrated with a metric-based distillation
method with any form (\textit{e.g.} vanilla KD, DKD, PKD and DIST) and a
meta-encoder with any available architecture (\textit{e.g.} CNN, MLP and
Transformer). By introducing stochastic interpolants, FM-KD is readily amenable
to arbitrary noise schedules (\textit{e.g.}, VP-ODE, VE-ODE, Rectified flow)
for normalized flow path estimation. We theoretically demonstrate that the
training objective of FM-KT is equivalent to minimizing the upper bound of the
teacher feature map or logit negative log-likelihood. Besides, FM-KT can be
viewed as a unique implicit ensemble method that leads to performance gains. By
slightly modifying the FM-KT framework, FM-KT can also be transformed into an
online distillation framework OFM-KT with desirable performance gains. Through
extensive experiments on CIFAR-100, ImageNet-1k, and MS-COCO datasets, we
empirically validate the scalability and state-of-the-art performance of our
proposed methods among relevant comparison approaches.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02017" title="Abstract">arXiv:2402.02017</a> [<a href="/pdf/2402.02017" title="Download PDF">pdf</a>, <a href="/format/2402.02017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Value-Aided Conditional Supervised Learning for Offline RL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jeonghye Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Suyoung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+W">Woojun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Sung%2C+Y">Youngchul Sung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Offline reinforcement learning (RL) has seen notable advancements through
return-conditioned supervised learning (RCSL) and value-based methods, yet each
approach comes with its own set of practical challenges. Addressing these, we
propose Value-Aided Conditional Supervised Learning (VCS), a method that
effectively synergizes the stability of RCSL with the stitching ability of
value-based methods. Based on the Neural Tangent Kernel analysis to discern
instances where value function may not lead to stable stitching, VCS injects
the value aid into the RCSL's loss function dynamically according to the
trajectory return. Our empirical studies reveal that VCS not only significantly
outperforms both RCSL and value-based methods but also consistently achieves,
or often surpasses, the highest trajectory returns across diverse offline RL
benchmarks. This breakthrough in VCS paves new paths in offline RL, pushing the
limits of what can be achieved and fostering further innovations.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02018" title="Abstract">arXiv:2402.02018</a> [<a href="/pdf/2402.02018" title="Download PDF">pdf</a>, <a href="/format/2402.02018" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Position Paper: The Landscape and Challenges of HPC Research and LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Le Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+N+K">Nesreen K. Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Dutta%2C+A">Akash Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharjee%2C+A">Arijit Bhattacharjee</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Sixing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Mahmud%2C+Q+I">Quazi Ishtiaque Mahmud</a>, 
<a href="/search/cs?searchtype=author&query=Abebe%2C+W">Waqwoya Abebe</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+H">Hung Phan</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+A">Aishwarya Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Butler%2C+B">Branden Butler</a>, 
<a href="/search/cs?searchtype=author&query=Hasabnis%2C+N">Niranjan Hasabnis</a>, 
<a href="/search/cs?searchtype=author&query=Oren%2C+G">Gal Oren</a>, 
<a href="/search/cs?searchtype=author&query=Vo%2C+V+A">Vy A. Vo</a>, 
<a href="/search/cs?searchtype=author&query=Munoz%2C+J+P">Juan Pablo Munoz</a>, 
<a href="/search/cs?searchtype=author&query=Willke%2C+T+L">Theodore L. Willke</a>, 
<a href="/search/cs?searchtype=author&query=Mattson%2C+T">Tim Mattson</a>, 
<a href="/search/cs?searchtype=author&query=Jannesari%2C+A">Ali Jannesari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recently, language models (LMs), especially large language models (LLMs),
have revolutionized the field of deep learning. Both encoder-decoder models and
prompt-based techniques have shown immense potential for natural language
processing and code-based tasks. Over the past several years, many research
labs and institutions have invested heavily in high-performance computing,
approaching or breaching exascale performance levels. In this paper, we posit
that adapting and utilizing such language model-based techniques for tasks in
high-performance computing (HPC) would be very beneficial. This study presents
our reasoning behind the aforementioned position and highlights how existing
ideas can be improved and adapted for HPC tasks.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02020" title="Abstract">arXiv:2402.02020</a> [<a href="/pdf/2402.02020" title="Download PDF">pdf</a>, <a href="/ps/2402.02020" title="Download PostScript">ps</a>, <a href="/format/2402.02020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeuV-SLAM: Fast Neural Multiresolution Voxel Optimization for RGBD Dense  SLAM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+W">Wenzhi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lijun Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">We introduce NeuV-SLAM, a novel dense simultaneous localization and mapping
pipeline based on neural multiresolution voxels, characterized by ultra-fast
convergence and incremental expansion capabilities. This pipeline utilizes RGBD
images as input to construct multiresolution neural voxels, achieving rapid
convergence while maintaining robust incremental scene reconstruction and
camera tracking. Central to our methodology is to propose a novel implicit
representation, termed VDF that combines the implementation of neural signed
distance field (SDF) voxels with an SDF activation strategy. This approach
entails the direct optimization of color features and SDF values anchored
within the voxels, substantially enhancing the rate of scene convergence. To
ensure the acquisition of clear edge delineation, SDF activation is designed,
which maintains exemplary scene representation fidelity even under constraints
of voxel resolution. Furthermore, in pursuit of advancing rapid incremental
expansion with low computational overhead, we developed hashMV, a novel
hash-based multiresolution voxel management structure. This architecture is
complemented by a strategically designed voxel generation technique that
synergizes with a two-dimensional scene prior. Our empirical evaluations,
conducted on the Replica and ScanNet Datasets, substantiate NeuV-SLAM's
exceptional efficacy in terms of convergence speed, tracking accuracy, scene
reconstruction, and rendering quality.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02021" title="Abstract">arXiv:2402.02021</a> [<a href="/pdf/2402.02021" title="Download PDF">pdf</a>, <a href="/format/2402.02021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transfer Learning in ECG Diagnosis: Is It Effective?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+C+V">Cuong V. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Do%2C+C+D">Cuong D.Do</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The adoption of deep learning in ECG diagnosis is often hindered by the
scarcity of large, well-labeled datasets in real-world scenarios, leading to
the use of transfer learning to leverage features learned from larger datasets.
Yet the prevailing assumption that transfer learning consistently outperforms
training from scratch has never been systematically validated. In this study,
we conduct the first extensive empirical study on the effectiveness of transfer
learning in multi-label ECG classification, by investigating comparing the
fine-tuning performance with that of training from scratch, covering a variety
of ECG datasets and deep neural networks. We confirm that fine-tuning is the
preferable choice for small downstream datasets; however, when the dataset is
sufficiently large, training from scratch can achieve comparable performance,
albeit requiring a longer training time to catch up. Furthermore, we find that
transfer learning exhibits better compatibility with convolutional neural
networks than with recurrent neural networks, which are the two most prevalent
architectures for time-series ECG applications. Our results underscore the
importance of transfer learning in ECG diagnosis, yet depending on the amount
of available data, researchers may opt not to use it, considering the
non-negligible cost associated with pre-training.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02023" title="Abstract">arXiv:2402.02023</a> [<a href="/pdf/2402.02023" title="Download PDF">pdf</a>, <a href="/format/2402.02023" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Supervised Contrastive Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Junwoo Park</a>, 
<a href="/search/cs?searchtype=author&query=Gwak%2C+D">Daehoon Gwak</a>, 
<a href="/search/cs?searchtype=author&query=Choo%2C+J">Jaegul Choo</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+E">Edward Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at International Conference on Learning Representations (ICLR) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Long-term forecasting presents unique challenges due to the time and memory
complexity of handling long sequences. Existing methods, which rely on sliding
windows to process long sequences, struggle to effectively capture long-term
variations that are partially caught within the short window (i.e.,
outer-window variations). In this paper, we introduce a novel approach that
overcomes this limitation by employing contrastive learning and enhanced
decomposition architecture, specifically designed to focus on long-term
variations. To this end, our contrastive loss incorporates global
autocorrelation held in the whole time series, which facilitates the
construction of positive and negative pairs in a self-supervised manner. When
combined with our decomposition networks, our contrastive learning
significantly improves long-term forecasting performance. Extensive experiments
demonstrate that our approach outperforms 14 baseline models in multiple
experiments over nine long-term benchmarks, especially in challenging scenarios
that require a significantly long output for forecasting. Source code is
available at
https://github.com/junwoopark92/Self-Supervised-Contrastive-Forecsating.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02025" title="Abstract">arXiv:2402.02025</a> [<a href="/pdf/2402.02025" title="Download PDF">pdf</a>, <a href="/ps/2402.02025" title="Download PostScript">ps</a>, <a href="/format/2402.02025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Constraint Formulations in Safe Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wachi%2C+A">Akifumi Wachi</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xun Shen</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+Y">Yanan Sui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Ensuring safety is critical when applying reinforcement learning (RL) to
real-world problems. Consequently, safe RL emerges as a fundamental and
powerful paradigm for safely optimizing an agent's policy from experimental
data. A popular safe RL approach is based on a constrained criterion, which
solves the problem of maximizing expected cumulative reward under safety
constraints. Though there has been recently a surge of such attempts to achieve
safety in RL, a systematic understanding of the field is difficult due to 1)
the diversity of constraint representations and 2) little discussion of their
interrelations. To address this knowledge gap, we provide a comprehensive
review of representative constraint formulations, along with a curated
selection of algorithms specifically designed for each formulation.
Furthermore, we elucidate the theoretical underpinnings that reveal the
mathematical mutual relations among common problem formulations. We conclude
with a discussion of the current state and future directions of safe
reinforcement learning research.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02026" title="Abstract">arXiv:2402.02026</a> [<a href="/pdf/2402.02026" title="Download PDF">pdf</a>, <a href="/format/2402.02026" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal-Enhanced Objectness Learner for Corner Case Detection in  Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+L">Lixing Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+R">Ruixiao Shi</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiaoyang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yi Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages,6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Previous works on object detection have achieved high accuracy in closed-set
scenarios, but their performance in open-world scenarios is not satisfactory.
One of the challenging open-world problems is corner case detection in
autonomous driving. Existing detectors struggle with these cases, relying
heavily on visual appearance and exhibiting poor generalization ability. In
this paper, we propose a solution by reducing the discrepancy between known and
unknown classes and introduce a multimodal-enhanced objectness notion learner.
Leveraging both vision-centric and image-text modalities, our semi-supervised
learning framework imparts objectness knowledge to the student model, enabling
class-aware detection. Our approach, Multimodal-Enhanced Objectness Learner
(MENOL) for Corner Case Detection, significantly improves recall for novel
classes with lower training costs. By achieving a 76.6% mAR-corner and 79.8%
mAR-agnostic on the CODA-val dataset with just 5100 labeled training images,
MENOL outperforms the baseline ORE by 71.3% and 60.6%, respectively. The code
will be available at https://github.com/tryhiseyyysum/MENOL.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02028" title="Abstract">arXiv:2402.02028</a> [<a href="/pdf/2402.02028" title="Download PDF">pdf</a>, <a href="/format/2402.02028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlearnable Examples For Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yujing Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xingjun Ma</a>, 
<a href="/search/cs?searchtype=author&query=Erfani%2C+S+M">Sarah Monazam Erfani</a>, 
<a href="/search/cs?searchtype=author&query=Bailey%2C+J">James Bailey</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Unlearnable examples (UEs) refer to training samples modified to be
unlearnable to Deep Neural Networks (DNNs). These examples are usually
generated by adding error-minimizing noises that can fool a DNN model into
believing that there is nothing (no error) to learn from the data. The concept
of UE has been proposed as a countermeasure against unauthorized data
exploitation on personal data. While UE has been extensively studied on images,
it is unclear how to craft effective UEs for time series data. In this work, we
introduce the first UE generation method to protect time series data from
unauthorized training by deep learning models. To this end, we propose a new
form of error-minimizing noise that can be \emph{selectively} applied to
specific segments of time series, rendering them unlearnable to DNN models
while remaining imperceptible to human observers. Through extensive experiments
on a wide range of time series datasets, we demonstrate that the proposed UE
generation method is effective in both classification and generation tasks. It
can protect time series data against unauthorized exploitation, while
preserving their utility for legitimate usage, thereby contributing to the
development of secure and trustworthy machine learning systems.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02029" title="Abstract">arXiv:2402.02029</a> [<a href="/pdf/2402.02029" title="Download PDF">pdf</a>, <a href="/format/2402.02029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ScribFormer: Transformer Makes CNN Work Better for Scribble-based  Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zihan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yuan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+D">Dandan Shan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shuzhou Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qingde Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Beizhan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuanting Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+Q">Qingqi Hong</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+D">Dinggang Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transactions on Medical Imaging (TMI)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Most recent scribble-supervised segmentation methods commonly adopt a CNN
framework with an encoder-decoder architecture. Despite its multiple benefits,
this framework generally can only capture small-range feature dependency for
the convolutional layer with the local receptive field, which makes it
difficult to learn global shape information from the limited information
provided by scribble annotations. To address this issue, this paper proposes a
new CNN-Transformer hybrid solution for scribble-supervised medical image
segmentation called ScribFormer. The proposed ScribFormer model has a
triple-branch structure, i.e., the hybrid of a CNN branch, a Transformer
branch, and an attention-guided class activation map (ACAM) branch.
Specifically, the CNN branch collaborates with the Transformer branch to fuse
the local features learned from CNN with the global representations obtained
from Transformer, which can effectively overcome limitations of existing
scribble-supervised segmentation methods. Furthermore, the ACAM branch assists
in unifying the shallow convolution features and the deep convolution features
to improve model's performance further. Extensive experiments on two public
datasets and one private dataset show that our ScribFormer has superior
performance over the state-of-the-art scribble-supervised segmentation methods,
and achieves even better results than the fully-supervised segmentation
methods. The code is released at https://github.com/HUANGLIZI/ScribFormer.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02030" title="Abstract">arXiv:2402.02030</a> [<a href="/pdf/2402.02030" title="Download PDF">pdf</a>, <a href="/format/2402.02030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Panacea: Pareto Alignment via Preference Adaptation for LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yifan Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chengdong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Ziran Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qingfu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+S">Siyuan Qi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Current methods for large language model alignment typically use scalar human
preference labels. However, this convention tends to oversimplify the
multi-dimensional and heterogeneous nature of human preferences, leading to
reduced expressivity and even misalignment. This paper presents Panacea, an
innovative approach that reframes alignment as a multi-dimensional preference
optimization problem. Panacea trains a single model capable of adapting online
and Pareto-optimally to diverse sets of preferences without the need for
further tuning. A major challenge here is using a low-dimensional preference
vector to guide the model's behavior, despite it being governed by an
overwhelmingly large number of parameters. To address this, Panacea is designed
to use singular value decomposition (SVD)-based low-rank adaptation, which
allows the preference vector to be simply injected online as singular values.
Theoretically, we prove that Panacea recovers the entire Pareto front with
common loss aggregation methods under mild conditions. Moreover, our
experiments demonstrate, for the first time, the feasibility of aligning a
single LLM to represent a spectrum of human preferences through various
optimization methods. Our work marks a step forward in effectively and
efficiently aligning models to diverse and intricate human preferences in a
controllable and Pareto-optimal manner.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02031" title="Abstract">arXiv:2402.02031</a> [<a href="/pdf/2402.02031" title="Download PDF">pdf</a>, <a href="/format/2402.02031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-fidelity physics constrained neural networks for dynamical systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Sibo Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Arcucci%2C+R">Rossella Arcucci</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computer Methods in Applied Mechanics and Engineering. 2024 Feb
  15;420:116758
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">Physics-constrained neural networks are commonly employed to enhance
prediction robustness compared to purely data-driven models, achieved through
the inclusion of physical constraint losses during the model training process.
However, one of the major challenges of physics-constrained neural networks
consists of the training complexity especially for high-dimensional systems. In
fact, conventional physics-constrained models rely on singular-fidelity data
necessitating the assessment of physical constraints within high-dimensional
fields, which introduces computational difficulties. Furthermore, due to the
fixed input size of the neural networks, employing multi-fidelity training data
can also be cumbersome. In this paper, we propose the Multi-Scale
Physics-Constrained Neural Network (MSPCNN), which offers a novel methodology
for incorporating data with different levels of fidelity into a unified latent
space through a customised multi-fidelity autoencoder. Additionally, multiple
decoders are concurrently trained to map latent representations of inputs into
various fidelity physical spaces. As a result, during the training of
predictive models, physical constraints can be evaluated within low-fidelity
spaces, yielding a trade-off between training efficiency and accuracy. In
addition, unlike conventional methods, MSPCNN also manages to employ
multi-fidelity data to train the predictive model. We assess the performance of
MSPCNN in two fluid dynamics problems, namely a two-dimensional Burgers' system
and a shallow water system. Numerical results clearly demonstrate the
enhancement of prediction accuracy and noise robustness when introducing
physical constraints in low-fidelity fields. On the other hand, as expected,
the training complexity can be significantly reduced by computing physical
constraint loss in the low-fidelity field rather than the high-fidelity one.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02032" title="Abstract">arXiv:2402.02032</a> [<a href="/pdf/2402.02032" title="Download PDF">pdf</a>, <a href="/format/2402.02032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RobustTSF: Towards Theory and Design of Robust Time Series Forecasting  with Anomalies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Q">Qingsong Wen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Liang Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the 12th International Conference on Learning Representations (ICLR 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Time series forecasting is an important and forefront task in many real-world
applications. However, most of time series forecasting techniques assume that
the training data is clean without anomalies. This assumption is unrealistic
since the collected time series data can be contaminated in practice. The
forecasting model will be inferior if it is directly trained by time series
with anomalies. Thus it is essential to develop methods to automatically learn
a robust forecasting model from the contaminated data. In this paper, we first
statistically define three types of anomalies, then theoretically and
experimentally analyze the loss robustness and sample robustness when these
anomalies exist. Based on our analyses, we propose a simple and efficient
algorithm to learn a robust forecasting model. Extensive experiments show that
our method is highly robust and outperforms all existing approaches. The code
is available at https://github.com/haochenglouis/RobustTSF.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02033" title="Abstract">arXiv:2402.02033</a> [<a href="/pdf/2402.02033" title="Download PDF">pdf</a>, <a href="/ps/2402.02033" title="Download PostScript">ps</a>, <a href="/format/2402.02033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmark for CEC 2024 Competition on Multiparty Multiobjective  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+W">Wenjian Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Peilan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shengxiang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuhui Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 0 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">The competition focuses on Multiparty Multiobjective Optimization Problems
(MPMOPs), where multiple decision makers have conflicting objectives, as seen
in applications like UAV path planning. Despite their importance, MPMOPs remain
understudied in comparison to conventional multiobjective optimization. The
competition aims to address this gap by encouraging researchers to explore
tailored modeling approaches. The test suite comprises two parts: problems with
common Pareto optimal solutions and Biparty Multiobjective UAV Path Planning
(BPMO-UAVPP) problems with unknown solutions. Optimization algorithms for the
first part are evaluated using Multiparty Inverted Generational Distance
(MPIGD), and the second part is evaluated using Multiparty Hypervolume (MPHV)
metrics. The average algorithm ranking across all problems serves as a
performance benchmark.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02034" title="Abstract">arXiv:2402.02034</a> [<a href="/pdf/2402.02034" title="Download PDF">pdf</a>, <a href="/format/2402.02034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Post-Training Reverse-Engineering Defense Against Backdoors in  Deep Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xi Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+D+J">David J. Miller</a>, 
<a href="/search/cs?searchtype=author&query=Kesidis%2C+G">George Kesidis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">A variety of defenses have been proposed against backdoors attacks on deep
neural network (DNN) classifiers. Universal methods seek to reliably detect
and/or mitigate backdoors irrespective of the incorporation mechanism used by
the attacker, while reverse-engineering methods often explicitly assume one. In
this paper, we describe a new detector that: relies on internal feature map of
the defended DNN to detect and reverse-engineer the backdoor and identify its
target class; can operate post-training (without access to the training
dataset); is highly effective for various incorporation mechanisms (i.e., is
universal); and which has low computational overhead and so is scalable. Our
detection approach is evaluated for different attacks on a benchmark CIFAR-10
image classifier.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02036" title="Abstract">arXiv:2402.02036</a> [<a href="/pdf/2402.02036" title="Download PDF">pdf</a>, <a href="/format/2402.02036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpreting Graph Neural Networks with In-Distributed Proxies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuomin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaxing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+J">Jingchao Ni</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoting Li</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+Y">Yuchen Bian</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+M">Md Mezbahul Islam</a>, 
<a href="/search/cs?searchtype=author&query=Mondal%2C+A+M">Ananda Mohan Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hua Wei</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+D">Dongsheng Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 Pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph Neural Networks (GNNs) have become a building block in graph data
processing, with wide applications in critical domains. The growing needs to
deploy GNNs in high-stakes applications necessitate explainability for users in
the decision-making processes. A popular paradigm for the explainability of
GNNs is to identify explainable subgraphs by comparing their labels with the
ones of original graphs. This task is challenging due to the substantial
distributional shift from the original graphs in the training set to the set of
explainable subgraphs, which prevents accurate prediction of labels with the
subgraphs. To address it, in this paper, we propose a novel method that
generates proxy graphs for explainable subgraphs that are in the distribution
of training data. We introduce a parametric method that employs graph
generators to produce proxy graphs. A new training objective based on
information theory is designed to ensure that proxy graphs not only adhere to
the distribution of training data but also preserve essential explanatory
factors. Such generated proxy graphs can be reliably used for approximating the
predictions of the true labels of explainable subgraphs. Empirical evaluations
across various datasets demonstrate our method achieves more accurate
explanations for GNNs.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02037" title="Abstract">arXiv:2402.02037</a> [<a href="/pdf/2402.02037" title="Download PDF">pdf</a>, <a href="/format/2402.02037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EffiBench: Benchmarking the Efficiency of Automatically Generated Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Dong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J+M">Jie M.Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qing%2C+Y">Yuhao Qing</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+H">Heming Cui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 13 figures, 18 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Code generation models have increasingly become integral to aiding software
development, offering assistance in tasks such as code completion, debugging,
and code translation. Although current research has thoroughly examined the
correctness of code produced by code generation models, a vital aspect, i.e.,
the efficiency of the generated code, has often been neglected. This paper
presents EffiBench, a benchmark with 1,000 efficiency-critical coding problems
for assessing the efficiency of code generated by code generation models.
EffiBench contains a diverse set of LeetCode coding problems. Each problem is
paired with an executable human-written canonical solution. With EffiBench, we
empirically examine the capability of 21 Large Language Models (13 open-sourced
and 8 closed-sourced) in generating efficient code. The results demonstrate
that GPT-4-turbo generates the most efficient code, significantly outperforming
Palm-2-chat-bison, Claude-instant-1, Gemini-pro, GPT-4, and GPT-3.5.
Nevertheless, its code efficiency is still worse than the efficiency of
human-written canonical solutions. In particular, the average and worst
execution time of GPT-4-turbo generated code is 1.69 and 45.49 times that of
the canonical solutions.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02042" title="Abstract">arXiv:2402.02042</a> [<a href="/pdf/2402.02042" title="Download PDF">pdf</a>, <a href="/ps/2402.02042" title="Download PostScript">ps</a>, <a href="/format/2402.02042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning General Parameterized Policies for Infinite Horizon Average  Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+Q">Qinbo Bai</a>, 
<a href="/search/cs?searchtype=author&query=Mondal%2C+W+U">Washim Uddin Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+V">Vaneet Aggarwal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2309.01922">arXiv:2309.01922</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper explores the realm of infinite horizon average reward Constrained
Markov Decision Processes (CMDP). To the best of our knowledge, this work is
the first to delve into the regret and constraint violation analysis of average
reward CMDPs with a general policy parametrization. To address this challenge,
we propose a primal dual based policy gradient algorithm that adeptly manages
the constraints while ensuring a low regret guarantee toward achieving a global
optimal policy. In particular, we demonstrate that our proposed algorithm
achieves $\tilde{\mathcal{O}}({T}^{3/4})$ objective regret and
$\tilde{\mathcal{O}}({T}^{3/4})$ constraint violation bounds.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02043" title="Abstract">arXiv:2402.02043</a> [<a href="/pdf/2402.02043" title="Download PDF">pdf</a>, <a href="/format/2402.02043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Plug-in Tiny AI Module for Intelligent and Selective Sensor Data  Transmission
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenjun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Rezvani%2C+A">Arghavan Rezvani</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hanning Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+Y">Yang Ni</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Sanggeon Yun</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+S">Sungheon Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Imani%2C+M">Mohsen Imani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Applications in the Internet of Things (IoT) utilize machine learning to
analyze sensor-generated data. However, a major challenge lies in the lack of
targeted intelligence in current sensing systems, leading to vast data
generation and increased computational and communication costs. To address this
challenge, we propose a novel sensing module to equip sensing frameworks with
intelligent data transmission capabilities by integrating a highly efficient
machine learning model placed near the sensor. This model provides prompt
feedback for the sensing system to transmit only valuable data while discarding
irrelevant information by regulating the frequency of data transmission. The
near-sensor model is quantized and optimized for real-time sensor control. To
enhance the framework's performance, the training process is customized and a
"lazy" sensor deactivation strategy utilizing temporal information is
introduced. The suggested method is orthogonal to other IoT frameworks and can
be considered as a plugin for selective data transmission. The framework is
implemented, encompassing both software and hardware components. The
experiments demonstrate that the framework utilizing the suggested module
achieves over 85% system efficiency in terms of energy consumption and storage,
with negligible impact on performance. This methodology has the potential to
significantly reduce data output from sensors, benefiting a wide range of IoT
applications.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02044" title="Abstract">arXiv:2402.02044</a> [<a href="/pdf/2402.02044" title="Download PDF">pdf</a>, <a href="/format/2402.02044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Locally-Adaptive Quantization for Streaming Vector Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aguerrebere%2C+C">Cecilia Aguerrebere</a>, 
<a href="/search/cs?searchtype=author&query=Hildebrand%2C+M">Mark Hildebrand</a>, 
<a href="/search/cs?searchtype=author&query=Bhati%2C+I+S">Ishwar Singh Bhati</a>, 
<a href="/search/cs?searchtype=author&query=Willke%2C+T">Theodore Willke</a>, 
<a href="/search/cs?searchtype=author&query=Tepper%2C+M">Mariano Tepper</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Retrieving the most similar vector embeddings to a given query among a
massive collection of vectors has long been a key component of countless
real-world applications. The recently introduced Retrieval-Augmented Generation
is one of the most prominent examples. For many of these applications, the
database evolves over time by inserting new data and removing outdated data. In
these cases, the retrieval problem is known as streaming similarity search.
While Locally-Adaptive Vector Quantization (LVQ), a highly efficient vector
compression method, yields state-of-the-art search performance for non-evolving
databases, its usefulness in the streaming setting has not been yet
established. In this work, we study LVQ in streaming similarity search. In
support of our evaluation, we introduce two improvements of LVQ: Turbo LVQ and
multi-means LVQ that boost its search performance by up to 28% and 27%,
respectively. Our studies show that LVQ and its new variants enable blazing
fast vector search, outperforming its closest competitor by up to 9.4x for
identically distributed data and by up to 8.8x under the challenging scenario
of data distribution shifts (i.e., where the statistical distribution of the
data changes over time). We release our contributions as part of Scalable
Vector Search, an open-source library for high-performance similarity search.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02045" title="Abstract">arXiv:2402.02045</a> [<a href="/pdf/2402.02045" title="Download PDF">pdf</a>, <a href="/format/2402.02045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MLIP: Enhancing Medical Visual Representation with Divergence Encoder  and Knowledge-guided Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhe Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L+T">Laurence T. Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+B">Bocheng Ren</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+X">Xin Nie</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhangyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Cheng Tan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The scarcity of annotated data has sparked significant interest in
unsupervised pre-training methods that leverage medical reports as auxiliary
signals for medical visual representation learning. However, existing research
overlooks the multi-granularity nature of medical visual representation and
lacks suitable contrastive learning techniques to improve the models'
generalizability across different granularities, leading to the
underutilization of image-text information. To address this, we propose MLIP, a
novel framework leveraging domain-specific medical knowledge as guiding signals
to integrate language information into the visual domain through image-text
contrastive learning. Our model includes global contrastive learning with our
designed divergence encoder, local token-knowledge-patch alignment contrastive
learning, and knowledge-guided category-level contrastive learning with expert
knowledge. Experimental evaluations reveal the efficacy of our model in
enhancing transfer performance for tasks such as image classification, object
detection, and semantic segmentation. Notably, MLIP surpasses state-of-the-art
methods even with limited annotated data, highlighting the potential of
multimodal pre-training in advancing medical representation learning.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02046" title="Abstract">arXiv:2402.02046</a> [<a href="/pdf/2402.02046" title="Download PDF">pdf</a>, <a href="/format/2402.02046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TCI-Former: Thermal Conduction-Inspired Transformer for Infrared Small  Target Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianxiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhentao Tan</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+Q">Qi Chu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+N">Nenghai Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Infrared small target detection (ISTD) is critical to national security and
has been extensively applied in military areas. ISTD aims to segment small
target pixels from background. Most ISTD networks focus on designing feature
extraction blocks or feature fusion modules, but rarely describe the ISTD
process from the feature map evolution perspective. In the ISTD process, the
network attention gradually shifts towards target areas. We abstract this
process as the directional movement of feature map pixels to target areas
through convolution, pooling and interactions with surrounding pixels, which
can be analogous to the movement of thermal particles constrained by
surrounding variables and particles. In light of this analogy, we propose
Thermal Conduction-Inspired Transformer (TCI-Former) based on the theoretical
principles of thermal conduction. According to thermal conduction differential
equation in heat dynamics, we derive the pixel movement differential equation
(PMDE) in the image domain and further develop two modules: Thermal
Conduction-Inspired Attention (TCIA) and Thermal Conduction Boundary Module
(TCBM). TCIA incorporates finite difference method with PMDE to reach a
numerical approximation so that target body features can be extracted. To
further remove errors in boundary areas, TCBM is designed and supervised by
boundary masks to refine target body features with fine boundary details.
Experiments on IRSTD-1k and NUAA-SIRST demonstrate the superiority of our
method.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02047" title="Abstract">arXiv:2402.02047</a> [<a href="/pdf/2402.02047" title="Download PDF">pdf</a>, <a href="/format/2402.02047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quality and Trust in LLM-generated Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Spiess%2C+C">Claudio Spiess</a>, 
<a href="/search/cs?searchtype=author&query=Gros%2C+D">David Gros</a>, 
<a href="/search/cs?searchtype=author&query=Pai%2C+K+S">Kunal Suresh Pai</a>, 
<a href="/search/cs?searchtype=author&query=Pradel%2C+M">Michael Pradel</a>, 
<a href="/search/cs?searchtype=author&query=Rabin%2C+M+R+I">Md Rafiqul Islam Rabin</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+S">Susmit Jha</a>, 
<a href="/search/cs?searchtype=author&query=Devanbu%2C+P">Prem Devanbu</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+T">Toufique Ahmed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Machine learning models are widely used but can also often be wrong. Users
would benefit from a reliable indication of whether a given output from a given
model should be trusted, so a rational decision can be made whether to use the
output or not. For example, outputs can be associated with a confidence
measure; if this confidence measure is strongly associated with likelihood of
correctness, then the model is said to be well-calibrated. In this case, for
example, high-confidence outputs could be safely accepted, and low-confidence
outputs rejected.
<br />Calibration has so far been studied in non-generative (e.g., classification)
settings, especially in Software Engineering. However, generated code can quite
often be wrong: Developers need to know when they should e.g., directly use,
use after careful review, or discard model-generated code; thus Calibration is
vital in generative settings. However, the notion of correctness of generated
code is non-trivial, and thus so is Calibration. In this paper we make several
contributions. We develop a framework for evaluating the Calibration of
code-generating models. We consider several tasks, correctness criteria,
datasets, and approaches, and find that by and large generative code models are
not well-calibrated out of the box. We then show how Calibration can be
improved, using standard methods such as Platt scaling. Our contributions will
lead to better-calibrated decision-making in the current use of code generated
by language models, and offers a framework for future research to further
improve calibration methods for generative models in Software Engineering.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02051" title="Abstract">arXiv:2402.02051</a> [<a href="/pdf/2402.02051" title="Download PDF">pdf</a>, <a href="/format/2402.02051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonlinear subspace clustering by functional link neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+L">Long Shi</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+L">Lei Cao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhongpu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Badong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yu Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Nonlinear subspace clustering based on a feed-forward neural network has been
demonstrated to provide better clustering accuracy than some advanced subspace
clustering algorithms. While this approach demonstrates impressive outcomes, it
involves a balance between effectiveness and computational cost. In this study,
we employ a functional link neural network to transform data samples into a
nonlinear domain. Subsequently, we acquire a self-representation matrix through
a learning mechanism that builds upon the mapped samples. As the functional
link neural network is a single-layer neural network, our proposed method
achieves high computational efficiency while ensuring desirable clustering
performance. By incorporating the local similarity regularization to enhance
the grouping effect, our proposed method further improves the quality of the
clustering results. Additionally, we introduce a convex combination subspace
clustering scheme, which combining a linear subspace clustering method with the
functional link neural network subspace clustering approach. This combination
approach allows for a dynamic balance between linear and nonlinear
representations. Extensive experiments confirm the advancement of our methods.
The source code will be released on https://lshi91.github.io/ soon.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02052" title="Abstract">arXiv:2402.02052</a> [<a href="/pdf/2402.02052" title="Download PDF">pdf</a>, <a href="/ps/2402.02052" title="Download PostScript">ps</a>, <a href="/format/2402.02052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feature Selection using the concept of Peafowl Mating in IDS
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+P">Partha Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+J">Joy Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Pandey%2C+N">Nilesh Pandey</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Computer Networks &amp; Communications
  (IJCNC) Vol.16, No.1, January 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Cloud computing has high applicability as an Internet based service that
relies on sharing computing resources. Cloud computing provides services that
are Infrastructure based, Platform based and Software based. The popularity of
this technology is due to its superb performance, high level of computing
ability, low cost of services, scalability, availability and flexibility. The
obtainability and openness of data in cloud environment make it vulnerable to
the world of cyber-attacks. To detect the attacks Intrusion Detection System is
used, that can identify the attacks and ensure information security. Such a
coherent and proficient Intrusion Detection System is proposed in this paper to
achieve higher certainty levels regarding safety in cloud environment. In this
paper, the mating behavior of peafowl is incorporated into an optimization
algorithm which in turn is used as a feature selection algorithm. The algorithm
is used to reduce the huge size of cloud data so that the IDS can work
efficiently on the cloud to detect intrusions. The proposed model has been
experimented with NSL-KDD dataset as well as Kyoto dataset and have proved to
be a better as well as an efficient IDS.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02053" title="Abstract">arXiv:2402.02053</a> [<a href="/pdf/2402.02053" title="Download PDF">pdf</a>, <a href="/format/2402.02053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Affordable Generative Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yangbin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junyou Li</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Q">Qiang Fu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+D">Deheng Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">The emergence of large language models (LLMs) has significantly advanced the
simulation of believable interactive agents. However, the substantial cost on
maintaining the prolonged agent interactions poses challenge over the
deployment of believable LLM-based agents. Therefore, in this paper, we develop
Affordable Generative Agents (AGA), a framework for enabling the generation of
believable and low-cost interactions on both agent-environment and inter-agents
levels. Specifically, for agent-environment interactions, we substitute
repetitive LLM inferences with learned policies; while for inter-agent
interactions, we model the social relationships between agents and compress
auxiliary dialogue information. Extensive experiments on multiple environments
show the effectiveness and efficiency of our proposed framework. Also, we delve
into the mechanisms of emergent believable behaviors lying in LLM agents,
demonstrating that agents can only generate finite behaviors in fixed
environments, based upon which, we understand ways to facilitate emergent
interaction behaviors. Our code is publicly available at:
\url{https://github.com/AffordableGenerativeAgents/Affordable-Generative-Agents}.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02054" title="Abstract">arXiv:2402.02054</a> [<a href="/pdf/2402.02054" title="Download PDF">pdf</a>, <a href="/format/2402.02054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Scaling Laws on Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jingzhe Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+H">Haitao Mao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhikai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+N">Neil Shah</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep graph models (e.g., graph neural networks and graph transformers) have
become important techniques for leveraging knowledge across various types of
graphs. Yet, the scaling properties of deep graph models have not been
systematically investigated, casting doubt on the feasibility of achieving
large graph models through enlarging the model and dataset sizes. In this work,
we delve into neural scaling laws on graphs from both model and data
perspectives. We first verify the validity of such laws on graphs, establishing
formulations to describe the scaling behaviors. For model scaling, we
investigate the phenomenon of scaling law collapse and identify overfitting as
the potential reason. Moreover, we reveal that the model depth of deep graph
models can impact the model scaling behaviors, which differ from observations
in other domains such as CV and NLP. For data scaling, we suggest that the
number of graphs can not effectively metric the graph data volume in scaling
law since the sizes of different graphs are highly irregular. Instead, we
reform the data scaling law with the number of edges as the metric to address
the irregular graph sizes. We further demonstrate the reformed law offers a
unified view of the data scaling behaviors for various fundamental graph tasks
including node classification, link prediction, and graph classification. This
work provides valuable insights into neural scaling laws on graphs, which can
serve as an essential step toward large graph models.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02055" title="Abstract">arXiv:2402.02055</a> [<a href="/pdf/2402.02055" title="Download PDF">pdf</a>, <a href="/format/2402.02055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variance Alignment Score: A Simple But Tough-to-Beat Data Selection  Method for Multimodal Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yifang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+W">Wendan Yan</a>, 
<a href="/search/cs?searchtype=author&query=Jamieson%2C+K">Kevin Jamieson</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+S+S">Simon Shaolei Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In recent years, data selection has emerged as a core issue for large-scale
visual-language model pretraining, especially on noisy web-curated datasets.
One widely adopted strategy assigns quality scores such as CLIP similarity for
each sample and retains the data pairs with the highest scores. However, these
approaches are agnostic of data distribution and always fail to select the most
informative samples. To solve this problem, we propose a simple yet
theoretically principled metric named Variance Alignment Score (VAS), which has
the form $\langle \Sigma_{\text{test}}, \Sigma_i\rangle$. Here,
$\Sigma_{\text{test}}$ represents the target (cross-)covariance matrix we aim
to align, potentially based on prior knowledge, while $\Sigma_i$ denotes the
tensor product of single or multi-modal representations for the $i$-th sample.
We further design a new data selection method that maximizes the total VAS. We
provide theoretical analysis in a simplified setting to demonstrate the
theoretical advantage of VAS over random or other existing data selection.
Experimentally, applying VAS and CLIP scores together can outperform baselines
by a margin of $1.3\%$ average on 38 evaluation sets for noisy dataset DataComp
and $2.5\%$ on VTAB for high-quality dataset CC12M. Additionally, our ablation
study also shows visual features are better than text for calculating VAS, and
the related classical experimental design methods may fail under this context.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02056" title="Abstract">arXiv:2402.02056</a> [<a href="/pdf/2402.02056" title="Download PDF">pdf</a>, <a href="/format/2402.02056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AnthroScore: A Computational Linguistic Measure of Anthropomorphism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Myra Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Gligoric%2C+K">Kristina Gligoric</a>, 
<a href="/search/cs?searchtype=author&query=Piccardi%2C+T">Tiziano Piccardi</a>, 
<a href="/search/cs?searchtype=author&query=Jurafsky%2C+D">Dan Jurafsky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024 Main Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Anthropomorphism, or the attribution of human-like characteristics to
non-human entities, has shaped conversations about the impacts and
possibilities of technology. We present AnthroScore, an automatic metric of
implicit anthropomorphism in language. We use a masked language model to
quantify how non-human entities are implicitly framed as human by the
surrounding context. We show that AnthroScore corresponds with human judgments
of anthropomorphism and dimensions of anthropomorphism described in social
science literature. Motivated by concerns of misleading anthropomorphism in
computer science discourse, we use AnthroScore to analyze 15 years of research
papers and downstream news articles. In research papers, we find that
anthropomorphism has steadily increased over time, and that papers related to
language models have the most anthropomorphism. Within ACL papers, temporal
increases in anthropomorphism are correlated with key neural advancements.
Building upon concerns of scientific misinformation in mass media, we identify
higher levels of anthropomorphism in news headlines compared to the research
papers they cite. Since AnthroScore is lexicon-free, it can be directly applied
to a wide range of text sources.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02057" title="Abstract">arXiv:2402.02057</a> [<a href="/pdf/2402.02057" title="Download PDF">pdf</a>, <a href="/format/2402.02057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Break the Sequential Dependency of LLM Inference Using Lookahead  Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yichao Fu</a>, 
<a href="/search/cs?searchtype=author&query=Bailis%2C+P">Peter Bailis</a>, 
<a href="/search/cs?searchtype=author&query=Stoica%2C+I">Ion Stoica</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Autoregressive decoding of large language models (LLMs) is memory bandwidth
bounded, resulting in high latency and significant wastes of the parallel
processing power of modern accelerators. Existing methods for accelerating LLM
decoding often require a draft model (e.g., speculative decoding), which is
nontrivial to obtain and unable to generalize. In this paper, we introduce
Lookahead decoding, an exact, parallel decoding algorithm that accelerates LLM
decoding without needing auxiliary models or data stores. It allows trading
per-step log(FLOPs) to reduce the number of total decoding steps, is more
parallelizable on single or multiple modern accelerators, and is compatible
with concurrent memory-efficient attention (e.g., FlashAttention). Our
implementation of Lookahead decoding can speed up autoregressive decoding by up
to 1.8x on MT-bench and 4x with strong scaling on multiple GPUs in code
completion tasks. Our code is avialable at
https://github.com/hao-ai-lab/LookaheadDecoding
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02060" title="Abstract">arXiv:2402.02060</a> [<a href="/pdf/2402.02060" title="Download PDF">pdf</a>, <a href="/format/2402.02060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffVein: A Unified Diffusion Network for Finger Vein Segmentation and  Authentication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanjun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wenming Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Q">Qingmin Liao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Finger vein authentication, recognized for its high security and specificity,
has become a focal point in biometric research. Traditional methods
predominantly concentrate on vein feature extraction for discriminative
modeling, with a limited exploration of generative approaches. Suffering from
verification failure, existing methods often fail to obtain authentic vein
patterns by segmentation. To fill this gap, we introduce DiffVein, a unified
diffusion model-based framework which simultaneously addresses vein
segmentation and authentication tasks. DiffVein is composed of two dedicated
branches: one for segmentation and the other for denoising. For better feature
interaction between these two branches, we introduce two specialized modules to
improve their collective performance. The first, a mask condition module,
incorporates the semantic information of vein patterns from the segmentation
branch into the denoising process. Additionally, we also propose a Semantic
Difference Transformer (SD-Former), which employs Fourier-space self-attention
and cross-attention modules to extract category embedding before feeding it to
the segmentation task. In this way, our framework allows for a dynamic
interplay between diffusion and segmentation embeddings, thus vein segmentation
and authentication tasks can inform and enhance each other in the joint
training. To further optimize our model, we introduce a Fourier-space
Structural Similarity (FourierSIM) loss function, which is tailored to improve
the denoising network's learning efficacy. Extensive experiments on the USM and
THU-MVFV3V datasets substantiates DiffVein's superior performance, setting new
benchmarks in both vein segmentation and authentication tasks.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02061" title="Abstract">arXiv:2402.02061</a> [<a href="/pdf/2402.02061" title="Download PDF">pdf</a>, <a href="/ps/2402.02061" title="Download PostScript">ps</a>, <a href="/format/2402.02061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Islamic Lifestyle Applications: Meeting the Spiritual Needs of Modern  Muslims
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kabir%2C+M">Mohsinul Kabir</a>, 
<a href="/search/cs?searchtype=author&query=Kabir%2C+M+R">Mohammad Ridwan Kabir</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+R+S">Riasat Siam Islam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">We evaluated contemporary Islamic lifestyle applications supporting religious
practices and motivation among Muslims. We reviewed 11 popular applications
using self-determination theory and the technology-as-experience framework to
assess their support for motivation and affective needs. Most applications lack
features that foster autonomy, competence, and relatedness. We also interviewed
ten devoted Muslim application users to gain insights into their experiences
and unmet needs. Our findings indicate that existing applications fall short in
providing comprehensive learning, social connections, and scholar
consultations. We propose design implications based on our results, including
guided religious information, shareability, virtual community engagement,
scholarly question-answering, and personalized reminders. We aim to inform the
design of Islamic lifestyle applications that better facilitate ritual
practices, benefitting application designers and Muslim communities. Our
research provides valuable insights into the untapped potential for lifestyle
applications to act as religious companions supporting Muslims' spiritual
journey.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02062" title="Abstract">arXiv:2402.02062</a> [<a href="/pdf/2402.02062" title="Download PDF">pdf</a>, <a href="/format/2402.02062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parsimonious Learning-Augmented Approximations for Dense Instances of  $\mathcal{NP}$-hard Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bampis%2C+E">Evripidis Bampis</a>, 
<a href="/search/cs?searchtype=author&query=Escoffier%2C+B">Bruno Escoffier</a>, 
<a href="/search/cs?searchtype=author&query=Xefteris%2C+M">Michalis Xefteris</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">The classical work of (Arora et al., 1999) provides a scheme that gives, for
any $\epsilon&gt;0$, a polynomial time $1-\epsilon$ approximation algorithm for
dense instances of a family of $\mathcal{NP}$-hard problems, such as Max-CUT
and Max-$k$-SAT. In this paper we extend and speed up this scheme using a
logarithmic number of one-bit predictions. We propose a learning augmented
framework which aims at finding fast algorithms which guarantees approximation
consistency, smoothness and robustness with respect to the prediction error. We
provide such algorithms, which moreover use predictions parsimoniously, for
dense instances of various optimization problems.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02063" title="Abstract">arXiv:2402.02063</a> [<a href="/pdf/2402.02063" title="Download PDF">pdf</a>, <a href="/format/2402.02063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving the Learning of Code Review Successive Tasks with Cross-Task  Knowledge Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sghaier%2C+O+B">Oussama Ben Sghaier</a>, 
<a href="/search/cs?searchtype=author&query=Sahraoui%2C+H">Houari Sahraoui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> FSE'24. arXiv admin note: substantial text overlap with <a href="/abs/2309.03362">arXiv:2309.03362</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Code review is a fundamental process in software development that plays a
pivotal role in ensuring code quality and reducing the likelihood of errors and
bugs. However, code review can be complex, subjective, and time-consuming.
Quality estimation, comment generation, and code refinement constitute the
three key tasks of this process, and their automation has traditionally been
addressed separately in the literature using different approaches. In
particular, recent efforts have focused on fine-tuning pre-trained language
models to aid in code review tasks, with each task being considered in
isolation. We believe that these tasks are interconnected, and their
fine-tuning should consider this interconnection. In this paper, we introduce a
novel deep-learning architecture, named DISCOREV, which employs cross-task
knowledge distillation to address these tasks simultaneously. In our approach,
we utilize a cascade of models to enhance both comment generation and code
refinement models. The fine-tuning of the comment generation model is guided by
the code refinement model, while the fine-tuning of the code refinement model
is guided by the quality estimation model. We implement this guidance using two
strategies: a feedback-based learning objective and an embedding alignment
objective. We evaluate DISCOREV by comparing it to state-of-the-art methods
based on independent training and fine-tuning. Our results show that our
approach generates better review comments, as measured by the BLEU score, as
well as more accurate code refinement according to the CodeBLEU score
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02065" title="Abstract">arXiv:2402.02065</a> [<a href="/pdf/2402.02065" title="Download PDF">pdf</a>, <a href="/format/2402.02065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Implicit Networks for Image Deblurring using Jacobian-Free  Backpropagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Linghai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+S">Shuaicheng Tong</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Lisa Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent efforts in applying implicit networks to solve inverse problems in
imaging have achieved competitive or even superior results when compared to
feedforward networks. These implicit networks only require constant memory
during backpropagation, regardless of the number of layers. However, they are
not necessarily easy to train. Gradient calculations are computationally
expensive because they require backpropagating through a fixed point. In
particular, this process requires solving a large linear system whose size is
determined by the number of features in the fixed point iteration. This paper
explores a recently proposed method, Jacobian-free Backpropagation (JFB), a
backpropagation scheme that circumvents such calculation, in the context of
image deblurring problems. Our results show that JFB is comparable against
fine-tuned optimization schemes, state-of-the-art (SOTA) feedforward networks,
and existing implicit networks at a reduced computational cost.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02066" title="Abstract">arXiv:2402.02066</a> [<a href="/pdf/2402.02066" title="Download PDF">pdf</a>, <a href="/format/2402.02066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trustworthiness of $\mathbb{X}$ Users: A One-Class Classification  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+T">Tanveer Khan</a>, 
<a href="/search/cs?searchtype=author&query=Sohrab%2C+F">Fahad Sohrab</a>, 
<a href="/search/cs?searchtype=author&query=Michalas%2C+A">Antonis Michalas</a>, 
<a href="/search/cs?searchtype=author&query=Gabbouj%2C+M">Moncef Gabbouj</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">$\mathbb{X}$ (formerly Twitter) is a prominent online social media platform
that plays an important role in sharing information making the content
generated on this platform a valuable source of information. Ensuring trust on
$\mathbb{X}$ is essential to determine the user credibility and prevents issues
across various domains. While assigning credibility to $\mathbb{X}$ users and
classifying them as trusted or untrusted is commonly carried out using
traditional machine learning models, there is limited exploration about the use
of One-Class Classification (OCC) models for this purpose. In this study, we
use various OCC models for $\mathbb{X}$ user classification. Additionally, we
propose using a subspace-learning-based approach that simultaneously optimizes
both the subspace and data description for OCC. We also introduce a novel
regularization term for Subspace Support Vector Data Description (SSVDD),
expressing data concentration in a lower-dimensional subspace that captures
diverse graph structures. Experimental results show superior performance of the
introduced regularization term for SSVDD compared to baseline models and
state-of-the-art techniques for $\mathbb{X}$ user classification.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02067" title="Abstract">arXiv:2402.02067</a> [<a href="/pdf/2402.02067" title="Download PDF">pdf</a>, <a href="/format/2402.02067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RIDERS: Radar-Infrared Depth Estimation for Robust Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Han Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yukai Ma</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuehao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yaqing Gu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weihua Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+X">Xingxing Zuo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Dense depth recovery is crucial in autonomous driving, serving as a
foundational element for obstacle avoidance, 3D object detection, and local
path planning. Adverse weather conditions, including haze, dust, rain, snow,
and darkness, introduce significant challenges to accurate dense depth
estimation, thereby posing substantial safety risks in autonomous driving.
These challenges are particularly pronounced for traditional depth estimation
methods that rely on short electromagnetic wave sensors, such as visible
spectrum cameras and near-infrared LiDAR, due to their susceptibility to
diffraction noise and occlusion in such environments. To fundamentally overcome
this issue, we present a novel approach for robust metric depth estimation by
fusing a millimeter-wave Radar and a monocular infrared thermal camera, which
are capable of penetrating atmospheric particles and unaffected by lighting
conditions. Our proposed Radar-Infrared fusion method achieves highly accurate
and finely detailed dense depth estimation through three stages, including
monocular depth prediction with global scale alignment, quasi-dense Radar
augmentation by learning Radar-pixels correspondences, and local scale
refinement of dense depth using a scale map learner. Our method achieves
exceptional visual quality and accurate metric estimation by addressing the
challenges of ambiguity and misalignment that arise from directly fusing
multi-modal long-wave features. We evaluate the performance of our approach on
the NTU4DRadLM dataset and our self-collected challenging ZJU-Multispectrum
dataset. Especially noteworthy is the unprecedented robustness demonstrated by
our proposed method in smoky scenarios. Our code will be released at
\url{https://github.com/MMOCKING/RIDERS}.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02070" title="Abstract">arXiv:2402.02070</a> [<a href="/pdf/2402.02070" title="Download PDF">pdf</a>, <a href="/format/2402.02070" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HotRAP: Hot Record Retention and Promotion for LSM-trees with tiered  storage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+J">Jiansheng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+F">Fangzhou Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Huanchen Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">The multi-level design of Log-Structured Merge-trees (LSM-trees) naturally
fits the tiered storage architecture: the upper levels (recently
inserted/updated records) are kept in fast storage to guarantee performance
while the lower levels (the majority of records) are placed in slower but
cheaper storage to reduce cost. However, frequently accessed records may have
been compacted and reside in slow storage, and existing algorithms are
inefficient in promoting these ``hot'' records to fast storage, leading to
compromised read performance. We present HotRAP, a key-value store based on
RocksDB that can timely promote hot records individually from slow to fast
storage and keep them in fast storage while they are hot. HotRAP uses an
on-disk data structure (a specially-made LSM-tree) to track the hotness of keys
and includes three pathways to ensure that hot records reach fast storage with
short delays. Our experiments show that HotRAP outperforms state-of-the-art
LSM-trees on tiered storage by up to 3.3$\times$ compared to the second best
for read-only and read-write-balanced workloads with common access skew
patterns.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02074" title="Abstract">arXiv:2402.02074</a> [<a href="/pdf/2402.02074" title="Download PDF">pdf</a>, <a href="/format/2402.02074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiple-Crop Human Mesh Recovery with Contrastive Learning and Camera  Consistency in A Single Image
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nie%2C+Y">Yongwei Nie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Changzhen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+C">Chengjiang Long</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guiqing Li</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+H">Hongmin Cai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We tackle the problem of single-image Human Mesh Recovery (HMR). Previous
approaches are mostly based on a single crop. In this paper, we shift the
single-crop HMR to a novel multiple-crop HMR paradigm. Cropping a human from
image multiple times by shifting and scaling the original bounding box is
feasible in practice, easy to implement, and incurs neglectable cost, but
immediately enriches available visual details. With multiple crops as input, we
manage to leverage the relation among these crops to extract discriminative
features and reduce camera ambiguity. Specifically, (1) we incorporate a
contrastive learning scheme to enhance the similarity between features
extracted from crops of the same human. (2) We also propose a crop-aware fusion
scheme to fuse the features of multiple crops for regressing the target mesh.
(3) We compute local cameras for all the input crops and build a
camera-consistency loss between the local cameras, which reward us with less
ambiguous cameras. Based on the above innovations, our proposed method
outperforms previous approaches as demonstrated by the extensive experiments.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02077" title="Abstract">arXiv:2402.02077</a> [<a href="/pdf/2402.02077" title="Download PDF">pdf</a>, <a href="/format/2402.02077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Content Planning for Navigating Trade-offs in  Knowledge-Grounded Dialogue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chawla%2C+K">Kushal Chawla</a>, 
<a href="/search/cs?searchtype=author&query=Rashkin%2C+H">Hannah Rashkin</a>, 
<a href="/search/cs?searchtype=author&query=Tomar%2C+G+S">Gaurav Singh Tomar</a>, 
<a href="/search/cs?searchtype=author&query=Reitter%2C+D">David Reitter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EACL 2024 Main Conference (Long)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Knowledge-grounded dialogue generation is a challenging task because it
requires satisfying two fundamental yet often competing constraints: being
responsive in a manner that is specific to what the conversation partner has
said while also being attributable to an underlying source document. In this
work, we bring this trade-off between these two objectives (specificity and
attribution) to light and ask the question: Can explicit content planning
before the response generation help the model to address this challenge? To
answer this question, we design a framework called PLEDGE, which allows us to
experiment with various plan variables explored in prior work, supporting both
metric-agnostic and metric-aware approaches. While content planning shows
promise, our results on whether it can actually help to navigate this trade-off
are mixed -- planning mechanisms that are metric-aware (use automatic metrics
during training) are better at automatic evaluations but underperform in human
judgment compared to metric-agnostic mechanisms. We discuss how this may be
caused by over-fitting to automatic metrics and the need for future work to
better calibrate these metrics towards human judgment. We hope the observations
from our analysis will inform future work that aims to apply content planning
in this context.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02078" title="Abstract">arXiv:2402.02078</a> [<a href="/pdf/2402.02078" title="Download PDF">pdf</a>, <a href="/format/2402.02078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Robustness of Task-oriented Dialogue Systems for  Colloquial German Varieties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Artemova%2C+E">Ekaterina Artemova</a>, 
<a href="/search/cs?searchtype=author&query=Blaschke%2C+V">Verena Blaschke</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in EACL 2024 (main)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Mainstream cross-lingual task-oriented dialogue (ToD) systems leverage the
transfer learning paradigm by training a joint model for intent recognition and
slot-filling in English and applying it, zero-shot, to other languages. We
address a gap in prior research, which often overlooked the transfer to
lower-resource colloquial varieties due to limited test data. Inspired by prior
work on English varieties, we craft and manually evaluate perturbation rules
that transform German sentences into colloquial forms and use them to
synthesize test sets in four ToD datasets. Our perturbation rules cover 18
distinct language phenomena, enabling us to explore the impact of each
perturbation on slot and intent performance. Using these new datasets, we
conduct an experimental evaluation across six different transformers. Here, we
demonstrate that when applied to colloquial varieties, ToD systems maintain
their intent recognition performance, losing 6% (4.62 percentage points) in
accuracy on average. However, they exhibit a significant drop in slot
detection, with a decrease of 31% (21 percentage points) in slot F1 score. Our
findings are further supported by a transfer experiment from Standard American
English to synthetic Urban African American Vernacular English.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02079" title="Abstract">arXiv:2402.02079</a> [<a href="/pdf/2402.02079" title="Download PDF">pdf</a>, <a href="/format/2402.02079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prototypical Contrastive Learning through Alignment and Uniformity for  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ou%2C+Y">Yangxun Ou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+F">Fenglin Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yupeng Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Graph Collaborative Filtering (GCF), one of the most widely adopted
recommendation system methods, effectively captures intricate relationships
between user and item interactions. Graph Contrastive Learning (GCL) based GCF
has gained significant attention as it leverages self-supervised techniques to
extract valuable signals from real-world scenarios. However, many methods
usually learn the instances of discrimination tasks that involve the
construction of contrastive pairs through random sampling. GCL approaches
suffer from sampling bias issues, where the negatives might have a semantic
structure similar to that of the positives, thus leading to a loss of effective
feature representation. To address these problems, we present the
\underline{Proto}typical contrastive learning through \underline{A}lignment and
\underline{U}niformity for recommendation, which is called \textbf{ProtoAU}.
Specifically, we first propose prototypes (cluster centroids) as a latent space
to ensure consistency across different augmentations from the origin graph,
aiming to eliminate the need for random sampling of contrastive pairs.
Furthermore, the absence of explicit negatives means that directly optimizing
the consistency loss between instance and prototype could easily result in
dimensional collapse issues. Therefore, we propose aligning and maintaining
uniformity in the prototypes of users and items as optimization objectives to
prevent falling into trivial solutions. Finally, we conduct extensive
experiments on four datasets and evaluate their performance on the task of link
prediction. Experimental results demonstrate that the proposed ProtoAU
outperforms other representative methods. The source codes of our proposed
ProtoAU are available at \url{https://github.com/oceanlvr/ProtoAU}.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02080" title="Abstract">arXiv:2402.02080</a> [<a href="/pdf/2402.02080" title="Download PDF">pdf</a>, <a href="/format/2402.02080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Translation Errors Significantly Impact Low-Resource Languages in  Cross-Lingual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+A+S">Ashish Sunil Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Fazili%2C+B">Barah Fazili</a>, 
<a href="/search/cs?searchtype=author&query=Jyothi%2C+P">Preethi Jyothi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to main proceedings of "The 18th Conference of the European Chapter of the Association for Computational Linguistics"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Popular benchmarks (e.g., XNLI) used to evaluate cross-lingual language
understanding consist of parallel versions of English evaluation sets in
multiple target languages created with the help of professional translators.
When creating such parallel data, it is critical to ensure high-quality
translations for all target languages for an accurate characterization of
cross-lingual transfer. In this work, we find that translation inconsistencies
do exist and interestingly they disproportionally impact low-resource languages
in XNLI. To identify such inconsistencies, we propose measuring the gap in
performance between zero-shot evaluations on the human-translated and
machine-translated target text across multiple target languages; relatively
large gaps are indicative of translation errors. We also corroborate that
translation errors exist for two target languages, namely Hindi and Urdu, by
doing a manual reannotation of human-translated test instances in these two
languages and finding poor agreement with the original English labels these
instances were supposed to inherit.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02081" title="Abstract">arXiv:2402.02081</a> [<a href="/pdf/2402.02081" title="Download PDF">pdf</a>, <a href="/format/2402.02081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Risk-Sensitive Diffusion: Learning the Underlying Distribution from  Noisy Samples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yangming Li</a>, 
<a href="/search/cs?searchtype=author&query=Luyten%2C+M+R">Max Ruiz Luyten</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">While achieving remarkable performances, we show that diffusion models are
fragile to the presence of noisy samples, limiting their potential in the vast
amount of settings where, unlike image synthesis, we are not blessed with clean
data. Motivated by our finding that such fragility originates from the
distribution gaps between noisy and clean samples along the diffusion process,
we introduce risk-sensitive SDE, a stochastic differential equation that is
parameterized by the risk (i.e., data "dirtiness") to adjust the distributions
of noisy samples, reducing misguidance while benefiting from their contained
information. The optimal expression for risk-sensitive SDE depends on the
specific noise distribution, and we derive its parameterizations that minimize
the misguidance of noisy samples for both Gaussian and general non-Gaussian
perturbations. We conduct extensive experiments on both synthetic and
real-world datasets (e.g., medical time series), showing that our model
effectively recovers the clean data distribution from noisy samples,
significantly outperforming conditional generation baselines.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02082" title="Abstract">arXiv:2402.02082</a> [<a href="/pdf/2402.02082" title="Download PDF">pdf</a>, <a href="/format/2402.02082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GliDe with a CaPE: A Low-Hassle Method to Accelerate Speculative  Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+C">Cunxiao Du</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jing Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Yuanchen%2C+X">Xu Yuanchen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiawei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Sicheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shenggui Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+L">Liqiang Nie</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zhaopeng Tu</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yang You</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Speculative decoding is a relatively new decoding framework that leverages
small and efficient draft models to reduce the latency of LLMs. In this study,
we introduce GliDe and CaPE, two low-hassle modifications to vanilla
speculative decoding to further improve the decoding speed of a frozen LLM.
Specifically, GliDe is a modified draft model architecture that reuses the
cached keys and values from the target LLM, while CaPE is a proposal expansion
method that uses the draft model's confidence scores to help select additional
candidate tokens for verification. Extensive experiments on different
benchmarks demonstrate that our proposed GliDe draft model significantly
reduces the expected decoding latency. Additional evaluation using walltime
reveals that GliDe can accelerate Vicuna models up to 2.17x and further extend
the improvement to 2.61x with CaPE. We will release our code, data, and the
trained draft models.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02084" title="Abstract">arXiv:2402.02084</a> [<a href="/pdf/2402.02084" title="Download PDF">pdf</a>, <a href="/format/2402.02084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting the Markov Property for Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+C">Cunxiao Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zhaopeng Tu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jing Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this paper, we re-examine the Markov property in the context of neural
machine translation. We design a Markov Autoregressive Transformer~(MAT) and
undertake a comprehensive assessment of its performance across four WMT
benchmarks. Our findings indicate that MAT with an order larger than 4 can
generate translations with quality on par with that of conventional
autoregressive transformers. In addition, counter-intuitively, we also find
that the advantages of utilizing a higher-order MAT do not specifically
contribute to the translation of longer sentences.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02085" title="Abstract">arXiv:2402.02085</a> [<a href="/pdf/2402.02085" title="Download PDF">pdf</a>, <a href="/format/2402.02085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeCoF: Generated Video Detection via Frame Consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Long Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiajia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+H">Hongping Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Y">Yong Liao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haiyang Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The escalating quality of video generated by advanced video generation
methods leads to new security challenges in society, which makes generated
video detection an urgent research priority.To foster collaborative research in
this area, we construct the first open-source dataset explicitly for generated
video detection, providing a valuable resource for the community to benchmark
and improve detection methodologies. Through a series of carefully designed
probe experiments, our study explores the significance of temporal and spatial
artifacts in developing general and robust detectors for generated video. Based
on the principle of video frame consistency, we introduce a simple yet
effective detection model (DeCoF) that eliminates the impact of spatial
artifacts during generalizing feature learning. Our extensive experiments
demonstrate the efficacy of DeCoF in detecting videos produced by unseen video
generation models and confirm its powerful generalization capabilities across
several commercial proprietary models.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02088" title="Abstract">arXiv:2402.02088</a> [<a href="/pdf/2402.02088" title="Download PDF">pdf</a>, <a href="/format/2402.02088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DCS-Net: Pioneering Leakage-Free Point Cloud Pretraining Framework with  Global Insights
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhe Li</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhangyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Cheng Tan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L+T">Laurence T. Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Masked autoencoding and generative pretraining have achieved remarkable
success in computer vision and natural language processing, and more recently,
they have been extended to the point cloud domain. Nevertheless, existing point
cloud models suffer from the issue of information leakage due to the
pre-sampling of center points, which leads to trivial proxy tasks for the
models. These approaches primarily focus on local feature reconstruction,
limiting their ability to capture global patterns within point clouds. In this
paper, we argue that the reduced difficulty of pretext tasks hampers the
model's capacity to learn expressive representations. To address these
limitations, we introduce a novel solution called the Differentiable Center
Sampling Network (DCS-Net). It tackles the information leakage problem by
incorporating both global feature reconstruction and local feature
reconstruction as non-trivial proxy tasks, enabling simultaneous learning of
both the global and local patterns within point cloud. Experimental results
demonstrate that our method enhances the expressive capacity of existing point
cloud models and effectively addresses the issue of information leakage.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02089" title="Abstract">arXiv:2402.02089</a> [<a href="/pdf/2402.02089" title="Download PDF">pdf</a>, <a href="/format/2402.02089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recent Advances in Digital Image and Video Forensics, Anti-forensics and  Counter Anti-forensics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Al-Fehani%2C+M">Maryam Al-Fehani</a>, 
<a href="/search/cs?searchtype=author&query=Al-Kuwari%2C+S">Saif Al-Kuwari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Image and video forensics have recently gained increasing attention due to
the proliferation of manipulated images and videos, especially on social media
platforms, such as Twitter and Instagram, which spread disinformation and fake
news. This survey explores image and video identification and forgery detection
covering both manipulated digital media and generative media. However, media
forgery detection techniques are susceptible to anti-forensics; on the other
hand, such anti-forensics techniques can themselves be detected. We therefore
further cover both anti-forensics and counter anti-forensics techniques in
image and video. Finally, we conclude this survey by highlighting some open
problems in this domain.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02090" title="Abstract">arXiv:2402.02090</a> [<a href="/pdf/2402.02090" title="Download PDF">pdf</a>, <a href="/format/2402.02090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physical Perception Network and an All-weather Multi-modality Benchmark  for Adverse Weather Image Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xilai Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wuyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaosong Li</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+H">Haishu Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multi-modality image fusion (MMIF) integrates the complementary information
from different modal images to provide comprehensive and objective
interpretation of a scenes. However, existing MMIF methods lack the ability to
resist different weather interferences in real-life scenarios, preventing them
from being useful in practical applications such as autonomous driving. To
bridge this research gap, we proposed an all-weather MMIF model. Regarding deep
learning architectures, their network designs are often viewed as a black box,
which limits their multitasking capabilities. For deweathering module, we
propose a physically-aware clear feature prediction module based on an
atmospheric scattering model that can deduce variations in light transmittance
from both scene illumination and depth. For fusion module, We utilize a
learnable low-rank representation model to decompose images into low-rank and
sparse components. This highly interpretable feature separation allows us to
better observe and understand images. Furthermore, we have established a
benchmark for MMIF research under extreme weather conditions. It encompasses
multiple scenes under three types of weather: rain, haze, and snow, with each
weather condition further subdivided into various impact levels. Extensive
fusion experiments under adverse weather demonstrate that the proposed
algorithm has excellent detail recovery and multi-modality feature extraction
capabilities.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02092" title="Abstract">arXiv:2402.02092</a> [<a href="/pdf/2402.02092" title="Download PDF">pdf</a>, <a href="/format/2402.02092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crash-perching on vertical poles with a hugging-wing robot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Askari%2C+M">Mohammad Askari</a>, 
<a href="/search/cs?searchtype=author&query=Benciolini%2C+M">Michele Benciolini</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+H">Hoang-Vu Phan</a>, 
<a href="/search/cs?searchtype=author&query=Stewart%2C+W">William Stewart</a>, 
<a href="/search/cs?searchtype=author&query=Ijspeert%2C+A+J">Auke J. Ijspeert</a>, 
<a href="/search/cs?searchtype=author&query=Floreano%2C+D">Dario Floreano</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures. Supplementary material available. Under review at Communications Engineering
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Perching with winged Unmanned Aerial Vehicles has often been solved by means
of complex control or intricate appendages. Here, we present a simple yet novel
method that relies on passive wing morphing for crash-landing on trees and
other types of vertical poles. Inspired by the adaptability of animals' and
bats' limbs in gripping and holding onto trees, we design dual-purpose wings
that enable both aerial gliding and perching on poles. With an upturned nose
design, the robot can passively reorient from horizontal flight to vertical
upon a head-on crash with a pole, followed by hugging with its wings to perch.
We characterize the performance of reorientation and perching in terms of
impact speed and angle, pole material, and size. The robot robustly reorients
at impact angles above 15{\deg} and speeds of 3 m/s to 9 m/s, and can hold onto
various pole types larger than 28% of its wingspan in diameter. We demonstrate
crash-perching on tree trunks with an overall success rate of 71%. The method
opens up new possibilities for the use of aerial robots in applications such as
inspection, maintenance, and biodiversity conservation.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02093" title="Abstract">arXiv:2402.02093</a> [<a href="/pdf/2402.02093" title="Download PDF">pdf</a>, <a href="/format/2402.02093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wireguard: An Efficient Solution for Securing IoT Device Connectivity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jumakhan%2C+H">Haseebullah Jumakhan</a>, 
<a href="/search/cs?searchtype=author&query=Mirzaeinia%2C+A">Amir Mirzaeinia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">The proliferation of vulnerable Internet-of-Things (IoT) devices has enabled
large-scale cyberattacks. Solutions like Hestia and HomeSnitch have failed to
comprehensively address IoT security needs. This research evaluates if
Wireguard, an emerging VPN protocol, can provide efficient security tailored
for resource-constrained IoT systems. We compared Wireguards performance
against standard protocols OpenVPN and IPsec in a simulated IoT environment.
Metrics measured included throughput, latency, and jitter during file
transfers. Initial results reveal Wireguard's potential as a lightweight yet
robust IoT security solution despite disadvantages for Wireguard in our
experimental environment. With further testing, Wireguards simplicity and low
overhead could enable widespread VPN adoption to harden IoT devices against
attacks. The protocols advantages in setup time, performance, and compatibility
make it promising for integration especially on weak IoT processors and
networks.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02094" title="Abstract">arXiv:2402.02094</a> [<a href="/pdf/2402.02094" title="Download PDF">pdf</a>, <a href="/format/2402.02094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Semantic-Visual Alignment for Zero-Shot Remote Sensing Image Scene  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wenjia Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiuniu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhiwei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+M">Mugen Peng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yirong Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in ISPRS P&amp;RS. The code is available at <a href="https://github.com/wenjiaXu/RS_Scene_ZSL">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ISPRS Journal of Photogrammetry and Remote Sensing, Volume 198,
  2023, Pages 140-152
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep neural networks have achieved promising progress in remote sensing (RS)
image classification, for which the training process requires abundant samples
for each class. However, it is time-consuming and unrealistic to annotate
labels for each RS category, given the fact that the RS target database is
increasing dynamically. Zero-shot learning (ZSL) allows for identifying novel
classes that are not seen during training, which provides a promising solution
for the aforementioned problem. However, previous ZSL models mainly depend on
manually-labeled attributes or word embeddings extracted from language models
to transfer knowledge from seen classes to novel classes. Besides, pioneer ZSL
models use convolutional neural networks pre-trained on ImageNet, which focus
on the main objects appearing in each image, neglecting the background context
that also matters in RS scene classification. To address the above problems, we
propose to collect visually detectable attributes automatically. We predict
attributes for each class by depicting the semantic-visual similarity between
attributes and images. In this way, the attribute annotation process is
accomplished by machine instead of human as in other methods. Moreover, we
propose a Deep Semantic-Visual Alignment (DSVA) that take advantage of the
self-attention mechanism in the transformer to associate local image regions
together, integrating the background context information for prediction. The
DSVA model further utilizes the attribute attention maps to focus on the
informative image regions that are essential for knowledge transfer in ZSL, and
maps the visual images into attribute space to perform ZSL classification. With
extensive experiments, we show that our model outperforms other
state-of-the-art models by a large margin on a challenging large-scale RS scene
classification benchmark.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02095" title="Abstract">arXiv:2402.02095</a> [<a href="/pdf/2402.02095" title="Download PDF">pdf</a>, <a href="/format/2402.02095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seeing is not always believing: The Space of Harmless Perturbations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shaofeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Benhao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jie Li</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yuan Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In the context of deep neural networks, we expose the existence of a harmless
perturbation space, where perturbations leave the network output entirely
unaltered. Perturbations within this harmless perturbation space, regardless of
their magnitude when applied to images, exhibit no impact on the network's
outputs of the original images. Specifically, given any linear layer within the
network, where the input dimension $n$ exceeds the output dimension $m$, we
demonstrate the existence of a continuous harmless perturbation subspace with a
dimension of $(n-m)$. Inspired by this, we solve for a family of general
perturbations that consistently influence the network output, irrespective of
their magnitudes. With these theoretical findings, we explore the application
of harmless perturbations for privacy-preserving data usage. Our work reveals
the difference between DNNs and human perception that the significant
perturbations captured by humans may not affect the recognition of DNNs. As a
result, we utilize this gap to design a type of harmless perturbation that is
meaningless for humans while maintaining its recognizable features for DNNs.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02096" title="Abstract">arXiv:2402.02096</a> [<a href="/pdf/2402.02096" title="Download PDF">pdf</a>, <a href="/format/2402.02096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decomposition-based and Interference Perception for Infrared and Visible  Image Fusion in Complex Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xilai Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaosong Li</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+H">Haishu Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Infrared and visible image fusion has emerged as a prominent research in
computer vision. However, little attention has been paid on complex scenes
fusion, causing existing techniques to produce sub-optimal results when suffers
from real interferences. To fill this gap, we propose a decomposition-based and
interference perception image fusion method. Specifically, we classify the
pixels of visible image from the degree of scattering of light transmission,
based on which we then separate the detail and energy information of the image.
This refined decomposition facilitates the proposed model in identifying more
interfering pixels that are in complex scenes. To strike a balance between
denoising and detail preservation, we propose an adaptive denoising scheme for
fusing detail components. Meanwhile, we propose a new weighted fusion rule by
considering the distribution of image energy information from the perspective
of multiple directions. Extensive experiments in complex scenes fusions cover
adverse weathers, noise, blur, overexposure, fire, as well as downstream tasks
including semantic segmentation, object detection, salient object detection and
depth estimation, consistently indicate the effectiveness and superiority of
the proposed method compared with the recent representative methods.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02097" title="Abstract">arXiv:2402.02097</a> [<a href="/pdf/2402.02097" title="Download PDF">pdf</a>, <a href="/format/2402.02097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Settling Decentralized Multi-Agent Coordinated Exploration by Novelty  Sharing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Haobin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Z">Ziluo Ding</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zongqing Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Exploration in decentralized cooperative multi-agent reinforcement learning
faces two challenges. One is that the novelty of global states is unavailable,
while the novelty of local observations is biased. The other is how agents can
explore in a coordinated way. To address these challenges, we propose MACE, a
simple yet effective multi-agent coordinated exploration method. By
communicating only local novelty, agents can take into account other agents'
local novelty to approximate the global novelty. Further, we newly introduce
weighted mutual information to measure the influence of one agent's action on
other agents' accumulated novelty. We convert it as an intrinsic reward in
hindsight to encourage agents to exert more influence on other agents'
exploration and boost coordinated exploration. Empirically, we show that MACE
achieves superior performance in three multi-agent environments with sparse
rewards.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02099" title="Abstract">arXiv:2402.02099</a> [<a href="/pdf/2402.02099" title="Download PDF">pdf</a>, <a href="/format/2402.02099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing the Evaluation of Cross-Lingual Knowledge Transfer in  Multilingual Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rajaee%2C+S">Sara Rajaee</a>, 
<a href="/search/cs?searchtype=author&query=Monz%2C+C">Christof Monz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advances in training multilingual language models on large datasets
seem to have shown promising results in knowledge transfer across languages and
achieve high performance on downstream tasks. However, we question to what
extent the current evaluation benchmarks and setups accurately measure
zero-shot cross-lingual knowledge transfer. In this work, we challenge the
assumption that high zero-shot performance on target tasks reflects high
cross-lingual ability by introducing more challenging setups involving
instances with multiple languages. Through extensive experiments and analysis,
we show that the observed high performance of multilingual models can be
largely attributed to factors not requiring the transfer of actual linguistic
knowledge, such as task- and surface-level knowledge. More specifically, we
observe what has been transferred across languages is mostly data artifacts and
biases, especially for low-resource languages. Our findings highlight the
overlooked drawbacks of existing cross-lingual test data and evaluation setups,
calling for a more nuanced understanding of the cross-lingual capabilities of
multilingual models.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02101" title="Abstract">arXiv:2402.02101</a> [<a href="/pdf/2402.02101" title="Download PDF">pdf</a>, <a href="/format/2402.02101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Large Language Models Good Prompt Optimizers?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+R">Ruotian Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaolei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jian Li</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+N">Nan Du</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">LLM-based Automatic Prompt Optimization, which typically utilizes LLMs as
Prompt Optimizers to self-reflect and refine prompts, has shown promising
performance in recent studies. Despite the success, the underlying mechanism of
this approach remains unexplored, and the true effectiveness of LLMs as Prompt
Optimizers requires further validation. In this work, we conducted a
comprehensive study to uncover the actual mechanism of LLM-based Prompt
Optimization. Our findings reveal that the LLM optimizers struggle to identify
the true causes of errors during reflection, tending to be biased by their own
prior knowledge rather than genuinely reflecting on the errors. Furthermore,
even when the reflection is semantically valid, the LLM optimizers often fail
to generate appropriate prompts for the target models with a single prompt
refinement step, partly due to the unpredictable behaviors of the target
models. Based on the observations, we introduce a new "Automatic Behavior
Optimization" paradigm, which directly optimizes the target model's behavior in
a more controllable manner. We hope our study can inspire new directions for
automatic prompt optimization development.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02103" title="Abstract">arXiv:2402.02103</a> [<a href="/pdf/2402.02103" title="Download PDF">pdf</a>, <a href="/format/2402.02103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> D&#xe9;j&#xe0; Vu Memorization in Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jayaraman%2C+B">Bargav Jayaraman</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+C">Chuan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhuri%2C+K">Kamalika Chaudhuri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Vision-Language Models (VLMs) have emerged as the state-of-the-art
representation learning solution, with myriads of downstream applications such
as image classification, retrieval and generation. A natural question is
whether these models memorize their training data, which also has implications
for generalization. We propose a new method for measuring memorization in VLMs,
which we call d\'ej\`a vu memorization. For VLMs trained on image-caption
pairs, we show that the model indeed retains information about individual
objects in the training images beyond what can be inferred from correlations or
the image caption. We evaluate d\'ej\`a vu memorization at both sample and
population level, and show that it is significant for OpenCLIP trained on as
many as 50M image-caption pairs. Finally, we show that text randomization
considerably mitigates memorization while only moderately impacting the model's
downstream task performance.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02104" title="Abstract">arXiv:2402.02104</a> [<a href="/pdf/2402.02104" title="Download PDF">pdf</a>, <a href="/format/2402.02104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Structure-Aware Representations of Dependent Types
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kogkalidis%2C+K">Konstantinos Kogkalidis</a>, 
<a href="/search/cs?searchtype=author&query=Melkonian%2C+O">Orestis Melkonian</a>, 
<a href="/search/cs?searchtype=author&query=Bernardy%2C+J">Jean-Philippe Bernardy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, submitted to ICML2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">Agda is a dependently-typed programming language and a proof assistant,
pivotal in proof formalization and programming language theory. This paper
extends the Agda ecosystem into machine learning territory, and, vice versa,
makes Agda-related resources available to machine learning practitioners. We
introduce and release a novel dataset of Agda program-proofs that is elaborate
and extensive enough to support various machine learning applications -- the
first of its kind. Leveraging the dataset's ultra-high resolution, detailing
proof states at the sub-type level, we propose a novel neural architecture
targeted at faithfully representing dependently-typed programs on the basis of
structural rather than nominal principles. We instantiate and evaluate our
architecture in a premise selection setup, where it achieves strong initial
results.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02105" title="Abstract">arXiv:2402.02105</a> [<a href="/pdf/2402.02105" title="Download PDF">pdf</a>, <a href="/format/2402.02105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ParZC: Parametric Zero-Cost Proxies for Efficient NAS
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+P">Peijie Dong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lujun Li</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xinglin Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zimian Wei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+X">Xiaowen Chu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent advancements in Zero-shot Neural Architecture Search (NAS) highlight
the efficacy of zero-cost proxies in various NAS benchmarks. Several studies
propose the automated design of zero-cost proxies to achieve SOTA performance
but require tedious searching progress. Furthermore, we identify a critical
issue with current zero-cost proxies: they aggregate node-wise zero-cost
statistics without considering the fact that not all nodes in a neural network
equally impact performance estimation. Our observations reveal that node-wise
zero-cost statistics significantly vary in their contributions to performance,
with each node exhibiting a degree of uncertainty. Based on this insight, we
introduce a novel method called Parametric Zero-Cost Proxies (ParZC) framework
to enhance the adaptability of zero-cost proxies through parameterization. To
address the node indiscrimination, we propose a Mixer Architecture with
Bayesian Network (MABN) to explore the node-wise zero-cost statistics and
estimate node-specific uncertainty. Moreover, we propose DiffKendall as a loss
function to directly optimize Kendall's Tau coefficient in a differentiable
manner so that our ParZC can better handle the discrepancies in ranking
architectures. Comprehensive experiments on NAS-Bench-101, 201, and NDS
demonstrate the superiority of our proposed ParZC compared to existing
zero-shot NAS methods. Additionally, we demonstrate the versatility and
adaptability of ParZC by transferring it to the Vision Transformer search
space.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02106" title="Abstract">arXiv:2402.02106</a> [<a href="/pdf/2402.02106" title="Download PDF">pdf</a>, <a href="/format/2402.02106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CFD-DEM modeling of fracture initiation with polymer injection in  granular media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kazidenov%2C+D">Daniyar Kazidenov</a>, 
<a href="/search/cs?searchtype=author&query=Amanbek%2C+Y">Yerlan Amanbek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">We numerically study mechanisms and conditions of fracture initiation in
granular material induced by non-Newtonian polymer solutions. A coupling
approach of computational fluid dynamics and discrete element method is
utilized to model the fluid flow in a porous medium. The flow behavior of
polymer solutions and drag force acting on particles are calculated based on a
power-law model. The adequacy of the numerical model is confirmed by comparing
the results with a laboratory experiment. The numerical results are consistent
with the experimental data presenting similar tendencies in dimensionless
parameters that incorporate fluid flow rate, rheology, peak pressure, and
confining stress. Results show that fluid flow rate, rheology, and solid
material characteristics strongly influence fracture initiation behavior.
Injecting a more viscous guar-based solution results in wider fractures induced
by a grain displacement. A less viscous XG-based solution creates more linear
fractures dominated by an infiltration. The peak pressure ratio between two
fluids is higher in rigid material compared to softer material. Finally, the
dimensionless parameters $1/\Pi_1$ and $\tau_2$, which consider fluid and solid
material properties accordingly, are good indicators in determining fracture
initiation induced by shear-thinning fluids. Our numerical results show that
fracture initiation occurs above $1/\Pi_1 = 0.06$ and $\tau_2 = 2\cdot
10^{-7}$.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02108" title="Abstract">arXiv:2402.02108</a> [<a href="/pdf/2402.02108" title="Download PDF">pdf</a>, <a href="/format/2402.02108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Synthetic to Real: Unveiling the Power of Synthetic Data for Video  Person Re-ID
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiangqun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+R">Ruize Han</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+W">Wei Feng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we study a new problem of cross-domain video based person
re-identification (Re-ID). Specifically, we take the synthetic video dataset as
the source domain for training and use the real-world videos for testing, which
significantly reduces the dependence on real training data collection and
annotation. To unveil the power of synthetic data for video person Re-ID, we
first propose a self-supervised domain invariant feature learning strategy for
both static and temporal features. Then, to further improve the person
identification ability in the target domain, we develop a mean-teacher scheme
with the self-supervised ID consistency loss. Experimental results on four real
datasets verify the rationality of cross-synthetic-real domain adaption and the
effectiveness of our method. We are also surprised to find that the synthetic
data performs even better than the real data in the cross-domain setting.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02110" title="Abstract">arXiv:2402.02110</a> [<a href="/pdf/2402.02110" title="Download PDF">pdf</a>, <a href="/format/2402.02110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Composite Active Learning: Towards Multi-Domain Active Learning with  Theoretical Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+G">Guang-Yuan Hao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hengguan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haotian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jie Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Active learning (AL) aims to improve model performance within a fixed
labeling budget by choosing the most informative data points to label. Existing
AL focuses on the single-domain setting, where all data come from the same
domain (e.g., the same dataset). However, many real-world tasks often involve
multiple domains. For example, in visual recognition, it is often desirable to
train an image classifier that works across different environments (e.g.,
different backgrounds), where images from each environment constitute one
domain. Such a multi-domain AL setting is challenging for prior methods because
they (1) ignore the similarity among different domains when assigning labeling
budget and (2) fail to handle distribution shift of data across different
domains. In this paper, we propose the first general method, dubbed composite
active learning (CAL), for multi-domain AL. Our approach explicitly considers
the domain-level and instance-level information in the problem; CAL first
assigns domain-level budgets according to domain-level importance, which is
estimated by optimizing an upper error bound that we develop; with the
domain-level budgets, CAL then leverages a certain instance-level query
strategy to select samples to label from each domain. Our theoretical analysis
shows that our method achieves a better error bound compared to current AL
methods. Our empirical results demonstrate that our approach significantly
outperforms the state-of-the-art AL methods on both synthetic and real-world
multi-domain datasets. Code is available at
https://github.com/Wang-ML-Lab/multi-domain-active-learning.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02112" title="Abstract">arXiv:2402.02112</a> [<a href="/pdf/2402.02112" title="Download PDF">pdf</a>, <a href="/format/2402.02112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> S-NeRF++: Autonomous Driving Simulation via Neural Reconstruction and  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yurui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Ziyang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenye Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Feihu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiachen Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Li Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Autonomous driving simulation system plays a crucial role in enhancing
self-driving data and simulating complex and rare traffic scenarios, ensuring
navigation safety. However, traditional simulation systems, which often heavily
rely on manual modeling and 2D image editing, struggled with scaling to
extensive scenes and generating realistic simulation data. In this study, we
present S-NeRF++, an innovative autonomous driving simulation system based on
neural reconstruction. Trained on widely-used self-driving datasets such as
nuScenes and Waymo, S-NeRF++ can generate a large number of realistic street
scenes and foreground objects with high rendering quality as well as offering
considerable flexibility in manipulation and simulation. Specifically, S-NeRF++
is an enhanced neural radiance field for synthesizing large-scale scenes and
moving vehicles, with improved scene parameterization and camera pose learning.
The system effectively utilizes noisy and sparse LiDAR data to refine training
and address depth outliers, ensuring high quality reconstruction and novel-view
rendering. It also provides a diverse foreground asset bank through
reconstructing and generating different foreground vehicles to support
comprehensive scenario creation. Moreover, we have developed an advanced
foreground-background fusion pipeline that skillfully integrates illumination
and shadow effects, further enhancing the realism of our simulations. With the
high-quality simulated data provided by our S-NeRF++, we found the perception
methods enjoy performance boost on several autonomous driving downstream tasks,
which further demonstrate the effectiveness of our proposed simulator.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02113" title="Abstract">arXiv:2402.02113</a> [<a href="/pdf/2402.02113" title="Download PDF">pdf</a>, <a href="/format/2402.02113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-shot Sentiment Analysis in Low-Resource Languages Using a  Multilingual Sentiment Lexicon
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koto%2C+F">Fajri Koto</a>, 
<a href="/search/cs?searchtype=author&query=Beck%2C+T">Tilman Beck</a>, 
<a href="/search/cs?searchtype=author&query=Talat%2C+Z">Zeerak Talat</a>, 
<a href="/search/cs?searchtype=author&query=Gurevych%2C+I">Iryna Gurevych</a>, 
<a href="/search/cs?searchtype=author&query=Baldwin%2C+T">Timothy Baldwin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Improving multilingual language models capabilities in low-resource languages
is generally difficult due to the scarcity of large-scale data in those
languages. In this paper, we relax the reliance on texts in low-resource
languages by using multilingual lexicons in pretraining to enhance multilingual
capabilities. Specifically, we focus on zero-shot sentiment analysis tasks
across 34 languages, including 6 high/medium-resource languages, 25
low-resource languages, and 3 code-switching datasets. We demonstrate that
pretraining using multilingual lexicons, without using any sentence-level
sentiment data, achieves superior zero-shot performance compared to models
fine-tuned on English sentiment datasets, and large language models like
GPT--3.5, BLOOMZ, and XGLM. These findings are observable for unseen
low-resource languages to code-mixed scenarios involving high-resource
languages.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02114" title="Abstract">arXiv:2402.02114</a> [<a href="/pdf/2402.02114" title="Download PDF">pdf</a>, <a href="/format/2402.02114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Handling Delayed Feedback in Distributed Online Optimization : A  Projection-Free Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Tuan-Anh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Thang%2C+N+K">Nguyen Kim Thang</a>, 
<a href="/search/cs?searchtype=author&query=Trystram%2C+D">Denis Trystram</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Learning at the edges has become increasingly important as large quantities
of data are continually generated locally. Among others, this paradigm requires
algorithms that are simple (so that they can be executed by local devices),
robust (again uncertainty as data are continually generated), and reliable in a
distributed manner under network issues, especially delays. In this study, we
investigate the problem of online convex optimization under adversarial delayed
feedback. We propose two projection-free algorithms for centralised and
distributed settings in which they are carefully designed to achieve a regret
bound of O(\sqrt{B}) where B is the sum of delay, which is optimal for the OCO
problem in the delay setting while still being projection-free. We provide an
extensive theoretical study and experimentally validate the performance of our
algorithms by comparing them with existing ones on real-world problems.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02121" title="Abstract">arXiv:2402.02121</a> [<a href="/pdf/2402.02121" title="Download PDF">pdf</a>, <a href="/format/2402.02121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing crop classification accuracy by synthetic SAR-Optical data  generation using deep learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mirzaei%2C+A">Ali Mirzaei</a>, 
<a href="/search/cs?searchtype=author&query=Bagheri%2C+H">Hossein Bagheri</a>, 
<a href="/search/cs?searchtype=author&query=Khosravi%2C+I">Iman Khosravi</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ISPRS Int. J. Geo-Inf. 2023, 12(11), 450
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Crop classification using remote sensing data has emerged as a prominent
research area in recent decades. Studies have demonstrated that fusing SAR and
optical images can significantly enhance the accuracy of classification.
However, a major challenge in this field is the limited availability of
training data, which adversely affects the performance of classifiers. In
agricultural regions, the dominant crops typically consist of one or two
specific types, while other crops are scarce. Consequently, when collecting
training samples to create a map of agricultural products, there is an
abundance of samples from the dominant crops, forming the majority classes.
Conversely, samples from other crops are scarce, representing the minority
classes. Addressing this issue requires overcoming several challenges and
weaknesses associated with traditional data generation methods. These methods
have been employed to tackle the imbalanced nature of the training data.
Nevertheless, they still face limitations in effectively handling the minority
classes. Overall, the issue of inadequate training data, particularly for
minority classes, remains a hurdle that traditional methods struggle to
overcome. In this research, We explore the effectiveness of conditional tabular
generative adversarial network (CTGAN) as a synthetic data generation method
based on a deep learning network, in addressing the challenge of limited
training data for minority classes in crop classification using the fusion of
SAR-optical data. Our findings demonstrate that the proposed method generates
synthetic data with higher quality that can significantly increase the number
of samples for minority classes leading to better performance of crop
classifiers.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02124" title="Abstract">arXiv:2402.02124</a> [<a href="/pdf/2402.02124" title="Download PDF">pdf</a>, <a href="/format/2402.02124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grammar-based evolutionary approach for automated workflow composition  with domain-specific operators and ensemble diversity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barbudo%2C+R">Rafael Barbudo</a>, 
<a href="/search/cs?searchtype=author&query=Ram%C3%ADrez%2C+A">Aurora Ram&#xed;rez</a>, 
<a href="/search/cs?searchtype=author&query=Romero%2C+J+R">Jos&#xe9; Ra&#xfa;l Romero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 7 figures, 6 tables, journal paper
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Applied Soft Computing, 111292. 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The process of extracting valuable and novel insights from raw data involves
a series of complex steps. In the realm of Automated Machine Learning (AutoML),
a significant research focus is on automating aspects of this process,
specifically tasks like selecting algorithms and optimising their
hyper-parameters. A particularly challenging task in AutoML is automatic
workflow composition (AWC). AWC aims to identify the most effective sequence of
data preprocessing and ML algorithms, coupled with their best hyper-parameters,
for a specific dataset. However, existing AWC methods are limited in how many
and in what ways they can combine algorithms within a workflow.
<br />Addressing this gap, this paper introduces EvoFlow, a grammar-based
evolutionary approach for AWC. EvoFlow enhances the flexibility in designing
workflow structures, empowering practitioners to select algorithms that best
fit their specific requirements. EvoFlow stands out by integrating two
innovative features. First, it employs a suite of genetic operators, designed
specifically for AWC, to optimise both the structure of workflows and their
hyper-parameters. Second, it implements a novel updating mechanism that
enriches the variety of predictions made by different workflows. Promoting this
diversity helps prevent the algorithm from overfitting. With this aim, EvoFlow
builds an ensemble whose workflows differ in their misclassified instances.
<br />To evaluate EvoFlow's effectiveness, we carried out empirical validation
using a set of classification benchmarks. We begin with an ablation study to
demonstrate the enhanced performance attributable to EvoFlow's unique
components. Then, we compare EvoFlow with other AWC approaches, encompassing
both evolutionary and non-evolutionary techniques. Our findings show that
EvoFlow's specialised genetic operators and updating mechanism substantially
outperform current leading methods[..]
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02130" title="Abstract">arXiv:2402.02130</a> [<a href="/pdf/2402.02130" title="Download PDF">pdf</a>, <a href="/format/2402.02130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rendering Graphs for Graph Reasoning in Multimodal Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yanbin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+S">Shuai Fu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Weisen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Kwok%2C+J+T">James T. Kwok</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) are increasingly used for various tasks with
graph structures, such as robotic planning, knowledge graph completion, and
common-sense reasoning. Though LLMs can comprehend graph information in a
textual format, they overlook the rich visual modality, which is an intuitive
way for humans to comprehend structural information and conduct graph
reasoning. The potential benefits and capabilities of representing graph
structures as visual images (i.e., visual graph) is still unexplored. In this
paper, we take the first step in incorporating visual information into graph
reasoning tasks and propose a new benchmark GITQA, where each sample is a tuple
(graph, image, textual description). We conduct extensive experiments on the
GITQA benchmark using state-of-the-art multimodal LLMs. Results on graph
reasoning tasks show that combining textual and visual information together
performs better than using one modality alone. Moreover, the LLaVA-7B/13B
models finetuned on the training set achieve higher accuracy than the
closed-source model GPT-4(V). We also study the effects of augmentations in
graph reasoning.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02135" title="Abstract">arXiv:2402.02135</a> [<a href="/pdf/2402.02135" title="Download PDF">pdf</a>, <a href="/format/2402.02135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Moral Judgment and Reasoning Capability of LLMs Change with Language?  A Study using the Multilingual Defining Issues Test
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khandelwal%2C+A">Aditi Khandelwal</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+U">Utkarsh Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Tanmay%2C+K">Kumar Tanmay</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+M">Monojit Choudhury</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024 (main)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper explores the moral judgment and moral reasoning abilities
exhibited by Large Language Models (LLMs) across languages through the Defining
Issues Test. It is a well known fact that moral judgment depends on the
language in which the question is asked. We extend the work of beyond English,
to 5 new languages (Chinese, Hindi, Russian, Spanish and Swahili), and probe
three LLMs -- ChatGPT, GPT-4 and Llama2Chat-70B -- that shows substantial
multilingual text processing and generation abilities. Our study shows that the
moral reasoning ability for all models, as indicated by the post-conventional
score, is substantially inferior for Hindi and Swahili, compared to Spanish,
Russian, Chinese and English, while there is no clear trend for the performance
of the latter four languages. The moral judgments too vary considerably by the
language.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02136" title="Abstract">arXiv:2402.02136</a> [<a href="/pdf/2402.02136" title="Download PDF">pdf</a>, <a href="/format/2402.02136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> User Intent Recognition and Satisfaction with Large Language Models: A  User Study with ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bodonhelyi%2C+A">Anna Bodonhelyi</a>, 
<a href="/search/cs?searchtype=author&query=Bozkir%2C+E">Efe Bozkir</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shuo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kasneci%2C+E">Enkelejda Kasneci</a>, 
<a href="/search/cs?searchtype=author&query=Kasneci%2C+G">Gjergji Kasneci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">The rapid evolution of large language models such as GPT-4 Turbo represents
an impactful paradigm shift in digital interaction and content engagement.
While these models encode vast amounts of human-generated knowledge and excel
in processing diverse data types, recent research shows that they often face
the challenge of accurately responding to specific user intents, leading to
increased user dissatisfaction. Based on a fine-grained intent taxonomy and
intent-based prompt reformulations, we analyze (1) the quality of intent
recognition and (2) user satisfaction with answers from intent-based prompt
reformulations for two recent ChatGPT models, GPT-3.5 Turbo and GPT-4 Turbo.
The results reveal that GPT-4 outperforms GPT-3.5 on the recognition of common
intents, but is conversely often outperformed by GPT-3.5 on the recognition of
less frequent intents. Moreover, whenever the user intent is correctly
recognized, while users are more satisfied with the answers to intent-based
reformulations of GPT 4 compared to GPT-3.5, they tend to be more satisfied
with the answers of the models to their original prompts compared to the
reformulated ones. Finally, the study indicates that users can quickly learn to
formulate their prompts more effectively, once they are shown possible
reformulation templates.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02139" title="Abstract">arXiv:2402.02139</a> [<a href="/pdf/2402.02139" title="Download PDF">pdf</a>, <a href="/format/2402.02139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Deep Ensemble Forest for High Resolution Mapping of PM2.5 from  MODIS MAIAC AOD in Tehran, Iran
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bagheri%2C+H">Hossein Bagheri</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Environ Monit Assess 195, 377 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">High resolution mapping of PM2.5 concentration over Tehran city is
challenging because of the complicated behavior of numerous sources of
pollution and the insufficient number of ground air quality monitoring
stations. Alternatively, high resolution satellite Aerosol Optical Depth (AOD)
data can be employed for high resolution mapping of PM2.5. For this purpose,
different data-driven methods have been used in the literature. Recently, deep
learning methods have demonstrated their ability to estimate PM2.5 from AOD
data. However, these methods have several weaknesses in solving the problem of
estimating PM2.5 from satellite AOD data. In this paper, the potential of the
deep ensemble forest method for estimating the PM2.5 concentration from AOD
data was evaluated. The results showed that the deep ensemble forest method
with R2 = 0.74 gives a higher accuracy of PM2.5 estimation than deep learning
methods (R2 = 0.67) as well as classic data-driven methods such as random
forest (R2 = 0.68). Additionally, the estimated values of PM2.5 using the deep
ensemble forest algorithm were used along with ground data to generate a high
resolution map of PM2.5. Evaluation of the produced PM2.5 map revealed the good
performance of the deep ensemble forest for modeling the variation of PM2.5 in
the city of Tehran.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02140" title="Abstract">arXiv:2402.02140</a> [<a href="/pdf/2402.02140" title="Download PDF">pdf</a>, <a href="/format/2402.02140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Visual Compression: A Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bolin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+S">Shanzhi Yin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Peilin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shiqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yan Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Artificial Intelligence Generated Content (AIGC) is leading a new technical
revolution for the acquisition of digital content and impelling the progress of
visual compression towards competitive performance gains and diverse
functionalities over traditional codecs. This paper provides a thorough review
on the recent advances of generative visual compression, illustrating great
potentials and promising applications in ultra-low bitrate communication,
user-specified reconstruction/filtering, and intelligent machine analysis. In
particular, we review the visual data compression methodologies with deep
generative models, and summarize how compact representation and high-fidelity
reconstruction could be actualized via generative techniques. In addition, we
generalize related generative compression technologies for machine vision and
intelligent analytics. Finally, we discuss the fundamental challenges on
generative visual compression techniques and envision their future research
directions.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02141" title="Abstract">arXiv:2402.02141</a> [<a href="/pdf/2402.02141" title="Download PDF">pdf</a>, <a href="/ps/2402.02141" title="Download PostScript">ps</a>, <a href="/format/2402.02141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-shot sketch-based remote sensing image retrieval based on  multi-level and attention-guided tokenization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Bo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaoshuang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+B">Beiping Song</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhuang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Effectively and efficiently retrieving images from remote sensing databases
is a critical challenge in the realm of remote sensing big data. Utilizing
hand-drawn sketches as retrieval inputs offers intuitive and user-friendly
advantages, yet the potential of multi-level feature integration from sketches
remains underexplored, leading to suboptimal retrieval performance. To address
this gap, our study introduces a novel zero-shot, sketch-based retrieval method
for remote sensing images, leveraging multi-level, attention-guided
tokenization. This approach starts by employing multi-level self-attention
feature extraction to tokenize the query sketches, as well as self-attention
feature extraction to tokenize the candidate images. It then employs
cross-attention mechanisms to establish token correspondence between these two
modalities, facilitating the computation of sketch-to-image similarity. Our
method demonstrates superior retrieval accuracy over existing sketch-based
remote sensing image retrieval techniques, as evidenced by tests on four
datasets. Notably, it also exhibits robust zero-shot learning capabilities and
strong generalizability in handling unseen categories and novel remote sensing
data. The method's scalability can be further enhanced by the pre-calculation
of retrieval tokens for all candidate images in a database. This research
underscores the significant potential of multi-level, attention-guided
tokenization in cross-modal remote sensing image retrieval. For broader
accessibility and research facilitation, we have made the code and dataset used
in this study publicly available online. Code and dataset are available at
https://github.com/Snowstormfly/Cross-modal-retrieval-MLAGT.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02144" title="Abstract">arXiv:2402.02144</a> [<a href="/pdf/2402.02144" title="Download PDF">pdf</a>, <a href="/format/2402.02144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probing Critical Learning Dynamics of PLMs for Hate Speech Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Masud%2C+S">Sarah Masud</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+M+A">Mohammad Aflah Khan</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+V">Vikram Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Akhtar%2C+M+S">Md Shad Akhtar</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+T">Tanmoy Chakraborty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 9 figures, 14 tables. Accepted at EACL'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Despite the widespread adoption, there is a lack of research into how various
critical aspects of pretrained language models (PLMs) affect their performance
in hate speech detection. Through five research questions, our findings and
recommendations lay the groundwork for empirically investigating different
aspects of PLMs' use in hate speech detection. We deep dive into comparing
different pretrained models, evaluating their seed robustness, finetuning
settings, and the impact of pretraining data collection time. Our analysis
reveals early peaks for downstream tasks during pretraining, the limited
benefit of employing a more recent pretraining corpus, and the significance of
specific layers during finetuning. We further call into question the use of
domain-specific models and highlight the need for dynamic datasets for
benchmarking hate speech detection.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02145" title="Abstract">arXiv:2402.02145</a> [<a href="/pdf/2402.02145" title="Download PDF">pdf</a>, <a href="/format/2402.02145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Sentiment Polarity Reduction in News Presentation through  Contextual Perturbation and Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuila%2C+A">Alapan Kuila</a>, 
<a href="/search/cs?searchtype=author&query=Jena%2C+S">Somnath Jena</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+S">Sudeshna Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Chakrabarti%2C+P+P">Partha Pratim Chakrabarti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ICON 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In today's media landscape, where news outlets play a pivotal role in shaping
public opinion, it is imperative to address the issue of sentiment manipulation
within news text. News writers often inject their own biases and emotional
language, which can distort the objectivity of reporting. This paper introduces
a novel approach to tackle this problem by reducing the polarity of latent
sentiments in news content. Drawing inspiration from adversarial attack-based
sentence perturbation techniques and a prompt based method using ChatGPT, we
employ transformation constraints to modify sentences while preserving their
core semantics. Using three perturbation methods: replacement, insertion, and
deletion coupled with a context-aware masked language model, we aim to maximize
the desired sentiment score for targeted news aspects through a beam search
algorithm. Our experiments and human evaluations demonstrate the effectiveness
of these two models in achieving reduced sentiment polarity with minimal
modifications while maintaining textual similarity, fluency, and grammatical
correctness. Comparative analysis confirms the competitive performance of the
adversarial attack based perturbation methods and prompt-based methods,
offering a promising solution to foster more objective news reporting and
combat emotional language bias in the media.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02146" title="Abstract">arXiv:2402.02146</a> [<a href="/pdf/2402.02146" title="Download PDF">pdf</a>, <a href="/format/2402.02146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emergency Computing: An Adaptive Collaborative Inference Method Based on  Hierarchical Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+W">Weiqi Fu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lianming Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Li Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+A">Aiguo Fei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">In achieving effective emergency response, the timely acquisition of
environmental information, seamless command data transmission, and prompt
decision-making are crucial. This necessitates the establishment of a resilient
emergency communication dedicated network, capable of providing communication
and sensing services even in the absence of basic infrastructure. In this
paper, we propose an Emergency Network with Sensing, Communication,
Computation, Caching, and Intelligence (E-SC3I). The framework incorporates
mechanisms for emergency computing, caching, integrated communication and
sensing, and intelligence empowerment. E-SC3I ensures rapid access to a large
user base, reliable data transmission over unstable links, and dynamic network
deployment in a changing environment. However, these advantages come at the
cost of significant computation overhead. Therefore, we specifically
concentrate on emergency computing and propose an adaptive collaborative
inference method (ACIM) based on hierarchical reinforcement learning.
Experimental results demonstrate our method's ability to achieve rapid
inference of AI models with constrained computational and communication
resources.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02147" title="Abstract">arXiv:2402.02147</a> [<a href="/pdf/2402.02147" title="Download PDF">pdf</a>, <a href="/format/2402.02147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Team Collaboration vs Competition: New Fictitious Play Dynamics for  Multi-team Zero-Sum Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Donmez%2C+A+S">Ahmed Said Donmez</a>, 
<a href="/search/cs?searchtype=author&query=Arslantas%2C+Y">Yuksel Arslantas</a>, 
<a href="/search/cs?searchtype=author&query=Sayin%2C+M+O">Muhammed O. Sayin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">This paper presents a new variant of fictitious play (FP) called
team-fictitious-play (Team-FP) that can reach equilibrium in multi-team
competition, different from the other variants of FP. We specifically focus on
zero-sum potential team games with network separable interactions (ZSPTGs),
unifying potential games (if there is a single team) and zero-sum polymatrix
games (if each team has a single member) due to their wide range of
applications from robotics to financial markets beyond two-team games. Similar
to the FP dynamics, in Team-FP, agents follow a simple behavioral rule where
they respond (with some inertia and exploration in the update of actions) to
the last actions of the neighboring team members and the beliefs formed about
the other neighbors' strategies as if the opponents are playing according to
some stationary strategy. We show the almost sure convergence of the empirical
averages of teams' action profiles to near team-Nash equilibrium in ZSPTGs
under standard assumptions on the step sizes used. We formulate a bound on the
approximation error, decaying with the exploration in the agents' responses. We
further examine the performance of the Team-FP dynamics numerically.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02149" title="Abstract">arXiv:2402.02149</a> [<a href="/pdf/2402.02149" title="Download PDF">pdf</a>, <a href="/format/2402.02149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Diffusion Models for Inverse Problems Using Optimal Posterior  Covariance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+X">Xinyu Peng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Ziyang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+W">Wenrui Dai</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+N">Nuoqian Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenglin Li</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">Junni Zou</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Hongkai Xiong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent diffusion models provide a promising zero-shot solution to noisy
linear inverse problems without retraining for specific inverse problems. In
this paper, we propose the first unified interpretation for existing zero-shot
methods from the perspective of approximating the conditional posterior mean
for the reverse diffusion process of conditional sampling. We reveal that
recent methods are equivalent to making isotropic Gaussian approximations to
intractable posterior distributions over clean images given diffused noisy
images, with the only difference in the handcrafted design of isotropic
posterior covariances. Inspired by this finding, we propose a general
plug-and-play posterior covariance optimization based on maximum likelihood
estimation to improve recent methods. To achieve optimal posterior covariance
without retraining, we provide general solutions based on two approaches
specifically designed to leverage pre-trained models with and without reverse
covariances. Experimental results demonstrate that the proposed methods
significantly enhance the overall performance or robustness to hyperparameters
of recent methods. Code is available at
https://github.com/xypeng9903/k-diffusion-inverse-problems
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02150" title="Abstract">arXiv:2402.02150</a> [<a href="/pdf/2402.02150" title="Download PDF">pdf</a>, <a href="/format/2402.02150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Driven Prediction of Seismic Intensity Distributions Featuring  Hybrid Classification-Regression Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mizutani%2C+K">Koyu Mizutani</a>, 
<a href="/search/cs?searchtype=author&query=Mitarai%2C+H">Haruki Mitarai</a>, 
<a href="/search/cs?searchtype=author&query=Miyazaki%2C+K">Kakeru Miyazaki</a>, 
<a href="/search/cs?searchtype=author&query=Kumano%2C+S">Soichiro Kumano</a>, 
<a href="/search/cs?searchtype=author&query=Yamasaki%2C+T">Toshihiko Yamasaki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Earthquakes are among the most immediate and deadly natural disasters that
humans face. Accurately forecasting the extent of earthquake damage and
assessing potential risks can be instrumental in saving numerous lives. In this
study, we developed linear regression models capable of predicting seismic
intensity distributions based on earthquake parameters: location, depth, and
magnitude. Because it is completely data-driven, it can predict intensity
distributions without geographical information. The dataset comprises seismic
intensity data from earthquakes that occurred in the vicinity of Japan between
1997 and 2020, specifically containing 1,857 instances of earthquakes with a
magnitude of 5.0 or greater, sourced from the Japan Meteorological Agency. We
trained both regression and classification models and combined them to take
advantage of both to create a hybrid model. The proposed model outperformed
commonly used Ground Motion Prediction Equations (GMPEs) in terms of the
correlation coefficient, F1 score, and MCC. Furthermore, the proposed model can
predict even abnormal seismic intensity distributions, a task at conventional
GMPEs often struggle.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02152" title="Abstract">arXiv:2402.02152</a> [<a href="/pdf/2402.02152" title="Download PDF">pdf</a>, <a href="/ps/2402.02152" title="Download PostScript">ps</a>, <a href="/format/2402.02152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Position Paper: Why the Shooting in the Dark Method Dominates  Recommender Systems Practice; A Call to Abandon Anti-Utopian Thinking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rohde%2C+D">David Rohde</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Applied recommender systems research is in a curious position. While there is
a very rigorous protocol for measuring performance by A/B testing, best
practice for finding a `B' to test does not explicitly target performance but
rather targets a proxy measure. The success or failure of a given A/B test then
depends entirely on if the proposed proxy is better correlated to performance
than the previous proxy. No principle exists to identify if one proxy is better
than another offline, leaving the practitioners shooting in the dark. The
purpose of this position paper is to question this anti-Utopian thinking and
argue that a non-standard use of the deep learning stacks actually has the
potential to unlock reward optimizing recommendation.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02154" title="Abstract">arXiv:2402.02154</a> [<a href="/pdf/2402.02154" title="Download PDF">pdf</a>, <a href="/format/2402.02154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating the Robustness of Off-Road Autonomous Driving Segmentation  against Adversarial Attacks: A Dataset-Centric analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deoli%2C+P">Pankaj Deoli</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+R">Rohit Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Vierling%2C+A">Axel Vierling</a>, 
<a href="/search/cs?searchtype=author&query=Berns%2C+K">Karsten Berns</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This study investigates the vulnerability of semantic segmentation models to
adversarial input perturbations, in the domain of off-road autonomous driving.
Despite good performance in generic conditions, the state-of-the-art
classifiers are often susceptible to (even) small perturbations, ultimately
resulting in inaccurate predictions with high confidence. Prior research has
directed their focus on making models more robust by modifying the architecture
and training with noisy input images, but has not explored the influence of
datasets in adversarial attacks. Our study aims to address this gap by
examining the impact of non-robust features in off-road datasets and comparing
the effects of adversarial attacks on different segmentation network
architectures. To enable this, a robust dataset is created consisting of only
robust features and training the networks on this robustified dataset. We
present both qualitative and quantitative analysis of our findings, which have
important implications on improving the robustness of machine learning models
in off-road autonomous driving applications. Additionally, this work
contributes to the safe navigation of autonomous robot Unimog U5023 in rough
off-road unstructured environments by evaluating the robustness of segmentation
outputs. The code is publicly available at
https://github.com/rohtkumar/adversarial_attacks_ on_segmentation
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02158" title="Abstract">arXiv:2402.02158</a> [<a href="/pdf/2402.02158" title="Download PDF">pdf</a>, <a href="/format/2402.02158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PatSTEG: Modeling Formation Dynamics of Patent Citation Networks via The  Semantic-Topological Evolutionary Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miao%2C+R">Ran Miao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xueyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+L">Liang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhifei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+M">Minghua Wan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Cairong Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Digital Libraries (cs.DL)

</div>
<p class="mathjax">Patent documents in the patent database (PatDB) are crucial for research,
development, and innovation as they contain valuable technical information.
However, PatDB presents a multifaceted challenge compared to publicly available
preprocessed databases due to the intricate nature of the patent text and the
inherent sparsity within the patent citation network. Although patent text
analysis and citation analysis bring new opportunities to explore patent data
mining, no existing work exploits the complementation of them. To this end, we
propose a joint semantic-topological evolutionary graph learning approach
(PatSTEG) to model the formation dynamics of patent citation networks. More
specifically, we first create a real-world dataset of Chinese patents named
CNPat and leverage its patent texts and citations to construct a patent
citation network. Then, PatSTEG is modeled to study the evolutionary dynamics
of patent citation formation by considering the semantic and topological
information jointly. Extensive experiments are conducted on CNPat and public
datasets to prove the superiority of PatSTEG over other state-of-the-art
methods. All the results provide valuable references for patent literature
research and technical exploration.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02160" title="Abstract">arXiv:2402.02160</a> [<a href="/pdf/2402.02160" title="Download PDF">pdf</a>, <a href="/format/2402.02160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Poisoning for In-context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+P">Pengfei He</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Han Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+Y">Yue Xing</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yamada%2C+M">Makoto Yamada</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">In the domain of large language models (LLMs), in-context learning (ICL) has
been recognized for its innovative ability to adapt to new tasks, relying on
examples rather than retraining or fine-tuning. This paper delves into the
critical issue of ICL's susceptibility to data poisoning attacks, an area not
yet fully explored. We wonder whether ICL is vulnerable, with adversaries
capable of manipulating example data to degrade model performance. To address
this, we introduce ICLPoison, a specialized attacking framework conceived to
exploit the learning mechanisms of ICL. Our approach uniquely employs discrete
text perturbations to strategically influence the hidden states of LLMs during
the ICL process. We outline three representative strategies to implement
attacks under our framework, each rigorously evaluated across a variety of
models and tasks. Our comprehensive tests, including trials on the
sophisticated GPT-4 model, demonstrate that ICL's performance is significantly
compromised under our framework. These revelations indicate an urgent need for
enhanced defense mechanisms to safeguard the integrity and reliability of LLMs
in applications relying on in-context learning.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02164" title="Abstract">arXiv:2402.02164</a> [<a href="/pdf/2402.02164" title="Download PDF">pdf</a>, <a href="/ps/2402.02164" title="Download PostScript">ps</a>, <a href="/format/2402.02164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TSIS: A Supplementary Algorithm to t-SMILES for Fragment-based Molecular  Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Juan-Ni Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+L">Li-Juan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hai-Long Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+R">Ru-Qin Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Biomolecules (q-bio.BM)

</div>
<p class="mathjax">String-based molecular representations, such as SMILES, are a de facto
standard for linearly representing molecular information. However, the must be
paired symbols and the parsing algorithm result in long grammatical
dependencies, making it difficult for even state-of-the-art deep learning
models to accurately comprehend the syntax and semantics. Although DeepSMILES
and SELFIES have addressed certain limitations, they still struggle with
advanced grammar, which makes some strings difficult to read. This study
introduces a supplementary algorithm, TSIS (TSID Simplified), to t-SMILES
family. Comparative experiments between TSIS and another fragment-based linear
solution, SAFE, indicate that SAFE presents challenges in managing long-term
dependencies in grammar. TSIS continues to use the tree defined in t-SMILES as
its foundational data structure, which sets it apart from the SAFE model. The
performance of TSIS models surpasses that of SAFE models, indicating that the
tree structure of the t-SMILES family provides certain advantages.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02165" title="Abstract">arXiv:2402.02165</a> [<a href="/pdf/2402.02165" title="Download PDF">pdf</a>, <a href="/format/2402.02165" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Optimal Adversarial Robust Q-learning with Bellman  Infinity-error
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoran Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zicheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+W">Wang Luo</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Congying Han</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yudong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+T">Tiande Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+S">Shichen Liao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Establishing robust policies is essential to counter attacks or disturbances
affecting deep reinforcement learning (DRL) agents. Recent studies explore
state-adversarial robustness and suggest the potential lack of an optimal
robust policy (ORP), posing challenges in setting strict robustness
constraints. This work further investigates ORP: At first, we introduce a
consistency assumption of policy (CAP) stating that optimal actions in the
Markov decision process remain consistent with minor perturbations, supported
by empirical and theoretical evidence. Building upon CAP, we crucially prove
the existence of a deterministic and stationary ORP that aligns with the
Bellman optimal policy. Furthermore, we illustrate the necessity of
$L^{\infty}$-norm when minimizing Bellman error to attain ORP. This finding
clarifies the vulnerability of prior DRL algorithms that target the Bellman
optimal policy with $L^{1}$-norm and motivates us to train a Consistent
Adversarial Robust Deep Q-Network (CAR-DQN) by minimizing a surrogate of
Bellman Infinity-error. The top-tier performance of CAR-DQN across various
benchmarks validates its practical effectiveness and reinforces the soundness
of our theoretical analysis.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02167" title="Abstract">arXiv:2402.02167</a> [<a href="/pdf/2402.02167" title="Download PDF">pdf</a>, <a href="/format/2402.02167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vi(E)va LLM! A Conceptual Stack for Evaluating and Interpreting  Generative AI-based Visualizations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Podo%2C+L">Luca Podo</a>, 
<a href="/search/cs?searchtype=author&query=Ishmal%2C+M">Muhammad Ishmal</a>, 
<a href="/search/cs?searchtype=author&query=Angelini%2C+M">Marco Angelini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The automatic generation of visualizations is an old task that, through the
years, has shown more and more interest from the research and practitioner
communities. Recently, large language models (LLM) have become an interesting
option for supporting generative tasks related to visualization, demonstrating
initial promising results. At the same time, several pitfalls, like the
multiple ways of instructing an LLM to generate the desired result, the
different perspectives leading the generation (code-based, image-based,
grammar-based), and the presence of hallucinations even for the visualization
generation task, make their usage less affordable than expected. Following
similar initiatives for benchmarking LLMs, this paper copes with the problem of
modeling the evaluation of a generated visualization through an LLM. We propose
a theoretical evaluation stack, EvaLLM, that decomposes the evaluation effort
in its atomic components, characterizes their nature, and provides an overview
of how to implement and interpret them. We also designed and implemented an
evaluation platform that provides a benchmarking resource for the visualization
generation task. The platform supports automatic and manual scoring conducted
by multiple assessors to support a fine-grained and semantic evaluation based
on the EvaLLM stack. Two case studies on GPT3.5-turbo with Code Interpreter and
Llama2-70-b models show the benefits of EvaLLM and illustrate interesting
results on the current state-of-the-art LLM-generated visualizations.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02168" title="Abstract">arXiv:2402.02168</a> [<a href="/pdf/2402.02168" title="Download PDF">pdf</a>, <a href="/format/2402.02168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One Graph Model for Cross-domain Dynamic Link Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanwen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chow%2C+W">Wei Chow</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+Z">Ziwei Chai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chunping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yang Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This work proposes DyExpert, a dynamic graph model for cross-domain link
prediction. It can explicitly model historical evolving processes to learn the
evolution pattern of a specific downstream graph and subsequently make
pattern-specific link predictions. DyExpert adopts a decode-only transformer
and is capable of efficiently parallel training and inference by
\textit{conditioned link generation} that integrates both evolution modeling
and link prediction. DyExpert is trained by extensive dynamic graphs across
diverse domains, comprising 6M dynamic edges. Extensive experiments on eight
untrained graphs demonstrate that DyExpert achieves state-of-the-art
performance in cross-domain link prediction. Compared to the advanced baseline
under the same setting, DyExpert achieves an average of 11.40% improvement
Average Precision across eight graphs. More impressive, it surpasses the fully
supervised performance of 8 advanced baselines on 6 untrained graphs.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02172" title="Abstract">arXiv:2402.02172</a> [<a href="/pdf/2402.02172" title="Download PDF">pdf</a>, <a href="/format/2402.02172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaborative Agents for Software Engineering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+D">Daniel Tang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhenghan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kisub Kim</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yewei Song</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+H">Haoye Tian</a>, 
<a href="/search/cs?searchtype=author&query=Ezzini%2C+S">Saad Ezzini</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yongfeng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Bissyande%2C+J+K+T+F">Jacques Klein Tegawende F. Bissyande</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Code review is a heavily collaborative process, which aims at ensuring the
overall quality and reliability of software. While it provides massive
benefits, the implementation of code review in an organization faces several
challenges that make its automation appealing. Automated code review tools have
been around for a while and are now improving thanks to the adoption of novel
AI models, which help can learn about standard practices and systematically
check that the reviewed code adheres to them. Unfortunately, existing methods
fall short: they often target a single input-output generative model, which
cannot simulate the collaboration interactions in code review to account for
various perspectives; they are also sub-performing on various critical code
review sub-tasks. In this paper, we advance the state of the art in code review
automation by introducing CodeAgent, a novel multi-agent-based system for code
review. Fundamentally, CodeAgent is steered by QA-Checker (short for
``Question-Answer Checking"), a supervision agent, designed specifically to
ensure that all agents' contributions remain relevant to the initial review
question. CodeAgent is autonomous, multi-agent, and Large language
model-driven. To demonstrate the effectiveness of CodeAgent, we performed
experiments to assess its capabilities in various tasks including 1) detection
of inconsistencies between code changes and commit messages, 2) detection of
vulnerability introduction by commits, and 3) validation of adherence to code
style. Our website is accessed in
\url{https://code-agent-new.vercel.app/index.html}.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02175" title="Abstract">arXiv:2402.02175</a> [<a href="/pdf/2402.02175" title="Download PDF">pdf</a>, <a href="/format/2402.02175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Complex Question Answering over Knowledge Graphs through  Evidence Pattern Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+W">Wentao Ding</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinmao Li</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+L">Liangchuan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+Y">Yuzhong Qu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to TheWebConf'24 (WWW 2024). This is a preprint version; the CR version will include more details. Github: <a href="https://github.com/nju-websoft/EPR-KGQA">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Information retrieval (IR) methods for KGQA consist of two stages: subgraph
extraction and answer reasoning. We argue current subgraph extraction methods
underestimate the importance of structural dependencies among evidence facts.
We propose Evidence Pattern Retrieval (EPR) to explicitly model the structural
dependencies during subgraph extraction. We implement EPR by indexing the
atomic adjacency pattern of resource pairs. Given a question, we perform dense
retrieval to obtain atomic patterns formed by resource pairs. We then enumerate
their combinations to construct candidate evidence patterns. These evidence
patterns are scored using a neural model, and the best one is selected to
extract a subgraph for downstream answer reasoning. Experimental results
demonstrate that the EPR-based approach has significantly improved the F1
scores of IR-KGQA methods by over 10 points on ComplexWebQuestions and achieves
competitive performance on WebQuestionsSP.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02181" title="Abstract">arXiv:2402.02181</a> [<a href="/pdf/2402.02181" title="Download PDF">pdf</a>, <a href="/ps/2402.02181" title="Download PostScript">ps</a>, <a href="/format/2402.02181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Ontology-Based multi-domain model in Social Network Analysis:  Experimental validation and case study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Rodr%C3%ADguez%2C+I">Isa&#xed;as Garc&#xed;a-Rodr&#xed;guez</a>, 
<a href="/search/cs?searchtype=author&query=Benavides%2C+C">Carmen Benavides</a>, 
<a href="/search/cs?searchtype=author&query=Al%C3%A1iz-Moret%C3%B3n%2C+H">H&#xe9;ctor Al&#xe1;iz-Moret&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Gayo%2C+J+E+L">Jos&#xe9; Emilio Labra Gayo</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Information Sciences, Volume 540, November 2020, Pages 390-413
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The use of social network theory and methods of analysis have been applied to
different domains in recent years, including public health. The complete
procedure for carrying out a social network analysis (SNA) is a time-consuming
task that entails a series of steps in which the expert in social network
analysis could make mistakes. This research presents a multi-domain knowledge
model capable of automatically gathering data and carrying out different social
network analyses in different domains, without errors and obtaining the same
conclusions that an expert in SNA would obtain. The model is represented in an
ontology called OntoSNAQA, which is made up of classes, properties and rules
representing the domains of People, Questionnaires and Social Network Analysis.
Besides the ontology itself, different rules are represented by SWRL and SPARQL
queries. A Knowledge Based System was created using OntoSNAQA and applied to a
real case study in order to show the advantages of the approach. Finally, the
results of an SNA analysis obtained through the model were compared to those
obtained from some of the most widely used SNA applications: UCINET, Pajek,
Cytoscape and Gephi, to test and confirm the validity of the model.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02182" title="Abstract">arXiv:2402.02182</a> [<a href="/pdf/2402.02182" title="Download PDF">pdf</a>, <a href="/format/2402.02182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Cross-domain Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xuan%2C+Y">Yuner Xuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">It is always a challenge for recommender systems to give high-quality
outcomes to cold-start users. One potential solution to alleviate the data
sparsity problem for cold-start users in the target domain is to add data from
the auxiliary domain. Finding a proper way to extract knowledge from an
auxiliary domain and transfer it into a target domain is one of the main
objectives for cross-domain recommendation (CDR) research. Among the existing
methods, mapping approach is a popular one to implement cross-domain
recommendation models (CDRs). For models of this type, a mapping module plays
the role of transforming data from one domain to another. It primarily
determines the performance of mapping approach CDRs. Recently, diffusion
probability models (DPMs) have achieved impressive success for image synthesis
related tasks. They involve recovering images from noise-added samples, which
can be viewed as a data transformation process with outstanding performance. To
further enhance the performance of CDRs, we first reveal the potential
connection between DPMs and mapping modules of CDRs, and then propose a novel
CDR model named Diffusion Cross-domain Recommendation (DiffCDR). More
specifically, we first adopt the theory of DPM and design a Diffusion Module
(DIM), which generates user's embedding in target domain. To reduce the
negative impact of randomness introduced in DIM and improve the stability, we
employ an Alignment Module to produce the aligned user embeddings. In addition,
we consider the label data of the target domain and form the task-oriented loss
function, which enables our DiffCDR to adapt to specific tasks. By conducting
extensive experiments on datasets collected from reality, we demonstrate the
effectiveness and adaptability of DiffCDR to outperform baseline models on
various CDR tasks in both cold-start and warm-start scenarios.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02183" title="Abstract">arXiv:2402.02183</a> [<a href="/pdf/2402.02183" title="Download PDF">pdf</a>, <a href="/ps/2402.02183" title="Download PostScript">ps</a>, <a href="/format/2402.02183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Respiratory Pathologies Using Convolutional Neural Networks  and Variational Autoencoders for Unbalancing Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Ord%C3%A1s%2C+M+T">Mar&#xed;a Teresa Garc&#xed;a-Ord&#xe1;s</a>, 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Rodr%C3%ADguez%2C+I">Isa&#xed;as Garc&#xed;a-Rodr&#xed;guez</a>, 
<a href="/search/cs?searchtype=author&query=Benavides%2C+C">Carmen Benavides</a>, 
<a href="/search/cs?searchtype=author&query=Alaiz-Moret%C3%B3n%2C+H">H&#xe9;ctor Alaiz-Moret&#xf3;n</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Sensors 2020, Volume 20 Issue 4, ID 1214
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The aim of this paper was the detection of pathologies through respiratory
sounds. The ICBHI (International Conference on Biomedical and Health
Informatics) Benchmark was used. This dataset is composed of 920 sounds of
which 810 are of chronic diseases, 75 of non-chronic diseases and only 35 of
healthy individuals. As more than 88% of the samples of the dataset are from
the same class (Chronic), the use of a Variational Convolutional Autoencoder
was proposed to generate new labeled data and other well known oversampling
techniques after determining that the dataset classes are unbalanced. Once the
preprocessing step was carried out, a Convolutional Neural Network (CNN) was
used to classify the respiratory sounds into healthy, chronic, and non-chronic
disease. In addition, we carried out a more challenging classification trying
to distinguish between the different types of pathologies or healthy: URTI,
COPD, Bronchiectasis, Pneumonia, and Bronchiolitis. We achieved results up to
0.993 F-Score in the three-label classification and 0.990 F-Score in the more
challenging six-class classification.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02184" title="Abstract">arXiv:2402.02184</a> [<a href="/pdf/2402.02184" title="Download PDF">pdf</a>, <a href="/ps/2402.02184" title="Download PostScript">ps</a>, <a href="/format/2402.02184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sentiment analysis in non-fixed length audios using a Fully  Convolutional Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Ord%C3%A1s%2C+M+T">Mar&#xed;a Teresa Garc&#xed;a-Ord&#xe1;s</a>, 
<a href="/search/cs?searchtype=author&query=Alaiz-Moret%C3%B3n%2C+H">H&#xe9;ctor Alaiz-Moret&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Rodr%C3%ADguez%2C+I">Isa&#xed;as Garc&#xed;a-Rodr&#xed;guez</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Olalla%2C+O">Oscar Garc&#xed;a-Olalla</a>, 
<a href="/search/cs?searchtype=author&query=Benavides%2C+C">Carmen Benavides</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Biomedical Signal Processing and Control, Volume 69, August 2021,
  ID 102946
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this work, a sentiment analysis method that is capable of accepting audio
of any length, without being fixed a priori, is proposed. Mel spectrogram and
Mel Frequency Cepstral Coefficients are used as audio description methods and a
Fully Convolutional Neural Network architecture is proposed as a classifier.
The results have been validated using three well known datasets: EMODB,
RAVDESS, and TESS. The results obtained were promising, outperforming the
state-of-the-art methods. Also, thanks to the fact that the proposed method
admits audios of any size, it allows a sentiment analysis to be made in near
real time, which is very interesting for a wide range of fields such as call
centers, medical consultations, or financial brokers.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02186" title="Abstract">arXiv:2402.02186</a> [<a href="/pdf/2402.02186" title="Download PDF">pdf</a>, <a href="/format/2402.02186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolution Guided Generative Flow Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ikram%2C+Z">Zarif Ikram</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Ling Pan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dianbo Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 16 figues
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Generative Flow Networks (GFlowNets) are a family of probabilistic generative
models that learn to sample compositional objects proportional to their
rewards. One big challenge of GFlowNets is training them effectively when
dealing with long time horizons and sparse rewards. To address this, we propose
Evolution guided generative flow networks (EGFN), a simple but powerful
augmentation to the GFlowNets training using Evolutionary algorithms (EA). Our
method can work on top of any GFlowNets training objective, by training a set
of agent parameters using EA, storing the resulting trajectories in the
prioritized replay buffer, and training the GFlowNets agent using the stored
trajectories. We present a thorough investigation over a wide range of toy and
real-world benchmark tasks showing the effectiveness of our method in handling
long trajectories and sparse rewards.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02188" title="Abstract">arXiv:2402.02188</a> [<a href="/pdf/2402.02188" title="Download PDF">pdf</a>, <a href="/ps/2402.02188" title="Download PostScript">ps</a>, <a href="/format/2402.02188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diabetes detection using deep learning techniques with oversampling and  feature augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Ord%C3%A1s%2C+M+T">Mar&#xed;a Teresa Garc&#xed;a-Ord&#xe1;s</a>, 
<a href="/search/cs?searchtype=author&query=Benavides%2C+C">Carmen Benavides</a>, 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>, 
<a href="/search/cs?searchtype=author&query=Alaiz-Moret%C3%B3n%2C+H">H&#xe9;ctor Alaiz-Moret&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Rodr%C3%ADguez%2C+I">Isa&#xed;as Garc&#xed;a-Rodr&#xed;guez</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computer Methods and Programs in Biomedicine, Volume 202, April
  2021, ID 105968
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Background and objective: Diabetes is a chronic pathology which is affecting
more and more people over the years. It gives rise to a large number of deaths
each year. Furthermore, many people living with the disease do not realize the
seriousness of their health status early enough. Late diagnosis brings about
numerous health problems and a large number of deaths each year so the
development of methods for the early diagnosis of this pathology is essential.
<br />Methods: In this paper, a pipeline based on deep learning techniques is
proposed to predict diabetic people. It includes data augmentation using a
variational autoencoder (VAE), feature augmentation using an sparse autoencoder
(SAE) and a convolutional neural network for classification. Pima Indians
Diabetes Database, which takes into account information on the patients such as
the number of pregnancies, glucose or insulin level, blood pressure or age, has
been evaluated.
<br />Results: A 92.31% of accuracy was obtained when CNN classifier is trained
jointly the SAE for featuring augmentation over a well balanced dataset. This
means an increment of 3.17% of accuracy with respect the state-of-the-art.
<br />Conclusions: Using a full deep learning pipeline for data preprocessing and
classification has demonstrate to be very promising in the diabetes detection
field outperforming the state-of-the-art proposals.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02189" title="Abstract">arXiv:2402.02189</a> [<a href="/pdf/2402.02189" title="Download PDF">pdf</a>, <a href="/format/2402.02189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DoF Analysis for (M, N)-Channels through a Number-Filling Puzzle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bi%2C+Y">Yue Bi</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+C">Cunqing Hua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">We consider a $\sf K$ user interference network with general connectivity,
described by a matrix $\mat{N}$, and general message flows, described by a
matrix $\mat{M}$. Previous studies have demonstrated that the standard
interference scheme (IA) might not be optimal for networks with sparse
connectivity. In this paper, we formalize a general IA coding scheme and an
intuitive number-filling puzzle for given $\mat{M}$ and $\mat{N}$ in a way that
the score of the solution to the puzzle determines the optimum sum degrees that
can be achieved by the IA scheme. A solution to the puzzle is proposed for a
general class of symmetric channels, and it is shown that this solution leads
to significantly higher $\SDoF$ than the standard IA scheme.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02192" title="Abstract">arXiv:2402.02192</a> [<a href="/pdf/2402.02192" title="Download PDF">pdf</a>, <a href="/format/2402.02192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RecNet: An Invertible Point Cloud Encoding through Range Image  Embeddings for Multi-Robot Map Sharing and Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stathoulopoulos%2C+N">Nikolaos Stathoulopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Saucedo%2C+M+A+V">Mario A.V. Saucedo</a>, 
<a href="/search/cs?searchtype=author&query=Koval%2C+A">Anton Koval</a>, 
<a href="/search/cs?searchtype=author&query=Nikolakopoulos%2C+G">George Nikolakopoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in the 2024 IEEE International Conference on Robotics and Automation in Yokohama, (ICRA24). The current version does not include changes according to the reviewers' suggestions
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In the field of resource-constrained robots and the need for effective place
recognition in multi-robotic systems, this article introduces RecNet, a novel
approach that concurrently addresses both challenges. The core of RecNet's
methodology involves a transformative process: it projects 3D point clouds into
depth images, compresses them using an encoder-decoder framework, and
subsequently reconstructs the range image, seamlessly restoring the original
point cloud. Additionally, RecNet utilizes the latent vector extracted from
this process for efficient place recognition tasks. This unique approach not
only achieves comparable place recognition results but also maintains a compact
representation, suitable for seamless sharing among robots to reconstruct their
collective maps. The evaluation of RecNet encompasses an array of metrics,
including place recognition performance, structural similarity of the
reconstructed point clouds, and the bandwidth transmission advantages, derived
from sharing only the latent vectors. This reconstructed map paves a
groundbreaking way for exploring its usability in navigation, localization,
map-merging, and other relevant missions. Our proposed approach is rigorously
assessed using both a publicly available dataset and field experiments,
confirming its efficacy and potential for real-world applications.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02197" title="Abstract">arXiv:2402.02197</a> [<a href="/pdf/2402.02197" title="Download PDF">pdf</a>, <a href="/format/2402.02197" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical solution to a Parabolic-ODE Solow model with spatial diffusion  and technology-induced motility
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ure%C3%B1a%2C+N">Nicol&#xe1;s Ure&#xf1;a</a>, 
<a href="/search/math?searchtype=author&query=Vargas%2C+A+M">Antonio M. Vargas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; General Economics (econ.GN)

</div>
<p class="mathjax">This work studies a parabolic-ODE PDE's system which describes the evolution
of the physical capital "$k$" and technological progress "$A$", using a
meshless in one and two dimensional bounded domain with regular boundary. The
well-known Solow model is extended by considering the spatial diffusion of both
capital anf technology. Moreover, we study the case in which no spatial
diffusion of the technology progress occurs. For such models, we propound
schemes based on the Generalized Finite Difference method and proof the
convergence of the numerical solution to the continuous one. Several examples
show the dynamics of the model for a wide range of parameters. These examples
illustrate the accuary of the numerical method.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02200" title="Abstract">arXiv:2402.02200</a> [<a href="/pdf/2402.02200" title="Download PDF">pdf</a>, <a href="/format/2402.02200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Less is More: Physical-enhanced Radar-Inertial Odometry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qiucan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuchen Liang</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Z">Zhijian Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Shaojie Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Huan Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Radar offers the advantage of providing additional physical properties
related to observed objects. In this study, we design a physical-enhanced
radar-inertial odometry system that capitalizes on the Doppler velocities and
radar cross-section information. The filter for static radar points,
correspondence estimation, and residual functions are all strengthened by
integrating the physical properties. We conduct experiments on both public
datasets and our self-collected data, with different mobile platforms and
sensor types. Our quantitative results demonstrate that the proposed
radar-inertial odometry system outperforms alternative methods using the
physical-enhanced components. Our findings also reveal that using the physical
properties results in fewer radar points for odometry estimation, but the
performance is still guaranteed and even improved, thus aligning with the
``less is more'' principle.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02205" title="Abstract">arXiv:2402.02205</a> [<a href="/pdf/2402.02205" title="Download PDF">pdf</a>, <a href="/format/2402.02205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT-4V as Traffic Assistant: An In-depth Look at Vision Language Model  on Complex Traffic Events
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xingcheng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Knoll%2C+A+C">Alois C. Knoll</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The recognition and understanding of traffic incidents, particularly traffic
accidents, is a topic of paramount importance in the realm of intelligent
transportation systems and intelligent vehicles. This area has continually
captured the extensive focus of both the academic and industrial sectors.
Identifying and comprehending complex traffic events is highly challenging,
primarily due to the intricate nature of traffic environments, diverse
observational perspectives, and the multifaceted causes of accidents. These
factors have persistently impeded the development of effective solutions. The
advent of large vision-language models (VLMs) such as GPT-4V, has introduced
innovative approaches to addressing this issue. In this paper, we explore the
ability of GPT-4V with a set of representative traffic incident videos and
delve into the model's capacity of understanding these complex traffic
situations. We observe that GPT-4V demonstrates remarkable cognitive,
reasoning, and decision-making ability in certain classic traffic events.
Concurrently, we also identify certain limitations of GPT-4V, which constrain
its understanding in more intricate scenarios. These limitations merit further
exploration and resolution.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02207" title="Abstract">arXiv:2402.02207</a> [<a href="/pdf/2402.02207" title="Download PDF">pdf</a>, <a href="/format/2402.02207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zong%2C+Y">Yongshuo Zong</a>, 
<a href="/search/cs?searchtype=author&query=Bohdal%2C+O">Ondrej Bohdal</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tingyang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yongxin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hospedales%2C+T">Timothy Hospedales</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Current vision large language models (VLLMs) exhibit remarkable capabilities
yet are prone to generate harmful content and are vulnerable to even the
simplest jailbreaking attacks. Our initial analysis finds that this is due to
the presence of harmful data during vision-language instruction fine-tuning,
and that VLLM fine-tuning can cause forgetting of safety alignment previously
learned by the underpinning LLM. To address this issue, we first curate a
vision-language safe instruction-following dataset VLGuard covering various
harmful categories. Our experiments demonstrate that integrating this dataset
into standard vision-language fine-tuning or utilizing it for post-hoc
fine-tuning effectively safety aligns VLLMs. This alignment is achieved with
minimal impact on, or even enhancement of, the models' helpfulness. The
versatility of our safety fine-tuning dataset makes it a valuable resource for
safety-testing existing VLLMs, training new models or safeguarding pre-trained
VLLMs. Empirical results demonstrate that fine-tuned VLLMs effectively reject
unsafe instructions and substantially reduce the success rates of several
black-box adversarial attacks, which approach zero in many cases. The code and
dataset are available at https://github.com/ys-zong/VLGuard.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02208" title="Abstract">arXiv:2402.02208</a> [<a href="/pdf/2402.02208" title="Download PDF">pdf</a>, <a href="/format/2402.02208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit Neural Representation of Tileable Material Textures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paz%2C+H">Hallison Paz</a>, 
<a href="/search/cs?searchtype=author&query=Novello%2C+T">Tiago Novello</a>, 
<a href="/search/cs?searchtype=author&query=Velho%2C+L">Luiz Velho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">We explore sinusoidal neural networks to represent periodic tileable
textures. Our approach leverages the Fourier series by initializing the first
layer of a sinusoidal neural network with integer frequencies with a period
$P$. We prove that the compositions of sinusoidal layers generate only integer
frequencies with period $P$. As a result, our network learns a continuous
representation of a periodic pattern, enabling direct evaluation at any spatial
coordinate without the need for interpolation. To enforce the resulting pattern
to be tileable, we add a regularization term, based on the Poisson equation, to
the loss function. Our proposed neural implicit representation is compact and
enables efficient reconstruction of high-resolution textures with high visual
fidelity and sharpness across multiple levels of detail. We present
applications of our approach in the domain of anti-aliased surface.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02209" title="Abstract">arXiv:2402.02209</a> [<a href="/pdf/2402.02209" title="Download PDF">pdf</a>, <a href="/format/2402.02209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Exploitation of DCT-Traces in the Generative-AI Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pontorno%2C+O">Orazio Pontorno</a> (1), 
<a href="/search/cs?searchtype=author&query=Guarnera%2C+L">Luca Guarnera</a> (1), 
<a href="/search/cs?searchtype=author&query=Battiato%2C+S">Sebastiano Battiato</a> (1) ((1) University of Catania)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Since their appearance, Deepfakes represent one of the toughest challenges in
the world of Cybersecurity and Digital Forensics. In recent years, researchers
have discovered that generative models leave unique traces in synthetic data
that, if analyzed and identified in detail, can be exploited to improve the
generalization limitations of existing deepfake detectors. To capture this
evidence, in this paper we analyzed deepfake images in the frequency domain,
examining in detail the beta-AC coefficients of the Discrete Cosine Transform
(DCT). Recognizing that not all coefficients contribute equally to image
recognition, we hypothesize the existence of a unique "discriminative
fingerprint" for each type of image, embedded in specific combinations of
coefficients. To identify them, Machine Learning classifiers were trained on
various combinations of coefficients. The integration of the Explainable AI
(XAI) LIME algorithm combined with a neural classifier to explore alternative
combinations of coefficients provides a deeper insight into the discriminative
features of synthetic images. Experimental results reveal the significant
potential of using a specific combination of beta-AC coefficients in order to
improve the analysis of traces left by generative models.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02210" title="Abstract">arXiv:2402.02210</a> [<a href="/pdf/2402.02210" title="Download PDF">pdf</a>, <a href="/format/2402.02210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wavelet-Decoupling Contrastive Enhancement Network for Fine-Grained  Skeleton-Based Action Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+H">Haochen Chang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yilin Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jixiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaofeng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Skeleton-based action recognition has attracted much attention, benefiting
from its succinctness and robustness. However, the minimal inter-class
variation in similar action sequences often leads to confusion. The inherent
spatiotemporal coupling characteristics make it challenging to mine the subtle
differences in joint motion trajectories, which is critical for distinguishing
confusing fine-grained actions. To alleviate this problem, we propose a
Wavelet-Attention Decoupling (WAD) module that utilizes discrete wavelet
transform to effectively disentangle salient and subtle motion features in the
time-frequency domain. Then, the decoupling attention adaptively recalibrates
their temporal responses. To further amplify the discrepancies in these subtle
motion features, we propose a Fine-grained Contrastive Enhancement (FCE) module
to enhance attention towards trajectory features by contrastive learning.
Extensive experiments are conducted on the coarse-grained dataset NTU RGB+D and
the fine-grained dataset FineGYM. Our methods perform competitively compared to
state-of-the-art methods and can discriminate confusing fine-grained actions
well.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02211" title="Abstract">arXiv:2402.02211</a> [<a href="/pdf/2402.02211" title="Download PDF">pdf</a>, <a href="/format/2402.02211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Query-decision Regression between Shortest Path and Minimum Steiner Tree
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tong%2C+G">Guangmo Tong</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Peng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Samizadeh%2C+M">Mina Samizadeh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PAKDD 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Considering a graph with unknown weights, can we find the shortest path for a
pair of nodes if we know the minimal Steiner trees associated with some subset
of nodes? That is, with respect to a fixed latent decision-making system (e.g.,
a weighted graph), we seek to solve one optimization problem (e.g., the
shortest path problem) by leveraging information associated with another
optimization problem (e.g., the minimal Steiner tree problem). In this paper,
we study such a prototype problem called \textit{query-decision regression with
task shifts}, focusing on the shortest path problem and the minimum Steiner
tree problem. We provide theoretical insights regarding the design of
realizable hypothesis spaces for building scoring models, and present two
principled learning frameworks. Our experimental studies show that such
problems can be solved to a decent extent with statistical significance.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02212" title="Abstract">arXiv:2402.02212</a> [<a href="/pdf/2402.02212" title="Download PDF">pdf</a>, <a href="/format/2402.02212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Data Generation Perspective to the Mechanism of In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+H">Haitao Mao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Guangliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rongrong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In-Context Learning (ICL) empowers Large Language Models (LLMs) with the
capacity to learn in context, achieving downstream generalization without
gradient updates but with a few in-context examples. Despite the encouraging
empirical success, the underlying mechanism of ICL remains unclear, and
existing research offers various viewpoints of understanding. These studies
propose intuition-driven and ad-hoc technical solutions for interpreting ICL,
illustrating an ambiguous road map. In this paper, we leverage a data
generation perspective to reinterpret recent efforts and demonstrate the
potential broader usage of popular technical solutions, approaching a
systematic angle. For a conceptual definition, we rigorously adopt the terms of
skill learning and skill recognition. The difference between them is skill
learning can learn new data generation functions from in-context data. We also
provide a comprehensive study on the merits and weaknesses of different
solutions, and highlight the uniformity among them given the perspective of
data generation, establishing a technical foundation for future research to
incorporate the strengths of different lines of research.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02216" title="Abstract">arXiv:2402.02216</a> [<a href="/pdf/2402.02216" title="Download PDF">pdf</a>, <a href="/format/2402.02216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+H">Haitao Mao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhikai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+W">Wenzhuo Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jianan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+N">Neil Shah</a>, 
<a href="/search/cs?searchtype=author&query=Galkin%2C+M">Michael Galkin</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph Foundation Model (GFM) is a new trending research topic in the graph
domain, aiming to develop a graph model capable of generalizing across
different graphs and tasks. However, a versatile GFM has not yet been achieved.
The key challenge in building GFM is how to enable positive transfer across
graphs with diverse structural patterns. Inspired by the existing foundation
models in the CV and NLP domains, we propose a novel perspective for the GFM
development by advocating for a ``graph vocabulary'', in which the basic
transferable units underlying graphs encode the invariance on graphs. We ground
the graph vocabulary construction from essential aspects including network
analysis, theoretical foundations, and stability. Such a vocabulary perspective
can potentially advance the future GFM design following the neural scaling
laws.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02217" title="Abstract">arXiv:2402.02217</a> [<a href="/pdf/2402.02217" title="Download PDF">pdf</a>, <a href="/format/2402.02217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoFiNet: Unveiling Camouflaged Objects with Multi-Scale Finesse
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+C">Cunhan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heyan Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Camouflaged Object Detection (COD) is a critical aspect of computer vision
aimed at identifying concealed objects, with applications spanning military,
industrial, medical and monitoring domains. To address the problem of poor
detail segmentation effect, we introduce a novel method for camouflage object
detection, named CoFiNet. Our approach primarily focuses on multi-scale feature
fusion and extraction, with special attention to the model's segmentation
effectiveness for detailed features, enhancing its ability to effectively
detect camouflaged objects. CoFiNet adopts a coarse-to-fine strategy. A
multi-scale feature integration module is laveraged to enhance the model's
capability of fusing context feature. A multi-activation selective kernel
module is leveraged to grant the model the ability to autonomously alter its
receptive field, enabling it to selectively choose an appropriate receptive
field for camouflaged objects of different sizes. During mask generation, we
employ the dual-mask strategy for image segmentation, separating the
reconstruction of coarse and fine masks, which significantly enhances the
model's learning capacity for details. Comprehensive experiments were conducted
on four different datasets, demonstrating that CoFiNet achieves
state-of-the-art performance across all datasets. The experiment results of
CoFiNet underscore its effectiveness in camouflage object detection and
highlight its potential in various practical application scenarios.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02218" title="Abstract">arXiv:2402.02218</a> [<a href="/pdf/2402.02218" title="Download PDF">pdf</a>, <a href="/format/2402.02218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Intelligence in Africa: a survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tapo%2C+A+A">Allahsera Auguste Tapo</a>, 
<a href="/search/cs?searchtype=author&query=Traore%2C+A">Ali Traore</a>, 
<a href="/search/cs?searchtype=author&query=Danioko%2C+S">Sidy Danioko</a>, 
<a href="/search/cs?searchtype=author&query=Tembine%2C+H">Hamidou Tembine</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted and to be presented at DSAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the last 5 years, the availability of large audio datasets in African
countries has opened unlimited opportunities to build machine intelligence (MI)
technologies that are closer to the people and speak, learn, understand, and do
businesses in local languages, including for those who cannot read and write.
Unfortunately, these audio datasets are not fully exploited by current MI
tools, leaving several Africans out of MI business opportunities. Additionally,
many state-of-the-art MI models are not culture-aware, and the ethics of their
adoption indexes are questionable. The lack thereof is a major drawback in many
applications in Africa. This paper summarizes recent developments in machine
intelligence in Africa from a multi-layer multiscale and culture-aware ethics
perspective, showcasing MI use cases in 54 African countries through 400
articles on MI research, industry, government actions, as well as uses in art,
music, the informal economy, and small businesses in Africa. The survey also
opens discussions on the reliability of MI rankings and indexes in the African
continent as well as algorithmic definitions of unclear terms used in MI.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02219" title="Abstract">arXiv:2402.02219</a> [<a href="/pdf/2402.02219" title="Download PDF">pdf</a>, <a href="/format/2402.02219" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prediction-for-CompAction: navigation in social environments using  generalized cognitive maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Atienza%2C+J+A+V">Jos&#xe9; Antonio Villacorta Atienza</a>, 
<a href="/search/cs?searchtype=author&query=Tapia%2C+C+C">Carlos Calvo Tapia</a>, 
<a href="/search/cs?searchtype=author&query=Slizneva%2C+V+A+M">Valeriy A. Makarov Slizneva</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Biological Cybernetics 109(3) 307-320, 2015
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Dynamical Systems (math.DS)

</div>
<p class="mathjax">The ultimate navigation efficiency of mobile robots in human environments
will depend on how we will appraise them: merely as impersonal machines or as
human-like agents. In the latter case, an agent may take advantage of the
cooperative collision avoidance, given that it possesses recursive cognition,
i.e.,the agent's decisions depend on the decisions made by humans that in turn
depend on the agent's decisions. To deal with this high-level cognitive skill,
we propose a neural network architecture implementing Prediction-for-CompAction
paradigm. The network predicts possible human-agent collisions and compacts the
time dimension by projecting a given dynamic situation into a static map.
Thereby emerging compact cognitive map can be readily used as a "dynamic GPS"
for planning actions or mental evaluation of the convenience of cooperation in
a given context. We provide numerical evidence that cooperation yields
additional room for more efficient navigation in cluttered pedestrian flows,
and the agent can choose path to the target significantly shorter than a robot
treated by humans as a functional machine. Moreover, the navigation safety,
i.e., the chances to avoid accidental collisions, increases under cooperation.
Remarkably, these benefits yield no additional load to the mean society effort.
Thus, the proposed strategy is socially compliant, and the humanoid agent can
behave as "one of us".
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02224" title="Abstract">arXiv:2402.02224</a> [<a href="/pdf/2402.02224" title="Download PDF">pdf</a>, <a href="/format/2402.02224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MSPM: A Multi-Site Physiological Monitoring Dataset for Remote Pulse,  Respiration, and Blood Pressure Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Speth%2C+J">Jeremy Speth</a>, 
<a href="/search/cs?searchtype=author&query=Vance%2C+N">Nathan Vance</a>, 
<a href="/search/cs?searchtype=author&query=Sporrer%2C+B">Benjamin Sporrer</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+L">Lu Niu</a>, 
<a href="/search/cs?searchtype=author&query=Flynn%2C+P">Patrick Flynn</a>, 
<a href="/search/cs?searchtype=author&query=Czajka%2C+A">Adam Czajka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Visible-light cameras can capture subtle physiological biomarkers without
physical contact with the subject. We present the Multi-Site Physiological
Monitoring (MSPM) dataset, which is the first dataset collected to support the
study of simultaneous camera-based vital signs estimation from multiple
locations on the body. MSPM enables research on remote photoplethysmography
(rPPG), respiration rate, and pulse transit time (PTT); it contains
ground-truth measurements of pulse oximetry (at multiple body locations) and
blood pressure using contacting sensors. We provide thorough experiments
demonstrating the suitability of MSPM to support research on rPPG, respiration
rate, and PTT. Cross-dataset rPPG experiments reveal that MSPM is a challenging
yet high quality dataset, with intra-dataset pulse rate mean absolute error
(MAE) below 4 beats per minute (BPM), and cross-dataset pulse rate MAE below 2
BPM in certain cases. Respiration experiments find a MAE of 1.09 breaths per
minute by extracting motion features from the chest. PTT experiments find that
across the pairs of different body sites, there is high correlation between
remote PTT and contact-measured PTT, which facilitates the possibility for
future camera-based PTT research.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02225" title="Abstract">arXiv:2402.02225</a> [<a href="/pdf/2402.02225" title="Download PDF">pdf</a>, <a href="/format/2402.02225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking the Starting Point: Enhancing Performance and Fairness of  Federated Learning via Collaborative Pre-Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+Y">Yun-Wei Chu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+D">Dong-Jun Han</a>, 
<a href="/search/cs?searchtype=author&query=Hosseinalipour%2C+S">Seyyedali Hosseinalipour</a>, 
<a href="/search/cs?searchtype=author&query=Brinton%2C+C+G">Christopher G. Brinton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Most existing federated learning (FL) methodologies have assumed training
begins from a randomly initialized model. Recently, several studies have
empirically demonstrated that leveraging a pre-trained model can offer
advantageous initializations for FL. In this paper, we propose a collaborative
pre-training approach, CoPreFL, which strategically designs a pre-trained model
to serve as a good initialization for any downstream FL task. The key idea of
our pre-training algorithm is a meta-learning procedure which mimics downstream
distributed scenarios, enabling it to adapt to any unforeseen FL task.
CoPreFL's pre-training optimization procedure also strikes a balance between
average performance and fairness, with the aim of addressing these competing
challenges in downstream FL tasks through intelligent initializations.
Extensive experimental results validate that our pre-training method provides a
robust initialization for any unseen downstream FL task, resulting in enhanced
average performance and more equitable predictions.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02227" title="Abstract">arXiv:2402.02227</a> [<a href="/pdf/2402.02227" title="Download PDF">pdf</a>, <a href="/format/2402.02227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Invisible Finger: Practical Electromagnetic Interference Attack on  Touchscreen-based Electronic Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shan%2C+H">Haoqi Shan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Boyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+Z">Zihao Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Sullivan%2C+D">Dean Sullivan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yier Jin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by 2022 IEEE Symposium on Security and Privacy (SP) and won distinguished paper award
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Touchscreen-based electronic devices such as smart phones and smart tablets
are widely used in our daily life. While the security of electronic devices
have been heavily investigated recently, the resilience of touchscreens against
various attacks has yet to be thoroughly investigated. In this paper, for the
first time, we show that touchscreen-based electronic devices are vulnerable to
intentional electromagnetic interference (IEMI) attacks in a systematic way and
how to conduct this attack in a practical way. Our contribution lies in not
just demonstrating the attack, but also analyzing and quantifying the
underlying mechanism allowing the novel IEMI attack on touchscreens in detail.
We show how to calculate both the minimum amount of electric field and signal
frequency required to induce touchscreen ghost touches. We further analyze our
IEMI attack on real touchscreens with different magnitudes, frequencies,
duration, and multitouch patterns. The mechanism of controlling the
touchscreen-enabled electronic devices with IEMI signals is also elaborated. We
design and evaluate an out-of-sight touchscreen locator and touch injection
feedback mechanism to assist a practical IEMI attack. Our attack works directly
on the touchscreen circuit regardless of the touchscreen scanning mechanism or
operating system. Our attack can inject short-tap, long-press, and
omni-directional gestures on touchscreens from a distance larger than the
average thickness of common tabletops. Compared with the state-of-the-art
touchscreen attack, ours can accurately inject different types of touch events
without the need for sensing signal synchronization, which makes our attack
more robust and practical. In addition, rather than showing a simple
proof-of-concept attack, we present and demonstrate the first ready-to-use IEMI
based touchscreen attack vector with end-to-end attack scenarios.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02229" title="Abstract">arXiv:2402.02229</a> [<a href="/pdf/2402.02229" title="Download PDF">pdf</a>, <a href="/format/2402.02229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vanilla Bayesian Optimization Performs Great in High Dimension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hvarfner%2C+C">Carl Hvarfner</a>, 
<a href="/search/cs?searchtype=author&query=Hellsten%2C+E+O">Erik Orm Hellsten</a>, 
<a href="/search/cs?searchtype=author&query=Nardi%2C+L">Luigi Nardi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">High-dimensional problems have long been considered the Achilles' heel of
Bayesian optimization algorithms. Spurred by the curse of dimensionality, a
large collection of algorithms aim to make it more performant in this setting,
commonly by imposing various simplifying assumptions on the objective. In this
paper, we identify the degeneracies that make vanilla Bayesian optimization
poorly suited to high-dimensional tasks, and further show how existing
algorithms address these degeneracies through the lens of lowering the model
complexity. Moreover, we propose an enhancement to the prior assumptions that
are typical to vanilla Bayesian optimization algorithms, which reduces the
complexity to manageable levels without imposing structural restrictions on the
objective. Our modification - a simple scaling of the Gaussian process
lengthscale prior with the dimensionality - reveals that standard Bayesian
optimization works drastically better than previously thought in high
dimensions, clearly outperforming existing state-of-the-art algorithms on
multiple commonly considered real-world high-dimensional tasks.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02230" title="Abstract">arXiv:2402.02230</a> [<a href="/pdf/2402.02230" title="Download PDF">pdf</a>, <a href="/format/2402.02230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning with Differential Privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Banse%2C+A">Adrien Banse</a>, 
<a href="/search/cs?searchtype=author&query=Kreischer%2C+J">Jan Kreischer</a>, 
<a href="/search/cs?searchtype=author&query=J%C3%BCrgens%2C+X+O+i">Xavier Oliva i J&#xfc;rgens</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Machine Learning (ML) &amp; Federated Learning (FL); 4 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated learning (FL), as a type of distributed machine learning, is
capable of significantly preserving client's private data from being shared
among different parties. Nevertheless, private information can still be
divulged by analyzing uploaded parameter weights from clients. In this report,
we showcase our empirical benchmark of the effect of the number of clients and
the addition of differential privacy (DP) mechanisms on the performance of the
model on different types of data. Our results show that non-i.i.d and small
datasets have the highest decrease in performance in a distributed and
differentially private setting.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02232" title="Abstract">arXiv:2402.02232</a> [<a href="/pdf/2402.02232" title="Download PDF">pdf</a>, <a href="/format/2402.02232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Longitudinal Control Volumes: A Novel Centralized Estimation and Control  Framework for Distributed Multi-Agent Sorting Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maier%2C+J">James Maier</a>, 
<a href="/search/cs?searchtype=author&query=Sriganesh%2C+P">Prasanna Sriganesh</a>, 
<a href="/search/cs?searchtype=author&query=Travers%2C+M">Matthew Travers</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to be published at ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Centralized control of a multi-agent system improves upon distributed control
especially when multiple agents share a common task e.g., sorting different
materials in a recycling facility. Traditionally, each agent in a sorting
facility is tuned individually which leads to suboptimal performance if one
agent is less efficient than the others. Centralized control overcomes this
bottleneck by leveraging global system state information, but it can be
computationally expensive. In this work, we propose a novel framework called
Longitudinal Control Volumes (LCV) to model the flow of material in a recycling
facility. We then employ a Kalman Filter that incorporates local measurements
of materials into a global estimation of the material flow in the system. We
utilize a model predictive control algorithm that optimizes the rate of
material flow using the global state estimate in real-time. We show that our
proposed framework outperforms distributed control methods by 40-100% in
simulation and physical experiments.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02234" title="Abstract">arXiv:2402.02234</a> [<a href="/pdf/2402.02234" title="Download PDF">pdf</a>, <a href="/ps/2402.02234" title="Download PostScript">ps</a>, <a href="/format/2402.02234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Epidemics on Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kreischer%2C+J">Jan Kreischer</a>, 
<a href="/search/cs?searchtype=author&query=Iten%2C+A">Adrian Iten</a>, 
<a href="/search/cs?searchtype=author&query=Jehoul%2C+A">Astrid Jehoul</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Despite centuries of work on containment and mitigation strategies,
infectious diseases are still a major problem facing humanity. This work is
concerned with simulating heterogeneous contact structures and understanding
how the structure of the underlying network affects the spread of the disease.
For example, it has been empirically demonstrated and validated that scale free
networks do not have an epidemic threshold. Understanding the relationship
between network structure and disease dynamics can help to develop better
mitigation strategies and more effective interventions.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02235" title="Abstract">arXiv:2402.02235</a> [<a href="/pdf/2402.02235" title="Download PDF">pdf</a>, <a href="/format/2402.02235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image Fusion via Vision-Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zixiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+L">Lilun Deng</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+H">Haowen Bai</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yukun Cui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yulun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+H">Haotong Qin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dongdong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiangshe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Van+Gool%2C+L">Luc Van Gool</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Image fusion integrates essential information from multiple source images
into a single composite, emphasizing the highlighting structure and textures,
and refining imperfect areas. Existing methods predominantly focus on
pixel-level and semantic visual features for recognition. However, they
insufficiently explore the deeper semantic information at a text-level beyond
vision. Therefore, we introduce a novel fusion paradigm named image Fusion via
vIsion-Language Model (FILM), for the first time, utilizing explicit textual
information in different source images to guide image fusion. In FILM, input
images are firstly processed to generate semantic prompts, which are then fed
into ChatGPT to obtain rich textual descriptions. These descriptions are fused
in the textual domain and guide the extraction of crucial visual features from
the source images through cross-attention, resulting in a deeper level of
contextual understanding directed by textual semantic information. The final
fused image is created by vision feature decoder. This paradigm achieves
satisfactory results in four image fusion tasks: infrared-visible, medical,
multi-exposure, and multi-focus image fusion. We also propose a vision-language
dataset containing ChatGPT-based paragraph descriptions for the ten image
fusion datasets in four fusion tasks, facilitating future research in
vision-language model-based image fusion. Code and dataset will be released.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02239" title="Abstract">arXiv:2402.02239</a> [<a href="/pdf/2402.02239" title="Download PDF">pdf</a>, <a href="/format/2402.02239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributional Reduction: Unifying Dimensionality Reduction and  Clustering with Gromov-Wasserstein Projection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Van+Assel%2C+H">Hugues Van Assel</a>, 
<a href="/search/cs?searchtype=author&query=Vincent-Cuaz%2C+C">C&#xe9;dric Vincent-Cuaz</a>, 
<a href="/search/cs?searchtype=author&query=Courty%2C+N">Nicolas Courty</a>, 
<a href="/search/cs?searchtype=author&query=Flamary%2C+R">R&#xe9;mi Flamary</a>, 
<a href="/search/cs?searchtype=author&query=Frossard%2C+P">Pascal Frossard</a>, 
<a href="/search/cs?searchtype=author&query=Vayer%2C+T">Titouan Vayer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Unsupervised learning aims to capture the underlying structure of potentially
large and high-dimensional datasets. Traditionally, this involves using
dimensionality reduction methods to project data onto interpretable spaces or
organizing points into meaningful clusters. In practice, these methods are used
sequentially, without guaranteeing that the clustering aligns well with the
conducted dimensionality reduction. In this work, we offer a fresh perspective:
that of distributions. Leveraging tools from optimal transport, particularly
the Gromov-Wasserstein distance, we unify clustering and dimensionality
reduction into a single framework called distributional reduction. This allows
us to jointly address clustering and dimensionality reduction with a single
optimization problem. Through comprehensive experiments, we highlight the
versatility and interpretability of our method and show that it outperforms
existing approaches across a variety of image and genomics datasets.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02240" title="Abstract">arXiv:2402.02240</a> [<a href="/pdf/2402.02240" title="Download PDF">pdf</a>, <a href="/ps/2402.02240" title="Download PostScript">ps</a>, <a href="/format/2402.02240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recommendations on Statistical Randomness Test Batteries for  Cryptographic Purposes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luengo%2C+E+A">Elena Almaraz Luengo</a>, 
<a href="/search/cs?searchtype=author&query=Villalba%2C+L+J+G">Luis Javier Garc&#xed;a Villalba</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ACM Computing Surveys, Vol. 54, No. 80, pp. 12420, May 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computation (stat.CO)

</div>
<p class="mathjax">Security in different applications is closely related to the goodness of the
sequences generated for such purposes. Not only in Cryptography but also in
other areas, it is necessary to obtain long sequences of random numbers or
that, at least, behave as such. To decide whether the generator used produces
sequences that are random, unpredictable and independent, statistical checks
are needed. Different batteries of hypothesis tests have been proposed for this
purpose.
<br />In this work, a survey of the main test batteries is presented, indicating
their pros and cons, giving some guidelines for their use and presenting some
practical examples.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02242" title="Abstract">arXiv:2402.02242</a> [<a href="/pdf/2402.02242" title="Download PDF">pdf</a>, <a href="/format/2402.02242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xin%2C+Y">Yi Xin</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Siqi Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Haodi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+J">Junlong Du</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaohong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yue Fan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qing Li</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yuntao Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IJCAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large-scale pre-trained vision models (PVMs) have shown great potential for
adaptability across various downstream vision tasks. However, with
state-of-the-art PVMs growing to billions or even trillions of parameters, the
standard full fine-tuning paradigm is becoming unsustainable due to high
computational and storage demands. In response, researchers are exploring
parameter-efficient fine-tuning (PEFT), which seeks to exceed the performance
of full fine-tuning with minimal parameter modifications. This survey provides
a comprehensive overview and future directions for visual PEFT, offering a
systematic review of the latest advancements. First, we provide a formal
definition of PEFT and discuss model pre-training methods. We then categorize
existing methods into three categories: addition-based, partial-based, and
unified-based. Finally, we introduce the commonly used datasets and
applications and suggest potential future research challenges. A comprehensive
collection of resources is available at
https://github.com/synbol/Awesome-Parameter-Efficient-Transfer-Learning.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02243" title="Abstract">arXiv:2402.02243</a> [<a href="/pdf/2402.02243" title="Download PDF">pdf</a>, <a href="/ps/2402.02243" title="Download PostScript">ps</a>, <a href="/format/2402.02243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Writ Large: LLMs, ChatGPT, Grounding, Meaning and Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Harnad%2C+S">Stevan Harnad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 48 pages, 25 references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Apart from what (little) OpenAI may be concealing from us, we all know
(roughly) how ChatGPT works (its huge text database, its statistics, its vector
representations, and their huge number of parameters, its next-word training,
and so on). But none of us can say (hand on heart) that we are not surprised by
what ChatGPT has proved to be able to do with these resources. This has even
driven some of us to conclude that ChatGPT actually understands. It is not true
that it understands. But it is also not true that we understand how it can do
what it can do. I will suggest some hunches about benign biases: convergent
constraints that emerge at LLM scale that may be helping ChatGPT do so much
better than we would have expected. These biases are inherent in the nature of
language itself, at LLM scale, and they are closely linked to what it is that
ChatGPT lacks, which is direct sensorimotor grounding to connect its words to
their referents and its propositions to their meanings. These convergent biases
are related to (1) the parasitism of indirect verbal grounding on direct
sensorimotor grounding, (2) the circularity of verbal definition, (3) the
mirroring of language production and comprehension, (4) iconicity in
propositions at LLM scale, (5) computational counterparts of human categorical
perception in category learning by neural nets, and perhaps also (6) a
conjecture by Chomsky about the laws of thought. The exposition will be in the
form of a dialogue with ChatGPT-4.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02244" title="Abstract">arXiv:2402.02244</a> [<a href="/pdf/2402.02244" title="Download PDF">pdf</a>, <a href="/format/2402.02244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond the Limits: A Survey of Techniques to Extend the Context Length  in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xindi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Salmani%2C+M">Mahsa Salmani</a>, 
<a href="/search/cs?searchtype=author&query=Omidi%2C+P">Parsa Omidi</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xiangyu Ren</a>, 
<a href="/search/cs?searchtype=author&query=Rezagholizadeh%2C+M">Mehdi Rezagholizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Eshaghi%2C+A">Armaghan Eshaghi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recently, large language models (LLMs) have shown remarkable capabilities
including understanding context, engaging in logical reasoning, and generating
responses. However, this is achieved at the expense of stringent computational
and memory requirements, hindering their ability to effectively support long
input sequences. This survey provides an inclusive review of the recent
techniques and methods devised to extend the sequence length in LLMs, thereby
enhancing their capacity for long-context understanding. In particular, we
review and categorize a wide range of techniques including architectural
modifications, such as modified positional encoding and altered attention
mechanisms, which are designed to enhance the processing of longer sequences
while avoiding a proportional increase in computational requirements. The
diverse methodologies investigated in this study can be leveraged across
different phases of LLMs, i.e., training, fine-tuning and inference. This
enables LLMs to efficiently process extended sequences. The limitations of the
current methodologies is discussed in the last section along with the
suggestions for future research directions, underscoring the importance of
sequence length in the continued advancement of LLMs.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02245" title="Abstract">arXiv:2402.02245</a> [<a href="/pdf/2402.02245" title="Download PDF">pdf</a>, <a href="/format/2402.02245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Generative Adversarial Networks for Binary Semantic  Segmentation on Imbalanced Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Gabbouj%2C+M">Moncef Gabbouj</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Anomalous pavement surface conditions detection aims to detect pixels
representing anomalous states, such as cracks, on pavement surface images
automatically by algorithms. Recently, deep learning models have been
intensively applied to related topics with outstanding performance. However,
most existing deep learning-related solutions rarely achieve a stable
performance on diverse datasets. To address this issue, in this work, we
propose a deep learning framework based on conditional Generative Adversarial
Networks for anomalous region detection on pavement images at the pixel level.
In particular, the proposed framework is developed to enhance the generator's
ability to estimate the probability feature map from heterogeneous inputs with
two training stages and multiscale feature representation. Moreover, several
attention mechanisms are incorporated into the proposed framework to mitigate
the performance deterioration of model training on severely imbalanced
datasets. We implement experiments on six accessible pavement datasets.
Extensive qualitative and quantitative experiments demonstrate that the
proposed framework can achieve SOTA results on these datasets efficiently and
robustly.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02246" title="Abstract">arXiv:2402.02246</a> [<a href="/pdf/2402.02246" title="Download PDF">pdf</a>, <a href="/format/2402.02246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ExTTNet: A Deep Learning Algorithm for Extracting Table Texts from  Invoice Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akdo%C4%9Fan%2C+A">Adem Akdo&#x11f;an</a>, 
<a href="/search/cs?searchtype=author&query=Kurt%2C+M">Murat Kurt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">In this work, product tables in invoices are obtained autonomously via a deep
learning model, which is named as ExTTNet. Firstly, text is obtained from
invoice images using Optical Character Recognition (OCR) techniques. Tesseract
OCR engine [37] is used for this process. Afterwards, the number of existing
features is increased by using feature extraction methods to increase the
accuracy. Labeling process is done according to whether each text obtained as a
result of OCR is a table element or not. In this study, a multilayer artificial
neural network model is used. The training has been carried out with an Nvidia
RTX 3090 graphics card and taken $162$ minutes. As a result of the training,
the F1 score is $0.92$.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02247" title="Abstract">arXiv:2402.02247</a> [<a href="/pdf/2402.02247" title="Download PDF">pdf</a>, <a href="/format/2402.02247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Novel approaches for the reliable and efficient numerical evaluation of  the Landau operator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Carrillo%2C+J+A">Jose Antonio Carrillo</a>, 
<a href="/search/math?searchtype=author&query=Thalhammer%2C+M">Mechthild Thalhammer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">When applying Hamiltonian operator splitting methods for the time integration
of multi-species Vlasov-Maxwell-Landau systems, the reliable and efficient
numerical approximation of the Landau equation represents a fundamental
component of the entire algorithm. Substantial computational issues arise from
the treatment of the physically most relevant three-dimensional case with
Coulomb interaction. This work is concerned with the introduction and numerical
comparison of novel approaches for the evaluation of the Landau collision
operator. In the spirit of collocation, common tools are the identification of
fundamental integrals, series expansions of the integral kernel and the density
function on the main part of the velocity domain, and interpolation as well as
quadrature approximation nearby the singularity of the kernel. Focusing on the
favourable choice of the Fourier spectral method, their practical
implementation uses the reduction to basic integrals, fast Fourier techniques,
and summations along certain directions. Moreover, an important observation is
that a significant percentage of the overall computational effort can be
transferred to precomputations which are independent of the density function.
For the purpose of exposition and numerical validation, the cases of constant,
regular, and singular integral kernels are distinguished, and the procedure is
adapted accordingly to the increasing complexity of the problem. With regard to
the time integration of the Landau equation, the most expedient approach is
applied in such a manner that the conservation of mass is ensured.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02249" title="Abstract">arXiv:2402.02249</a> [<a href="/pdf/2402.02249" title="Download PDF">pdf</a>, <a href="/format/2402.02249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Don&#x27;t Label Twice: Quantity Beats Quality when Comparing Binary  Classifiers on a Budget
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dorner%2C+F+E">Florian E. Dorner</a>, 
<a href="/search/cs?searchtype=author&query=Hardt%2C+M">Moritz Hardt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 3 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We study how to best spend a budget of noisy labels to compare the accuracy
of two binary classifiers. It's common practice to collect and aggregate
multiple noisy labels for a given data point into a less noisy label via a
majority vote. We prove a theorem that runs counter to conventional wisdom. If
the goal is to identify the better of two classifiers, we show it's best to
spend the budget on collecting a single label for more samples. Our result
follows from a non-trivial application of Cram\'er's theorem, a staple in the
theory of large deviations. We discuss the implications of our work for the
design of machine learning benchmarks, where they overturn some time-honored
recommendations. In addition, our results provide sample size bounds superior
to what follows from Hoeffding's bound.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02252" title="Abstract">arXiv:2402.02252</a> [<a href="/pdf/2402.02252" title="Download PDF">pdf</a>, <a href="/format/2402.02252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaboration of Digital Twins through Linked Open Data: Architecture  with FIWARE as Enabling Technology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Conde%2C+J">Javier Conde</a>, 
<a href="/search/cs?searchtype=author&query=Munoz-Arcentales%2C+A">Andres Munoz-Arcentales</a>, 
<a href="/search/cs?searchtype=author&query=Alonso%2C+%C3%81">&#xc1;lvaro Alonso</a>, 
<a href="/search/cs?searchtype=author&query=Huecas%2C+G">Gabriel Huecas</a>, 
<a href="/search/cs?searchtype=author&query=Salvach%C3%BAa%2C+J">Joaqu&#xed;n Salvach&#xfa;a</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">The collaboration of the real world and the virtual world, known as Digital
Twin, has become a trend with numerous successful use cases. However, there are
challenges mentioned in the literature that must be addressed. One of the most
important issues is the difficulty of collaboration of Digital Twins due to the
lack of standardization in their implementation. This article continues a
previous work that proposed a generic architecture based on the FIWARE
components to build Digital Twins in any field. Our work proposes the use of
Linked Open Data as a mechanism to facilitate the communication of Digital
Twins. We validate our proposal with a use case of an urban Digital Twin that
collaborates with a parking Digital Twin. We conclude that Linked Open Data in
combination with the FIWARE ecosystem is a real reference option to deploy
Digital Twins and to enable the collaboration between Digital Twins.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02254" title="Abstract">arXiv:2402.02254</a> [<a href="/pdf/2402.02254" title="Download PDF">pdf</a>, <a href="/ps/2402.02254" title="Download PostScript">ps</a>, <a href="/format/2402.02254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Teacher-Student Learning based Low Complexity Relay Selection in  Wireless Powered Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Onalan%2C+A+G">Aysun Gurur Onalan</a>, 
<a href="/search/cs?searchtype=author&query=Kopru%2C+B">Berkay Kopru</a>, 
<a href="/search/cs?searchtype=author&query=Coleri%2C+S">Sinem Coleri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Radio Frequency Energy Harvesting (RF-EH) networks are key enablers of
massive Internet-of-things by providing controllable and long-distance energy
transfer to energy-limited devices. Relays, helping either energy or
information transfer, have been demonstrated to significantly improve the
performance of these networks. This paper studies the joint relay selection,
scheduling, and power control problem in multiple-source-multiple-relay RF-EH
networks under nonlinear EH conditions. We first obtain the optimal solution to
the scheduling and power control problem for the given relay selection. Then,
the relay selection problem is formulated as a classification problem, for
which two convolutional neural network (CNN) based architectures are proposed.
While the first architecture employs conventional 2D convolution blocks and
benefits from skip connections between layers; the second architecture replaces
them with inception blocks, to decrease trainable parameter size without
sacrificing accuracy for memory-constrained applications. To decrease the
runtime complexity further, teacher-student learning is employed such that the
teacher network is larger, and the student is a smaller size CNN-based
architecture distilling the teacher's knowledge. A novel dichotomous
search-based algorithm is employed to determine the best architecture for the
student network. Our simulation results demonstrate that the proposed solutions
provide lower complexity than the state-of-art iterative approaches without
compromising optimality.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02255" title="Abstract">arXiv:2402.02255</a> [<a href="/pdf/2402.02255" title="Download PDF">pdf</a>, <a href="/format/2402.02255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Frequency Explains the Inverse Correlation of Large Language Models&#x27;  Size, Training Data Amount, and Surprisal&#x27;s Fit to Reading Times
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oh%2C+B">Byung-Doh Oh</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+S">Shisen Yue</a>, 
<a href="/search/cs?searchtype=author&query=Schuler%2C+W">William Schuler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent studies have shown that as Transformer-based language models become
larger and are trained on very large amounts of data, the fit of their
surprisal estimates to naturalistic human reading times degrades. The current
work presents a series of analyses showing that word frequency is a key
explanatory factor underlying these two trends. First, residual errors from
four language model families on four corpora show that the inverse correlation
between model size and fit to reading times is the strongest on the subset of
least frequent words, which is driven by excessively accurate predictions of
larger model variants. Additionally, training dynamics reveal that during later
training steps, all model variants learn to predict rare words and that larger
model variants do so more accurately, which explains the detrimental effect of
both training data amount and model size on fit to reading times. Finally, a
feature attribution analysis demonstrates that larger model variants are able
to accurately predict rare words based on both an effectively longer context
window size as well as stronger local associations compared to smaller model
variants. Taken together, these results indicate that Transformer-based
language models' surprisal estimates diverge from human-like expectations due
to the superhumanly complex associations they learn for predicting rare words.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02258" title="Abstract">arXiv:2402.02258</a> [<a href="/pdf/2402.02258" title="Download PDF">pdf</a>, <a href="/format/2402.02258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XTSFormer: Cross-Temporal-Scale Transformer for Irregular Time Event  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+T">Tingsong Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zelin Xu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+W">Wenchong He</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jim Su</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yupu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Opoku%2C+R">Raymond Opoku</a>, 
<a href="/search/cs?searchtype=author&query=Ison%2C+R">Ronald Ison</a>, 
<a href="/search/cs?searchtype=author&query=Petho%2C+J">Jason Petho</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+J">Jiang Bian</a>, 
<a href="/search/cs?searchtype=author&query=Tighe%2C+P">Patrick Tighe</a>, 
<a href="/search/cs?searchtype=author&query=Rashidi%2C+P">Parisa Rashidi</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhe Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Event prediction aims to forecast the time and type of a future event based
on a historical event sequence. Despite its significance, several challenges
exist, including the irregularity of time intervals between consecutive events,
the existence of cycles, periodicity, and multi-scale event interactions, as
well as the high computational costs for long event sequences. Existing neural
temporal point processes (TPPs) methods do not capture the multi-scale nature
of event interactions, which is common in many real-world applications such as
clinical event data. To address these issues, we propose the
cross-temporal-scale transformer (XTSFormer), designed specifically for
irregularly timed event data. Our model comprises two vital components: a novel
Feature-based Cycle-aware Time Positional Encoding (FCPE) that adeptly captures
the cyclical nature of time, and a hierarchical multi-scale temporal attention
mechanism. These scales are determined by a bottom-up clustering algorithm.
Extensive experiments on several real-world datasets show that our XTSFormer
outperforms several baseline methods in prediction performance.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02262" title="Abstract">arXiv:2402.02262</a> [<a href="/pdf/2402.02262" title="Download PDF">pdf</a>, <a href="/format/2402.02262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Quality Matters: Suicide Intention Detection on Social Media Posts  Using a RoBERTa-CNN Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+E">Emily Lin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jian Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hsingyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mahoor%2C+M+H">Mohammad H. Mahoor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 1 figure, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Suicide remains a global health concern for the field of health, which
urgently needs innovative approaches for early detection and intervention. In
this paper, we focus on identifying suicidal intentions in SuicideWatch Reddit
posts and present a novel approach to suicide detection using the cutting-edge
RoBERTa-CNN model, a variant of RoBERTa (Robustly optimized BERT approach).
RoBERTa is used for various Natural Language Processing (NLP) tasks, including
text classification and sentiment analysis. The effectiveness of the RoBERTa
lies in its ability to capture textual information and form semantic
relationships within texts. By adding the Convolution Neural Network (CNN)
layer to the original model, the RoBERTa enhances its ability to capture
important patterns from heavy datasets. To evaluate the RoBERTa-CNN, we
experimented on the Suicide and Depression Detection dataset and obtained solid
results. For example, RoBERTa-CNN achieves 98% mean accuracy with the standard
deviation (STD) of 0.0009. It also reaches over 97.5% mean AUC value with an
STD of 0.0013. In the meanwhile, RoBERTa-CNN outperforms competitive methods,
demonstrating the robustness and ability to capture nuanced linguistic patterns
for suicidal intentions. Therefore, RoBERTa-CNN can detect suicide intention on
text data very well.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02263" title="Abstract">arXiv:2402.02263</a> [<a href="/pdf/2402.02263" title="Download PDF">pdf</a>, <a href="/format/2402.02263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly  Mixed Classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yatong Bai</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mo Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+V+M">Vishal M. Patel</a>, 
<a href="/search/cs?searchtype=author&query=Sojoudi%2C+S">Somayeh Sojoudi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Adversarial robustness often comes at the cost of degraded accuracy, impeding
the real-life application of robust classification models. Training-based
solutions for better trade-offs are limited by incompatibilities with
already-trained high-performance large models, necessitating the exploration of
training-free ensemble approaches. Observing that robust models are more
confident in correct predictions than in incorrect ones on clean and
adversarial data alike, we speculate amplifying this "benign confidence
property" can reconcile accuracy and robustness in an ensemble setting. To
achieve so, we propose "MixedNUTS", a training-free method where the output
logits of a robust classifier and a standard non-robust classifier are
processed by nonlinear transformations with only three parameters, which are
optimized through an efficient algorithm. MixedNUTS then converts the
transformed logits into probabilities and mixes them as the overall output. On
CIFAR-10, CIFAR-100, and ImageNet datasets, experimental results with custom
strong adaptive attacks demonstrate MixedNUTS's vastly improved accuracy and
near-SOTA robustness -- it boosts CIFAR-100 clean accuracy by 7.86 points,
sacrificing merely 0.87 points in robust accuracy.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02265" title="Abstract">arXiv:2402.02265</a> [<a href="/pdf/2402.02265" title="Download PDF">pdf</a>, <a href="/format/2402.02265" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterization of the Distortion-Perception Tradeoff for Finite  Channels with Arbitrary Metrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Freirich%2C+D">Dror Freirich</a>, 
<a href="/search/cs?searchtype=author&query=Weinberger%2C+N">Nir Weinberger</a>, 
<a href="/search/cs?searchtype=author&query=Meir%2C+R">Ron Meir</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP); Machine Learning (stat.ML)

</div>
<p class="mathjax">Whenever inspected by humans, reconstructed signals should not be
distinguished from real ones. Typically, such a high perceptual quality comes
at the price of high reconstruction error, and vice versa. We study this
distortion-perception (DP) tradeoff over finite-alphabet channels, for the
Wasserstein-$1$ distance induced by a general metric as the perception index,
and an arbitrary distortion matrix. Under this setting, we show that computing
the DP function and the optimal reconstructions is equivalent to solving a set
of linear programming problems. We provide a structural characterization of the
DP tradeoff, where the DP function is piecewise linear in the perception index.
We further derive a closed-form expression for the case of binary sources.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02268" title="Abstract">arXiv:2402.02268</a> [<a href="/pdf/2402.02268" title="Download PDF">pdf</a>, <a href="/format/2402.02268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning with New Knowledge: Fundamentals, Advances, and  Futures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lixu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Jiahua Dong</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+A">Ating Yin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qinbin Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qi Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Federated Learning (FL) is a privacy-preserving distributed learning approach
that is rapidly developing in an era where privacy protection is increasingly
valued. It is this rapid development trend, along with the continuous emergence
of new demands for FL in the real world, that prompts us to focus on a very
important problem: Federated Learning with New Knowledge. The primary challenge
here is to effectively incorporate various new knowledge into existing FL
systems and evolve these systems to reduce costs, extend their lifespan, and
facilitate sustainable development. In this paper, we systematically define the
main sources of new knowledge in FL, including new features, tasks, models, and
algorithms. For each source, we thoroughly analyze and discuss how to
incorporate new knowledge into existing FL systems and examine the impact of
the form and timing of new knowledge arrival on the incorporation process.
Furthermore, we comprehensively discuss the potential future directions for FL
with new knowledge, considering a variety of factors such as scenario setups,
efficiency, and security. There is also a continuously updating repository for
this topic: https://github.com/conditionWang/FLNK.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02273" title="Abstract">arXiv:2402.02273</a> [<a href="/pdf/2402.02273" title="Download PDF">pdf</a>, <a href="/format/2402.02273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Augmenting MRI scan data with real-time predictions of glioblastoma  brain tumor evolution using exponential time integrators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Pabisz%2C+M">Magdalena Pabisz</a>, 
<a href="/search/math?searchtype=author&query=Mu%C3%B1oz-Matute%2C+J">Judit Mu&#xf1;oz-Matute</a>, 
<a href="/search/math?searchtype=author&query=Paszy%C5%84ski%2C+M">Maciej Paszy&#x144;ski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We present an ultra-fast simulator to augment the MRI scan imaging of
glioblastoma brain tumors with predictions of future evolution. We consider the
glioblastoma tumor growth model based on the Fisher-Kolmogorov
diffusion-reaction equation with logistic growth. For the discretization we
employ finite differences in space coupled with a time integrator in time
employing the routines from [Al-Mohy, et. al. Computing the action of the
matrix exponential, with an application to exponential integrators, SIAM
Journal on Scientific Computing, 2011] to compute the actions of the
exponentials of the linear operator. By combining these methods, we can perform
the prediction of the tumor evolution for several months forward within a
couple of seconds on a modern laptop. This method does not require HPC
supercomputing centers, and it can be performed on the fly using a laptop with
Windows 10, Octave simulations, and ParaView visualization. We illustrate our
simulations by predicting the tumor growth evolution based on three-dimensional
MRI scan data.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02274" title="Abstract">arXiv:2402.02274</a> [<a href="/pdf/2402.02274" title="Download PDF">pdf</a>, <a href="/ps/2402.02274" title="Download PostScript">ps</a>, <a href="/format/2402.02274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InceptionCapsule: Inception-Resnet and CapsuleNet with self-attention  for medical image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sadeghnezhad%2C+E">Elham Sadeghnezhad</a>, 
<a href="/search/cs?searchtype=author&query=Salem%2C+S">Sajjad Salem</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Initial weighting is significant in deep neural networks because the random
selection of weights produces different outputs and increases the probability
of overfitting and underfitting. On the other hand, vector-based approaches to
extract vector features need rich vectors for more accurate classification. The
InceptionCapsule approach is presented to alleviate these two problems. This
approach uses transfer learning and the Inception-ResNet model to avoid random
selection of weights, which takes initial weights from ImageNet. It also uses
the output of Inception middle layers to generate rich vectors. Extracted
vectors are given to a capsule network for learning, which is equipped with an
attention technique. Kvasir data and BUSI with the GT dataset were used to
evaluate this approach. This model was able to achieve 97.62 accuracies in
5-class classification and also achieved 94.30 accuracies in 8-class
classification on Kvasir. In the BUSI with GT dataset, the proposed approach
achieved accuracy=98.88, Precision=95.34, and F1-score=93.74, which are
acceptable results compared to other approaches in the literature.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02275" title="Abstract">arXiv:2402.02275</a> [<a href="/pdf/2402.02275" title="Download PDF">pdf</a>, <a href="/format/2402.02275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SudokuSens: Enhancing Deep Learning Robustness for IoT Sensing  Applications using a Generative Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianshi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruijie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kara%2C+D">Denizhan Kara</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shengzhong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wertheimer%2C+D">Davis Wertheimer</a>, 
<a href="/search/cs?searchtype=author&query=Viros-i-Martin%2C+A">Antoni Viros-i-Martin</a>, 
<a href="/search/cs?searchtype=author&query=Ganti%2C+R">Raghu Ganti</a>, 
<a href="/search/cs?searchtype=author&query=Srivatsa%2C+M">Mudhakar Srivatsa</a>, 
<a href="/search/cs?searchtype=author&query=Abdelzaher%2C+T">Tarek Abdelzaher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in ACM Conference on Embedded Networked Sensor Systems (SenSys 23), November, 2023, Istanbul, Turkiye
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper introduces SudokuSens, a generative framework for automated
generation of training data in machine-learning-based Internet-of-Things (IoT)
applications, such that the generated synthetic data mimic experimental
configurations not encountered during actual sensor data collection. The
framework improves the robustness of resulting deep learning models, and is
intended for IoT applications where data collection is expensive. The work is
motivated by the fact that IoT time-series data entangle the signatures of
observed objects with the confounding intrinsic properties of the surrounding
environment and the dynamic environmental disturbances experienced. To
incorporate sufficient diversity into the IoT training data, one therefore
needs to consider a combinatorial explosion of training cases that are
multiplicative in the number of objects considered and the possible
environmental conditions in which such objects may be encountered. Our
framework substantially reduces these multiplicative training needs. To
decouple object signatures from environmental conditions, we employ a
Conditional Variational Autoencoder (CVAE) that allows us to reduce data
collection needs from multiplicative to (nearly) linear, while synthetically
generating (data for) the missing conditions. To obtain robustness with respect
to dynamic disturbances, a session-aware temporal contrastive learning approach
is taken. Integrating the aforementioned two approaches, SudokuSens
significantly improves the robustness of deep learning for IoT applications. We
explore the degree to which SudokuSens benefits downstream inference tasks in
different data sets and discuss conditions under which the approach is
particularly effective.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02277" title="Abstract">arXiv:2402.02277</a> [<a href="/pdf/2402.02277" title="Download PDF">pdf</a>, <a href="/format/2402.02277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Bayesian Optimization via Exogenous Distribution Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Shaogang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+X">Xiaoning Qian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Maximizing a target variable as an operational objective in a structured
causal model is an important problem. Existing Causal Bayesian Optimization
(CBO) methods either rely on hard interventions that alter the causal structure
to maximize the reward; or introduce action nodes to endogenous variables so
that the data generation mechanisms are adjusted to achieve the objective. In
this paper, a novel method is introduced to learn the distribution of exogenous
variables, which is typically ignored or marginalized through expectation by
existing methods.
<br />Exogenous distribution learning improves the approximation accuracy of
structured causal models in a surrogate model that is usually trained with
limited observational data. Moreover, the learned exogenous distribution
extends existing CBO to general causal schemes beyond Additive Noise Models
(ANM). The recovery of exogenous variables allows us to use a more flexible
prior for noise or unobserved hidden variables. A new CBO method is developed
by leveraging the learned exogenous distribution. Experiments on different
datasets and applications show the benefits of our proposed method.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02280" title="Abstract">arXiv:2402.02280</a> [<a href="/pdf/2402.02280" title="Download PDF">pdf</a>, <a href="/format/2402.02280" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spline-Based Stochastic Collocation Methods for Uncertainty  Quantification in Nonlinear Hyperbolic PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chertock%2C+A">Alina Chertock</a>, 
<a href="/search/math?searchtype=author&query=Iskhakov%2C+A+S">Arsen S. Iskhakov</a>, 
<a href="/search/math?searchtype=author&query=Janajra%2C+S">Safa Janajra</a>, 
<a href="/search/math?searchtype=author&query=Kurganov%2C+A">Alexander Kurganov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we study the stochastic collocation (SC) methods for
uncertainty quantification (UQ) in hyperbolic systems of nonlinear partial
differential equations (PDEs). In these methods, the underlying PDEs are
numerically solved at a set of collocation points in random space. A standard
SC approach is based on a generalized polynomial chaos (gPC) expansion, which
relies on choosing the collocation points based on the prescribed probability
distribution and approximating the computed solution by a linear combination of
orthogonal polynomials in the random variable. We demonstrate that this
approach struggles to accurately capture discontinuous solutions, often leading
to oscillations (Gibbs phenomenon) that deviate significantly from the physical
solutions. We explore alternative SC methods, in which one can choose an
arbitrary set of collocation points and employ shape-preserving splines to
interpolate the solution in a random space. Our study demonstrates the
effectiveness of spline-based collocation in accurately capturing and assessing
uncertainties while suppressing oscillations. We illustrate the superiority of
the spline-based collocation on two numerical examples, including the inviscid
Burgers and shallow water equations.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02284" title="Abstract">arXiv:2402.02284</a> [<a href="/pdf/2402.02284" title="Download PDF">pdf</a>, <a href="/ps/2402.02284" title="Download PostScript">ps</a>, <a href="/format/2402.02284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variable-order fractional Laplacian and its accurate and efficient  computations with meshfree methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wu%2C+Y">Yixuan Wu</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+Y">Yanzhi Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The variable-order fractional Laplacian plays an important role in the study
of heterogeneous systems. In this paper, we propose the first numerical methods
for the variable-order Laplacian $(-\Delta)^{\alpha({\bf x})/2}$ with $0 &lt;
\alpha({\bf x}) \le 2$, which will also be referred as the variable-order
fractional Laplacian if $\alpha({\bf x})$ is strictly less than 2. We present a
class of hypergeometric functions whose variable-order Laplacian can be
analytically expressed. Building on these analytical results, we design the
meshfree methods based on globally supported radial basis functions (RBFs),
including Gaussian, generalized inverse multiquadric, and Bessel-type RBFs, to
approximate the variable-order Laplacian $(-\Delta)^{\alpha({\bf x})/2}$. Our
meshfree methods integrate the advantages of both pseudo-differential and
hypersingular integral forms of the variable-order fractional Laplacian, and
thus avoid numerically approximating the hypersingular integral. Moreover, our
methods are simple and flexible of domain geometry, and their computer
implementation remains the same for any dimension $d \ge 1$. Compared to finite
difference methods, our methods can achieve a desired accuracy with much fewer
points. This fact makes our method much attractive for problems involving
variable-order fractional Laplacian where the number of points required is a
critical cost. We then apply our method to study solution behaviors of
variable-order fractional PDEs arising in different fields, including
transition of waves between classical and fractional media, and coexistence of
anomalous and normal diffusion in both diffusion equation and the Allen-Cahn
equation. These results would provide insights for further understanding and
applications of variable-order fractional derivatives.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02285" title="Abstract">arXiv:2402.02285</a> [<a href="/pdf/2402.02285" title="Download PDF">pdf</a>, <a href="/format/2402.02285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State  Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+A">Atharva Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Tseng%2C+B">Bo-Hsiang Tseng</a>, 
<a href="/search/cs?searchtype=author&query=Moniz%2C+J+R+A">Joel Ruben Antony Moniz</a>, 
<a href="/search/cs?searchtype=author&query=Piraviperumal%2C+D">Dhivya Piraviperumal</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Bhargava%2C+S">Shruti Bhargava</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages. 4 figures, EACL 2024 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In-context learning with Large Language Models (LLMs) has emerged as a
promising avenue of research in Dialog State Tracking (DST). However, the
best-performing in-context learning methods involve retrieving and adding
similar examples to the prompt, requiring access to labeled training data.
Procuring such training data for a wide range of domains and applications is
time-consuming, expensive, and, at times, infeasible. While zero-shot learning
requires no training data, it significantly lags behind the few-shot setup.
Thus, `\textit{Can we efficiently generate synthetic data for any dialogue
schema to enable few-shot prompting?}' Addressing this question, we propose
\method, a data generation framework tailored for DST, utilizing LLMs. Our
approach only requires the dialogue schema and a few hand-crafted dialogue
templates to synthesize natural, coherent, and free-flowing dialogues with DST
annotations. Few-shot learning using data from {\method} results in $4-5%$
improvement in Joint Goal Accuracy over the zero-shot baseline on MultiWOZ 2.1
and 2.4. Remarkably, our few-shot learning approach recovers nearly $98%$ of
the performance compared to the few-shot setup using human-annotated training
data. Our synthetic data and code can be accessed at
https://github.com/apple/ml-synthdst
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02286" title="Abstract">arXiv:2402.02286</a> [<a href="/pdf/2402.02286" title="Download PDF">pdf</a>, <a href="/format/2402.02286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Level Feature Aggregation and Recursive Alignment Network for  Real-Time Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanhua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Ke Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yulin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wuwei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 9 figures. Manuscript completed on April 30, 2022, and then submitted to Transactions on Image Processing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Real-time semantic segmentation is a crucial research for real-world
applications. However, many methods lay particular emphasis on reducing the
computational complexity and model size, while largely sacrificing the
accuracy. In some scenarios, such as autonomous navigation and driver
assistance system, accuracy and speed are equally important. To tackle this
problem, we propose a novel Multi-level Feature Aggregation and Recursive
Alignment Network (MFARANet), aiming to achieve high segmentation accuracy at
real-time inference speed. We employ ResNet-18 as the backbone to ensure
efficiency, and propose three core components to compensate for the reduced
model capacity due to the shallow backbone. Specifically, we first design
Multi-level Feature Aggregation Module (MFAM) to aggregate the hierarchical
features in the encoder to each scale to benefit subsequent spatial alignment
and multi-scale inference. Then, we build Recursive Alignment Module (RAM) by
combining the flow-based alignment module with recursive upsampling
architecture for accurate and efficient spatial alignment between multi-scale
score maps. Finally, the Adaptive Scores Fusion Module (ASFM) is proposed to
adaptively fuse multi-scale scores so that the final prediction can favor
objects of multiple scales. Comprehensive experiments on three benchmark
datasets including Cityscapes, CamVid and PASCAL-Context show the effectiveness
and efficiency of our method. In particular, we achieve a better balance
between speed and accuracy than state-of-the-art real-time methods on
Cityscapes and CamVid datasets. Code is available at:
https://github.com/Yanhua-Zhang/MFARANet.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02287" title="Abstract">arXiv:2402.02287</a> [<a href="/pdf/2402.02287" title="Download PDF">pdf</a>, <a href="/format/2402.02287" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Future Directions in Foundations of Graph Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morris%2C+C">Christopher Morris</a>, 
<a href="/search/cs?searchtype=author&query=Dym%2C+N">Nadav Dym</a>, 
<a href="/search/cs?searchtype=author&query=Maron%2C+H">Haggai Maron</a>, 
<a href="/search/cs?searchtype=author&query=Ceylan%2C+%C4%B0+%C4%B0">&#x130;smail &#x130;lkan Ceylan</a>, 
<a href="/search/cs?searchtype=author&query=Frasca%2C+F">Fabrizio Frasca</a>, 
<a href="/search/cs?searchtype=author&query=Levie%2C+R">Ron Levie</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+D">Derek Lim</a>, 
<a href="/search/cs?searchtype=author&query=Bronstein%2C+M">Michael Bronstein</a>, 
<a href="/search/cs?searchtype=author&query=Grohe%2C+M">Martin Grohe</a>, 
<a href="/search/cs?searchtype=author&query=Jegelka%2C+S">Stefanie Jegelka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Discrete Mathematics (cs.DM); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)

</div>
<p class="mathjax">Machine learning on graphs, especially using graph neural networks (GNNs),
has seen a surge in interest due to the wide availability of graph data across
a broad spectrum of disciplines, from life to social and engineering sciences.
Despite their practical success, our theoretical understanding of the
properties of GNNs remains highly incomplete. Recent theoretical advancements
primarily focus on elucidating the coarse-grained expressive power of GNNs,
predominantly employing combinatorial techniques. However, these studies do not
perfectly align with practice, particularly in understanding the generalization
behavior of GNNs when trained with stochastic first-order optimization
techniques. In this position paper, we argue that the graph machine learning
community needs to shift its attention to developing a more balanced theory of
graph machine learning, focusing on a more thorough understanding of the
interplay of expressive power, generalization, and optimization.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02288" title="Abstract">arXiv:2402.02288</a> [<a href="/pdf/2402.02288" title="Download PDF">pdf</a>, <a href="/format/2402.02288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $\textit{A Contrario}$ Paradigm for YOLO-based Infrared Small Target  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ciocarlan%2C+A">Alina Ciocarlan</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%A9garat-Mascle%2C+S+L">Sylvie Le H&#xe9;garat-Mascle</a>, 
<a href="/search/cs?searchtype=author&query=Lefebvre%2C+S">Sidonie Lefebvre</a>, 
<a href="/search/cs?searchtype=author&query=Woiselle%2C+A">Arnaud Woiselle</a>, 
<a href="/search/cs?searchtype=author&query=Barbanson%2C+C">Clara Barbanson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Detecting small to tiny targets in infrared images is a challenging task in
computer vision, especially when it comes to differentiating these targets from
noisy or textured backgrounds. Traditional object detection methods such as
YOLO struggle to detect tiny objects compared to segmentation neural networks,
resulting in weaker performance when detecting small targets. To reduce the
number of false alarms while maintaining a high detection rate, we introduce an
$\textit{a contrario}$ decision criterion into the training of a YOLO detector.
The latter takes advantage of the $\textit{unexpectedness}$ of small targets to
discriminate them from complex backgrounds. Adding this statistical criterion
to a YOLOv7-tiny bridges the performance gap between state-of-the-art
segmentation methods for infrared small target detection and object detection
networks. It also significantly increases the robustness of YOLO towards
few-shot settings.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02289" title="Abstract">arXiv:2402.02289</a> [<a href="/pdf/2402.02289" title="Download PDF">pdf</a>, <a href="/format/2402.02289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SemPool: Simple, robust, and interpretable KG pooling for enhancing  language models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mavromatis%2C+C">Costas Mavromatis</a>, 
<a href="/search/cs?searchtype=author&query=Karypis%2C+P">Petros Karypis</a>, 
<a href="/search/cs?searchtype=author&query=Karypis%2C+G">George Karypis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Knowledge Graph (KG) powered question answering (QA) performs complex
reasoning over language semantics as well as knowledge facts. Graph Neural
Networks (GNNs) learn to aggregate information from the underlying KG, which is
combined with Language Models (LMs) for effective reasoning with the given
question. However, GNN-based methods for QA rely on the graph information of
the candidate answer nodes, which limits their effectiveness in more
challenging settings where critical answer information is not included in the
KG. We propose a simple graph pooling approach that learns useful semantics of
the KG that can aid the LM's reasoning and that its effectiveness is robust
under graph perturbations. Our method, termed SemPool, represents KG facts with
pre-trained LMs, learns to aggregate their semantic information, and fuses it
at different layers of the LM. Our experimental results show that SemPool
outperforms state-of-the-art GNN-based methods by 2.27% accuracy points on
average when answer information is missing from the KG. In addition, SemPool
offers interpretability on what type of graph information is fused at different
LM layers.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02295" title="Abstract">arXiv:2402.02295</a> [<a href="/pdf/2402.02295" title="Download PDF">pdf</a>, <a href="/format/2402.02295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WENO reconstructions of unconditionally optimal high order
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Baeza%2C+A">Antonio Baeza</a>, 
<a href="/search/math?searchtype=author&query=B%C3%BCrger%2C+R">Raimund B&#xfc;rger</a>, 
<a href="/search/math?searchtype=author&query=Mulet%2C+P">Pep Mulet</a>, 
<a href="/search/math?searchtype=author&query=Zor%C3%ADo%2C+D">David Zor&#xed;o</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> SIAM J. NUMER. ANAL. Vol. 57, No. 6, pp. 2760-2784 (2019)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A modified Weighted Essentially Non-Oscillatory (WENO) reconstruction
technique preventing accuracy loss near critical points (regardless of their
order) of the underlying data is presented. This approach only uses local data
from the reconstruction stencil and does not rely on any sort of scaling
parameters. The key novel ingredient is a weight design based on a new
smoothness indicator, which defines the first WENO reconstruction procedure
that never loses accuracy on smooth data, regardless of the presence of
critical points of any order, and is therefore addressed as optimal WENO
(OWENO) method. The corresponding weights are non-dimensional and
scale-independent. The weight designs are supported by theoretical results
concerning the accuracy of the smoothness indicators. The method is validated
by numerical tests related to algebraic equations, scalar conservation laws,
and systems of conservation laws.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02298" title="Abstract">arXiv:2402.02298</a> [<a href="/pdf/2402.02298" title="Download PDF">pdf</a>, <a href="/format/2402.02298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polyp-DAM: Polyp segmentation via depth anything model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhuoran Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yeying Jin</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xiuyi Jia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recently, large models (Segment Anything model) came on the scene to provide
a new baseline for polyp segmentation tasks. This demonstrates that large
models with a sufficient image level prior can achieve promising performance on
a given task. In this paper, we unfold a new perspective on polyp segmentation
modeling by leveraging the Depth Anything Model (DAM) to provide depth prior to
polyp segmentation models. Specifically, the input polyp image is first passed
through a frozen DAM to generate a depth map. The depth map and the input polyp
images are then concatenated and fed into a convolutional neural network with
multiscale to generate segmented images. Extensive experimental results
demonstrate the effectiveness of our method, and in addition, we observe that
our method still performs well on images of polyps with noise. The URL of our
code is \url{https://github.com/zzr-idam/Polyp-DAM}.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02299" title="Abstract">arXiv:2402.02299</a> [<a href="/pdf/2402.02299" title="Download PDF">pdf</a>, <a href="/format/2402.02299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Review and Comparison of AI Enhanced Side Channel Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Panoff%2C+M">Max Panoff</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Honggang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+H">Haoqi Shan</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yier Jin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by ACM Journal on Emerging Technologies in Computing Systems (JETC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Side Channel Analysis (SCA) presents a clear threat to privacy and security
in modern computing systems. The vast majority of communications are secured
through cryptographic algorithms. These algorithms are often provably-secure
from a cryptographical perspective, but their implementation on real hardware
introduces vulnerabilities. Adversaries can exploit these vulnerabilities to
conduct SCA and recover confidential information, such as secret keys or
internal states. The threat of SCA has greatly increased as machine learning,
and in particular deep learning, enhanced attacks become more common. In this
work, we will examine the latest state-of-the-art deep learning techniques for
side channel analysis, the theory behind them, and how they are conducted. Our
focus will be on profiling attacks using deep learning techniques, but we will
also examine some new and emerging methodologies enhanced by deep learning
techniques, such as non-profiled attacks, artificial trace generation, and
others. Finally, different deep learning enhanced SCA schemes attempted against
the ANSSI SCA Database (ASCAD) and their relative performance will be evaluated
and compared. This will lead to new research directions to secure cryptographic
implementations against the latest SCA attacks.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02300" title="Abstract">arXiv:2402.02300</a> [<a href="/pdf/2402.02300" title="Download PDF">pdf</a>, <a href="/format/2402.02300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An efficient third-order WENO scheme with unconditionally optimal  accuracy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Baeza%2C+A">Antonio Baeza</a>, 
<a href="/search/math?searchtype=author&query=B%C3%BCrger%2C+R">Raimund B&#xfc;rger</a>, 
<a href="/search/math?searchtype=author&query=Mulet%2C+P">Pep Mulet</a>, 
<a href="/search/math?searchtype=author&query=Zor%C3%ADo%2C+D">David Zor&#xed;o</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> SIAM J. SCI. COMPUT.Vol. 42, No. 2, pp. A1028-A1051 (2020)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A novel scheme, based on third-order Weighted Essentially Non-Oscillatory
(WENO) reconstructions, is presented. It attains unconditionally optimal
accuracy when the data is smooth enough, even in presence of critical points,
and second-order accuracy if a discontinuity crosses the data. The key to
attribute these properties to this scheme is the inclusion of an additional
node in the data stencil, which is only used in the computation of the weights
measuring the smoothness. The accuracy properties of this scheme are proven in
detail and several numerical experiments are presented, which show that this
scheme is more efficient in terms of the error reduction versus CPU time than
its traditional third-order counterparts as well as several higher-order WENO
schemes that are found in the literature.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02301" title="Abstract">arXiv:2402.02301</a> [<a href="/pdf/2402.02301" title="Download PDF">pdf</a>, <a href="/ps/2402.02301" title="Download PostScript">ps</a>, <a href="/format/2402.02301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MATLAB Simulator of Level-Index Arithmetic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mikaitis%2C+M">Mantas Mikaitis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Software (cs.MS)</span>; Hardware Architecture (cs.AR); Numerical Analysis (math.NA)

</div>
<p class="mathjax">Level-index arithmetic appeared in the 1980s. One of its principal purposes
was to abolish the issues caused by underflows and overflows in floating point.
However, level-index arithmetic does not expand the set of numbers but spaces
out the numbers of large magnitude even more than floating-point
representations to move the infinities further away from zero: gaps between
numbers on both ends of the range become very large. It is a trade-off which
did not seem to convince the hardware manufacturers. We revisit it by
presenting a custom precision simulator in MATLAB. This toolbox is useful for
exploring performance of level-index arithmetic in research projects, such as
using 8-bit and 16-bit level-index representations in machine learning
algorithms where narrow bit-width is desired but overflow/underflow of
equivalent width floating-point representations causes difficulties.
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02307" title="Abstract">arXiv:2402.02307</a> [<a href="/pdf/2402.02307" title="Download PDF">pdf</a>, <a href="/format/2402.02307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Activity and Data Detection for Massive Grant-Free Access Using  Deterministic Non-Orthogonal Signatures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+N+Y">Nam Yul Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wei Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Grant-free access is a key enabler for connecting wireless devices with low
latency and low signaling overhead in massive machine-type communications
(mMTC). For massive grant-free access, user-specific signatures are uniquely
assigned to mMTC devices. In this paper, we first derive a sufficient condition
for the successful identification of active devices through maximum likelihood
(ML) estimation in massive grant-free access. The condition is represented by
the coherence of a signature sequence matrix containing the signatures of all
devices. Then, we present a design framework of non-orthogonal signature
sequences in a deterministic fashion. The design principle relies on unimodular
masking sequences with low correlation, which are applied as masking sequences
to the columns of the discrete Fourier transform (DFT) matrix. For example
constructions, we use four polyphase masking sequences represented by
characters over finite fields. Leveraging algebraic techniques, we show that
the signature sequence matrix of proposed non-orthogonal sequences has
theoretically bounded low coherence. Simulation results demonstrate that the
deterministic non-orthogonal signatures achieve the excellent performance of
joint activity and data detection by ML- and approximate message passing
(AMP)-based algorithms for massive grant-free access in mMTC.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02308" title="Abstract">arXiv:2402.02308</a> [<a href="/pdf/2402.02308" title="Download PDF">pdf</a>, <a href="/format/2402.02308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language-guided Active Sensing of Confined, Cluttered Environments via  Object Rearrangement Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weihan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Hanwen Ren</a>, 
<a href="/search/cs?searchtype=author&query=Qureshi%2C+A+H">Ahmed H. Qureshi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IEEE/RAS ICRA'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Language-guided active sensing is a robotics subtask where a robot with an
onboard sensor interacts efficiently with the environment via object
manipulation to maximize perceptual information, following given language
instructions. These tasks appear in various practical robotics applications,
such as household service, search and rescue, and environment monitoring.
Despite many applications, the existing works do not account for language
instructions and have mainly focused on surface sensing, i.e., perceiving the
environment from the outside without rearranging it for dense sensing.
Therefore, in this paper, we introduce the first language-guided active sensing
approach that allows users to observe specific parts of the environment via
object manipulation. Our method spatially associates the environment with
language instructions, determines the best camera viewpoints for perception,
and then iteratively selects and relocates the best view-blocking objects to
provide the dense perception of the region of interest. We evaluate our method
against different baseline algorithms in simulation and also demonstrate it in
real-world confined cabinet-like settings with multiple unknown objects. Our
results show that the proposed method exhibits better performance across
different metrics and successfully generalizes to real-world complex scenarios.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02309" title="Abstract">arXiv:2402.02309</a> [<a href="/pdf/2402.02309" title="Download PDF">pdf</a>, <a href="/format/2402.02309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jailbreaking Attack against Multimodal Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niu%2C+Z">Zhenxing Niu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Haodong Ren</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xinbo Gao</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+G">Gang Hua</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+R">Rong Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper focuses on jailbreaking attacks against multi-modal large language
models (MLLMs), seeking to elicit MLLMs to generate objectionable responses to
harmful user queries. A maximum likelihood-based algorithm is proposed to find
an \emph{image Jailbreaking Prompt} (imgJP), enabling jailbreaks against MLLMs
across multiple unseen prompts and images (i.e., data-universal property). Our
approach exhibits strong model-transferability, as the generated imgJP can be
transferred to jailbreak various models, including MiniGPT-v2, LLaVA,
InstructBLIP, and mPLUG-Owl2, in a black-box manner. Moreover, we reveal a
connection between MLLM-jailbreaks and LLM-jailbreaks. As a result, we
introduce a construction-based method to harness our approach for
LLM-jailbreaks, demonstrating greater efficiency than current state-of-the-art
methods. The code is available here. \textbf{Warning: some content generated by
language models may be offensive to some readers.}
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02313" title="Abstract">arXiv:2402.02313</a> [<a href="/pdf/2402.02313" title="Download PDF">pdf</a>, <a href="/format/2402.02313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CNS-Edit: 3D Shape Editing via Coupled Neural Shape Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jingyu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+K">Ka-Hei Hui</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengzhe Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+C">Chi-Wing Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">This paper introduces a new approach based on a coupled representation and a
neural volume optimization to implicitly perform 3D shape editing in latent
space. This work has three innovations. First, we design the coupled neural
shape (CNS) representation for supporting 3D shape editing. This representation
includes a latent code, which captures high-level global semantics of the
shape, and a 3D neural feature volume, which provides a spatial context to
associate with the local shape changes given by the editing. Second, we
formulate the coupled neural shape optimization procedure to co-optimize the
two coupled components in the representation subject to the editing operation.
Last, we offer various 3D shape editing operators, i.e., copy, resize, delete,
and drag, and derive each into an objective for guiding the CNS optimization,
such that we can iteratively co-optimize the latent code and neural feature
volume to match the editing target. With our approach, we can achieve a rich
variety of editing results that are not only aware of the shape semantics but
are also not easy to achieve by existing approaches. Both quantitative and
qualitative evaluations demonstrate the strong capabilities of our approach
over the state-of-the-art solutions.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02314" title="Abstract">arXiv:2402.02314</a> [<a href="/pdf/2402.02314" title="Download PDF">pdf</a>, <a href="/format/2402.02314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Selecting Large Language Model to Fine-tune via Rectified Scaling Law
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Haowei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Baizhou Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Haotian Ye</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qinyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sujian Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jianzhu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiaojun Wan</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">James Zou</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yitao Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">The ever-growing ecosystem of LLMs has posed a challenge in selecting the
most appropriate pre-trained model to fine-tune amidst a sea of options. Given
constrained resources, fine-tuning all models and making selections afterward
is unrealistic. In this work, we formulate this resource-constrained selection
task into predicting fine-tuning performance and illustrate its natural
connection with scaling laws. Unlike pre-training, We find that the fine-tuning
scaling curve includes not just the well-known "power phase" but also the
previously unobserved "pre-power phase". We also explain why existing scaling
laws fail to capture this phase transition phenomenon both theoretically and
empirically. To address this, we introduce the concept of "pre-learned data
size" into our rectified scaling law, which overcomes theoretical limitations
and fits experimental results much better. By leveraging our law, we propose a
novel LLM selection algorithm that selects the near-optimal model with hundreds
of times less resource consumption, while other methods may provide negatively
correlated selection.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02315" title="Abstract">arXiv:2402.02315</a> [<a href="/pdf/2402.02315" title="Download PDF">pdf</a>, <a href="/format/2402.02315" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Large Language Models in Finance (FinLLMs)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jean Lee</a>, 
<a href="/search/cs?searchtype=author&query=Stevens%2C+N">Nicholas Stevens</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S+C">Soyeon Caren Han</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+M">Minseok Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> More information on <a href="https://github.com/adlnlp/FinLLMs">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; General Finance (q-fin.GN)

</div>
<p class="mathjax">Large Language Models (LLMs) have shown remarkable capabilities across a wide
variety of Natural Language Processing (NLP) tasks and have attracted attention
from multiple domains, including financial services. Despite the extensive
research into general-domain LLMs, and their immense potential in finance,
Financial LLM (FinLLM) research remains limited. This survey provides a
comprehensive overview of FinLLMs, including their history, techniques,
performance, and opportunities and challenges. Firstly, we present a
chronological overview of general-domain Pre-trained Language Models (PLMs)
through to current FinLLMs, including the GPT-series, selected open-source
LLMs, and financial LMs. Secondly, we compare five techniques used across
financial PLMs and FinLLMs, including training methods, training data, and
fine-tuning methods. Thirdly, we summarize the performance evaluations of six
benchmark tasks and datasets. In addition, we provide eight advanced financial
NLP tasks and datasets for developing more sophisticated FinLLMs. Finally, we
discuss the opportunities and the challenges facing FinLLMs, such as
hallucination, privacy, and efficiency. To support AI research in finance, we
compile a collection of accessible datasets and evaluation benchmarks on
GitHub.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02316" title="Abstract">arXiv:2402.02316</a> [<a href="/pdf/2402.02316" title="Download PDF">pdf</a>, <a href="/format/2402.02316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Your Diffusion Model is Secretly a Certifiably Robust Classifier
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huanran Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yinpeng Dong</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+S">Shitong Shao</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Z">Zhongkai Hao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hang Su</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Diffusion models are recently employed as generative classifiers for robust
classification. However, a comprehensive theoretical understanding of the
robustness of diffusion classifiers is still lacking, leading us to question
whether they will be vulnerable to future stronger attacks. In this study, we
propose a new family of diffusion classifiers, named Noised Diffusion
Classifiers~(NDCs), that possess state-of-the-art certified robustness.
Specifically, we generalize the diffusion classifiers to classify
Gaussian-corrupted data by deriving the evidence lower bounds (ELBOs) for these
distributions, approximating the likelihood using the ELBO, and calculating
classification probabilities via Bayes' theorem. We integrate these generalized
diffusion classifiers with randomized smoothing to construct smoothed
classifiers possessing non-constant Lipschitzness. Experimental results
demonstrate the superior certified robustness of our proposed NDCs. Notably, we
are the first to achieve 80\%+ and 70\%+ certified robustness on CIFAR-10 under
adversarial perturbations with $\ell_2$ norm less than 0.25 and 0.5,
respectively, using a single off-the-shelf diffusion model without any
additional data.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02317" title="Abstract">arXiv:2402.02317</a> [<a href="/pdf/2402.02317" title="Download PDF">pdf</a>, <a href="/format/2402.02317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> INViT: A Generalizable Routing Problem Solver with Invariant Nested View  Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+H">Han Fang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhihao Song</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+P">Paul Weng</a>, 
<a href="/search/cs?searchtype=author&query=Ban%2C+Y">Yutong Ban</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recently, deep reinforcement learning has shown promising results for
learning fast heuristics to solve routing problems. Meanwhile, most of the
solvers suffer from generalizing to an unseen distribution or distributions
with different scales. To address this issue, we propose a novel architecture,
called Invariant Nested View Transformer (INViT), which is designed to enforce
a nested design together with invariant views inside the encoders to promote
the generalizability of the learned solver. It applies a modified policy
gradient algorithm enhanced with data augmentations. We demonstrate that the
proposed INViT achieves a dominant generalization performance on both TSP and
CVRP problems with various distributions and different problem scales.
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02318" title="Abstract">arXiv:2402.02318</a> [<a href="/pdf/2402.02318" title="Download PDF">pdf</a>, <a href="/format/2402.02318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diversity Measurement and Subset Selection for Instruction Tuning  Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peiqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yikang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhen Guo</a>, 
<a href="/search/cs?searchtype=author&query=Stallone%2C+M">Matthew Stallone</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Golland%2C+P">Polina Golland</a>, 
<a href="/search/cs?searchtype=author&query=Panda%2C+R">Rameswar Panda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We aim to select data subsets for the fine-tuning of large language models to
more effectively follow instructions. Prior work has emphasized the importance
of diversity in dataset curation but relied on heuristics such as the number of
tasks. In this paper, we use determinantal point processes to capture the
diversity and quality of instruction tuning datasets for subset selection. We
propose to measure dataset diversity with log determinant distance that is the
distance between the dataset of interest and a maximally diverse reference
dataset. Our experiments demonstrate that the proposed diversity measure in the
normalized weight gradient space is correlated with downstream
instruction-following performance. Consequently, it can be used to inform when
data selection is the most helpful and to analyze dataset curation strategies.
We demonstrate the utility of our approach on various instruction tuning
datasets.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02319" title="Abstract">arXiv:2402.02319</a> [<a href="/pdf/2402.02319" title="Download PDF">pdf</a>, <a href="/ps/2402.02319" title="Download PostScript">ps</a>, <a href="/format/2402.02319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smart Textile-Driven Soft Spine Exosuit for Lifting Tasks in Industrial  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K">Kefan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+B">Bibhu Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+P+T">Phuoc Thien Phan</a>, 
<a href="/search/cs?searchtype=author&query=Davies%2C+J">James Davies</a>, 
<a href="/search/cs?searchtype=author&query=Thai%2C+M+T">Mai Thanh Thai</a>, 
<a href="/search/cs?searchtype=author&query=Hoang%2C+T+T">Trung Thien Hoang</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+C+C">Chi Cong Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+A">Adrienne Ji</a>, 
<a href="/search/cs?searchtype=author&query=Nicotra%2C+E">Emanuele Nicotra</a>, 
<a href="/search/cs?searchtype=author&query=Lovell%2C+N+H">Nigel H. Lovell</a>, 
<a href="/search/cs?searchtype=author&query=Do%2C+T+N">Thanh Nho Do</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Work related musculoskeletal disorders (WMSDs) are often caused by repetitive
lifting, making them a significant concern in occupational health. Although
wearable assist devices have become the norm for mitigating the risk of back
pain, most spinal assist devices still possess a partially rigid structure that
impacts the user comfort and flexibility. This paper addresses this issue by
presenting a smart textile actuated spine assistance robotic exosuit (SARE),
which can conform to the back seamlessly without impeding the user movement and
is incredibly lightweight. The SARE can assist the human erector spinae to
complete any action with virtually infinite degrees of freedom. To detect the
strain on the spine and to control the smart textile automatically, a soft
knitting sensor which utilizes fluid pressure as sensing element is used. The
new device is validated experimentally with human subjects where it reduces
peak electromyography (EMG) signals of lumbar erector spinae by around 32
percent in loaded and around 22 percent in unloaded conditions. Moreover, the
integrated EMG decreased by around 24.2 percent under loaded condition and
around 23.6 percent under unloaded condition. In summary, the artificial muscle
wearable device represents an anatomical solution to reduce the risk of muscle
strain, metabolic energy cost and back pain associated with repetitive lifting
tasks.
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02320" title="Abstract">arXiv:2402.02320</a> [<a href="/pdf/2402.02320" title="Download PDF">pdf</a>, <a href="/format/2402.02320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spin: An Efficient Secure Computation Framework with GPU Acceleration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wuxuan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xiangjun Song</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+S">Shenbai Hong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haijun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B">Bo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yi Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Accuracy and efficiency remain challenges for multi-party computation (MPC)
frameworks. Spin is a GPU-accelerated MPC framework that supports multiple
computation parties and a dishonest majority adversarial setup. We propose
optimized protocols for non-linear functions that are critical for machine
learning, as well as several novel optimizations specific to attention that is
the fundamental unit of Transformer models, allowing Spin to perform
non-trivial CNNs training and Transformer inference without sacrificing
security. At the backend level, Spin leverages GPU, CPU, and RDMA-enabled smart
network cards for acceleration. Comprehensive evaluations demonstrate that Spin
can be up to $2\times$ faster than the state-of-the-art for deep neural network
training. For inference on a Transformer model with 18.9 million parameters,
our attention-specific optimizations enable Spin to achieve better efficiency,
less communication, and better accuracy.
</p>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02321" title="Abstract">arXiv:2402.02321</a> [<a href="/pdf/2402.02321" title="Download PDF">pdf</a>, <a href="/format/2402.02321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Active Learning for Graphs with Noisy Structures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chi%2C+H">Hongliang Chi</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+C">Cong Qi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Suhang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yao Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph Neural Networks (GNNs) have seen significant success in tasks such as
node classification, largely contingent upon the availability of sufficient
labeled nodes. Yet, the excessive cost of labeling large-scale graphs led to a
focus on active learning on graphs, which aims for effective data selection to
maximize downstream model performance. Notably, most existing methods assume
reliable graph topology, while real-world scenarios often present noisy graphs.
Given this, designing a successful active learning framework for noisy graphs
is highly needed but challenging, as selecting data for labeling and obtaining
a clean graph are two tasks naturally interdependent: selecting high-quality
data requires clean graph structure while cleaning noisy graph structure
requires sufficient labeled data. Considering the complexity mentioned above,
we propose an active learning framework, GALClean, which has been specifically
designed to adopt an iterative approach for conducting both data selection and
graph purification simultaneously with best information learned from the prior
iteration. Importantly, we summarize GALClean as an instance of the
Expectation-Maximization algorithm, which provides a theoretical understanding
of its design and mechanisms. This theory naturally leads to an enhanced
version, GALClean+. Extensive experiments have demonstrated the effectiveness
and robustness of our proposed method across various types and levels of noisy
graphs.
</p>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02322" title="Abstract">arXiv:2402.02322</a> [<a href="/pdf/2402.02322" title="Download PDF">pdf</a>, <a href="/format/2402.02322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Incremental Optimization for Best Subset Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Shaogang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+X">Xiaoning Qian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2207.02058">arXiv:2207.02058</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Best subset selection is considered the `gold standard' for many sparse
learning problems. A variety of optimization techniques have been proposed to
attack this non-smooth non-convex problem. In this paper, we investigate the
dual forms of a family of $\ell_0$-regularized problems. An efficient
primal-dual algorithm is developed based on the primal and dual problem
structures. By leveraging the dual range estimation along with the incremental
strategy, our algorithm potentially reduces redundant computation and improves
the solutions of best subset selection. Theoretical analysis and experiments on
synthetic and real-world datasets validate the efficiency and statistical
properties of the proposed solutions.
</p>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02325" title="Abstract">arXiv:2402.02325</a> [<a href="/pdf/2402.02325" title="Download PDF">pdf</a>, <a href="/format/2402.02325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Role of Momentum in Smoothing Objective Function in Implicit Graduated  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sato%2C+N">Naoki Sato</a>, 
<a href="/search/cs?searchtype=author&query=Iiduka%2C+H">Hideaki Iiduka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">While stochastic gradient descent (SGD) with momentum has fast convergence
and excellent generalizability, a theoretical explanation for this is lacking.
In this paper, we show that SGD with momentum smooths the objective function,
the degree of which is determined by the learning rate, the batch size, the
momentum factor, the variance of the stochastic gradient, and the upper bound
of the gradient norm. This theoretical finding reveals why momentum improves
generalizability and provides new insights into the role of the
hyperparameters, including momentum factor. We also present an implicit
graduated optimization algorithm that exploits the smoothing properties of SGD
with momentum and provide experimental results supporting our assertion that
SGD with momentum smooths the objective function.
</p>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02327" title="Abstract">arXiv:2402.02327</a> [<a href="/pdf/2402.02327" title="Download PDF">pdf</a>, <a href="/format/2402.02327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bootstrapping Audio-Visual Segmentation by Strengthening Audio Cues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianxiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhentao Tan</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+T">Tao Gong</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+Q">Qi Chu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Le Lu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jieping Ye</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+N">Nenghai Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">How to effectively interact audio with vision has garnered considerable
interest within the multi-modality research field. Recently, a novel
audio-visual segmentation (AVS) task has been proposed, aiming to segment the
sounding objects in video frames under the guidance of audio cues. However,
most existing AVS methods are hindered by a modality imbalance where the visual
features tend to dominate those of the audio modality, due to a unidirectional
and insufficient integration of audio cues. This imbalance skews the feature
representation towards the visual aspect, impeding the learning of joint
audio-visual representations and potentially causing segmentation inaccuracies.
To address this issue, we propose AVSAC. Our approach features a Bidirectional
Audio-Visual Decoder (BAVD) with integrated bidirectional bridges, enhancing
audio cues and fostering continuous interplay between audio and visual
modalities. This bidirectional interaction narrows the modality imbalance,
facilitating more effective learning of integrated audio-visual
representations. Additionally, we present a strategy for audio-visual
frame-wise synchrony as fine-grained guidance of BAVD. This strategy enhances
the share of auditory components in visual features, contributing to a more
balanced audio-visual representation learning. Extensive experiments show that
our method attains new benchmarks in AVS performance.
</p>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02328" title="Abstract">arXiv:2402.02328</a> [<a href="/pdf/2402.02328" title="Download PDF">pdf</a>, <a href="/format/2402.02328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-driven algorithm design using neural networks with applications to  branch-and-cut
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hongyu Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Khalife%2C+S">Sammy Khalife</a>, 
<a href="/search/cs?searchtype=author&query=Fiedorowicz%2C+B">Barbara Fiedorowicz</a>, 
<a href="/search/cs?searchtype=author&query=Basu%2C+A">Amitabh Basu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Data-driven algorithm design is a paradigm that uses statistical and machine
learning techniques to select from a class of algorithms for a computational
problem an algorithm that has the best expected performance with respect to
some (unknown) distribution on the instances of the problem. We build upon
recent work in this line of research by introducing the idea where, instead of
selecting a single algorithm that has the best performance, we allow the
possibility of selecting an algorithm based on the instance to be solved. In
particular, given a representative sample of instances, we learn a neural
network that maps an instance of the problem to the most appropriate algorithm
{\em for that instance}. We formalize this idea and derive rigorous sample
complexity bounds for this learning problem, in the spirit of recent work in
data-driven algorithm design. We then apply this approach to the problem of
making good decisions in the branch-and-cut framework for mixed-integer
optimization (e.g., which cut to add?). In other words, the neural network will
take as input a mixed-integer optimization instance and output a decision that
will result in a small branch-and-cut tree for that instance. Our computational
results provide evidence that our particular way of using neural networks for
cut selection can make a significant impact in reducing branch-and-cut tree
sizes, compared to previous data-driven approaches.
</p>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02330" title="Abstract">arXiv:2402.02330</a> [<a href="/pdf/2402.02330" title="Download PDF">pdf</a>, <a href="/format/2402.02330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhance Reasoning for Large Language Models in the Game Werewolf
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shuang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Liwen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shiwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Q">Qiang Fu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yang Wei</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Haobo Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">This paper presents an innovative framework that integrates Large Language
Models (LLMs) with an external Thinker module to enhance the reasoning
capabilities of LLM-based agents. Unlike augmenting LLMs with prompt
engineering, Thinker directly harnesses knowledge from databases and employs
various optimization techniques. The framework forms a reasoning hierarchy
where LLMs handle intuitive System-1 tasks such as natural language processing,
while the Thinker focuses on cognitive System-2 tasks that require complex
logical analysis and domain-specific knowledge. Our framework is presented
using a 9-player Werewolf game that demands dual-system reasoning. We introduce
a communication protocol between LLMs and the Thinker, and train the Thinker
using data from 18800 human sessions and reinforcement learning. Experiments
demonstrate the framework's effectiveness in deductive reasoning, speech
generation, and online game evaluation. Additionally, we fine-tune a 6B LLM to
surpass GPT4 when integrated with the Thinker. This paper also contributes the
largest dataset for social deduction games to date.
</p>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02332" title="Abstract">arXiv:2402.02332</a> [<a href="/pdf/2402.02332" title="Download PDF">pdf</a>, <a href="/format/2402.02332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minusformer: Improving Time Series Forecasting by Progressively Learning  Residuals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+D">Daojun Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haixia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+D">Dongfeng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bingzheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Minggao Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this paper, we find that ubiquitous time series (TS) forecasting models
are prone to severe overfitting. To cope with this problem, we embrace a
de-redundancy approach to progressively reinstate the intrinsic values of TS
for future intervals. Specifically, we renovate the vanilla Transformer by
reorienting the information aggregation mechanism from addition to subtraction.
Then, we incorporate an auxiliary output branch into each block of the original
model to construct a highway leading to the ultimate prediction. The output of
subsequent modules in this branch will subtract the previously learned results,
enabling the model to learn the residuals of the supervision signal, layer by
layer. This designing facilitates the learning-driven implicit progressive
decomposition of the input and output streams, empowering the model with
heightened versatility, interpretability, and resilience against overfitting.
Since all aggregations in the model are minus signs, which is called
Minusformer. Extensive experiments demonstrate the proposed method outperform
existing state-of-the-art methods, yielding an average performance improvement
of 11.9% across various datasets.
</p>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02333" title="Abstract">arXiv:2402.02333</a> [<a href="/pdf/2402.02333" title="Download PDF">pdf</a>, <a href="/format/2402.02333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Copyright Protection in Generative AI: A Technical Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jie Ren</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Han Xu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+P">Pengfei He</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yingqian Cui</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+S">Shenglai Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiankun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+H">Hongzhi Wen</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+J">Jiayuan Ding</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yi Chang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiliang Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Generative AI has witnessed rapid advancement in recent years, expanding
their capabilities to create synthesized content such as text, images, audio,
and code. The high fidelity and authenticity of contents generated by these
Deep Generative Models (DGMs) have sparked significant copyright concerns.
There have been various legal debates on how to effectively safeguard
copyrights in DGMs. This work delves into this issue by providing a
comprehensive overview of copyright protection from a technical perspective. We
examine from two distinct viewpoints: the copyrights pertaining to the source
data held by the data owners and those of the generative models maintained by
the model builders. For data copyright, we delve into methods data owners can
protect their content and DGMs can be utilized without infringing upon these
rights. For model copyright, our discussion extends to strategies for
preventing model theft and identifying outputs generated by specific models.
Finally, we highlight the limitations of existing techniques and identify areas
that remain unexplored. Furthermore, we discuss prospective directions for the
future of copyright protection, underscoring its importance for the sustainable
and ethical development of Generative AI.
</p>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02334" title="Abstract">arXiv:2402.02334</a> [<a href="/pdf/2402.02334" title="Download PDF">pdf</a>, <a href="/format/2402.02334" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Arithmetic Feature Interaction Is Necessary for Deep Tabular Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+R">Renjun Hu</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+H">Haochao Ying</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xing Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Wei Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 8 figures, to be published to AAAI2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Until recently, the question of the effective inductive bias of deep models
on tabular data has remained unanswered. This paper investigates the hypothesis
that arithmetic feature interaction is necessary for deep tabular learning. To
test this point, we create a synthetic tabular dataset with a mild feature
interaction assumption and examine a modified transformer architecture enabling
arithmetical feature interactions, referred to as AMFormer. Results show that
AMFormer outperforms strong counterparts in fine-grained tabular data modeling,
data efficiency in training, and generalization. This is attributed to its
parallel additive and multiplicative attention operators and prompt-based
optimization, which facilitate the separation of tabular samples in an extended
space with arithmetically-engineered features. Our extensive experiments on
real-world data also validate the consistent effectiveness, efficiency, and
rationale of AMFormer, suggesting it has established a strong inductive bias
for deep learning on tabular data. Code is available at
https://github.com/aigc-apps/AMFormer.
</p>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02335" title="Abstract">arXiv:2402.02335</a> [<a href="/pdf/2402.02335" title="Download PDF">pdf</a>, <a href="/ps/2402.02335" title="Download PostScript">ps</a>, <a href="/format/2402.02335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video Editing for Video Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Flanagan%2C+K">Kevin Flanagan</a>, 
<a href="/search/cs?searchtype=author&query=Fragomeni%2C+A">Adriano Fragomeni</a>, 
<a href="/search/cs?searchtype=author&query=Wray%2C+M">Michael Wray</a>, 
<a href="/search/cs?searchtype=author&query=Damen%2C+D">Dima Damen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Though pre-training vision-language models have demonstrated significant
benefits in boosting video-text retrieval performance from large-scale web
videos, fine-tuning still plays a critical role with manually annotated clips
with start and end times, which requires considerable human effort. To address
this issue, we explore an alternative cheaper source of annotations, single
timestamps, for video-text retrieval. We initialise clips from timestamps in a
heuristic way to warm up a retrieval model. Then a video clip editing method is
proposed to refine the initial rough boundaries to improve retrieval
performance. A student-teacher network is introduced for video clip editing.
The teacher model is employed to edit the clips in the training set whereas the
student model trains on the edited clips. The teacher weights are updated from
the student's after the student's performance increases. Our method is model
agnostic and applicable to any retrieval models. We conduct experiments based
on three state-of-the-art retrieval models, COOT, VideoCLIP and CLIP4Clip.
Experiments conducted on three video retrieval datasets, YouCook2, DiDeMo and
ActivityNet-Captions show that our edited clips consistently improve retrieval
performance over initial clips across all the three retrieval models.
</p>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02337" title="Abstract">arXiv:2402.02337</a> [<a href="/pdf/2402.02337" title="Download PDF">pdf</a>, <a href="/format/2402.02337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eigen Is All You Need: Efficient Lidar-Inertial Continuous-Time Odometry  with Internal Association
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Thien-Minh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xinhang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+T">Tongxing Jin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yizhuo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianping Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Shenghai Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Lihua Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this paper, we propose a continuous-time lidar-inertial odometry (CT-LIO)
system named SLICT2, which promotes two main insights. One, contrary to
conventional wisdom, CT-LIO algorithm can be optimized by linear solvers in
only a few iterations, which is more efficient than commonly used nonlinear
solvers. Two, CT-LIO benefits more from the correct association than the number
of iterations. Based on these ideas, we implement our method with a customized
solver where the feature association process is performed immediately after
each incremental step, and the solution can converge within a few iterations.
Our implementation can achieve real-time performance with a high density of
control points while yielding competitive performance in highly dynamical
motion scenarios. We demonstrate the advantages of our method by comparing with
other existing state-of-the-art CT-LIO methods. The source code will be
released for the benefit of the community.
</p>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02338" title="Abstract">arXiv:2402.02338</a> [<a href="/pdf/2402.02338" title="Download PDF">pdf</a>, <a href="/format/2402.02338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model Adaptation for Networking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Duo Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xianda Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yaqi Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Junchen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+S">Shuguang Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fangxin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Many networking tasks now employ deep learning (DL) to solve complex
prediction and system optimization problems. However, current design philosophy
of DL-based algorithms entails intensive engineering overhead due to the manual
design of deep neural networks (DNNs) for different networking tasks. Besides,
DNNs tend to achieve poor generalization performance on unseen data
distributions/environments.
<br />Motivated by the recent success of large language models (LLMs), for the
first time, this work studies the LLM adaptation for networking to explore a
more sustainable design philosophy. With the massive pre-trained knowledge and
powerful inference ability, LLM can serve as the foundation model, and is
expected to achieve "one model for all" with even better performance and
stronger generalization for various tasks. In this paper, we present NetLLM,
the first LLM adaptation framework that efficiently adapts LLMs to solve
networking problems. NetLLM addresses many practical challenges in LLM
adaptation, from how to process task-specific information with LLMs, to how to
improve the efficiency of answer generation and acquiring domain knowledge for
networking. Across three networking-related use cases - viewport prediction
(VP), adaptive bitrate streaming (ABR) and cluster job scheduling (CJS), we
showcase the effectiveness of NetLLM in LLM adaptation for networking. Results
show that the adapted LLM surpasses state-of-the-art algorithms by 10.1-36.6%
for VP, 14.5-36.6% for ABR, 6.8-41.3% for CJS, and also achieves superior
generalization performance.
</p>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02339" title="Abstract">arXiv:2402.02339</a> [<a href="/pdf/2402.02339" title="Download PDF">pdf</a>, <a href="/format/2402.02339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty-Aware Testing-Time Optimization for 3D Human Pose Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Ti Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mengyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+B">Bin Ren</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yingxuan You</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Sebe%2C+N">Nicu Sebe</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xia Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Although data-driven methods have achieved success in 3D human pose
estimation, they often suffer from domain gaps and exhibit limited
generalization. In contrast, optimization-based methods excel in fine-tuning
for specific cases but are generally inferior to data-driven methods in overall
performance. We observe that previous optimization-based methods commonly rely
on projection constraint, which only ensures alignment in 2D space, potentially
leading to the overfitting problem. To address this, we propose an
Uncertainty-Aware testing-time Optimization (UAO) framework, which keeps the
prior information of pre-trained model and alleviates the overfitting problem
using the uncertainty of joints. Specifically, during the training phase, we
design an effective 2D-to-3D network for estimating the corresponding 3D pose
while quantifying the uncertainty of each 3D joint. For optimization during
testing, the proposed optimization framework freezes the pre-trained model and
optimizes only a latent state. Projection loss is then employed to ensure the
generated poses are well aligned in 2D space for high-quality optimization.
Furthermore, we utilize the uncertainty of each joint to determine how much
each joint is allowed for optimization. The effectiveness and superiority of
the proposed framework are validated through extensive experiments on two
challenging datasets: Human3.6M and MPI-INF-3DHP. Notably, our approach
outperforms the previous best result by a large margin of 4.5% on Human3.6M.
Our source code will be open-sourced.
</p>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02340" title="Abstract">arXiv:2402.02340</a> [<a href="/pdf/2402.02340" title="Download PDF">pdf</a>, <a href="/format/2402.02340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Semantic Proxies from Visual Prompts for Parameter-Efficient  Fine-Tuning in Deep Metric Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+L">Li Ren</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+K">Kien Hua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep Metric Learning (DML) has long attracted the attention of the machine
learning community as a key objective. Existing solutions concentrate on
fine-tuning the pre-trained models on conventional image datasets. As a result
of the success of recent pre-trained models trained from larger-scale datasets,
it is challenging to adapt the model to the DML tasks in the local data domain
while retaining the previously gained knowledge. In this paper, we investigate
parameter-efficient methods for fine-tuning the pre-trained model for DML
tasks. In particular, we propose a novel and effective framework based on
learning Visual Prompts (VPT) in the pre-trained Vision Transformers (ViT).
Based on the conventional proxy-based DML paradigm, we augment the proxy by
incorporating the semantic information from the input image and the ViT, in
which we optimize the visual prompts for each class. We demonstrate that our
new approximations with semantic information are superior to representative
capabilities, thereby improving metric learning performance. We conduct
extensive experiments to demonstrate that our proposed framework is effective
and efficient by evaluating popular DML benchmarks. In particular, we
demonstrate that our fine-tuning method achieves comparable or even better
performance than recent state-of-the-art full fine-tuning works of DML while
tuning only a small percentage of total parameters.
</p>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02342" title="Abstract">arXiv:2402.02342</a> [<a href="/pdf/2402.02342" title="Download PDF">pdf</a>, <a href="/format/2402.02342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MetaOptimize: A Framework for Optimizing Step Sizes and Other  Meta-parameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharifnassab%2C+A">Arsalan Sharifnassab</a>, 
<a href="/search/cs?searchtype=author&query=Salehkaleybar%2C+S">Saber Salehkaleybar</a>, 
<a href="/search/cs?searchtype=author&query=Sutton%2C+R">Richard Sutton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper addresses the challenge of optimizing meta-parameters (i.e.,
hyperparameters) in machine learning algorithms, a critical factor influencing
training efficiency and model performance. Moving away from the computationally
expensive traditional meta-parameter search methods, we introduce MetaOptimize
framework that dynamically adjusts meta-parameters, particularly step sizes
(also known as learning rates), during training. More specifically,
MetaOptimize can wrap around any first-order optimization algorithm, tuning
step sizes on the fly to minimize a specific form of regret that accounts for
long-term effect of step sizes on training, through a discounted sum of future
losses. We also introduce low complexity variants of MetaOptimize that, in
conjunction with its adaptability to multiple optimization algorithms,
demonstrate performance competitive to those of best hand-crafted learning rate
schedules across various machine learning applications.
</p>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02344" title="Abstract">arXiv:2402.02344</a> [<a href="/pdf/2402.02344" title="Download PDF">pdf</a>, <a href="/ps/2402.02344" title="Download PostScript">ps</a>, <a href="/format/2402.02344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Secure mmWave RSMA Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+H">Hongjiang Lei</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Sha Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ansari%2C+I+S">Imran Shafique Ansari</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yun Li</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+G">Gaofeng Pan</a>, 
<a href="/search/cs?searchtype=author&query=Alouini%2C+M">Mohamed-Slim Alouini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages,8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">This work considers a multiple-input-single-output mmWave RSMA system wherein
a base station serves two users in the presence of a passive eavesdropper.
Different eavesdropping scenarios are considered corresponding to the
overlapped resolvable paths between the main and the wiretap channels under the
considered transmission schemes. The analytical expressions for the secrecy
outage probability are derived respectively through the Gaussian Chebyshev
quadrature method. Monte Carlo simulation results are presented to validate the
correctness of the derived analytical expressions and demonstrate the effects
of system parameters on the SOP of the considered mmWave RSMA systems.
</p>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02345" title="Abstract">arXiv:2402.02345</a> [<a href="/pdf/2402.02345" title="Download PDF">pdf</a>, <a href="/ps/2402.02345" title="Download PostScript">ps</a>, <a href="/format/2402.02345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stereographic Spherical Sliced Wasserstein Distances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tran%2C+H">Huy Tran</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yikun Bai</a>, 
<a href="/search/cs?searchtype=author&query=Kothapalli%2C+A">Abihith Kothapalli</a>, 
<a href="/search/cs?searchtype=author&query=Shahbazi%2C+A">Ashkan Shahbazi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinran Liu</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+R+D">Rocio Diaz Martin</a>, 
<a href="/search/cs?searchtype=author&query=Kolouri%2C+S">Soheil Kolouri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">Comparing spherical probability distributions is of great interest in various
fields, including geology, medical domains, computer vision, and deep
representation learning. The utility of optimal transport-based distances, such
as the Wasserstein distance, for comparing probability measures has spurred
active research in developing computationally efficient variations of these
distances for spherical probability measures. This paper introduces a
high-speed and highly parallelizable distance for comparing spherical measures
using the stereographic projection and the generalized Radon transform, which
we refer to as the Stereographic Spherical Sliced Wasserstein (S3W) distance.
We carefully address the distance distortion caused by the stereographic
projection and provide an extensive theoretical analysis of our proposed metric
and its rotationally invariant variation. Finally, we evaluate the performance
of the proposed metrics and compare them with recent baselines in terms of both
speed and accuracy through a wide range of numerical studies, including
gradient flows and self-supervised learning.
</p>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02346" title="Abstract">arXiv:2402.02346</a> [<a href="/pdf/2402.02346" title="Download PDF">pdf</a>, <a href="/format/2402.02346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Closed-Loop Unsupervised Representation Disentanglement with $&#x3b2;$-VAE  Distillation and Diffusion Probabilistic Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xin Jin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bohan Li</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+B">BAAO Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenyao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Ziqiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+W">Wenjun Zeng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Representation disentanglement may help AI fundamentally understand the real
world and thus benefit both discrimination and generation tasks. It currently
has at least three unresolved core issues: (i) heavy reliance on label
annotation and synthetic data -- causing poor generalization on natural
scenarios; (ii) heuristic/hand-craft disentangling constraints make it hard to
adaptively achieve an optimal training trade-off; (iii) lacking reasonable
evaluation metric, especially for the real label-free data. To address these
challenges, we propose a \textbf{C}losed-\textbf{L}oop unsupervised
representation \textbf{Dis}entanglement approach dubbed \textbf{CL-Dis}.
Specifically, we use diffusion-based autoencoder (Diff-AE) as a backbone while
resorting to $\beta$-VAE as a co-pilot to extract semantically disentangled
representations. The strong generation ability of diffusion model and the good
disentanglement ability of VAE model are complementary. To strengthen
disentangling, VAE-latent distillation and diffusion-wise feedback are
interconnected in a closed-loop system for a further mutual promotion. Then, a
self-supervised \textbf{Navigation} strategy is introduced to identify
interpretable semantic directions in the disentangled latent space. Finally, a
new metric based on content tracking is designed to evaluate the
disentanglement effect. Experiments demonstrate the superiority of CL-Dis on
applications like real image manipulation and visual analysis.
</p>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02347" title="Abstract">arXiv:2402.02347</a> [<a href="/pdf/2402.02347" title="Download PDF">pdf</a>, <a href="/format/2402.02347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fangzhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pilanci%2C+M">Mert Pilanci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">In this work we study the enhancement of Low Rank Adaptation (LoRA)
fine-tuning procedure by introducing a Riemannian preconditioner in its
optimization step. Specifically, we introduce an $r\times r$ preconditioner in
each gradient step where $r$ is the LoRA rank. This preconditioner requires a
small change to existing optimizer code and creates virtually minuscule storage
and runtime overhead. Our experimental results with both large language models
and text-to-image diffusion models show that with our preconditioner, the
convergence and reliability of SGD and AdamW can be significantly enhanced.
Moreover, the training process becomes much more robust to hyperparameter
choices such as learning rate. Theoretically, we show that fine-tuning a
two-layer ReLU network in the convex paramaterization with our preconditioner
has convergence rate independent of condition number of the data matrix. This
new Riemannian preconditioner, previously explored in classic low-rank matrix
recovery, is introduced to deep learning tasks for the first time in our work.
We release our code at
https://github.com/pilancilab/Riemannian_Preconditioned_LoRA.
</p>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02349" title="Abstract">arXiv:2402.02349</a> [<a href="/pdf/2402.02349" title="Download PDF">pdf</a>, <a href="/ps/2402.02349" title="Download PostScript">ps</a>, <a href="/format/2402.02349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision Transformer-based Multimodal Feature Fusion Network for Lymphoma  Segmentation on PET/CT Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Huan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+L">Liheng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shenmiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Longxi Li</a>, 
<a href="/search/cs?searchtype=author&query=Nan%2C+J">Jiaofen Nan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yanting Li</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Chuang Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+F">Fubao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Weihua Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures; reference added
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Background: Diffuse large B-cell lymphoma (DLBCL) segmentation is a challenge
in medical image analysis. Traditional segmentation methods for lymphoma
struggle with the complex patterns and the presence of DLBCL lesions.
Objective: We aim to develop an accurate method for lymphoma segmentation with
18F-Fluorodeoxyglucose positron emission tomography (PET) and computed
tomography (CT) images. Methods: Our lymphoma segmentation approach combines a
vision transformer with dual encoders, adeptly fusing PET and CT data via
multimodal cross-attention fusion (MMCAF) module. In this study, PET and CT
data from 165 DLBCL patients were analyzed. A 5-fold cross-validation was
employed to evaluate the performance and generalization ability of our method.
Ground truths were annotated by experienced nuclear medicine experts. We
calculated the total metabolic tumor volume (TMTV) and performed a statistical
analysis on our results. Results: The proposed method exhibited accurate
performance in DLBCL lesion segmentation, achieving a Dice similarity
coefficient of 0.9173$\pm$0.0071, a Hausdorff distance of 2.71$\pm$0.25mm, a
sensitivity of 0.9462$\pm$0.0223, and a specificity of 0.9986$\pm$0.0008.
Additionally, a Pearson correlation coefficient of 0.9030$\pm$0.0179 and an
R-square of 0.8586$\pm$0.0173 were observed in TMTV when measured on manual
annotation compared to our segmentation results. Conclusion: This study
highlights the advantages of MMCAF and vision transformer for lymphoma
segmentation using PET and CT, offering great promise for computer-aided
lymphoma diagnosis and treatment.
</p>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02350" title="Abstract">arXiv:2402.02350</a> [<a href="/pdf/2402.02350" title="Download PDF">pdf</a>, <a href="/format/2402.02350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interference-Aware Emergent Random Access Protocol for Downlink LEO  Satellite Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lim%2C+C">Chang-Yong Lim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jihong Park</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jinho Choi</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Ju-Hyung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+D">Daesub Oh</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Heewook Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 pages, 4 figures, 1 table; submitted to IEEE for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this article, we propose a multi-agent deep reinforcement learning (MADRL)
framework to train a multiple access protocol for downlink low earth orbit
(LEO) satellite networks. By improving the existing learned protocol, emergent
random access channel (eRACH), our proposed method, coined centralized and
compressed emergent signaling for eRACH (Ce2RACH), can mitigate inter-satellite
interference by exchanging additional signaling messages jointly learned
through the MADRL training process. Simulations demonstrate that Ce2RACH
achieves up to 36.65% higher network throughput compared to eRACH, while the
cost of signaling messages increase linearly with the number of users.
</p>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02352" title="Abstract">arXiv:2402.02352</a> [<a href="/pdf/2402.02352" title="Download PDF">pdf</a>, <a href="/format/2402.02352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Region-Based Representations Revisited
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shlapentokh-Rothman%2C+M">Michal Shlapentokh-Rothman</a>, 
<a href="/search/cs?searchtype=author&query=Blume%2C+A">Ansel Blume</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuqun Wu</a>, 
<a href="/search/cs?searchtype=author&query=V%2C+S+T">Sethuraman T V</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+H">Heyi Tao</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+Y">Jae Yong Lee</a>, 
<a href="/search/cs?searchtype=author&query=Torres%2C+W">Wilfredo Torres</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu-Xiong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hoiem%2C+D">Derek Hoiem</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We investigate whether region-based representations are effective for
recognition. Regions were once a mainstay in recognition approaches, but pixel
and patch-based features are now used almost exclusively. We show that recent
class-agnostic segmenters like SAM can be effectively combined with strong
unsupervised representations like DINOv2 and used for a wide variety of tasks,
including semantic segmentation, object-based image retrieval, and multi-image
analysis. Once the masks and features are extracted, these representations,
even with linear decoders, enable competitive performance, making them well
suited to applications that require custom queries. The compactness of the
representation also makes it well-suited to video analysis and other problems
requiring inference across many images.
</p>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02354" title="Abstract">arXiv:2402.02354</a> [<a href="/pdf/2402.02354" title="Download PDF">pdf</a>, <a href="/ps/2402.02354" title="Download PostScript">ps</a>, <a href="/format/2402.02354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Paradigm for Potential Model Performance Improvement in Classification  and Regression Problems. A Proof of Concept
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lobo-Cabrera%2C+F+J">Francisco Javier Lobo-Cabrera</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">A methodology that seeks to enhance model prediction performance is
presented. The method involves generating multiple auxiliary models that
capture relationships between attributes as a function of each other. Such
information serves to generate additional informative columns in the dataset
that can potentially enhance target prediction. A proof of case and related
code is provided.
</p>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02355" title="Abstract">arXiv:2402.02355</a> [<a href="/pdf/2402.02355" title="Download PDF">pdf</a>, <a href="/format/2402.02355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symbol: Generating Flexible Black-Box Optimizers through Symbolic  Equation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiacheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zeyuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Hongshu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yining Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yue-jiao Gong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accpted as a conference paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Recent Meta-learning for Black-Box Optimization (MetaBBO) methods harness
neural networks to meta-learn configurations of traditional black-box
optimizers. Despite their success, they are inevitably restricted by the
limitations of predefined hand-crafted optimizers. In this paper, we present
\textsc{Symbol}, a novel framework that promotes the automated discovery of
black-box optimizers through symbolic equation learning. Specifically, we
propose a Symbolic Equation Generator (SEG) that allows closed-form
optimization rules to be dynamically generated for specific tasks and
optimization steps. Within \textsc{Symbol}, we then develop three distinct
strategies based on reinforcement learning, so as to meta-learn the SEG
efficiently. Extensive experiments reveal that the optimizers generated by
\textsc{Symbol} not only surpass the state-of-the-art BBO and MetaBBO
baselines, but also exhibit exceptional zero-shot generalization abilities
across entirely unseen tasks with different problem dimensions, population
sizes, and optimization horizons. Furthermore, we conduct in-depth analyses of
our \textsc{Symbol} framework and the optimization rules that it generates,
underscoring its desirable flexibility and interpretability.
</p>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02357" title="Abstract">arXiv:2402.02357</a> [<a href="/pdf/2402.02357" title="Download PDF">pdf</a>, <a href="/format/2402.02357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-modal Causal Structure Learning and Root Cause Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Lecheng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhengzhang Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jingrui He</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haifeng Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the Web Conference 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Effective root cause analysis (RCA) is vital for swiftly restoring services,
minimizing losses, and ensuring the smooth operation and management of complex
systems. Previous data-driven RCA methods, particularly those employing causal
discovery techniques, have primarily focused on constructing dependency or
causal graphs for backtracking the root causes. However, these methods often
fall short as they rely solely on data from a single modality, thereby
resulting in suboptimal solutions. In this work, we propose Mulan, a unified
multi-modal causal structure learning method for root cause localization. We
leverage a log-tailored language model to facilitate log representation
learning, converting log sequences into time-series data. To explore intricate
relationships across different modalities, we propose a contrastive
learning-based approach to extract modality-invariant and modality-specific
representations within a shared latent space. Additionally, we introduce a
novel key performance indicator-aware attention mechanism for assessing
modality reliability and co-learning a final causal graph. Finally, we employ
random walk with restart to simulate system fault propagation and identify
potential root causes. Extensive experiments on three real-world datasets
validate the effectiveness of our proposed framework.
</p>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02358" title="Abstract">arXiv:2402.02358</a> [<a href="/pdf/2402.02358" title="Download PDF">pdf</a>, <a href="/ps/2402.02358" title="Download PostScript">ps</a>, <a href="/format/2402.02358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Repairing Reed-Solomon Codes over Prime Fields via Exponential Sums
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Con%2C+R">Roni Con</a>, 
<a href="/search/cs?searchtype=author&query=Shutty%2C+N">Noah Shutty</a>, 
<a href="/search/cs?searchtype=author&query=Tamo%2C+I">Itzhak Tamo</a>, 
<a href="/search/cs?searchtype=author&query=Wootters%2C+M">Mary Wootters</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">This paper presents two repair schemes for low-rate Reed-Solomon (RS) codes
over prime fields that can repair any node by downloading a constant number of
bits from each surviving node. The total bandwidth resulting from these schemes
is greater than that incurred during trivial repair; however, this is
particularly relevant in the context of leakage-resilient secret sharing. In
that framework, our results provide attacks showing that $k$-out-of-$n$
Shamir's Secret Sharing over prime fields for small $k$ is not
leakage-resilient, even when the parties leak only a constant number of bits.
To the best of our knowledge, these are the first such attacks.
<br />Our results are derived from a novel connection between exponential sums and
the repair of RS codes. Specifically, we establish that non-trivial bounds on
certain exponential sums imply the existence of explicit nonlinear repair
schemes for RS codes over prime fields.
</p>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02361" title="Abstract">arXiv:2402.02361</a> [<a href="/pdf/2402.02361" title="Download PDF">pdf</a>, <a href="/format/2402.02361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pruner: An Efficient Cross-Platform Tensor Compiler with Dual Awareness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiao%2C+L">Liang Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jun Shi</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+X">Xiaoyu Hao</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+X">Xi Fang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+M">Minfan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Ziqi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junshi Chen</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+H">Hong An</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bing Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Honghui Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Tensor program optimization on Deep Learning Accelerators (DLAs) is critical
for efficient model deployment. Although search-based Deep Learning Compilers
(DLCs) have achieved significant performance gains compared to manual methods,
they still suffer from the persistent challenges of low search efficiency and
poor cross-platform adaptability. In this paper, we propose $\textbf{Pruner}$,
following hardware/software co-design principles to hierarchically boost tensor
program optimization. Pruner comprises two primary components: a Parameterized
Static Analyzer ($\textbf{PSA}$) and a Pattern-aware Cost Model
($\textbf{PaCM}$). The former serves as a hardware-aware and formulaic
performance analysis tool, guiding the pruning of the search space, while the
latter enables the performance prediction of tensor programs according to the
critical data-flow patterns. Furthermore, to ensure effective cross-platform
adaptation, we design a Momentum Transfer Learning ($\textbf{MTL}$) strategy
using a Siamese network, which establishes a bidirectional feedback mechanism
to improve the robustness of the pre-trained cost model. The extensive
experimental results demonstrate the effectiveness and advancement of the
proposed Pruner in various tensor program tuning tasks across both online and
offline scenarios, with low resource overhead. The code is available at
https://github.com/qiaolian9/Pruner.
</p>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02362" title="Abstract">arXiv:2402.02362</a> [<a href="/pdf/2402.02362" title="Download PDF">pdf</a>, <a href="/format/2402.02362" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unification of Symmetries Inside Neural Networks: Transformer,  Feedforward and Neural ODE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+K">Koji Hashimoto</a>, 
<a href="/search/cs?searchtype=author&query=Hirono%2C+Y">Yuji Hirono</a>, 
<a href="/search/cs?searchtype=author&query=Sannai%2C+A">Akiyoshi Sannai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); High Energy Physics - Theory (hep-th); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">Understanding the inner workings of neural networks, including transformers,
remains one of the most challenging puzzles in machine learning. This study
introduces a novel approach by applying the principles of gauge symmetries, a
key concept in physics, to neural network architectures. By regarding model
functions as physical observables, we find that parametric redundancies of
various machine learning models can be interpreted as gauge symmetries. We
mathematically formulate the parametric redundancies in neural ODEs, and find
that their gauge symmetries are given by spacetime diffeomorphisms, which play
a fundamental role in Einstein's theory of gravity. Viewing neural ODEs as a
continuum version of feedforward neural networks, we show that the parametric
redundancies in feedforward neural networks are indeed lifted to
diffeomorphisms in neural ODEs. We further extend our analysis to transformer
models, finding natural correspondences with neural ODEs and their gauge
symmetries. The concept of gauge symmetries sheds light on the complex behavior
of deep learning models through physics and provides us with a unifying
perspective for analyzing various machine learning architectures.
</p>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02364" title="Abstract">arXiv:2402.02364</a> [<a href="/pdf/2402.02364" title="Download PDF">pdf</a>, <a href="/format/2402.02364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Developmental Landscape of In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoogland%2C+J">Jesse Hoogland</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">George Wang</a>, 
<a href="/search/cs?searchtype=author&query=Farrugia-Roberts%2C+M">Matthew Farrugia-Roberts</a>, 
<a href="/search/cs?searchtype=author&query=Carroll%2C+L">Liam Carroll</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+S">Susan Wei</a>, 
<a href="/search/cs?searchtype=author&query=Murfet%2C+D">Daniel Murfet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We show that in-context learning emerges in transformers in discrete
developmental stages, when they are trained on either language modeling or
linear regression tasks. We introduce two methods for detecting the milestones
that separate these stages, by probing the geometry of the population loss in
both parameter space and function space. We study the stages revealed by these
new methods using a range of behavioral and structural metrics to establish
their validity.
</p>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02366" title="Abstract">arXiv:2402.02366</a> [<a href="/pdf/2402.02366" title="Download PDF">pdf</a>, <a href="/format/2402.02366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transolver: A Fast Transformer Solver for PDEs on General Geometries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haixu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Huakun Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haowen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianmin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+M">Mingsheng Long</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Transformers have empowered many milestones across various fields and have
recently been applied to solve partial differential equations (PDEs). However,
since PDEs are typically discretized into large-scale meshes with complex
geometries, it is challenging for Transformers to capture intricate physical
correlations directly from massive individual points. Going beyond superficial
and unwieldy meshes, we present Transolver based on a more foundational idea,
which is learning intrinsic physical states hidden behind discretized
geometries. Specifically, we propose a new Physics-Attention to adaptively
split the discretized domain into a series of learnable slices of flexible
shapes, where mesh points under similar physical states will be ascribed to the
same slice. By calculating attention to physics-aware tokens encoded from
slices, Transovler can effectively capture intricate physical correlations
under complex geometrics, which also empowers the solver with endogenetic
geometry-general modeling capacity and can be efficiently computed in linear
complexity. Transolver achieves consistent state-of-the-art with 22\% relative
gain across six standard benchmarks and also excels in large-scale industrial
simulations, including car and airfoil designs.
</p>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02367" title="Abstract">arXiv:2402.02367</a> [<a href="/pdf/2402.02367" title="Download PDF">pdf</a>, <a href="/ps/2402.02367" title="Download PostScript">ps</a>, <a href="/format/2402.02367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Intrinsic Properties of Medical Images for Self-Supervised  Binary Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+P">Pranav Singh</a>, 
<a href="/search/cs?searchtype=author&query=Cirrone%2C+J">Jacopo Cirrone</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 12 figures, and 10 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advancements in self-supervised learning have unlocked the potential
to harness unlabeled data for auxiliary tasks, facilitating the learning of
beneficial priors. This has been particularly advantageous in fields like
medical image analysis, where labeled data are scarce. Although effective for
classification tasks, this methodology has shown limitations in more complex
applications, such as medical image segmentation. In this paper, we introduce
Medical imaging Enhanced with Dynamic Self-Adaptive Semantic Segmentation
(MedSASS), a dedicated self-supervised framework tailored for medical image
segmentation. We evaluate MedSASS against existing state-of-the-art methods
across four diverse medical datasets, showcasing its superiority. MedSASS
outperforms existing CNN-based self-supervised methods by 3.83% and matches the
performance of ViT-based methods. Furthermore, when MedSASS is trained
end-to-end, covering both encoder and decoder, it demonstrates significant
improvements of 14.4% for CNNs and 6% for ViT-based architectures compared to
existing state-of-the-art self-supervised strategies.
</p>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02368" title="Abstract">arXiv:2402.02368</a> [<a href="/pdf/2402.02368" title="Download PDF">pdf</a>, <a href="/format/2402.02368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Timer: Transformers for Time Series Analysis at Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haoran Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiangdong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianmin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+M">Mingsheng Long</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Deep learning has contributed remarkably to the advancement of time series
analysis. Still, deep models can encounter performance bottlenecks in
real-world small-sample scenarios, which can be concealed due to the
performance saturation with small models on current benchmarks. Meanwhile,
large models have demonstrated great powers in these scenarios through
large-scale pre-training. Continuous progresses have been achieved as the
emergence of large language models, exhibiting unprecedented ability in
few-shot generalization, scalability, and task generality, which is however
absent in time series models. To change the current practices of training small
models on specific datasets from scratch, this paper aims at an early
development of large time series models (LTSM). During pre-training, we curate
large-scale datasets with up to 1 billion time points, unify heterogeneous time
series into single-series sequence (S3) format, and develop the GPT-style
architecture toward LTSMs. To meet diverse application needs, we convert
forecasting, imputation, and anomaly detection of time series into a unified
generative task. The outcome of this study is a Time Series Transformer
(Timer), that is pre-trained by autoregressive next token prediction on large
multi-domain datasets, and is fine-tuned to downstream scenarios with promising
abilities as an LTSM.
</p>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02369" title="Abstract">arXiv:2402.02369</a> [<a href="/pdf/2402.02369" title="Download PDF">pdf</a>, <a href="/format/2402.02369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> M$^3$Face: A Unified Multi-Modal Multilingual Framework for Human Face  Generation and Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mofayezi%2C+M">Mohammadreza Mofayezi</a>, 
<a href="/search/cs?searchtype=author&query=Alipour%2C+R">Reza Alipour</a>, 
<a href="/search/cs?searchtype=author&query=Kakavand%2C+M+A">Mohammad Ali Kakavand</a>, 
<a href="/search/cs?searchtype=author&query=Asgari%2C+E">Ehsaneddin Asgari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Multimedia (cs.MM)

</div>
<p class="mathjax">Human face generation and editing represent an essential task in the era of
computer vision and the digital world. Recent studies have shown remarkable
progress in multi-modal face generation and editing, for instance, using face
segmentation to guide image generation. However, it may be challenging for some
users to create these conditioning modalities manually. Thus, we introduce
M3Face, a unified multi-modal multilingual framework for controllable face
generation and editing. This framework enables users to utilize only text input
to generate controlling modalities automatically, for instance, semantic
segmentation or facial landmarks, and subsequently generate face images. We
conduct extensive qualitative and quantitative experiments to showcase our
frameworks face generation and editing capabilities. Additionally, we propose
the M3CelebA Dataset, a large-scale multi-modal and multilingual face dataset
containing high-quality images, semantic segmentations, facial landmarks, and
different captions for each image in multiple languages. The code and the
dataset will be released upon publication.
</p>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02370" title="Abstract">arXiv:2402.02370</a> [<a href="/pdf/2402.02370" title="Download PDF">pdf</a>, <a href="/format/2402.02370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoTimes: Autoregressive Time Series Forecasters via Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+G">Guo Qin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiangdong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianmin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+M">Mingsheng Long</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Foundation models of time series have not been fully developed due to the
limited availability of large-scale time series and the underexploration of
scalable pre-training. Based on the similar sequential structure of time series
and natural language, increasing research demonstrates the feasibility of
leveraging large language models (LLM) for time series. Nevertheless, prior
methods may overlook the consistency in aligning time series and natural
language, resulting in insufficient utilization of the LLM potentials. To fully
exploit the general-purpose token transitions learned from language modeling,
we propose AutoTimes to repurpose LLMs as Autoregressive Time series
forecasters, which is consistent with the acquisition and utilization of LLMs
without updating the parameters. The consequent forecasters can handle flexible
series lengths and achieve competitive performance as prevalent models.
Further, we present token-wise prompting that utilizes corresponding timestamps
to make our method applicable to multimodal scenarios. Analysis demonstrates
our forecasters inherit zero-shot and in-context learning capabilities of LLMs.
Empirically, AutoTimes exhibits notable method generality and achieves enhanced
performance by basing on larger LLMs, additional texts, or time series as
instructions.
</p>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02374" title="Abstract">arXiv:2402.02374</a> [<a href="/pdf/2402.02374" title="Download PDF">pdf</a>, <a href="/format/2402.02374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PromptRR: Diffusion Models as Prompt Generators for Single Image  Reflection Removal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Wanglong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaihao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+W">Wenhan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Tae-Kyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+T">Tong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongdong Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming-Hsuan Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Existing single image reflection removal (SIRR) methods using deep learning
tend to miss key low-frequency (LF) and high-frequency (HF) differences in
images, affecting their effectiveness in removing reflections. To address this
problem, this paper proposes a novel prompt-guided reflection removal
(PromptRR) framework that uses frequency information as new visual prompts for
better reflection performance. Specifically, the proposed framework decouples
the reflection removal process into the prompt generation and subsequent
prompt-guided restoration. For the prompt generation, we first propose a prompt
pre-training strategy to train a frequency prompt encoder that encodes the
ground-truth image into LF and HF prompts. Then, we adopt diffusion models
(DMs) as prompt generators to generate the LF and HF prompts estimated by the
pre-trained frequency prompt encoder. For the prompt-guided restoration, we
integrate specially generated prompts into the PromptFormer network, employing
a novel Transformer-based prompt block to effectively steer the model toward
enhanced reflection removal. The results on commonly used benchmarks show that
our method outperforms state-of-the-art approaches. The codes and models are
available at https://github.com/TaoWangzj/PromptRR.
</p>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02377" title="Abstract">arXiv:2402.02377</a> [<a href="/pdf/2402.02377" title="Download PDF">pdf</a>, <a href="/format/2402.02377" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NOAH: Learning Pairwise Object Category Attentions for Image  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Aojun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+A">Anbang Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This research work was completed in 2023. Code and pre-trained models are available at <a href="https://github.com/OSVAI/NOAH">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">A modern deep neural network (DNN) for image classification tasks typically
consists of two parts: a backbone for feature extraction, and a head for
feature encoding and class predication. We observe that the head structures of
mainstream DNNs adopt a similar feature encoding pipeline, exploiting global
feature dependencies while disregarding local ones. In this paper, we revisit
the feature encoding problem, and propose Non-glObal Attentive Head (NOAH) that
relies on a new form of dot-product attention called pairwise object category
attention (POCA), efficiently exploiting spatially dense category-specific
attentions to augment classification performance. NOAH introduces a neat
combination of feature split, transform and merge operations to learn POCAs at
local to global scales. As a drop-in design, NOAH can be easily used to replace
existing heads of various types of DNNs, improving classification performance
while maintaining similar model efficiency. We validate the effectiveness of
NOAH on ImageNet classification benchmark with 25 DNN architectures spanning
convolutional neural networks, vision transformers and multi-layer perceptrons.
In general, NOAH is able to significantly improve the performance of
lightweight DNNs, e.g., showing 3.14\%|5.3\%|1.9\% top-1 accuracy improvement
to MobileNetV2 (0.5x)|Deit-Tiny (0.5x)|gMLP-Tiny (0.5x). NOAH also generalizes
well when applied to medium-size and large-size DNNs. We further show that NOAH
retains its efficacy on other popular multi-class and multi-label image
classification benchmarks as well as in different training regimes, e.g.,
showing 3.6\%|1.1\% mAP improvement to large ResNet101|ViT-Large on MS-COCO
dataset. Project page: https://github.com/OSVAI/NOAH.
</p>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02379" title="Abstract">arXiv:2402.02379</a> [<a href="/pdf/2402.02379" title="Download PDF">pdf</a>, <a href="/format/2402.02379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking the Evaluation of Pre-trained Text-and-Layout Models from an  Entity-Centric Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yixi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+C">Chenshu Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Y">Yi Tu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Ya Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recently developed pre-trained text-and-layout models (PTLMs) have shown
remarkable success in multiple information extraction tasks on visually-rich
documents. However, the prevailing evaluation pipeline may not be sufficiently
robust for assessing the information extraction ability of PTLMs, due to
inadequate annotations within the benchmarks. Therefore, we claim the necessary
standards for an ideal benchmark to evaluate the information extraction ability
of PTLMs. We then introduce EC-FUNSD, an entity-centric benckmark designed for
the evaluation of semantic entity recognition and entity linking on
visually-rich documents. This dataset contains diverse formats of document
layouts and annotations of semantic-driven entities and their relations.
Moreover, this dataset disentangles the falsely coupled annotation of segment
and entity that arises from the block-level annotation of FUNSD. Experiment
results demonstrate that state-of-the-art PTLMs exhibit overfitting tendencies
on the prevailing benchmarks, as their performance sharply decrease when the
dataset bias is removed.
</p>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02380" title="Abstract">arXiv:2402.02380</a> [<a href="/pdf/2402.02380" title="Download PDF">pdf</a>, <a href="/ps/2402.02380" title="Download PostScript">ps</a>, <a href="/format/2402.02380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Large Language Models in Analysing Classroom Dialogue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+Y">Yun Long</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haifeng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">This study explores the application of Large Language Models (LLMs),
specifically GPT-4, in the analysis of classroom dialogue, a crucial research
task for both teaching diagnosis and quality improvement. Recognizing the
knowledge-intensive and labor-intensive nature of traditional qualitative
methods in educational research, this study investigates the potential of LLM
to streamline and enhance the analysis process. The study involves datasets
from a middle school, encompassing classroom dialogues across mathematics and
Chinese classes. These dialogues were manually coded by educational experts and
then analyzed using a customised GPT-4 model. This study focuses on comparing
manual annotations with the outputs of GPT-4 to evaluate its efficacy in
analyzing educational dialogues. Time efficiency, inter-coder agreement, and
inter-coder reliability between human coders and GPT-4 are evaluated. Results
indicate substantial time savings with GPT-4, and a high degree of consistency
in coding between the model and human coders, with some discrepancies in
specific codes. These findings highlight the strong potential of LLM in
teaching evaluation and facilitation.
</p>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02381" title="Abstract">arXiv:2402.02381</a> [<a href="/pdf/2402.02381" title="Download PDF">pdf</a>, <a href="/format/2402.02381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empowering Computing and Networks Convergence System with Distributed  Cooperative Routing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yujiao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Q">Qingmin Jia</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+M">Meng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+R">Renchao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F+R">F.Richard Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submit to IEEE Network
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The emergence of intelligent applications and recent advances in the fields
of computing and networks are driving the development of computing and networks
convergence (CNC) system. However, existing researches failed to achieve
comprehensive scheduling optimization of computing and network resources. This
shortfall results in some requirements of computing requests unable to be
guaranteed in an end-to-end service pattern, negatively impacting the
development of CNC systems. In this article, we propose a distributed
cooperative routing framework for the CNC system to ensure the deadline
requirements and minimize the computation cost of requests. The framework
includes trading plane, management plane, control plane and forwarding plane.
The cross-plane cooperative end-to-end routing schemes consider both
computation efficiency of heterogeneous servers and the network congestion
degrees while making routing plan, thereby determining where to execute
requests and corresponding routing paths. Simulations results substantiates the
performance of our routing schemes in scheduling computing requests in the CNC
system.
</p>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02382" title="Abstract">arXiv:2402.02382</a> [<a href="/pdf/2402.02382" title="Download PDF">pdf</a>, <a href="/format/2402.02382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting the Power of Prompt for Visual Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuzhu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Lechao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+C">Chaowei Fang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dingwen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+M">Manni Duan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Meng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Visual prompt tuning (VPT) is a promising solution incorporating learnable
prompt tokens to customize pre-trained models for downstream tasks. However,
VPT and its variants often encounter challenges like prompt initialization,
prompt length, and subpar performance in self-supervised pretraining, hindering
successful contextual adaptation. This study commences by exploring the
correlation evolvement between prompts and patch tokens during proficient
training. Inspired by the observation that the prompt tokens tend to share high
mutual information with patch tokens, we propose initializing prompts with
downstream token prototypes. The strategic initialization, a stand-in for the
previous initialization, substantially improves performance in fine-tuning. To
refine further, we optimize token construction with a streamlined pipeline that
maintains excellent performance with almost no increase in computational
expenses compared to VPT. Exhaustive experiments show our proposed approach
outperforms existing methods by a remarkable margin. For instance, it surpasses
full fine-tuning in 19 out of 24 tasks, using less than 0.4% of learnable
parameters on the FGVC and VTAB-1K benchmarks. Notably, our method
significantly advances the adaptation for self-supervised pretraining,
achieving impressive task performance gains of at least 10% to 30%. Besides,
the experimental results demonstrate the proposed SPT is robust to prompt
lengths and scales well with model capacity and training data size. We finally
provide an insightful exploration into the amount of target data facilitating
the adaptation of pre-trained models to downstream tasks.
</p>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02385" title="Abstract">arXiv:2402.02385</a> [<a href="/pdf/2402.02385" title="Download PDF">pdf</a>, <a href="/format/2402.02385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Robotics with Foundation Models: toward Embodied AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhiyuan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Kun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Junjie Wen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinming Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Ning Liu</a>, 
<a href="/search/cs?searchtype=author&query=Che%2C+Z">Zhengping Che</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jian Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">While the exploration for embodied AI has spanned multiple decades, it
remains a persistent challenge to endow agents with human-level intelligence,
including perception, learning, reasoning, decision-making, control, and
generalization capabilities, so that they can perform general-purpose tasks in
open, unstructured, and dynamic environments. Recent advances in computer
vision, natural language processing, and multi-modality learning have shown
that the foundation models have superhuman capabilities for specific tasks.
They not only provide a solid cornerstone for integrating basic modules into
embodied AI systems but also shed light on how to scale up robot learning from
a methodological perspective. This survey aims to provide a comprehensive and
up-to-date overview of foundation models in robotics, focusing on autonomous
manipulation and encompassing high-level planning and low-level control.
Moreover, we showcase their commonly used datasets, simulators, and benchmarks.
Importantly, we emphasize the critical challenges intrinsic to this field and
delineate potential avenues for future research, contributing to advancing the
frontier of academic and industrial discourse.
</p>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02387" title="Abstract">arXiv:2402.02387</a> [<a href="/pdf/2402.02387" title="Download PDF">pdf</a>, <a href="/format/2402.02387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Brain-Body-Task Co-Adaptation can Improve Autonomous Learning and Speed  of Bipedal Walking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Urbina-Mel%C3%A9ndez%2C+D">Dar&#xed;o Urbina-Mel&#xe9;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Azadjou%2C+H">Hesam Azadjou</a>, 
<a href="/search/cs?searchtype=author&query=Valero-Cuevas%2C+F+J">Francisco J. Valero-Cuevas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Inspired by animals that co-adapt their brain and body to interact with the
environment, we present a tendon-driven and over-actuated (i.e., n joint, n+1
actuators) bipedal robot that (i) exploits its backdrivable mechanical
properties to manage body-environment interactions without explicit control,
and (ii) uses a simple 3-layer neural network to learn to walk after only 2
minutes of 'natural' motor babbling (i.e., an exploration strategy that is
compatible with leg and task dynamics; akin to childsplay). This brain-body
collaboration first learns to produce feet cyclical movements 'in air' and,
without further tuning, can produce locomotion when the biped is lowered to be
in slight contact with the ground. In contrast, training with 2 minutes of
'naive' motor babbling (i.e., an exploration strategy that ignores leg task
dynamics), does not produce consistent cyclical movements 'in air', and
produces erratic movements and no locomotion when in slight contact with the
ground. When further lowering the biped and making the desired leg trajectories
reach 1cm below ground (causing the desired-vs-obtained trajectories error to
be unavoidable), cyclical movements based on either natural or naive babbling
presented almost equally persistent trends, and locomotion emerged with naive
babbling. Therefore, we show how continual learning of walking in unforeseen
circumstances can be driven by continual physical adaptation rooted in the
backdrivable properties of the plant and enhanced by exploration strategies
that exploit plant dynamics. Our studies also demonstrate that the bio-inspired
codesign and co-adaptations of limbs and control strategies can produce
locomotion without explicit control of trajectory errors.
</p>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02388" title="Abstract">arXiv:2402.02388</a> [<a href="/pdf/2402.02388" title="Download PDF">pdf</a>, <a href="/format/2402.02388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solution-oriented Agent-based Models Generation with Verifier-assisted  Iterative In-context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niu%2C+T">Tong Niu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weihao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Rong Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Software Engineering (cs.SE)

</div>
<p class="mathjax">Agent-based models (ABMs) stand as an essential paradigm for proposing and
validating hypothetical solutions or policies aimed at addressing challenges
posed by complex systems and achieving various objectives. This process demands
labor-intensive endeavors and multidisciplinary expertise. Large language
models (LLMs) encapsulating cross-domain knowledge and programming proficiency
could potentially alleviate the difficulty of this process. However, LLMs excel
in handling sequential information, making it challenging for analyzing the
intricate interactions and nonlinear dynamics inherent in ABMs. Additionally,
due to the lack of self-evaluation capability of LLMs, relying solely on LLMs
is insufficient to effectively accomplish this process. In this paper, we
present SAGE, a general solution-oriented ABM generation framework designed for
automatic modeling and generating solutions for targeted problems. Unlike
approaches reliant on expert handcrafting or resource-intensive neural network
training, SAGE establishes a verifier-assisted iterative in-context learning
process employing large language models (LLMs) to leverages their inherent
cross-domain knowledge for tackling intricate demands from diverse domain
scenarios. In SAGE, we introduce an semi-structured conceptual representation
expliciting the intricate structures of ABMs and an objective representation to
guide LLMs in modeling scenarios and proposing hypothetical solutions through
in-context learning. To ensure the model executability and solution
feasibility, SAGE devises a two-level verifier with chain-of-thought prompting
tailored to the complex interactions and non-linear dynamics of ABMs, driving
the iterative generation optimization. Moreover, we construct an evaluation
dataset of solution-oriented ABMs from open sources.It contains practical
models across various domains.
</p>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02389" title="Abstract">arXiv:2402.02389</a> [<a href="/pdf/2402.02389" title="Download PDF">pdf</a>, <a href="/format/2402.02389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KICGPT: Large Language Model with Knowledge in Context for Knowledge  Graph Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yanbin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qiushi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Kwok%2C+J+T">James T. Kwok</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Knowledge Graph Completion (KGC) is crucial for addressing knowledge graph
incompleteness and supporting downstream applications. Many models have been
proposed for KGC. They can be categorized into two main classes: triple-based
and text-based approaches. Triple-based methods struggle with long-tail
entities due to limited structural information and imbalanced entity
distributions. Text-based methods alleviate this issue but require costly
training for language models and specific finetuning for knowledge graphs,
which limits their efficiency. To alleviate these limitations, in this paper,
we propose KICGPT, a framework that integrates a large language model (LLM) and
a triple-based KGC retriever. It alleviates the long-tail problem without
incurring additional training overhead. KICGPT uses an in-context learning
strategy called Knowledge Prompt, which encodes structural knowledge into
demonstrations to guide the LLM. Empirical results on benchmark datasets
demonstrate the effectiveness of KICGPT with smaller training overhead and no
finetuning.
</p>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02390" title="Abstract">arXiv:2402.02390</a> [<a href="/pdf/2402.02390" title="Download PDF">pdf</a>, <a href="/ps/2402.02390" title="Download PostScript">ps</a>, <a href="/format/2402.02390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Upper Bound for the Size of a Trifferent Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhandari%2C+S">Siddharth Bhandari</a>, 
<a href="/search/cs?searchtype=author&query=Khetan%2C+A">Abhishek Khetan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
<p class="mathjax">A subset $\mathcal{C}\subseteq\{0,1,2\}^n$ is said to be a
$\textit{trifferent}$ code (of block length $n$) if for every three distinct
codewords $x,y, z \in \mathcal{C}$, there is a coordinate $i\in
\{1,2,\ldots,n\}$ where they all differ, that is, $\{x(i),y(i),z(i)\}$ is same
as $\{0,1,2\}$. Let $T(n)$ denote the size of the largest trifferent code of
block length $n$. Understanding the asymptotic behavior of $T(n)$ is closely
related to determining the zero-error capacity of the $(3/2)$-channel defined
by Elias'88, and is a long-standing open problem in the area. Elias had shown
that $T(n)\leq 2\times (3/2)^n$ and prior to our work the best upper bound was
$T(n)\leq 0.6937 \times (3/2)^n$ due to Kurz'23. We improve this bound to
$T(n)\leq c \times n^{-2/5}\times (3/2)^n$ where $c$ is an absolute constant.
</p>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02392" title="Abstract">arXiv:2402.02392</a> [<a href="/pdf/2402.02392" title="Download PDF">pdf</a>, <a href="/format/2402.02392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeLLMa: A Framework for Decision Making Under Uncertainty with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+O">Ollie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+D">Deqing Fu</a>, 
<a href="/search/cs?searchtype=author&query=Yogatama%2C+D">Dani Yogatama</a>, 
<a href="/search/cs?searchtype=author&query=Neiswanger%2C+W">Willie Neiswanger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) are increasingly used across society, including
in domains like business, engineering, and medicine. These fields often grapple
with decision-making under uncertainty, a critical yet challenging task. In
this paper, we show that directly prompting LLMs on these types of
decision-making problems yields poor results, especially as the problem
complexity increases. To overcome this limitation, we propose DeLLMa
(Decision-making Large Language Model assistant), a framework designed to
enhance decision-making accuracy in uncertain environments. DeLLMa involves a
multi-step scaffolding procedure, drawing upon principles from decision theory
and utility theory, to provide an optimal and human-auditable decision-making
process. We validate our framework on decision-making environments involving
real agriculture and finance data. Our results show that DeLLMa can
significantly improve LLM decision-making performance, achieving up to a 40%
increase in accuracy over competing methods.
</p>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02395" title="Abstract">arXiv:2402.02395</a> [<a href="/pdf/2402.02395" title="Download PDF">pdf</a>, <a href="/format/2402.02395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A fast and gridless ORKA algorithm for tracking moving and deforming  objects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bossmann%2C+F">Florian Bossmann</a>, 
<a href="/search/math?searchtype=author&query=Ma%2C+J">Jianwei Ma</a>, 
<a href="/search/math?searchtype=author&query=wu%2C+W">Wenze wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Identifying objects in given data is a task frequently encountered in many
applications. Finding vehicles or persons in video data, tracking seismic waves
in geophysical exploration data, or predicting a storm front movement from
meteorological measurements are only some of the possible applications. In many
cases, the object of interest changes its form or position from one measurement
to another. For example, vehicles in a video may change its position or angle
to the camera in each frame. Seismic waves can change its arrival time,
frequency, or intensity depending on the sensor position. Storm fronts can
change its form and position over time. This complicates the identification and
tracking as the algorithm needs to deal with the changing object over the given
measurements. In a previous work, the authors presented a new algorithm to
solve this problem - Object reconstruction using K-approximation (ORKA). The
algorithm can solve the problem at hand but suffers from two disadvantages. On
the one hand, the reconstructed object movement is bound to a grid that depends
on the data resolution. On the other hand, the complexity of the algorithm
increases exponentially with the resolution. We overcome both disadvantages by
introducing an iterative strategy that uses a resampling method to create
multiple resolutions of the data. In each iteration the resolution is increased
to reconstruct more details of the object of interest. This way, we can even go
beyond the original resolution by artificially upsampling the data. We give
error bounds and a complexity analysis of the new method. Furthermore, we
analyze its performance in several numerical experiments as well as on real
data. We also give a brief introduction on the original ORKA algorithm.
Knowledge of the previous work is thus not required.
</p>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02396" title="Abstract">arXiv:2402.02396</a> [<a href="/pdf/2402.02396" title="Download PDF">pdf</a>, <a href="/ps/2402.02396" title="Download PostScript">ps</a>, <a href="/format/2402.02396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Selected aspects of tractability analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kritzer%2C+P">Peter Kritzer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We give an overview of certain aspects of tractability analysis of
multivariate problems. This paper is not intended to give a complete account of
the subject, but provides an insight into how the theory works for particular
types of problems, with a special focus on the more recent developments
regarding exponential and generalized tractability. We illustrate the
theoretical results by several examples throughout the article.
</p>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02399" title="Abstract">arXiv:2402.02399</a> [<a href="/pdf/2402.02399" title="Download PDF">pdf</a>, <a href="/format/2402.02399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FreDF: Learning to Forecast in Frequency Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Licheng Pan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhichao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Degui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yifei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinggao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Applications (stat.AP); Machine Learning (stat.ML)

</div>
<p class="mathjax">Time series modeling is uniquely challenged by the presence of
autocorrelation in both historical and label sequences. Current research
predominantly focuses on handling autocorrelation within the historical
sequence but often neglects its presence in the label sequence. Specifically,
emerging forecast models mainly conform to the direct forecast (DF) paradigm,
generating multi-step forecasts under the assumption of conditional
independence within the label sequence. This assumption disregards the inherent
autocorrelation in the label sequence, thereby limiting the performance of
DF-based models. In response to this gap, we introduce the Frequency-enhanced
Direct Forecast (FreDF), which bypasses the complexity of label autocorrelation
by learning to forecast in the frequency domain. Our experiments demonstrate
that FreDF substantially outperforms existing state-of-the-art methods
including iTransformer and is compatible with a variety of forecast models.
</p>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02401" title="Abstract">arXiv:2402.02401</a> [<a href="/pdf/2402.02401" title="Download PDF">pdf</a>, <a href="/format/2402.02401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI-Generated Content Enhanced Computer-Aided Diagnosis Model for Thyroid  Nodules: A ChatGPT-Style Assistant
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jincao Yao</a> (1 and 2 and 3 and 4 and 5 and 6), 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunpeng Wang</a> (7), 
<a href="/search/cs?searchtype=author&query=Lei%2C+Z">Zhikai Lei</a> (8), 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kai Wang</a> (9), 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoxian Li</a> (10)
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jianhua Zhou</a> (10), 
<a href="/search/cs?searchtype=author&query=Hao%2C+X">Xiang Hao</a> (7), 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jiafei Shen</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhenping Wang</a> (9), 
<a href="/search/cs?searchtype=author&query=Ru%2C+R">Rongrong Ru</a> (11), 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yaqing Chen</a> (11), 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yahan Zhou</a> (6), 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanming Zhang</a> (12 and 13), 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Ping Liang</a> (14), 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Dong Xu</a> (1 and 2 and 3 and 4 and 5 and 6) ((1) Department of Radiology, Zhejiang Cancer Hospital, Hangzhou, 310022, China (2) Hangzhou Institute of Medicine (HIM), Chinese Academy of Sciences, Hangzhou, 310000, China,(3) Key Laboratory of Head and Neck Cancer Translational Research of Zhejiang Province, Hangzhou, 310022, China,(4) Zhejiang Provincial Research Center for Cancer Intelligent Diagnosis and Molecular Technology, Hangzhou, 310000, China, (5) Wenling Medical Big Data and Artificial Intelligence Research Institute, 24th Floor, Machang Road, Taizhou, 310061, China,(6) Taizhou Key Laboratory of Minimally Invasive Interventional Therapy and Artificial Intelligence, Taizhou Campus of Zhejiang Cancer Hospital (Taizhou Cancer Hospital), Taizhou, 317502, China,(7) College of Optical Science and Engineering, Zhejiang University, No.38 of Zheda Road, Hangzhou, Zhejiang Province, China,(8) Zhejiang Provincial Hospital of Chinese Medicine, 54 Youdian Road, Hangzhou, 310003, China,(9) Department of Ultrasound, The Affiliated Dongyang Hospital of Wenzhou Medical University, Dongyang, 322100, China,(10) Department of Ultrasound, Sun Yat sen University Cancer Center, State Key Laboratory of Oncology in South China, Collaborative Innovation Center for Cancer Medicine, Guangzhou, 510060, China, (11) Affiliated Xiaoshan Hospital, Hangzhou Normal University, No.728 North Yucai Road, Hangzhou, 311202, China,(12) Zhejiang Provincial People&#x27;s Hospital Affiliated People&#x27;s Hospital, Hangzhou Medical College, Hangzhou, 314408, China,(13) Key Laboratory of Endocrine Gland Diseases of Zhejiang Province, Hangzhou, 314408, China,(14) Department of Ultrasound, Chinese PLA General Hospital, Chinese PLA Medical School, Beijing, 100853, China)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">An artificial intelligence-generated content-enhanced computer-aided
diagnosis (AIGC-CAD) model, designated as ThyGPT, has been developed. This
model, inspired by the architecture of ChatGPT, could assist radiologists in
assessing the risk of thyroid nodules through semantic-level human-machine
interaction. A dataset comprising 19,165 thyroid nodule ultrasound cases from
Zhejiang Cancer Hospital was assembled to facilitate the training and
validation of the model. After training, ThyGPT could automatically evaluate
thyroid nodule and engage in effective communication with physicians through
human-computer interaction. The performance of ThyGPT was rigorously quantified
using established metrics such as the receiver operating characteristic (ROC)
curve, area under the curve (AUC), sensitivity, and specificity. The empirical
findings revealed that radiologists, when supplemented with ThyGPT, markedly
surpassed the diagnostic acumen of their peers utilizing traditional methods as
well as the performance of the model in isolation. These findings suggest that
AIGC-CAD systems, exemplified by ThyGPT, hold the promise to fundamentally
transform the diagnostic workflows of radiologists in forthcoming years.
</p>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02405" title="Abstract">arXiv:2402.02405</a> [<a href="/pdf/2402.02405" title="Download PDF">pdf</a>, <a href="/format/2402.02405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Angle Robustness Unmanned Aerial Vehicle Navigation in GNSS-Denied  Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuxin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zunlei Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haofei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+J">Jie Lei</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Li Sun</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+M">Mingli Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Due to the inability to receive signals from the Global Navigation Satellite
System (GNSS) in extreme conditions, achieving accurate and robust navigation
for Unmanned Aerial Vehicles (UAVs) is a challenging task. Recently emerged,
vision-based navigation has been a promising and feasible alternative to
GNSS-based navigation. However, existing vision-based techniques are inadequate
in addressing flight deviation caused by environmental disturbances and
inaccurate position predictions in practical settings. In this paper, we
present a novel angle robustness navigation paradigm to deal with flight
deviation in point-to-point navigation tasks. Additionally, we propose a model
that includes the Adaptive Feature Enhance Module, Cross-knowledge
Attention-guided Module and Robust Task-oriented Head Module to accurately
predict direction angles for high-precision navigation. To evaluate the
vision-based navigation methods, we collect a new dataset termed as UAV_AR368.
Furthermore, we design the Simulation Flight Testing Instrument (SFTI) using
Google Earth to simulate different flight environments, thereby reducing the
expenses associated with real flight testing. Experiment results demonstrate
that the proposed model outperforms the state-of-the-art by achieving
improvements of 26.0% and 45.6% in the success rate of arrival under ideal and
disturbed circumstances, respectively.
</p>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02407" title="Abstract">arXiv:2402.02407</a> [<a href="/pdf/2402.02407" title="Download PDF">pdf</a>, <a href="/format/2402.02407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Defining Neural Network Architecture through Polytope Structures of  Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sangmin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Mammadov%2C+A">Abbas Mammadov</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J+C">Jong Chul Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Current theoretical and empirical research in neural networks suggests that
complex datasets require large network architectures for thorough
classification, yet the precise nature of this relationship remains unclear.
This paper tackles this issue by defining upper and lower bounds for neural
network widths, which are informed by the polytope structure of the dataset in
question. We also delve into the application of these principles to simplicial
complexes and specific manifold shapes, explaining how the requirement for
network width varies in accordance with the geometric complexity of the
dataset. Moreover, we develop an algorithm to investigate a converse situation
where the polytope structure of a dataset can be inferred from its
corresponding trained neural networks. Through our algorithm, it is established
that popular datasets such as MNIST, Fashion-MNIST, and CIFAR10 can be
efficiently encapsulated using no more than two polytopes with a small number
of faces.
</p>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02408" title="Abstract">arXiv:2402.02408</a> [<a href="/pdf/2402.02408" title="Download PDF">pdf</a>, <a href="/format/2402.02408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large  Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuanchang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuosheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hai Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Despite the rapid progress of large language models (LLMs), their task
performance remains sensitive to prompt design. Recent studies have explored
leveraging the LLM itself as an optimizer to identify optimal prompts that
maximize task accuracy. However, when evaluating prompts, such approaches
heavily rely on elusive manually annotated gold labels to calculate task
accuracy for each candidate prompt, which hinders the widespread implementation
and generality. To overcome the limitation, this work proposes a gold
label-agnostic prompt evaluation (GLaPE) to alleviate dependence on gold
labels. Motivated by the observed correlation between self-consistency and the
accuracy of the answer, we adopt self-consistency as the initial evaluation
score. Subsequently, we refine the scores of prompts producing identical
answers to be mutually consistent. Experimental results show that GLaPE
provides reliable evaluations uniform with accuracy, even in the absence of
gold labels. Moreover, on six popular reasoning tasks, our GLaPE-based prompt
optimization yields effective prompts comparable to accuracy-based ones. The
code is publicly available at https://github.com/thunderous77/GLaPE.
</p>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02411" title="Abstract">arXiv:2402.02411</a> [<a href="/pdf/2402.02411" title="Download PDF">pdf</a>, <a href="/format/2402.02411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-Inspired Degradation Models for Hyperspectral Image Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lian%2C+J">Jie Lian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lizhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Dian%2C+R">Renwei Dian</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zhiwei Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hua Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The fusion of a low-spatial-resolution hyperspectral image (LR-HSI) with a
high-spatial-resolution multispectral image (HR-MSI) has garnered increasing
research interest. However, most fusion methods solely focus on the fusion
algorithm itself and overlook the degradation models, which results in
unsatisfactory performance in practical scenarios. To fill this gap, we propose
physics-inspired degradation models (PIDM) to model the degradation of LR-HSI
and HR-MSI, which comprises a spatial degradation network (SpaDN) and a
spectral degradation network (SpeDN). SpaDN and SpeDN are designed based on two
insights. First, we employ spatial warping and spectral modulation operations
to simulate lens aberrations, thereby introducing non-uniformity into the
spatial and spectral degradation processes. Second, we utilize asymmetric
downsampling and parallel downsampling operations to separately reduce the
spatial and spectral resolutions of the images, thus ensuring the matching of
spatial and spectral degradation processes with specific physical
characteristics. Once SpaDN and SpeDN are established, we adopt a
self-supervised training strategy to optimize the network parameters and
provide a plug-and-play solution for fusion methods. Comprehensive experiments
demonstrate that our proposed PIDM can boost the fusion performance of existing
fusion methods in practical scenarios.
</p>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02412" title="Abstract">arXiv:2402.02412</a> [<a href="/pdf/2402.02412" title="Download PDF">pdf</a>, <a href="/format/2402.02412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Approximation Schemes for Stabbing Rectilinear Polygons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+A">Arindam Khan</a>, 
<a href="/search/cs?searchtype=author&query=Subramanian%2C+A">Aditya Subramanian</a>, 
<a href="/search/cs?searchtype=author&query=Widmann%2C+T">Tobias Widmann</a>, 
<a href="/search/cs?searchtype=author&query=Wiese%2C+A">Andreas Wiese</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">We study the problem of stabbing rectilinear polygons, where we are given $n$
rectilinear polygons in the plane that we want to stab, i.e., we want to select
horizontal line segments such that for each given rectilinear polygon there is
a line segment that intersects two opposite (parallel) edges of it. Our goal is
to find a set of line segments of minimum total length such that all polygons
are stabbed. For the special case of rectangles, there is a
$O(1)$-approximation algorithm and the problem is $\mathsf{NP}$-hard [Chan et
al.]. Also, the problem admits a QPTAS [Eisenbrand et al.] and even a PTAS
[Khan et al.]. However, the approximability for the setting of more general
polygons, e.g., L-shapes or T-shapes, is completely open.
<br />In this paper, we characterize the conditions under which the problem admits
a $(1+\varepsilon)$-approximation algorithm. We assume that each input polygon
is composed of rectangles that are placed on top of each other such that, for
each pair of adjacent edges between rectangles, one edge contains the other. We
show that if all input polygons satisfy the hourglass condition, then the
problem admits a QPTAS. In particular, it is thus unlikely that this case is
$\mathsf{APX}$-hard. Furthermore, we show that there exists a PTAS if each
input polygon is composed out of rectangles with a bounded range of widths. On
the other hand, if the input polygons do not satisfy these conditions, we prove
that the problem is $\mathsf{APX}$-hard, already if all input polygons have
only eight edges. We remark that all polygons with fewer edges automatically
satisfy the hourglass condition. On the other hand, for arbitrary rectilinear
polygons we even show a lower bound of $\Omega(\log n)$ for the possible
approximation ratio, which implies that the best possible ratio is in
$\Theta(\log n)$ since the problem is a special case of Set Cover.
</p>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02414" title="Abstract">arXiv:2402.02414</a> [<a href="/pdf/2402.02414" title="Download PDF">pdf</a>, <a href="/format/2402.02414" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Navigate Biopsy with Ultrasound under Augmented Reality Device: Towards  Higher System Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haowei Li</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+W">Wenqing Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jiasheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Y">Yuqi Ji</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+L">Long Qian</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+H">Hui Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhe Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guangzhi Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Purpose: Biopsies play a crucial role in determining the classification and
staging of tumors. Ultrasound is frequently used in this procedure to provide
real-time anatomical information. Using augmented reality (AR), surgeons can
visualize ultrasound data and spatial navigation information seamlessly
integrated with real tissues. This innovation facilitates faster and more
precise biopsy operations. Methods: We developed an AR biopsy navigation system
with low display latency and high accuracy. Ultrasound data is initially read
by an image capture card and streamed to Unity via net communication. In Unity,
navigation information is rendered and transmitted to the HoloLens 2 device
using holographic remoting. Retro-reflective tool tracking is implemented on
the HoloLens 2, enabling simultaneous tracking of the ultrasound probe and
biopsy needle. Distinct navigation information is provided during in-plane and
out-of-plane punctuation. To evaluate the effectiveness of our system, we
conducted a study involving ten participants, for puncture accuracy and biopsy
time, comparing to traditional methods. Results: Our proposed framework enables
ultrasound visualization in AR with only $16.22\pm11.45ms$ additional latency.
Navigation accuracy reached $1.23\pm 0.68mm$ in the image plane and $0.95\pm
0.70mm$ outside the image plane. Remarkably, the utilization of our system led
to $98\%$ and $95\%$ success rate in out-of-plane and in-plane biopsy.
Conclusion: To sum up, this paper introduces an AR-based ultrasound biopsy
navigation system characterized by high navigation accuracy and minimal
latency. The system provides distinct visualization contents during in-plane
and out-of-plane operations according to their different characteristics. Use
case study in this paper proved that our system can help young surgeons perform
biopsy faster and more accurately.
</p>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02416" title="Abstract">arXiv:2402.02416</a> [<a href="/pdf/2402.02416" title="Download PDF">pdf</a>, <a href="/format/2402.02416" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jiaming Ji</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Boyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+H">Hantao Lou</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+D">Donghai Hong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Borong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xuehai Pan</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+J">Juntao Dai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Efforts to align Large Language Models (LLMs) are mainly conducted via
Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF
encounters major challenges including training reward models, actor-critic
engineering, and importantly, it requires access to LLM parameters. Here we
introduce Aligner, a new efficient alignment paradigm that bypasses the whole
RLHF process by learning the correctional residuals between the aligned and the
unaligned answers. Our Aligner offers several key advantages. Firstly, it is an
autoregressive seq2seq model that is trained on the query-answer-correction
dataset via supervised learning; this offers a parameter-efficient alignment
solution with minimal resources. Secondly, the Aligner facilitates
weak-to-strong generalization; finetuning large pretrained models by Aligner's
supervisory signals demonstrates strong performance boost. Thirdly, Aligner
functions as a model-agnostic plug-and-play module, allowing for its direct
application on different open-source and API-based models. Remarkably,
Aligner-7B improves 11 different LLMs by 18% in helpfulness and 23% in
harmlessness on average (GPT-4 by 26.9% and 17.5%). When finetuning (strong)
Llama2-70B with (weak) Aligner-7B's supervision, we can improve Llama2 by 8.2%
in helpfulness and 61.6% in harmlessness. See our dataset and code at
\url{https://aligner2024.github.io}.
</p>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02418" title="Abstract">arXiv:2402.02418</a> [<a href="/pdf/2402.02418" title="Download PDF">pdf</a>, <a href="/format/2402.02418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> eXplainable Bayesian Multi-Perspective Generative Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+E">EuiYul Song</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+P">Philhoon Oh</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sangryul Kim</a>, 
<a href="/search/cs?searchtype=author&query=Thorne%2C+J">James Thorne</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Modern deterministic retrieval pipelines prioritize achieving
state-of-the-art performance but often lack interpretability in
decision-making. These models face challenges in assessing uncertainty, leading
to overconfident predictions. To overcome these limitations, we integrate
uncertainty calibration and interpretability into a retrieval pipeline.
Specifically, we introduce Bayesian methodologies and multi-perspective
retrieval to calibrate uncertainty within a retrieval pipeline. We incorporate
techniques such as LIME and SHAP to analyze the behavior of a black-box
reranker model. The importance scores derived from these explanation
methodologies serve as supplementary relevance scores to enhance the base
reranker model. We evaluate the resulting performance enhancements achieved
through uncertainty calibration and interpretable reranking on Question
Answering and Fact Checking tasks. Our methods demonstrate substantial
performance improvements across three KILT datasets.
</p>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02420" title="Abstract">arXiv:2402.02420</a> [<a href="/pdf/2402.02420" title="Download PDF">pdf</a>, <a href="/format/2402.02420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Factuality of Large Language Models in the Year 2024
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuxia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Minghan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Manzoor%2C+M+A">Muhammad Arslan Manzoor</a>, 
<a href="/search/cs?searchtype=author&query=Georgiev%2C+G">Georgi Georgiev</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+R+J">Rocktim Jyoti Das</a>, 
<a href="/search/cs?searchtype=author&query=Nakov%2C+P">Preslav Nakov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 1 figure and 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs), especially when instruction-tuned for chat,
have become part of our daily lives, freeing people from the process of
searching, extracting, and integrating information from multiple sources by
offering a straightforward answer to a variety of questions in a single place.
Unfortunately, in many cases, LLM responses are factually incorrect, which
limits their applicability in real-world scenarios. As a result, research on
evaluating and improving the factuality of LLMs has attracted a lot of research
attention recently. In this survey, we critically analyze existing work with
the aim to identify the major challenges and their associated causes, pointing
out to potential solutions for improving the factuality of LLMs, and analyzing
the obstacles to automated factuality evaluation for open-ended text
generation. We further offer an outlook on where future research should go.
</p>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02421" title="Abstract">arXiv:2402.02421</a> [<a href="/pdf/2402.02421" title="Download PDF">pdf</a>, <a href="/format/2402.02421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Phase field cohesive zone modeling for fatigue crack propagation in  quasi-brittle materials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baktheer%2C+A">A. Baktheer</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez-Pa%C3%B1eda%2C+E">E. Mart&#xed;nez-Pa&#xf1;eda</a>, 
<a href="/search/cs?searchtype=author&query=Aldakheel%2C+F">F. Aldakheel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Materials Science (cond-mat.mtrl-sci); Applied Physics (physics.app-ph)

</div>
<p class="mathjax">The phase field method has gathered significant attention in the past decade
due to its versatile applications in engineering contexts, including fatigue
crack propagation modeling. Particularly, the phase field cohesive zone method
(PF-CZM) has emerged as a promising approach for modeling fracture behavior in
quasi-brittle materials, such as concrete. The present contribution expands the
applicability of the PF-CZM to include the modeling of fatigue-induced crack
propagation. This study critically examines the validity of the extended PF-CZM
approach by evaluating its performance across various fatigue behaviours,
encompassing hysteretic behavior, S-N curves, fatigue creep curves, and the
Paris law. The experimental investigations and validation span a diverse
spectrum of loading scenarios, encompassing pre- and post-peak cyclic loading,
as well as low- and high-cyclic fatigue loading. The validation process
incorporates 2D and 3D boundary value problems, considering mode I and
mixed-modes fatigue crack propagation. The results obtained from this study
show a wide range of validity, underscoring the remarkable potential of the
proposed PF-CZM approach to accurately capture the propagation of fatigue
cracks in concrete-like materials. Furthermore, the paper outlines
recommendations to improve the predictive capabilities of the model concerning
key fatigue characteristics.
</p>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02423" title="Abstract">arXiv:2402.02423</a> [<a href="/pdf/2402.02423" title="Download PDF">pdf</a>, <a href="/format/2402.02423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement  Learning with Diverse Human Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yifu Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+J">Jianye Hao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zibin Dong</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+H">Hebin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zhixin Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+K">Kai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yan Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICLR 2024. The website is available at <a href="https://uni-rlhf.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">Reinforcement Learning with Human Feedback (RLHF) has received significant
attention for performing tasks without the need for costly manual reward design
by aligning human preferences. It is crucial to consider diverse human feedback
types and various learning methods in different environments. However,
quantifying progress in RLHF with diverse feedback is challenging due to the
lack of standardized annotation platforms and widely used unified benchmarks.
To bridge this gap, we introduce Uni-RLHF, a comprehensive system
implementation tailored for RLHF. It aims to provide a complete workflow from
real human feedback, fostering progress in the development of practical
problems. Uni-RLHF contains three packages: 1) a universal multi-feedback
annotation platform, 2) large-scale crowdsourced feedback datasets, and 3)
modular offline RLHF baseline implementations. Uni-RLHF develops a
user-friendly annotation interface tailored to various feedback types,
compatible with a wide range of mainstream RL environments. We then establish a
systematic pipeline of crowdsourced annotations, resulting in large-scale
annotated datasets comprising more than 15 million steps across 30+ popular
tasks. Through extensive experiments, the results in the collected datasets
demonstrate competitive performance compared to those from well-designed manual
rewards. We evaluate various design choices and offer insights into their
strengths and potential areas of improvement. We wish to build valuable
open-source platforms, datasets, and baselines to facilitate the development of
more robust and reliable RLHF solutions based on realistic human feedback. The
website is available at https://uni-rlhf.github.io/.
</p>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02425" title="Abstract">arXiv:2402.02425</a> [<a href="/pdf/2402.02425" title="Download PDF">pdf</a>, <a href="/format/2402.02425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EuLagNet: Eulerian Fluid Prediction with Lagrangian Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Q">Qilong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haixu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+L">Lanxiang Xing</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianmin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+M">Mingsheng Long</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">Accurately predicting the future fluid is important to extensive areas, such
as meteorology, oceanology and aerodynamics. However, since the fluid is
usually observed from an Eulerian perspective, its active and intricate
dynamics are seriously obscured and confounded in static grids, bringing horny
challenges to the prediction. This paper introduces a new Lagrangian-guided
paradigm to tackle the tanglesome fluid dynamics. Instead of solely predicting
the future based on Eulerian observations, we propose the Eulerian-Lagrangian
Dual Recurrent Network (EuLagNet), which captures multiscale fluid dynamics by
tracking movements of adaptively sampled key particles on multiple scales and
integrating dynamics information over time. Concretely, a EuLag Block is
presented to communicate the learned Eulerian and Lagrangian features at each
moment and scale, where the motion of tracked particles is inferred from
Eulerian observations and their accumulated dynamics information is
incorporated into Eulerian fields to guide future prediction. Tracking key
particles not only provides a clear and interpretable clue for fluid dynamics
but also makes our model free from modeling complex correlations among massive
grids for better efficiency. Experimentally, EuLagNet excels in three
challenging fluid prediction tasks, covering both 2D and 3D, simulated and
real-world fluids.
</p>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02426" title="Abstract">arXiv:2402.02426</a> [<a href="/pdf/2402.02426" title="Download PDF">pdf</a>, <a href="/format/2402.02426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid-Prediction Integrated Planning for Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haochen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhiyu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenhui Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haohan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+X">Xiaoyu Mo</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+C">Chen Lv</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Autonomous driving systems require the ability to fully understand and
predict the surrounding environment to make informed decisions in complex
scenarios. Recent advancements in learning-based systems have highlighted the
importance of integrating prediction and planning modules. However, this
integration has brought forth three major challenges: inherent trade-offs by
sole prediction, consistency between prediction patterns, and social coherence
in prediction and planning. To address these challenges, we introduce a
hybrid-prediction integrated planning (HPP) system, which possesses three
novelly designed modules. First, we introduce marginal-conditioned occupancy
prediction to align joint occupancy with agent-wise perceptions. Our proposed
MS-OccFormer module achieves multi-stage alignment per occupancy forecasting
with consistent awareness from agent-wise motion predictions. Second, we
propose a game-theoretic motion predictor, GTFormer, to model the interactive
future among individual agents with their joint predictive awareness. Third,
hybrid prediction patterns are concurrently integrated with Ego Planner and
optimized by prediction guidance. HPP achieves state-of-the-art performance on
the nuScenes dataset, demonstrating superior accuracy and consistency for
end-to-end paradigms in prediction and planning. Moreover, we test the
long-term open-loop and closed-loop performance of HPP on the Waymo Open Motion
Dataset and CARLA benchmark, surpassing other integrated prediction and
planning pipelines with enhanced accuracy and compatibility.
</p>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02428" title="Abstract">arXiv:2402.02428</a> [<a href="/pdf/2402.02428" title="Download PDF">pdf</a>, <a href="/format/2402.02428" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Stability of Strategic Energy Storage Operation in Wholesale  Electricity Markets (Extended Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Navon%2C+A">Aviad Navon</a>, 
<a href="/search/eess?searchtype=author&query=Belikov%2C+J">Juri Belikov</a>, 
<a href="/search/eess?searchtype=author&query=Orda%2C+A">Ariel Orda</a>, 
<a href="/search/eess?searchtype=author&query=Levron%2C+Y">Yoash Levron</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">High shares of variable renewable energy necessitate substantial energy
storage capacity. However, it remains unclear how to design a market that, on
the one hand, ensures a stable and sufficient income for storage firms, and, on
the other hand, maintains stable and affordable electricity costs for the
consumers. Here, we use a game theoretic model to study storage competition in
wholesale electricity markets. A main result is that these types of games are
not necessarily stable. In particular, we find that under certain conditions,
which imply a combination of a high share of variable renewable energy sources
and low flexibility of conventional power plants, the system will not converge
to an equilibrium. However, we demonstrate that a price cap on storage price
bids can ensure convergence to a stable solution. Moreover, we find that when
the flexibility of conventional power plants is low, while the storage usage
for energy balancing increases with renewable energy generation, the
profitability of using storage for the sole purpose of energy arbitrage
decreases.
</p>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02429" title="Abstract">arXiv:2402.02429</a> [<a href="/pdf/2402.02429" title="Download PDF">pdf</a>, <a href="/format/2402.02429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards an Information Theoretic Framework of Context-Based Offline  Meta-Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lanqing Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Shatong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Junqiao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Heng%2C+P">Pheng-Ann Heng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 8 figures, 5 tables. TLDR: We propose a novel information theoretic framework of the context-based offline meta-RL paradigm, which unifies several mainstream methods and leads to a general and state-of-the-art algorithm called UNICORN
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">As a marriage between offline RL and meta-RL, the advent of offline
meta-reinforcement learning (OMRL) has shown great promise in enabling RL
agents to multi-task and quickly adapt while acquiring knowledge safely. Among
which, Context-based OMRL (COMRL) as a popular paradigm, aims to learn a
universal policy conditioned on effective task representations. In this work,
by examining several key milestones in the field of COMRL, we propose to
integrate these seemingly independent methodologies into a unified information
theoretic framework. Most importantly, we show that the pre-existing COMRL
algorithms are essentially optimizing the same mutual information objective
between the task variable $\boldsymbol{M}$ and its latent representation
$\boldsymbol{Z}$ by implementing various approximate bounds. Based on the
theoretical insight and the information bottleneck principle, we arrive at a
novel algorithm dubbed UNICORN, which exhibits remarkable generalization across
a broad spectrum of RL benchmarks, context shift scenarios, data qualities and
deep learning architectures, attaining the new state-of-the-art. We believe
that our framework could open up avenues for new optimality bounds and COMRL
algorithms.
</p>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02430" title="Abstract">arXiv:2402.02430</a> [<a href="/pdf/2402.02430" title="Download PDF">pdf</a>, <a href="/format/2402.02430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Low-level Representations for Ultra-Fast Road Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Huan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+F">Feng Xue</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yucong Li</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+S">Shi Gong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yiqun Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yu Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 7 figures, TEEE TITS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Achieving real-time and accuracy on embedded platforms has always been the
pursuit of road segmentation methods. To this end, they have proposed many
lightweight networks. However, they ignore the fact that roads are "stuff"
(background or environmental elements) rather than "things" (specific
identifiable objects), which inspires us to explore the feasibility of
representing roads with low-level instead of high-level features. Surprisingly,
we find that the primary stage of mainstream network models is sufficient to
represent most pixels of the road for segmentation. Motivated by this, we
propose a Low-level Feature Dominated Road Segmentation network (LFD-RoadSeg).
Specifically, LFD-RoadSeg employs a bilateral structure. The spatial detail
branch is firstly designed to extract low-level feature representation for the
road by the first stage of ResNet-18. To suppress texture-less regions mistaken
as the road in the low-level feature, the context semantic branch is then
designed to extract the context feature in a fast manner. To this end, in the
second branch, we asymmetrically downsample the input image and design an
aggregation module to achieve comparable receptive fields to the third stage of
ResNet-18 but with less time consumption. Finally, to segment the road from the
low-level feature, a selective fusion module is proposed to calculate
pixel-wise attention between the low-level representation and context feature,
and suppress the non-road low-level response by this attention. On KITTI-Road,
LFD-RoadSeg achieves a maximum F1-measure (MaxF) of 95.21% and an average
precision of 93.71%, while reaching 238 FPS on a single TITAN Xp and 54 FPS on
a Jetson TX2, all with a compact model size of just 936k parameters. The source
code is available at https://github.com/zhouhuan-hust/LFD-RoadSeg.
</p>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02431" title="Abstract">arXiv:2402.02431</a> [<a href="/pdf/2402.02431" title="Download PDF">pdf</a>, <a href="/format/2402.02431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Mutual Excitation for Hand-to-Hand and Human-to-Human  Interaction Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mengyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Songtao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+F">Fanyang Meng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hong Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recognizing interactive actions, including hand-to-hand interaction and
human-to-human interaction, has attracted increasing attention for various
applications in the field of video analysis and human-robot interaction.
Considering the success of graph convolution in modeling topology-aware
features from skeleton data, recent methods commonly operate graph convolution
on separate entities and use late fusion for interactive action recognition,
which can barely model the mutual semantic relationships between pairwise
entities. To this end, we propose a mutual excitation graph convolutional
network (me-GCN) by stacking mutual excitation graph convolution (me-GC)
layers. Specifically, me-GC uses a mutual topology excitation module to firstly
extract adjacency matrices from individual entities and then adaptively model
the mutual constraints between them. Moreover, me-GC extends the above idea and
further uses a mutual feature excitation module to extract and merge deep
features from pairwise entities. Compared with graph convolution, our proposed
me-GC gradually learns mutual information in each layer and each stage of graph
convolution operations. Extensive experiments on a challenging hand-to-hand
interaction dataset, i.e., the Assembely101 dataset, and two large-scale
human-to-human interaction datasets, i.e., NTU60-Interaction and
NTU120-Interaction consistently verify the superiority of our proposed method,
which outperforms the state-of-the-art GCN-based and Transformer-based methods.
</p>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02433" title="Abstract">arXiv:2402.02433</a> [<a href="/pdf/2402.02433" title="Download PDF">pdf</a>, <a href="/format/2402.02433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty-Aware Perceiver
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+E">EuiYul Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The Perceiver makes few architectural assumptions about the relationship
among its inputs with quadratic scalability on its memory and computation time.
Indeed, the Perceiver model outpaces or is competitive with ResNet-50 and ViT
in terms of accuracy to some degree. However, the Perceiver does not take
predictive uncertainty and calibration into account. The Perceiver also
generalizes its performance on three datasets, three models, one evaluation
metric, and one hyper-parameter setting. Worst of all, the Perceiver's relative
performance improvement against other models is marginal. Furthermore, its
reduction of architectural prior is not substantial; is not equivalent to its
quality. Thereby, I invented five mutations of the Perceiver, the
Uncertainty-Aware Perceivers, that obtain uncertainty estimates and measured
their performance on three metrics. Experimented with CIFAR-10 and CIFAR-100,
the Uncertainty-Aware Perceivers make considerable performance enhancement
compared to the Perceiver.
</p>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02437" title="Abstract">arXiv:2402.02437</a> [<a href="/pdf/2402.02437" title="Download PDF">pdf</a>, <a href="/format/2402.02437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional cooperation with longer memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Glynatsi%2C+N+E">Nikoleta E. Glynatsi</a>, 
<a href="/search/cs?searchtype=author&query=Nowak%2C+M+A">Martin A. Nowak</a>, 
<a href="/search/cs?searchtype=author&query=Hilbe%2C+C">Christian Hilbe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Direct reciprocity is a wide-spread mechanism for evolution of cooperation.
In repeated interactions, players can condition their behavior on previous
outcomes. A well known approach is given by reactive strategies, which respond
to the co-player's previous move. Here we extend reactive strategies to longer
memories. A reactive-$n$ strategy takes into account the sequence of the last
$n$ moves of the co-player. A reactive-$n$ counting strategy records how often
the co-player has cooperated during the last $n$ rounds. We derive an algorithm
to identify all partner strategies among reactive-$n$ strategies. We give
explicit conditions for all partner strategies among reactive-2, reactive-3
strategies, and reactive-$n$ counting strategies. Partner strategies are those
that ensure mutual cooperation without exploitation. We perform evolutionary
simulations and find that longer memory increases the average cooperation rate
for reactive-$n$ strategies but not for reactive counting strategies. Paying
attention to the sequence of moves is necessary for reaping the advantages of
longer memory.
</p>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02438" title="Abstract">arXiv:2402.02438</a> [<a href="/pdf/2402.02438" title="Download PDF">pdf</a>, <a href="/ps/2402.02438" title="Download PostScript">ps</a>, <a href="/format/2402.02438" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast and interpretable Support Vector Classification based on the  truncated ANOVA decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akhalaya%2C+K">Kseniya Akhalaya</a>, 
<a href="/search/cs?searchtype=author&query=Nestler%2C+F">Franziska Nestler</a>, 
<a href="/search/cs?searchtype=author&query=Potts%2C+D">Daniel Potts</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Support Vector Machines (SVMs) are an important tool for performing
classification on scattered data, where one usually has to deal with many data
points in high-dimensional spaces. We propose solving SVMs in primal form using
feature maps based on trigonometric functions or wavelets. In small dimensional
settings the Fast Fourier Transform (FFT) and related methods are a powerful
tool in order to deal with the considered basis functions. For growing
dimensions the classical FFT-based methods become inefficient due to the curse
of dimensionality. Therefore, we restrict ourselves to multivariate basis
functions, each one of them depends only on a small number of dimensions. This
is motivated by the well-known sparsity of effects and recent results regarding
the reconstruction of functions from scattered data in terms of truncated
analysis of variance (ANOVA) decomposition, which makes the resulting model
even interpretable in terms of importance of the features as well as their
couplings. The usage of small superposition dimensions has the consequence that
the computational effort no longer grows exponentially but only polynomially
with respect to the dimension. In order to enforce sparsity regarding the basis
coefficients, we use the frequently applied $\ell_2$-norm and, in addition,
$\ell_1$-norm regularization. The found classifying function, which is the
linear combination of basis functions, and its variance can then be analyzed in
terms of the classical ANOVA decomposition of functions. Based on numerical
examples we show that we are able to recover the signum of a function that
perfectly fits our model assumptions. We obtain better results with
$\ell_1$-norm regularization, both in terms of accuracy and clarity of
interpretability.
</p>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02439" title="Abstract">arXiv:2402.02439</a> [<a href="/pdf/2402.02439" title="Download PDF">pdf</a>, <a href="/format/2402.02439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based  Trajectory Stitching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guanghe Li</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+Y">Yixiang Shan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhengbang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+T">Ting Long</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weinan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In offline reinforcement learning (RL), the performance of the learned policy
highly depends on the quality of offline datasets. However, in many cases, the
offline dataset contains very limited optimal trajectories, which poses a
challenge for offline RL algorithms as agents must acquire the ability to
transit to high-reward regions. To address this issue, we introduce
Diffusion-based Trajectory Stitching (DiffStitch), a novel diffusion-based data
augmentation pipeline that systematically generates stitching transitions
between trajectories. DiffStitch effectively connects low-reward trajectories
with high-reward trajectories, forming globally optimal trajectories to address
the challenges faced by offline RL algorithms. Empirical experiments conducted
on D4RL datasets demonstrate the effectiveness of DiffStitch across RL
methodologies. Notably, DiffStitch demonstrates substantial enhancements in the
performance of one-step methods (IQL), imitation learning methods (TD3+BC), and
trajectory optimization methods (DT).
</p>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02441" title="Abstract">arXiv:2402.02441</a> [<a href="/pdf/2402.02441" title="Download PDF">pdf</a>, <a href="/format/2402.02441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TopoX: A Suite of Python Packages for Machine Learning on Topological  Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hajij%2C+M">Mustafa Hajij</a>, 
<a href="/search/cs?searchtype=author&query=Papillon%2C+M">Mathilde Papillon</a>, 
<a href="/search/cs?searchtype=author&query=Frantzen%2C+F">Florian Frantzen</a>, 
<a href="/search/cs?searchtype=author&query=Agerberg%2C+J">Jens Agerberg</a>, 
<a href="/search/cs?searchtype=author&query=AlJabea%2C+I">Ibrahem AlJabea</a>, 
<a href="/search/cs?searchtype=author&query=Ballester%2C+R">Ruben Ballester</a>, 
<a href="/search/cs?searchtype=author&query=Battiloro%2C+C">Claudio Battiloro</a>, 
<a href="/search/cs?searchtype=author&query=Bern%C3%A1rdez%2C+G">Guillermo Bern&#xe1;rdez</a>, 
<a href="/search/cs?searchtype=author&query=Birdal%2C+T">Tolga Birdal</a>, 
<a href="/search/cs?searchtype=author&query=Brent%2C+A">Aiden Brent</a>, 
<a href="/search/cs?searchtype=author&query=Chin%2C+P">Peter Chin</a>, 
<a href="/search/cs?searchtype=author&query=Escalera%2C+S">Sergio Escalera</a>, 
<a href="/search/cs?searchtype=author&query=Gardaa%2C+O+H">Odin Hoff Gardaa</a>, 
<a href="/search/cs?searchtype=author&query=Gopalakrishnan%2C+G">Gurusankar Gopalakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Govil%2C+D">Devendra Govil</a>, 
<a href="/search/cs?searchtype=author&query=Hoppe%2C+J">Josef Hoppe</a>, 
<a href="/search/cs?searchtype=author&query=Karri%2C+M+R">Maneel Reddy Karri</a>, 
<a href="/search/cs?searchtype=author&query=Khouja%2C+J">Jude Khouja</a>, 
<a href="/search/cs?searchtype=author&query=Lecha%2C+M">Manuel Lecha</a>, 
<a href="/search/cs?searchtype=author&query=Livesay%2C+N">Neal Livesay</a>, 
<a href="/search/cs?searchtype=author&query=Mei%C3%9Fner%2C+J">Jan Mei&#xdf;ner</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+S">Soham Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Nikitin%2C+A">Alexander Nikitin</a>, 
<a href="/search/cs?searchtype=author&query=Papamarkou%2C+T">Theodore Papamarkou</a>, 
<a href="/search/cs?searchtype=author&query=Pr%27%7Bi%7Dlepok%2C+J">Jaro Pr&#x27;{i}lepok</a>, 
<a href="/search/cs?searchtype=author&query=Ramamurthy%2C+K+N">Karthikeyan Natesan Ramamurthy</a>, 
<a href="/search/cs?searchtype=author&query=Rosen%2C+P">Paul Rosen</a>, 
<a href="/search/cs?searchtype=author&query=Guzm%27%7Ba%7Dn-S%27%7Ba%7Denz%2C+A">Aldo Guzm&#x27;{a}n-S&#x27;{a}enz</a>, 
<a href="/search/cs?searchtype=author&query=Salatiello%2C+A">Alessandro Salatiello</a>, 
<a href="/search/cs?searchtype=author&query=Samaga%2C+S+N">Shreyas N. Samaga</a>, 
<a href="/search/cs?searchtype=author&query=Schaub%2C+M+T">Michael T. Schaub</a>, 
<a href="/search/cs?searchtype=author&query=Scofano%2C+L">Luca Scofano</a>, 
<a href="/search/cs?searchtype=author&query=Spinelli%2C+I">Indro Spinelli</a>, 
<a href="/search/cs?searchtype=author&query=Telyatnikov%2C+L">Lev Telyatnikov</a>, 
<a href="/search/cs?searchtype=author&query=Truong%2C+Q">Quang Truong</a>, 
<a href="/search/cs?searchtype=author&query=Walters%2C+R">Robin Walters</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Maosheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zaghen%2C+O">Olga Zaghen</a>, 
<a href="/search/cs?searchtype=author&query=Zamzmi%2C+G">Ghada Zamzmi</a>, 
<a href="/search/cs?searchtype=author&query=Zia%2C+A">Ali Zia</a>, 
<a href="/search/cs?searchtype=author&query=Miolane%2C+N">Nina Miolane</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation (stat.CO)

</div>
<p class="mathjax">We introduce topox, a Python software suite that provides reliable and
user-friendly building blocks for computing and machine learning on topological
domains that extend graphs: hypergraphs, simplicial, cellular, path and
combinatorial complexes. topox consists of three packages: toponetx facilitates
constructing and computing on these domains, including working with nodes,
edges and higher-order cells; topoembedx provides methods to embed topological
domains into vector spaces, akin to popular graph-based embedding algorithms
such as node2vec; topomodelx is built on top of PyTorch and offers a
comprehensive toolbox of higher-order message passing functions for neural
networks on topological domains. The extensively documented and unit-tested
source code of topox is available under MIT license at
https://github.com/pyt-team.
</p>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02442" title="Abstract">arXiv:2402.02442</a> [<a href="/pdf/2402.02442" title="Download PDF">pdf</a>, <a href="/format/2402.02442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Momentum Accelerated Algorithm for ReLU-based Nonlinear Matrix  Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qingsong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+C">Chunfeng Cui</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+D">Deren Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Recently, there has been a growing interest in the exploration of Nonlinear
Matrix Decomposition (NMD) due to its close ties with neural networks. NMD aims
to find a low-rank matrix from a sparse nonnegative matrix with a per-element
nonlinear function. A typical choice is the Rectified Linear Unit (ReLU)
activation function. To address over-fitting in the existing ReLU-based NMD
model (ReLU-NMD), we propose a Tikhonov regularized ReLU-NMD model, referred to
as ReLU-NMD-T. Subsequently, we introduce a momentum accelerated algorithm for
handling the ReLU-NMD-T model. A distinctive feature, setting our work apart
from most existing studies, is the incorporation of both positive and negative
momentum parameters in our algorithm. Our numerical experiments on real-world
datasets show the effectiveness of the proposed model and algorithm. Moreover,
the code is available at https://github.com/nothing2wang/NMD-TM.
</p>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02444" title="Abstract">arXiv:2402.02444</a> [<a href="/pdf/2402.02444" title="Download PDF">pdf</a>, <a href="/format/2402.02444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BECLR: Batch Enhanced Contrastive Few-Shot Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Poulakakis-Daktylidis%2C+S">Stylianos Poulakakis-Daktylidis</a>, 
<a href="/search/cs?searchtype=author&query=Jamali-Rad%2C+H">Hadi Jamali-Rad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 Spotlight Presentation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Learning quickly from very few labeled samples is a fundamental attribute
that separates machines and humans in the era of deep representation learning.
Unsupervised few-shot learning (U-FSL) aspires to bridge this gap by discarding
the reliance on annotations at training time. Intrigued by the success of
contrastive learning approaches in the realm of U-FSL, we structurally approach
their shortcomings in both pretraining and downstream inference stages. We
propose a novel Dynamic Clustered mEmory (DyCE) module to promote a highly
separable latent representation space for enhancing positive sampling at the
pretraining phase and infusing implicit class-level insights into unsupervised
contrastive learning. We then tackle the, somehow overlooked yet critical,
issue of sample bias at the few-shot inference stage. We propose an iterative
Optimal Transport-based distribution Alignment (OpTA) strategy and demonstrate
that it efficiently addresses the problem, especially in low-shot scenarios
where FSL approaches suffer the most from sample bias. We later on discuss that
DyCE and OpTA are two intertwined pieces of a novel end-to-end approach (we
coin as BECLR), constructively magnifying each other's impact. We then present
a suite of extensive quantitative and qualitative experimentation to
corroborate that BECLR sets a new state-of-the-art across ALL existing U-FSL
benchmarks (to the best of our knowledge), and significantly outperforms the
best of the current baselines (codebase available at:
https://github.com/stypoumic/BECLR).
</p>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02446" title="Abstract">arXiv:2402.02446</a> [<a href="/pdf/2402.02446" title="Download PDF">pdf</a>, <a href="/format/2402.02446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LQER: Low-Rank Quantization Error Reconstruction for LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">Jianyi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Constantinides%2C+G+A">George A. Constantinides</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yiren Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Post-training quantization of Large Language Models (LLMs) is challenging. In
this work, we introduce Low-rank Quantization Error Reduction (LQER), which
combines quantization and low-rank approximation to recover the model
capability. LQER leverages an activation-induced scale matrix to drive the
singular value distribution of quantization error towards a desirable
distribution, which enables nearly-lossless W4A8 quantization on various LLMs
and downstream tasks without the need for knowledge distillation, grid search,
or gradient-base iterative optimization. Unlike existing methods, the
computation pattern of LQER eliminates the need for specialized Scatter and
Gather processes to collect high-precision weights from irregular memory
locations. Our W4A8 LLMs achieve near-lossless performance on six popular
downstream tasks, while using 1.36$\times$ fewer hardware resources than the
leading state-of-the-art method. We will open-source our framework once the
paper is accepted.
</p>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02447" title="Abstract">arXiv:2402.02447</a> [<a href="/pdf/2402.02447" title="Download PDF">pdf</a>, <a href="/format/2402.02447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking MLPerf Training: A Case Study on Optimizing BERT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yongdeok Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+J">Jaehyung Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Myeongwoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+C">Changin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Heejae Kim</a>, 
<a href="/search/cs?searchtype=author&query=Tuvshinjargal%2C+N">Narankhuu Tuvshinjargal</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seungwon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanzi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+Y">Yuan Pei</a>, 
<a href="/search/cs?searchtype=author&query=Linghu%2C+X">Xiongzhan Linghu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jingkun Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yuehua Dai</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+S">Sungjoo Yoo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Total 15 pages (Appendix 3 pages)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Speeding up the large-scale distributed training is challenging in that it
requires improving various components of training including load balancing,
communication, optimizers, etc. We present novel approaches for fast
large-scale training of BERT model which individually ameliorates each
component thereby leading to a new level of BERT training performance. Load
balancing is imperative in distributed BERT training since its training
datasets are characterized by samples with various lengths. Communication cost,
which is proportional to the scale of distributed training, needs to be hidden
by useful computation. In addition, the optimizers, e.g., ADAM, LAMB, etc.,
need to be carefully re-evaluated in the context of large-scale distributed
training. We propose two new ideas, (1) local presorting based on dataset
stratification for load balancing and (2) bucket-wise gradient clipping before
allreduce which allows us to benefit from the overlap of gradient computation
and synchronization as well as the fast training of gradient clipping before
allreduce. We also re-evaluate existing optimizers via hyperparameter
optimization and utilize ADAM, which also contributes to fast training via
larger batches than existing methods. Our proposed methods, all combined, give
the fastest MLPerf BERT training of 25.1 (22.3) seconds on 1,024 NVIDIA A100
GPUs, which is 1.33x (1.13x) and 1.57x faster than the other top two (one)
submissions to MLPerf v1.1 (v2.0). Our implementation and evaluation results
are available at MLPerf v1.1~v2.1.
</p>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02449" title="Abstract">arXiv:2402.02449</a> [<a href="/pdf/2402.02449" title="Download PDF">pdf</a>, <a href="/format/2402.02449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Surfing the modeling of PoS taggers in low-resource scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferro%2C+M+V">Manuel Vilares Ferro</a>, 
<a href="/search/cs?searchtype=author&query=Bilbao%2C+V+M+D">V&#xed;ctor M. Darriba Bilbao</a>, 
<a href="/search/cs?searchtype=author&query=Ribadas-Pena%2C+F+J">Francisco J. Ribadas-Pena</a>, 
<a href="/search/cs?searchtype=author&query=Gil%2C+J+G">Jorge Gra&#xf1;a Gil</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 papes, 5 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Mathematics 2022, 10(19), 3526
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The recent trend towards the application of deep structured techniques has
revealed the limits of huge models in natural language processing. This has
reawakened the interest in traditional machine learning algorithms, which have
proved still to be competitive in certain contexts, in particular low-resource
settings. In parallel, model selection has become an essential task to boost
performance at reasonable cost, even more so when we talk about processes
involving domains where the training and/or computational resources are scarce.
Against this backdrop, we evaluate the early estimation of learning curves as a
practical mechanism for selecting the most appropriate model in scenarios
characterized by the use of non-deep learners in resource-lean settings. On the
basis of a formal approximation model previously evaluated under conditions of
wide availability of training and validation resources, we study the
reliability of such an approach in a different and much more demanding
operationalenvironment. Using as case study the generation of PoS taggers for
Galician, a language belonging to the Western Ibero-Romance group, the
experimental results are consistent with our expectations.
</p>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02452" title="Abstract">arXiv:2402.02452</a> [<a href="/pdf/2402.02452" title="Download PDF">pdf</a>, <a href="/format/2402.02452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XAI-CF -- Examining the Role of Explainable Artificial Intelligence in  Cyber Forensics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alam%2C+S">Shahid Alam</a>, 
<a href="/search/cs?searchtype=author&query=Altiparmak%2C+Z">Zeynep Altiparmak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">With the rise of complex cyber devices Cyber Forensics (CF) is facing many
new challenges. For example, there are dozens of systems running on
smartphones, each with more than millions of downloadable applications. Sifting
through this large amount of data and making sense requires new techniques,
such as from the field of Artificial Intelligence (AI). To apply these
techniques successfully in CF, we need to justify and explain the results to
the stakeholders of CF, such as forensic analysts and members of the court, for
them to make an informed decision. If we want to apply AI successfully in CF,
there is a need to develop trust in AI systems. Some other factors in accepting
the use of AI in CF are to make AI authentic, interpretable, understandable,
and interactive. This way, AI systems will be more acceptable to the public and
ensure alignment with legal standards. An explainable AI (XAI) system can play
this role in CF, and we call such a system XAI-CF. XAI-CF is indispensable and
is still in its infancy. In this paper, we explore and make a case for the
significance and advantages of XAI-CF. We strongly emphasize the need to build
a successful and practical XAI-CF system and discuss some of the main
requirements and prerequisites of such a system. We present a formal definition
of the terms CF and XAI-CF and a comprehensive literature review of previous
works that apply and utilize XAI to build and increase trust in CF. We discuss
some challenges facing XAI-CF. We also provide some concrete solutions to these
challenges. We identify key insights and future research directions for
building XAI applications for CF. This paper is an effort to explore and
familiarize the readers with the role of XAI applications in CF, and we believe
that our work provides a promising basis for future researchers interested in
XAI-CF.
</p>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02453" title="Abstract">arXiv:2402.02453</a> [<a href="/pdf/2402.02453" title="Download PDF">pdf</a>, <a href="/format/2402.02453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI Art Neural Constellation: Revealing the Collective and Contrastive  State of AI-Generated and Human Art
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+F+F">Faizan Farooq Khan</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Diana Kim</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+D">Divyansh Jha</a>, 
<a href="/search/cs?searchtype=author&query=Mohamed%2C+Y">Youssef Mohamed</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+H+H">Hanna H Chang</a>, 
<a href="/search/cs?searchtype=author&query=Elgammal%2C+A">Ahmed Elgammal</a>, 
<a href="/search/cs?searchtype=author&query=Elliott%2C+L">Luba Elliott</a>, 
<a href="/search/cs?searchtype=author&query=Elhoseiny%2C+M">Mohamed Elhoseiny</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Discovering the creative potentials of a random signal to various artistic
expressions in aesthetic and conceptual richness is a ground for the recent
success of generative machine learning as a way of art creation. To understand
the new artistic medium better, we conduct a comprehensive analysis to position
AI-generated art within the context of human art heritage. Our comparative
analysis is based on an extensive dataset, dubbed ``ArtConstellation,''
consisting of annotations about art principles, likability, and emotions for
6,000 WikiArt and 3,200 AI-generated artworks. After training various
state-of-the-art generative models, art samples are produced and compared with
WikiArt data on the last hidden layer of a deep-CNN trained for style
classification. We actively examined the various art principles to interpret
the neural representations and used them to drive the comparative knowledge
about human and AI-generated art. A key finding in the semantic analysis is
that AI-generated artworks are visually related to the principle concepts for
modern period art made in 1800-2000. In addition, through Out-Of-Distribution
(OOD) and In-Distribution (ID) detection in CLIP space, we find that
AI-generated artworks are ID to human art when they depict landscapes and
geometric abstract figures, while detected as OOD when the machine art consists
of deformed and twisted figures. We observe that machine-generated art is
uniquely characterized by incomplete and reduced figuration. Lastly, we
conducted a human survey about emotional experience. Color composition and
familiar subjects are the key factors of likability and emotions in art
appreciation. We propose our whole methodologies and collected dataset as our
analytical framework to contrast human and AI-generated art, which we refer to
as ``ArtNeuralConstellation''. Code is available at:
https://github.com/faixan-khan/ArtNeuralConstellation
</p>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02454" title="Abstract">arXiv:2402.02454</a> [<a href="/pdf/2402.02454" title="Download PDF">pdf</a>, <a href="/format/2402.02454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Role of Initialization on the Implicit Bias in Deep Linear  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gruber%2C+O">Oria Gruber</a>, 
<a href="/search/cs?searchtype=author&query=Avron%2C+H">Haim Avron</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Despite Deep Learning's (DL) empirical success, our theoretical understanding
of its efficacy remains limited. One notable paradox is that while conventional
wisdom discourages perfect data fitting, deep neural networks are designed to
do just that, yet they generalize effectively. This study focuses on exploring
this phenomenon attributed to the implicit bias at play. Various sources of
implicit bias have been identified, such as step size, weight initialization,
optimization algorithm, and number of parameters. In this work, we focus on
investigating the implicit bias originating from weight initialization. To this
end, we examine the problem of solving underdetermined linear systems in
various contexts, scrutinizing the impact of initialization on the implicit
regularization when using deep networks to solve such systems. Our findings
elucidate the role of initialization in the optimization and generalization
paradoxes, contributing to a more comprehensive understanding of DL's
performance characteristics.
</p>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02455" title="Abstract">arXiv:2402.02455</a> [<a href="/pdf/2402.02455" title="Download PDF">pdf</a>, <a href="/format/2402.02455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Decentralized Identifiers and Verifiable Credentials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mazzocca%2C+C">Carlo Mazzocca</a>, 
<a href="/search/cs?searchtype=author&query=Acar%2C+A">Abbas Acar</a>, 
<a href="/search/cs?searchtype=author&query=Uluagac%2C+S">Selcuk Uluagac</a>, 
<a href="/search/cs?searchtype=author&query=Montanari%2C+R">Rebecca Montanari</a>, 
<a href="/search/cs?searchtype=author&query=Bellavista%2C+P">Paolo Bellavista</a>, 
<a href="/search/cs?searchtype=author&query=Conti%2C+M">Mauro Conti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 15 figures, and 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Digital identity has always been considered the keystone for implementing
secure and trustworthy communications among parties. The ever-evolving digital
landscape has gone through many technological transformations that have also
affected the way entities are digitally identified. During this digital
evolution, identity management has shifted from centralized to decentralized
approaches. The last era of this journey is represented by the emerging
Self-Sovereign Identity (SSI), which gives users full control over their data.
SSI leverages decentralized identifiers (DIDs) and verifiable credentials
(VCs), which have been recently standardized by the World Wide Web Community
(W3C). These technologies have the potential to build more secure and
decentralized digital identity systems, remarkably contributing to
strengthening the security of communications that typically involve many
distributed participants. It is worth noting that the scope of DIDs and VCs
extends beyond individuals, encompassing a broad range of entities including
cloud, edge, and Internet of Things (IoT) resources. However, due to their
novelty, existing literature lacks a comprehensive survey on how DIDs and VCs
have been employed in different application domains, which go beyond SSI
systems. This paper provides readers with a comprehensive overview of such
technologies from different perspectives. Specifically, we first provide the
background on DIDs and VCs. Then, we analyze available implementations and
offer an in-depth review of how these technologies have been employed across
different use-case scenarios. Furthermore, we examine recent regulations and
initiatives that have been emerging worldwide. Finally, we present some
challenges that hinder their adoption in real-world scenarios and future
research directions.
</p>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02456" title="Abstract">arXiv:2402.02456</a> [<a href="/pdf/2402.02456" title="Download PDF">pdf</a>, <a href="/format/2402.02456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering More Effective Tensor Network Structure Search Algorithms  via Large Language Models (LLMs)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+J">Junhua Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Guoxu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chao Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qibin Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Tensor network structure search (TN-SS), aiming at searching for suitable
tensor network (TN) structures in representing high-dimensional problems,
largely promotes the efficacy of TN in various machine learning applications.
Nonetheless, finding a satisfactory TN structure using existing algorithms
remains challenging. To develop more effective algorithms and avoid the human
labor-intensive development process, we explore the knowledge embedded in large
language models (LLMs) for the automatic design of TN-SS algorithms. Our
approach, dubbed GPTN-SS, leverages an elaborate crafting LLM-based prompting
system that operates in an evolutionary-like manner. The experimental results,
derived from real-world data, demonstrate that GPTN-SS can effectively leverage
the insights gained from existing methods to develop novel TN-SS algorithms
that achieve a better balance between exploration and exploitation. These
algorithms exhibit superior performance in searching the high-quality TN
structures for natural image compression and model parameters compression while
also demonstrating generalizability in their performance.
</p>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02457" title="Abstract">arXiv:2402.02457</a> [<a href="/pdf/2402.02457" title="Download PDF">pdf</a>, <a href="/format/2402.02457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Risk-aware Planning Framework of UGVs in Off-Road Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Junkai Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhenhua Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zihan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+C">Changlong Hao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wenliang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuning Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Lei He</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shaobing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianqiang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 18 figures, submit to T-IV
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Planning module is an essential component of intelligent vehicle study. In
this paper, we address the risk-aware planning problem of UGVs through a
global-local planning framework which seamlessly integrates risk assessment
methods. In particular, a global planning algorithm named Coarse2fine A* is
proposed, which incorporates a potential field approach to enhance the safety
of the planning results while ensuring the efficiency of the algorithm. A
deterministic sampling method for local planning is leveraged and modified to
suit off-road environment. It also integrates a risk assessment model to
emphasize the avoidance of local risks. The performance of the algorithm is
demonstrated through simulation experiments by comparing it with baseline
algorithms, where the results of Coarse2fine A* are shown to be approximately
30% safer than those of the baseline algorithms. The practicality and
effectiveness of the proposed planning framework are validated by deploying it
on a real-world system consisting of a control center and a practical UGV
platform.
</p>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02460" title="Abstract">arXiv:2402.02460</a> [<a href="/pdf/2402.02460" title="Download PDF">pdf</a>, <a href="/format/2402.02460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Review of multimodal machine learning approaches in healthcare
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krones%2C+F">Felix Krones</a>, 
<a href="/search/cs?searchtype=author&query=Marikkar%2C+U">Umar Marikkar</a>, 
<a href="/search/cs?searchtype=author&query=Parsons%2C+G">Guy Parsons</a>, 
<a href="/search/cs?searchtype=author&query=Szmul%2C+A">Adam Szmul</a>, 
<a href="/search/cs?searchtype=author&query=Mahdi%2C+A">Adam Mahdi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Machine learning methods in healthcare have traditionally focused on using
data from a single modality, limiting their ability to effectively replicate
the clinical practice of integrating multiple sources of information for
improved decision making. Clinicians typically rely on a variety of data
sources including patients' demographic information, laboratory data, vital
signs and various imaging data modalities to make informed decisions and
contextualise their findings. Recent advances in machine learning have
facilitated the more efficient incorporation of multimodal data, resulting in
applications that better represent the clinician's approach. Here, we provide a
review of multimodal machine learning approaches in healthcare, offering a
comprehensive overview of recent literature. We discuss the various data
modalities used in clinical diagnosis, with a particular emphasis on imaging
data. We evaluate fusion techniques, explore existing multimodal datasets and
examine common training strategies.
</p>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02463" title="Abstract">arXiv:2402.02463</a> [<a href="/pdf/2402.02463" title="Download PDF">pdf</a>, <a href="/format/2402.02463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Fast Method for Lasso and Logistic Lasso
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Siu-Wing Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+M+T">Man Ting Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We propose a fast method for solving compressed sensing, Lasso regression,
and Logistic Lasso regression problems that iteratively runs an appropriate
solver using an active set approach. We design a strategy to update the active
set that achieves a large speedup over a single call of several solvers,
including gradient projection for sparse reconstruction (GPSR), lassoglm of
Matlab, and glmnet. For compressed sensing, the hybrid of our method and GPSR
is 31.41 times faster than GPSR on average for Gaussian ensembles and 25.64
faster on average for binary ensembles. For Lasso regression, the hybrid of our
method and GPSR achieves a 30.67-fold average speedup in our experiments. In
our experiments on Logistic Lasso regression, the hybrid of our method and
lassoglm gives an 11.95-fold average speedup, and the hybrid of our method and
glmnet gives a 1.40-fold average speedup.
</p>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02464" title="Abstract">arXiv:2402.02464</a> [<a href="/pdf/2402.02464" title="Download PDF">pdf</a>, <a href="/format/2402.02464" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhangyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+D">Daize Dong</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Cheng Tan</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+J">Jun Xia</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Bozhen Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Can we model non-Euclidean graphs as pure language or even Euclidean vectors
while retaining their inherent information? The non-Euclidean property have
posed a long term challenge in graph modeling. Despite recent GNN and
Graphformer efforts encoding graphs as Euclidean vectors, recovering original
graph from the vectors remains a challenge. We introduce GraphsGPT, featuring a
Graph2Seq encoder that transforms non-Euclidean graphs into learnable graph
words in a Euclidean space, along with a GraphGPT decoder that reconstructs the
original graph from graph words to ensure information equivalence. We pretrain
GraphsGPT on 100M molecules and yield some interesting findings: (1) Pretrained
Graph2Seq excels in graph representation learning, achieving state-of-the-art
results on 8/9 graph classification and regression tasks. (2) Pretrained
GraphGPT serves as a strong graph generator, demonstrated by its ability to
perform both unconditional and conditional graph generation. (3)
Graph2Seq+GraphGPT enables effective graph mixup in the Euclidean space,
overcoming previously known non-Euclidean challenge. (4) Our proposed novel
edge-centric GPT pretraining task is effective in graph fields, underscoring
its success in both representation and generation.
</p>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02468" title="Abstract">arXiv:2402.02468</a> [<a href="/pdf/2402.02468" title="Download PDF">pdf</a>, <a href="/format/2402.02468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Peer Adaptation with Context-aware Exploration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Long Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuanfei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+F">Fangwei Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Song-Chun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhou Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Fast adapting to unknown peers (partners or opponents) with different
strategies is a key challenge in multi-agent games. To do so, it is crucial for
the agent to efficiently probe and identify the peer's strategy, as this is the
prerequisite for carrying out the best response in adaptation. However, it is
difficult to explore the strategies of unknown peers, especially when the games
are partially observable and have a long horizon. In this paper, we propose a
peer identification reward, which rewards the learning agent based on how well
it can identify the behavior pattern of the peer over the historical context,
such as the observation over multiple episodes. This reward motivates the agent
to learn a context-aware policy for effective exploration and fast adaptation,
i.e., to actively seek and collect informative feedback from peers when
uncertain about their policies and to exploit the context to perform the best
response when confident. We evaluate our method on diverse testbeds that
involve competitive (Kuhn Poker), cooperative (PO-Overcooked), or mixed
(Predator-Prey-W) games with peer agents. We demonstrate that our method
induces more active exploration behavior, achieving faster adaptation and
better outcomes than existing methods.
</p>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02473" title="Abstract">arXiv:2402.02473</a> [<a href="/pdf/2402.02473" title="Download PDF">pdf</a>, <a href="/ps/2402.02473" title="Download PostScript">ps</a>, <a href="/format/2402.02473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Downlink Localization in Near-Field and Far-Field
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mylonopoulos%2C+G">Georgios Mylonopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Makki%2C+B">Behrooz Makki</a>, 
<a href="/search/cs?searchtype=author&query=Fodor%2C+G">G&#xe1;bor Fodor</a>, 
<a href="/search/cs?searchtype=author&query=Buzzi%2C+S">Stefano Buzzi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper considers the problem of downlink localization of user equipment
devices (UEs) that are either in the near-field (NF) or in the far-field (FF)
of the array of the serving base station (BS). We propose a dual signaling
scheme, which can be implemented at the BS, for localizing such UEs. The first
scheme assumes FF, while the other assumes NF conditions. Both schemes comprise
a beam-sweeping technique, employed by the BS, and a localization algorithm,
employed by the UEs. The FF-based scheme enables beam-steering with a low
signaling overhead, which is utilized for the proposed localization algorithm,
while the NF-based scheme operates with a higher complexity. Specifically, our
proposed localization scheme takes advantage of the relaxed structure of the
FF, which yields low computational complexity, but is not suitable for
operating in the NF. Since the compatibility and the performance of the FF-
based scheme depends on the BS-to-UE distance, we study the limitations of
FF-based procedure, explore the trade-off in terms of performance and resource
requirements for the two schemes, and propose a triggering condition for
operating the component schemes of the dual scheme. Also, we study the
performance of an iterative localization algorithm that takes into account the
accuracy-complexity trade-off and adapts to the actual position of the UE. We
find that the conventional Fraunhofer distance is not sufficient for adapting
localization algorithms in the mixed NF and FF environment.
</p>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02474" title="Abstract">arXiv:2402.02474</a> [<a href="/pdf/2402.02474" title="Download PDF">pdf</a>, <a href="/format/2402.02474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Spectral Improvement for Unsupervised Image Instance Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arefi%2C+F">Farnoosh Arefi</a>, 
<a href="/search/cs?searchtype=author&query=Mansourian%2C+A+M">Amir M. Mansourian</a>, 
<a href="/search/cs?searchtype=author&query=Kasaei%2C+S">Shohreh Kasaei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 13 figures and 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep spectral methods reframe the image decomposition process as a graph
partitioning task by extracting features using self-supervised learning and
utilizing the Laplacian of the affinity matrix to obtain eigensegments.
However, instance segmentation has received less attention compared to other
tasks within the context of deep spectral methods. This paper addresses the
fact that not all channels of the feature map extracted from a self-supervised
backbone contain sufficient information for instance segmentation purposes. In
fact, Some channels are noisy and hinder the accuracy of the task. To overcome
this issue, this paper proposes two channel reduction modules: Noise Channel
Reduction (NCR) and Deviation-based Channel Reduction (DCR). The NCR retains
channels with lower entropy, as they are less likely to be noisy, while DCR
prunes channels with low standard deviation, as they lack sufficient
information for effective instance segmentation. Furthermore, the paper
demonstrates that the dot product, commonly used in deep spectral methods, is
not suitable for instance segmentation due to its sensitivity to feature map
values, potentially leading to incorrect instance segments. A new similarity
metric called Bray-Curtis over Chebyshev (BoC) is proposed to address this
issue. It takes into account the distribution of features in addition to their
values, providing a more robust similarity measure for instance segmentation.
Quantitative and qualitative results on the Youtube-VIS2019 dataset highlight
the improvements achieved by the proposed channel reduction methods and the use
of BoC instead of the conventional dot product for creating the affinity
matrix. These improvements are observed in terms of mean Intersection over
Union and extracted instance segments, demonstrating enhanced instance
segmentation performance. The code is available on:
https://github.com/farnooshar/SpecUnIIS
</p>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02475" title="Abstract">arXiv:2402.02475</a> [<a href="/pdf/2402.02475" title="Download PDF">pdf</a>, <a href="/format/2402.02475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TimeSiam: A Pre-Training Framework for Siamese Time-Series Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Jiaxiang Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haixu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Y">Yunzhong Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Li Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianmin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+M">Mingsheng Long</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Time series pre-training has recently garnered wide attention for its
potential to reduce labeling expenses and benefit various downstream tasks.
Prior methods are mainly based on pre-training techniques well-acknowledged in
vision or language, such as masked modeling and contrastive learning. However,
randomly masking time series or calculating series-wise similarity will distort
or neglect inherent temporal correlations crucial in time series data. To
emphasize temporal correlation modeling, this paper proposes TimeSiam as a
simple but effective self-supervised pre-training framework for Time series
based on Siamese networks. Concretely, TimeSiam pre-trains Siamese encoders to
capture intrinsic temporal correlations between randomly sampled past and
current subseries. With a simple data augmentation method (e.g.~masking),
TimeSiam can benefit from diverse augmented subseries and learn internal
time-dependent representations through a past-to-current reconstruction.
Moreover, learnable lineage embeddings are also introduced to distinguish
temporal distance between sampled series and further foster the learning of
diverse temporal correlations. TimeSiam consistently outperforms extensive
advanced pre-training baselines, demonstrating superior forecasting and
classification capabilities across 13 standard benchmarks in both intra- and
cross-domain scenarios.
</p>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02478" title="Abstract">arXiv:2402.02478</a> [<a href="/pdf/2402.02478" title="Download PDF">pdf</a>, <a href="/format/2402.02478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Why are hyperbolic neural networks effective? A study on hierarchical  representation capability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+S">Shicheng Tan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Huanjing Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanping Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Hyperbolic Neural Networks (HNNs), operating in hyperbolic space, have been
widely applied in recent years, motivated by the existence of an optimal
embedding in hyperbolic space that can preserve data hierarchical relationships
(termed Hierarchical Representation Capability, HRC) more accurately than
Euclidean space. However, there is no evidence to suggest that HNNs can achieve
this theoretical optimal embedding, leading to much research being built on
flawed motivations. In this paper, we propose a benchmark for evaluating HRC
and conduct a comprehensive analysis of why HNNs are effective through
large-scale experiments. Inspired by the analysis results, we propose several
pre-training strategies to enhance HRC and improve the performance of
downstream tasks, further validating the reliability of the analysis.
Experiments show that HNNs cannot achieve the theoretical optimal embedding.
The HRC is significantly affected by the optimization objectives and
hierarchical structures, and enhancing HRC through pre-training strategies can
significantly improve the performance of HNNs.
</p>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02479" title="Abstract">arXiv:2402.02479</a> [<a href="/pdf/2402.02479" title="Download PDF">pdf</a>, <a href="/ps/2402.02479" title="Download PostScript">ps</a>, <a href="/format/2402.02479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BRAIn: Bayesian Reward-conditioned Amortized Inference for natural  language generation from feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pandey%2C+G">Gaurav Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Nandwani%2C+Y">Yatin Nandwani</a>, 
<a href="/search/cs?searchtype=author&query=Naseem%2C+T">Tahira Naseem</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+M">Mayank Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Guangxuan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Raghu%2C+D">Dinesh Raghu</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+S">Sachindra Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Munawar%2C+A">Asim Munawar</a>, 
<a href="/search/cs?searchtype=author&query=Astudillo%2C+R+F">Ram&#xf3;n Fernandez Astudillo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Following the success of Proximal Policy Optimization (PPO) for Reinforcement
Learning from Human Feedback (RLHF), new techniques such as Sequence Likelihood
Calibration (SLiC) and Direct Policy Optimization (DPO) have been proposed that
are offline in nature and use rewards in an indirect manner. These techniques,
in particular DPO, have recently become the tools of choice for LLM alignment
due to their scalability and performance. However, they leave behind important
features of the PPO approach. Methods such as SLiC or RRHF make use of the
Reward Model (RM) only for ranking/preference, losing fine-grained information
and ignoring the parametric form of the RM (eg., Bradley-Terry, Plackett-Luce),
while methods such as DPO do not use even a separate reward model. In this
work, we propose a novel approach, named BRAIn, that re-introduces the RM as
part of a distribution matching approach.BRAIn considers the LLM distribution
conditioned on the assumption of output goodness and applies Bayes theorem to
derive an intractable posterior distribution where the RM is explicitly
represented. BRAIn then distills this posterior into an amortized inference
network through self-normalized importance sampling, leading to a scalable
offline algorithm that significantly outperforms prior art in summarization and
AntropicHH tasks. BRAIn also has interesting connections to PPO and DPO for
specific RM choices.
</p>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02483" title="Abstract">arXiv:2402.02483</a> [<a href="/pdf/2402.02483" title="Download PDF">pdf</a>, <a href="/ps/2402.02483" title="Download PostScript">ps</a>, <a href="/format/2402.02483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Blockchain in E-Government Services: Status and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mansour%2C+M">Manal Mansour</a>, 
<a href="/search/cs?searchtype=author&query=Salama%2C+M">May Salama</a>, 
<a href="/search/cs?searchtype=author&query=Helmi%2C+H">Hala Helmi</a>, 
<a href="/search/cs?searchtype=author&query=Mursi%2C+M">Mona Mursi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Blockchain technology is referred to as a very secure decentralized,
distributed ledger that records the history of any digital asset. It is being
used in numerous governmental and private sector organizations across numerous
nations. Surveying the current state of blockchain applications and
difficulties in e-government services is the goal of this review. Held to the
account are use cases for current facilities that use blockchain. Finally, it
examines the research gap in blockchain deployment and makes suggestions for
future work for additional research.
</p>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02484" title="Abstract">arXiv:2402.02484</a> [<a href="/pdf/2402.02484" title="Download PDF">pdf</a>, <a href="/format/2402.02484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weisfeiler Leman for Euclidean Equivariant Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hordan%2C+S">Snir Hordan</a>, 
<a href="/search/cs?searchtype=author&query=Amir%2C+T">Tal Amir</a>, 
<a href="/search/cs?searchtype=author&query=Dym%2C+N">Nadav Dym</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The $k$-Weifeiler-Leman ($k$-WL) graph isomorphism test hierarchy is a common
method for assessing the expressive power of graph neural networks (GNNs).
Recently, the $2$-WL test was proven to be complete on weighted graphs which
encode $3\mathrm{D}$ point cloud data. Consequently, GNNs whose expressive
power is equivalent to the $2$-WL test are provably universal on point clouds.
Yet, this result is limited to invariant continuous functions on point clouds.
<br />In this paper we extend this result in three ways: Firstly, we show that
$2$-WL tests can be extended to point clouds which include both positions and
velocity, a scenario often encountered in applications. Secondly, we show that
PPGN (Maron et al., 2019) can simulate $2$-WL uniformly on all point clouds
with low complexity. Finally, we show that a simple modification of this PPGN
architecture can be used to obtain a universal equivariant architecture that
can approximate all continuous equivariant functions uniformly.
<br />Building on our results, we develop our WeLNet architecture, which can
process position-velocity pairs, compute functions fully equivariant to
permutations and rigid motions, and is provably complete and universal.
Remarkably, WeLNet is provably complete precisely in the setting in which it is
implemented in practice. Our theoretical results are complemented by
experiments showing WeLNet sets new state-of-the-art results on the N-Body
dynamics task and the GEOM-QM9 molecular conformation generation task.
</p>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02491" title="Abstract">arXiv:2402.02491</a> [<a href="/pdf/2402.02491" title="Download PDF">pdf</a>, <a href="/format/2402.02491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VM-UNet: Vision Mamba UNet for Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ruan%2C+J">Jiacheng Ruan</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+S">Suncheng Xiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 2 figures, 3 tables. Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In the realm of medical image segmentation, both CNN-based and
Transformer-based models have been extensively explored. However, CNNs exhibit
limitations in long-range modeling capabilities, whereas Transformers are
hampered by their quadratic computational complexity. Recently, State Space
Models (SSMs), exemplified by Mamba, have emerged as a promising approach. They
not only excel in modeling long-range interactions but also maintain a linear
computational complexity. In this paper, leveraging state space models, we
propose a U-shape architecture model for medical image segmentation, named
Vision Mamba UNet (VM-UNet). Specifically, the Visual State Space (VSS) block
is introduced as the foundation block to capture extensive contextual
information, and an asymmetrical encoder-decoder structure is constructed. We
conduct comprehensive experiments on the ISIC17, ISIC18, and Synapse datasets,
and the results indicate that VM-UNet performs competitively in medical image
segmentation tasks. To our best knowledge, this is the first medical image
segmentation model constructed based on the pure SSM-based model. We aim to
establish a baseline and provide valuable insights for the future development
of more efficient and effective SSM-based segmentation systems. Our code is
available at https://github.com/JCruan519/VM-UNet.
</p>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02499" title="Abstract">arXiv:2402.02499</a> [<a href="/pdf/2402.02499" title="Download PDF">pdf</a>, <a href="/format/2402.02499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robot Trajectron: Trajectory Prediction-based Shared Control for Robot  Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+P">Pinhao Song</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pengteng Li</a>, 
<a href="/search/cs?searchtype=author&query=Aertbelien%2C+E">Erwin Aertbelien</a>, 
<a href="/search/cs?searchtype=author&query=Detry%2C+R">Renaud Detry</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICRA2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We address the problem of (a) predicting the trajectory of an arm reaching
motion, based on a few seconds of the motion's onset, and (b) leveraging this
predictor to facilitate shared-control manipulation tasks, easing the cognitive
load of the operator by assisting them in their anticipated direction of
motion. Our novel intent estimator, dubbed the \emph{Robot Trajectron} (RT),
produces a probabilistic representation of the robot's anticipated trajectory
based on its recent position, velocity and acceleration history. Taking arm
dynamics into account allows RT to capture the operator's intent better than
other SOTA models that only use the arm's position, making it particularly
well-suited to assist in tasks where the operator's intent is susceptible to
change. We derive a novel shared-control solution that combines RT's predictive
capacity to a representation of the locations of potential reaching targets.
Our experiments demonstrate RT's effectiveness in both intent estimation and
shared-control tasks. We will make the code and data supporting our experiments
publicly available at https://github.com/mousecpn/Robot-Trajectron.git.
</p>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02500" title="Abstract">arXiv:2402.02500</a> [<a href="/pdf/2402.02500" title="Download PDF">pdf</a>, <a href="/format/2402.02500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Point Cloud Matters: Rethinking the Impact of Different Observation  Spaces on Robot Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Haoyi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yating Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Di Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+W">Weicai Ye</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tong He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this study, we explore the influence of different observation spaces on
robot learning, focusing on three predominant modalities: RGB, RGB-D, and point
cloud. Through extensive experimentation on over 17 varied contact-rich
manipulation tasks, conducted across two benchmarks and simulators, we have
observed a notable trend: point cloud-based methods, even those with the
simplest designs, frequently surpass their RGB and RGB-D counterparts in
performance. This remains consistent in both scenarios: training from scratch
and utilizing pretraining. Furthermore, our findings indicate that point cloud
observations lead to improved policy zero-shot generalization in relation to
various geometry and visual clues, including camera viewpoints, lighting
conditions, noise levels and background appearance. The outcomes suggest that
3D point cloud is a valuable observation modality for intricate robotic tasks.
We will open-source all our codes and checkpoints, hoping that our insights can
help design more generalizable and robust robotic models.
</p>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02501" title="Abstract">arXiv:2402.02501</a> [<a href="/pdf/2402.02501" title="Download PDF">pdf</a>, <a href="/format/2402.02501" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Data and Semantics Lossy Compression: Nonasymptotic Converse  Bounds and Second-Order Asymptotics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Huiyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuxuan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+S">Shuo Shao</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xiaojun Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">This paper studies the joint data and semantics lossy compression problem,
i.e., an extension of the hidden lossy source coding problem that entails
recovering both the hidden and observable sources. We aim to study the
nonasymptotic and second-order properties of this problem, especially the
converse aspect. Specifically, we begin by deriving general nonasymptotic
converse bounds valid for general sources and distortion measures, utilizing
properties of distortion-tilted information. Subsequently, a second-order
converse bound is derived under the standard block coding setting through
asymptotic analysis of the nonasymptotic bounds. This bound is tight since it
coincides with a known second-order achievability bound. We then examine the
case of erased fair coin flips (EFCF), providing its specific nonasymptotic
achievability and converse bounds. Numerical results under the EFCF case
demonstrate that our second-order asymptotic approximation effectively
approximates the optimum rate at given blocklengths.
</p>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02503" title="Abstract">arXiv:2402.02503</a> [<a href="/pdf/2402.02503" title="Download PDF">pdf</a>, <a href="/ps/2402.02503" title="Download PostScript">ps</a>, <a href="/format/2402.02503" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GeReA: Question-Aware Prompt Captions for Knowledge-based Visual  Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Ziyu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shutao Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+B">Bin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+J">Jianfei Cai</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+Z">Zuxiang Long</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+F">Fuyan Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Knowledge-based visual question answering (VQA) requires world knowledge
beyond the image for accurate answer. Recently, instead of extra knowledge
bases, a large language model (LLM) like GPT-3 is activated as an implicit
knowledge engine to jointly acquire and reason the necessary knowledge for
answering by converting images into textual information (e.g., captions and
answer candidates). However, such conversion may introduce irrelevant
information, which causes the LLM to misinterpret images and ignore visual
details crucial for accurate knowledge. We argue that multimodal large language
model (MLLM) is a better implicit knowledge engine than the LLM for its
superior capability of visual understanding. Despite this, how to activate the
capacity of MLLM as the implicit knowledge engine has not been explored yet.
Therefore, we propose GeReA, a generate-reason framework that prompts a MLLM
like InstructBLIP with question relevant vision and language information to
generate knowledge-relevant descriptions and reasons those descriptions for
knowledge-based VQA. Specifically, the question-relevant image regions and
question-specific manual prompts are encoded in the MLLM to generate the
knowledge relevant descriptions, referred to as question-aware prompt captions.
After that, the question-aware prompt captions, image-question pair, and
similar samples are sent into the multi-modal reasoning model to learn a joint
knowledge-image-question representation for answer prediction. GeReA unlocks
the use of MLLM as the implicit knowledge engine, surpassing all previous
state-of-the-art methods on OK-VQA and A-OKVQA datasets, with test accuracies
of 66.5% and 63.3% respectively. Our code will be released at
https://github.com/Upper9527/GeReA.
</p>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02506" title="Abstract">arXiv:2402.02506</a> [<a href="/pdf/2402.02506" title="Download PDF">pdf</a>, <a href="/format/2402.02506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Device Scheduling and Assignment in Hierarchical Federated Learning for  Internet of Things
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tinghao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+K">Kwok-Yan Lam</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in IEEE Internet of Things Journal (IoT-J)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Federated Learning (FL) is a promising machine learning approach for Internet
of Things (IoT), but it has to address network congestion problems when the
population of IoT devices grows. Hierarchical FL (HFL) alleviates this issue by
distributing model aggregation to multiple edge servers. Nevertheless, the
challenge of communication overhead remains, especially in scenarios where all
IoT devices simultaneously join the training process. For scalability,
practical HFL schemes select a subset of IoT devices to participate in the
training, hence the notion of device scheduling. In this setting, only selected
IoT devices are scheduled to participate in the global training, with each of
them being assigned to one edge server. Existing HFL assignment methods are
primarily based on search mechanisms, which suffer from high latency in finding
the optimal assignment. This paper proposes an improved K-Center algorithm for
device scheduling and introduces a deep reinforcement learning-based approach
for assigning IoT devices to edge servers. Experiments show that scheduling 50%
of IoT devices is generally adequate for achieving convergence in HFL with much
lower time delay and energy consumption. In cases where reduction in energy
consumption (such as in Green AI) and reduction of messages (to avoid burst
traffic) are key objectives, scheduling 30% IoT devices allows a substantial
reduction in energy and messages with similar model accuracy.
</p>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02511" title="Abstract">arXiv:2402.02511</a> [<a href="/pdf/2402.02511" title="Download PDF">pdf</a>, <a href="/format/2402.02511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PoCo: Policy Composition from and for Heterogeneous Robot Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lirui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jialiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yilun Du</a>, 
<a href="/search/cs?searchtype=author&query=Adelson%2C+E+H">Edward H. Adelson</a>, 
<a href="/search/cs?searchtype=author&query=Tedrake%2C+R">Russ Tedrake</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Training general robotic policies from heterogeneous data for different tasks
is a significant challenge. Existing robotic datasets vary in different
modalities such as color, depth, tactile, and proprioceptive information, and
collected in different domains such as simulation, real robots, and human
videos. Current methods usually collect and pool all data from one domain to
train a single policy to handle such heterogeneity in tasks and domains, which
is prohibitively expensive and difficult. In this work, we present a flexible
approach, dubbed Policy Composition, to combine information across such diverse
modalities and domains for learning scene-level and task-level generalized
manipulation skills, by composing different data distributions represented with
diffusion models. Our method can use task-level composition for multi-task
manipulation and be composed with analytic cost functions to adapt policy
behaviors at inference time. We train our method on simulation, human, and real
robot data and evaluate in tool-use tasks. The composed policy achieves robust
and dexterous performance under varying scenes and tasks and outperforms
baselines from a single data source in both simulation and real-world
experiments. See https://liruiw.github.io/policycomp for more details .
</p>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02513" title="Abstract">arXiv:2402.02513</a> [<a href="/pdf/2402.02513" title="Download PDF">pdf</a>, <a href="/ps/2402.02513" title="Download PostScript">ps</a>, <a href="/format/2402.02513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Early stopping by correlating online indicators in neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferro%2C+M+V">Manuel Vilares Ferro</a>, 
<a href="/search/cs?searchtype=author&query=Mosquera%2C+Y+D">Yerai Doval Mosquera</a>, 
<a href="/search/cs?searchtype=author&query=Pena%2C+F+J+R">Francisco J. Ribadas Pena</a>, 
<a href="/search/cs?searchtype=author&query=Bilbao%2C+V+M+D">Victor M. Darriba Bilbao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 6 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Neural Networks, 159 (2023), pp 109-124. ISSN 1879-2782. Elsevier
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">In order to minimize the generalization error in neural networks, a novel
technique to identify overfitting phenomena when training the learner is
formally introduced. This enables support of a reliable and trustworthy early
stopping condition, thus improving the predictive power of that type of
modeling. Our proposal exploits the correlation over time in a collection of
online indicators, namely characteristic functions for indicating if a set of
hypotheses are met, associated with a range of independent stopping conditions
built from a canary judgment to evaluate the presence of overfitting. That way,
we provide a formal basis for decision making in terms of interrupting the
learning process.
<br />As opposed to previous approaches focused on a single criterion, we take
advantage of subsidiarities between independent assessments, thus seeking both
a wider operating range and greater diagnostic reliability. With a view to
illustrating the effectiveness of the halting condition described, we choose to
work in the sphere of natural language processing, an operational continuum
increasingly based on machine learning. As a case study, we focus on parser
generation, one of the most demanding and complex tasks in the domain. The
selection of cross-validation as a canary function enables an actual comparison
with the most representative early stopping conditions based on overfitting
identification, pointing to a promising start toward an optimal bias and
variance control.
</p>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02514" title="Abstract">arXiv:2402.02514</a> [<a href="/pdf/2402.02514" title="Download PDF">pdf</a>, <a href="/format/2402.02514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Supervision by Gaussian Pseudo-label-based Morphological Attention  for Abdominal Aorta Segmentation in Non-Contrast CTs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Q">Qixiang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Lucas%2C+A">Antoine Lucas</a>, 
<a href="/search/cs?searchtype=author&query=Kaladji%2C+A">Adrien Kaladji</a>, 
<a href="/search/cs?searchtype=author&query=Haigron%2C+P">Pascal Haigron</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by 21st IEEE International Symposium on Biomedical Imaging
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The segmentation of the abdominal aorta in non-contrast CT images is a
non-trivial task for computer-assisted endovascular navigation, particularly in
scenarios where contrast agents are unsuitable. While state-of-the-art deep
learning segmentation models have been proposed recently for this task, they
are trained on manually annotated strong labels. However, the inherent
ambiguity in the boundary of the aorta in non-contrast CT may undermine the
reliability of strong labels, leading to potential overfitting risks. This
paper introduces a Gaussian-based pseudo label, integrated into conventional
deep learning models through deep supervision, to achieve Morphological
Attention (MA) enhancement. As the Gaussian pseudo label retains the
morphological features of the aorta without explicitly representing its
boundary distribution, we suggest that it preserves aortic morphology during
training while mitigating the negative impact of ambiguous boundaries, reducing
the risk of overfitting. It is introduced in various 2D/3D deep learning models
and validated on our local data set of 30 non-contrast CT volumes comprising
5749 CT slices. The results underscore the effectiveness of MA in preserving
the morphological characteristics of the aorta and addressing overfitting
concerns, thereby enhancing the performance of the models.
</p>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02515" title="Abstract">arXiv:2402.02515</a> [<a href="/pdf/2402.02515" title="Download PDF">pdf</a>, <a href="/ps/2402.02515" title="Download PostScript">ps</a>, <a href="/format/2402.02515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling of learning curves with applications to pos tagging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferro%2C+M+V">Manuel Vilares Ferro</a>, 
<a href="/search/cs?searchtype=author&query=Bilbao%2C+V+M+D">Victor M. Darriba Bilbao</a>, 
<a href="/search/cs?searchtype=author&query=Pena%2C+F+J+R">Francisco J. Ribadas Pena</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 11 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computer Speech &amp; Language, 41, pp 1-28 (2017). ISSN 0885-2308.
  Elsevier
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">An algorithm to estimate the evolution of learning curves on the whole of a
training data base, based on the results obtained from a portion and using a
functional strategy, is introduced. We approximate iteratively the sought value
at the desired time, independently of the learning technique used and once a
point in the process, called prediction level, has been passed. The proposal
proves to be formally correct with respect to our working hypotheses and
includes a reliable proximity condition. This allows the user to fix a
convergence threshold with respect to the accuracy finally achievable, which
extends the concept of stopping criterion and seems to be effective even in the
presence of distorting observations.
<br />Our aim is to evaluate the training effort, supporting decision making in
order to reduce the need for both human and computational resources during the
learning process. The proposal is of interest in at least three operational
procedures. The first is the anticipation of accuracy gain, with the purpose of
measuring how much work is needed to achieve a certain degree of performance.
The second relates the comparison of efficiency between systems at training
time, with the objective of completing this task only for the one that best
suits our requirements. The prediction of accuracy is also a valuable item of
information for customizing systems, since we can estimate in advance the
impact of settings on both the performance and the development costs. Using the
generation of part-of-speech taggers as an example application, the
experimental results are consistent with our expectations.
</p>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02516" title="Abstract">arXiv:2402.02516</a> [<a href="/pdf/2402.02516" title="Download PDF">pdf</a>, <a href="/ps/2402.02516" title="Download PostScript">ps</a>, <a href="/format/2402.02516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive scheduling for adaptive sampling in POS taggers construction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferro%2C+M+V">Manuel Vilares Ferro</a>, 
<a href="/search/cs?searchtype=author&query=Bilbao%2C+V+M+D">Victor M. Darriba Bilbao</a>, 
<a href="/search/cs?searchtype=author&query=Ferro%2C+J+V">Jes&#xfa;s Vilares Ferro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pager, 10 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computer Speech &amp; Language, 60, 101020 (2020), pp 1-18. ISSN
  0885-2308. Elsevier
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce an adaptive scheduling for adaptive sampling as a novel way of
machine learning in the construction of part-of-speech taggers. The goal is to
speed up the training on large data sets, without significant loss of
performance with regard to an optimal configuration. In contrast to previous
methods using a random, fixed or regularly rising spacing between the
instances, ours analyzes the shape of the learning curve geometrically in
conjunction with a functional model to increase or decrease it at any time. The
algorithm proves to be formally correct regarding our working hypotheses.
Namely, given a case, the following one is the nearest ensuring a net gain of
learning ability from the former, it being possible to modulate the level of
requirement for this condition. We also improve the robustness of sampling by
paying greater attention to those regions of the training data base subject to
a temporary inflation in performance, thus preventing the learning from
stopping prematurely.
<br />The proposal has been evaluated on the basis of its reliability to identify
the convergence of models, corroborating our expectations. While a concrete
halting condition is used for testing, users can choose any condition
whatsoever to suit their own specific needs.
</p>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02518" title="Abstract">arXiv:2402.02518</a> [<a href="/pdf/2402.02518" title="Download PDF">pdf</a>, <a href="/ps/2402.02518" title="Download PostScript">ps</a>, <a href="/format/2402.02518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent Graph Diffusion: A Unified Framework for Generation and  Prediction on Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+Z">Zhou Cai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Muhan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this paper, we propose the first framework that enables solving graph
learning tasks of all levels (node, edge and graph) and all types (generation,
regression and classification) with one model. We first propose Latent Graph
Diffusion (LGD), a generative model that can generate node, edge, and
graph-level features of all categories simultaneously. We achieve this goal by
embedding the graph structures and features into a latent space leveraging a
powerful encoder which can also be decoded, then training a diffusion model in
the latent space. LGD is also capable of conditional generation through a
specifically designed cross-attention mechanism. Then we formulate prediction
tasks including regression and classification as (conditional) generation,
which enables our LGD to solve tasks of all levels and all types with provable
guarantees. We verify the effectiveness of our framework with extensive
experiments, where our models achieve state-of-the-art or highly competitive
results across generation and regression tasks.
</p>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02519" title="Abstract">arXiv:2402.02519</a> [<a href="/pdf/2402.02519" title="Download PDF">pdf</a>, <a href="/format/2402.02519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SIMPL: A Simple and Efficient Multi-agent Motion Prediction Baseline for  Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peiliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sikang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Shaojie Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is available at <a href="https://github.com/HKUST-Aerial-Robotics/SIMPL">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper presents a Simple and effIcient Motion Prediction baseLine (SIMPL)
for autonomous vehicles. Unlike conventional agent-centric methods with high
accuracy but repetitive computations and scene-centric methods with compromised
accuracy and generalizability, SIMPL delivers real-time, accurate motion
predictions for all relevant traffic participants. To achieve improvements in
both accuracy and inference speed, we propose a compact and efficient global
feature fusion module that performs directed message passing in a symmetric
manner, enabling the network to forecast future motion for all road users in a
single feed-forward pass and mitigating accuracy loss caused by viewpoint
shifting. Additionally, we investigate the continuous trajectory
parameterization using Bernstein basis polynomials in trajectory decoding,
allowing evaluations of states and their higher-order derivatives at any
desired time point, which is valuable for downstream planning tasks. As a
strong baseline, SIMPL exhibits highly competitive performance on Argoverse 1 &amp;
2 motion forecasting benchmarks compared with other state-of-the-art methods.
Furthermore, its lightweight design and low inference latency make SIMPL highly
extensible and promising for real-world onboard deployment. We open-source the
code at https://github.com/HKUST-Aerial-Robotics/SIMPL.
</p>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02521" title="Abstract">arXiv:2402.02521</a> [<a href="/pdf/2402.02521" title="Download PDF">pdf</a>, <a href="/format/2402.02521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neuromorphic hardware for sustainable AI data centers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vogginger%2C+B">Bernhard Vogginger</a>, 
<a href="/search/cs?searchtype=author&query=Rostami%2C+A">Amirhossein Rostami</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+V">Vaibhav Jain</a>, 
<a href="/search/cs?searchtype=author&query=Arfa%2C+S">Sirine Arfa</a>, 
<a href="/search/cs?searchtype=author&query=Hantsch%2C+A">Andreas Hantsch</a>, 
<a href="/search/cs?searchtype=author&query=Kappel%2C+D">David Kappel</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%A4fer%2C+M">Michael Sch&#xe4;fer</a>, 
<a href="/search/cs?searchtype=author&query=Faltings%2C+U">Ulrike Faltings</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+H+A">Hector A. Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mayr%2C+C">Christian Mayr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 2 figures, submitted to NICE 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">As humans advance toward a higher level of artificial intelligence, it is
always at the cost of escalating computational resource consumption, which
requires developing novel solutions to meet the exponential growth of AI
computing demand. Neuromorphic hardware takes inspiration from how the brain
processes information and promises energy-efficient computing of AI workloads.
Despite its potential, neuromorphic hardware has not found its way into
commercial AI data centers. In this article, we try to analyze the underlying
reasons for this and derive requirements and guidelines to promote neuromorphic
systems for efficient and sustainable cloud computing: We first review
currently available neuromorphic hardware systems and collect examples where
neuromorphic solutions excel conventional AI processing on CPUs and GPUs. Next,
we identify applications, models and algorithms which are commonly deployed in
AI data centers as further directions for neuromorphic algorithms research.
Last, we derive requirements and best practices for the hardware and software
integration of neuromorphic systems into data centers. With this article, we
hope to increase awareness of the challenges of integrating neuromorphic
hardware into data centers and to guide the community to enable sustainable and
energy-efficient AI at scale.
</p>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02522" title="Abstract">arXiv:2402.02522</a> [<a href="/pdf/2402.02522" title="Download PDF">pdf</a>, <a href="/ps/2402.02522" title="Download PostScript">ps</a>, <a href="/format/2402.02522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Absolute convergence and error thresholds in non-active adaptive  sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferro%2C+M+V">Manuel Vilares Ferro</a>, 
<a href="/search/cs?searchtype=author&query=Bilbao%2C+V+M+D">Victor M. Darriba Bilbao</a>, 
<a href="/search/cs?searchtype=author&query=Ferro%2C+J+V">Jes&#xfa;s Vilares Ferro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 10 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Computer and System Sciences, 129 (2020) , pp 39-61.
  ISSN 1090-2724. Elsevier
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Non-active adaptive sampling is a way of building machine learning models
from a training data base which are supposed to dynamically and automatically
derive guaranteed sample size. In this context and regardless of the strategy
used in both scheduling and generating of weak predictors, a proposal for
calculating absolute convergence and error thresholds is described. We not only
make it possible to establish when the quality of the model no longer
increases, but also supplies a proximity condition to estimate in absolute
terms how close it is to achieving such a goal, thus supporting decision making
for fine-tuning learning parameters in model selection. The technique proves
its correctness and completeness with respect to our working hypotheses, in
addition to strengthening the robustness of the sampling scheme. Tests meet our
expectations and illustrate the proposal in the domain of natural language
processing, taking the generation of part-of-speech taggers as case study.
</p>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02523" title="Abstract">arXiv:2402.02523</a> [<a href="/pdf/2402.02523" title="Download PDF">pdf</a>, <a href="/format/2402.02523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FEniCSx Preconditioning Tools (FEniCSx-pctools)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C5%98eho%C5%99%2C+M">Martin &#x158;eho&#x159;</a>, 
<a href="/search/cs?searchtype=author&query=Hale%2C+J+S">Jack S. Hale</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Software (cs.MS)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">FEniCSx Preconditioning Tools (FEniCSx-pctools) is a software package for
easing the specification of PETSc-based block preconditioning strategies in the
DOLFINx finite element solver of the FEniCS Project. It attaches all of the
necessary metadata to the block-structured linear systems in order that
block-structured preconditioners can be applied straightforwardly via PETSc's
options-based configuration system. Fast prototyping is facilitated thanks to
the implementation in Python, and all intensive operations are executed in
C/C++. FEniCSx-pctools is available under the LGPLv3 or later license.
</p>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02526" title="Abstract">arXiv:2402.02526</a> [<a href="/pdf/2402.02526" title="Download PDF">pdf</a>, <a href="/format/2402.02526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CompeteSMoE -- Effective Training of Sparse Mixture of Experts via  Competition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pham%2C+Q">Quang Pham</a>, 
<a href="/search/cs?searchtype=author&query=Do%2C+G">Giang Do</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+H">Huy Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">TrungTin Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chenghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sartipi%2C+M">Mina Sartipi</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+B+T">Binh T. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Ramasamy%2C+S">Savitha Ramasamy</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoli Li</a>, 
<a href="/search/cs?searchtype=author&query=Hoi%2C+S">Steven Hoi</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+N">Nhat Ho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Sparse mixture of experts (SMoE) offers an appealing solution to scale up the
model complexity beyond the mean of increasing the network's depth or width.
However, effective training of SMoE has proven to be challenging due to the
representation collapse issue, which causes parameter redundancy and limited
representation potentials. In this work, we propose a competition mechanism to
address this fundamental challenge of representation collapse. By routing
inputs only to experts with the highest neural response, we show that, under
mild assumptions, competition enjoys the same convergence rate as the optimal
estimator. We further propose CompeteSMoE, an effective and efficient algorithm
to train large language models by deploying a simple router that predicts the
competition outcomes. Consequently, CompeteSMoE enjoys strong performance gains
from the competition routing policy while having low computation overheads. Our
extensive empirical evaluations on two transformer architectures and a wide
range of tasks demonstrate the efficacy, robustness, and scalability of
CompeteSMoE compared to state-of-the-art SMoE strategies.
</p>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02527" title="Abstract">arXiv:2402.02527</a> [<a href="/pdf/2402.02527" title="Download PDF">pdf</a>, <a href="/ps/2402.02527" title="Download PostScript">ps</a>, <a href="/format/2402.02527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Redefining Computing: Rise of ARM from consumer to Cloud for energy  efficiency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahman%2C+T+N">Tahmid Noor Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+N">Nusaiba Khan</a>, 
<a href="/search/cs?searchtype=author&query=Zaman%2C+Z+I">Zarif Ishmam Zaman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Today, our lifestyle revolves around digital devices powered by
microprocessors of different instruction set architectures (ISA). Among them,
the most common are x86 and ARM, the brainpower of our computers and
smartphones. Reduced instruction set computing (RISC) is the basis of ARM
architecture, designed to offer greater energy efficiency. On the other hand,
principles of complex instruction set computing (CISC) are utilized by x86
processors, which handle heavier computing tasks while being more power-hungry.
The rise of smartphones over a decade has changed the laptop market. It also
influenced customers towards ARM-based energy-efficient laptops. This
transition in the computing segment is seen not only in consumers but also in
commercial settings, especially data centers. Usually, data centers are
designed to operate 24/7, where energy is a big concern. So, ARM chips have
started making their way to cloud servers. This paper comprehensively analyzes
and compares the ARM and x86 architectures to unravel the factors contributing
to ARM's increasing dominance in the market. It also explores the impact of
smartphones on the laptop market and assesses the significance of
system-on-a-chip (SoC). Focusing on the proper utilization of energy and
sustainability, our paper offers valuable insights into the growing trend of
adopting ARM processors in the computing industry.
</p>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02533" title="Abstract">arXiv:2402.02533</a> [<a href="/pdf/2402.02533" title="Download PDF">pdf</a>, <a href="/format/2402.02533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying and Extracting Pedestrian Behavior in Critical Traffic  Situations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schachner%2C+M">Martin Schachner</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+B">Bernd Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Weissenbacher%2C+F">Fabian Weissenbacher</a>, 
<a href="/search/cs?searchtype=author&query=Kirillova%2C+N">Nadezda Kirillova</a>, 
<a href="/search/cs?searchtype=author&query=Possegger%2C+H">Horst Possegger</a>, 
<a href="/search/cs?searchtype=author&query=Bischof%2C+H">Horst Bischof</a>, 
<a href="/search/cs?searchtype=author&query=Klug%2C+C">Corina Klug</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 8 figures, ITSC 2023 accepted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">A better understanding of interactive pedestrian behavior in critical traffic
situations is essential for the development of enhanced pedestrian safety
systems. Real-world traffic observations play a decisive role in this, since
they represent behavior in an unbiased way. In this work, we present an
approach of how a subset of very considerable pedestrian-vehicle interactions
can be derived from a camera-based observation system. For this purpose, we
have examined road user trajectories automatically for establishing temporal
and spatial relationships, using 110h hours of video recordings. In order to
identify critical interactions, our approach combines the metric
post-encroachment time with a newly introduced motion adaption metric. From
more than 11,000 reconstructed pedestrian trajectories, 259 potential scenarios
remained, using a post-encroachment time threshold of 2s. However, in 95% of
cases, no adaptation of the pedestrian behavior was observed due to avoiding
criticality. Applying the proposed motion adaption metric, only 21 critical
scenarios remained. Manual investigations revealed that critical pedestrian
vehicle interactions were present in 7 of those. They were further analyzed and
made publicly available for developing pedestrian behavior models3. The results
indicate that critical interactions in which the pedestrian perceives and
reacts to the vehicle at a relatively late stage can be extracted using the
proposed method.
</p>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02540" title="Abstract">arXiv:2402.02540</a> [<a href="/pdf/2402.02540" title="Download PDF">pdf</a>, <a href="/format/2402.02540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Embedding Non-Distortive Cancelable Face Template Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zakharov%2C+D">Dmytro Zakharov</a>, 
<a href="/search/cs?searchtype=author&query=Kuznetsov%2C+O">Oleksandr Kuznetsov</a>, 
<a href="/search/cs?searchtype=author&query=Frontoni%2C+E">Emanuele Frontoni</a>, 
<a href="/search/cs?searchtype=author&query=Kryvinska%2C+N">Natalia Kryvinska</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Biometric authentication systems are crucial for security, but developing
them involves various complexities, including privacy, security, and achieving
high accuracy without directly storing pure biometric data in storage. We
introduce an innovative image distortion technique that makes facial images
unrecognizable to the eye but still identifiable by any custom embedding neural
network model. Using the proposed approach, we test the reliability of
biometric recognition networks by determining the maximum image distortion that
does not change the predicted identity. Through experiments on MNIST and LFW
datasets, we assess its effectiveness and compare it based on the traditional
comparison metrics.
</p>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02541" title="Abstract">arXiv:2402.02541</a> [<a href="/pdf/2402.02541" title="Download PDF">pdf</a>, <a href="/format/2402.02541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Generation for Zero-shot Knowledge-based VQA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+R">Rui Cao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jing Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted as Findings in EACL 2023;
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Previous solutions to knowledge-based visual question answering~(K-VQA)
retrieve knowledge from external knowledge bases and use supervised learning to
train the K-VQA model. Recently pre-trained LLMs have been used as both a
knowledge source and a zero-shot QA model for K-VQA and demonstrated promising
results. However, these recent methods do not explicitly show the knowledge
needed to answer the questions and thus lack interpretability. Inspired by
recent work on knowledge generation from LLMs for text-based QA, in this work
we propose and test a similar knowledge-generation-based K-VQA method, which
first generates knowledge from an LLM and then incorporates the generated
knowledge for K-VQA in a zero-shot manner. We evaluate our method on two K-VQA
benchmarks and found that our method performs better than previous zero-shot
K-VQA methods and our generated knowledge is generally relevant and helpful.
</p>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02543" title="Abstract">arXiv:2402.02543</a> [<a href="/pdf/2402.02543" title="Download PDF">pdf</a>, <a href="/format/2402.02543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safeguarding the Truth of High-Value Price Oracle Task: A Dynamically  Adjusted Truth Discovery Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xian%2C+Y">Youquan Xian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Peng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongcheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xueying Zeng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Computational Engineering, Finance, and Science (cs.CE); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">In recent years, the Decentralized Finance (DeFi) market has witnessed
numerous attacks on the price oracle, leading to substantial economic losses.
Despite the advent of truth discovery methods opening up new avenues for oracle
development, it falls short in addressing high-value attacks on price oracle
tasks. Consequently, this paper introduces a dynamically adjusted truth
discovery method safeguarding the truth of high-value price oracle tasks. In
the truth aggregation stage, we enhance future considerations to improve the
precision of aggregated truth. During the credibility update phase, credibility
is dynamically assessed based on the task's value and the Cumulative Potential
Economic Contribution (CPEC) of information sources. Experimental results
demonstrate a significant reduction in data deviation by 65.8\% and potential
economic loss by 66.5\%, compared to the baseline scheme, in the presence of
high-value attacks.
</p>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02544" title="Abstract">arXiv:2402.02544</a> [<a href="/pdf/2402.02544" title="Download PDF">pdf</a>, <a href="/format/2402.02544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal  Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Muhtar%2C+D">Dilxat Muhtar</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenshi Li</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+F">Feng Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xueliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+P">Pengfeng Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 8 figures. Github <a href="https://github.com/NJU-LHRS/LHRS-Bot">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The revolutionary capabilities of large language models (LLMs) have paved the
way for multimodal large language models (MLLMs) and fostered diverse
applications across various specialized domains. In the remote sensing (RS)
field, however, the diverse geographical landscapes and varied objects in RS
imagery are not adequately considered in recent MLLM endeavors. To bridge this
gap, we construct a large-scale RS image-text dataset, LHRS-Align, and an
informative RS-specific instruction dataset, LHRS-Instruct, leveraging the
extensive volunteered geographic information (VGI) and globally available RS
images. Building on this foundation, we introduce LHRS-Bot, an MLLM tailored
for RS image understanding through a novel multi-level vision-language
alignment strategy and a curriculum learning method. Comprehensive experiments
demonstrate that LHRS-Bot exhibits a profound understanding of RS images and
the ability to perform nuanced reasoning within the RS domain.
</p>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02545" title="Abstract">arXiv:2402.02545</a> [<a href="/pdf/2402.02545" title="Download PDF">pdf</a>, <a href="/format/2402.02545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Classification of Tennis Actions Using Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hovad%2C+E">Emil Hovad</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Hougaard-Jensen%2C+T">Therese Hougaard-Jensen</a> (2), 
<a href="/search/cs?searchtype=author&query=Clemmensen%2C+L+K+H">Line Katrine Harder Clemmensen</a> (2) ((1) Alexandra Instituttet A/S, Rued Langgaards Vej 7, 2300 K&#xf8;benhavn S, Denmark, (2) Department of Mathematics and Computer Science, Technical University of Denmark, Richard Petersens Plads, Building 324, 2800 Kgs. Lyngby, Denmark)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advances of deep learning makes it possible to identify specific
events in videos with greater precision. This has great relevance in sports
like tennis in order to e.g., automatically collect game statistics, or replay
actions of specific interest for game strategy or player improvements. In this
paper, we investigate the potential and the challenges of using deep learning
to classify tennis actions. Three models of different size, all based on the
deep learning architecture SlowFast were trained and evaluated on the academic
tennis dataset THETIS. The best models achieve a generalization accuracy of 74
%, demonstrating a good performance for tennis action classification. We
provide an error analysis for the best model and pinpoint directions for
improvement of tennis datasets in general. We discuss the limitations of the
data set, general limitations of current publicly available tennis data-sets,
and future steps needed to make progress.
</p>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02547" title="Abstract">arXiv:2402.02547</a> [<a href="/pdf/2402.02547" title="Download PDF">pdf</a>, <a href="/ps/2402.02547" title="Download PostScript">ps</a>, <a href="/format/2402.02547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integration of cognitive tasks into artificial general intelligence test  for large models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qu%2C+Y">Youzhi Qu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+C">Chen Wei</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+P">Penghui Du</a>, 
<a href="/search/cs?searchtype=author&query=Che%2C+W">Wenxin Che</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+Y">Yatao Bian</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+F">Feiyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Bin Hu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+K">Kai Du</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haiyan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jia Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Quanying Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">During the evolution of large models, performance evaluation is necessarily
performed on the intermediate models to assess their capabilities, and on the
well-trained model to ensure safety before practical application. However,
current model evaluations mainly rely on specific tasks and datasets, lacking a
united framework for assessing the multidimensional intelligence of large
models. In this perspective, we advocate for a comprehensive framework of
artificial general intelligence (AGI) test, aimed at fulfilling the testing
needs of large language models and multi-modal large models with enhanced
capabilities. The AGI test framework bridges cognitive science and natural
language processing to encompass the full spectrum of intelligence facets,
including crystallized intelligence, a reflection of amassed knowledge and
experience; fluid intelligence, characterized by problem-solving and adaptive
reasoning; social intelligence, signifying comprehension and adaptation within
multifaceted social scenarios; and embodied intelligence, denoting the ability
to interact with its physical environment. To assess the multidimensional
intelligence of large models, the AGI test consists of a battery of
well-designed cognitive tests adopted from human intelligence tests, and then
naturally encapsulates into an immersive virtual community. We propose that the
complexity of AGI testing tasks should increase commensurate with the
advancements in large models. We underscore the necessity for the
interpretation of test results to avoid false negatives and false positives. We
believe that cognitive science-inspired AGI tests will effectively guide the
targeted improvement of large models in specific dimensions of intelligence and
accelerate the integration of large models into human society.
</p>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02548" title="Abstract">arXiv:2402.02548</a> [<a href="/pdf/2402.02548" title="Download PDF">pdf</a>, <a href="/format/2402.02548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;What&#x27;s my model inside of?&quot;: Exploring the role of environments for  grounded natural language understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tamari%2C+R">Ronen Tamari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PhD Thesis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">In contrast to classical cognitive science which studied brains in isolation,
ecological approaches focused on the role of the body and environment in
shaping cognition. Similarly, in this thesis we adopt an ecological approach to
grounded natural language understanding (NLU) research. Grounded language
understanding studies language understanding systems situated in the context of
events, actions and precepts in naturalistic/simulated virtual environments.
Where classic research tends to focus on designing new models and optimization
methods while treating environments as given, we explore the potential of
environment design for improving data collection and model development. We
developed novel training and annotation approaches for procedural text
understanding based on text-based game environments. We also drew upon embodied
cognitive linguistics literature to propose a roadmap for grounded NLP
research, and to inform the development of a new benchmark for measuring the
progress of large language models on challenging commonsense reasoning tasks.
We leveraged the richer supervision provided by text-based game environments to
develop Breakpoint Transformers, a novel approach to modeling intermediate
semantic information in long narrative or procedural texts. Finally, we
integrated theories on the role of environments in collective human
intelligence to propose a design for AI-augmented "social thinking
environments" for knowledge workers like scientists.
</p>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02549" title="Abstract">arXiv:2402.02549</a> [<a href="/pdf/2402.02549" title="Download PDF">pdf</a>, <a href="/format/2402.02549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Large Language Models Table-based Fact-Checkers?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hangwen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+Q">Qingyi Si</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+P">Peng Fu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiping Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CSCWD 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Table-based Fact Verification (TFV) aims to extract the entailment relation
between statements and structured tables. Existing TFV methods based on
small-scaled models suffer from insufficient labeled data and weak zero-shot
ability. Recently, the appearance of Large Language Models (LLMs) has gained
lots of attraction in research fields. They have shown powerful zero-shot and
in-context learning abilities on several NLP tasks, but their potential on TFV
is still unknown. In this work, we implement a preliminary study about whether
LLMs are table-based fact-checkers. In detail, we design diverse prompts to
explore how the in-context learning can help LLMs in TFV, i.e., zero-shot and
few-shot TFV capability. Besides, we carefully design and construct TFV
instructions to study the performance gain brought by the instruction tuning of
LLMs. Experimental results demonstrate that LLMs can achieve acceptable results
on zero-shot and few-shot TFV with prompt engineering, while instruction-tuning
can stimulate the TFV capability significantly. We also make some valuable
findings about the format of zero-shot prompts and the number of in-context
examples. Finally, we analyze some possible directions to promote the accuracy
of TFV via LLMs, which is beneficial to further research of table reasoning.
</p>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02551" title="Abstract">arXiv:2402.02551</a> [<a href="/pdf/2402.02551" title="Download PDF">pdf</a>, <a href="/format/2402.02551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Obstacle Avoidance Deep Reinforcement Learning-Based Trajectory Planner  with Robust Low-Level Control for Robotic Manipulators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shahna%2C+M+H">Mehdi Heydari Shahna</a>, 
<a href="/search/cs?searchtype=author&query=Kolagar%2C+S+A+A">Seyed Adel Alizadeh Kolagar</a>, 
<a href="/search/cs?searchtype=author&query=Mattila%2C+J">Jouni Mattila</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted for possible publication in the IEEE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">In robotics, contemporary strategies are learning-based, characterized by a
complex black-box nature and a lack of interpretability, which may pose
challenges in ensuring stability and safety. To address these issues, we
propose integrating an obstacle-free deep reinforcement learning (DRL)
trajectory planner with a novel auto-tuning low- and joint-level control
strategy, all while actively engaging in the learning phase through
interactions with the environment. This approach circumvents the complexities
associated with computations while also addressing nonrepetitive and random
obstacle avoidance tasks. First, a model-free DRL agent to plan
velocity-bounded and obstacle-free motion is employed for a manipulator with
'n' degrees of freedom (DoF) in task space through joint-level reasoning. This
plan is then input into a robust subsystem-based adaptive controller, which
produces the necessary torques, while the Cuckoo Search Optimization (CSO)
algorithm enhances control gains to minimize the time required to reach, time
taken to stabilize, the maximum deviation from the desired value, and
persistent tracking error in the steady state. This approach guarantees that
position and velocity errors exponentially converge to zero, accounting for any
initial and end-point variations, unknown modeling errors, and external
disturbances. Theoretical assertions are validated through the presentation of
simulation outcomes.
</p>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02554" title="Abstract">arXiv:2402.02554</a> [<a href="/pdf/2402.02554" title="Download PDF">pdf</a>, <a href="/format/2402.02554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeSparsify: Adversarial Attack Against Token Sparsification Mechanisms  in Vision Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yehezkel%2C+O">Oryan Yehezkel</a>, 
<a href="/search/cs?searchtype=author&query=Zolfi%2C+A">Alon Zolfi</a>, 
<a href="/search/cs?searchtype=author&query=Baras%2C+A">Amit Baras</a>, 
<a href="/search/cs?searchtype=author&query=Elovici%2C+Y">Yuval Elovici</a>, 
<a href="/search/cs?searchtype=author&query=Shabtai%2C+A">Asaf Shabtai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Vision transformers have contributed greatly to advancements in the computer
vision domain, demonstrating state-of-the-art performance in diverse tasks
(e.g., image classification, object detection). However, their high
computational requirements grow quadratically with the number of tokens used.
Token sparsification techniques have been proposed to address this issue. These
techniques employ an input-dependent strategy, in which uninformative tokens
are discarded from the computation pipeline, improving the model's efficiency.
However, their dynamism and average-case assumption makes them vulnerable to a
new threat vector - carefully crafted adversarial examples capable of fooling
the sparsification mechanism, resulting in worst-case performance. In this
paper, we present DeSparsify, an attack targeting the availability of vision
transformers that use token sparsification mechanisms. The attack aims to
exhaust the operating system's resources, while maintaining its stealthiness.
Our evaluation demonstrates the attack's effectiveness on three token
sparsification techniques and examines the attack's transferability between
them and its effect on the GPU resources. To mitigate the impact of the attack,
we propose various countermeasures.
</p>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02555" title="Abstract">arXiv:2402.02555</a> [<a href="/pdf/2402.02555" title="Download PDF">pdf</a>, <a href="/format/2402.02555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizable Entity Grounding via Assistance of Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+L">Lu Qi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yi-Wen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lehan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+T">Tiancheng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiangtai Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+W">Weidong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming-Hsuan Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this work, we propose a novel approach to densely ground visual entities
from a long caption. We leverage a large multimodal model (LMM) to extract
semantic nouns, a class-agnostic segmentation model to generate entity-level
segmentation, and the proposed multi-modal feature fusion module to associate
each semantic noun with its corresponding segmentation mask. Additionally, we
introduce a strategy of encoding entity segmentation masks into a colormap,
enabling the preservation of fine-grained predictions from features of
high-resolution masks. This approach allows us to extract visual features from
low-resolution images using the CLIP vision encoder in the LMM, which is more
computationally efficient than existing approaches that use an additional
encoder for high-resolution images. Our comprehensive experiments demonstrate
the superiority of our method, outperforming state-of-the-art techniques on
three tasks, including panoptic narrative grounding, referring expression
segmentation, and panoptic segmentation.
</p>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02558" title="Abstract">arXiv:2402.02558</a> [<a href="/pdf/2402.02558" title="Download PDF">pdf</a>, <a href="/format/2402.02558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Robustness in Biomedical NLI Models: A Probing Approach for  Clinical Trials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mustafa%2C+A">Ata Mustafa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models have revolutionized various fields and industries, such
as Conversational AI, Content Generation, Information Retrieval, Business
Intelligence, and Medical, to name a few. One major application in the field of
medical is to analyze and investigate clinical trials for entailment
tasks.However, It has been observed that Large Language Models are susceptible
to shortcut learning, factual inconsistency, and performance degradation with
little variation in context. Adversarial and robust testing is performed to
ensure the integrity of models output. But, ambiguity still persists. In order
to ensure the integrity of the reasoning performed and investigate the model
has correct syntactic and semantic understanding probing is used. Here, I used
mnestic probing to investigate the Sci-five model, trained on clinical trial. I
investigated the model for feature learnt with respect to natural logic. To
achieve the target, I trained task specific probes. Used these probes to
investigate the final layers of trained model. Then, fine tuned the trained
model using iterative null projection. The results shows that model accuracy
improved. During experimentation, I observed that size of the probe has affect
on the fine tuning process.
</p>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02559" title="Abstract">arXiv:2402.02559</a> [<a href="/pdf/2402.02559" title="Download PDF">pdf</a>, <a href="/format/2402.02559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NavHint: Vision and Language Navigation Agent with a Hint Generator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Quan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Kordjamshidi%2C+P">Parisa Kordjamshidi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Existing work on vision and language navigation mainly relies on
navigation-related losses to establish the connection between vision and
language modalities, neglecting aspects of helping the navigation agent build a
deep understanding of the visual environment. In our work, we provide indirect
supervision to the navigation agent through a hint generator that provides
detailed visual descriptions. The hint generator assists the navigation agent
in developing a global understanding of the visual environment. It directs the
agent's attention toward related navigation details, including the relevant
sub-instruction, potential challenges in recognition and ambiguities in
grounding, and the targeted viewpoint description. To train the hint generator,
we construct a synthetic dataset based on landmarks in the instructions and
visible and distinctive objects in the visual environment. We evaluate our
method on the R2R and R4R datasets and achieve state-of-the-art on several
metrics. The experimental results demonstrate that generating hints not only
enhances the navigation performance but also helps improve the interpretability
of the agent's actions.
</p>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02561" title="Abstract">arXiv:2402.02561</a> [<a href="/pdf/2402.02561" title="Download PDF">pdf</a>, <a href="/format/2402.02561" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foundation Model Makes Clustering a Better Initialization for Active  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Han Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+C">Chuan Hong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Active learning selects the most informative samples from the unlabeled
dataset to annotate in the context of a limited annotation budget. While
numerous methods have been proposed for subsequent sample selection based on an
initialized model, scant attention has been paid to the indispensable phase of
active learning: selecting samples for model initialization. Most of the
previous studies resort to random sampling or naive clustering. However, random
sampling is prone to fluctuation, and naive clustering suffers from convergence
speed, particularly when dealing with high-dimensional data such as imaging
data. In this work, we propose to integrate foundation models with clustering
methods to select samples for active learning initialization. Foundation models
refer to those trained on massive datasets by the self-supervised paradigm and
capable of generating informative and compacted embeddings for various
downstream tasks. Leveraging these embeddings to replace raw features such as
pixel values, clustering quickly converges and identifies better initial
samples. For a comprehensive comparison, we included a classic
ImageNet-supervised model to acquire embeddings. Experiments on two clinical
tasks of image classification and segmentation demonstrated that foundation
model-based clustering efficiently pinpointed informative initial samples,
leading to models showcasing enhanced performance than the baseline methods. We
envisage that this study provides an effective paradigm for future active
learning.
</p>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02563" title="Abstract">arXiv:2402.02563</a> [<a href="/pdf/2402.02563" title="Download PDF">pdf</a>, <a href="/format/2402.02563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DefInt: A Default-interventionist Framework for Efficient Reasoning with  Hybrid Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shang%2C+Y">Yu Shang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yu Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+F">Fengli Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 10 figures, 14 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) have shown impressive emergent abilities in a
wide range of tasks, but still face challenges in handling complex reasoning
problems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT)
have predominately focused on enhancing accuracy, but overlook the rapidly
increasing token cost, which could be particularly problematic for open-ended
real-world tasks with huge solution spaces. Motivated by the dual process
theory of human cognition, we propose a Default-Interventionist framework
(DefInt) to unleash the synergistic potential of hybrid LLMs. By default,
DefInt uses smaller-scale language models to generate low-cost reasoning
thoughts, which resembles the fast intuitions produced by System 1. If the
intuitions are considered with low confidence, DefInt will invoke the
reflective reasoning of scaled-up language models as the intervention of System
2, which can override the default thoughts and rectify the reasoning process.
Experiments on five representative reasoning tasks show that DefInt
consistently achieves state-of-the-art reasoning accuracy and solution
diversity. More importantly, it substantially reduces the token cost by 49%-79%
compared to the second accurate baselines. Specifically, the open-ended tasks
have an average 75% token cost reduction. Code repo with all prompts will be
released upon publication.
</p>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02564" title="Abstract">arXiv:2402.02564</a> [<a href="/pdf/2402.02564" title="Download PDF">pdf</a>, <a href="/format/2402.02564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Truly Joint Neural Architecture for Segmentation and Parsing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Levi%2C+D+Y">Danit Yshaayahu Levi</a>, 
<a href="/search/cs?searchtype=author&query=Tsarfaty%2C+R">Reut Tsarfaty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Contemporary multilingual dependency parsers can parse a diverse set of
languages, but for Morphologically Rich Languages (MRLs), performance is
attested to be lower than other languages. The key challenge is that, due to
high morphological complexity and ambiguity of the space-delimited input
tokens, the linguistic units that act as nodes in the tree are not known in
advance. Pre-neural dependency parsers for MRLs subscribed to the joint
morpho-syntactic hypothesis, stating that morphological segmentation and
syntactic parsing should be solved jointly, rather than as a pipeline where
segmentation precedes parsing. However, neural state-of-the-art parsers to date
use a strict pipeline. In this paper we introduce a joint neural architecture
where a lattice-based representation preserving all morphological ambiguity of
the input is provided to an arc-factored model, which then solves the
morphological segmentation and syntactic parsing tasks at once. Our experiments
on Hebrew, a rich and highly ambiguous MRL, demonstrate state-of-the-art
performance on parsing, tagging and segmentation of the Hebrew section of UD,
using a single model. This proposed architecture is LLM-based and language
agnostic, providing a solid foundation for MRLs to obtain further performance
improvements and bridge the gap with other languages.
</p>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02566" title="Abstract">arXiv:2402.02566</a> [<a href="/pdf/2402.02566" title="Download PDF">pdf</a>, <a href="/format/2402.02566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STAGE: Scalable and Traversability-Aware Graph based Exploration Planner  for Dynamically Varying Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patel%2C+A">Akash Patel</a>, 
<a href="/search/cs?searchtype=author&query=Saucedo%2C+M+A+V">Mario A V Saucedo</a>, 
<a href="/search/cs?searchtype=author&query=Kanellakis%2C+C">Christoforos Kanellakis</a>, 
<a href="/search/cs?searchtype=author&query=Nikolakopoulos%2C+G">George Nikolakopoulos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this article, we propose a novel navigation framework that leverages a two
layered graph representation of the environment for efficient large-scale
exploration, while it integrates a novel uncertainty awareness scheme to handle
dynamic scene changes in previously explored areas. The framework is structured
around a novel goal oriented graph representation, that consists of, i) the
local sub-graph and ii) the global graph layer respectively. The local
sub-graphs encode local volumetric gain locations as frontiers, based on the
direct pointcloud visibility, allowing fast graph building and path planning.
Additionally, the global graph is build in an efficient way, using node-edge
information exchange only on overlapping regions of sequential sub-graphs.
Different from the state-of-the-art graph based exploration methods, the
proposed approach efficiently re-uses sub-graphs built in previous iterations
to construct the global navigation layer. Another merit of the proposed scheme
is the ability to handle scene changes (e.g. blocked pathways), adaptively
updating the obstructed part of the global graph from traversable to
not-traversable. This operation involved oriented sample space of a path
segment in the global graph layer, while removing the respective edges from
connected nodes of the global graph in cases of obstructions. As such, the
exploration behavior is directing the robot to follow another route in the
global re-positioning phase through path-way updates in the global graph.
Finally, we showcase the performance of the method both in simulation runs as
well as deployed in real-world scene involving a legged robot carrying camera
and lidar sensor.
</p>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02567" title="Abstract">arXiv:2402.02567</a> [<a href="/pdf/2402.02567" title="Download PDF">pdf</a>, <a href="/ps/2402.02567" title="Download PostScript">ps</a>, <a href="/format/2402.02567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> First order complexity of finite random structures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Demin%2C+D">Danila Demin</a>, 
<a href="/search/cs?searchtype=author&query=Zhukovskii%2C+M">Maksim Zhukovskii</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO); Logic (math.LO); Probability (math.PR)

</div>
<p class="mathjax">For a sequence of random structures with $n$-element domains over a
relational signature, we define its first order (FO) complexity as a certain
subset in the Banach space $\ell^{\infty}/c_0$. The well-known FO zero-one law
and FO convergence law correspond to FO complexities equal to $\{0,1\}$ and a
subset of $\mathbb{R}$, respectively. We present a hierarchy of FO complexity
classes, introduce a stochastic FO reduction that allows to transfer complexity
results between different random structures, and deduce using this tool several
new logical limit laws for binomial random structures. Finally, we introduce a
conditional distribution on graphs, subject to a FO sentence $\varphi$, that
generalises certain well-known random graph models, show instances of this
distribution for every complexity class, and prove that the set of all
$\varphi$ validating 0--1 law is not recursively enumerable.
</p>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02570" title="Abstract">arXiv:2402.02570</a> [<a href="/pdf/2402.02570" title="Download PDF">pdf</a>, <a href="/format/2402.02570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gazebo Plants: Simulating Plant-Robot Interaction with Cosserat Rods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Junchen Deng</a>, 
<a href="/search/cs?searchtype=author&query=Marri%2C+S">Samhita Marri</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+J">Jonathan Klein</a>, 
<a href="/search/cs?searchtype=author&query=Pa%C5%82ubicki%2C+W">Wojtek Pa&#x142;ubicki</a>, 
<a href="/search/cs?searchtype=author&query=Pirk%2C+S">S&#xf6;ren Pirk</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhary%2C+G">Girish Chowdhary</a>, 
<a href="/search/cs?searchtype=author&query=Michels%2C+D+L">Dominik L. Michels</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Upon request, we are happy to share our GazeboPlants plugin open-source (MPL 2.0)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Robotic harvesting has the potential to positively impact agricultural
productivity, reduce costs, improve food quality, enhance sustainability, and
to address labor shortage. In the rapidly advancing field of agricultural
robotics, the necessity of training robots in a virtual environment has become
essential. Generating training data to automatize the underlying computer
vision tasks such as image segmentation, object detection and classification,
also heavily relies on such virtual environments as synthetic data is often
required to overcome the shortage and lack of variety of real data sets.
However, physics engines commonly employed within the robotics community, such
as ODE, Simbody, Bullet, and DART, primarily support motion and collision
interaction of rigid bodies. This inherent limitation hinders experimentation
and progress in handling non-rigid objects such as plants and crops. In this
contribution, we present a plugin for the Gazebo simulation platform based on
Cosserat rods to model plant motion. It enables the simulation of plants and
their interaction with the environment. We demonstrate that, using our plugin,
users can conduct harvesting simulations in Gazebo by simulating a robotic arm
picking fruits and achieve results comparable to real-world experiments.
</p>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02571" title="Abstract">arXiv:2402.02571</a> [<a href="/pdf/2402.02571" title="Download PDF">pdf</a>, <a href="/format/2402.02571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple Stochastic Stopping Games: A Generator and Benchmark Library
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rudich%2C+A">Avi Rudich</a>, 
<a href="/search/cs?searchtype=author&query=Rudich%2C+I">Isaac Rudich</a>, 
<a href="/search/cs?searchtype=author&query=Rue%2C+R">Rachel Rue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 1 figure, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Simple Stochastic Games (SSGs) were introduced by Anne Condon in 1990, as the
simplest version of Stochastic Games for which there is no known
polynomial-time algorithm. Condon showed that Stochastic Games are
polynomial-time reducible to SSGs, which in turn are polynomial-time reducible
to Stopping Games. SSGs are games where all decisions are binary and every move
has a random outcome with a known probability distribution. Stopping Games are
SSGs that are guaranteed to terminate. There are many algorithms for SSGs, most
of which are fast in practice, but they all lack theoretical guarantees for
polynomial-time convergence. The pursuit of a polynomial-time algorithm for
SSGs is an active area of research. This paper is intended to support such
research by making it easier to study the graphical structure of SSGs. Our
contributions are: (1) a generating algorithm for Stopping Games, (2) a proof
that the algorithm can generate any game, (3) a list of additional
polynomial-time reductions that can be made to Stopping Games, (4) an open
source generator for generating fully reduced instances of Stopping Games that
comes with instructions and is fully documented, (5) a benchmark set of such
instances, (6) and an analysis of how two main algorithm types perform on our
benchmark set.
</p>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02572" title="Abstract">arXiv:2402.02572</a> [<a href="/pdf/2402.02572" title="Download PDF">pdf</a>, <a href="/format/2402.02572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Quantitative Discourse Analysis of Asian Workers in the US Historical  Newspapers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jaihyun Park</a>, 
<a href="/search/cs?searchtype=author&query=Cordell%2C+R">Ryan Cordell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3rd International Conference on Natural Language Processing for Digital Humanities (NLP4DH)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Warning: This paper contains examples of offensive language targetting
marginalized population. The digitization of historical texts invites
researchers to explore the large-scale corpus of historical texts with
computational methods. In this study, we present computational text analysis on
a relatively understudied topic of how Asian workers are represented in
historical newspapers in the United States. We found that the word "coolie" was
semantically different in some States (e.g., Massachusetts, Rhode Island,
Wyoming, Oklahoma, and Arkansas) with the different discourses around coolie.
We also found that then-Confederate newspapers and then-Union newspapers formed
distinctive discourses by measuring over-represented words. Newspapers from
then-Confederate States associated coolie with slavery-related words. In
addition, we found Asians were perceived to be inferior to European immigrants
and subjected to the target of racism. This study contributes to supplementing
the qualitative analysis of racism in the United States with quantitative
discourse analysis.
</p>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02574" title="Abstract">arXiv:2402.02574</a> [<a href="/pdf/2402.02574" title="Download PDF">pdf</a>, <a href="/format/2402.02574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatio-temporal Prompting Network for Robust Video Feature Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+G">Guanxiong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaoyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jiankang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zafeiriou%2C+S">Stefanos Zafeiriou</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+Y">Yang Hua</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 International Conference on Computer Vision (ICCV)
  13541-13551
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Frame quality deterioration is one of the main challenges in the field of
video understanding. To compensate for the information loss caused by
deteriorated frames, recent approaches exploit transformer-based integration
modules to obtain spatio-temporal information. However, these integration
modules are heavy and complex. Furthermore, each integration module is
specifically tailored for its target task, making it difficult to generalise to
multiple tasks. In this paper, we present a neat and unified framework, called
Spatio-Temporal Prompting Network (STPN). It can efficiently extract robust and
accurate video features by dynamically adjusting the input features in the
backbone network. Specifically, STPN predicts several video prompts containing
spatio-temporal information of neighbour frames. Then, these video prompts are
prepended to the patch embeddings of the current frame as the updated input for
video feature extraction. Moreover, STPN is easy to generalise to various video
tasks because it does not contain task-specific modules. Without bells and
whistles, STPN achieves state-of-the-art performance on three widely-used
datasets for different video understanding tasks, i.e., ImageNetVID for video
object detection, YouTubeVIS for video instance segmentation, and GOT-10k for
visual object tracking. Code is available at
https://github.com/guanxiongsun/vfe.pytorch.
</p>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02576" title="Abstract">arXiv:2402.02576</a> [<a href="/pdf/2402.02576" title="Download PDF">pdf</a>, <a href="/format/2402.02576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AM-CCA: A Memory-Driven System for Fine-Grain and Dynamic Computations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chandio%2C+B+Q">Bibrak Qamar Chandio</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+P">Prateek Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Brodowicz%2C+M">Maciej Brodowicz</a>, 
<a href="/search/cs?searchtype=author&query=Sterling%2C+T">Thomas Sterling</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">Techniques of computer systems that have been successfully deployed for dense
regular workloads fall short of achieving their goals of scalability and
efficiency when applied to irregular and dynamic applications. This is
primarily due to the discontent between the multiple layers of the system
design from hardware architecture, execution model, programming model, to
data-structure and application code. The paper approaches this issue by
addressing all layers of the system design. It presents and argues key design
principles needed for scalable and efficient dynamic graph processing, and from
which it builds: 1) a fine-grain memory driven architecture that supports
asynchronous active messages, 2) a programming and execution model that allows
spawning tasks from within the data-parallelism, 3) and a data-structure that
parallelizes vertex object across many compute cells and yet provides a single
programming abstraction to the data object.
<br />Simulated experimental results show performance gain of geomean $2.38 \times$
against an state-of-the-art similar system for graph traversals and yet being
able to natively support dynamic graph processing. It uses programming
abstractions of actions, introduces new dynamic graph storage scheme, and
message delivery mechanisms with continuations that contain post-completion
actions. Continuations seamlessly adjusts, prior or running, execution to
mutations in the input graph and enable dynamic graph processing.
</p>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02582" title="Abstract">arXiv:2402.02582</a> [<a href="/pdf/2402.02582" title="Download PDF">pdf</a>, <a href="/format/2402.02582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the development of an application for the compilation of global sea  level changes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Odhavji%2C+M">Mihir Odhavji</a>, 
<a href="/search/cs?searchtype=author&query=Oliveira%2C+M+A">Maria Alexandra Oliveira</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+J+N">Jo&#xe3;o Nuno Silva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">There is a lot of data about mean sea level variation from studies conducted
around the globe. This data is dispersed, lacks organization along with
standardization, and in most cases, it is not available online. In some
instances, when it is available, it is often in unpractical ways and different
formats. Analyzing it would be inefficient and very time-consuming. In addition
to all of that, to successfully process spatial-temporal data, the user has to
be equipped with particular skills and tools used for geographic data like
PostGIS, PostgreSQL and GeoAlchemy. The presented solution is to develop a web
application that solves some of the issues faced by researchers. The web
application allows the user to add data, be it through forms in a browser or
automated with the help of an API. The application also assists with data
querying, processing and visualization by making tables, showing maps and
drawing graphs. Comparing data points from different areas and publications is
also made possible. The implemented web application permits the query and
storage of spatial-temporal data about mean sea level variation in a
simplified, easily accessible and user-friendly manner. It will also allow the
realization of more global studies.
</p>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02583" title="Abstract">arXiv:2402.02583</a> [<a href="/pdf/2402.02583" title="Download PDF">pdf</a>, <a href="/format/2402.02583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffEditor: Boosting Accuracy and Flexibility on Diffusion-based Image  Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mou%2C+C">Chong Mou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xintao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jiechong Song</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+Y">Ying Shan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jian Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large-scale Text-to-Image (T2I) diffusion models have revolutionized image
generation over the last few years. Although owning diverse and high-quality
generation capabilities, translating these abilities to fine-grained image
editing remains challenging. In this paper, we propose DiffEditor to rectify
two weaknesses in existing diffusion-based image editing: (1) in complex
scenarios, editing results often lack editing accuracy and exhibit unexpected
artifacts; (2) lack of flexibility to harmonize editing operations, e.g.,
imagine new content. In our solution, we introduce image prompts in
fine-grained image editing, cooperating with the text prompt to better describe
the editing content. To increase the flexibility while maintaining content
consistency, we locally combine stochastic differential equation (SDE) into the
ordinary differential equation (ODE) sampling. In addition, we incorporate
regional score-based gradient guidance and a time travel strategy into the
diffusion sampling, further improving the editing quality. Extensive
experiments demonstrate that our method can efficiently achieve
state-of-the-art performance on various fine-grained image editing tasks,
including editing within a single image (e.g., object moving, resizing, and
content dragging) and across images (e.g., appearance replacing and object
pasting). Our source code is released at
https://github.com/MC-E/DragonDiffusion.
</p>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02586" title="Abstract">arXiv:2402.02586</a> [<a href="/pdf/2402.02586" title="Download PDF">pdf</a>, <a href="/format/2402.02586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ClipFormer: Key-Value Clipping of Transformers on Memristive Crossbars  for Write Noise Mitigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharjee%2C+A">Abhiroop Bhattacharjee</a>, 
<a href="/search/cs?searchtype=author&query=Moitra%2C+A">Abhishek Moitra</a>, 
<a href="/search/cs?searchtype=author&query=Panda%2C+P">Priyadarshini Panda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 10 figures, 3 tables, 1 appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Transformers have revolutionized various real-world applications from natural
language processing to computer vision. However, traditional von-Neumann
computing paradigm faces memory and bandwidth limitations in accelerating
transformers owing to their massive model sizes. To this end, In-memory
Computing (IMC) crossbars based on Non-volatile Memories (NVMs), due to their
ability to perform highly parallelized Matrix-Vector-Multiplications (MVMs)
with high energy-efficiencies, have emerged as a promising solution for
accelerating transformers. However, analog MVM operations in crossbars
introduce non-idealities, such as stochastic read &amp; write noise, which affect
the inference accuracy of the deployed transformers. Specifically, we find
pre-trained Vision Transformers (ViTs) to be vulnerable on crossbars due to the
impact of write noise on the dynamically-generated Key (K) and Value (V)
matrices in the attention layers, an effect not accounted for in prior studies.
We, thus, propose ClipFormer, a transformation on the K and V matrices during
inference, to boost the non-ideal accuracies of pre-trained ViT models.
ClipFormer requires no additional hardware and training overhead and is
amenable to transformers deployed on any memristive crossbar platform. Our
experiments on Imagenet-1k dataset using pre-trained DeiT-S transformers,
subjected to standard training and variation-aware-training, show &gt;10-40%
higher non-ideal accuracies at the high write noise regime by applying
ClipFormer.
</p>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02588" title="Abstract">arXiv:2402.02588</a> [<a href="/pdf/2402.02588" title="Download PDF">pdf</a>, <a href="/format/2402.02588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controller Synthesis from Noisy-Input Noisy-Output Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+L">Lidong Li</a>, 
<a href="/search/eess?searchtype=author&query=Bisoffi%2C+A">Andrea Bisoffi</a>, 
<a href="/search/eess?searchtype=author&query=De+Persis%2C+C">Claudio De Persis</a>, 
<a href="/search/eess?searchtype=author&query=Monshizadeh%2C+N">Nima Monshizadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We consider the problem of synthesizing a dynamic output-feedback controller
for a linear system, using solely input-output data corrupted by measurement
noise. To handle input-output data, an auxiliary representation of the original
system is introduced. By exploiting the structure of the auxiliary system, we
design a controller that robustly stabilizes all possible systems consistent
with data. Notably, we also provide a novel solution to extend the results to
generic multi-input multi-output systems. The findings are illustrated by
numerical examples.
</p>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02591" title="Abstract">arXiv:2402.02591</a> [<a href="/pdf/2402.02591" title="Download PDF">pdf</a>, <a href="/ps/2402.02591" title="Download PostScript">ps</a>, <a href="/format/2402.02591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the performance of phonetic algorithms in microtext normalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Doval%2C+Y">Yerai Doval</a>, 
<a href="/search/cs?searchtype=author&query=Vilares%2C+M">Manuel Vilares</a>, 
<a href="/search/cs?searchtype=author&query=Vilares%2C+J">Jes&#xfa;s Vilares</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in journal Expert Systems with Applications
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Expert Systems with Applications, Volume 113, 2018, Pages 213-222
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">User-generated content published on microblogging social networks constitutes
a priceless source of information. However, microtexts usually deviate from the
standard lexical and grammatical rules of the language, thus making its
processing by traditional intelligent systems very difficult. As an answer,
microtext normalization consists in transforming those non-standard microtexts
into standard well-written texts as a preprocessing step, allowing traditional
approaches to continue with their usual processing. Given the importance of
phonetic phenomena in non-standard text formation, an essential element of the
knowledge base of a normalizer would be the phonetic rules that encode these
phenomena, which can be found in the so-called phonetic algorithms.
<br />In this work we experiment with a wide range of phonetic algorithms for the
English language. The aim of this study is to determine the best phonetic
algorithms within the context of candidate generation for microtext
normalization. In other words, we intend to find those algorithms that taking
as input non-standard terms to be normalized allow us to obtain as output the
smallest possible sets of normalization candidates which still contain the
corresponding target standard words. As it will be stated, the choice of the
phonetic algorithm will depend heavily on the capabilities of the candidate
selection mechanism which we usually find at the end of a microtext
normalization pipeline. The faster it can make the right choices among big
enough sets of candidates, the more we can sacrifice on the precision of the
phonetic algorithms in favour of coverage in order to increase the overall
performance of the normalization system.
<br />KEYWORDS: microtext normalization; phonetic algorithm; fuzzy matching;
Twitter; texting
</p>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02592" title="Abstract">arXiv:2402.02592</a> [<a href="/pdf/2402.02592" title="Download PDF">pdf</a>, <a href="/format/2402.02592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified Training of Universal Time Series Forecasting Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Woo%2C+G">Gerald Woo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chenghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Akshat Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Caiming Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Savarese%2C+S">Silvio Savarese</a>, 
<a href="/search/cs?searchtype=author&query=Sahoo%2C+D">Doyen Sahoo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Deep learning for time series forecasting has traditionally operated within a
one-model-per-dataset framework, limiting its potential to leverage the
game-changing impact of large pre-trained models. The concept of universal
forecasting, emerging from pre-training on a vast collection of time series
datasets, envisions a single Large Time Series Model capable of addressing
diverse downstream forecasting tasks. However, constructing such a model poses
unique challenges specific to time series data: i) cross-frequency learning,
ii) accommodating an arbitrary number of variates for multivariate time series,
and iii) addressing the varying distributional properties inherent in
large-scale data. To address these challenges, we present novel enhancements to
the conventional time series Transformer architecture, resulting in our
proposed Masked Encoder-based Universal Time Series Forecasting Transformer
(Moirai). Trained on our newly introduced Large-scale Open Time Series Archive
(LOTSA) featuring over 27B observations across nine domains, Moirai achieves
competitive or superior performance as a zero-shot forecaster when compared to
full-shot models. Code, model weights, and data will be released.
</p>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02593" title="Abstract">arXiv:2402.02593</a> [<a href="/pdf/2402.02593" title="Download PDF">pdf</a>, <a href="/format/2402.02593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Continuously Differentiable Activation Functions for Learning  in Quantized Noisy Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shah%2C+V">Vivswan Shah</a>, 
<a href="/search/cs?searchtype=author&query=Youngblood%2C+N">Nathan Youngblood</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Real-world analog systems intrinsically suffer from noise that can impede
model convergence and accuracy on a variety of deep learning models. We
demonstrate that differentiable activations like GELU and SiLU enable robust
propagation of gradients which help to mitigate analog quantization error that
is ubiquitous to all analog systems. We perform analysis and training of
convolutional, linear, and transformer networks in the presence of quantized
noise. Here, we are able to demonstrate that continuously differentiable
activation functions are significantly more noise resilient over conventional
rectified activations. As in the case of ReLU, the error in gradients are 100x
higher than those in GELU near zero. Our findings provide guidance for
selecting appropriate activations to realize performant and reliable hardware
implementations across several machine learning domains such as computer
vision, signal processing, and beyond.
</p>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02596" title="Abstract">arXiv:2402.02596</a> [<a href="/pdf/2402.02596" title="Download PDF">pdf</a>, <a href="/format/2402.02596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual Interior-Point Optimization Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Klamkin%2C+M">Michael Klamkin</a>, 
<a href="/search/cs?searchtype=author&query=Tanneau%2C+M">Mathieu Tanneau</a>, 
<a href="/search/cs?searchtype=author&query=Van+Hentenryck%2C+P">Pascal Van Hentenryck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper introduces Dual Interior Point Learning (DIPL) and Dual
Supergradient Learning (DSL) to learn dual feasible solutions to parametric
linear programs with bounded variables, which are pervasive across many
industries. DIPL mimics a novel dual interior point algorithm while DSL mimics
classical dual supergradient ascent. DIPL and DSL ensure dual feasibility by
predicting dual variables associated with the constraints then exploiting the
flexibility of the duals of the bound constraints. DIPL and DSL complement
existing primal learning methods by providing a certificate of quality. They
are shown to produce high-fidelity dual-feasible solutions to large-scale
optimal power flow problems providing valid dual bounds under 0.5% optimality
gap.
</p>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02597" title="Abstract">arXiv:2402.02597</a> [<a href="/pdf/2402.02597" title="Download PDF">pdf</a>, <a href="/ps/2402.02597" title="Download PostScript">ps</a>, <a href="/format/2402.02597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient simulation strategy for PCM-based cold-energy storage systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bejarano%2C+G">G. Bejarano</a>, 
<a href="/search/eess?searchtype=author&query=Vargas%2C+M">M. Vargas</a>, 
<a href="/search/eess?searchtype=author&query=Ortega%2C+M+G">M. G. Ortega</a>, 
<a href="/search/eess?searchtype=author&query=Casta%C3%B1o%2C+F">F.Casta&#xf1;o</a>, 
<a href="/search/eess?searchtype=author&query=Normey-Rico%2C+J+E">J. E. Normey-Rico</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 14 figures. Preprint submitted to Applied Thermal Engineering
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This work addresses computationally efficient simulation of a novel thermal
energy storage (TES) system based on phase change material (PCM), designed to
complement a vapour-compression refrigeration system. A recently proposed
dynamic discrete model of the PCM spherical nodules within the TES tank is
taken as starting point, where the number of layers is shown to represent a
trade-off between accuracy and computational load. Furthermore, the TES model
is intended to be combined with that of the refrigeration cycle to address
efficient cold-energy management strategies, where computation time is a hard
constraint. Therefore, a significant reduction on the computational load is
required. A computationally affordable discrete model is proposed, which holds
the same layered structure while applying a much loose-fitting time
discretisation. Comparative simulations show that the time-efficient discrete
model incurs minor inaccuracy whereas the computational load is drastically
reduced.
</p>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02598" title="Abstract">arXiv:2402.02598</a> [<a href="/pdf/2402.02598" title="Download PDF">pdf</a>, <a href="/format/2402.02598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthesizing Follow-Up Drive Data for Enhanced Road Safety in  Intelligent Driving Function Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schick%2C+N">Nico Schick</a>, 
<a href="/search/cs?searchtype=author&query=%C4%8Ci%C4%8Dak%2C+F">Franjo &#x10c;i&#x10d;ak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This study underscores the vital importance of intelligent driving functions
in enhancing road safety and driving comfort. Central to our research is the
challenge of obtaining sufficient test data for evaluating these functions,
especially in high-risk, safety-critical driving scenarios. Such scenarios
often suffer from a dearth of available data, primarily due to their inherent
complexity and the risks involved.
<br />Addressing this gap, our research introduces a novel methodology designed to
create a wide array of diverse and realistic safety-critical driving scenarios.
This approach significantly broadens the testing spectrum for driver assistance
systems and autonomous vehicle functions. We particularly focus on the
follow-up drive scenario due to its high relevance in practical applications.
Here, vehicle movements are intricately modeled using kinematic equations,
incorporating factors like driver reaction times. We vary parameters to
generate a spectrum of plausible driving scenarios.
<br />The utilization of the Difference Space Stopping (DSS) metric is a pivotal
element in our research. This metric plays a crucial role in the safety
evaluation of follow-up drives, facilitating a more thorough and comprehensive
validation process. By doing so, our methodology enhances the reliability and
safety assessment of driver assistance and autonomous driving systems,
specifically tailored for the most challenging and safety-critical scenarios.
</p>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02599" title="Abstract">arXiv:2402.02599</a> [<a href="/pdf/2402.02599" title="Download PDF">pdf</a>, <a href="/format/2402.02599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modelling and cooling power control of a TES-backed-up  vapour-compression refrigeration system
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rodr%C3%ADguez%2C+D">D. Rodr&#xed;guez</a>, 
<a href="/search/eess?searchtype=author&query=Bejarano%2C+G">G. Bejarano</a>, 
<a href="/search/eess?searchtype=author&query=Vargas%2C+M">M. Vargas</a>, 
<a href="/search/eess?searchtype=author&query=Lemos%2C+J+M">J. M. Lemos</a>, 
<a href="/search/eess?searchtype=author&query=Ortega%2C+M+G">M. G. Ortega</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 20 figures. Preprint submitted to Applied Thermal Engineering
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This work addresses the modelling, power control, and optimization of a
thermal energy storage (TES) system combined with a vapour-compression
refrigeration facility based on phase change materials (PCM). Given a novel
design of a PCM-based TES tank and its interconnection with an existing
refrigeration system, the joint dynamic modelling is first studied, exploring
the different time scales that coexist at the interconnected system. Diverse
operating modes are defined, according to the intended use of the TES tank as a
cold-energy buffer to decouple cooling demand and production, whereas the
static characteristic and power limits are calculated and show the high
coupling between the main cooling powers involved (TES charging/discharging
power, and direct power production at the evaporator). In this light, a
decoupling control strategy is proposed, where the low-level controllers are
simply PI regulators and the refrigerant/secondary mass flows are considered as
virtual manipulated variables, applying a feedforward-based cascade strategy.
The control performance is evaluated through a thorough simulation that
includes all operating modes, where the reference tracking is shown to be fast
and reliable enough to address high-level scheduling strategies, where the
references on the main cooling powers are intended to be imposed considering
economic and efficiency criteria.
</p>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02600" title="Abstract">arXiv:2402.02600</a> [<a href="/pdf/2402.02600" title="Download PDF">pdf</a>, <a href="/format/2402.02600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evading Deep Learning-Based Malware Detectors via Obfuscation: A Deep  Reinforcement Learning Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Etter%2C+B">Brian Etter</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J+L">James Lee Hu</a>, 
<a href="/search/cs?searchtype=author&query=Ebrahimi%2C+M">Mohammedreza Ebrahimi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weifeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hsinchun Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Adversarial Malware Generation (AMG), the generation of adversarial malware
variants to strengthen Deep Learning (DL)-based malware detectors has emerged
as a crucial tool in the development of proactive cyberdefense. However, the
majority of extant works offer subtle perturbations or additions to executable
files and do not explore full-file obfuscation. In this study, we show that an
open-source encryption tool coupled with a Reinforcement Learning (RL)
framework can successfully obfuscate malware to evade state-of-the-art malware
detection engines and outperform techniques that use advanced modification
methods. Our results show that the proposed method improves the evasion rate
from 27%-49% compared to widely-used state-of-the-art reinforcement
learning-based methods.
</p>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02602" title="Abstract">arXiv:2402.02602</a> [<a href="/pdf/2402.02602" title="Download PDF">pdf</a>, <a href="/ps/2402.02602" title="Download PostScript">ps</a>, <a href="/format/2402.02602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Models of High-Level Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arellanes%2C+D">Damian Arellanes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Classical models of computation have been successful in capturing the very
essence of individual computing devices. Although they are useful to understand
computability power and limitations in the small, such models are not suitable
to study large-scale complex computations. Accordingly, plenty of formalisms
have been proposed in the last half century as an attempt to raise the level of
abstraction, with the aim of describing not only a single computing device but
interactions among a collection of them. In this paper, we encompass such
formalisms into a common framework which we refer to as Models of High-Level
Computation. We particularly discuss the semantics, some of the key properties,
paradigms and future directions of such models.
</p>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02603" title="Abstract">arXiv:2402.02603</a> [<a href="/pdf/2402.02603" title="Download PDF">pdf</a>, <a href="/ps/2402.02603" title="Download PostScript">ps</a>, <a href="/format/2402.02603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Review of Full-Sized Autonomous Racing Vehicle Sensor Architecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mar%2C+M">Manuel Mar</a>, 
<a href="/search/cs?searchtype=author&query=Chellapandi%2C+V">Vishnu Chellapandi</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Liangqi Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dietz%2C+E">Eric Dietz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In the landscape of technological innovation, autonomous racing is a dynamic
and challenging domain that not only pushes the limits of technology, but also
plays a crucial role in advancing and fostering a greater acceptance of
autonomous systems. This paper thoroughly explores challenges and advances in
autonomous racing vehicle design and performance, focusing on Roborace and the
Indy Autonomous Challenge (IAC). This review provides a detailed analysis of
sensor setups, architectural nuances, and test metrics on these cutting-edge
platforms. In Roborace, the evolution from Devbot 1.0 to Robocar and Devbot 2.0
is detailed, revealing insights into sensor configurations and performance
outcomes. The examination extends to the IAC, which is dedicated to high-speed
self-driving vehicles, emphasizing developmental trajectories and sensor
adaptations. By reviewing these platforms, the analysis provides valuable
insight into autonomous driving racing, contributing to a broader understanding
of sensor architectures and the challenges faced. This review supports future
advances in full-scale autonomous racing technology.
</p>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02607" title="Abstract">arXiv:2402.02607</a> [<a href="/pdf/2402.02607" title="Download PDF">pdf</a>, <a href="/format/2402.02607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flexible Non-interactive Short-term Implicit Certificate Generation for  VANETs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Rui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yun Lu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Jianping Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">A leading industry standard for secure and trusted communication in vehicular
ad-hoc networks (VANETs) is the Security Credential Management System (SCMS).
It uses anonymous certificates, functioning as pseudonyms, to preserve the
privacy of vehicles. With the rapid development of advanced applications in
VANETs, such as crowdsensing and federated learning, vehicles need to
communicate with each other or infrastructures more frequently, leading to a
higher demand for pseudonyms. However, the current approach of certificate
provisioning in SCMS is not able to fully support pseudonyms, due to storage
limitation, cost of connectivity establishment, and communication overhead of
certificate downloading. To tackle this challenge, we propose a non-interactive
approach for SCMS, allowing vehicles themselves to generate short-term key
pairs and anonymous implicit certificates. Our evaluation and comparison with
previous work show that our solution not only effectively reduces the
communication cost, but also grants vehicles greater flexibility in certificate
generation and use. On the technical side, to the best of our knowledge, this
is the first work which (1) applies sanitizable signature for non-interactive
anonymous certificate generation, and (2) is specifically designed for SCMS,
which opens up possibilities for extensions and applications in industry.
</p>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02608" title="Abstract">arXiv:2402.02608</a> [<a href="/pdf/2402.02608" title="Download PDF">pdf</a>, <a href="/format/2402.02608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Inverse Reinforcement Learning with Expert Bootstrapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">David Wu</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+S">Sanjiban Choudhury</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Existing inverse reinforcement learning methods (e.g. MaxEntIRL, $f$-IRL)
search over candidate reward functions and solve a reinforcement learning
problem in the inner loop. This creates a rather strange inversion where a
harder problem, reinforcement learning, is in the inner loop of a presumably
easier problem, imitation learning. In this work, we show that better
utilization of expert demonstrations can reduce the need for hard exploration
in the inner RL loop, hence accelerating learning. Specifically, we propose two
simple recipes: (1) placing expert transitions into the replay buffer of the
inner RL algorithm (e.g. Soft-Actor Critic) which directly informs the learner
about high reward states instead of forcing the learner to discover them
through extensive exploration, and (2) using expert actions in Q value
bootstrapping in order to improve the target Q value estimates and more
accurately describe high value expert states. Our methods show significant
gains over a MaxEntIRL baseline on the benchmark MuJoCo suite of tasks,
speeding up recovery to 70\% of deterministic expert performance by 2.13x on
HalfCheetah-v2, 2.6x on Ant-v2, 18x on Hopper-v2, and 3.36x on Walker2d-v2.
</p>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02611" title="Abstract">arXiv:2402.02611</a> [<a href="/pdf/2402.02611" title="Download PDF">pdf</a>, <a href="/format/2402.02611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial  Reasoning Problems?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mittal%2C+C">Chinmay Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Kartik%2C+K">Krishna Kartik</a>, 
<a href="/search/cs?searchtype=author&query=Mausam">Mausam</a>, 
<a href="/search/cs?searchtype=author&query=Singla%2C+P">Parag Singla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent works have explored the use of LLMs for reasoning tasks focussing on
relatively simple problems, such as logical question answering. In our work, we
wish to tackle more complicated problems, significantly expanding the
capabilities of these models. Particularly, we explore whether LLMs can solve
challenging first-order combinatorial reasoning problems, an example being the
popular puzzle Sudoku. These problems have an underlying first-order structure
described by a general description in natural language and can be instantiated
to instances of varying sizes. Moreover these problems are computationally
intensive requiring several reasoning steps to reach the solution. We present
PuzzleBench a dataset of 31 such challenging puzzles. We observe that LLMs even
when aided by symbolic solvers perform rather poorly on our benchmark. In
response we propose a new approach, Puzzle-LM which combines LLMs with both
symbolic solvers and program interpreters enabling them to reason about such
challenging problems. We also show how feedback from smaller solved instances
can help improve this reasoning ability.
</p>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02612" title="Abstract">arXiv:2402.02612</a> [<a href="/pdf/2402.02612" title="Download PDF">pdf</a>, <a href="/format/2402.02612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Explicit-Input Assistance for Teleoperation in Clutter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Walker%2C+N">Nick Walker</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xuning Yang</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+A">Animesh Garg</a>, 
<a href="/search/cs?searchtype=author&query=Cakmak%2C+M">Maya Cakmak</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+D">Dieter Fox</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez-D%27Arpino%2C+C">Claudia P&#xe9;rez-D&#x27;Arpino</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The performance of prediction-based assistance for robot teleoperation
degrades in unseen or goal-rich environments due to incorrect or
quickly-changing intent inferences. Poor predictions can confuse operators or
cause them to change their control input to implicitly signal their goal,
resulting in unnatural movement. We present a new assistance algorithm and
interface for robotic manipulation where an operator can explicitly communicate
a manipulation goal by pointing the end-effector. Rapid optimization and
parallel collision checking in a local region around the pointing target enable
direct, interactive control over grasp and place pose candidates. We compare
the explicit pointing interface to an implicit inference-based assistance
scheme in a within-subjects user study (N=20) where participants teleoperate a
simulated robot to complete a multi-step singulation and stacking task in
cluttered environments. We find that operators prefer the explicit interface,
which improved completion time, pick and place success rates, and NASA TLX
scores. Our code is available at https://github.com/NVlabs/fast-explicit-teleop
</p>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02616" title="Abstract">arXiv:2402.02616</a> [<a href="/pdf/2402.02616" title="Download PDF">pdf</a>, <a href="/format/2402.02616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Virtues of Pessimism in Inverse Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">David Wu</a>, 
<a href="/search/cs?searchtype=author&query=Swamy%2C+G">Gokul Swamy</a>, 
<a href="/search/cs?searchtype=author&query=Bagnell%2C+J+A">J. Andrew Bagnell</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z+S">Zhiwei Steven Wu</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+S">Sanjiban Choudhury</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Inverse Reinforcement Learning (IRL) is a powerful framework for learning
complex behaviors from expert demonstrations. However, it traditionally
requires repeatedly solving a computationally expensive reinforcement learning
(RL) problem in its inner loop. It is desirable to reduce the exploration
burden by leveraging expert demonstrations in the inner-loop RL. As an example,
recent work resets the learner to expert states in order to inform the learner
of high-reward expert states. However, such an approach is infeasible in the
real world. In this work, we consider an alternative approach to speeding up
the RL subroutine in IRL: \emph{pessimism}, i.e., staying close to the expert's
data distribution, instantiated via the use of offline RL algorithms. We
formalize a connection between offline RL and IRL, enabling us to use an
arbitrary offline RL algorithm to improve the sample efficiency of IRL. We
validate our theory experimentally by demonstrating a strong correlation
between the efficacy of an offline RL algorithm and how well it works as part
of an IRL procedure. By using a strong offline RL algorithm as part of an IRL
procedure, we are able to find policies that match expert performance
significantly more efficiently than the prior art.
</p>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02617" title="Abstract">arXiv:2402.02617</a> [<a href="/pdf/2402.02617" title="Download PDF">pdf</a>, <a href="/format/2402.02617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Layer-Wise Analysis of Self-Supervised Acoustic Word Embeddings: A Study  on Speech Emotion Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saliba%2C+A">Alexandra Saliba</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuanchao Li</a>, 
<a href="/search/cs?searchtype=author&query=Sanabria%2C+R">Ramon Sanabria</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+C">Catherine Lai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICASSP2024 Self-supervision in Audio, Speech and Beyond (SASB) workshop. First two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The efficacy of self-supervised speech models has been validated, yet the
optimal utilization of their representations remains challenging across diverse
tasks. In this study, we delve into Acoustic Word Embeddings (AWEs), a
fixed-length feature derived from continuous representations, to explore their
advantages in specific tasks. AWEs have previously shown utility in capturing
acoustic discriminability. In light of this, we propose measuring layer-wise
similarity between AWEs and word embeddings, aiming to further investigate the
inherent context within AWEs. Moreover, we evaluate the contribution of AWEs,
in comparison to other types of speech features, in the context of Speech
Emotion Recognition (SER). Through a comparative experiment and a layer-wise
accuracy analysis on two distinct corpora, IEMOCAP and ESD, we explore
differences between AWEs and raw self-supervised representations, as well as
the proper utilization of AWEs alone and in combination with word embeddings.
Our findings underscore the acoustic context conveyed by AWEs and showcase the
highly competitive SER accuracies by appropriately employing AWEs.
</p>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02619" title="Abstract">arXiv:2402.02619</a> [<a href="/pdf/2402.02619" title="Download PDF">pdf</a>, <a href="/format/2402.02619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Increasing Trust in Language Models through the Reuse of Verified  Circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Quirke%2C+P">Philip Quirke</a>, 
<a href="/search/cs?searchtype=author&query=Neo%2C+C">Clement Neo</a>, 
<a href="/search/cs?searchtype=author&query=Barez%2C+F">Fazl Barez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Language Models (LMs) are increasingly used for a wide range of prediction
tasks, but their training can often neglect rare edge cases, reducing their
reliability. Here, we define a stringent standard of trustworthiness whereby
the task algorithm and circuit implementation must be verified, accounting for
edge cases, with no known failure modes. We show that a transformer model can
be trained to meet this standard if built using mathematically and logically
specified frameworks. In this paper, we fully verify a model for n-digit
integer addition. To exhibit the reusability of verified modules, we insert the
trained integer addition model into an untrained model and train the combined
model to perform both addition and subtraction. We find extensive reuse of the
addition circuits for both tasks, easing verification of the more complex
subtractor model. We discuss how inserting verified task modules into LMs can
leverage model reuse to improve verifiability and trustworthiness of language
models built using them. The reuse of verified circuits reduces the effort to
verify more complex composite models which we believe to be a significant step
towards safety of language models.
</p>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02621" title="Abstract">arXiv:2402.02621</a> [<a href="/pdf/2402.02621" title="Download PDF">pdf</a>, <a href="/format/2402.02621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perfect Multi-User Distributed Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khalesi%2C+A">Ali Khalesi</a>, 
<a href="/search/cs?searchtype=author&query=Elia%2C+P">Petros Elia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is submitted to the IEEE International Symposium on Information Theory (ISIT) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">In this paper, we investigate the problem of multi-user linearly decomposable
function computation, where $N$ servers help compute functions for $K$ users,
and where each such function can be expressed as a linear combination of $L$
basis subfunctions. The process begins with each server computing some of the
subfunctions, then broadcasting a linear combination of its computed outputs to
a selected group of users, and finally having each user linearly combine its
received data to recover its function. As it has become recently known, this
problem can be translated into a matrix decomposition problem
$\mathbf{F}=\mathbf{D}\mathbf{E}$, where $\mathbf{F} \in \mathbf{GF}(q)^{K
\times L}$ describes the coefficients that define the users' demands, where
$\mathbf{E} \in \mathbf{GF}(q)^{N \times L}$ describes which subfunction each
server computes and how it combines the computed outputs, and where $\mathbf{D}
\in \mathbf{GF}(q)^{K \times N}$ describes which servers each user receives
data from and how it combines this data. Our interest here is in reducing the
total number of subfunction computations across the servers (cumulative
computational cost), as well as the worst-case load which can be a measure of
computational delay. Our contribution consists of novel bounds on the two
computing costs, where these bounds are linked here to the covering and packing
radius of classical codes. One of our findings is that in certain cases, our
distributed computing problem -- and by extension our matrix decomposition
problem -- is treated optimally when $\mathbf{F}$ is decomposed into a parity
check matrix $\mathbf{D}$ of a perfect code, and a matrix $\mathbf{E}$ which
has as columns the coset leaders of this same code.
</p>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02622" title="Abstract">arXiv:2402.02622</a> [<a href="/pdf/2402.02622" title="Download PDF">pdf</a>, <a href="/format/2402.02622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DenseFormer: Enhancing Information Flow in Transformers via Depth  Weighted Averaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pagliardini%2C+M">Matteo Pagliardini</a>, 
<a href="/search/cs?searchtype=author&query=Mohtashami%2C+A">Amirkeivan Mohtashami</a>, 
<a href="/search/cs?searchtype=author&query=Fleuret%2C+F">Francois Fleuret</a>, 
<a href="/search/cs?searchtype=author&query=Jaggi%2C+M">Martin Jaggi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The transformer architecture from Vaswani et al. (2017) is now ubiquitous
across application domains, from natural language processing to speech
processing and image understanding. We propose DenseFormer, a simple
modification to the standard architecture that improves the perplexity of the
model without increasing its size -- adding a few thousand parameters for
large-scale models in the 100B parameters range. Our approach relies on an
additional averaging step after each transformer block, which computes a
weighted average of current and past representations -- we refer to this
operation as Depth-Weighted-Average (DWA). The learned DWA weights exhibit
coherent patterns of information flow, revealing the strong and structured
reuse of activations from distant layers. Experiments demonstrate that
DenseFormer is more data efficient, reaching the same perplexity of much deeper
transformer models, and that for the same perplexity, these new models
outperform transformer baselines in terms of memory efficiency and inference
time.
</p>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02623" title="Abstract">arXiv:2402.02623</a> [<a href="/pdf/2402.02623" title="Download PDF">pdf</a>, <a href="/ps/2402.02623" title="Download PostScript">ps</a>, <a href="/format/2402.02623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Market Dynamics: Unraveling Informational Efficiency in UK  Horse Racing Betting Markets Through Betfair&#x27;s Time Series Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tondapu%2C+N">Narayan Tondapu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Using Betfair's time series data, an analysis of the United Kingdom (UK)
horse racing market reveals an interesting paradox: a market with short tails,
rapidly decaying autocorrelations, and no long-term memory. There seems to be a
remarkably high level of informational efficiency in betting exchange returns,
in contrast to financial assets that are characterized by heavy tails and
volatility clustering. The generalized Gaussian unconditional distribution with
a light tail point to a market where knowledge is quickly assimilated and
reflected in prices. This is further supported by the extremely quick fading of
autocorrelations and the absence of gain-loss asymmetry. Therefore, in addition
to measuring long-range memory, the Hurst exponent also shows mean reversion, a
sign that markets respond quickly to fresh information.
</p>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02624" title="Abstract">arXiv:2402.02624</a> [<a href="/pdf/2402.02624" title="Download PDF">pdf</a>, <a href="/format/2402.02624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Safe Reinforcement Learning driven Weights-varying Model Predictive  Control for Autonomous Vehicle Motion Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zarrouki%2C+B">Baha Zarrouki</a>, 
<a href="/search/cs?searchtype=author&query=Spanakakis%2C+M">Marios Spanakakis</a>, 
<a href="/search/cs?searchtype=author&query=Betz%2C+J">Johannes Betz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">Determining the optimal cost function parameters of Model Predictive Control
(MPC) to optimize multiple control objectives is a challenging and
time-consuming task. Multiobjective Bayesian Optimization (BO) techniques solve
this problem by determining a Pareto optimal parameter set for an MPC with
static weights. However, a single parameter set may not deliver the most
optimal closed-loop control performance when the context of the MPC operating
conditions changes during its operation, urging the need to adapt the cost
function weights at runtime. Deep Reinforcement Learning (RL) algorithms can
automatically learn context-dependent optimal parameter sets and dynamically
adapt for a Weightsvarying MPC (WMPC). However, learning cost function weights
from scratch in a continuous action space may lead to unsafe operating states.
To solve this, we propose a novel approach limiting the RL actions within a
safe learning space representing a catalog of pre-optimized BO Pareto-optimal
weight sets. We conceive a RL agent not to learn in a continuous space but to
proactively anticipate upcoming control tasks and to choose the most optimal
discrete actions, each corresponding to a single set of Pareto optimal weights,
context-dependent. Hence, even an untrained RL agent guarantees a safe and
optimal performance. Experimental results demonstrate that an untrained RL-WMPC
shows Pareto-optimal closed-loop behavior and training the RL-WMPC helps
exhibit a performance beyond the Pareto-front.
</p>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02625" title="Abstract">arXiv:2402.02625</a> [<a href="/pdf/2402.02625" title="Download PDF">pdf</a>, <a href="/format/2402.02625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Transformer RNNs with Multiple Temporal Perspectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dumitru%2C+R">Razvan-Gabriel Dumitru</a>, 
<a href="/search/cs?searchtype=author&query=Peteleaza%2C+D">Darius Peteleaza</a>, 
<a href="/search/cs?searchtype=author&query=Surdeanu%2C+M">Mihai Surdeanu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 8 figures, 4 tables, in review for ICML 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">We introduce the concept of multiple temporal perspectives, a novel approach
applicable to Recurrent Neural Network (RNN) architectures for enhancing their
understanding of sequential data. This method involves maintaining diverse
temporal views of previously encountered text, significantly enriching the
language models' capacity to interpret context. To show the efficacy of this
approach, we incorporate it into the Receptance Weighted Key Value (RWKV)
architecture, addressing its inherent challenge of retaining all historical
information within a single hidden state. Notably, this improvement is achieved
with a minimal increase in the number of parameters --even as little as
$0.04\%$ of the original number of parameters. Further, the additional
parameters necessary for the multiple temporal perspectives are fine-tuned with
minimal computational overhead, avoiding the need for a full pre-training. The
resulting model maintains linear computational complexity during prompt
inference, ensuring consistent efficiency across various sequence lengths. The
empirical results and ablation studies included in our research validate the
effectiveness of our approach, showcasing improved performance across multiple
benchmarks. The code, model weights and datasets are open-sourced at:
https://github.com/RazvanDu/TemporalRNNs.
</p>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02626" title="Abstract">arXiv:2402.02626</a> [<a href="/pdf/2402.02626" title="Download PDF">pdf</a>, <a href="/format/2402.02626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Position bias in features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Demsyn-Jones%2C+R">Richard Demsyn-Jones</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The purpose of modeling document relevance for search engines is to rank
better in subsequent searches. Document-specific historical click-through rates
can be important features in a dynamic ranking system which updates as we
accumulate more sample. This paper describes the properties of several such
features, and tests them in controlled experiments. Extending the inverse
propensity weighting method to documents creates an unbiased estimate of
document relevance. This feature can approximate relevance accurately, leading
to near-optimal ranking in ideal circumstances. However, it has high variance
that is increasing with respect to the degree of position bias. Furthermore,
inaccurate position bias estimation leads to poor performance. Under several
scenarios this feature can perform worse than biased click-through rates. This
paper underscores the need for accurate position bias estimation, and is unique
in suggesting simultaneous use of biased and unbiased position bias features.
</p>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02627" title="Abstract">arXiv:2402.02627</a> [<a href="/pdf/2402.02627" title="Download PDF">pdf</a>, <a href="/format/2402.02627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stability Analysis of Various Symbolic Rule Extraction Methods from  Recurrent Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dave%2C+N">Neisarg Dave</a>, 
<a href="/search/cs?searchtype=author&query=Kifer%2C+D">Daniel Kifer</a>, 
<a href="/search/cs?searchtype=author&query=Giles%2C+C+L">C. Lee Giles</a>, 
<a href="/search/cs?searchtype=author&query=Mali%2C+A">Ankur Mali</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper analyzes two competing rule extraction methodologies: quantization
and equivalence query. We trained $3600$ RNN models, extracting $18000$ DFA
with a quantization approach (k-means and SOM) and $3600$ DFA by equivalence
query($L^{*}$) methods across $10$ initialization seeds. We sampled the
datasets from $7$ Tomita and $4$ Dyck grammars and trained them on $4$ RNN
cells: LSTM, GRU, O2RNN, and MIRNN. The observations from our experiments
establish the superior performance of O2RNN and quantization-based rule
extraction over others. $L^{*}$, primarily proposed for regular grammars,
performs similarly to quantization methods for Tomita languages when neural
networks are perfectly trained. However, for partially trained RNNs, $L^{*}$
shows instability in the number of states in DFA, e.g., for Tomita 5 and Tomita
6 languages, $L^{*}$ produced more than $100$ states. In contrast, quantization
methods result in rules with number of states very close to ground truth DFA.
Among RNN cells, O2RNN produces stable DFA consistently compared to other
cells. For Dyck Languages, we observe that although GRU outperforms other RNNs
in network performance, the DFA extracted by O2RNN has higher performance and
better stability. The stability is computed as the standard deviation of
accuracy on test sets on networks trained across $10$ seeds. On Dyck Languages,
quantization methods outperformed $L^{*}$ with better stability in accuracy and
the number of states. $L^{*}$ often showed instability in accuracy in the order
of $16\% - 22\%$ for GRU and MIRNN while deviation for quantization methods
varied in $5\% - 15\%$. In many instances with LSTM and GRU, DFA's extracted by
$L^{*}$ even failed to beat chance accuracy ($50\%$), while those extracted by
quantization method had standard deviation in the $7\%-17\%$ range. For O2RNN,
both rule extraction methods had deviation in the $0.5\% - 3\%$ range.
</p>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02629" title="Abstract">arXiv:2402.02629</a> [<a href="/pdf/2402.02629" title="Download PDF">pdf</a>, <a href="/format/2402.02629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PROSAC: Provably Safe Certification for Machine Learning Models under  Adversarial Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziquan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhi%2C+Z">Zhuo Zhi</a>, 
<a href="/search/cs?searchtype=author&query=Bogunovic%2C+I">Ilija Bogunovic</a>, 
<a href="/search/cs?searchtype=author&query=Gerner-Beuerle%2C+C">Carsten Gerner-Beuerle</a>, 
<a href="/search/cs?searchtype=author&query=Rodrigues%2C+M">Miguel Rodrigues</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">It is widely known that state-of-the-art machine learning models, including
vision and language models, can be seriously compromised by adversarial
perturbations. It is therefore increasingly relevant to develop capabilities to
certify their performance in the presence of the most effective adversarial
attacks. Our paper offers a new approach to certify the performance of machine
learning models in the presence of adversarial attacks with population level
risk guarantees. In particular, we introduce the notion of $(\alpha,\zeta)$
machine learning model safety. We propose a hypothesis testing procedure, based
on the availability of a calibration set, to derive statistical guarantees
providing that the probability of declaring that the adversarial (population)
risk of a machine learning model is less than $\alpha$ (i.e. the model is
safe), while the model is in fact unsafe (i.e. the model adversarial population
risk is higher than $\alpha$), is less than $\zeta$. We also propose Bayesian
optimization algorithms to determine efficiently whether a machine learning
model is $(\alpha,\zeta)$-safe in the presence of an adversarial attack, along
with statistical guarantees. We apply our framework to a range of machine
learning models including various sizes of vision Transformer (ViT) and ResNet
models impaired by a variety of adversarial attacks, such as AutoAttack,
SquareAttack and natural evolution strategy attack, to illustrate the operation
of our approach. Importantly, we show that ViT's are generally more robust to
adversarial attacks than ResNets, and ViT-large is more robust than smaller
models. Our approach goes beyond existing empirical adversarial risk-based
certification guarantees. It formulates rigorous (and provable) performance
guarantees that can be used to satisfy regulatory requirements mandating the
use of state-of-the-art technical tools.
</p>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02630" title="Abstract">arXiv:2402.02630</a> [<a href="/pdf/2402.02630" title="Download PDF">pdf</a>, <a href="/format/2402.02630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cryptographically Assured Information Flow: Assured Remote Execution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dyer%2C+S+L">Scott L. Dyer</a>, 
<a href="/search/cs?searchtype=author&query=Femrite%2C+C+A">Christian A. Femrite</a>, 
<a href="/search/cs?searchtype=author&query=Guttman%2C+J+D">Joshua D. Guttman</a>, 
<a href="/search/cs?searchtype=author&query=Lanson%2C+J+P">Julian P. Lanson</a>, 
<a href="/search/cs?searchtype=author&query=Liskov%2C+M+D">Moses D. Liskov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 62 pp
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Assured Remote Execution on a device is the ability of suitably authorized
parties to construct secure channels with known processes -- i.e. processes
executing known code -- running on it. Assured Remote Execution requires a
hardware basis including cryptographic primitives.
<br />In this paper, we show that a simple hardware-level mechanism called
Cryptographically Assured Information Flow (CAIF) enables Assured Remote
Execution. CAIF is akin to some operations in existing Trusted Execution
Environments, but securely implements an ideal functionality defined in terms
of logging and confidential escrow.
<br />We show how to achieve Assured Remote Execution for a wide variety of
processes on a CAIF device. Cryptographic protocol analysis demonstrates our
security goals are achieved even against a strong adversary that may modify our
programs and execute unauthorized programs on the device.
<br />Assured Remote Execution enables useful functionality such as trustworthy
remote attestation, and provides some of the support needed for secure remote
reprogramming.
<br />Acknowledgment. We are grateful to the MITRE Independent Research and
Development Program for support.
</p>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02631" title="Abstract">arXiv:2402.02631</a> [<a href="/pdf/2402.02631" title="Download PDF">pdf</a>, <a href="/format/2402.02631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Understand: Identifying Interactions via the Mobius  Transform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+J+S">Justin S. Kang</a>, 
<a href="/search/cs?searchtype=author&query=Erginbas%2C+Y+E">Yigit E. Erginbas</a>, 
<a href="/search/cs?searchtype=author&query=Butler%2C+L">Landon Butler</a>, 
<a href="/search/cs?searchtype=author&query=Pedarsani%2C+R">Ramtin Pedarsani</a>, 
<a href="/search/cs?searchtype=author&query=Ramchandran%2C+K">Kannan Ramchandran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">One of the most fundamental problems in machine learning is finding
interpretable representations of the functions we learn. The Mobius transform
is a useful tool for this because its coefficients correspond to unique
importance scores on sets of input variables. The Mobius Transform is strongly
related (and in some cases equivalent) to the concept of Shapley value, which
is a widely used game-theoretic notion of importance. This work focuses on the
(typical) regime where the fraction of non-zero Mobius coefficients (and thus
interactions between inputs) is small compared to the set of all $2^n$ possible
interactions between $n$ inputs. When there are $K = O(2^{n \delta})$ with
$\delta \leq \frac{1}{3}$ non-zero coefficients chosen uniformly at random, our
algorithm exactly recovers the Mobius transform in $O(Kn)$ samples and
$O(Kn^2)$ time with vanishing error as $K \rightarrow \infty$, the first
non-adaptive algorithm to do so. We also uncover a surprising connection
between group testing and the Mobius transform. In the case where all
interactions are between at most $t = \Theta(n^{\alpha})$ inputs, for $\alpha &lt;
0.409$, we are able to leverage results from group testing to provide the first
algorithm that computes the Mobius transform in $O(Kt\log n)$ sample complexity
and $O(K\mathrm{poly}(n))$ time with vanishing error as $K \rightarrow \infty$.
Finally, we present a robust version of this algorithm that achieves the same
sample and time complexity under some assumptions, but with a factor depending
on noise variance. Our work is deeply interdisciplinary, drawing from tools
spanning across signal processing, algebra, information theory, learning theory
and group testing to address this important problem at the forefront of machine
learning.
</p>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02632" title="Abstract">arXiv:2402.02632</a> [<a href="/pdf/2402.02632" title="Download PDF">pdf</a>, <a href="/format/2402.02632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GIRT-Model: Automated Generation of Issue Report Templates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nikeghbal%2C+N">Nafiseh Nikeghbal</a>, 
<a href="/search/cs?searchtype=author&query=Kargaran%2C+A+H">Amir Hossein Kargaran</a>, 
<a href="/search/cs?searchtype=author&query=Heydarnoori%2C+A">Abbas Heydarnoori</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to be published at the 21st IEEE/ACM International Conference on Mining Software Repositories (MSR 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Platforms such as GitHub and GitLab introduce Issue Report Templates (IRTs)
to enable more effective issue management and better alignment with developer
expectations. However, these templates are not widely adopted in most
repositories, and there is currently no tool available to aid developers in
generating them. In this work, we introduce GIRT-Model, an assistant language
model that automatically generates IRTs based on the developer's instructions
regarding the structure and necessary fields. We create GIRT-Instruct, a
dataset comprising pairs of instructions and IRTs, with the IRTs sourced from
GitHub repositories. We use GIRT-Instruct to instruction-tune a T5-base model
to create the GIRT-Model. In our experiments, GIRT-Model outperforms general
language models (T5 and Flan-T5 with different parameter sizes) in IRT
generation by achieving significantly higher scores in ROUGE, BLEU, METEOR, and
human evaluation. Additionally, we analyze the effectiveness of GIRT-Model in a
user study in which participants wrote short IRTs with GIRT-Model. Our results
show that the participants find GIRT-Model useful in the automated generation
of templates. We hope that through the use of GIRT-Model, we can encourage more
developers to adopt IRTs in their repositories. We publicly release our code,
dataset, and model at https://github.com/ISE-Research/girt-model.
</p>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02633" title="Abstract">arXiv:2402.02633</a> [<a href="/pdf/2402.02633" title="Download PDF">pdf</a>, <a href="/format/2402.02633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting Machine Translation Performance on Low-Resource Languages:  The Role of Domain Similarity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khiu%2C+E">Eric Khiu</a>, 
<a href="/search/cs?searchtype=author&query=Toossi%2C+H">Hasti Toossi</a>, 
<a href="/search/cs?searchtype=author&query=Anugraha%2C+D">David Anugraha</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaxu Li</a>, 
<a href="/search/cs?searchtype=author&query=Flores%2C+J+A+P">Juan Armando Parra Flores</a>, 
<a href="/search/cs?searchtype=author&query=Roman%2C+L+A">Leandro Acros Roman</a>, 
<a href="/search/cs?searchtype=author&query=Do%C4%9Fru%C3%B6z%2C+A+S">A. Seza Do&#x11f;ru&#xf6;z</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+E+A">En-Shiun Annie Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures, accepted to EACL 2024, findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Fine-tuning and testing a multilingual large language model is expensive and
challenging for low-resource languages (LRLs). While previous studies have
predicted the performance of natural language processing (NLP) tasks using
machine learning methods, they primarily focus on high-resource languages,
overlooking LRLs and shifts across domains. Focusing on LRLs, we investigate
three factors: the size of the fine-tuning corpus, the domain similarity
between fine-tuning and testing corpora, and the language similarity between
source and target languages. We employ classical regression models to assess
how these factors impact the model's performance. Our results indicate that
domain similarity has the most critical impact on predicting the performance of
Machine Translation models.
</p>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02634" title="Abstract">arXiv:2402.02634</a> [<a href="/pdf/2402.02634" title="Download PDF">pdf</a>, <a href="/format/2402.02634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Key-Graph Transformer for Image Restoration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+B">Bin Ren</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yawei Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jingyun Liang</a>, 
<a href="/search/cs?searchtype=author&query=Ranjan%2C+R">Rakesh Ranjan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Mengyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cucchiara%2C+R">Rita Cucchiara</a>, 
<a href="/search/cs?searchtype=author&query=Van+Gool%2C+L">Luc Van Gool</a>, 
<a href="/search/cs?searchtype=author&query=Sebe%2C+N">Nicu Sebe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">While it is crucial to capture global information for effective image
restoration (IR), integrating such cues into transformer-based methods becomes
computationally expensive, especially with high input resolution. Furthermore,
the self-attention mechanism in transformers is prone to considering
unnecessary global cues from unrelated objects or regions, introducing
computational inefficiencies. In response to these challenges, we introduce the
Key-Graph Transformer (KGT) in this paper. Specifically, KGT views patch
features as graph nodes. The proposed Key-Graph Constructor efficiently forms a
sparse yet representative Key-Graph by selectively connecting essential nodes
instead of all the nodes. Then the proposed Key-Graph Attention is conducted
under the guidance of the Key-Graph only among selected nodes with linear
computational complexity within each window. Extensive experiments across 6 IR
tasks confirm the proposed KGT's state-of-the-art performance, showcasing
advancements both quantitatively and qualitatively.
</p>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02635" title="Abstract">arXiv:2402.02635</a> [<a href="/pdf/2402.02635" title="Download PDF">pdf</a>, <a href="/format/2402.02635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Principled Risk Scores for Space Cyber Risk Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ear%2C+E">Ekzhin Ear</a>, 
<a href="/search/cs?searchtype=author&query=Bailey%2C+B">Brandon Bailey</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shouhuai Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Space is an emerging domain critical to humankind. Correspondingly, space
cybersecurity is an emerging field with much research to be done. To help space
cybersecurity practitioners better manage cyber risks, The Aerospace
Corporation proposed Notional Risk Scores (NRS) within their Space Attack
Research and Tactic Analysis (SPARTA) framework, which can be applied to
quantify the cyber risks associated with space infrastructures and systems.
While intended for adoption by practitioners, NRS has not been analyzed with
real-world scenarios, putting its effectiveness into question. In this paper we
analyze NRS via a real-world cyber attack scenario against a satellite, and
characterize the strengths, weaknesses, and applicability of NRS. The
characterization prompts us to propose a set of desired properties to guide the
design of future NRS. As a first step along this direction, we further propose
a formalism to serve as a baseline for designing future NRS with those desired
properties.
</p>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02636" title="Abstract">arXiv:2402.02636</a> [<a href="/pdf/2402.02636" title="Download PDF">pdf</a>, <a href="/format/2402.02636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Large Language Models Learn Independent Causal Mechanisms?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gendron%2C+G">Ga&#xeb;l Gendron</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+B+T">Bao Trung Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+A+Y">Alex Yuxuan Peng</a>, 
<a href="/search/cs?searchtype=author&query=Witbrock%2C+M">Michael Witbrock</a>, 
<a href="/search/cs?searchtype=author&query=Dobbie%2C+G">Gillian Dobbie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 8 pages for the main paper and 9 pages for references and appendices, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
<p class="mathjax">Despite impressive performance on language modelling and complex reasoning
tasks, Large Language Models (LLMs) fall short on the same tasks in uncommon
settings or with distribution shifts, exhibiting some lack of generalisation
ability. This issue has usually been alleviated by feeding more training data
into the LLM. However, this method is brittle, as the scope of tasks may not be
readily predictable or may evolve, and updating the model with new data
generally requires extensive additional training. By contrast, systems, such as
causal models, that learn abstract variables and causal relationships can
demonstrate increased robustness against changes in the distribution. One
reason for this success is the existence and use of Independent Causal
Mechanisms (ICMs) representing high-level concepts that only sparsely interact.
In this work, we apply two concepts from causality to learn ICMs within LLMs.
We develop a new LLM architecture composed of multiple sparsely interacting
language modelling modules. We introduce a routing scheme to induce
specialisation of the network into domain-specific modules. We also present a
Mutual Information minimisation objective that trains a separate module to
learn abstraction and domain-invariant mechanisms. We show that such causal
constraints can improve out-of-distribution performance on abstract and causal
reasoning tasks.
</p>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02637" title="Abstract">arXiv:2402.02637</a> [<a href="/pdf/2402.02637" title="Download PDF">pdf</a>, <a href="/format/2402.02637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $C^*$-Algebraic Machine Learning: Moving in a New Direction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+Y">Yuka Hashimoto</a>, 
<a href="/search/cs?searchtype=author&query=Ikeda%2C+M">Masahiro Ikeda</a>, 
<a href="/search/cs?searchtype=author&query=Kadri%2C+H">Hachem Kadri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> position paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Operator Algebras (math.OA); Machine Learning (stat.ML)

</div>
<p class="mathjax">Machine learning has a long collaborative tradition with several fields of
mathematics, such as statistics, probability and linear algebra. We propose a
new direction for machine learning research: $C^*$-algebraic ML $-$ a
cross-fertilization between $C^*$-algebra and machine learning. The
mathematical concept of $C^*$-algebra is a natural generalization of the space
of complex numbers. It enables us to unify existing learning strategies, and
construct a new framework for more diverse and information-rich data models. We
explain why and how to use $C^*$-algebras in machine learning, and provide
technical considerations that go into the design of $C^*$-algebraic learning
models in the contexts of kernel methods and neural networks. Furthermore, we
discuss open questions and challenges in $C^*$-algebraic ML and give our
thoughts for future development and applications.
</p>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02639" title="Abstract">arXiv:2402.02639</a> [<a href="/pdf/2402.02639" title="Download PDF">pdf</a>, <a href="/format/2402.02639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> It&#x27;s how you do things that matters&quot;: Attending to Process to Better  Serve Indigenous Communities with Language Technologies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cooper%2C+N">Ned Cooper</a>, 
<a href="/search/cs?searchtype=author&query=Heldreth%2C+C">Courtney Heldreth</a>, 
<a href="/search/cs?searchtype=author&query=Hutchinson%2C+B">Ben Hutchinson</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 18th Conference of the European Chapter of the
  Association for Computational Linguistics (EACL 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Indigenous languages are historically under-served by Natural Language
Processing (NLP) technologies, but this is changing for some languages with the
recent scaling of large multilingual models and an increased focus by the NLP
community on endangered languages. This position paper explores ethical
considerations in building NLP technologies for Indigenous languages, based on
the premise that such projects should primarily serve Indigenous communities.
We report on interviews with 17 researchers working in or with Aboriginal
and/or Torres Strait Islander communities on language technology projects in
Australia. Drawing on insights from the interviews, we recommend practices for
NLP researchers to increase attention to the process of engagements with
Indigenous communities, rather than focusing only on decontextualised
artefacts.
</p>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02642" title="Abstract">arXiv:2402.02642</a> [<a href="/pdf/2402.02642" title="Download PDF">pdf</a>, <a href="/format/2402.02642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Object Graph Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thimmaiah%2C+A">Aditya Thimmaiah</a>, 
<a href="/search/cs?searchtype=author&query=Lampropoulos%2C+L">Leonidas Lampropoulos</a>, 
<a href="/search/cs?searchtype=author&query=Rossbach%2C+C+J">Christopher J. Rossbach</a>, 
<a href="/search/cs?searchtype=author&query=Gligoric%2C+M">Milos Gligoric</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, ICSE 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Databases (cs.DB); Programming Languages (cs.PL)

</div>
<p class="mathjax">We introduce Object Graph Programming (OGO), which enables reading and
modifying an object graph (i.e., the entire state of the object heap) via
declarative queries. OGO models the objects and their relations in the heap as
an object graph thereby treating the heap as a graph database: each node in the
graph is an object (e.g., an instance of a class or an instance of a metadata
class) and each edge is a relation between objects (e.g., a field of one object
references another object). We leverage Cypher, the most popular query language
for graph databases, as OGO's query language. Unlike LINQ, which uses
collections (e.g., List) as a source of data, OGO views the entire object graph
as a single "collection". OGO is ideal for querying collections (just like
LINQ), introspecting the runtime system state (e.g., finding all instances of a
given class or accessing fields via reflection), and writing assertions that
have access to the entire program state. We prototyped OGO for Java in two
ways: (a) by translating an object graph into a Neo4j database on which we run
Cypher queries, and (b) by implementing our own in-memory graph query engine
that directly queries the object heap. We used OGO to rewrite hundreds of
statements in large open-source projects into OGO queries. We report our
experience and performance of our prototypes.
</p>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02643" title="Abstract">arXiv:2402.02643</a> [<a href="/pdf/2402.02643" title="Download PDF">pdf</a>, <a href="/format/2402.02643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-Enhanced Data Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xuanhe Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xinyang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guoliang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Machine learning (ML) techniques for optimizing data management problems have
been extensively studied and widely deployed in recent five years. However
traditional ML methods have limitations on generalizability (adapting to
different scenarios) and inference ability (understanding the context).
Fortunately, large language models (LLMs) have shown high generalizability and
human-competitive abilities in understanding context, which are promising for
data management tasks (e.g., database diagnosis, database tuning). However,
existing LLMs have several limitations: hallucination, high cost, and low
accuracy for complicated tasks. To address these challenges, we design LLMDB,
an LLM-enhanced data management paradigm which has generalizability and high
inference ability while avoiding hallucination, reducing LLM cost, and
achieving high accuracy. LLMDB embeds domain-specific knowledge to avoid
hallucination by LLM fine-tuning and prompt engineering. LLMDB reduces the high
cost of LLMs by vector databases which provide semantic search and caching
abilities. LLMDB improves the task accuracy by LLM agent which provides
multiple-round inference and pipeline executions. We showcase three real-world
scenarios that LLMDB can well support, including query rewrite, database
diagnosis and data analytics. We also summarize the open research challenges of
LLMDB.
</p>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02644" title="Abstract">arXiv:2402.02644</a> [<a href="/pdf/2402.02644" title="Download PDF">pdf</a>, <a href="/format/2402.02644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational DAG Estimation via State Augmentation With Stochastic  Permutations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bonilla%2C+E+V">Edwin V. Bonilla</a>, 
<a href="/search/cs?searchtype=author&query=Elinas%2C+P">Pantelis Elinas</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">He Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Filippone%2C+M">Maurizio Filippone</a>, 
<a href="/search/cs?searchtype=author&query=Kitsios%2C+V">Vassili Kitsios</a>, 
<a href="/search/cs?searchtype=author&query=O%27Kane%2C+T">Terry O&#x27;Kane</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Estimating the structure of a Bayesian network, in the form of a directed
acyclic graph (DAG), from observational data is a statistically and
computationally hard problem with essential applications in areas such as
causal discovery. Bayesian approaches are a promising direction for solving
this task, as they allow for uncertainty quantification and deal with
well-known identifiability issues. From a probabilistic inference perspective,
the main challenges are (i) representing distributions over graphs that satisfy
the DAG constraint and (ii) estimating a posterior over the underlying
combinatorial space. We propose an approach that addresses these challenges by
formulating a joint distribution on an augmented space of DAGs and
permutations. We carry out posterior estimation via variational inference,
where we exploit continuous relaxations of discrete distributions. We show that
our approach can outperform competitive Bayesian and non-Bayesian benchmarks on
a range of synthetic and real datasets.
</p>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02648" title="Abstract">arXiv:2402.02648</a> [<a href="/pdf/2402.02648" title="Download PDF">pdf</a>, <a href="/format/2402.02648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahn%2C+J">Jinwoo Ahn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Still Ongoing Work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) frequently suffer from knowledge-intensive
questions, often being inconsistent by providing different outputs despite
given the same input. The response quality worsens when the user expresses a
firm opposing stance which causes the LLMs to adjust its response despite the
correct initial one. These behaviors decrease the reliability and validity of
the responses provided by these models. In this paper, we attempt to 1) raise
awareness of the inherent risks that follow from overly relying on AI agents
like ChatGPT by showing how Chain-of-Feedback (CoF) triggers LLMs to deviate
more from the actual answer and 2) suggest a novel prompting method, Recursive
Chain of Feedback (R-CoF), that we are conducting further study. The CoF system
takes in an open-ended multi-step question. Then, we repetitively provide
meaningless feedback requesting another attempt. Our preliminary experiments
show that such feedback only decreases the quality of the response. On the
other hand, to mitigate the effects of the aforementioned inconsistencies, we
present a novel method of recursively revising the initial incorrect reasoning
provided by the LLM by repetitively breaking down each incorrect step into
smaller individual problems.
</p>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02649" title="Abstract">arXiv:2402.02649</a> [<a href="/pdf/2402.02649" title="Download PDF">pdf</a>, <a href="/format/2402.02649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Densely Decoded Networks with Adaptive Deep Supervision for Medical  Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Suraj Mishra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Medical image segmentation using deep neural networks has been highly
successful. However, the effectiveness of these networks is often limited by
inadequate dense prediction and inability to extract robust features. To
achieve refined dense prediction, we propose densely decoded networks (ddn), by
selectively introducing 'crutch' network connections. Such 'crutch' connections
in each upsampling stage of the network decoder (1) enhance target localization
by incorporating high resolution features from the encoder, and (2) improve
segmentation by facilitating multi-stage contextual information flow. Further,
we present a training strategy based on adaptive deep supervision (ads), which
exploits and adapts specific attributes of input dataset, for robust feature
extraction. In particular, ads strategically locates and deploys auxiliary
supervision, by matching the average input object size with the layer-wise
effective receptive fields (lerf) of a network, resulting in a class of ddns.
Such inclusion of 'companion objective' from a specific hidden layer, helps the
model pay close attention to some distinct input-dependent features, which the
network might otherwise 'ignore' during training. Our new networks and training
strategy are validated on 4 diverse datasets of different modalities,
demonstrating their effectiveness.
</p>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02651" title="Abstract">arXiv:2402.02651</a> [<a href="/pdf/2402.02651" title="Download PDF">pdf</a>, <a href="/format/2402.02651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision-Language Models Provide Promptable Representations for  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">William Chen</a>, 
<a href="/search/cs?searchtype=author&query=Mees%2C+O">Oier Mees</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Aviral Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Humans can quickly learn new behaviors by leveraging background world
knowledge. In contrast, agents trained with reinforcement learning (RL)
typically learn behaviors from scratch. We thus propose a novel approach that
uses the vast amounts of general and indexable world knowledge encoded in
vision-language models (VLMs) pre-trained on Internet-scale data for embodied
RL. We initialize policies with VLMs by using them as promptable
representations: embeddings that are grounded in visual observations and encode
semantic features based on the VLM's internal knowledge, as elicited through
prompts that provide task context and auxiliary information. We evaluate our
approach on visually-complex, long horizon RL tasks in Minecraft and robot
navigation in Habitat. We find that our policies trained on embeddings
extracted from general-purpose VLMs outperform equivalent policies trained on
generic, non-promptable image embeddings. We also find our approach outperforms
instruction-following methods and performs comparably to domain-specific
embeddings.
</p>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02653" title="Abstract">arXiv:2402.02653</a> [<a href="/pdf/2402.02653" title="Download PDF">pdf</a>, <a href="/format/2402.02653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning with Mixture of Prototypes for Out-of-Distribution Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Haodong Lu</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+D">Dong Gong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+J">Jason Xue</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+L">Lina Yao</a>, 
<a href="/search/cs?searchtype=author&query=Moore%2C+K">Kristen Moore</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Out-of-distribution (OOD) detection aims to detect testing samples far away
from the in-distribution (ID) training data, which is crucial for the safe
deployment of machine learning models in the real world. Distance-based OOD
detection methods have emerged with enhanced deep representation learning. They
identify unseen OOD samples by measuring their distances from ID class
centroids or prototypes. However, existing approaches learn the representation
relying on oversimplified data assumptions, e.g, modeling ID data of each class
with one centroid class prototype or using loss functions not designed for OOD
detection, which overlook the natural diversities within the data. Naively
enforcing data samples of each class to be compact around only one prototype
leads to inadequate modeling of realistic data and limited performance. To
tackle these issues, we propose PrototypicAl Learning with a Mixture of
prototypes (PALM) which models each class with multiple prototypes to capture
the sample diversities, and learns more faithful and compact samples embeddings
to enhance OOD detection. Our method automatically identifies and dynamically
updates prototypes, assigning each sample to a subset of prototypes via
reciprocal neighbor soft assignment weights. PALM optimizes a maximum
likelihood estimation (MLE) loss to encourage the sample embeddings to be
compact around the associated prototypes, as well as a contrastive loss on all
prototypes to enhance intra-class compactness and inter-class discrimination at
the prototype level. Moreover, the automatic estimation of prototypes enables
our approach to be extended to the challenging OOD detection task with
unlabelled ID data. Extensive experiments demonstrate the superiority of PALM,
achieving state-of-the-art average AUROC performance of 93.82 on the
challenging CIFAR-100 benchmark. Code is available at
https://github.com/jeff024/PALM.
</p>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02655" title="Abstract">arXiv:2402.02655</a> [<a href="/pdf/2402.02655" title="Download PDF">pdf</a>, <a href="/format/2402.02655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VlogQA: Task, Dataset, and Baseline Models for Vietnamese Spoken-Based  Machine Reading Comprehension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ngo%2C+T+P">Thinh Phuoc Ngo</a>, 
<a href="/search/cs?searchtype=author&query=Dang%2C+K+T+A">Khoa Tran Anh Dang</a>, 
<a href="/search/cs?searchtype=author&query=Luu%2C+S+T">Son T. Luu</a>, 
<a href="/search/cs?searchtype=author&query=Van+Nguyen%2C+K">Kiet Van Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+N+L">Ngan Luu-Thuy Nguyen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as main conference paper at EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper presents the development process of a Vietnamese spoken language
corpus for machine reading comprehension (MRC) tasks and provides insights into
the challenges and opportunities associated with using real-world data for
machine reading comprehension tasks. The existing MRC corpora in Vietnamese
mainly focus on formal written documents such as Wikipedia articles, online
newspapers, or textbooks. In contrast, the VlogQA consists of 10,076
question-answer pairs based on 1,230 transcript documents sourced from YouTube
-- an extensive source of user-uploaded content, covering the topics of food
and travel. By capturing the spoken language of native Vietnamese speakers in
natural settings, an obscure corner overlooked in Vietnamese research, the
corpus provides a valuable resource for future research in reading
comprehension tasks for the Vietnamese language. Regarding performance
evaluation, our deep-learning models achieved the highest F1 score of 75.34% on
the test set, indicating significant progress in machine reading comprehension
for Vietnamese spoken language data. In terms of EM, the highest score we
accomplished is 53.97%, which reflects the challenge in processing spoken-based
content and highlights the need for further improvement.
</p>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02656" title="Abstract">arXiv:2402.02656</a> [<a href="/pdf/2402.02656" title="Download PDF">pdf</a>, <a href="/format/2402.02656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RACER: An LLM-powered Methodology for Scalable Analysis of  Semi-structured Mental Health Interviews
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+S+H">Satpreet Harcharan Singh</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+K">Kevin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Bhasin%2C+K">Kanchan Bhasin</a>, 
<a href="/search/cs?searchtype=author&query=Sabharwal%2C+A">Ashutosh Sabharwal</a>, 
<a href="/search/cs?searchtype=author&query=Moukaddam%2C+N">Nidal Moukaddam</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+A+B">Ankit B Patel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Semi-structured interviews (SSIs) are a commonly employed data-collection
method in healthcare research, offering in-depth qualitative insights into
subject experiences. Despite their value, the manual analysis of SSIs is
notoriously time-consuming and labor-intensive, in part due to the difficulty
of extracting and categorizing emotional responses, and challenges in scaling
human evaluation for large populations. In this study, we develop RACER, a
Large Language Model (LLM) based expert-guided automated pipeline that
efficiently converts raw interview transcripts into insightful domain-relevant
themes and sub-themes. We used RACER to analyze SSIs conducted with 93
healthcare professionals and trainees to assess the broad personal and
professional mental health impacts of the COVID-19 crisis. RACER achieves
moderately high agreement with two human evaluators (72%), which approaches the
human inter-rater agreement (77%). Interestingly, LLMs and humans struggle with
similar content involving nuanced emotional, ambivalent/dialectical, and
psychological statements. Our study highlights the opportunities and challenges
in using LLMs to improve research efficiency and opens new avenues for scalable
analysis of SSIs in healthcare research.
</p>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02658" title="Abstract">arXiv:2402.02658</a> [<a href="/pdf/2402.02658" title="Download PDF">pdf</a>, <a href="/format/2402.02658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-step Problem Solving Through a Verifier: An Empirical Analysis on  Model-induced Process Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yunxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuexin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+L">Liangchen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+L">Le Hou</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hongkun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+J">Jingbo Shang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Process supervision, using a trained verifier to evaluate the intermediate
steps generated by reasoner, has demonstrated significant improvements in
multi-step problem solving. In this paper, to avoid expensive human annotation
effort on the verifier training data, we introduce Model-induced Process
Supervision (MiPS), a novel method for automating data curation. MiPS annotates
an intermediate step by sampling completions of this solution through the
reasoning model, and obtaining an accuracy defined as the proportion of correct
completions. Errors in the reasoner would cause MiPS to underestimate the
accuracy of intermediate steps, therefore, we suggest and empirically show that
verification focusing on high predicted scores of the verifier shall be
preferred over that of low predicted scores, contrary to prior work. Our
approach significantly improves the performance of PaLM 2 on math and coding
tasks (accuracy +0.67% on GSM8K, +4.16% on MATH, +0.92% on MBPP compared with
an output supervision trained verifier). Additionally, our study demonstrates
that the verifier exhibits strong generalization ability across different
reasoning models.
</p>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02662" title="Abstract">arXiv:2402.02662</a> [<a href="/pdf/2402.02662" title="Download PDF">pdf</a>, <a href="/format/2402.02662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Image-Caption Encoding for Improving Zero-Shot Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+E+Y">Eric Yang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+C">Christopher Liao</a>, 
<a href="/search/cs?searchtype=author&query=Ravi%2C+S">Sathvik Ravi</a>, 
<a href="/search/cs?searchtype=author&query=Tsiligkaridis%2C+T">Theodoros Tsiligkaridis</a>, 
<a href="/search/cs?searchtype=author&query=Kulis%2C+B">Brian Kulis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advances in vision-language models have combined contrastive
approaches with generative methods to achieve state-of-the-art (SOTA) on
downstream inference tasks like zero-shot image classification. However, a
persistent issue of these models for image classification is their
out-of-distribution (OOD) generalization capabilities. We first show that when
an OOD data point is misclassified, the correct class can be typically found in
the Top-K predicted classes. In order to steer the model prediction toward the
correct class within the top predicted classes, we propose the Image-Caption
Encoding (ICE) method, a straightforward approach that directly enforces
consistency between the image-conditioned and caption-conditioned predictions
at evaluation time only. Intuitively, we take advantage of unique properties of
the generated captions to guide our local search for the correct class label
within the Top-K predicted classes. We show that our method can be easily
combined with other SOTA methods to enhance Top-1 OOD accuracies by 0.5% on
average and up to 3% on challenging datasets. Our code:
https://github.com/Chris210634/ice
</p>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02663" title="Abstract">arXiv:2402.02663</a> [<a href="/pdf/2402.02663" title="Download PDF">pdf</a>, <a href="/format/2402.02663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counterfactual Fairness Is Not Demographic Parity, and Other  Observations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Silva%2C+R">Ricardo Silva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Blanket statements of equivalence between causal concepts and purely
probabilistic concepts should be approached with care. In this short note, I
examine a recent claim that counterfactual fairness is equivalent to
demographic parity. The claim fails to hold up upon closer examination. I will
take the opportunity to address some broader misunderstandings about
counterfactual fairness.
</p>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02665" title="Abstract">arXiv:2402.02665</a> [<a href="/pdf/2402.02665" title="Download PDF">pdf</a>, <a href="/ps/2402.02665" title="Download PostScript">ps</a>, <a href="/format/2402.02665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Utility-Based Reinforcement Learning: Unifying Single-objective and  Multi-objective Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vamplew%2C+P">Peter Vamplew</a>, 
<a href="/search/cs?searchtype=author&query=Foale%2C+C">Cameron Foale</a>, 
<a href="/search/cs?searchtype=author&query=Hayes%2C+C+F">Conor F. Hayes</a>, 
<a href="/search/cs?searchtype=author&query=Mannion%2C+P">Patrick Mannion</a>, 
<a href="/search/cs?searchtype=author&query=Howley%2C+E">Enda Howley</a>, 
<a href="/search/cs?searchtype=author&query=Dazeley%2C+R">Richard Dazeley</a>, 
<a href="/search/cs?searchtype=author&query=Johnson%2C+S">Scott Johnson</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%A4llstr%C3%B6m%2C+J">Johan K&#xe4;llstr&#xf6;m</a>, 
<a href="/search/cs?searchtype=author&query=Ramos%2C+G">Gabriel Ramos</a>, 
<a href="/search/cs?searchtype=author&query=R%C4%83dulescu%2C+R">Roxana R&#x103;dulescu</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%B6pke%2C+W">Willem R&#xf6;pke</a>, 
<a href="/search/cs?searchtype=author&query=Roijers%2C+D+M">Diederik M. Roijers</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for the Blue Sky Track at AAMAS'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Research in multi-objective reinforcement learning (MORL) has introduced the
utility-based paradigm, which makes use of both environmental rewards and a
function that defines the utility derived by the user from those rewards. In
this paper we extend this paradigm to the context of single-objective
reinforcement learning (RL), and outline multiple potential benefits including
the ability to perform multi-policy learning across tasks relating to uncertain
objectives, risk-aware RL, discounting, and safe RL. We also examine the
algorithmic implications of adopting a utility-based approach.
</p>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02667" title="Abstract">arXiv:2402.02667</a> [<a href="/pdf/2402.02667" title="Download PDF">pdf</a>, <a href="/format/2402.02667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Priori Error Estimation of Physics-Informed Neural Networks Solving  Allen--Cahn and Cahn--Hilliard Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+G">Guangtao Zhang</a>, 
<a href="/search/math?searchtype=author&query=Lin%2C+J">Jiani Lin</a>, 
<a href="/search/math?searchtype=author&query=Zhai%2C+Q">Qijia Zhai</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+H">Huiyu Yang</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+X">Xujun Chen</a>, 
<a href="/search/math?searchtype=author&query=Zheng%2C+X">Xiaoning Zheng</a>, 
<a href="/search/math?searchtype=author&query=Leong%2C+I+T">Ieng Tak Leong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper aims to analyze errors in the implementation of the
Physics-Informed Neural Network (PINN) for solving the Allen--Cahn (AC) and
Cahn--Hilliard (CH) partial differential equations (PDEs). The accuracy of PINN
is still challenged when dealing with strongly non-linear and higher-order
time-varying PDEs. To address this issue, we introduce a stable and bounded
self-adaptive weighting scheme known as Residuals-RAE, which ensures fair
training and effectively captures the solution. By incorporating this new
training loss function, we conduct numerical experiments on 1D and 2D AC and CH
systems to validate our theoretical findings. Our theoretical analysis
demonstrates that feedforward neural networks with two hidden layers and tanh
activation function effectively bound the PINN approximation errors for the
solution field, temporal derivative, and nonlinear term of the AC and CH
equations by the training loss and number of collocation points.
</p>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02668" title="Abstract">arXiv:2402.02668</a> [<a href="/pdf/2402.02668" title="Download PDF">pdf</a>, <a href="/format/2402.02668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Practical Rateless Set Reconciliation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gilad%2C+Y">Yossi Gilad</a>, 
<a href="/search/cs?searchtype=author&query=Alizadeh%2C+M">Mohammad Alizadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Set reconciliation, where two parties hold fixed-length bit strings and run a
protocol to learn the strings they are missing from each other, is a
fundamental task in many distributed systems. We present Rateless Invertible
Bloom Lookup Tables (Rateless IBLT), the first set reconciliation protocol, to
the best of our knowledge, that achieves low computation cost and near-optimal
communication cost across a wide range of scenarios: set differences of one to
millions, bit strings of a few bytes to megabytes, and workloads injected by
potential adversaries. Rateless IBLT is based on a novel encoder that
incrementally encodes the set difference into an infinite stream of coded
symbols, resembling rateless error-correcting codes. We compare Rateless IBLT
with state-of-the-art set reconciliation schemes and demonstrate significant
improvements. Rateless IBLT achieves 3--4x lower communication cost than
non-rateless schemes with similar computation cost, and 2--2000x lower
computation cost than schemes with similar communication cost. We show the
real-world benefits of Rateless IBLT by applying it to synchronize the state of
the Ethereum blockchain, and demonstrate 5.6x lower end-to-end completion time
and 4.4x lower communication cost compared to the system used in production.
</p>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02673" title="Abstract">arXiv:2402.02673</a> [<a href="/pdf/2402.02673" title="Download PDF">pdf</a>, <a href="/format/2402.02673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Framework of Multi-Stage Multi-Winner Voting: An Axiomatic  Exploration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+S">Shengjie Gong</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Lingxiao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shuangping Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhiqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+T">Tao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xiang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chunxue Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Multi-winner voting plays a crucial role in selecting representative
committees based on voter preferences. Previous research has predominantly
focused on single-stage voting rules, which are susceptible to manipulation
during preference collection. In order to mitigate manipulation and increase
the cost associated with it, we propose the introduction of multiple stages in
the voting procedure, leading to the development of a unified framework of
multi-stage multi-winner voting rules. To shed light on this framework of
voting methods, we conduct an axiomatic study, establishing provable conditions
for achieving desired axioms within our model. Our theoretical findings can
serve as a guide for the selection of appropriate multi-stage multi-winner
voting rules.
</p>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02675" title="Abstract">arXiv:2402.02675</a> [<a href="/pdf/2402.02675" title="Download PDF">pdf</a>, <a href="/format/2402.02675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Verifiable evaluations of machine learning models using zkSNARKs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=South%2C+T">Tobin South</a>, 
<a href="/search/cs?searchtype=author&query=Camuto%2C+A">Alexander Camuto</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+S">Shrey Jain</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+S">Shayla Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Mahari%2C+R">Robert Mahari</a>, 
<a href="/search/cs?searchtype=author&query=Paquin%2C+C">Christian Paquin</a>, 
<a href="/search/cs?searchtype=author&query=Morton%2C+J">Jason Morton</a>, 
<a href="/search/cs?searchtype=author&query=Pentland%2C+A+%27">Alex &#x27;Sandy&#x27; Pentland</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">In a world of increasing closed-source commercial machine learning models,
model evaluations from developers must be taken at face value. These benchmark
results, whether over task accuracy, bias evaluations, or safety checks, are
traditionally impossible to verify by a model end-user without the costly or
impossible process of re-performing the benchmark on black-box model outputs.
This work presents a method of verifiable model evaluation using model
inference through zkSNARKs. The resulting zero-knowledge computational proofs
of model outputs over datasets can be packaged into verifiable evaluation
attestations showing that models with fixed private weights achieve stated
performance or fairness metrics over public inputs. These verifiable
attestations can be performed on any standard neural network model with varying
compute requirements. For the first time, we demonstrate this across a sample
of real-world models and highlight key challenges and design solutions. This
presents a new transparency paradigm in the verifiable evaluation of private
models.
</p>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02676" title="Abstract">arXiv:2402.02676</a> [<a href="/pdf/2402.02676" title="Download PDF">pdf</a>, <a href="/format/2402.02676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Gig&#x27;s Up: How ChatGPT Stacks Up Against Quora on Gig Economy  Insights
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lancaster%2C+T">Thomas Lancaster</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Generative AI is changing the way in which humans seek to find answers to
questions in different fields including on the gig economy and labour markets,
but there is limited information available about closely ChatGPT simulated
output matches that obtainable from existing question and answer platforms.
This paper uses ChatGPT as a research assistant to explore how far ChatGPT can
replicate Quora question and answers, using data from the gig economy as an
indicative case study. The results from content analysis suggest that Quora is
likely to be asked questions from users looking to make money and answers are
likely to include personal experiences and examples. ChatGPT simulated versions
are less personal and more concept-based, including considerations on
employment implications and labour rights. It appears therefore that generative
AI simulates only part of what a human would want in their answers relating to
the gig economy. The paper proposes that a similar comparative methodology
would also be useful across other research fields to help in establishing the
best real world uses of generative AI.
</p>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02678" title="Abstract">arXiv:2402.02678</a> [<a href="/pdf/2402.02678" title="Download PDF">pdf</a>, <a href="/format/2402.02678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counterfactual Explanations of Black-box Machine Learning Models using  Causal Discovery with Applications to Credit Rating
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takahashi%2C+D">Daisuke Takahashi</a>, 
<a href="/search/cs?searchtype=author&query=Shimizu%2C+S">Shohei Shimizu</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+T">Takuma Tanaka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Explainable artificial intelligence (XAI) has helped elucidate the internal
mechanisms of machine learning algorithms, bolstering their reliability by
demonstrating the basis of their predictions. Several XAI models consider
causal relationships to explain models by examining the input-output
relationships of prediction models and the dependencies between features. The
majority of these models have been based their explanations on counterfactual
probabilities, assuming that the causal graph is known. However, this
assumption complicates the application of such models to real data, given that
the causal relationships between features are unknown in most cases. Thus, this
study proposed a novel XAI framework that relaxed the constraint that the
causal graph is known. This framework leveraged counterfactual probabilities
and additional prior information on causal structure, facilitating the
integration of a causal graph estimated through causal discovery methods and a
black-box classification model. Furthermore, explanatory scores were estimated
based on counterfactual probabilities. Numerical experiments conducted
employing artificial data confirmed the possibility of estimating the
explanatory score more accurately than in the absence of a causal graph.
Finally, as an application to real data, we constructed a classification model
of credit ratings assigned by Shiga Bank, Shiga prefecture, Japan. We
demonstrated the effectiveness of the proposed method in cases where the causal
graph is unknown.
</p>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02680" title="Abstract">arXiv:2402.02680</a> [<a href="/pdf/2402.02680" title="Download PDF">pdf</a>, <a href="/format/2402.02680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models are Geographically Biased
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manvi%2C+R">Rohin Manvi</a>, 
<a href="/search/cs?searchtype=author&query=Khanna%2C+S">Samar Khanna</a>, 
<a href="/search/cs?searchtype=author&query=Burke%2C+M">Marshall Burke</a>, 
<a href="/search/cs?searchtype=author&query=Lobell%2C+D">David Lobell</a>, 
<a href="/search/cs?searchtype=author&query=Ermon%2C+S">Stefano Ermon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) inherently carry the biases contained in their
training corpora, which can lead to the perpetuation of societal harm. As the
impact of these foundation models grows, understanding and evaluating their
biases becomes crucial to achieving fairness and accuracy. We propose to study
what LLMs know about the world we live in through the lens of geography. This
approach is particularly powerful as there is ground truth for the numerous
aspects of human life that are meaningfully projected onto geographic space
such as culture, race, language, politics, and religion. We show various
problematic geographic biases, which we define as systemic errors in geospatial
predictions. Initially, we demonstrate that LLMs are capable of making accurate
zero-shot geospatial predictions in the form of ratings that show strong
monotonic correlation with ground truth (Spearman's $\rho$ of up to 0.89). We
then show that LLMs exhibit common biases across a range of objective and
subjective topics. In particular, LLMs are clearly biased against locations
with lower socioeconomic conditions (e.g. most of Africa) on a variety of
sensitive subjective topics such as attractiveness, morality, and intelligence
(Spearman's $\rho$ of up to 0.70). Finally, we introduce a bias score to
quantify this and find that there is significant variation in the magnitude of
bias across existing LLMs.
</p>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02681" title="Abstract">arXiv:2402.02681</a> [<a href="/pdf/2402.02681" title="Download PDF">pdf</a>, <a href="/format/2402.02681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equivariant Symmetry Breaking Sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">YuQing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Smidt%2C+T">Tess Smidt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 30 figures Submitted to ICML 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Equivariant neural networks (ENNs) have been shown to be extremely effective
in applications involving underlying symmetries. By construction ENNs cannot
produce lower symmetry outputs given a higher symmetry input. However,
spontaneous symmetry breaking occurs in many physical systems and we may obtain
a less symmetric stable state from an initial highly symmetric one. Hence, it
is imperative that we understand how to systematically break symmetry in ENNs.
In this work, we propose a novel symmetry breaking framework that is fully
equivariant. We emphasize that our approach is general and applicable to
equivariance under any group. To achieve this, we introduce the idea of
symmetry breaking sets (SBS). Rather than redesign existing networks, we design
sets of symmetry breaking objects which we feed into our network based on the
symmetry of our inputs and outputs. We show there is a natural way to define
equivariance on these sets, which gives an additional constraint. Minimizing
the size of these sets equates to data efficiency. We prove that minimizing
these sets translates to a well studied group theory problem, and tabulate
solutions to this problem for the point groups. Finally, we provide some
examples of symmetry breaking to demonstrate how our approach works in
practice.
</p>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02687" title="Abstract">arXiv:2402.02687</a> [<a href="/pdf/2402.02687" title="Download PDF">pdf</a>, <a href="/format/2402.02687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Poisson Process for Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoxing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaxing Li</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+C">Chao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weifeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaokang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">BayesianOptimization(BO) is a sample-efficient black-box optimizer, and
extensive methods have been proposed to build the absolute function response of
the black-box function through a probabilistic surrogate model, including
Tree-structured Parzen Estimator (TPE), random forest (SMAC), and Gaussian
process (GP). However, few methods have been explored to estimate the relative
rankings of candidates, which can be more robust to noise and have better
practicality than absolute function responses, especially when the function
responses are intractable but preferences can be acquired. To this end, we
propose a novel ranking-based surrogate model based on the Poisson process and
introduce an efficient BO framework, namely Poisson Process Bayesian
Optimization (PoPBO). Two tailored acquisition functions are further derived
from classic LCB and EI to accommodate it. Compared to the classic GP-BO
method, our PoPBO has lower computation costs and better robustness to noise,
which is verified by abundant experiments. The results on both simulated and
real-world benchmarks, including hyperparameter optimization (HPO) and neural
architecture search (NAS), show the effectiveness of PoPBO.
</p>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02688" title="Abstract">arXiv:2402.02688</a> [<a href="/pdf/2402.02688" title="Download PDF">pdf</a>, <a href="/ps/2402.02688" title="Download PostScript">ps</a>, <a href="/format/2402.02688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Successive Bayesian Reconstructor for FAS Channel Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zijian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jieao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+L">Linglong Dai</a>, 
<a href="/search/cs?searchtype=author&query=Heath%2C+R+W">Robert W. Heath Jr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE WCNC 2024. This paper proposes S-BAR as a general solution to estimate FAS channels. More insights can be found in the journal version of this paper: <a href="/abs/2312.06551">arXiv:2312.06551</a>. arXiv admin note: substantial text overlap with <a href="/abs/2312.06551">arXiv:2312.06551</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP); Systems and Control (eess.SY)

</div>
<p class="mathjax">Fluid antenna systems (FASs) can reconfigure their locations freely within a
spatially continuous space. To keep favorable antenna positions, the channel
state information (CSI) acquisition for FASs is essential. While some
techniques have been proposed, most existing FAS channel estimators require
several channel assumptions, such as slow variation and angular-domain
sparsity. When these assumptions are not reasonable, the model mismatch may
lead to unpredictable performance loss. In this paper, we propose the
successive Bayesian reconstructor (S-BAR) as a general solution to estimate FAS
channels. Unlike model-based estimators, the proposed S-BAR is prior-aided,
which builds the experiential kernel for CSI acquisition. Inspired by Bayesian
regression, the key idea of S-BAR is to model the FAS channels as a stochastic
process, whose uncertainty can be successively eliminated by kernel-based
sampling and regression. In this way, the predictive mean of the regressed
stochastic process can be viewed as the maximum a posterior (MAP) estimator of
FAS channels. Simulation results verify that, in both model-mismatched and
model-matched cases, the proposed S-BAR can achieve higher estimation accuracy
than the existing schemes.
</p>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02690" title="Abstract">arXiv:2402.02690</a> [<a href="/pdf/2402.02690" title="Download PDF">pdf</a>, <a href="/format/2402.02690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Competitive Equilibrium in Microgrids With Dynamic Loads
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Salehi%2C+Z">Zeinab Salehi</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Y">Yijun Chen</a>, 
<a href="/search/eess?searchtype=author&query=Petersen%2C+I+R">Ian R. Petersen</a>, 
<a href="/search/eess?searchtype=author&query=Ratnam%2C+E+L">Elizabeth L. Ratnam</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+G">Guodong Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, we consider microgrids that interconnect prosumers with
distributed energy resources and dynamic loads. Prosumers are connected through
the microgrid to trade energy and gain profit while respecting the network
constraints. We establish a local energy market by defining a competitive
equilibrium which balances energy and satisfies voltage constraints within the
microgrid for all time. Using duality theory, we prove that under some
convexity assumptions, a competitive equilibrium is equivalent to a social
welfare maximization solution. Additionally, we show that a competitive
equilibrium is equivalent to a Nash equilibrium of a standard game. In general,
the energy price for each prosumer is different, leading to the concept of
locational prices. We investigate a case under which all prosumers have the
same locational prices. Additionally, we show that under some assumptions on
the resource supply and network topology, locational prices decay to zero after
a period of time, implying the available supply will be more than the demand
required to stabilize the system. Finally, two numerical examples are provided
to validate the results, one of which is a direct application of our results on
electric vehicle charging control.
</p>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02691" title="Abstract">arXiv:2402.02691</a> [<a href="/pdf/2402.02691" title="Download PDF">pdf</a>, <a href="/ps/2402.02691" title="Download PostScript">ps</a>, <a href="/format/2402.02691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ALIVE: A Low-Cost Interactive Vaccine Storage Environment Module  ensuring easy portability and remote tracking of operational logistics to the  last mile
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Datta%2C+A">Arkadeep Datta</a> (1), 
<a href="/search/eess?searchtype=author&query=Mukhopadhyay%2C+A">Arani Mukhopadhyay</a> (1 and 2), 
<a href="/search/eess?searchtype=author&query=Datta%2C+A">Amitava Datta</a> (1), 
<a href="/search/eess?searchtype=author&query=Ganguly%2C+R">Ranjan Ganguly</a> (1) ((1) Advanced Materials Research and Applications (AMRA) Laboratory, Department of Power Engineering, Jadavpur University, Kolkata, India. (2) University of Illinois Chicago, US.)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at the International Conference on Robotics, Control, Automation, and Artificial Intelligence (RCAAI 2023). Corresponding: arkadeepdatta@gmail.com
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The COVID-19 pandemic has profoundly reshaped our lives, prompting a search
for solutions to its far-reaching effects. Vaccines emerged as a beacon of
hope, yet reaching remote areas faces last-mile hurdles and cost issues due to
loss of vaccine potency due to poor temperature regulation of the storage units
and unanticipated vaccine wastage en route, a common occurrence in conventional
vaccine transportation methods. We introduce ALIVE, a low-cost Interactive
Vaccine Storage Environment module. ALIVE provides an off-grid, self-sufficient
solution for vaccine storage and transport, enabled by active cooling
technology. ALIVE's innovation lies in its integration with the Internet of
Things (IoT), allowing real-time monitoring and control. This IoT-enabled
Application Programming Interface (API) features a data acquisition and
environment parameter control system, managing oversight and decision-making.
ALIVE's compact, lightweight design makes it adaptable to various logistical
scenarios, while its versatility enables it to maintain both time-invariant and
time-dependent thermophysical and spatial parameters. Operationalized through a
PID algorithm, ALIVE ensures precise temperature control within the vaccine
chamber. Its dynamic features, such as remote actuation and data sharing,
demonstrate its adaptability and potential applications. Despite the frugal
nature of development, the system promises significant benefits, including
reduced vaccine loss and remote monitoring advantages. Collaborations with
healthcare partners seek to further enhance ALIVE's readiness and expand its
impact. ALIVE revolutionizes vaccine logistics, offering scalable,
cost-effective solutions for bridging accessibility gaps in challenging
distribution scenarios. Its adaptability positions it for widespread
application, from last-mile vaccine delivery to environment-controlled supply
chains and beyond.
</p>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02692" title="Abstract">arXiv:2402.02692</a> [<a href="/pdf/2402.02692" title="Download PDF">pdf</a>, <a href="/format/2402.02692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Guarantees for Link Prediction using Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chung%2C+A">Alan Chung</a>, 
<a href="/search/cs?searchtype=author&query=Saberi%2C+A">Amin Saberi</a>, 
<a href="/search/cs?searchtype=author&query=Austern%2C+M">Morgane Austern</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">This paper derives statistical guarantees for the performance of Graph Neural
Networks (GNNs) in link prediction tasks on graphs generated by a graphon. We
propose a linear GNN architecture (LG-GNN) that produces consistent estimators
for the underlying edge probabilities. We establish a bound on the mean squared
error and give guarantees on the ability of LG-GNN to detect high-probability
edges. Our guarantees hold for both sparse and dense graphs. Finally, we
demonstrate some of the shortcomings of the classical GCN architecture, as well
as verify our results on real and synthetic datasets.
</p>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02695" title="Abstract">arXiv:2402.02695</a> [<a href="/pdf/2402.02695" title="Download PDF">pdf</a>, <a href="/format/2402.02695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Class Probabilities for Black-box Sentence-level Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moraffah%2C+R">Raha Moraffah</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huan Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Sentence-level attacks craft adversarial sentences that are synonymous with
correctly-classified sentences but are misclassified by the text classifiers.
Under the black-box setting, classifiers are only accessible through their
feedback to queried inputs, which is predominately available in the form of
class probabilities. Even though utilizing class probabilities results in
stronger attacks, due to the challenges of using them for sentence-level
attacks, existing attacks use either no feedback or only the class labels.
Overcoming the challenges, we develop a novel algorithm that uses class
probabilities for black-box sentence-level attacks, investigate the
effectiveness of using class probabilities on the attack's success, and examine
the question if it is worthy or practical to use class probabilities by
black-box sentence-level attacks. We conduct extensive evaluations of the
proposed attack comparing with the baselines across various classifiers and
benchmark datasets.
</p>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02696" title="Abstract">arXiv:2402.02696</a> [<a href="/pdf/2402.02696" title="Download PDF">pdf</a>, <a href="/format/2402.02696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Feature Selection for Responsible Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moraffah%2C+R">Raha Moraffah</a>, 
<a href="/search/cs?searchtype=author&query=Sheth%2C+P">Paras Sheth</a>, 
<a href="/search/cs?searchtype=author&query=Vishnubhatla%2C+S">Saketh Vishnubhatla</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huan Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Machine Learning (ML) has become an integral aspect of many real-world
applications. As a result, the need for responsible machine learning has
emerged, focusing on aligning ML models to ethical and social values, while
enhancing their reliability and trustworthiness. Responsible ML involves many
issues. This survey addresses four main issues: interpretability, fairness,
adversarial robustness, and domain generalization. Feature selection plays a
pivotal role in the responsible ML tasks. However, building upon statistical
correlations between variables can lead to spurious patterns with biases and
compromised performance. This survey focuses on the current study of causal
feature selection: what it is and how it can reinforce the four aspects of
responsible ML. By identifying features with causal impacts on outcomes and
distinguishing causality from correlation, causal feature selection is posited
as a unique approach to ensuring ML models to be ethically and socially
responsible in high-stakes applications.
</p>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02697" title="Abstract">arXiv:2402.02697</a> [<a href="/pdf/2402.02697" title="Download PDF">pdf</a>, <a href="/ps/2402.02697" title="Download PostScript">ps</a>, <a href="/format/2402.02697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Equilibrium Models are Almost Equivalent to Not-so-deep Explicit  Models for High-dimensional Gaussian Mixtures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ling%2C+Z">Zenan Ling</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Longbo Li</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zhanbo Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yixuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Feng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+R+C">Robert C. Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Z">Zhenyu Liao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Deep equilibrium models (DEQs), as a typical implicit neural network, have
demonstrated remarkable success on various tasks. There is, however, a lack of
theoretical understanding of the connections and differences between implicit
DEQs and explicit neural network models. In this paper, leveraging recent
advances in random matrix theory (RMT), we perform an in-depth analysis on the
eigenspectra of the conjugate kernel (CK) and neural tangent kernel (NTK)
matrices for implicit DEQs, when the input data are drawn from a
high-dimensional Gaussian mixture. We prove, in this setting, that the spectral
behavior of these Implicit-CKs and NTKs depend on the DEQ activation function
and initial weight variances, but only via a system of four nonlinear
equations. As a direct consequence of this theoretical result, we demonstrate
that a shallow explicit network can be carefully designed to produce the same
CK or NTK as a given DEQ. Despite derived here for Gaussian mixture data,
empirical results show the proposed theory and design principle also apply to
popular real-world datasets.
</p>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02698" title="Abstract">arXiv:2402.02698</a> [<a href="/pdf/2402.02698" title="Download PDF">pdf</a>, <a href="/format/2402.02698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Expectations: Learning with Stochastic Dominance Made Practical
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cen%2C+S">Shicong Cen</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+J">Jincheng Mei</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+H">Hanjun Dai</a>, 
<a href="/search/cs?searchtype=author&query=Schuurmans%2C+D">Dale Schuurmans</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+Y">Yuejie Chi</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+B">Bo Dai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
<p class="mathjax">Stochastic dominance models risk-averse preferences for decision making with
uncertain outcomes, which naturally captures the intrinsic structure of the
underlying uncertainty, in contrast to simply resorting to the expectations.
Despite theoretically appealing, the application of stochastic dominance in
machine learning has been scarce, due to the following challenges:
$\textbf{i)}$, the original concept of stochastic dominance only provides a
$\textit{partial order}$, therefore, is not amenable to serve as an optimality
criterion; and $\textbf{ii)}$, an efficient computational recipe remains
lacking due to the continuum nature of evaluating stochastic dominance.%, which
barriers its application for machine learning.
<br />In this work, we make the first attempt towards establishing a general
framework of learning with stochastic dominance. We first generalize the
stochastic dominance concept to enable feasible comparisons between any
arbitrary pair of random variables. We next develop a simple and
computationally efficient approach for finding the optimal solution in terms of
stochastic dominance, which can be seamlessly plugged into many learning tasks.
Numerical experiments demonstrate that the proposed method achieves comparable
performance as standard risk-neutral strategies and obtains better trade-offs
against risk across a variety of applications including supervised learning,
reinforcement learning, and portfolio optimization.
</p>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02699" title="Abstract">arXiv:2402.02699</a> [<a href="/pdf/2402.02699" title="Download PDF">pdf</a>, <a href="/format/2402.02699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Data Augmentation for Robust Speaker Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhenyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junhui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Namin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lantian Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Data augmentation (DA) has gained widespread popularity in deep speaker
models due to its ease of implementation and significant effectiveness. It
enriches training data by simulating real-life acoustic variations, enabling
deep neural networks to learn speaker-related representations while
disregarding irrelevant acoustic variations, thereby improving robustness and
generalization. However, a potential issue with the vanilla DA is augmentation
residual, i.e., unwanted distortion caused by different types of augmentation.
To address this problem, this paper proposes a novel approach called
adversarial data augmentation (A-DA) which combines DA with adversarial
learning. Specifically, it involves an additional augmentation classifier to
categorize various augmentation types used in data augmentation. This
adversarial learning empowers the network to generate speaker embeddings that
can deceive the augmentation classifier, making the learned speaker embeddings
more robust in the face of augmentation variations. Experiments conducted on
VoxCeleb and CN-Celeb datasets demonstrate that our proposed A-DA outperforms
standard DA in both augmentation matched and mismatched test conditions,
showcasing its superior robustness and generalization against acoustic
variations.
</p>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02700" title="Abstract">arXiv:2402.02700</a> [<a href="/pdf/2402.02700" title="Download PDF">pdf</a>, <a href="/ps/2402.02700" title="Download PostScript">ps</a>, <a href="/format/2402.02700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sample Complexity Characterization for Linear Contextual MDPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Junze Deng</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+S">Shaofeng Zou</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yingbin Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to AIstats2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Contextual Markov decision processes (CMDPs) describe a class of
reinforcement learning problems in which the transition kernels and reward
functions can change over time with different MDPs indexed by a context
variable. While CMDPs serve as an important framework to model many real-world
applications with time-varying environments, they are largely unexplored from
theoretical perspective. In this paper, we study CMDPs under two linear
function approximation models: Model I with context-varying representations and
common linear weights for all contexts; and Model II with common
representations for all contexts and context-varying linear weights. For both
models, we propose novel model-based algorithms and show that they enjoy
guaranteed $\epsilon$-suboptimality gap with desired polynomial sample
complexity. In particular, instantiating our result for the first model to the
tabular CMDP improves the existing result by removing the reachability
assumption. Our result for the second model is the first-known result for such
a type of function approximation models. Comparison between our results for the
two models further indicates that having context-varying features leads to much
better sample efficiency than having common representations for all contexts
under linear CMDPs.
</p>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02701" title="Abstract">arXiv:2402.02701</a> [<a href="/pdf/2402.02701" title="Download PDF">pdf</a>, <a href="/format/2402.02701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding What Affects Generalization Gap in Visual Reinforcement  Learning: Theory and Empirical Evidence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+J">Jiafei Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+L">Le Wan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiu Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zongqing Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Part of this work is accepted as AAMAS 2024 extended abstract
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Recently, there are many efforts attempting to learn useful policies for
continuous control in visual reinforcement learning (RL). In this scenario, it
is important to learn a generalizable policy, as the testing environment may
differ from the training environment, e.g., there exist distractors during
deployment. Many practical algorithms are proposed to handle this problem.
However, to the best of our knowledge, none of them provide a theoretical
understanding of what affects the generalization gap and why their proposed
methods work. In this paper, we bridge this issue by theoretically answering
the key factors that contribute to the generalization gap when the testing
environment has distractors. Our theories indicate that minimizing the
representation distance between training and testing environments, which aligns
with human intuition, is the most critical for the benefit of reducing the
generalization gap. Our theoretical results are supported by the empirical
evidence in the DMControl Generalization Benchmark (DMC-GB).
</p>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02705" title="Abstract">arXiv:2402.02705</a> [<a href="/pdf/2402.02705" title="Download PDF">pdf</a>, <a href="/format/2402.02705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Representation Surgery for Multi-Task Model Merging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+E">Enneng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhenyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+G">Guibing Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaojun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xingwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Multi-task learning (MTL) compresses the information from multiple tasks into
a unified backbone to improve computational efficiency and generalization.
Recent work directly merges multiple independently trained models to perform
MTL instead of collecting their raw data for joint training, greatly expanding
the application scenarios of MTL. However, by visualizing the representation
distribution of existing model merging schemes, we find that the merged model
often suffers from the dilemma of representation bias. That is, there is a
significant discrepancy in the representation distribution between the merged
and individual models, resulting in poor performance of merged MTL. In this
paper, we propose a representation surgery solution called "Surgery" to reduce
representation bias in the merged model. Specifically, Surgery is a lightweight
task-specific module that takes the representation of the merged model as input
and attempts to output the biases contained in the representation from the
merged model. We then designed an unsupervised optimization objective that
updates the Surgery module by minimizing the distance between the merged
model's representation and the individual model's representation. Extensive
experiments demonstrate significant MTL performance improvements when our
Surgery module is applied to state-of-the-art (SOTA) model merging schemes.
</p>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02708" title="Abstract">arXiv:2402.02708</a> [<a href="/pdf/2402.02708" title="Download PDF">pdf</a>, <a href="/format/2402.02708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CRANE: A Redundant, Multi-Degree-of-Freedom Computed Tomography Robot  for Heightened Needle Dexterity within a Medical Imaging Bore
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schreiber%2C+D">Dimitrious Schreiber</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhaowei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Henderson%2C+T">Taylor Henderson</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Derek Chen</a>, 
<a href="/search/cs?searchtype=author&query=Norbasha%2C+A">Alexander Norbasha</a>, 
<a href="/search/cs?searchtype=author&query=Yip%2C+M+C">Michael C. Yip</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 13 figures, Transactions on Robotics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Computed Tomography (CT) image guidance enables accurate and safe minimally
invasive treatment of diseases, including cancer and chronic pain, with
needle-like tools via a percutaneous approach. The physician incrementally
inserts and adjusts the needle with intermediate images due to the accuracy
limitation of free-hand adjustment and patient physiological motion. Scanning
frequency is limited to minimize ionizing radiation exposure for the patient
and physician. Robots can provide high positional accuracy and compensate for
physiological motion with fewer scans. To accomplish this, the robots must
operate within the confined imaging bore while retaining sufficient dexterity
to insert and manipulate the needle. This paper presents CRANE: CT Robotic Arm
and Needle Emplacer, a CT-compatible robot with a design focused on system
dexterity that enables physicians to manipulate and insert needles within the
scanner bore as naturally as they would be able to by hand. We define abstract
and measurable clinically motivated metrics for in-bore dexterity applicable to
general-purpose intra-bore image-guided needle placement robots, develop an
automatic robot planning and control method for intra-bore needle manipulation
and device setup, and demonstrate the redundant linkage design provides
dexterity across various human morphology and meets the clinical requirements
for target accuracy during an in-situ evaluation.
</p>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02711" title="Abstract">arXiv:2402.02711</a> [<a href="/pdf/2402.02711" title="Download PDF">pdf</a>, <a href="/format/2402.02711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Architectural Strategies for the optimization of Physics-Informed Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saratchandran%2C+H">Hemanth Saratchandran</a>, 
<a href="/search/cs?searchtype=author&query=Chng%2C+S">Shin-Fang Chng</a>, 
<a href="/search/cs?searchtype=author&query=Lucey%2C+S">Simon Lucey</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Physics-informed neural networks (PINNs) offer a promising avenue for
tackling both forward and inverse problems in partial differential equations
(PDEs) by incorporating deep learning with fundamental physics principles.
Despite their remarkable empirical success, PINNs have garnered a reputation
for their notorious training challenges across a spectrum of PDEs. In this
work, we delve into the intricacies of PINN optimization from a neural
architecture perspective. Leveraging the Neural Tangent Kernel (NTK), our study
reveals that Gaussian activations surpass several alternate activations when it
comes to effectively training PINNs. Building on insights from numerical linear
algebra, we introduce a preconditioned neural architecture, showcasing how such
tailored architectures enhance the optimization process. Our theoretical
findings are substantiated through rigorous validation against established PDEs
within the scientific literature.
</p>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02712" title="Abstract">arXiv:2402.02712</a> [<a href="/pdf/2402.02712" title="Download PDF">pdf</a>, <a href="/format/2402.02712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unconditionally energy stable IEQ-FEMs for the Cahn-Hilliard equation  and Allen-Cahn equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+Y">Yaoyao Chen</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+H">Hailiang Liu</a>, 
<a href="/search/math?searchtype=author&query=Yi%2C+N">Nianyu Yi</a>, 
<a href="/search/math?searchtype=author&query=Yin%2C+P">Peimeng Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we present several unconditionally energy-stable invariant
energy quadratization (IEQ) finite element methods (FEMs) with linear, first-
and second-order accuracy for solving both the Cahn-Hilliard equation and the
Allen-Cahn equation. For time discretization, we compare three distinct IEQ-FEM
schemes that position the intermediate function introduced by the IEQ approach
in different function spaces: finite element space, continuous function space,
or a combination of these spaces. Rigorous proofs establishing the existence
and uniqueness of the numerical solution, along with analyses of energy
dissipation for both equations and mass conservation for the Cahn-Hilliard
equation, are provided. The proposed schemes' accuracy, efficiency, and
solution properties are demonstrated through numerical experiments.
</p>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02713" title="Abstract">arXiv:2402.02713</a> [<a href="/pdf/2402.02713" title="Download PDF">pdf</a>, <a href="/format/2402.02713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Position Paper: What Can Large Language Models Tell Us about Time Series  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+M">Ming Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kexin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuxuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Bin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shirui Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Q">Qingsong Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 8 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Time series analysis is essential for comprehending the complexities inherent
in various real-world systems and applications. Although large language models
(LLMs) have recently made significant strides, the development of artificial
general intelligence (AGI) equipped with time series analysis capabilities
remains in its nascent phase. Most existing time series models heavily rely on
domain knowledge and extensive model tuning, predominantly focusing on
prediction tasks. In this paper, we argue that current LLMs have the potential
to revolutionize time series analysis, thereby promoting efficient
decision-making and advancing towards a more universal form of time series
analytical intelligence. Such advancement could unlock a wide range of
possibilities, including modality switching and time series question answering.
We encourage researchers and practitioners to recognize the potential of LLMs
in advancing time series analysis and emphasize the need for trust in these
related efforts. Furthermore, we detail the seamless integration of time series
analysis with existing LLM technologies and outline promising avenues for
future research.
</p>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02716" title="Abstract">arXiv:2402.02716</a> [<a href="/pdf/2402.02716" title="Download PDF">pdf</a>, <a href="/format/2402.02716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the planning of LLM agents: A survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weiwen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaolong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xingmei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+D">Defu Lian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yasheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+R">Ruiming Tang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Enhong Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 tables, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">As Large Language Models (LLMs) have shown significant intelligence, the
progress to leverage LLMs as planning modules of autonomous agents has
attracted more attention. This survey provides the first systematic view of
LLM-based agents planning, covering recent works aiming to improve planning
ability. We provide a taxonomy of existing works on LLM-Agent planning, which
can be categorized into Task Decomposition, Plan Selection, External Module,
Reflection and Memory. Comprehensive analyses are conducted for each direction,
and further challenges for the field of research are discussed.
</p>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02718" title="Abstract">arXiv:2402.02718</a> [<a href="/pdf/2402.02718" title="Download PDF">pdf</a>, <a href="/format/2402.02718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising Time Cycle Modeling for Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Sicong Xie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qunwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weidi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+K">Kaiming Shen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shaohu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+W">Wenliang Zhong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recently, modeling temporal patterns of user-item interactions have attracted
much attention in recommender systems. We argue that existing methods ignore
the variety of temporal patterns of user behaviors. We define the subset of
user behaviors that are irrelevant to the target item as noises, which limits
the performance of target-related time cycle modeling and affect the
recommendation performance. In this paper, we propose Denoising Time Cycle
Modeling (DiCycle), a novel approach to denoise user behaviors and select the
subset of user behaviors that are highly related to the target item. DiCycle is
able to explicitly model diverse time cycle patterns for recommendation.
Extensive experiments are conducted on both public benchmarks and a real-world
dataset, demonstrating the superior performance of DiCycle over the
state-of-the-art recommendation methods.
</p>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02719" title="Abstract">arXiv:2402.02719</a> [<a href="/pdf/2402.02719" title="Download PDF">pdf</a>, <a href="/ps/2402.02719" title="Download PostScript">ps</a>, <a href="/format/2402.02719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Budget-feasible Egalitarian Allocation of Conflicting Jobs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Sushmita Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+P">Pallavi Jain</a>, 
<a href="/search/cs?searchtype=author&query=Mohanapriya%2C+A">A. Mohanapriya</a>, 
<a href="/search/cs?searchtype=author&query=Tripathi%2C+V">Vikash Tripathi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in 23rd International Conference on Autonomous Agents and Multiagent Systems(AAMAS 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Allocating conflicting jobs among individuals while respecting a budget
constraint for each individual is an optimization problem that arises in
various real-world scenarios. In this paper, we consider the situation where
each individual derives some satisfaction from each job. We focus on finding a
feasible allocation of conflicting jobs that maximize egalitarian cost, i.e.
the satisfaction of the \nc{individual who is worst-off}. To the best of our
knowledge, this is the first paper to combine egalitarianism,
budget-feasibility, and conflict-freeness in allocations. We provide a
systematic study of the computational complexity of finding budget-feasible
conflict-free egalitarian allocation and show that our problem generalizes a
large number of classical optimization problems. Therefore, unsurprisingly, our
problem is \NPH even for two individuals and when there is no conflict between
any jobs. We show that the problem admits algorithms when studied in the realm
of approximation algorithms and parameterized algorithms with a host of natural
parameters that match and in some cases improve upon the running time of known
algorithms.
</p>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02720" title="Abstract">arXiv:2402.02720</a> [<a href="/pdf/2402.02720" title="Download PDF">pdf</a>, <a href="/format/2402.02720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discounted Adaptive Online Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bombara%2C+D">David Bombara</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Heng Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Online learning is not always about memorizing everything. Since the future
can be statistically very different from the past, a critical challenge is to
gracefully forget the history while new data comes in. To formalize this
intuition, we revisit the classical notion of discounted regret using recently
developed techniques in adaptive online learning. Our main result is a new
algorithm that adapts to the complexity of both the loss sequence and the
comparator, improving the widespread non-adaptive algorithm - gradient descent
with a constant learning rate. In particular, our theoretical guarantee does
not require any structural assumption beyond convexity, and the algorithm is
provably robust to suboptimal hyperparameter tuning. We further demonstrate
such benefits through online conformal prediction, a downstream online learning
task with set-membership decisions.
</p>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02724" title="Abstract">arXiv:2402.02724</a> [<a href="/pdf/2402.02724" title="Download PDF">pdf</a>, <a href="/format/2402.02724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FDNet: Frequency Domain Denoising Network For Cell Segmentation in  Astrocytes Derived From Induced Pluripotent Stem Cells
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoran Li</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jiahua Shi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huaming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+B">Bo Du</a>, 
<a href="/search/cs?searchtype=author&query=Maksour%2C+S">Simon Maksour</a>, 
<a href="/search/cs?searchtype=author&query=Phillips%2C+G">Gabrielle Phillips</a>, 
<a href="/search/cs?searchtype=author&query=Dottori%2C+M">Mirella Dottori</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jun Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by The IEEE International Symposium on Biomedical Imaging (ISBI) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Artificially generated induced pluripotent stem cells (iPSCs) from somatic
cells play an important role for disease modeling and drug screening of
neurodegenerative diseases. Astrocytes differentiated from iPSCs are important
targets to investigate neuronal metabolism. The astrocyte differentiation
progress can be monitored through the variations of morphology observed from
microscopy images at different differentiation stages, then determined by
molecular biology techniques upon maturation. However, the astrocytes usually
``perfectly'' blend into the background and some of them are covered by
interference information (i.e., dead cells, media sediments, and cell debris),
which makes astrocytes difficult to observe. Due to the lack of annotated
datasets, the existing state-of-the-art deep learning approaches cannot be used
to address this issue. In this paper, we introduce a new task named astrocyte
segmentation with a novel dataset, called IAI704, which contains 704 images and
their corresponding pixel-level annotation masks. Moreover, a novel frequency
domain denoising network, named FDNet, is proposed for astrocyte segmentation.
In detail, our FDNet consists of a contextual information fusion module (CIF),
an attention block (AB), and a Fourier transform block (FTB). CIF and AB fuse
multi-scale feature embeddings to localize the astrocytes. FTB transforms
feature embeddings into the frequency domain and conducts a high-pass filter to
eliminate interference information. Experimental results demonstrate the
superiority of our proposed FDNet over the state-of-the-art substitutes in
astrocyte segmentation, shedding insights for iPSC differentiation progress
prediction.
</p>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02725" title="Abstract">arXiv:2402.02725</a> [<a href="/pdf/2402.02725" title="Download PDF">pdf</a>, <a href="/ps/2402.02725" title="Download PostScript">ps</a>, <a href="/format/2402.02725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Innovative Cybersickness Detection: Exploring Head Movement Patterns in  Virtual Reality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salehi%2C+M">Masoud Salehi</a>, 
<a href="/search/cs?searchtype=author&query=Javadpour%2C+N">Nikoo Javadpour</a>, 
<a href="/search/cs?searchtype=author&query=Beisner%2C+B">Brietta Beisner</a>, 
<a href="/search/cs?searchtype=author&query=Sanaei%2C+M">Mohammadamin Sanaei</a>, 
<a href="/search/cs?searchtype=author&query=Gilbert%2C+S+B">Stephen B. Gilbert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 3 Figures, 3 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Despite the widespread adoption of Virtual Reality (VR) technology,
cybersickness remains a barrier for some users. This research investigates head
movement patterns as a novel physiological marker for cybersickness detection.
Unlike traditional markers, head movements provide a continuous, non-invasive
measure that can be easily captured through the sensors embedded in all
commercial VR headsets. We used a publicly available dataset from a VR
experiment involving 75 participants and analyzed head movements across six
axes. An extensive feature extraction process was then performed on the head
movement dataset and its derivatives, including velocity, acceleration, and
jerk. Three categories of features were extracted, encompassing statistical,
temporal, and spectral features. Subsequently, we employed the Recursive
Feature Elimination method to select the most important and effective features.
In a series of experiments, we trained a variety of machine learning
algorithms. The results demonstrate a 76% accuracy and 83% precision in
predicting cybersickness in the subjects based on the head movements. This
study contribution to the cybersickness literature lies in offering a
preliminary analysis of a new source of data and providing insight into the
relationship of head movements and cybersickness.
</p>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02726" title="Abstract">arXiv:2402.02726</a> [<a href="/pdf/2402.02726" title="Download PDF">pdf</a>, <a href="/format/2402.02726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How do software practitioners perceive human-centric defects?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chauhan%2C+V">Vedant Chauhan</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+C">Chetan Arora</a>, 
<a href="/search/cs?searchtype=author&query=Khalajzadeh%2C+H">Hourieh Khalajzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Grundy%2C+J">John Grundy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Context: Human-centric software design and development focuses on how users
want to carry out their tasks rather than making users accommodate their
software. Software users can have different genders, ages, cultures, languages,
disabilities, socioeconomic statuses, and educational backgrounds, among many
other differences. Due to the inherently varied nature of these differences and
their impact on software usage, preferences and issues of users can vary,
resulting in user-specific defects that we term as `human-centric defects'
(HCDs).
<br />Objective: This research aims to understand the perception and current
management practices of such human-centric defects by software practitioners,
identify key challenges in reporting, understanding and fixing them, and
provide recommendations to improve HCDs management in software engineering.
<br />Method: We conducted a survey and interviews with software engineering
practitioners to gauge their knowledge and experience on HCDs and the defect
tracking process.
<br />Results: We analysed fifty (50) survey- and ten (10) interview- responses
from SE practitioners and identified that there are multiple gaps in the
current management of HCDs in software engineering practice. There is a lack of
awareness regarding human-centric aspects, causing them to be lost or
under-appreciated during software development. Our results revealed that
handling HCDs could be improved by following a better feedback process with
end-users, a more descriptive taxonomy, and suitable automation.
<br />Conclusion: HCDs present a major challenge to software practitioners, given
their diverse end-user base. In the software engineering domain, research on
HCDs has been limited and requires effort from the research and practice
communities to create better awareness and support regarding human-centric
aspects.
</p>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02727" title="Abstract">arXiv:2402.02727</a> [<a href="/pdf/2402.02727" title="Download PDF">pdf</a>, <a href="/format/2402.02727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Local Projection Stabilised HHO Method for the Oseen Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Mallik%2C+G">Gouranga Mallik</a>, 
<a href="/search/math?searchtype=author&query=Biswas%2C+R">Rahul Biswas</a>, 
<a href="/search/math?searchtype=author&query=Gudi%2C+T">Thirupathi Gudi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Fluid flow problems with high Reynolds number show spurious oscillations in
their solution when solved using standard Galerkin finite element methods.
These Oscillations can be eradicated using various stabilisation techniques. In
this article, we use a local projection stabilisation for a Hybrid High-Order
approximation of the Oseen problem. We prove an existence-uniqueness result
under a SUPG-like norm. We derive an optimal order error estimate under this
norm for equal order polynomial discretisation of velocity and pressure spaces.
</p>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02729" title="Abstract">arXiv:2402.02729</a> [<a href="/pdf/2402.02729" title="Download PDF">pdf</a>, <a href="/ps/2402.02729" title="Download PostScript">ps</a>, <a href="/format/2402.02729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast and Accurate Cooperative Radio Map Estimation Enabled by GAN
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zezhong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+G">Guangxu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junting Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+S">Shuguang Cui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">In the 6G era, real-time radio resource monitoring and management are urged
to support diverse wireless-empowered applications. This calls for fast and
accurate estimation on the distribution of the radio resources, which is
usually represented by the spatial signal power strength over the geographical
environment, known as a radio map. In this paper, we present a cooperative
radio map estimation (CRME) approach enabled by the generative adversarial
network (GAN), called as GAN-CRME, which features fast and accurate radio map
estimation without the transmitters' information. The radio map is inferred by
exploiting the interaction between distributed received signal strength (RSS)
measurements at mobile users and the geographical map using a deep neural
network estimator, resulting in low data-acquisition cost and computational
complexity. Moreover, a GAN-based learning algorithm is proposed to boost the
inference capability of the deep neural network estimator by exploiting the
power of generative AI. Simulation results showcase that the proposed GAN-CRME
is even capable of coarse error-correction when the geographical map
information is inaccurate.
</p>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02730" title="Abstract">arXiv:2402.02730</a> [<a href="/pdf/2402.02730" title="Download PDF">pdf</a>, <a href="/ps/2402.02730" title="Download PostScript">ps</a>, <a href="/format/2402.02730" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How phonemes contribute to deep speaker models?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pengqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lantian Li</a>, 
<a href="/search/cs?searchtype=author&query=Hamdulla%2C+A">Askar Hamdulla</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dong Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>

</div>
<p class="mathjax">Which phonemes convey more speaker traits is a long-standing question, and
various perception experiments were conducted with human subjects. For speaker
recognition, studies were conducted with the conventional statistical models
and the drawn conclusions are more or less consistent with the perception
results. However, which phonemes are more important with modern deep neural
models is still unexplored, due to the opaqueness of the decision process. This
paper conducts a novel study for the attribution of phonemes with two types of
deep speaker models that are based on TDNN and CNN respectively, from the
perspective of model explanation. Specifically, we conducted the study by two
post-explanation methods: LayerCAM and Time Align Occlusion (TAO). Experimental
results showed that: (1) At the population level, vowels are more important
than consonants, confirming the human perception studies. However, fricatives
are among the most unimportant phonemes, which contrasts with previous studies.
(2) At the speaker level, a large between-speaker variation is observed
regarding phoneme importance, indicating that whether a phoneme is important or
not is largely speaker-dependent.
</p>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02731" title="Abstract">arXiv:2402.02731</a> [<a href="/pdf/2402.02731" title="Download PDF">pdf</a>, <a href="/format/2402.02731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing Augustin Information via Hybrid Geodesically Convex  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guan-Ren Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tsai%2C+C">Chung-En Tsai</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hao-Chung Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yen-Huan Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">We propose a Riemannian gradient descent with the Poincar\'e metric to
compute the order-$\alpha$ Augustin information, a widely used quantity for
characterizing exponential error behaviors in information theory. We prove that
the algorithm converges to the optimum at a rate of $\mathcal{O}(1 / T)$. As
far as we know, this is the first algorithm with a non-asymptotic optimization
error guarantee for all positive orders. Numerical experimental results
demonstrate the empirical efficiency of the algorithm. Our result is based on a
novel hybrid analysis of Riemannian gradient descent for functions that are
geodesically convex in a Riemannian metric and geodesically smooth in another.
</p>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02732" title="Abstract">arXiv:2402.02732</a> [<a href="/pdf/2402.02732" title="Download PDF">pdf</a>, <a href="/format/2402.02732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Generative Approach to Surrogate-based Black-box Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moraffah%2C+R">Raha Moraffah</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huan Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Surrogate-based black-box attacks have exposed the heightened vulnerability
of DNNs. These attacks are designed to craft adversarial examples for any
samples with black-box target feedback for only a given set of samples.
State-of-the-art surrogate-based attacks involve training a discriminative
surrogate that mimics the target's outputs. The goal is to learn the decision
boundaries of the target. The surrogate is then attacked by white-box attacks
to craft adversarial examples similar to the original samples but belong to
other classes. With limited samples, the discriminative surrogate fails to
accurately learn the target's decision boundaries, and these surrogate-based
attacks suffer from low success rates. Different from the discriminative
approach, we propose a generative surrogate that learns the distribution of
samples residing on or close to the target's decision boundaries. The
distribution learned by the generative surrogate can be used to craft
adversarial examples that have imperceptible differences from the original
samples but belong to other classes. The proposed generative approach results
in attacks with remarkably high attack success rates on various targets and
datasets.
</p>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02733" title="Abstract">arXiv:2402.02733</a> [<a href="/pdf/2402.02733" title="Download PDF">pdf</a>, <a href="/format/2402.02733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+B">Bumsoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Muqeet%2C+A">Abdul Muqeet</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kyuchul Lee</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+S">Sanghyun Seo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 9 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
<p class="mathjax">Face re-aging is a prominent field in computer vision and graphics, with
significant applications in photorealistic domains such as movies, advertising,
and live streaming. Recently, the need to apply face re-aging to
non-photorealistic images, like comics, illustrations, and animations, has
emerged as an extension in various entertainment sectors. However, the absence
of a network capable of seamlessly editing the apparent age on NPR images means
that these tasks have been confined to a naive approach, applying each task
sequentially. This often results in unpleasant artifacts and a loss of facial
attributes due to domain discrepancies. In this paper, we introduce a novel
one-stage method for face re-aging combined with portrait style transfer,
executed in a single generative step. We leverage existing face re-aging and
style transfer networks, both trained within the same PR domain. Our method
uniquely fuses distinct latent vectors, each responsible for managing
aging-related attributes and NPR appearance. Adopting an exemplar-based
approach, our method offers greater flexibility than domain-level fine-tuning
approaches, which typically require separate training or fine-tuning for each
domain. This effectively addresses the limitation of requiring paired datasets
for re-aging and domain-level, data-driven approaches for stylization. Our
experiments show that our model can effortlessly generate re-aged images while
simultaneously transferring the style of examples, maintaining both natural
appearance and controllability.
</p>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02734" title="Abstract">arXiv:2402.02734</a> [<a href="/pdf/2402.02734" title="Download PDF">pdf</a>, <a href="/format/2402.02734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InVA: Integrative Variational Autoencoder for Harmonization of  Multi-modal Neuroimaging Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+B">Bowen Lei</a>, 
<a href="/search/cs?searchtype=author&query=Guhaniyogi%2C+R">Rajarshi Guhaniyogi</a>, 
<a href="/search/cs?searchtype=author&query=Chandra%2C+K">Krishnendu Chandra</a>, 
<a href="/search/cs?searchtype=author&query=Scheffler%2C+A">Aaron Scheffler</a>, 
<a href="/search/cs?searchtype=author&query=Mallick%2C+B">Bani Mallick</a> (for the Alzheimer&#x27;s Disease Neuroimaging Initiative)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Neural and Evolutionary Computing (cs.NE); Applications (stat.AP); Machine Learning (stat.ML)

</div>
<p class="mathjax">There is a significant interest in exploring non-linear associations among
multiple images derived from diverse imaging modalities. While there is a
growing literature on image-on-image regression to delineate predictive
inference of an image based on multiple images, existing approaches have
limitations in efficiently borrowing information between multiple imaging
modalities in the prediction of an image. Building on the literature of
Variational Auto Encoders (VAEs), this article proposes a novel approach,
referred to as Integrative Variational Autoencoder (\texttt{InVA}) method,
which borrows information from multiple images obtained from different sources
to draw predictive inference of an image. The proposed approach captures
complex non-linear association between the outcome image and input images,
while allowing rapid computation. Numerical results demonstrate substantial
advantages of \texttt{InVA} over VAEs, which typically do not allow borrowing
information between input images. The proposed framework offers highly accurate
predictive inferences for costly positron emission topography (PET) from
multiple measures of cortical structure in human brain scans readily available
from magnetic resonance imaging (MRI).
</p>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02735" title="Abstract">arXiv:2402.02735</a> [<a href="/pdf/2402.02735" title="Download PDF">pdf</a>, <a href="/format/2402.02735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Timed-Elastic-Band Based Variable Splitting for Autonomous Trajectory  Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhu%2C+H">Hao Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Jin%2C+K">Kefan Jin</a>, 
<a href="/search/eess?searchtype=author&query=Gao%2C+R">Rui Gao</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jialin Wang</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+C+-+R">C.-J. Richard Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Existing trajectory planning methods are struggling to handle the issue of
autonomous track swinging during navigation, resulting in significant errors
when reaching the destination. In this article, we address autonomous
trajectory planning problems, which aims at developing innovative solutions to
enhance the adaptability and robustness of unmanned systems in navigating
complex and dynamic environments. We first introduce the variable splitting
(VS) method as a constrained optimization method to reimagine the renowned
Timed-Elastic-Band (TEB) algorithm, resulting in a novel collision avoidance
approach named Timed-Elastic-Band based variable splitting (TEB-VS). The
proposed TEB-VS demonstrates superior navigation stability, while maintaining
nearly identical resource consumption to TEB. We then analyze the convergence
of the proposed TEB-VS method. To evaluate the effectiveness and efficiency of
TEB-VS, extensive experiments have been conducted using TurtleBot2 in both
simulated environments and real-world datasets.
</p>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02736" title="Abstract">arXiv:2402.02736</a> [<a href="/pdf/2402.02736" title="Download PDF">pdf</a>, <a href="/format/2402.02736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Motion Cues to Supervise Single-Frame Body Pose and Shape  Estimation in Low Data Regimes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Davydov%2C+A">Andrey Davydov</a>, 
<a href="/search/cs?searchtype=author&query=Sidnev%2C+A">Alexey Sidnev</a>, 
<a href="/search/cs?searchtype=author&query=Sanakoyeu%2C+A">Artsiom Sanakoyeu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuhua Chen</a>, 
<a href="/search/cs?searchtype=author&query=Salzmann%2C+M">Mathieu Salzmann</a>, 
<a href="/search/cs?searchtype=author&query=Fua%2C+P">Pascal Fua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages; TMLR
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">When enough annotated training data is available, supervised deep-learning
algorithms excel at estimating human body pose and shape using a single camera.
The effects of too little such data being available can be mitigated by using
other information sources, such as databases of body shapes, to learn priors.
Unfortunately, such sources are not always available either. We show that, in
such cases, easy-to-obtain unannotated videos can be used instead to provide
the required supervisory signals. Given a trained model using too little
annotated data, we compute poses in consecutive frames along with the optical
flow between them. We then enforce consistency between the image optical flow
and the one that can be inferred from the change in pose from one frame to the
next. This provides enough additional supervision to effectively refine the
network weights and to perform on par with methods trained using far more
annotated data.
</p>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02738" title="Abstract">arXiv:2402.02738</a> [<a href="/pdf/2402.02738" title="Download PDF">pdf</a>, <a href="/format/2402.02738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Robustness of LiDAR-Camera Fusion Model against Weather  Corruption from Fusion Strategy Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yihao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+K">Kaiyuan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qing Guo</a>, 
<a href="/search/cs?searchtype=author&query=Juefei-Xu%2C+F">Felix Juefei-Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xiaojun Jia</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianlin Li</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+G">Geguang Pu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In recent years, LiDAR-camera fusion models have markedly advanced 3D object
detection tasks in autonomous driving. However, their robustness against common
weather corruption such as fog, rain, snow, and sunlight in the intricate
physical world remains underexplored. In this paper, we evaluate the robustness
of fusion models from the perspective of fusion strategies on the corrupted
dataset. Based on the evaluation, we further propose a concise yet practical
fusion strategy to enhance the robustness of the fusion models, namely flexibly
weighted fusing features from LiDAR and camera sources to adapt to varying
weather scenarios. Experiments conducted on four types of fusion models, each
with two distinct lightweight implementations, confirm the broad applicability
and effectiveness of the approach.
</p>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02739" title="Abstract">arXiv:2402.02739</a> [<a href="/pdf/2402.02739" title="Download PDF">pdf</a>, <a href="/format/2402.02739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sui%2C+Y">Yang Sui</a>, 
<a href="/search/cs?searchtype=author&query=Phan%2C+H">Huy Phan</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jinqi Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianfang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zijie Tang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Cong Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yingying Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+B">Bo Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">In the exciting generative AI era, the diffusion model has emerged as a very
powerful and widely adopted content generation and editing tool for various
data modalities, making the study of their potential security risks very
necessary and critical. Very recently, some pioneering works have shown the
vulnerability of the diffusion model against backdoor attacks, calling for
in-depth analysis and investigation of the security challenges of this popular
and fundamental AI technique.
<br />In this paper, for the first time, we systematically explore the
detectability of the poisoned noise input for the backdoored diffusion models,
an important performance metric yet little explored in the existing works.
Starting from the perspective of a defender, we first analyze the properties of
the trigger pattern in the existing diffusion backdoor attacks, discovering the
important role of distribution discrepancy in Trojan detection. Based on this
finding, we propose a low-cost trigger detection mechanism that can effectively
identify the poisoned input noise. We then take a further step to study the
same problem from the attack side, proposing a backdoor attack strategy that
can learn the unnoticeable trigger to evade our proposed detection scheme.
<br />Empirical evaluations across various diffusion models and datasets
demonstrate the effectiveness of the proposed trigger detection and
detection-evading attack strategy. For trigger detection, our distribution
discrepancy-based solution can achieve a 100\% detection rate for the Trojan
triggers used in the existing works. For evading trigger detection, our
proposed stealthy trigger design approach performs end-to-end learning to make
the distribution of poisoned noise input approach that of benign noise,
enabling nearly 100\% detection pass rate with very high attack and benign
performance for the backdoored diffusion models.
</p>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02741" title="Abstract">arXiv:2402.02741</a> [<a href="/pdf/2402.02741" title="Download PDF">pdf</a>, <a href="/format/2402.02741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Glocal Hypergradient Estimation with Koopman Operator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hataya%2C+R">Ryuichiro Hataya</a>, 
<a href="/search/cs?searchtype=author&query=Kawahara%2C+Y">Yoshinobu Kawahara</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Gradient-based hyperparameter optimization methods update hyperparameters
using hypergradients, gradients of a meta criterion with respect to
hyperparameters. Previous research used two distinct update strategies:
optimizing hyperparameters using global hypergradients obtained after
completing model training or local hypergradients derived after every few model
updates. While global hypergradients offer reliability, their computational
cost is significant; conversely, local hypergradients provide speed but are
often suboptimal. In this paper, we propose glocal hypergradient estimation,
blending "global" quality with "local" efficiency. To this end, we use the
Koopman operator theory to linearize the dynamics of hypergradients so that the
global hypergradients can be efficiently approximated only by using a
trajectory of local hypergradients. Consequently, we can optimize
hyperparameters greedily using estimated global hypergradients, achieving both
reliability and efficiency simultaneously. Through numerical experiments of
hyperparameter optimization, including optimization of optimizers, we
demonstrate the effectiveness of the glocal hypergradient estimation.
</p>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02746" title="Abstract">arXiv:2402.02746</a> [<a href="/pdf/2402.02746" title="Download PDF">pdf</a>, <a href="/format/2402.02746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Standard Gaussian Process is All You Need for High-Dimensional Bayesian  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhitong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhe%2C+S">Shandian Zhe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">There has been a long-standing and widespread belief that Bayesian
Optimization (BO) with standard Gaussian process (GP), referred to as standard
BO, is ineffective in high-dimensional optimization problems. This perception
may partly stem from the intuition that GPs struggle with high-dimensional
inputs for covariance modeling and function estimation. While these concerns
seem reasonable, empirical evidence supporting this belief is lacking. In this
paper, we systematically investigated BO with standard GP regression across a
variety of synthetic and real-world benchmark problems for high-dimensional
optimization. Surprisingly, the performance with standard GP consistently ranks
among the best, often outperforming existing BO methods specifically designed
for high-dimensional optimization by a large margin. Contrary to the
stereotype, we found that standard GP can serve as a capable surrogate for
learning high-dimensional target functions. Without strong structural
assumptions, BO with standard GP not only excels in high-dimensional
optimization but also proves robust in accommodating various structures within
the target functions. Furthermore, with standard GP, achieving promising
optimization performance is possible by only using maximum likelihood
estimation, eliminating the need for expensive Markov-Chain Monte Carlo (MCMC)
sampling that might be required by more complex surrogate models. We thus
advocate for a re-evaluation and in-depth study of the potential of standard BO
in addressing high-dimensional problems.
</p>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02750" title="Abstract">arXiv:2402.02750</a> [<a href="/pdf/2402.02750" title="Download PDF">pdf</a>, <a href="/format/2402.02750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zirui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jiayi Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+H">Hongye Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+S">Shaochen Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhaozhuo Xu</a>, 
<a href="/search/cs?searchtype=author&query=Braverman%2C+V">Vladimir Braverman</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Beidi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xia Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Performance (cs.PF)

</div>
<p class="mathjax">Efficiently serving large language models (LLMs) requires batching many
requests together to reduce the cost per request. Yet, the key-value (KV)
cache, which stores attention keys and values to avoid re-computations,
significantly increases memory demands and becomes the new bottleneck in speed
and memory usage. This memory demand increases with larger batch sizes and
longer context lengths. Additionally, the inference speed is limited by the
size of KV cache, as the GPU's SRAM must load the entire KV cache from the main
GPU memory for each token generated, causing the computational core to be idle
during this process. A straightforward and effective solution to reduce KV
cache size is quantization, which decreases the total bytes taken by KV cache.
However, there is a lack of in-depth studies that explore the element
distribution of KV cache to understand the hardness and limitation of KV cache
quantization. To fill the gap, we conducted a comprehensive study on the
element distribution in KV cache of popular LLMs. Our findings indicate that
the key cache should be quantized per-channel, i.e., group elements along the
channel dimension and quantize them together. In contrast, the value cache
should be quantized per-token. From this analysis, we developed a tuning-free
2bit KV cache quantization algorithm, named KIVI. With the hardware-friendly
implementation, KIVI can enable Llama (Llama-2), Falcon, and Mistral models to
maintain almost the same quality while using $\mathbf{2.6\times}$ less peak
memory usage (including the model weight). This reduction in memory usage
enables up to $\mathbf{4\times}$ larger batch size, bringing
$\mathbf{2.35\times \sim 3.47\times}$ throughput on real LLM inference
workload. The source code is available at https://github.com/jy-yuan/KIVI.
</p>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02754" title="Abstract">arXiv:2402.02754</a> [<a href="/pdf/2402.02754" title="Download PDF">pdf</a>, <a href="/format/2402.02754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Focal Modulation Networks for Interpretable Sound Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Della+Libera%2C+L">Luca Della Libera</a>, 
<a href="/search/cs?searchtype=author&query=Subakan%2C+C">Cem Subakan</a>, 
<a href="/search/cs?searchtype=author&query=Ravanelli%2C+M">Mirco Ravanelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICASSP 2024 XAI-SA Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The increasing success of deep neural networks has raised concerns about
their inherent black-box nature, posing challenges related to interpretability
and trust. While there has been extensive exploration of interpretation
techniques in vision and language, interpretability in the audio domain has
received limited attention, primarily focusing on post-hoc explanations. This
paper addresses the problem of interpretability by-design in the audio domain
by utilizing the recently proposed attention-free focal modulation networks
(FocalNets). We apply FocalNets to the task of environmental sound
classification for the first time and evaluate their interpretability
properties on the popular ESC-50 dataset. Our method outperforms a similarly
sized vision transformer both in terms of accuracy and interpretability.
Furthermore, it is competitive against PIQ, a method specifically designed for
post-hoc interpretation in the audio domain.
</p>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02758" title="Abstract">arXiv:2402.02758</a> [<a href="/pdf/2402.02758" title="Download PDF">pdf</a>, <a href="/format/2402.02758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Effects of Shared Autonomy on Cognitive Load and Trust in  Human-Robot Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Jiahe Pan</a>, 
<a href="/search/cs?searchtype=author&query=Eden%2C+J">Jonathan Eden</a>, 
<a href="/search/cs?searchtype=author&query=Oetomo%2C+D">Denny Oetomo</a>, 
<a href="/search/cs?searchtype=author&query=Johal%2C+W">Wafa Johal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Teleoperation is increasingly recognized as a viable solution for deploying
robots in hazardous environments. Controlling a robot to perform a complex or
demanding task may overload operators resulting in poor performance. To design
a robot controller to assist the human in executing such challenging tasks, a
comprehensive understanding of the interplay between the robot's autonomous
behavior and the operator's internal state is essential. In this paper, we
investigate the relationships between robot autonomy and both the human user's
cognitive load and trust levels, and the potential existence of three-way
interactions in the robot-assisted execution of the task. Our user study (N=24)
results indicate that while autonomy level influences the teleoperator's
perceived cognitive load and trust, there is no clear interaction between these
factors. Instead, these elements appear to operate independently, thus
highlighting the need to consider both cognitive load and trust as distinct but
interrelated factors in varying the robot autonomy level in shared-control
settings. This insight is crucial for the development of more effective and
adaptable assistive robotic systems.
</p>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02761" title="Abstract">arXiv:2402.02761</a> [<a href="/pdf/2402.02761" title="Download PDF">pdf</a>, <a href="/ps/2402.02761" title="Download PostScript">ps</a>, <a href="/format/2402.02761" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transmission Line Detection Based on Improved Hough Transform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+W">Wei Song</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Man Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">To address the challenges of low detection accuracy and high false positive
rates of transmission lines in UAV (Unmanned Aerial Vehicle) images, we explore
the linear features and spatial distribution. We introduce an enhanced
stochastic Hough transform technique tailored for detecting transmission lines
in complex backgrounds. By employing the Hessian matrix for initial
preprocessing of transmission lines, and utilizing boundary search and pixel
row segmentation, our approach distinguishes transmission line areas from the
background. We significantly reduce both false positives and missed detections,
thereby improving the accuracy of transmission line identification. Experiments
demonstrate that our method not only processes images more rapidly, but also
yields superior detection results compared to conventional and random Hough
transform methods.
</p>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02763" title="Abstract">arXiv:2402.02763</a> [<a href="/pdf/2402.02763" title="Download PDF">pdf</a>, <a href="/format/2402.02763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meshfree multiscale method with partially explicit time discretization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Nikiforov%2C+D">Djulustan Nikiforov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, a multiscale approach with partially explicit time
discretization is proposed. The idea is to use a partially explicit time
scheme, considering a filtration problem in a fractured medium, where the
implicit scheme is used for nodes whose subdomains contain fractures, and the
explicit scheme is used for all others. In this way, it is possible to use a
time step that is independent of the diffusion coefficient for fractures.
Numerical results demonstrating high accuracy of calculations are presented.
</p>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02764" title="Abstract">arXiv:2402.02764</a> [<a href="/pdf/2402.02764" title="Download PDF">pdf</a>, <a href="/format/2402.02764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> List-aware Reranking-Truncation Joint Model for Search and  Retrieval-augmented Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shicheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+L">Liang Pang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Huawei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">The results of information retrieval (IR) are usually presented in the form
of a ranked list of candidate documents, such as web search for humans and
retrieval-augmented generation for large language models (LLMs). List-aware
retrieval aims to capture the list-level contextual features to return a better
list, mainly including reranking and truncation. Reranking finely re-scores the
documents in the list. Truncation dynamically determines the cut-off point of
the ranked list to achieve the trade-off between overall relevance and avoiding
misinformation from irrelevant documents. Previous studies treat them as two
separate tasks and model them separately. However, the separation is not
optimal. First, it is hard to share the contextual information of the ranking
list between the two tasks. Second, the separate pipeline usually meets the
error accumulation problem, where the small error from the reranking stage can
largely affect the truncation stage. To solve these problems, we propose a
Reranking-Truncation joint model (GenRT) that can perform the two tasks
concurrently. GenRT integrates reranking and truncation via generative paradigm
based on encoder-decoder architecture. We also design the novel loss functions
for joint optimization to make the model learn both tasks. Sharing parameters
by the joint model is conducive to making full use of the common modeling
information of the two tasks. Besides, the two tasks are performed concurrently
and co-optimized to solve the error accumulation problem between separate
stages. Experiments on public learning-to-rank benchmarks and open-domain Q\&amp;A
tasks show that our method achieves SOTA performance on both reranking and
truncation tasks for web search and retrieval-augmented LLMs.
</p>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02768" title="Abstract">arXiv:2402.02768</a> [<a href="/pdf/2402.02768" title="Download PDF">pdf</a>, <a href="/ps/2402.02768" title="Download PostScript">ps</a>, <a href="/format/2402.02768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intent Profiling and Translation Through Emergent Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mostafa%2C+S">Salwa Mostafa</a>, 
<a href="/search/cs?searchtype=author&query=Elbamby%2C+M+S">Mohammed S. Elbamby</a>, 
<a href="/search/cs?searchtype=author&query=Abdel-Aziz%2C+M+K">Mohamed K. Abdel-Aziz</a>, 
<a href="/search/cs?searchtype=author&query=Bennis%2C+M">Mehdi Bennis</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE International Conference on Communications (ICC2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">To effectively express and satisfy network application requirements,
intent-based network management has emerged as a promising solution. In
intent-based methods, users and applications express their intent in a
high-level abstract language to the network. Although this abstraction
simplifies network operation, it induces many challenges to efficiently express
applications' intents and map them to different network capabilities.
Therefore, in this work, we propose an AI-based framework for intent profiling
and translation. We consider a scenario where applications interacting with the
network express their needs for network services in their domain language. The
machine-to-machine communication (i.e., between applications and the network)
is complex since it requires networks to learn how to understand the domain
languages of each application, which is neither practical nor scalable.
Instead, a framework based on emergent communication is proposed for intent
profiling, in which applications express their abstract quality-of-experience
(QoE) intents to the network through emergent communication messages.
Subsequently, the network learns how to interpret these communication messages
and map them to network capabilities (i.e., slices) to guarantee the requested
Quality-of-Service (QoS). Simulation results show that the proposed method
outperforms self-learning slicing and other baselines, and achieves a
performance close to the perfect knowledge baseline.
</p>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02769" title="Abstract">arXiv:2402.02769</a> [<a href="/pdf/2402.02769" title="Download PDF">pdf</a>, <a href="/format/2402.02769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning from Teaching Regularization: Generalizable Correlations Should  be Easy to Imitate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+C">Can Jin</a>, 
<a href="/search/cs?searchtype=author&query=Che%2C+T">Tong Che</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hongwu Peng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yiyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Pavone%2C+M">Marco Pavone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Generalization remains a central challenge in machine learning. In this work,
we propose Learning from Teaching (LoT), a novel regularization technique for
deep neural networks to enhance generalization. Inspired by the human ability
to capture concise and abstract patterns, we hypothesize that generalizable
correlations are expected to be easier to teach. LoT operationalizes this
concept to improve the generalization of the main model with auxiliary student
learners. The student learners are trained by the main model and improve the
main model to capture more generalizable and teachable correlations by
providing feedback. Our experimental results across several domains, including
Computer Vision, Natural Language Processing, and Reinforcement Learning,
demonstrate that the introduction of LoT brings significant benefits compared
to merely training models on the original training data. It suggests the
effectiveness of LoT in identifying generalizable information without falling
into the swamp of complex patterns in data, making LoT a valuable addition to
the current machine learning frameworks.
</p>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02771" title="Abstract">arXiv:2402.02771</a> [<a href="/pdf/2402.02771" title="Download PDF">pdf</a>, <a href="/format/2402.02771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TensoSDF: Roughness-aware Tensorial Representation for Robust Geometry  and Material Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jia Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Beibei Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">Reconstructing objects with realistic materials from multi-view images is
problematic, since it is highly ill-posed. Although the neural reconstruction
approaches have exhibited impressive reconstruction ability, they are designed
for objects with specific materials (e.g., diffuse or specular materials). To
this end, we propose a novel framework for robust geometry and material
reconstruction, where the geometry is expressed with the implicit signed
distance field (SDF) encoded by a tensorial representation, namely TensoSDF. At
the core of our method is the roughness-aware incorporation of the radiance and
reflectance fields, which enables a robust reconstruction of objects with
arbitrary reflective materials. Furthermore, the tensorial representation
enhances geometry details in the reconstructed surface and reduces the training
time. Finally, we estimate the materials using an explicit mesh for efficient
intersection computation and an implicit SDF for accurate representation.
Consequently, our method can achieve more robust geometry reconstruction,
outperform the previous works in terms of relighting quality, and reduce 50%
training times and 70% inference time.
</p>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02772" title="Abstract">arXiv:2402.02772</a> [<a href="/pdf/2402.02772" title="Download PDF">pdf</a>, <a href="/format/2402.02772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Diffuser: Planning Towards High Return States via  Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shan%2C+Y">Yixiang Shan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhengbang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+T">Ting Long</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Q">Qifan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yi Chang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weinan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+L">Liang Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages with appendix and references, 10 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Applying diffusion models in reinforcement learning for long-term planning
has gained much attention recently. Several diffusion-based methods have
successfully leveraged the modeling capabilities of diffusion for arbitrary
distributions. These methods generate subsequent trajectories for planning and
have demonstrated significant improvement. However, these methods are limited
by their plain base distributions and their overlooking of the diversity of
samples, in which different states have different returns. They simply leverage
diffusion to learn the distribution of offline dataset, generate the
trajectories whose states share the same distribution with the offline dataset.
As a result, the probability of these models reaching the high-return states is
largely dependent on the dataset distribution. Even equipped with the guidance
model, the performance is still suppressed. To address these limitations, in
this paper, we propose a novel method called CDiffuser, which devises a return
contrast mechanism to pull the states in generated trajectories towards
high-return states while pushing them away from low-return states to improve
the base distribution. Experiments on 14 commonly used D4RL benchmarks
demonstrate the effectiveness of our proposed method. Our code is publicly
available at https://anonymous.4open.science/r/ContrastiveDiffuser.
</p>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02774" title="Abstract">arXiv:2402.02774</a> [<a href="/pdf/2402.02774" title="Download PDF">pdf</a>, <a href="/ps/2402.02774" title="Download PostScript">ps</a>, <a href="/format/2402.02774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Matroid Optimization through Fast Imprecise Oracles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eberle%2C+F">Franziska Eberle</a>, 
<a href="/search/cs?searchtype=author&query=Hommelsheim%2C+F">Felix Hommelsheim</a>, 
<a href="/search/cs?searchtype=author&query=Lindermayr%2C+A">Alexander Lindermayr</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Megow%2C+N">Nicole Megow</a>, 
<a href="/search/cs?searchtype=author&query=Schl%C3%B6ter%2C+J">Jens Schl&#xf6;ter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Querying complex models for precise information (e.g. traffic models,
database systems, large ML models) often entails intense computations and
results in long response times. Thus, weaker models which give imprecise
results quickly can be advantageous, provided inaccuracies can be resolved
using few queries to a stronger model. In the fundamental problem of computing
a maximum-weight basis of a matroid, a well-known generalization of many
combinatorial optimization problems, algorithms have access to a clean oracle
to query matroid information. We additionally equip algorithms with a fast but
dirty oracle modelling an unknown, potentially different matroid. We design and
analyze practical algorithms which only use few clean queries w.r.t. the
quality of the dirty oracle, while maintaining robustness against arbitrarily
poor dirty matroids, approaching the performance of classic algorithms for the
given problem. Notably, we prove that our algorithms are, in many respects,
best-possible. Further, we outline extensions to other matroid oracle types,
non-free dirty oracles and other matroid problems.
</p>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02781" title="Abstract">arXiv:2402.02781</a> [<a href="/pdf/2402.02781" title="Download PDF">pdf</a>, <a href="/format/2402.02781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual Knowledge Distillation for Efficient Sound Event Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+R+K">Rohan Kumar Das</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICASSP 2024 (Deep Neural Network Model Compression Workshop)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Sound event detection (SED) is essential for recognizing specific sounds and
their temporal locations within acoustic signals. This becomes challenging
particularly for on-device applications, where computational resources are
limited. To address this issue, we introduce a novel framework referred to as
dual knowledge distillation for developing efficient SED systems in this work.
Our proposed dual knowledge distillation commences with temporal-averaging
knowledge distillation (TAKD), utilizing a mean student model derived from the
temporal averaging of the student model's parameters. This allows the student
model to indirectly learn from a pre-trained teacher model, ensuring a stable
knowledge distillation. Subsequently, we introduce embedding-enhanced feature
distillation (EEFD), which involves incorporating an embedding distillation
layer within the student model to bolster contextual learning. On DCASE 2023
Task 4A public evaluation dataset, our proposed SED system with dual knowledge
distillation having merely one-third of the baseline model's parameters,
demonstrates superior performance in terms of PSDS1 and PSDS2. This highlights
the importance of proposed dual knowledge distillation for compact SED systems,
which can be ideal for edge devices.
</p>
</div>
</dd>
<dt><a name="item660">[660]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02782" title="Abstract">arXiv:2402.02782</a> [<a href="/pdf/2402.02782" title="Download PDF">pdf</a>, <a href="/format/2402.02782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Partial to Strictly Incremental Constituent Parsing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ezquerro%2C+A">Ana Ezquerro</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B3mez-Rodr%C3%ADguez%2C+C">Carlos G&#xf3;mez-Rodr&#xed;guez</a>, 
<a href="/search/cs?searchtype=author&query=Vilares%2C+D">David Vilares</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We study incremental constituent parsers to assess their capacity to output
trees based on prefix representations alone. Guided by strictly left-to-right
generative language models and tree-decoding modules, we build parsers that
adhere to a strong definition of incrementality across languages. This builds
upon work that asserted incrementality, but that mostly only enforced it on
either the encoder or the decoder. Finally, we conduct an analysis against
non-incremental and partially incremental models.
</p>
</div>
</dd>
<dt><a name="item661">[661]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02790" title="Abstract">arXiv:2402.02790</a> [<a href="/pdf/2402.02790" title="Download PDF">pdf</a>, <a href="/format/2402.02790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stable and Robust Deep Learning By Hyperbolic Tangent Exponential Linear  Unit (TeLU)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fernandez%2C+A">Alfredo Fernandez</a>, 
<a href="/search/cs?searchtype=author&query=Mali%2C+A">Ankur Mali</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">In this paper, we introduce the Hyperbolic Tangent Exponential Linear Unit
(TeLU), a novel neural network activation function, represented as $f(x) =
x{\cdot}tanh(e^x)$. TeLU is designed to overcome the limitations of
conventional activation functions like ReLU, GELU, and Mish by addressing the
vanishing and, to an extent, the exploding gradient problems. Our theoretical
analysis and empirical assessments reveal that TeLU outperforms existing
activation functions in stability and robustness, effectively adjusting
activation outputs' mean towards zero for enhanced training stability and
convergence. Extensive evaluations against popular activation functions (ReLU,
GELU, SiLU, Mish, Logish, Smish) across advanced architectures, including
Resnet-50, demonstrate TeLU's lower variance and superior performance, even
under hyperparameter conditions optimized for other functions. In large-scale
tests with challenging datasets like CIFAR-10, CIFAR-100, and TinyImageNet,
encompassing 860 scenarios, TeLU consistently showcased its effectiveness,
positioning itself as a potential new standard for neural network activation
functions, boosting stability and performance in diverse deep learning
applications.
</p>
</div>
</dd>
<dt><a name="item662">[662]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02791" title="Abstract">arXiv:2402.02791</a> [<a href="/pdf/2402.02791" title="Download PDF">pdf</a>, <a href="/format/2402.02791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Optimization and Architecture for Tiny Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yehui Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fangcheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+Y">Yunsheng Ni</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuchuan Tian</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Z">Zheyuan Bai</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yi-Qi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sichao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jui%2C+S">Shangling Jui</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kai Han</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunhe Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The power of large language models (LLMs) has been demonstrated through
numerous data and computing resources. However, the application of language
models on mobile devices is facing huge challenge on the computation and memory
costs, that is, tiny language models with high performance are urgently
required. Limited by the highly complex training process, there are many
details for optimizing language models that are seldom studied carefully. In
this study, based on a tiny language model with 1B parameters, we carefully
design a series of empirical study to analyze the effect of each component.
Three perspectives are mainly discussed, i.e., neural architecture, parameter
initialization, and optimization strategy. Several design formulas are
empirically proved especially effective for tiny language models, including
tokenizer compression, architecture tweaking, parameter inheritance and
multiple-round training. Then we train PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B
Pro on 1.6T multilingual corpora, following the established formulas.
Experimental results demonstrate the improved optimization and architecture
yield a notable average improvement of 8.87 on benchmark evaluation sets for
PanGu-$\pi$-1B Pro. Besides, PanGu-$\pi$-1.5B Pro surpasses a range of SOTA
models with larger model sizes, validating its superior performance. The code
will be released soon (https://github.com/YuchuanTian/RethinkTinyLM).
</p>
</div>
</dd>
<dt><a name="item663">[663]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02795" title="Abstract">arXiv:2402.02795</a> [<a href="/pdf/2402.02795" title="Download PDF">pdf</a>, <a href="/format/2402.02795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Learning-Based Caching Mechanism for Edge Content Delivery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Torabi%2C+H">Hoda Torabi</a>, 
<a href="/search/cs?searchtype=author&query=Khazaei%2C+H">Hamzeh Khazaei</a>, 
<a href="/search/cs?searchtype=author&query=Litoiu%2C+M">Marin Litoiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)

</div>
<p class="mathjax">With the advent of 5G networks and the rise of the Internet of Things (IoT),
Content Delivery Networks (CDNs) are increasingly extending into the network
edge. This shift introduces unique challenges, particularly due to the limited
cache storage and the diverse request patterns at the edge. These edge
environments can host traffic classes characterized by varied object-size
distributions and object-access patterns. Such complexity makes it difficult
for traditional caching strategies, which often rely on metrics like request
frequency or time intervals, to be effective. Despite these complexities, the
optimization of edge caching is crucial. Improved byte hit rates at the edge
not only alleviate the load on the network backbone but also minimize
operational costs and expedite content delivery to end-users.
<br />In this paper, we introduce HR-Cache, a comprehensive learning-based caching
framework grounded in the principles of Hazard Rate (HR) ordering, a rule
originally formulated to compute an upper bound on cache performance. HR-Cache
leverages this rule to guide future object eviction decisions. It employs a
lightweight machine learning model to learn from caching decisions made based
on HR ordering, subsequently predicting the "cache-friendliness" of incoming
requests. Objects deemed "cache-averse" are placed into cache as priority
candidates for eviction. Through extensive experimentation, we demonstrate that
HR-Cache not only consistently enhances byte hit rates compared to existing
state-of-the-art methods but also achieves this with minimal prediction
overhead.
<br />Our experimental results, using three real-world traces and one synthetic
trace, indicate that HR-Cache consistently achieves 2.2-14.6% greater WAN
traffic savings than LRU. It outperforms not only heuristic caching strategies
but also the state-of-the-art learning-based algorithm.
</p>
</div>
</dd>
<dt><a name="item664">[664]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02797" title="Abstract">arXiv:2402.02797</a> [<a href="/pdf/2402.02797" title="Download PDF">pdf</a>, <a href="/format/2402.02797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Attention-Guided Feature Fusion Network for Saliency Detection of  Surface Defects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xiaoheng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+F">Feng Yan</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yang Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Ke Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Shuai Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianzhu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+Y">Yanwei Pang</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+J">Jianwei Niu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mingliang Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Surface defect inspection plays an important role in the process of
industrial manufacture and production. Though Convolutional Neural Network
(CNN) based defect inspection methods have made huge leaps, they still confront
a lot of challenges such as defect scale variation, complex background, low
contrast, and so on. To address these issues, we propose a joint
attention-guided feature fusion network (JAFFNet) for saliency detection of
surface defects based on the encoder-decoder network. JAFFNet mainly
incorporates a joint attention-guided feature fusion (JAFF) module into
decoding stages to adaptively fuse low-level and high-level features. The JAFF
module learns to emphasize defect features and suppress background noise during
feature fusion, which is beneficial for detecting low-contrast defects. In
addition, JAFFNet introduces a dense receptive field (DRF) module following the
encoder to capture features with rich context information, which helps detect
defects of different scales. The JAFF module mainly utilizes a learned joint
channel-spatial attention map provided by high-level semantic features to guide
feature fusion. The attention map makes the model pay more attention to defect
features. The DRF module utilizes a sequence of multi-receptive-field (MRF)
units with each taking as inputs all the preceding MRF feature maps and the
original input. The obtained DRF features capture rich context information with
a large range of receptive fields. Extensive experiments conducted on
SD-saliency-900, Magnetic tile, and DAGM 2007 indicate that our method achieves
promising performance in comparison with other state-of-the-art methods.
Meanwhile, our method reaches a real-time defect detection speed of 66 FPS.
</p>
</div>
</dd>
<dt><a name="item665">[665]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02798" title="Abstract">arXiv:2402.02798</a> [<a href="/pdf/2402.02798" title="Download PDF">pdf</a>, <a href="/format/2402.02798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Numerical Approach to Coil Placement in Cerebral  Aneurysms: Mathematical Modeling and In Silico Occlusion Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Holzberger%2C+F">Fabian Holzberger</a>, 
<a href="/search/cs?searchtype=author&query=Muhr%2C+M">Markus Muhr</a>, 
<a href="/search/cs?searchtype=author&query=Wohlmuth%2C+B">Barbara Wohlmuth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Endovascular coil embolization is one of the primary treatment techniques for
cerebral aneurysms. Although it is a well established and minimally invasive
method, it bears the risk of sub-optimal coil placement which can lead to
incomplete occlusion of the aneurysm possibly causing recurrence. One of the
key features of coils is that they have an imprinted natural shape supporting
the fixation within the aneurysm. For the spatial discretization our
mathematical coil model is based on the Discrete Elastic Rod model which
results in a dimension-reduced 1D system of differential equations. We include
bending and twisting responses to account for the coils natural curvature.
Collisions between coil segments and the aneurysm-wall are handled by an
efficient contact algorithm that relies on an octree based collision detection.
The numerical solution of the model is obtained by a symplectic semi-implicit
Euler time stepping method. Our model can be easily incorporated into blood
flow simulations of embolized aneurysms. In order to differentiate optimal from
sub-optimal placements, we employ a suitable in silico Raymond-Roy type
occlusion classification and measure the local packing density in the aneurysm
at its neck, wall-region and core. We investigate the impact of uncertainties
in the coil parameters and embolization procedure. To this end, we vary the
position and the angle of insertion of the microcatheter, and approximate the
local packing density distributions by evaluating sample statistics.
</p>
</div>
</dd>
<dt><a name="item666">[666]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02800" title="Abstract">arXiv:2402.02800</a> [<a href="/pdf/2402.02800" title="Download PDF">pdf</a>, <a href="/format/2402.02800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extreme Two-View Geometry From Object Poses with Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yujing Sun</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Caiyi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yuexin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yiu%2C+S+M">Siu Ming Yiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Human has an incredible ability to effortlessly perceive the viewpoint
difference between two images containing the same object, even when the
viewpoint change is astonishingly vast with no co-visible regions in the
images. This remarkable skill, however, has proven to be a challenge for
existing camera pose estimation methods, which often fail when faced with large
viewpoint differences due to the lack of overlapping local features for
matching. In this paper, we aim to effectively harness the power of object
priors to accurately determine two-view geometry in the face of extreme
viewpoint changes. In our method, we first mathematically transform the
relative camera pose estimation problem to an object pose estimation problem.
Then, to estimate the object pose, we utilize the object priors learned from a
diffusion model Zero123 to synthesize novel-view images of the object. The
novel-view images are matched to determine the object pose and thus the
two-view camera pose. In experiments, our method has demonstrated extraordinary
robustness and resilience to large viewpoint changes, consistently estimating
two-view poses with exceptional generalization ability across both synthetic
and real-world datasets. Code will be available at
https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models.
</p>
</div>
</dd>
<dt><a name="item667">[667]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02801" title="Abstract">arXiv:2402.02801</a> [<a href="/pdf/2402.02801" title="Download PDF">pdf</a>, <a href="/format/2402.02801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KS-Lottery: Finding Certified Lottery Tickets for Multilingual Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+F">Fei Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Shuai Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Q">Qiushi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The lottery ticket hypothesis posits the existence of ``winning tickets''
within a randomly initialized neural network. Do winning tickets exist for LLMs
in fine-tuning scenarios? How can we find such winning tickets? In this paper,
we propose KS-Lottery, a method to identify a small subset of LLM parameters
highly effective in multilingual fine-tuning. Our key idea is to use
Kolmogorov-Smirnov Test to analyze the distribution shift of parameters before
and after fine-tuning. We further theoretically prove that KS-Lottery can find
the certified winning tickets in the embedding layer, fine-tuning on the found
parameters is guaranteed to perform as well as full fine-tuning. Comparing
KS-Lottery with other parameter-efficient tuning algorithms on translation
tasks, the experimental results show that KS-Lottery finds a much smaller set
of parameters for fine-tuning while achieving the comparable performance as
full fine-tuning LLM. Surprisingly, we find that fine-tuning 18 tokens'
embedding of LLaMA suffices to reach the fine-tuning translation performance.
Code and model will be released to the public.
</p>
</div>
</dd>
<dt><a name="item668">[668]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02803" title="Abstract">arXiv:2402.02803</a> [<a href="/pdf/2402.02803" title="Download PDF">pdf</a>, <a href="/format/2402.02803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model Distilling Medication Recommendation Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qidong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuanshao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zijian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+F">Feng Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yefeng Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The recommendation of medication is a vital aspect of intelligent healthcare
systems, as it involves prescribing the most suitable drugs based on a
patient's specific health needs. Unfortunately, many sophisticated models
currently in use tend to overlook the nuanced semantics of medical data, while
only relying heavily on identities. Furthermore, these models face significant
challenges in handling cases involving patients who are visiting the hospital
for the first time, as they lack prior prescription histories to draw upon. To
tackle these issues, we harness the powerful semantic comprehension and
input-agnostic characteristics of Large Language Models (LLMs). Our research
aims to transform existing medication recommendation methodologies using LLMs.
In this paper, we introduce a novel approach called Large Language Model
Distilling Medication Recommendation (LEADER). We begin by creating appropriate
prompt templates that enable LLMs to suggest medications effectively. However,
the straightforward integration of LLMs into recommender systems leads to an
out-of-corpus issue specific to drugs. We handle it by adapting the LLMs with a
novel output layer and a refined tuning loss function. Although LLM-based
models exhibit remarkable capabilities, they are plagued by high computational
costs during inference, which is impractical for the healthcare sector. To
mitigate this, we have developed a feature-level knowledge distillation
technique, which transfers the LLM's proficiency to a more compact model.
Extensive experiments conducted on two real-world datasets, MIMIC-III and
MIMIC-IV, demonstrate that our proposed model not only delivers effective
results but also is efficient. To ease the reproducibility of our experiments,
we release the implementation code online.
</p>
</div>
</dd>
<dt><a name="item669">[669]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02805" title="Abstract">arXiv:2402.02805</a> [<a href="/pdf/2402.02805" title="Download PDF">pdf</a>, <a href="/format/2402.02805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-enhanced Large Language Models in Asynchronous Plan Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+F">Fangru Lin</a>, 
<a href="/search/cs?searchtype=author&query=La+Malfa%2C+E">Emanuele La Malfa</a>, 
<a href="/search/cs?searchtype=author&query=Hofmann%2C+V">Valentin Hofmann</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+E+M">Elle Michelle Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cohn%2C+A">Anthony Cohn</a>, 
<a href="/search/cs?searchtype=author&query=Pierrehumbert%2C+J+B">Janet B. Pierrehumbert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Reasoning about asynchronous plans is challenging since it requires
sequential and parallel planning to optimize time costs. Can large language
models (LLMs) succeed at this task? Here, we present the first large-scale
study investigating this question. We find that a representative set of closed
and open-source LLMs, including GPT-4 and LLaMA-2, behave poorly when not
supplied with illustrations about the task-solving process in our benchmark
AsyncHow. We propose a novel technique called Plan Like a Graph (PLaG) that
combines graphs with natural language prompts and achieves state-of-the-art
results. We show that although PLaG can boost model performance, LLMs still
suffer from drastic degradation when task complexity increases, highlighting
the limits of utilizing LLMs for simulating digital devices. We see our study
as an exciting step towards using LLMs as efficient autonomous agents.
</p>
</div>
</dd>
<dt><a name="item670">[670]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02806" title="Abstract">arXiv:2402.02806</a> [<a href="/pdf/2402.02806" title="Download PDF">pdf</a>, <a href="/format/2402.02806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty Quantification of Phase Transition Problems with an  Injection Boundary
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+Z">Zhenyi Zhang</a>, 
<a href="/search/math?searchtype=author&query=Ma%2C+S">Shengbo Ma</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+Z">Zhennan Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We develop an enthalpy-based modeling and computational framework to quantify
uncertainty in Stefan problems with an injection boundary. Inspired by airfoil
icing studies, we consider a system featuring an injection boundary inducing
domain changes and a free boundary separating phases, resulting in two types of
moving boundaries. Our proposed enthalpy-based formulation seamlessly
integrates thermal diffusion across the domain with energy fluxes at the
boundaries, addressing a modified injection condition for boundary movement.
Uncertainty then stems from random variations in the injection boundary. The
primary focus of our Uncertainty Quantification (UQ) centers on investigating
the effects of uncertainty on free boundary propagation. Through mapping to a
reference domain, we derive an enthalpy-based numerical scheme tailored to the
transformed coordinate system, facilitating a simple and efficient simulation.
Numerical and UQ studies in one and two dimensions validate the proposed model
and the extended enthalpy method. They offer intriguing insights into ice
accretion and other multiphysics processes involving phase transitions.
</p>
</div>
</dd>
<dt><a name="item671">[671]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02807" title="Abstract">arXiv:2402.02807</a> [<a href="/pdf/2402.02807" title="Download PDF">pdf</a>, <a href="/format/2402.02807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Sounds Sound for Phylogenetic Reconstruction?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=H%C3%A4user%2C+L">Luise H&#xe4;user</a>, 
<a href="/search/cs?searchtype=author&query=J%C3%A4ger%2C+G">Gerhard J&#xe4;ger</a>, 
<a href="/search/cs?searchtype=author&query=Rama%2C+T">Taraka Rama</a>, 
<a href="/search/cs?searchtype=author&query=List%2C+J">Johann-Mattis List</a>, 
<a href="/search/cs?searchtype=author&query=Stamatakis%2C+A">Alexandros Stamatakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted for SIGTYP (2024): H\"auser, Luise; J\"ager, Gerhard; List, Johann-Mattis; Rama, Taraka; and Stamatakis, Alexandros (2024): Are sounds sound for phylogenetic reconstruction? In: Proceedings of the 6th Workshop on Research in Computational Linguistic Typology and Multilingual NLP (SIGTYP 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In traditional studies on language evolution, scholars often emphasize the
importance of sound laws and sound correspondences for phylogenetic inference
of language family trees. However, to date, computational approaches have
typically not taken this potential into account. Most computational studies
still rely on lexical cognates as major data source for phylogenetic
reconstruction in linguistics, although there do exist a few studies in which
authors praise the benefits of comparing words at the level of sound sequences.
Building on (a) ten diverse datasets from different language families, and (b)
state-of-the-art methods for automated cognate and sound correspondence
detection, we test, for the first time, the performance of sound-based versus
cognate-based approaches to phylogenetic reconstruction. Our results show that
phylogenies reconstructed from lexical cognates are topologically closer, by
approximately one third with respect to the generalized quartet distance on
average, to the gold standard phylogenies than phylogenies reconstructed from
sound correspondences.
</p>
</div>
</dd>
<dt><a name="item672">[672]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02808" title="Abstract">arXiv:2402.02808</a> [<a href="/pdf/2402.02808" title="Download PDF">pdf</a>, <a href="/ps/2402.02808" title="Download PostScript">ps</a>, <a href="/format/2402.02808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Hybrid Finite-Difference-Particle Method for Chemotaxis Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chertock%2C+A">Alina Chertock</a>, 
<a href="/search/math?searchtype=author&query=Cui%2C+S">Shumo Cui</a>, 
<a href="/search/math?searchtype=author&query=Kurganov%2C+A">Alexander Kurganov</a>, 
<a href="/search/math?searchtype=author&query=Wang%2C+C">Chenxi Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Chemotaxis systems play a crucial role in modeling the dynamics of bacterial
and cellular behaviors, including propagation, aggregation, and pattern
formation, all under the influence of chemical signals. One notable
characteristic of these systems is their ability to simulate concentration
phenomena, where cell density undergoes rapid growth near specific
concentration points or along certain curves. Such growth can result in
singular, spiky structures and lead to finite-time blowups.
<br />Our investigation focuses on the dynamics of the Patlak-Keller-Segel
chemotaxis system and its two-species extensions. In the latter case, different
species may exhibit distinct chemotactic sensitivities, giving rise to very
different rates of cell density growth. Such a situation may be extremely
challenging for numerical methods as they may fail to accurately capture the
blowup of the slower-growing species mainly due to excessive numerical
dissipation.
<br />In this paper, we propose a hybrid finite-difference-particle (FDP) method,
in which a particle method is used to solve the chemotaxis equation(s), while
finite difference schemes are employed to solve the chemoattractant equation.
Thanks to the low-dissipation nature of the particle method, the proposed
hybrid scheme is particularly adept at capturing the blowup behaviors in both
one- and two-species cases. The proposed hybrid FDP methods are tested on a
series of challenging examples, and the obtained numerical results demonstrate
that our hybrid method can provide sharp resolution of the singular structures
even with a relatively small number of particles. Moreover, in the two-species
case, our method adeptly captures the blowing-up solution for the component
with lower chemotactic sensitivity, a feature not observed in other works.
</p>
</div>
</dd>
<dt><a name="item673">[673]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02811" title="Abstract">arXiv:2402.02811</a> [<a href="/pdf/2402.02811" title="Download PDF">pdf</a>, <a href="/format/2402.02811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-scale fMRI time series analysis for understanding  neurodegeneration in MCI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=R.%2C+A">Ammu R.</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+D">Debanjali Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Acharya%2C+A">Ameiy Acharya</a>, 
<a href="/search/cs?searchtype=author&query=Aithal%2C+N">Ninad Aithal</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+N">Neelam Sinha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 3 figures and 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this study, we present a technique that spans multi-scale views (global
scale -- meaning brain network-level and local scale -- examining each
individual ROI that constitutes the network) applied to resting-state fMRI
volumes. Deep learning based classification is utilized in understanding
neurodegeneration. The novelty of the proposed approach lies in utilizing two
extreme scales of analysis. One branch considers the entire network within
graph-analysis framework. Concurrently, the second branch scrutinizes each ROI
within a network independently, focusing on evolution of dynamics. For each
subject, graph-based approach employs partial correlation to profile the
subject in a single graph where each ROI is a node, providing insights into
differences in levels of participation. In contrast, non-linear analysis
employs recurrence plots to profile a subject as a multichannel 2D image,
revealing distinctions in underlying dynamics. The proposed approach is
employed for classification of a cohort of 50 healthy control (HC) and 50 Mild
Cognitive Impairment (MCI), sourced from ADNI dataset. Results point to: (1)
reduced activity in ROIs such as PCC in MCI (2) greater activity in occipital
in MCI, which is not seen in HC (3) when analysed for dynamics, all ROIs in MCI
show greater predictability in time-series.
</p>
</div>
</dd>
<dt><a name="item674">[674]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02812" title="Abstract">arXiv:2402.02812</a> [<a href="/pdf/2402.02812" title="Download PDF">pdf</a>, <a href="/format/2402.02812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> State estimation of urban air pollution with statistical, physical, and  super-learning graph models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dolbeault%2C+M">Matthieu Dolbeault</a>, 
<a href="/search/cs?searchtype=author&query=Mula%2C+O">Olga Mula</a>, 
<a href="/search/cs?searchtype=author&query=Somacal%2C+A">Agust&#xed;n Somacal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">We consider the problem of real-time reconstruction of urban air pollution
maps. The task is challenging due to the heterogeneous sources of available
data, the scarcity of direct measurements, the presence of noise, and the large
surfaces that need to be considered. In this work, we introduce different
reconstruction methods based on posing the problem on city graphs. Our
strategies can be classified as fully data-driven, physics-driven, or hybrid,
and we combine them with super-learning models. The performance of the methods
is tested in the case of the inner city of Paris, France.
</p>
</div>
</dd>
<dt><a name="item675">[675]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02816" title="Abstract">arXiv:2402.02816</a> [<a href="/pdf/2402.02816" title="Download PDF">pdf</a>, <a href="/format/2402.02816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intersectional Two-sided Fairness in Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+P">Peijie Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Weizhi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+P">Peng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shaoping Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Fairness of recommender systems (RS) has attracted increasing attention
recently. Based on the involved stakeholders, the fairness of RS can be divided
into user fairness, item fairness, and two-sided fairness which considers both
user and item fairness simultaneously. However, we argue that the
intersectional two-sided unfairness may still exist even if the RS is two-sided
fair, which is observed and shown by empirical studies on real-world data in
this paper, and has not been well-studied previously. To mitigate this problem,
we propose a novel approach called Intersectional Two-sided Fairness
Recommendation (ITFR). Our method utilizes a sharpness-aware loss to perceive
disadvantaged groups, and then uses collaborative loss balance to develop
consistent distinguishing abilities for different intersectional groups.
Additionally, predicted score normalization is leveraged to align positive
predicted scores to fairly treat positives in different intersectional groups.
Extensive experiments and analyses on three public datasets show that our
proposed approach effectively alleviates the intersectional two-sided
unfairness and consistently outperforms previous state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item676">[676]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02820" title="Abstract">arXiv:2402.02820</a> [<a href="/pdf/2402.02820" title="Download PDF">pdf</a>, <a href="/format/2402.02820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting VAE for Unsupervised Time Series Anomaly Detection: A  Frequency Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zexin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+C">Changhua Pei</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+M">Minghua Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhihan Li</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+D">Dan Pei</a>, 
<a href="/search/cs?searchtype=author&query=Rajmohan%2C+S">Saravan Rajmohan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dongmei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Q">Qingwei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haiming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianhui Li</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+G">Gaogang Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WWW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Time series Anomaly Detection (AD) plays a crucial role for web systems.
Various web systems rely on time series data to monitor and identify anomalies
in real time, as well as to initiate diagnosis and remediation procedures.
Variational Autoencoders (VAEs) have gained popularity in recent decades due to
their superior de-noising capabilities, which are useful for anomaly detection.
However, our study reveals that VAE-based methods face challenges in capturing
long-periodic heterogeneous patterns and detailed short-periodic trends
simultaneously. To address these challenges, we propose Frequency-enhanced
Conditional Variational Autoencoder (FCVAE), a novel unsupervised AD method for
univariate time series. To ensure an accurate AD, FCVAE exploits an innovative
approach to concurrently integrate both the global and local frequency features
into the condition of Conditional Variational Autoencoder (CVAE) to
significantly increase the accuracy of reconstructing the normal data. Together
with a carefully designed "target attention" mechanism, our approach allows the
model to pick the most useful information from the frequency domain for better
short-periodic trend construction. Our FCVAE has been evaluated on public
datasets and a large-scale cloud system, and the results demonstrate that it
outperforms state-of-the-art methods. This confirms the practical applicability
of our approach in addressing the limitations of current VAE-based anomaly
detection models.
</p>
</div>
</dd>
<dt><a name="item677">[677]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02823" title="Abstract">arXiv:2402.02823</a> [<a href="/pdf/2402.02823" title="Download PDF">pdf</a>, <a href="/format/2402.02823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evading Data Contamination Detection for Language Models is (too) Easy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dekoninck%2C+J">Jasper Dekoninck</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+M+N">Mark Niklas M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Baader%2C+M">Maximilian Baader</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+M">Marc Fischer</a>, 
<a href="/search/cs?searchtype=author&query=Vechev%2C+M">Martin Vechev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models are widespread, with their performance on benchmarks
frequently guiding user preferences for one model over another. However, the
vast amount of data these models are trained on can inadvertently lead to
contamination with public benchmarks, thus compromising performance
measurements. While recently developed contamination detection methods try to
address this issue, they overlook the possibility of deliberate contamination
by malicious model providers aiming to evade detection. We argue that this
setting is of crucial importance as it casts doubt on the reliability of public
benchmarks. To more rigorously study this issue, we propose a categorization of
both model providers and contamination detection methods. This reveals
vulnerabilities in existing methods that we exploit with EAL, a simple yet
effective contamination technique that significantly inflates benchmark
performance while completely evading current detection methods.
</p>
</div>
</dd>
<dt><a name="item678">[678]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02824" title="Abstract">arXiv:2402.02824</a> [<a href="/pdf/2402.02824" title="Download PDF">pdf</a>, <a href="/ps/2402.02824" title="Download PostScript">ps</a>, <a href="/format/2402.02824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FAIR-USE4OS: From open source to Open Source
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sonabend%2C+R">Raphael Sonabend</a>, 
<a href="/search/cs?searchtype=author&query=Gruson%2C+H">Hugo Gruson</a>, 
<a href="/search/cs?searchtype=author&query=Wolansky%2C+L">Leo Wolansky</a>, 
<a href="/search/cs?searchtype=author&query=Kiragga%2C+A">Agnes Kiragga</a>, 
<a href="/search/cs?searchtype=author&query=Katz%2C+D+S">Daniel S. Katz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">This paper extends the FAIR (Findable, Accessible, Interoperable, Reusable)
guidelines to provide criteria for assessing if software is Open Source. By
adding 'USE' (User-Centered, Sustainable, Equitable), software development can
adhere to open source best practice by incorporating user-input early on,
ensuring front-end designs are accessible to all possible stakeholders, and
planning long-term sustainability alongside software design. The FAIR-USE4OS
guidelines will allow funders and researchers to more effectively evaluate and
plan Open Source software projects. There is good evidence of funders
increasingly mandating that all funded research software is open-source;
however, even under the FAIR guidelines, this could simply mean software
released on GitHub with a Zenodo DOI. By employing the FAIR-USE4OS guidelines,
best practice can be demonstrated from the very beginning of the design process
and the software has the greatest chance of success by being truly 'Open
Source'.
</p>
</div>
</dd>
<dt><a name="item679">[679]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02826" title="Abstract">arXiv:2402.02826</a> [<a href="/pdf/2402.02826" title="Download PDF">pdf</a>, <a href="/ps/2402.02826" title="Download PostScript">ps</a>, <a href="/format/2402.02826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SynthVision -- Harnessing Minimal Input for Maximal Output in Computer  Vision Models using Synthetic Image data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kularathne%2C+Y">Yudara Kularathne</a>, 
<a href="/search/cs?searchtype=author&query=Janitha%2C+P">Prathapa Janitha</a>, 
<a href="/search/cs?searchtype=author&query=Ambepitiya%2C+S">Sithira Ambepitiya</a>, 
<a href="/search/cs?searchtype=author&query=Ahamed%2C+T">Thanveer Ahamed</a>, 
<a href="/search/cs?searchtype=author&query=Wijesundara%2C+D">Dinuka Wijesundara</a>, 
<a href="/search/cs?searchtype=author&query=Sothyrajah%2C+P">Prarththanan Sothyrajah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages 5 figures 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Rapid development of disease detection computer vision models is vital in
response to urgent medical crises like epidemics or events of bioterrorism.
However, traditional data gathering methods are too slow for these scenarios
necessitating innovative approaches to generate reliable models quickly from
minimal data. We demonstrate our new approach by building a comprehensive
computer vision model for detecting Human Papilloma Virus Genital warts using
only synthetic data. In our study, we employed a two phase experimental design
using diffusion models. In the first phase diffusion models were utilized to
generate a large number of diverse synthetic images from 10 HPV guide images
explicitly focusing on accurately depicting genital warts. The second phase
involved the training and testing vision model using this synthetic dataset.
This method aimed to assess the effectiveness of diffusion models in rapidly
generating high quality training data and the subsequent impact on the vision
model performance in medical image recognition. The study findings revealed
significant insights into the performance of the vision model trained on
synthetic images generated through diffusion models. The vision model showed
exceptional performance in accurately identifying cases of genital warts. It
achieved an accuracy rate of 96% underscoring its effectiveness in medical
image classification. For HPV cases the model demonstrated a high precision of
99% and a recall of 94%. In normal cases the precision was 95% with an
impressive recall of 99%. These metrics indicate the model capability to
correctly identify true positive cases and minimize false positives. The model
achieved an F1 Score of 96% for HPV cases and 97% for normal cases. The high F1
Score across both categories highlights the balanced nature of the model
precision and recall ensuring reliability and robustness in its predictions.
</p>
</div>
</dd>
<dt><a name="item680">[680]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02827" title="Abstract">arXiv:2402.02827</a> [<a href="/pdf/2402.02827" title="Download PDF">pdf</a>, <a href="/format/2402.02827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PowerGraph: A power grid benchmark dataset for graph neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Varbella%2C+A">Anna Varbella</a>, 
<a href="/search/cs?searchtype=author&query=Amara%2C+K">Kenza Amara</a>, 
<a href="/search/cs?searchtype=author&query=Gjorgiev%2C+B">Blazhe Gjorgiev</a>, 
<a href="/search/cs?searchtype=author&query=Sansavini%2C+G">Giovanni Sansavini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 4 figures, conference paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Public Graph Neural Networks (GNN) benchmark datasets facilitate the use of
GNN and enhance GNN applicability to diverse disciplines. The community
currently lacks public datasets of electrical power grids for GNN applications.
Indeed, GNNs can potentially capture complex power grid phenomena over
alternative machine learning techniques. Power grids are complex engineered
networks that are naturally amenable to graph representations. Therefore, GNN
have the potential for capturing the behavior of power grids over alternative
machine learning techniques. To this aim, we develop a graph dataset for
cascading failure events, which are the major cause of blackouts in electric
power grids. Historical blackout datasets are scarce and incomplete. The
assessment of vulnerability and the identification of critical components are
usually conducted via computationally expensive offline simulations of
cascading failures. Instead, we propose using machine learning models for the
online detection of cascading failures leveraging the knowledge of the system
state at the onset of the cascade. We develop PowerGraph, a graph dataset
modeling cascading failures in power grids, designed for two purposes, namely,
i) training GNN models for different graph-level tasks including multi-class
classification, binary classification, and regression, and ii) explaining GNN
models. The dataset generated via a physics-based cascading failure model
ensures the generality of the operating and environmental conditions by
spanning diverse failure scenarios. In addition, we foster the use of the
dataset to benchmark GNN explainability methods by assigning ground-truth
edge-level explanations. PowerGraph helps the development of better GNN models
for graph-level tasks and explainability, critical in many domains ranging from
chemistry to biology, where the systems and processes can be described as
graphs.
</p>
</div>
</dd>
<dt><a name="item681">[681]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02829" title="Abstract">arXiv:2402.02829</a> [<a href="/pdf/2402.02829" title="Download PDF">pdf</a>, <a href="/ps/2402.02829" title="Download PostScript">ps</a>, <a href="/format/2402.02829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Completeness of Interpolation Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hetzl%2C+S">Stefan Hetzl</a>, 
<a href="/search/cs?searchtype=author&query=Jalali%2C+R">Raheleh Jalali</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
<p class="mathjax">Craig interpolation is a fundamental property of classical and non-classic
logics with a plethora of applications from philosophical logic to
computer-aided verification. The question of which interpolants can be obtained
from an interpolation algorithm is of profound importance. Motivated by this
question, we initiate the study of completeness properties of interpolation
algorithms. An interpolation algorithm $\mathcal{I}$ is \emph{complete} if, for
every semantically possible interpolant $C$ of an implication $A \to B$, there
is a proof $P$ of $A \to B$ such that $C$ is logically equivalent to
$\mathcal{I}(P)$. We establish incompleteness and different kinds of
completeness results for several standard algorithms for resolution and the
sequent calculus for propositional, modal, and first-order logic.
</p>
</div>
</dd>
<dt><a name="item682">[682]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02831" title="Abstract">arXiv:2402.02831</a> [<a href="/pdf/2402.02831" title="Download PDF">pdf</a>, <a href="/ps/2402.02831" title="Download PostScript">ps</a>, <a href="/format/2402.02831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What do we teach to engineering students: embedded ethics, morality, and  politics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferdman%2C+A">Avigail Ferdman</a>, 
<a href="/search/cs?searchtype=author&query=Ratti%2C+E">Emanuele Ratti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">In the past few years, calls for integrating ethics modules in engineering
curricula have multiplied. Despite this positive trend, a number of issues with
these embedded programs remains. First, learning goals are underspecified. A
second limitation is the conflation of different dimensions under the same
banner, in particular confusion between ethics curricula geared towards
addressing the ethics of individual conduct and curricula geared towards
addressing ethics at the societal level. In this article, we propose a
tripartite framework to overcome these difficulties. Our framework analytically
decomposes an ethics module into three dimensions. First, there is the ethical
dimension, which pertains to the learning goals. Second, there is the moral
dimension, which addresses the moral relevance of engineers conduct. Finally,
there is the political dimension, which scales up issues of moral relevance at
the civic level. All in all, our framework has two advantages. First, it
provides analytic clarity, i.e. it enables course instructors to locate ethical
dilemmas in either the moral or political realm and to make use of the tools
and resources from moral and political philosophy. Second, it depicts a
comprehensive ethical training, which enables students to both reason about
moral issues in the abstract, and to socially contextualize potential
solutions.
</p>
</div>
</dd>
<dt><a name="item683">[683]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02833" title="Abstract">arXiv:2402.02833</a> [<a href="/pdf/2402.02833" title="Download PDF">pdf</a>, <a href="/format/2402.02833" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Behavior Tree Capabilities for Dynamic Multi-Robot Task Allocation with  Heterogeneous Robot Teams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heppner%2C+G">Georg Heppner</a>, 
<a href="/search/cs?searchtype=author&query=Oberacker%2C+D">David Oberacker</a>, 
<a href="/search/cs?searchtype=author&query=Roennau%2C+A">Arne Roennau</a>, 
<a href="/search/cs?searchtype=author&query=Dillmann%2C+R">R&#xfc;diger Dillmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in the Proceedings of the IEEE International Conference on Robotics and Automation 2024 (IEEE ICRA 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">While individual robots are becoming increasingly capable, with new sensors
and actuators, the complexity of expected missions increased exponentially in
comparison. To cope with this complexity, heterogeneous teams of robots have
become a significant research interest in recent years. Making effective use of
the robots and their unique skills in a team is challenging. Dynamic runtime
conditions often make static task allocations infeasible, therefore requiring a
dynamic, capability-aware allocation of tasks to team members. To this end, we
propose and implement a system that allows a user to specify missions using
Bheavior Trees (BTs), which can then, at runtime, be dynamically allocated to
the current robot team. The system allows to statically model an individual
robot's capabilities within our ros_bt_py BT framework. It offers a runtime
auction system to dynamically allocate tasks to the most capable robot in the
current team. The system leverages utility values and pre-conditions to ensure
that the allocation improves the overall mission execution quality while
preventing faulty assignments. To evaluate the system, we simulated a
find-and-decontaminate mission with a team of three heterogeneous robots and
analyzed the utilization and overall mission times as metrics. Our results show
that our system can improve the overall effectiveness of a team while allowing
for intuitive mission specification and flexibility in the team composition.
</p>
</div>
</dd>
<dt><a name="item684">[684]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02834" title="Abstract">arXiv:2402.02834</a> [<a href="/pdf/2402.02834" title="Download PDF">pdf</a>, <a href="/format/2402.02834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shortened LLaMA: A Simple Depth Pruning for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+B">Bo-Kyeong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+G">Geonmin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Tae-Ho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Castells%2C+T">Thibault Castells</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Shinkook Choi</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Junho Shin</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+H">Hyoung-Kyu Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Structured pruning of modern large language models (LLMs) has emerged as a
way of decreasing their high computational needs. Width pruning reduces the
size of projection weight matrices (e.g., by removing attention heads) while
maintaining the number of layers. Depth pruning, in contrast, removes entire
layers or blocks, while keeping the size of the remaining weights unchanged.
Most current research focuses on either width-only or a blend of width and
depth pruning, with little comparative analysis between the two units (width
vs. depth) concerning their impact on LLM inference efficiency. In this work,
we show that a simple depth pruning approach can compete with recent width
pruning methods in terms of zero-shot task performance. Our pruning method
boosts inference speeds, especially under memory-constrained conditions that
require limited batch sizes for running LLMs, where width pruning is
ineffective. We hope this work can help deploy LLMs on local and edge devices.
</p>
</div>
</dd>
<dt><a name="item685">[685]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02837" title="Abstract">arXiv:2402.02837</a> [<a href="/pdf/2402.02837" title="Download PDF">pdf</a>, <a href="/format/2402.02837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> With a Little Help from my (Linguistic) Friends: Topic Segmentation of  Multi-party Casual Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Decker%2C+A">Amandine Decker</a> (LORIA, UL, CNRS, SEMAGRAMME, GU), 
<a href="/search/cs?searchtype=author&query=Amblard%2C+M">Maxime Amblard</a> (SEMAGRAMME, LORIA)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CODI 2024 - 5th workshop on Computational Approaches to Discourse,
  Mar 2024, Malta, Malta
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Topics play an important role in the global organisation of a conversation as
what is currently discussed constrains the possible contributions of the
participant. Understanding the way topics are organised in interaction would
provide insight on the structure of dialogue beyond the sequence of utterances.
However, studying this high-level structure is a complex task that we try to
approach by first segmenting dialogues into smaller topically coherent sets of
utterances. Understanding the interactions between these segments would then
enable us to propose a model of topic organisation at a dialogue level. In this
paper we work with open-domain conversations and try to reach a comparable
level of accuracy as recent machine learning based topic segmentation models
but with a formal approach. The features we identify as meaningful for this
task help us understand better the topical structure of a conversation.
</p>
</div>
</dd>
<dt><a name="item686">[686]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02838" title="Abstract">arXiv:2402.02838</a> [<a href="/pdf/2402.02838" title="Download PDF">pdf</a>, <a href="/ps/2402.02838" title="Download PostScript">ps</a>, <a href="/format/2402.02838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HAPI-FHIR Server Implementation to Enhancing Interoperability among  Primary Care Health Information Systems in Sri Lanka: Review of the Technical  Use Case
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jayathissa%2C+P">Prabath Jayathissa</a>, 
<a href="/search/cs?searchtype=author&query=Hewapathirana%2C+R">Roshan Hewapathirana</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> HAPI-FHIR Server, Interoperability, Primary Care Health Information Systems, Health Information Systems
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> European Modern Studies Journal 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">This review underscores the vital role of interoperability in digital health,
advocating for a standardized framework. It focuses on implementing a Fast
Healthcare Interoperability Resources (FHIR) server, addressing technical,
semantic, and process challenges. FHIR's adaptability ensures uniformity within
Primary Care Health Information Systems, fostering interoperability. Patient
data management complexities highlight the pivotal role of semantic
interoperability in seamless patient care. FHIR standards enhance these
efforts, offering multiple pathways for data search. The ADR-guided FHIR server
implementation systematically addresses challenges related to patient identity,
biometrics, and data security. The detailed development phases emphasize
architecture, API integration, and security. The concluding stages incorporate
forward-looking approaches, including HHIMS Synthetic Dataset testing.
Envisioning FHIR integration as transformative, it anticipates a responsive
healthcare environment aligned with the evolving digital health landscape,
ensuring comprehensive, dynamic, and interconnected systems for efficient data
exchange and access.
</p>
</div>
</dd>
<dt><a name="item687">[687]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02842" title="Abstract">arXiv:2402.02842</a> [<a href="/pdf/2402.02842" title="Download PDF">pdf</a>, <a href="/format/2402.02842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trinity: Syncretizing Multi-/Long-tail/Long-term Interests All in One
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jing Yan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Liu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+J">Jianfei Cui</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhichen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Bin%2C+X">Xingyan Bin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Feng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zuotao Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Interest modeling in recommender system has been a constant topic for
improving user experience, and typical interest modeling tasks (e.g.
multi-interest, long-tail interest and long-term interest) have been
investigated in many existing works. However, most of them only consider one
interest in isolation, while neglecting their interrelationships. In this
paper, we argue that these tasks suffer from a common "interest amnesia"
problem, and a solution exists to mitigate it simultaneously. We figure that
long-term cues can be the cornerstone since they reveal multi-interest and
clarify long-tail interest. Inspired by the observation, we propose a novel and
unified framework in the retrieval stage, "Trinity", to solve interest amnesia
problem and improve multiple interest modeling tasks. We construct a real-time
clustering system that enables us to project items into enumerable clusters,
and calculate statistical interest histograms over these clusters. Based on
these histograms, Trinity recognizes underdelivered themes and remains stable
when facing emerging hot topics. Trinity is more appropriate for large-scale
industry scenarios because of its modest computational overheads. Its derived
retrievers have been deployed on the recommender system of Douyin,
significantly improving user experience and retention. We believe that such
practical experience can be well generalized to other scenarios.
</p>
</div>
</dd>
<dt><a name="item688">[688]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02844" title="Abstract">arXiv:2402.02844</a> [<a href="/pdf/2402.02844" title="Download PDF">pdf</a>, <a href="/format/2402.02844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparing Knowledge Sources for Open-Domain Scientific Claim  Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vladika%2C+J">Juraj Vladika</a>, 
<a href="/search/cs?searchtype=author&query=Matthes%2C+F">Florian Matthes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">The increasing rate at which scientific knowledge is discovered and health
claims shared online has highlighted the importance of developing efficient
fact-checking systems for scientific claims. The usual setting for this task in
the literature assumes that the documents containing the evidence for claims
are already provided and annotated or contained in a limited corpus. This
renders the systems unrealistic for real-world settings where knowledge sources
with potentially millions of documents need to be queried to find relevant
evidence. In this paper, we perform an array of experiments to test the
performance of open-domain claim verification systems. We test the final
verdict prediction of systems on four datasets of biomedical and health claims
in different settings. While keeping the pipeline's evidence selection and
verdict prediction parts constant, document retrieval is performed over three
common knowledge sources (PubMed, Wikipedia, Google) and using two different
information retrieval techniques. We show that PubMed works better with
specialized biomedical claims, while Wikipedia is more suited for everyday
health concerns. Likewise, BM25 excels in retrieval precision, while semantic
search in recall of relevant evidence. We discuss the results, outline frequent
retrieval patterns and challenges, and provide promising future directions.
</p>
</div>
</dd>
<dt><a name="item689">[689]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02847" title="Abstract">arXiv:2402.02847</a> [<a href="/pdf/2402.02847" title="Download PDF">pdf</a>, <a href="/ps/2402.02847" title="Download PostScript">ps</a>, <a href="/format/2402.02847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A unified rule format for bounded nondeterminism in SOS with terms as  labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aceto%2C+L">Luca Aceto</a>, 
<a href="/search/cs?searchtype=author&query=F%C3%A1bregas%2C+I">Ignacio F&#xe1;bregas</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-P%C3%A9rez%2C+%C3%81">&#xc1;lvaro Garc&#xed;a-P&#xe9;rez</a>, 
<a href="/search/cs?searchtype=author&query=Ing%C3%B3lfsd%C3%B3ttir%2C+A">Anna Ing&#xf3;lfsd&#xf3;ttir</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> J. Log. Algebraic Methods Program. Vol. 92. 2017
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">We present a unified rule format for structural operational semantics with
terms as labels that guarantees that the associated labelled transition system
has some bounded-nondeterminism property. The properties we consider include
finite branching, initials finiteness and image finiteness.
</p>
</div>
</dd>
<dt><a name="item690">[690]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02849" title="Abstract">arXiv:2402.02849</a> [<a href="/pdf/2402.02849" title="Download PDF">pdf</a>, <a href="/ps/2402.02849" title="Download PostScript">ps</a>, <a href="/format/2402.02849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do we need decay-preserving error estimate for solving parabolic  equations with initial singularity?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+J">Jiwei Zhang</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+Z">Zhimin Zhang</a>, 
<a href="/search/math?searchtype=author&query=Zhao%2C+C">Chengchao Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Solutions exhibiting weak initial singularities arise in various equations,
including diffusion and subdiffusion equations. When employing the well-known
L1 scheme to solve subdiffusion equations with weak singularities, numerical
simulations reveal that this scheme exhibits varying convergence rates for
different choices of model parameters (i.e., domain size, final time $T$, and
reaction coefficient $\kappa$). This elusive phenomenon is not unique to the L1
scheme but is also observed in other numerical methods for reaction-diffusion
equations such as the backward Euler (IE) scheme, Crank-Nicolson (C-N) scheme,
and two-step backward differentiation formula (BDF2) scheme. The existing
literature lacks an explanation for the existence of two different convergence
regimes, which has puzzled us for a long while and motivated us to study this
inconsistency between the standard convergence theory and numerical
experiences. In this paper, we provide a general methodology to systematically
obtain error estimates that incorporate the exponential decaying feature of the
solution. We term this novel error estimate the `decay-preserving error
estimate' and apply it to the aforementioned IE, C-N, and BDF2 schemes. Our
decay-preserving error estimate consists of a low-order term with an
exponential coefficient and a high-order term with an algebraic coefficient,
both of which depend on the model parameters. Our estimates reveal that the
varying convergence rates are caused by a trade-off between these two
components in different model parameter regimes. By considering the model
parameters, we capture different states of the convergence rate that
traditional error estimates fail to explain. This approach retains more
properties of the continuous solution. We validate our analysis with numerical
results.
</p>
</div>
</dd>
<dt><a name="item691">[691]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02851" title="Abstract">arXiv:2402.02851</a> [<a href="/pdf/2402.02851" title="Download PDF">pdf</a>, <a href="/format/2402.02851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Compositional Generalization via Compositional Feature  Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+H">Haozhe Si</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+H">Huajie Shao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Han Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is released at <a href="https://github.com/Haoxiang-Wang/Compositional-Feature-Alignment">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Real-world applications of machine learning models often confront data
distribution shifts, wherein discrepancies exist between the training and test
data distributions. In the common multi-domain multi-class setup, as the number
of classes and domains scales up, it becomes infeasible to gather training data
for every domain-class combination. This challenge naturally leads the quest
for models with Compositional Generalization (CG) ability, where models can
generalize to unseen domain-class combinations. To delve into the CG challenge,
we develop CG-Bench, a suite of CG benchmarks derived from existing real-world
image datasets, and observe that the prevalent pretraining-finetuning paradigm
on foundational models, such as CLIP and DINOv2, struggles with the challenge.
To address this challenge, we propose Compositional Feature Alignment (CFA), a
simple two-stage finetuning technique that i) learns two orthogonal linear
heads on a pretrained encoder with respect to class and domain labels, and ii)
fine-tunes the encoder with the newly learned head frozen. We theoretically and
empirically justify that CFA encourages compositional feature learning of
pretrained models. We further conduct extensive experiments on CG-Bench for
CLIP and DINOv2, two powerful pretrained vision foundation models. Experiment
results show that CFA outperforms common finetuning techniques in compositional
generalization, corroborating CFA's efficacy in compositional feature learning.
</p>
</div>
</dd>
<dt><a name="item692">[692]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02853" title="Abstract">arXiv:2402.02853</a> [<a href="/pdf/2402.02853" title="Download PDF">pdf</a>, <a href="/ps/2402.02853" title="Download PostScript">ps</a>, <a href="/format/2402.02853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Repeated-Root Cyclic Codes with Optimal Parameters or Best Parameters  Known
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+C">Cunsheng Ding</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Cyclic codes are the most studied subclass of linear codes and widely used in
data storage and communication systems. Many cyclic codes have optimal
parameters or the best parameters known. They are divided into simple-root
cyclic codes and repeated-root cyclic codes. Although there are a huge number
of references on cyclic codes, few of them are on repeated-root cyclic codes.
Hence, repeated-root cyclic codes are rarely studied. There are a few families
of distance-optimal repeated-root binary and $p$-ary cyclic codes for odd prime
$p$ in the literature. However, it is open whether there exists an infinite
family of distance-optimal repeated-root cyclic codes over $\bF_q$ for each
even $q \geq 4$.
<br />In this paper, three infinite families of distance-optimal repeated-root
cyclic codes with minimum distance 3 or 4 are constructed; two other infinite
families of repeated-root cyclic codes with minimum distance 3 or 4 are
developed; four infinite families of repeated-root cyclic codes with minimum
distance 6 or 8 are presented; and two infinite families of repeated-root
binary cyclic codes with parameters $[2n, k, d \geq (n-1)/\log_2 n]$, where
$n=2^m-1$ and $k \geq n$, are constructed. In addition, 27 repeated-root cyclic
codes of length up to $254$ over $\bF_q$ for $q \in \{2, 4, 8\}$ with optimal
parameters or best parameters known are obtained in this paper. The results of
this paper show that repeated-root cyclic codes could be very attractive and
are worth of further investigation.
</p>
</div>
</dd>
<dt><a name="item693">[693]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02855" title="Abstract">arXiv:2402.02855</a> [<a href="/pdf/2402.02855" title="Download PDF">pdf</a>, <a href="/format/2402.02855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Sparse Learning: A Novel Paradigm for Efficient Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+Y">Yongduo Sui</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiancan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Hui Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures, 4 tables. Accecpted by WSDM 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In the realm of deep learning-based recommendation systems, the increasing
computational demands, driven by the growing number of users and items, pose a
significant challenge to practical deployment. This challenge is primarily
twofold: reducing the model size while effectively learning user and item
representations for efficient recommendations. Despite considerable
advancements in model compression and architecture search, prevalent approaches
face notable constraints. These include substantial additional computational
costs from pre-training/re-training in model compression and an extensive
search space in architecture design. Additionally, managing complexity and
adhering to memory constraints is problematic, especially in scenarios with
strict time or space limitations. Addressing these issues, this paper
introduces a novel learning paradigm, Dynamic Sparse Learning (DSL), tailored
for recommendation models. DSL innovatively trains a lightweight sparse model
from scratch, periodically evaluating and dynamically adjusting each weight's
significance and the model's sparsity distribution during the training. This
approach ensures a consistent and minimal parameter budget throughout the full
learning lifecycle, paving the way for "end-to-end" efficiency from training to
inference. Our extensive experimental results underline DSL's effectiveness,
significantly reducing training and inference costs while delivering comparable
recommendation performance.
</p>
</div>
</dd>
<dt><a name="item694">[694]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02858" title="Abstract">arXiv:2402.02858</a> [<a href="/pdf/2402.02858" title="Download PDF">pdf</a>, <a href="/format/2402.02858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep autoregressive density nets vs neural ensembles for model-based  offline reinforcement learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Benechehab%2C+A">Abdelhakim Benechehab</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+A">Albert Thomas</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%A9gl%2C+B">Bal&#xe1;zs K&#xe9;gl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We consider the problem of offline reinforcement learning where only a set of
system transitions is made available for policy optimization. Following recent
advances in the field, we consider a model-based reinforcement learning
algorithm that infers the system dynamics from the available data and performs
policy optimization on imaginary model rollouts. This approach is vulnerable to
exploiting model errors which can lead to catastrophic failures on the real
system. The standard solution is to rely on ensembles for uncertainty
heuristics and to avoid exploiting the model where it is too uncertain. We
challenge the popular belief that we must resort to ensembles by showing that
better performance can be obtained with a single well-calibrated autoregressive
model on the D4RL benchmark. We also analyze static metrics of model-learning
and conclude on the important model properties for the final performance of the
agent.
</p>
</div>
</dd>
<dt><a name="item695">[695]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02861" title="Abstract">arXiv:2402.02861</a> [<a href="/pdf/2402.02861" title="Download PDF">pdf</a>, <a href="/format/2402.02861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Noisy Observations in Zero-Sum Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Athanasakos%2C+E+M">Emmanouil M Athanasakos</a> (NEO), 
<a href="/search/cs?searchtype=author&query=Perlaza%2C+S+M">Samir M Perlaza</a> (NEO, ECE, GAATI)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is submitted to the 2024 IEEE International Symposium on Information Theory (ISIT 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Information Theory (cs.IT); Machine Learning (stat.ML)

</div>
<p class="mathjax">This paper studies an instance of zero-sum games in which one player (the
leader) commits to its opponent (the follower) to choose its actions by
sampling a given probability measure (strategy). The actions of the leader are
observed by the follower as the output of an arbitrary channel. In response to
that, the follower chooses its action based on its current information, that
is, the leader's commitment and the corresponding noisy observation of its
action. Within this context, the equilibrium of the game with noisy action
observability is shown to always exist and the necessary conditions for its
uniqueness are identified. Interestingly, the noisy observations have important
impact on the cardinality of the follower's set of best responses. Under
particular conditions, such a set of best responses is proved to be a singleton
almost surely. The proposed model captures any channel noise with a density
with respect to the Lebesgue measure. As an example, the case in which the
channel is described by a Gaussian probability measure is investigated.
</p>
</div>
</dd>
<dt><a name="item696">[696]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02864" title="Abstract">arXiv:2402.02864</a> [<a href="/pdf/2402.02864" title="Download PDF">pdf</a>, <a href="/format/2402.02864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EEVEE: An Easy Annotation Tool for Natural Language Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sorensen%2C+A">Axel Sorensen</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+S">Siyao Peng</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Goot%2C+R">Rob van der Goot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages; accepted to The Linguistic Annotation Workshop (LAW) at EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Annotation tools are the starting point for creating Natural Language
Processing (NLP) datasets. There is a wide variety of tools available; setting
up these tools is however a hindrance. We propose EEVEE, an annotation tool
focused on simplicity, efficiency, and ease of use. It can run directly in the
browser (no setup required) and uses tab-separated files (as opposed to
character offsets or task-specific formats) for annotation. It allows for
annotation of multiple tasks on a single dataset and supports four task-types:
sequence labeling, span labeling, text classification and seq2seq.
</p>
</div>
</dd>
<dt><a name="item697">[697]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02868" title="Abstract">arXiv:2402.02868</a> [<a href="/pdf/2402.02868" title="Download PDF">pdf</a>, <a href="/format/2402.02868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-tuning Reinforcement Learning Models is Secretly a Forgetting  Mitigation Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wo%C5%82czyk%2C+M">Maciej Wo&#x142;czyk</a>, 
<a href="/search/cs?searchtype=author&query=Cupia%C5%82%2C+B">Bart&#x142;omiej Cupia&#x142;</a>, 
<a href="/search/cs?searchtype=author&query=Ostaszewski%2C+M">Mateusz Ostaszewski</a>, 
<a href="/search/cs?searchtype=author&query=Bortkiewicz%2C+M">Micha&#x142; Bortkiewicz</a>, 
<a href="/search/cs?searchtype=author&query=Zaj%C4%85c%2C+M">Micha&#x142; Zaj&#x105;c</a>, 
<a href="/search/cs?searchtype=author&query=Pascanu%2C+R">Razvan Pascanu</a>, 
<a href="/search/cs?searchtype=author&query=Kuci%C5%84ski%2C+%C5%81">&#x141;ukasz Kuci&#x144;ski</a>, 
<a href="/search/cs?searchtype=author&query=Mi%C5%82o%C5%9B%2C+P">Piotr Mi&#x142;o&#x15b;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Fine-tuning is a widespread technique that allows practitioners to transfer
pre-trained capabilities, as recently showcased by the successful applications
of foundation models. However, fine-tuning reinforcement learning (RL) models
remains a challenge. This work conceptualizes one specific cause of poor
transfer, accentuated in the RL setting by the interplay between actions and
observations: forgetting of pre-trained capabilities. Namely, a model
deteriorates on the state subspace of the downstream task not visited in the
initial phase of fine-tuning, on which the model behaved well due to
pre-training. This way, we lose the anticipated transfer benefits. We identify
conditions when this problem occurs, showing that it is common and, in many
cases, catastrophic. Through a detailed empirical analysis of the challenging
NetHack and Montezuma's Revenge environments, we show that standard knowledge
retention techniques mitigate the problem and thus allow us to take full
advantage of the pre-trained capabilities. In particular, in NetHack, we
achieve a new state-of-the-art for neural models, improving the previous best
score from $5$K to over $10$K points in the Human Monk scenario.
</p>
</div>
</dd>
<dt><a name="item698">[698]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02870" title="Abstract">arXiv:2402.02870</a> [<a href="/pdf/2402.02870" title="Download PDF">pdf</a>, <a href="/ps/2402.02870" title="Download PostScript">ps</a>, <a href="/format/2402.02870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistics without Interpretation: A Sober Look at Explainable Machine  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bordt%2C+S">Sebastian Bordt</a>, 
<a href="/search/cs?searchtype=author&query=von+Luxburg%2C+U">Ulrike von Luxburg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In the rapidly growing literature on explanation algorithms, it often remains
unclear what precisely these algorithms are for and how they should be used. We
argue that this is because explanation algorithms are often mathematically
complex but don't admit a clear interpretation. Unfortunately, complex
statistical methods that don't have a clear interpretation are bound to lead to
errors in interpretation, a fact that has become increasingly apparent in the
literature. In order to move forward, papers on explanation algorithms should
make clear how precisely the output of the algorithms should be interpreted.
They should also clarify what questions about the function can and cannot be
answered given the explanations. Our argument is based on the distinction
between statistics and their interpretation. It also relies on parallels
between explainable machine learning and applied statistics.
</p>
</div>
</dd>
<dt><a name="item699">[699]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02871" title="Abstract">arXiv:2402.02871</a> [<a href="/pdf/2402.02871" title="Download PDF">pdf</a>, <a href="/format/2402.02871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Code-Based Single-Server Private Information Retrieval: Circumventing  the Sub-Query Attack
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verma%2C+N">Neehar Verma</a>, 
<a href="/search/cs?searchtype=author&query=Hollanti%2C+C">Camilla Hollanti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The scheme proposed in this work is a modified version of the scheme in <a href="/abs/2001.07049">arXiv:2001.07049</a> (IEEE ISIT 2020) and provides a mend against the attack discovered in <a href="/abs/2004.00509">arXiv:2004.00509</a> (Cryptography and Communications, 2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Cryptography and Security (cs.CR); Combinatorics (math.CO)

</div>
<p class="mathjax">Private information retrieval from a single server is considered, utilizing
random linear codes. Presented is a modified version of the first code-based
single-server computational PIR scheme proposed by Holzbaur, Hollanti, and
Wachter-Zeh in [Holzbaur et al., "Computational Code-Based Single-Server
Private Information Retrieval", 2020 IEEE ISIT]. The original scheme was broken
in [Bordage et al., "On the privacy of a code-based single-server computational
PIR scheme", Cryptogr. Comm., 2021] by an attack arising from highly probable
rank differences in sub-matrices of the user's query. Here, this attack is now
circumvented by ensuring that the sub-matrices have negligible rank difference.
Furthermore, the rank difference cannot be attributed to the desired file
index, thereby ensuring the privacy of the scheme. In the case of retrieving
multiple files, the rate of the modified scheme is largely unaffected and at
par with the original scheme.
</p>
</div>
</dd>
<dt><a name="item700">[700]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02872" title="Abstract">arXiv:2402.02872</a> [<a href="/pdf/2402.02872" title="Download PDF">pdf</a>, <a href="/format/2402.02872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How do Large Language Models Learn In-Context? Query and Key Matrices of  In-Context Heads are Two Towers for Metric Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zeping Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ananiadou%2C+S">Sophia Ananiadou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We explore the mechanism of in-context learning and propose a hypothesis
using locate-and-project method. In shallow layers, the features of
demonstrations are merged into their corresponding labels, and the features of
the input text are aggregated into the last token. In deep layers, in-context
heads make great contributions. In each in-context head, the value-output
matrix extracts the labels' features. Query and key matrices compute the
attention weights between the input text and each demonstration. The larger the
attention weight is, the more label information is transferred into the last
token for predicting the next word. Query and key matrices can be regarded as
two towers for learning the similarity metric between the input text and each
demonstration. Based on this hypothesis, we explain why imbalanced labels and
demonstration order affect predictions. We conduct experiments on GPT2 large,
Llama 7B, 13B and 30B. The results can support our analysis. Overall, our study
provides a new method and a reasonable hypothesis for understanding the
mechanism of in-context learning. Our code will be released on github.
</p>
</div>
</dd>
<dt><a name="item701">[701]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02874" title="Abstract">arXiv:2402.02874</a> [<a href="/pdf/2402.02874" title="Download PDF">pdf</a>, <a href="/format/2402.02874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Morse frames
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bertrand%2C+G">Gilles Bertrand</a> (LIGM), 
<a href="/search/cs?searchtype=author&query=Najman%2C+L">Laurent Najman</a> (LIGM)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Conference on Discrete Geometry and Mathematical
  Morphology (DGMM), S. Brunetti; A. Frosini; S. Rinaldi, Apr 2024, Florence,
  Italy
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>

</div>
<p class="mathjax">In the context of discrete Morse theory, we introduce Morse frames, which are
maps that associate a set of critical simplexes to all simplexes. The main
example of Morse frames are the Morse references. In particular, these Morse
references allow computing Morse complexes, an important tool for homology. We
highlight the link between Morse references and gradient flows. We also propose
a novel presentation of the Annotation algorithm for persistent cohomology, as
a variant of a Morse frame. Finally, we propose another construction, that
takes advantage of the Morse reference for computing the Betti numbers in mod 2
arithmetic.
</p>
</div>
</dd>
<dt><a name="item702">[702]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02877" title="Abstract">arXiv:2402.02877</a> [<a href="/pdf/2402.02877" title="Download PDF">pdf</a>, <a href="/ps/2402.02877" title="Download PostScript">ps</a>, <a href="/format/2402.02877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feedback to the European Data Protection Board&#x27;s Guidelines 2/2023 on  Technical Scope of Art. 5(3) of ePrivacy Directive
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Santos%2C+C">Cristiana Santos</a>, 
<a href="/search/cs?searchtype=author&query=Bielova%2C+N">Nataliia Bielova</a> (PRIVATICS), 
<a href="/search/cs?searchtype=author&query=Roca%2C+V">Vincent Roca</a> (PRIVATICS), 
<a href="/search/cs?searchtype=author&query=Cunche%2C+M">Mathieu Cunche</a> (PRIVATICS), 
<a href="/search/cs?searchtype=author&query=Mertens%2C+G">Gilles Mertens</a> (PRIVATICS), 
<a href="/search/cs?searchtype=author&query=Kubicek%2C+K">Karel Kubicek</a> (ETHZ), 
<a href="/search/cs?searchtype=author&query=Haddadi%2C+H">Hamed Haddadi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">We very much welcome the EDPB's Guidelines. Please find hereunder our
feedback to the Guidelines 2/2023 on Technical Scope of Art. 5(3) of ePrivacy
Directive. Our comments are presented after a quotation from the proposed text
by the EDPB in a box.
</p>
</div>
</dd>
<dt><a name="item703">[703]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02883" title="Abstract">arXiv:2402.02883</a> [<a href="/pdf/2402.02883" title="Download PDF">pdf</a>, <a href="/format/2402.02883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximate Attributions for Off-the-Shelf Siamese Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%B6ller%2C+L">Lucas M&#xf6;ller</a>, 
<a href="/search/cs?searchtype=author&query=Nikolaev%2C+D">Dmitry Nikolaev</a>, 
<a href="/search/cs?searchtype=author&query=Pad%C3%B3%2C+S">Sebastian Pad&#xf3;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for EACL 2024, St. Julian's, Malta
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Siamese encoders such as sentence transformers are among the least understood
deep models. Established attribution methods cannot tackle this model class
since it compares two inputs rather than processing a single one. To address
this gap, we have recently proposed an attribution method specifically for
Siamese encoders (M\"oller et al., 2023). However, it requires models to be
adjusted and fine-tuned and therefore cannot be directly applied to
off-the-shelf models. In this work, we reassess these restrictions and propose
(i) a model with exact attribution ability that retains the original model's
predictive performance and (ii) a way to compute approximate attributions for
off-the-shelf models. We extensively compare approximate and exact attributions
and use them to analyze the models' attendance to different linguistic aspects.
We gain insights into which syntactic roles Siamese transformers attend to,
confirm that they mostly ignore negation, explore how they judge semantically
opposite adjectives, and find that they exhibit lexical bias.
</p>
</div>
</dd>
<dt><a name="item704">[704]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02885" title="Abstract">arXiv:2402.02885</a> [<a href="/pdf/2402.02885" title="Download PDF">pdf</a>, <a href="/format/2402.02885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Review on Building Blocks of Decentralized Artificial Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kersic%2C+V">Vid Kersic</a>, 
<a href="/search/cs?searchtype=author&query=Turkanovic%2C+M">Muhamed Turkanovic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Artificial intelligence is transforming our lives, and technological progress
and transfer from the academic and theoretical sphere to the real world are
accelerating yearly. But during that progress and transition, several open
problems and questions need to be addressed for the field to develop ethically,
such as digital privacy, ownership, and control. These are some of the reasons
why the currently most popular approaches of artificial intelligence, i.e.,
centralized AI (CEAI), are questionable, with other directions also being
widely explored, such as decentralized artificial intelligence (DEAI), to solve
some of the most reaching problems. This paper provides a systematic literature
review (SLR) of existing work in the field of DEAI, presenting the findings of
71 identified studies. The paper's primary focus is identifying the building
blocks of DEAI solutions and networks, tackling the DEAI analysis from a
bottom-up approach. In the end, future directions of research and open problems
are proposed.
</p>
</div>
</dd>
<dt><a name="item705">[705]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02886" title="Abstract">arXiv:2402.02886</a> [<a href="/pdf/2402.02886" title="Download PDF">pdf</a>, <a href="/format/2402.02886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time-Distributed Backdoor Attacks on Federated Spiking Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abad%2C+G">Gorka Abad</a>, 
<a href="/search/cs?searchtype=author&query=Picek%2C+S">Stjepan Picek</a>, 
<a href="/search/cs?searchtype=author&query=Urbieta%2C+A">Aitor Urbieta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">This paper investigates the vulnerability of spiking neural networks (SNNs)
and federated learning (FL) to backdoor attacks using neuromorphic data.
Despite the efficiency of SNNs and the privacy advantages of FL, particularly
in low-powered devices, we demonstrate that these systems are susceptible to
such attacks. We first assess the viability of using FL with SNNs using
neuromorphic data, showing its potential usage. Then, we evaluate the
transferability of known FL attack methods to SNNs, finding that these lead to
suboptimal attack performance. Therefore, we explore backdoor attacks involving
single and multiple attackers to improve the attack performance. Our primary
contribution is developing a novel attack strategy tailored to SNNs and FL,
which distributes the backdoor trigger temporally and across malicious devices,
enhancing the attack's effectiveness and stealthiness. In the best case, we
achieve a 100 attack success rate, 0.13 MSE, and 98.9 SSIM. Moreover, we adapt
and evaluate an existing defense against backdoor attacks, revealing its
inadequacy in protecting SNNs. This study underscores the need for robust
security measures in deploying SNNs and FL, particularly in the context of
backdoor attacks.
</p>
</div>
</dd>
<dt><a name="item706">[706]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02887" title="Abstract">arXiv:2402.02887</a> [<a href="/pdf/2402.02887" title="Download PDF">pdf</a>, <a href="/format/2402.02887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time-, Memory- and Parameter-Efficient Visual Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mercea%2C+O">Otniel-Bogdan Mercea</a>, 
<a href="/search/cs?searchtype=author&query=Gritsenko%2C+A">Alexey Gritsenko</a>, 
<a href="/search/cs?searchtype=author&query=Schmid%2C+C">Cordelia Schmid</a>, 
<a href="/search/cs?searchtype=author&query=Arnab%2C+A">Anurag Arnab</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">As foundation models become more popular, there is a growing need to
efficiently finetune them for downstream tasks. Although numerous adaptation
methods have been proposed, they are designed to be efficient only in terms of
how many parameters are trained. They, however, typically still require
backpropagating gradients throughout the model, meaning that their
training-time and -memory cost does not reduce as significantly. We propose an
adaptation method which does not backpropagate gradients through the backbone.
We achieve this by designing a lightweight network in parallel that operates on
features from the frozen, pretrained backbone. As a result, our method is
efficient not only in terms of parameters, but also in training-time and memory
usage. Our approach achieves state-of-the-art accuracy-parameter trade-offs on
the popular VTAB benchmark, and we further show how we outperform prior works
with respect to training-time and -memory usage too. We further demonstrate the
training efficiency and scalability of our method by adapting a vision
transformer backbone of 4 billion parameters for the computationally demanding
task of video classification, without any intricate model parallelism. Here, we
outperform a prior adaptor-based method which could only scale to a 1 billion
parameter backbone, or fully-finetuning a smaller backbone, with the same GPU
and less training time.
</p>
</div>
</dd>
<dt><a name="item707">[707]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02889" title="Abstract">arXiv:2402.02889</a> [<a href="/pdf/2402.02889" title="Download PDF">pdf</a>, <a href="/format/2402.02889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Federated Self-Supervised Learning for General Purpose Audio  Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rehman%2C+Y+A+U">Yasar Abbas Ur Rehman</a>, 
<a href="/search/cs?searchtype=author&query=Lau%2C+K+W">Kin Wai Lau</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yuyang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jiajun Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">The integration of Federated Learning (FL) and Self-supervised Learning (SSL)
offers a unique and synergetic combination to exploit the audio data for
general-purpose audio understanding, without compromising user data privacy.
However, rare efforts have been made to investigate the SSL models in the FL
regime for general-purpose audio understanding, especially when the training
data is generated by large-scale heterogeneous audio sources. In this paper, we
evaluate the performance of feature-matching and predictive audio-SSL
techniques when integrated into large-scale FL settings simulated with
non-independently identically distributed (non-iid) data. We propose a novel
Federated SSL (F-SSL) framework, dubbed FASSL, that enables learning
intermediate feature representations from large-scale decentralized
heterogeneous clients, holding unlabelled audio data. Our study has found that
audio F-SSL approaches perform on par with the centralized audio-SSL approaches
on the audio-retrieval task. Extensive experiments demonstrate the
effectiveness and significance of FASSL as it assists in obtaining the optimal
global model for state-of-the-art FL aggregation methods.
</p>
</div>
</dd>
<dt><a name="item708">[708]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02890" title="Abstract">arXiv:2402.02890</a> [<a href="/pdf/2402.02890" title="Download PDF">pdf</a>, <a href="/format/2402.02890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Black-Box Approximation and Optimization with Hierarchical Tucker  Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ryzhakov%2C+G">Gleb Ryzhakov</a>, 
<a href="/search/cs?searchtype=author&query=Chertkov%2C+A">Andrei Chertkov</a>, 
<a href="/search/cs?searchtype=author&query=Basharin%2C+A">Artem Basharin</a>, 
<a href="/search/cs?searchtype=author&query=Oseledets%2C+I">Ivan Oseledets</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">We develop a new method HTBB for the multidimensional black-box approximation
and gradient-free optimization, which is based on the low-rank hierarchical
Tucker decomposition with the use of the MaxVol indices selection procedure.
Numerical experiments for 14 complex model problems demonstrate the robustness
of the proposed method for dimensions up to 1000, while it shows significantly
more accurate results than classical gradient-free optimization methods, as
well as approximation and optimization methods based on the popular tensor
train decomposition, which represents a simpler case of a tensor network.
</p>
</div>
</dd>
<dt><a name="item709">[709]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02892" title="Abstract">arXiv:2402.02892</a> [<a href="/pdf/2402.02892" title="Download PDF">pdf</a>, <a href="/format/2402.02892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Motion-Aware Video Frame Interpolation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+P">Pengfei Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fuhua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B">Bin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuelong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Video frame interpolation methodologies endeavor to create novel frames
betwixt extant ones, with the intent of augmenting the video's frame frequency.
However, current methods are prone to image blurring and spurious artifacts in
challenging scenarios involving occlusions and discontinuous motion. Moreover,
they typically rely on optical flow estimation, which adds complexity to
modeling and computational costs. To address these issues, we introduce a
Motion-Aware Video Frame Interpolation (MA-VFI) network, which directly
estimates intermediate optical flow from consecutive frames by introducing a
novel hierarchical pyramid module. It not only extracts global semantic
relationships and spatial details from input frames with different receptive
fields, enabling the model to capture intricate motion patterns, but also
effectively reduces the required computational cost and complexity.
Subsequently, a cross-scale motion structure is presented to estimate and
refine intermediate flow maps by the extracted features. This approach
facilitates the interplay between input frame features and flow maps during the
frame interpolation process and markedly heightens the precision of the
intervening flow delineations. Finally, a discerningly fashioned loss centered
around an intermediate flow is meticulously contrived, serving as a deft rudder
to skillfully guide the prognostication of said intermediate flow, thereby
substantially refining the precision of the intervening flow mappings.
Experiments illustrate that MA-VFI surpasses several representative VFI methods
across various datasets, and can enhance efficiency while maintaining
commendable efficacy.
</p>
</div>
</dd>
<dt><a name="item710">[710]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02893" title="Abstract">arXiv:2402.02893</a> [<a href="/pdf/2402.02893" title="Download PDF">pdf</a>, <a href="/format/2402.02893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Performance of RIS-Aided Spatial Modulation for Downlink  Transmission
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xusheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingqing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wen Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">In this study, we explore the performance of a reconfigurable reflecting
surface (RIS)-assisted transmit spatial modulation (SM) system for downlink
transmission, wherein the deployment of RIS serves the purpose of blind area
coverage within the channel. At the receiving end, we present three detectors,
i.e., maximum likelihood (ML) detector, two-stage ML detection, and greedy
detector to recover the transmitted signal. By utilizing the ML detector, we
initially derive the conditional pair error probability expression for the
proposed scheme. Subsequently, we leverage the central limit theorem (CLT) to
obtain the probability density function of the combined channel. Following
this, the Gaussian-Chebyshev quadrature method is applied to derive a
closed-form expression for the unconditional pair error probability and
establish the union tight upper bound for the average bit error probability
(ABEP). Furthermore, we derive a closed-form expression for the ergodic
capacity of the proposed RIS-SM scheme. Monte Carlo simulations are conducted
not only to assess the complexity and reliability of the three detection
algorithms but also to validate the results obtained through theoretical
derivation results.
</p>
</div>
</dd>
<dt><a name="item711">[711]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02895" title="Abstract">arXiv:2402.02895</a> [<a href="/pdf/2402.02895" title="Download PDF">pdf</a>, <a href="/format/2402.02895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Noisy group testing via spatial coupling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Coja-Oghlan%2C+A">Amin Coja-Oghlan</a>, 
<a href="/search/cs?searchtype=author&query=Hahn-Klimroth%2C+M">Max Hahn-Klimroth</a>, 
<a href="/search/cs?searchtype=author&query=Hintze%2C+L">Lukas Hintze</a>, 
<a href="/search/cs?searchtype=author&query=Kaaser%2C+D">Dominik Kaaser</a>, 
<a href="/search/cs?searchtype=author&query=Krieg%2C+L">Lena Krieg</a>, 
<a href="/search/cs?searchtype=author&query=Rolvien%2C+M">Maurice Rolvien</a>, 
<a href="/search/cs?searchtype=author&query=Scheftelowitsch%2C+O">Olga Scheftelowitsch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Information Theory (cs.IT); Combinatorics (math.CO)

</div>
<p class="mathjax">We study the problem of identifying a small set $k\sim n^\theta$,
$0&lt;\theta&lt;1$, of infected individuals within a large population of size $n$ by
testing groups of individuals simultaneously. All tests are conducted
concurrently. The goal is to minimise the total number of tests required. In
this paper we make the (realistic) assumption that tests are noisy, i.e.\ that
a group that contains an infected individual may return a negative test result
or one that does not contain an infected individual may return a positive test
results with a certain probability. The noise need not be symmetric. We develop
an algorithm called SPARC that correctly identifies the set of infected
individuals up to $o(k)$ errors with high probability with the asymptotically
minimum number of tests. Additionally, we develop an algorithm called SPEX that
exactly identifies the set of infected individuals w.h.p. with a number of
tests that matches the information-theoretic lower bound for the constant
column design, a powerful and well-studied test design.
</p>
</div>
</dd>
<dt><a name="item712">[712]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02896" title="Abstract">arXiv:2402.02896</a> [<a href="/pdf/2402.02896" title="Download PDF">pdf</a>, <a href="/format/2402.02896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM Agents in Interaction: Measuring Personality Consistency and  Linguistic Alignment in Interacting Populations of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Frisch%2C+I">Ivar Frisch</a>, 
<a href="/search/cs?searchtype=author&query=Giulianelli%2C+M">Mario Giulianelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in Proceedings of the 1st Personalization of Generative AI Workshop, EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">While both agent interaction and personalisation are vibrant topics in
research on large language models (LLMs), there has been limited focus on the
effect of language interaction on the behaviour of persona-conditioned LLM
agents. Such an endeavour is important to ensure that agents remain consistent
to their assigned traits yet are able to engage in open, naturalistic
dialogues. In our experiments, we condition GPT-3.5 on personality profiles
through prompting and create a two-group population of LLM agents using a
simple variability-inducing sampling algorithm. We then administer personality
tests and submit the agents to a collaborative writing task, finding that
different profiles exhibit different degrees of personality consistency and
linguistic alignment to their conversational partners. Our study seeks to lay
the groundwork for better understanding of dialogue-based interaction between
LLMs and highlights the need for new approaches to crafting robust, more
human-like LLM personas for interactive environments.
</p>
</div>
</dd>
<dt><a name="item713">[713]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02902" title="Abstract">arXiv:2402.02902</a> [<a href="/pdf/2402.02902" title="Download PDF">pdf</a>, <a href="/format/2402.02902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The eXtended Virtual Element Method for elliptic problems with weakly  singular solutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Droniou%2C+J">Jerome Droniou</a>, 
<a href="/search/math?searchtype=author&query=Manzini%2C+G">Gianmarco Manzini</a>, 
<a href="/search/math?searchtype=author&query=Yemm%2C+L">Liam Yemm</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This paper introduces a novel eXtended virtual element method, an extension
of the conforming virtual element method. The XVEM is formulated by
incorporating appropriate enrichment functions in the local spaces. The method
is designed to handle highly generic enrichment functions, including
singularities arising from fractured domains. By achieving consistency on the
enrichment space, the method is proven to achieve arbitrary approximation
orders even in the presence of singular solutions. The paper includes a
complete convergence analysis under general assumptions on mesh regularity, and
numerical experiments validating the method's accuracy on various mesh
families, demonstrating optimal convergence rates in the $L^2$- and $H^1$-norms
on fractured or L-shaped domains.
</p>
</div>
</dd>
<dt><a name="item714">[714]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02904" title="Abstract">arXiv:2402.02904</a> [<a href="/pdf/2402.02904" title="Download PDF">pdf</a>, <a href="/format/2402.02904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Replication of Impedance Identification Experiments on a  Reinforcement-Learning-Controlled Digital Twin of Human Elbows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zebin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qingbo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Carlucho%2C+I">Ignacio Carlucho</a>, 
<a href="/search/cs?searchtype=author&query=Erden%2C+M+S">Mustafa Suphi Erden</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures; Submitted to WCCI-2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This study presents a pioneering effort to replicate human neuromechanical
experiments within a virtual environment utilising a digital human model. By
employing MyoSuite, a state-of-the-art human motion simulation platform
enhanced by Reinforcement Learning (RL), multiple types of impedance
identification experiments of human elbow were replicated on a musculoskeletal
model. We compared the elbow movement controlled by an RL agent with the motion
of an actual human elbow in terms of the impedance identified in
torque-perturbation experiments. The findings reveal that the RL agent exhibits
higher elbow impedance to stabilise the target elbow motion under perturbation
than a human does, likely due to its shorter reaction time and superior sensory
capabilities. This study serves as a preliminary exploration into the potential
of virtual environment simulations for neuromechanical research, offering an
initial yet promising alternative to conventional experimental approaches. An
RL-controlled digital twin with complete musculoskeletal models of the human
body is expected to be useful in designing experiments and validating
rehabilitation theory before experiments on real human subjects.
</p>
</div>
</dd>
<dt><a name="item715">[715]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02905" title="Abstract">arXiv:2402.02905</a> [<a href="/pdf/2402.02905" title="Download PDF">pdf</a>, <a href="/format/2402.02905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational discretizations of ideal magnetohydrodynamics in smooth  regime using finite element exterior calculus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Carlier%2C+V">Valentin Carlier</a>, 
<a href="/search/math?searchtype=author&query=Campos-Pinto%2C+M">Martin Campos-Pinto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We propose a new class of finite element approximations to ideal compressible
magnetohydrodynamic equations in smooth regime. Following variational
approximations developed for fluid models in the last decade, our
discretizations are built via a discrete variational principle mimicking the
continuous Euler-Poincare principle, and to further exploit the geometrical
structure of the problem, vector fields are represented by their action as Lie
derivatives on differential forms of any degree. The resulting semi-discrete
approximations are shown to conserve the total mass, entropy and energy of the
solutions for a wide class of finite element approximations. In addition, the
divergence-free nature of the magnetic field is preserved in a pointwise sense
and a time discretization is proposed, preserving those invariants and giving a
reversible scheme at the fully discrete level. Numerical simulations are
conducted to verify the accuracy of our approach and its ability to preserve
the invariants for several test problems.
</p>
</div>
</dd>
<dt><a name="item716">[716]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02906" title="Abstract">arXiv:2402.02906</a> [<a href="/pdf/2402.02906" title="Download PDF">pdf</a>, <a href="/format/2402.02906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ViewFusion: Learning Composable Diffusion Models for Novel View  Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Spiegl%2C+B">Bernard Spiegl</a>, 
<a href="/search/cs?searchtype=author&query=Perin%2C+A">Andrea Perin</a>, 
<a href="/search/cs?searchtype=author&query=Deny%2C+S">St&#xe9;phane Deny</a>, 
<a href="/search/cs?searchtype=author&query=Ilin%2C+A">Alexander Ilin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep learning is providing a wealth of new approaches to the old problem of
novel view synthesis, from Neural Radiance Field (NeRF) based approaches to
end-to-end style architectures. Each approach offers specific strengths but
also comes with specific limitations in their applicability. This work
introduces ViewFusion, a state-of-the-art end-to-end generative approach to
novel view synthesis with unparalleled flexibility. ViewFusion consists in
simultaneously applying a diffusion denoising step to any number of input views
of a scene, then combining the noise gradients obtained for each view with an
(inferred) pixel-weighting mask, ensuring that for each region of the target
scene only the most informative input views are taken into account. Our
approach resolves several limitations of previous approaches by (1) being
trainable and generalizing across multiple scenes and object classes, (2)
adaptively taking in a variable number of pose-free views at both train and
test time, (3) generating plausible views even in severely undetermined
conditions (thanks to its generative nature) -- all while generating views of
quality on par or even better than state-of-the-art methods. Limitations
include not generating a 3D embedding of the scene, resulting in a relatively
slow inference speed, and our method only being tested on the relatively small
dataset NMR. Code is available.
</p>
</div>
</dd>
<dt><a name="item717">[717]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02910" title="Abstract">arXiv:2402.02910</a> [<a href="/pdf/2402.02910" title="Download PDF">pdf</a>, <a href="/format/2402.02910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DS-MS-TCN: Otago Exercises Recognition with a Dual-Scale Multi-Stage  Temporal Convolutional Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shang%2C+M">Meng Shang</a>, 
<a href="/search/cs?searchtype=author&query=Dedeyne%2C+L">Lenore Dedeyne</a>, 
<a href="/search/cs?searchtype=author&query=Dupont%2C+J">Jolan Dupont</a>, 
<a href="/search/cs?searchtype=author&query=Vercauteren%2C+L">Laura Vercauteren</a>, 
<a href="/search/cs?searchtype=author&query=Amini%2C+N">Nadjia Amini</a>, 
<a href="/search/cs?searchtype=author&query=Lapauw%2C+L">Laurence Lapauw</a>, 
<a href="/search/cs?searchtype=author&query=Gielen%2C+E">Evelien Gielen</a>, 
<a href="/search/cs?searchtype=author&query=Verschueren%2C+S">Sabine Verschueren</a>, 
<a href="/search/cs?searchtype=author&query=Varon%2C+C">Carolina Varon</a>, 
<a href="/search/cs?searchtype=author&query=De+Raedt%2C+W">Walter De Raedt</a>, 
<a href="/search/cs?searchtype=author&query=Vanrumste%2C+B">Bart Vanrumste</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP)

</div>
<p class="mathjax">The Otago Exercise Program (OEP) represents a crucial rehabilitation
initiative tailored for older adults, aimed at enhancing balance and strength.
Despite previous efforts utilizing wearable sensors for OEP recognition,
existing studies have exhibited limitations in terms of accuracy and
robustness. This study addresses these limitations by employing a single
waist-mounted Inertial Measurement Unit (IMU) to recognize OEP exercises among
community-dwelling older adults in their daily lives. A cohort of 36 older
adults participated in laboratory settings, supplemented by an additional 7
older adults recruited for at-home assessments. The study proposes a Dual-Scale
Multi-Stage Temporal Convolutional Network (DS-MS-TCN) designed for two-level
sequence-to-sequence classification, incorporating them in one loss function.
In the first stage, the model focuses on recognizing each repetition of the
exercises (micro labels). Subsequent stages extend the recognition to encompass
the complete range of exercises (macro labels). The DS-MS-TCN model surpasses
existing state-of-the-art deep learning models, achieving f1-scores exceeding
80% and Intersection over Union (IoU) f1-scores surpassing 60% for all four
exercises evaluated. Notably, the model outperforms the prior study utilizing
the sliding window technique, eliminating the need for post-processing stages
and window size tuning. To our knowledge, we are the first to present a novel
perspective on enhancing Human Activity Recognition (HAR) systems through the
recognition of each repetition of activities.
</p>
</div>
</dd>
<dt><a name="item718">[718]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02915" title="Abstract">arXiv:2402.02915</a> [<a href="/pdf/2402.02915" title="Download PDF">pdf</a>, <a href="/format/2402.02915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Computational Model for the Assessment of Mutual Intelligibility Among  Closely Related Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nieder%2C+J">Jessica Nieder</a>, 
<a href="/search/cs?searchtype=author&query=List%2C+J">Johann-Mattis List</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in: Proceedings of the 6th Workshop on Research in Computational Linguistic Typology and Multilingual NLP (SIGTYP 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Closely related languages show linguistic similarities that allow speakers of
one language to understand speakers of another language without having actively
learned it. Mutual intelligibility varies in degree and is typically tested in
psycholinguistic experiments. To study mutual intelligibility computationally,
we propose a computer-assisted method using the Linear Discriminative Learner,
a computational model developed to approximate the cognitive processes by which
humans learn languages, which we expand with multilingual semantic vectors and
multilingual sound classes. We test the model on cognate data from German,
Dutch, and English, three closely related Germanic languages. We find that our
model's comprehension accuracy depends on 1) the automatic trimming of
inflections and 2) the language pair for which comprehension is tested. Our
multilingual modelling approach does not only offer new methodological findings
for automatic testing of mutual intelligibility across languages but also
extends the use of Linear Discriminative Learning to multilingual settings.
</p>
</div>
</dd>
<dt><a name="item719">[719]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02917" title="Abstract">arXiv:2402.02917</a> [<a href="/pdf/2402.02917" title="Download PDF">pdf</a>, <a href="/format/2402.02917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Construction of Optimal Algorithms for Function Approximation in  Gaussian Sobolev Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Suzuki%2C+Y">Yuya Suzuki</a>, 
<a href="/search/math?searchtype=author&query=Karvonen%2C+T">Toni Karvonen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This paper studies function approximation in Gaussian Sobolev spaces over the
real line and measures the error in a Gaussian-weighted $L^p$-norm. We
construct two linear approximation algorithms using $n$ function evaluations
that achieve the optimal or almost optimal rate of worst-case convergence in a
Gaussian Sobolev space of order $\alpha$. The first algorithm is based on
scaled trigonometric interpolation and achieves the optimal rate $n^{-\alpha}$
up to a logarithmic factor. This algorithm can be constructed in almost-linear
time with the fast Fourier transform. The second algorithm is more complicated,
being based on spline smoothing, but attains the optimal rate $n^{-\alpha}$.
</p>
</div>
</dd>
<dt><a name="item720">[720]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02920" title="Abstract">arXiv:2402.02920</a> [<a href="/pdf/2402.02920" title="Download PDF">pdf</a>, <a href="/format/2402.02920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A subspace method for large-scale trace ratio problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ferrandi%2C+G">G. Ferrandi</a>, 
<a href="/search/math?searchtype=author&query=Hochstenbach%2C+M+E">M. E. Hochstenbach</a>, 
<a href="/search/math?searchtype=author&query=Oliveira%2C+M+R">M. R. Oliveira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We present a subspace method to solve large-scale trace ratio problems. This
method is matrix-free, only needing the action of the two matrices in the trace
ratio. At each iteration, a smaller trace ratio problem is addressed in the
search subspace. Additionally, our algorithm is endowed with a restarting
strategy, that ensures the monotonicity of the trace ratio value throughout the
iterations. We also investigate the behavior of the approximate solution from a
theoretical viewpoint, extending existing results on Ritz values and vectors,
as the angle between the search subspace and the exact solution to the trace
ratio approaches zero. In the context of multigroup classification, numerical
experiments show that the new subspace method tends to be more efficient than
iterative approaches that need a (partial) eigenvalue decomposition in every
step.
</p>
</div>
</dd>
<dt><a name="item721">[721]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02921" title="Abstract">arXiv:2402.02921</a> [<a href="/pdf/2402.02921" title="Download PDF">pdf</a>, <a href="/format/2402.02921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mining a Minimal Set of Behavioral Patterns using Incremental Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Acheli%2C+M">Mehdi Acheli</a>, 
<a href="/search/cs?searchtype=author&query=Grigori%2C+D">Daniela Grigori</a>, 
<a href="/search/cs?searchtype=author&query=Weidlich%2C+M">Matthias Weidlich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Process mining provides methods to analyse event logs generated by
information systems during the execution of processes. It thereby supports the
design, validation, and execution of processes in domains ranging from
healthcare, through manufacturing, to e-commerce. To explore the regularities
of flexible processes that show a large behavioral variability, it was
suggested to mine recurrent behavioral patterns that jointly describe the
underlying process. Existing approaches to behavioral pattern mining, however,
suffer from two limitations. First, they show limited scalability as
incremental computation is incorporated only in the generation of pattern
candidates, but not in the evaluation of their quality. Second, process
analysis based on mined patterns shows limited effectiveness due to an
overwhelmingly large number of patterns obtained in practical application
scenarios, many of which are redundant. In this paper, we address these
limitations to facilitate the analysis of complex, flexible processes based on
behavioral patterns. Specifically, we improve COBPAM, our initial behavioral
pattern mining algorithm, by an incremental procedure to evaluate the quality
of pattern candidates, optimizing thereby its efficiency. Targeting a more
effective use of the resulting patterns, we further propose pruning strategies
for redundant patterns and show how relations between the remaining patterns
are extracted and visualized to provide process insights. Our experiments with
diverse real-world datasets indicate a considerable reduction of the runtime
needed for pattern mining, while a qualitative assessment highlights how
relations between patterns guide the analysis of the underlying process.
</p>
</div>
</dd>
<dt><a name="item722">[722]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02922" title="Abstract">arXiv:2402.02922</a> [<a href="/pdf/2402.02922" title="Download PDF">pdf</a>, <a href="/format/2402.02922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pixel-Wise Color Constancy via Smoothness Techniques in Multi-Illuminant  Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Entok%2C+U+C">Umut Cem Entok</a>, 
<a href="/search/cs?searchtype=author&query=Laakom%2C+F">Firas Laakom</a>, 
<a href="/search/cs?searchtype=author&query=Pakdaman%2C+F">Farhad Pakdaman</a>, 
<a href="/search/cs?searchtype=author&query=Gabbouj%2C+M">Moncef Gabbouj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Copyright 2024 IEEE - Submitted to IEEE ICIP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Most scenes are illuminated by several light sources, where the traditional
assumption of uniform illumination is invalid. This issue is ignored in most
color constancy methods, primarily due to the complex spatial impact of
multiple light sources on the image. Moreover, most existing multi-illuminant
methods fail to preserve the smooth change of illumination, which stems from
spatial dependencies in natural images. Motivated by this, we propose a novel
multi-illuminant color constancy method, by learning pixel-wise illumination
maps caused by multiple light sources. The proposed method enforces smoothness
within neighboring pixels, by regularizing the training with the total
variation loss. Moreover, a bilateral filter is provisioned further to enhance
the natural appearance of the estimated images, while preserving the edges.
Additionally, we propose a label-smoothing technique that enables the model to
generalize well despite the uncertainties in ground truth. Quantitative and
qualitative experiments demonstrate that the proposed method outperforms the
state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item723">[723]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02925" title="Abstract">arXiv:2402.02925</a> [<a href="/pdf/2402.02925" title="Download PDF">pdf</a>, <a href="/format/2402.02925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Test Case Prioritization in Industrial Test Result Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Torbunova%2C+A">Alina Torbunova</a>, 
<a href="/search/cs?searchtype=author&query=Strandberg%2C+P+E">Per Erik Strandberg</a>, 
<a href="/search/cs?searchtype=author&query=Porres%2C+I">Ivan Porres</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Regression testing in software development checks if new software features
affect existing ones. Regression testing is a key task in continuous
development and integration, where software is built in small increments and
new features are integrated as soon as possible. It is therefore important that
developers are notified about possible faults quickly. In this article, we
propose a test case prioritization schema that combines the use of a static and
a dynamic prioritization algorithm. The dynamic prioritization algorithm
rearranges the order of execution of tests on the fly, while the tests are
being executed. We propose to use a conditional probability dynamic algorithm
for this. We evaluate our solution on three industrial datasets and utilize
Average Percentage of Fault Detection for that. The main findings are that our
dynamic prioritization algorithm can: a) be applied with any static algorithm
that assigns a priority score to each test case b) can improve the performance
of the static algorithm if there are failure correlations between test cases c)
can also reduce the performance of the static algorithm, but only when the
static scheduling is performed at a near optimal level.
</p>
</div>
</dd>
<dt><a name="item724">[724]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02926" title="Abstract">arXiv:2402.02926</a> [<a href="/pdf/2402.02926" title="Download PDF">pdf</a>, <a href="/format/2402.02926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Cognate Detection as a Supervised Link Prediction Task with  Cognate Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akavarapu%2C+V+S+D+S+M">V.S.D.S.Mahesh Akavarapu</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+A">Arnab Bhattacharya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL-2024 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Identification of cognates across related languages is one of the primary
problems in historical linguistics. Automated cognate identification is helpful
for several downstream tasks including identifying sound correspondences,
proto-language reconstruction, phylogenetic classification, etc. Previous
state-of-the-art methods for cognate identification are mostly based on
distributions of phonemes computed across multilingual wordlists and make
little use of the cognacy labels that define links among cognate clusters. In
this paper, we present a transformer-based architecture inspired by
computational biology for the task of automated cognate detection. Beyond a
certain amount of supervision, this method performs better than the existing
methods, and shows steady improvement with further increase in supervision,
thereby proving the efficacy of utilizing the labeled information. We also
demonstrate that accepting multiple sequence alignments as input and having an
end-to-end architecture with link prediction head saves much computation time
while simultaneously yielding superior performance.
</p>
</div>
</dd>
<dt><a name="item725">[725]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02928" title="Abstract">arXiv:2402.02928</a> [<a href="/pdf/2402.02928" title="Download PDF">pdf</a>, <a href="/format/2402.02928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instance Segmentation XXL-CT Challenge of a Historic Airplane
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gruber%2C+R">Roland Gruber</a>, 
<a href="/search/cs?searchtype=author&query=Engster%2C+J+C">Johann Christopher Engster</a>, 
<a href="/search/cs?searchtype=author&query=Michen%2C+M">Markus Michen</a>, 
<a href="/search/cs?searchtype=author&query=Blum%2C+N">Nele Blum</a>, 
<a href="/search/cs?searchtype=author&query=Stille%2C+M">Maik Stille</a>, 
<a href="/search/cs?searchtype=author&query=Gerth%2C+S">Stefan Gerth</a>, 
<a href="/search/cs?searchtype=author&query=Wittenberg%2C+T">Thomas Wittenberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Instance segmentation of compound objects in XXL-CT imagery poses a unique
challenge in non-destructive testing. This complexity arises from the lack of
known reference segmentation labels, limited applicable segmentation tools, as
well as partially degraded image quality. To asses recent advancements in the
field of machine learning-based image segmentation, the "Instance Segmentation
XXL-CT Challenge of a Historic Airplane" was conducted. The challenge aimed to
explore automatic or interactive instance segmentation methods for an efficient
delineation of the different aircraft components, such as screws, rivets, metal
sheets or pressure tubes. We report the organization and outcome of this
challenge and describe the capabilities and limitations of the submitted
segmentation methods.
</p>
</div>
</dd>
<dt><a name="item726">[726]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02930" title="Abstract">arXiv:2402.02930</a> [<a href="/pdf/2402.02930" title="Download PDF">pdf</a>, <a href="/format/2402.02930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Embedding Hardware Approximations in Discrete Genetic-based Training for  Printed MLPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Afentaki%2C+F">Florentia Afentaki</a>, 
<a href="/search/cs?searchtype=author&query=Hefenbrock%2C+M">Michael Hefenbrock</a>, 
<a href="/search/cs?searchtype=author&query=Zervakis%2C+G">Georgios Zervakis</a>, 
<a href="/search/cs?searchtype=author&query=Tahoori%2C+M+B">Mehdi B. Tahoori</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at the 27th Design, Automation and Test in Europe Conference (DATE'24), Mar 25-27 2024, Valencia, Spain
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Printed Electronics (PE) stands out as a promisingtechnology for widespread
computing due to its distinct attributes, such as low costs and flexible
manufacturing. Unlike traditional silicon-based technologies, PE enables
stretchable, conformal,and non-toxic hardware. However, PE are constrained by
larger feature sizes, making it challenging to implement complex circuits such
as machine learning (ML) classifiers. Approximate computing has been proven to
reduce the hardware cost of ML circuits such as Multilayer Perceptrons (MLPs).
In this paper, we maximize the benefits of approximate computing by integrating
hardware approximation into the MLP training process. Due to the discrete
nature of hardware approximation, we propose and implement a genetic-based,
approximate, hardware-aware training approach specifically designed for printed
MLPs. For a 5% accuracy loss, our MLPs achieve over 5x area and power reduction
compared to the baseline while outperforming state of-the-art approximate and
stochastic printed MLPs.
</p>
</div>
</dd>
<dt><a name="item727">[727]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02932" title="Abstract">arXiv:2402.02932</a> [<a href="/pdf/2402.02932" title="Download PDF">pdf</a>, <a href="/format/2402.02932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain Adaptation of Multilingual Semantic Search -- Literature Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bringmann%2C+A">Anna Bringmann</a>, 
<a href="/search/cs?searchtype=author&query=Zhukova%2C+A">Anastasia Zhukova</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This literature review gives an overview of current approaches to perform
domain adaptation in a low-resource and approaches to perform multilingual
semantic search in a low-resource setting. We developed a new typology to
cluster domain adaptation approaches based on the part of dense textual
information retrieval systems, which they adapt, focusing on how to combine
them efficiently. We also explore the possibilities of combining multilingual
semantic search with domain adaptation approaches for dense retrievers in a
low-resource setting.
</p>
</div>
</dd>
<dt><a name="item728">[728]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02933" title="Abstract">arXiv:2402.02933</a> [<a href="/pdf/2402.02933" title="Download PDF">pdf</a>, <a href="/format/2402.02933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InterpretCC: Conditional Computation for Inherently Interpretable Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Swamy%2C+V">Vinitra Swamy</a>, 
<a href="/search/cs?searchtype=author&query=Blackwell%2C+J">Julian Blackwell</a>, 
<a href="/search/cs?searchtype=author&query=Frej%2C+J">Jibril Frej</a>, 
<a href="/search/cs?searchtype=author&query=Jaggi%2C+M">Martin Jaggi</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%A4ser%2C+T">Tanja K&#xe4;ser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Real-world interpretability for neural networks is a tradeoff between three
concerns: 1) it requires humans to trust the explanation approximation (e.g.
post-hoc approaches), 2) it compromises the understandability of the
explanation (e.g. automatically identified feature masks), and 3) it
compromises the model performance (e.g. decision trees). These shortcomings are
unacceptable for human-facing domains, like education, healthcare, or natural
language, which require trustworthy explanations, actionable interpretations,
and accurate predictions. In this work, we present InterpretCC (interpretable
conditional computation), a family of interpretable-by-design neural networks
that guarantee human-centric interpretability while maintaining comparable
performance to state-of-the-art models by adaptively and sparsely activating
features before prediction. We extend this idea into an interpretable
mixture-of-experts model, that allows humans to specify topics of interest,
discretely separates the feature space for each data point into topical
subnetworks, and adaptively and sparsely activates these topical subnetworks.
We demonstrate variations of the InterpretCC architecture for text and tabular
data across several real-world benchmarks: six online education courses, news
classification, breast cancer diagnosis, and review sentiment.
</p>
</div>
</dd>
<dt><a name="item729">[729]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02938" title="Abstract">arXiv:2402.02938</a> [<a href="/pdf/2402.02938" title="Download PDF">pdf</a>, <a href="/ps/2402.02938" title="Download PostScript">ps</a>, <a href="/format/2402.02938" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design and Implementation of an Automated Disaster-recovery System for a  Kubernetes Cluster Using LSTM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Ji-Beom Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Je-Bum Choi</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+E">Eun-Sung Jung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">With the increasing importance of data in the modern business environment,
effective data man-agement and protection strategies are gaining increasing
research attention. Data protection in a cloud environment is crucial for
safeguarding information assets and maintaining sustainable services. This
study introduces a system structure that integrates Kubernetes management
plat-forms with backup and restoration tools. This system is designed to
immediately detect disasters and automatically recover applications from
another kubernetes cluster. The experimental results show that this system
executes the restoration process within 15 s without human intervention,
enabling rapid recovery. This, in turn, significantly reduces the potential for
delays and errors compared with manual recovery processes, thereby enhancing
data management and recovery ef-ficiency in cloud environments. Moreover, our
research model predicts the CPU utilization of the cluster using Long
Short-Term Memory (LSTM). The necessity of scheduling through this predict is
made clearer through comparison with experiments without scheduling,
demonstrating its ability to prevent performance degradation. This research
highlights the efficiency and necessity of automatic recovery systems in cloud
environments, setting a new direction for future research.
</p>
</div>
</dd>
<dt><a name="item730">[730]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02941" title="Abstract">arXiv:2402.02941</a> [<a href="/pdf/2402.02941" title="Download PDF">pdf</a>, <a href="/ps/2402.02941" title="Download PostScript">ps</a>, <a href="/format/2402.02941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Synergies of Hybrid CNNs and ViTs Architectures for  Computer Vision: A survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yunusa%2C+H">Haruna Yunusa</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+S">Shiyin Qin</a>, 
<a href="/search/cs?searchtype=author&query=Chukkol%2C+A+H+A">Abdulrahman Hamman Adama Chukkol</a>, 
<a href="/search/cs?searchtype=author&query=Yusuf%2C+A+A">Abdulganiyu Abdu Yusuf</a>, 
<a href="/search/cs?searchtype=author&query=Bello%2C+I">Isah Bello</a>, 
<a href="/search/cs?searchtype=author&query=Lawan%2C+A">Adamu Lawan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The hybrid of Convolutional Neural Network (CNN) and Vision Transformers
(ViT) architectures has emerged as a groundbreaking approach, pushing the
boundaries of computer vision (CV). This comprehensive review provides a
thorough examination of the literature on state-of-the-art hybrid CNN-ViT
architectures, exploring the synergies between these two approaches. The main
content of this survey includes: (1) a background on the vanilla CNN and ViT,
(2) systematic review of various taxonomic hybrid designs to explore the
synergy achieved through merging CNNs and ViTs models, (3) comparative analysis
and application task-specific synergy between different hybrid architectures,
(4) challenges and future directions for hybrid models, (5) lastly, the survey
concludes with a summary of key findings and recommendations. Through this
exploration of hybrid CV architectures, the survey aims to serve as a guiding
resource, fostering a deeper understanding of the intricate dynamics between
CNNs and ViTs and their collective impact on shaping the future of CV
architectures.
</p>
</div>
</dd>
<dt><a name="item731">[731]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02946" title="Abstract">arXiv:2402.02946</a> [<a href="/pdf/2402.02946" title="Download PDF">pdf</a>, <a href="/format/2402.02946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HoughToRadon Transform: New Neural Network Layer for Features  Improvement in Projection Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhabitskaya%2C+A">Alexandra Zhabitskaya</a>, 
<a href="/search/cs?searchtype=author&query=Sheshkus%2C+A">Alexander Sheshkus</a>, 
<a href="/search/cs?searchtype=author&query=Arlazarov%2C+V+L">Vladimir L. Arlazarov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">In this paper, we introduce HoughToRadon Transform layer, a novel layer
designed to improve the speed of neural networks incorporated with Hough
Transform to solve semantic image segmentation problems. By placing it after a
Hough Transform layer, "inner" convolutions receive modified feature maps with
new beneficial properties, such as a smaller area of processed images and
parameter space linearity by angle and shift. These properties were not
presented in Hough Transform alone. Furthermore, HoughToRadon Transform layer
allows us to adjust the size of intermediate feature maps using two new
parameters, thus allowing us to balance the speed and quality of the resulting
neural network. Our experiments on the open MIDV-500 dataset show that this new
approach leads to time savings in document segmentation tasks and achieves
state-of-the-art 97.7% accuracy, outperforming HoughEncoder with larger
computational complexity.
</p>
</div>
</dd>
<dt><a name="item732">[732]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02949" title="Abstract">arXiv:2402.02949</a> [<a href="/pdf/2402.02949" title="Download PDF">pdf</a>, <a href="/format/2402.02949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kernel PCA for Out-of-Distribution Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+K">Kun Fang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Q">Qinghua Tao</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+K">Kexin Lv</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+M">Mingzhen He</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaolin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jie Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Out-of-Distribution (OoD) detection is vital for the reliability of Deep
Neural Networks (DNNs). Existing works have shown the insufficiency of
Principal Component Analysis (PCA) straightforwardly applied on the features of
DNNs in detecting OoD data from In-Distribution (InD) data. The failure of PCA
suggests that the network features residing in OoD and InD are not well
separated by simply proceeding in a linear subspace, which instead can be
resolved through proper nonlinear mappings. In this work, we leverage the
framework of Kernel PCA (KPCA) for OoD detection, seeking subspaces where OoD
and InD features are allocated with significantly different patterns. We devise
two feature mappings that induce non-linear kernels in KPCA to advocate the
separability between InD and OoD data in the subspace spanned by the principal
components. Given any test sample, the reconstruction error in such subspace is
then used to efficiently obtain the detection result with $\mathcal{O}(1)$ time
complexity in inference. Extensive empirical results on multiple OoD data sets
and network structures verify the superiority of our KPCA-based detector in
efficiency and efficacy with state-of-the-art OoD detection performances.
</p>
</div>
</dd>
<dt><a name="item733">[733]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02950" title="Abstract">arXiv:2402.02950</a> [<a href="/pdf/2402.02950" title="Download PDF">pdf</a>, <a href="/format/2402.02950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Entropy Can Simultaneously Benefit Transmission Efficiency and  Channel Security of Wireless Semantic Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rong%2C+Y">Yankai Rong</a>, 
<a href="/search/cs?searchtype=author&query=Nan%2C+G">Guoshun Nan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Minwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sihan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Songtao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuefei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+N">Nan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+S">Shixun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhaohui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Q">Qimei Cui</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+X">Xiaofeng Tao</a>, 
<a href="/search/cs?searchtype=author&query=Quek%2C+T+Q+S">Tony Q.S. Quek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Recently proliferated deep learning-based semantic communications (DLSC)
focus on how transmitted symbols efficiently convey a desired meaning to the
destination. However, the sensitivity of neural models and the openness of
wireless channels cause the DLSC system to be extremely fragile to various
malicious attacks. This inspires us to ask a question: ``Can we further exploit
the advantages of transmission efficiency in wireless semantic communications
while also alleviating its security disadvantages?''. Keeping this in mind, we
propose SemEntropy, a novel method that answers the above question by exploring
the semantics of data for both adaptive transmission and physical layer
encryption. Specifically, we first introduce semantic entropy, which indicates
the expectation of various semantic scores regarding the transmission goal of
the DLSC. Equipped with such semantic entropy, we can dynamically assign
informative semantics to Orthogonal Frequency Division Multiplexing (OFDM)
subcarriers with better channel conditions in a fine-grained manner. We also
use the entropy to guide semantic key generation to safeguard communications
over open wireless channels. By doing so, both transmission efficiency and
channel security can be simultaneously improved. Extensive experiments over
various benchmarks show the effectiveness of the proposed SemEntropy. We
discuss the reason why our proposed method benefits secure transmission of
DLSC, and also give some interesting findings, e.g., SemEntropy can keep the
semantic accuracy remain 95\% with 60\% less transmission.
</p>
</div>
</dd>
<dt><a name="item734">[734]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02951" title="Abstract">arXiv:2402.02951</a> [<a href="/pdf/2402.02951" title="Download PDF">pdf</a>, <a href="/format/2402.02951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Byzantine-Robust Learning: Adapting to Switching Byzantine  Workers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dorfman%2C+R">Ron Dorfman</a>, 
<a href="/search/cs?searchtype=author&query=Yehya%2C+N">Naseem Yehya</a>, 
<a href="/search/cs?searchtype=author&query=Levy%2C+K+Y">Kfir Y. Levy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Byzantine-robust learning has emerged as a prominent fault-tolerant
distributed machine learning framework. However, most techniques consider the
static setting, wherein the identity of Byzantine machines remains fixed during
the learning process. This assumption does not capture real-world dynamic
Byzantine behaviors, which may include transient malfunctions or targeted
temporal attacks. Addressing this limitation, we propose $\textsf{DynaBRO}$ --
a new method capable of withstanding $\mathcal{O}(\sqrt{T})$ rounds of
Byzantine identity alterations (where $T$ is the total number of training
rounds), while matching the asymptotic convergence rate of the static setting.
Our method combines a multi-level Monte Carlo (MLMC) gradient estimation
technique with robust aggregation of worker updates and incorporates a
fail-safe filter to limit bias from dynamic Byzantine strategies. Additionally,
by leveraging an adaptive learning rate, our approach eliminates the need for
knowing the percentage of Byzantine workers.
</p>
</div>
</dd>
<dt><a name="item735">[735]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02953" title="Abstract">arXiv:2402.02953</a> [<a href="/pdf/2402.02953" title="Download PDF">pdf</a>, <a href="/format/2402.02953" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unraveling the Key of Machine Learning Solutions for Android Malware  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiahao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+J">Jun Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Pierazzi%2C+F">Fabio Pierazzi</a>, 
<a href="/search/cs?searchtype=author&query=Cavallaro%2C+L">Lorenzo Cavallaro</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Z">Zhenkai Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Android malware detection serves as the front line against malicious apps.
With the rapid advancement of machine learning (ML), ML-based Android malware
detection has attracted increasing attention due to its capability of
automatically capturing malicious patterns from Android APKs. These
learning-driven methods have reported promising results in detecting malware.
However, the absence of an in-depth analysis of current research progress makes
it difficult to gain a holistic picture of the state of the art in this area.
<br />This paper presents a comprehensive investigation to date into ML-based
Android malware detection with empirical and quantitative analysis. We first
survey the literature, categorizing contributions into a taxonomy based on the
Android feature engineering and ML modeling pipeline. Then, we design a
general-propose framework for ML-based Android malware detection, re-implement
12 representative approaches from different research communities, and evaluate
them from three primary dimensions, i.e., effectiveness, robustness, and
efficiency. The evaluation reveals that ML-based approaches still face open
challenges and provides insightful findings like more powerful ML models are
not the silver bullet for designing better malware detectors. We further
summarize our findings and put forth recommendations to guide future research.
</p>
</div>
</dd>
<dt><a name="item736">[736]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02954" title="Abstract">arXiv:2402.02954</a> [<a href="/pdf/2402.02954" title="Download PDF">pdf</a>, <a href="/format/2402.02954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving Hierarchical Information-Sharing Dec-POMDPs: An Extensive-Form  Game Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peralez%2C+J">Johan Peralez</a>, 
<a href="/search/cs?searchtype=author&query=Delage%2C+A">Aur&#xe9;lien Delage</a>, 
<a href="/search/cs?searchtype=author&query=Buffet%2C+O">Olivier Buffet</a>, 
<a href="/search/cs?searchtype=author&query=Dibangoye%2C+J+S">Jilles S. Dibangoye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">A recent theory shows that a multi-player decentralized partially observable
Markov decision process can be transformed into an equivalent single-player
game, enabling the application of \citeauthor{bellman}'s principle of
optimality to solve the single-player game by breaking it down into
single-stage subgames. However, this approach entangles the decision variables
of all players at each single-stage subgame, resulting in backups with a
double-exponential complexity. This paper demonstrates how to disentangle these
decision variables while maintaining optimality under hierarchical information
sharing, a prominent management style in our society. To achieve this, we apply
the principle of optimality to solve any single-stage subgame by breaking it
down further into smaller subgames, enabling us to make single-player decisions
at a time. Our approach reveals that extensive-form games always exist with
solutions to a single-stage subgame, significantly reducing time complexity.
Our experimental results show that the algorithms leveraging these findings can
scale up to much larger multi-player games without compromising optimality.
</p>
</div>
</dd>
<dt><a name="item737">[737]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02956" title="Abstract">arXiv:2402.02956</a> [<a href="/pdf/2402.02956" title="Download PDF">pdf</a>, <a href="/format/2402.02956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaTreeFormer: Few Shot Domain Adaptation for Tree Counting from a  Single High-Resolution Image
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amirkolaee%2C+H+A">Hamed Amini Amirkolaee</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+M">Miaojing Shi</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Lianghua He</a>, 
<a href="/search/cs?searchtype=author&query=Mulligan%2C+M">Mark Mulligan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The process of estimating and counting tree density using only a single
aerial or satellite image is a difficult task in the fields of photogrammetry
and remote sensing. However, it plays a crucial role in the management of
forests. The huge variety of trees in varied topography severely hinders tree
counting models to perform well. The purpose of this paper is to propose a
framework that is learnt from the source domain with sufficient labeled trees
and is adapted to the target domain with only a limited number of labeled
trees. Our method, termed as AdaTreeFormer, contains one shared encoder with a
hierarchical feature extraction scheme to extract robust features from the
source and target domains. It also consists of three subnets: two for
extracting self-domain attention maps from source and target domains
respectively and one for extracting cross-domain attention maps. For the
latter, an attention-to-adapt mechanism is introduced to distill relevant
information from different domains while generating tree density maps; a
hierarchical cross-domain feature alignment scheme is proposed that
progressively aligns the features from the source and target domains. We also
adopt adversarial learning into the framework to further reduce the gap between
source and target domains. Our AdaTreeFormer is evaluated on six designed
domain adaptation tasks using three tree counting datasets, ie Jiangsu,
Yosemite, and London; and outperforms the state of the art methods
significantly.
</p>
</div>
</dd>
<dt><a name="item738">[738]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02957" title="Abstract">arXiv:2402.02957</a> [<a href="/pdf/2402.02957" title="Download PDF">pdf</a>, <a href="/format/2402.02957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Agent Reinforcement Learning for Offloading Cellular  Communications with Cooperating UAVs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mondal%2C+A">Abhishek Mondal</a>, 
<a href="/search/eess?searchtype=author&query=Mishra%2C+D">Deepak Mishra</a>, 
<a href="/search/eess?searchtype=author&query=Prasad%2C+G">Ganesh Prasad</a>, 
<a href="/search/eess?searchtype=author&query=Alexandropoulos%2C+G+C">George C. Alexandropoulos</a>, 
<a href="/search/eess?searchtype=author&query=Alnahari%2C+A">Azzam Alnahari</a>, 
<a href="/search/eess?searchtype=author&query=Jantti%2C+R">Riku Jantti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Effective solutions for intelligent data collection in terrestrial cellular
networks are crucial, especially in the context of Internet of Things
applications. The limited spectrum and coverage area of terrestrial base
stations pose challenges in meeting the escalating data rate demands of network
users. Unmanned aerial vehicles, known for their high agility, mobility, and
flexibility, present an alternative means to offload data traffic from
terrestrial BSs, serving as additional access points. This paper introduces a
novel approach to efficiently maximize the utilization of multiple UAVs for
data traffic offloading from terrestrial BSs. Specifically, the focus is on
maximizing user association with UAVs by jointly optimizing UAV trajectories
and users association indicators under quality of service constraints. Since,
the formulated UAVs control problem is nonconvex and combinatorial, this study
leverages the multi agent reinforcement learning framework. In this framework,
each UAV acts as an independent agent, aiming to maintain inter UAV cooperative
behavior. The proposed approach utilizes the finite state Markov decision
process to account for UAVs velocity constraints and the relationship between
their trajectories and state space. A low complexity distributed state action
reward state action algorithm is presented to determine UAVs optimal sequential
decision making policies over training episodes. The extensive simulation
results validate the proposed analysis and offer valuable insights into the
optimal UAV trajectories. The derived trajectories demonstrate superior average
UAV association performance compared to benchmark techniques such as Q learning
and particle swarm optimization.
</p>
</div>
</dd>
<dt><a name="item739">[739]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02961" title="Abstract">arXiv:2402.02961</a> [<a href="/pdf/2402.02961" title="Download PDF">pdf</a>, <a href="/format/2402.02961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GitBug-Java: A Reproducible Benchmark of Recent Java Bugs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Silva%2C+A">Andr&#xe9; Silva</a>, 
<a href="/search/cs?searchtype=author&query=Saavedra%2C+N">Nuno Saavedra</a>, 
<a href="/search/cs?searchtype=author&query=Monperrus%2C+M">Martin Monperrus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to MSR '24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Bug-fix benchmarks are essential for evaluating methodologies in automatic
program repair (APR) and fault localization (FL). However, existing benchmarks,
exemplified by Defects4J, need to evolve to incorporate recent bug-fixes
aligned with contemporary development practices. Moreover, reproducibility, a
key scientific principle, has been lacking in bug-fix benchmarks. To address
these gaps, we present GitBug-Java, a reproducible benchmark of recent Java
bugs. GitBug-Java features 199 bugs extracted from the 2023 commit history of
55 notable open-source repositories. The methodology for building GitBug-Java
ensures the preservation of bug-fixes in fully-reproducible environments. We
publish GitBug-Java at https://github.com/gitbugactions/gitbug-java.
</p>
</div>
</dd>
<dt><a name="item740">[740]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02964" title="Abstract">arXiv:2402.02964</a> [<a href="/pdf/2402.02964" title="Download PDF">pdf</a>, <a href="/format/2402.02964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixed Noise and Posterior Estimation with Conditional DeepGEM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hagemann%2C+P">Paul Hagemann</a>, 
<a href="/search/cs?searchtype=author&query=Hertrich%2C+J">Johannes Hertrich</a>, 
<a href="/search/cs?searchtype=author&query=Casfor%2C+M">Maren Casfor</a>, 
<a href="/search/cs?searchtype=author&query=Heidenreich%2C+S">Sebastian Heidenreich</a>, 
<a href="/search/cs?searchtype=author&query=Steidl%2C+G">Gabriele Steidl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Analysis, Statistics and Probability (physics.data-an)

</div>
<p class="mathjax">Motivated by indirect measurements and applications from nanometrology with a
mixed noise model, we develop a novel algorithm for jointly estimating the
posterior and the noise parameters in Bayesian inverse problems. We propose to
solve the problem by an expectation maximization (EM) algorithm. Based on the
current noise parameters, we learn in the E-step a conditional normalizing flow
that approximates the posterior. In the M-step, we propose to find the noise
parameter updates again by an EM algorithm, which has analytical formulas. We
compare the training of the conditional normalizing flow with the forward and
reverse KL, and show that our model is able to incorporate information from
many measurements, unlike previous approaches.
</p>
</div>
</dd>
<dt><a name="item741">[741]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02968" title="Abstract">arXiv:2402.02968</a> [<a href="/pdf/2402.02968" title="Download PDF">pdf</a>, <a href="/format/2402.02968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Delving into Multi-modal Multi-task Foundation Models for Road Scene  Understanding: From Learning Paradigm Perspectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Sheng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+W">Wanxin Tian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Rui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+L">Luanxuan Hou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiubao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Haifeng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Ruiqi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+S">Shuyi Geng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+L">Ling Shao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+B">Bojun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qun Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+G">Guobin Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 9 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Foundation models have indeed made a profound impact on various fields,
emerging as pivotal components that significantly shape the capabilities of
intelligent systems. In the context of intelligent vehicles, leveraging the
power of foundation models has proven to be transformative, offering notable
advancements in visual understanding. Equipped with multi-modal and multi-task
learning capabilities, multi-modal multi-task visual understanding foundation
models (MM-VUFMs) effectively process and fuse data from diverse modalities and
simultaneously handle various driving-related tasks with powerful adaptability,
contributing to a more holistic understanding of the surrounding scene. In this
survey, we present a systematic analysis of MM-VUFMs specifically designed for
road scenes. Our objective is not only to provide a comprehensive overview of
common practices, referring to task-specific models, unified multi-modal
models, unified multi-task models, and foundation model prompting techniques,
but also to highlight their advanced capabilities in diverse learning
paradigms. These paradigms include open-world understanding, efficient transfer
for road scenes, continual learning, interactive and generative capability.
Moreover, we provide insights into key challenges and future trends, such as
closed-loop driving systems, interpretability, embodied driving agents, and
world models. To facilitate researchers in staying abreast of the latest
developments in MM-VUFMs for road scenes, we have established a continuously
updated repository at https://github.com/rolsheng/MM-VUFM4DS
</p>
</div>
</dd>
<dt><a name="item742">[742]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02972" title="Abstract">arXiv:2402.02972</a> [<a href="/pdf/2402.02972" title="Download PDF">pdf</a>, <a href="/format/2402.02972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieval-Augmented Score Distillation for Text-to-3D Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seo%2C+J">Junyoung Seo</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+S">Susung Hong</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+W">Wooseok Jang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+I+H">In&#xe8;s Hyeonsu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kwak%2C+M">Minseop Kwak</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Doyup Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seungryong Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://ku-cvlab.github.io/RetDream/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Text-to-3D generation has achieved significant success by incorporating
powerful 2D diffusion models, but insufficient 3D prior knowledge also leads to
the inconsistency of 3D geometry. Recently, since large-scale multi-view
datasets have been released, fine-tuning the diffusion model on the multi-view
datasets becomes a mainstream to solve the 3D inconsistency problem. However,
it has confronted with fundamental difficulties regarding the limited quality
and diversity of 3D data, compared with 2D data. To sidestep these trade-offs,
we explore a retrieval-augmented approach tailored for score distillation,
dubbed RetDream. We postulate that both expressiveness of 2D diffusion models
and geometric consistency of 3D assets can be fully leveraged by employing the
semantically relevant assets directly within the optimization process. To this
end, we introduce novel framework for retrieval-based quality enhancement in
text-to-3D generation. We leverage the retrieved asset to incorporate its
geometric prior in the variational objective and adapt the diffusion model's 2D
prior toward view consistency, achieving drastic improvements in both geometry
and fidelity of generated scenes. We conduct extensive experiments to
demonstrate that RetDream exhibits superior quality with increased geometric
consistency. Project page is available at https://ku-cvlab.github.io/RetDream/.
</p>
</div>
</dd>
<dt><a name="item743">[743]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02973" title="Abstract">arXiv:2402.02973</a> [<a href="/pdf/2402.02973" title="Download PDF">pdf</a>, <a href="/format/2402.02973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are We There Yet? Unraveling the State-of-the-Art Smart Contract Fuzzers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shuohan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zihao Li</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+L">Luyi Yan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weimin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Muhui Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenxu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xiapu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hao Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICSE 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Given the growing importance of smart contracts in various applications,
ensuring their security and reliability is critical. Fuzzing, an effective
vulnerability detection technique, has recently been widely applied to smart
contracts. Despite numerous studies, a systematic investigation of smart
contract fuzzing techniques remains lacking. In this paper, we fill this gap
by: 1) providing a comprehensive review of current research in contract
fuzzing, and 2) conducting an in-depth empirical study to evaluate
state-of-the-art contract fuzzers' usability. To guarantee a fair evaluation,
we employ a carefully-labeled benchmark and introduce a set of pragmatic
performance metrics, evaluating fuzzers from five complementary perspectives.
Based on our findings, we provide direction for the future research and
development of contract fuzzers.
</p>
</div>
</dd>
<dt><a name="item744">[744]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02975" title="Abstract">arXiv:2402.02975</a> [<a href="/pdf/2402.02975" title="Download PDF">pdf</a>, <a href="/format/2402.02975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Putting Context in Context: the Impact of Discussion Structure on Text  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Penzo%2C+N">Nicol&#xf2; Penzo</a>, 
<a href="/search/cs?searchtype=author&query=Longa%2C+A">Antonio Longa</a>, 
<a href="/search/cs?searchtype=author&query=Lepri%2C+B">Bruno Lepri</a>, 
<a href="/search/cs?searchtype=author&query=Tonelli%2C+S">Sara Tonelli</a>, 
<a href="/search/cs?searchtype=author&query=Guerini%2C+M">Marco Guerini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024 main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Current text classification approaches usually focus on the content to be
classified. Contextual aspects (both linguistic and extra-linguistic) are
usually neglected, even in tasks based on online discussions. Still in many
cases the multi-party and multi-turn nature of the context from which these
elements are selected can be fruitfully exploited. In this work, we propose a
series of experiments on a large dataset for stance detection in English, in
which we evaluate the contribution of different types of contextual
information, i.e. linguistic, structural and temporal, by feeding them as
natural language input into a transformer-based model. We also experiment with
different amounts of training data and analyse the topology of local discussion
networks in a privacy-compliant way. Results show that structural information
can be highly beneficial to text classification but only under certain
circumstances (e.g. depending on the amount of training data and on discussion
chain complexity). Indeed, we show that contextual information on smaller
datasets from other classification tasks does not yield significant
improvements. Our framework, based on local discussion networks, allows the
integration of structural information, while minimising user profiling, thus
preserving their privacy.
</p>
</div>
</dd>
<dt><a name="item745">[745]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02976" title="Abstract">arXiv:2402.02976</a> [<a href="/pdf/2402.02976" title="Download PDF">pdf</a>, <a href="/ps/2402.02976" title="Download PostScript">ps</a>, <a href="/format/2402.02976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting, Voting Classifiers and Randomized Sample Compression Schemes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=da+Cunha%2C+A">Arthur da Cunha</a>, 
<a href="/search/cs?searchtype=author&query=Larsen%2C+K+G">Kasper Green Larsen</a>, 
<a href="/search/cs?searchtype=author&query=Ritzert%2C+M">Martin Ritzert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In boosting, we aim to leverage multiple weak learners to produce a strong
learner. At the center of this paradigm lies the concept of building the strong
learner as a voting classifier, which outputs a weighted majority vote of the
weak learners. While many successful boosting algorithms, such as the iconic
AdaBoost, produce voting classifiers, their theoretical performance has long
remained sub-optimal: the best known bounds on the number of training examples
necessary for a voting classifier to obtain a given accuracy has so far always
contained at least two logarithmic factors above what is known to be achievable
by general weak-to-strong learners. In this work, we break this barrier by
proposing a randomized boosting algorithm that outputs voting classifiers whose
generalization error contains a single logarithmic dependency on the sample
size. We obtain this result by building a general framework that extends sample
compression methods to support randomized learning algorithms based on
sub-sampling.
</p>
</div>
</dd>
<dt><a name="item746">[746]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02977" title="Abstract">arXiv:2402.02977</a> [<a href="/pdf/2402.02977" title="Download PDF">pdf</a>, <a href="/format/2402.02977" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Flow Models: Flowing in Your Style
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Do%2C+K">Kien Do</a>, 
<a href="/search/cs?searchtype=author&query=Kieu%2C+D">Duc Kieu</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Toan Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D">Dang Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+H">Hung Le</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D">Dung Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Thin Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce a variational inference interpretation for models of "posterior
flows" - generalizations of "probability flows" to a broader class of
stochastic processes not necessarily diffusion processes. We coin the resulting
models as "Variational Flow Models". Additionally, we propose a systematic
training-free method to transform the posterior flow of a "linear" stochastic
process characterized by the equation Xt = at * X0 + st * X1 into a straight
constant-speed (SC) flow, reminiscent of Rectified Flow. This transformation
facilitates fast sampling along the original posterior flow without training a
new model of the SC flow. The flexibility of our approach allows us to extend
our transformation to inter-convert two posterior flows from distinct "linear"
stochastic processes. Moreover, we can easily integrate high-order numerical
solvers into the transformed SC flow, further enhancing sampling accuracy and
efficiency. Rigorous theoretical analysis and extensive experimental results
substantiate the advantages of our framework.
</p>
</div>
</dd>
<dt><a name="item747">[747]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02978" title="Abstract">arXiv:2402.02978</a> [<a href="/pdf/2402.02978" title="Download PDF">pdf</a>, <a href="/format/2402.02978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Datalog Tools for Meta-reasoning over OWL 2 QL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qureshi%2C+H+M">Haya Majid Qureshi</a>, 
<a href="/search/cs?searchtype=author&query=Faber%2C+W">Wolfgang Faber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under consideration in Theory and Practice of Logic Programming (TPLP)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Metamodeling is a general approach to expressing knowledge about classes and
properties in an ontology. It is a desirable modeling feature in multiple
applications that simplifies the extension and reuse of ontologies.
Nevertheless, allowing metamodeling without restrictions is problematic for
several reasons, mainly due to undecidability issues. Practical languages,
therefore, forbid classes to occur as instances of other classes or treat such
occurrences as semantically different objects. Specifically, meta-querying in
SPARQL under the Direct Semantic Entailment Regime (DSER) uses the latter
approach, thereby effectively not supporting meta-queries. However, several
extensions enabling different metamodeling features have been proposed over the
last decade. This paper deals with the Metamodeling Semantics (MS) over OWL 2
QL and the Metamodeling Semantic Entailment Regime (MSER), as proposed in
Lenzerini et al. (2015) and Lenzerini et al. (2020); Cima et al. (2017). A
reduction from OWL 2 QL to Datalog for meta-querying was proposed in Cima et
al. (2017). In this paper, we experiment with various logic programming tools
that support Datalog querying to determine their suitability as back-ends to
MSER query answering. These tools stem from different logic programming
paradigms (Prolog, pure Datalog, Answer Set Programming, Hybrid Knowledge
Bases). Our work shows that the Datalog approach to MSER querying is practical
also for sizeable ontologies with limited resources (time and memory). This
paper significantly extends Qureshi &amp; Faber (2021) by a more detailed
experimental analysis and more background. Under consideration in Theory and
Practice of Logic Programming (TPLP).
</p>
</div>
</dd>
<dt><a name="item748">[748]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02979" title="Abstract">arXiv:2402.02979</a> [<a href="/pdf/2402.02979" title="Download PDF">pdf</a>, <a href="/format/2402.02979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fully Generalized Reactivity(1) Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ehlers%2C+R">R&#xfc;diger Ehlers</a>, 
<a href="/search/cs?searchtype=author&query=Khalimov%2C+A">Ayrat Khalimov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is an extended version of the paper accepted at TACAS'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
<p class="mathjax">Generalized Reactivity(1) (GR(1)) synthesis is a reactive synthesis approach
in which the specification is split into two parts: a symbolic game graph,
describing the safe transitions of a system, a liveness specification in a
subset of Linear Temporal Logic (LTL) on top of it. Many specifications can
naturally be written in this restricted form, and the restriction gives rise to
a scalable synthesis procedure -- the reasons for the high popularity of the
approach. For specifications even slightly beyond GR(1), however, the approach
is inapplicable. This necessitates a transition to synthesizers for full LTL
specifications, introducing a huge efficiency drop. This paper proposes a
synthesis approach that smoothly bridges the efficiency gap from GR(1) to LTL
by unifying synthesis for both classes of specifications. The approach
leverages a recently introduced canonical representation of omega-regular
languages based on a chain of good-for-games co-B\"uchi automata (COCOA). By
constructing COCOA for the liveness part of a specification, we can then build
a fixpoint formula that can be efficiently evaluated on the symbolic game
graph. The COCOA-based synthesis approach outperforms standard approaches and
retains the efficiency of GR(1) synthesis for specifications in GR(1) form and
those with few non-GR(1) specification parts.
</p>
</div>
</dd>
<dt><a name="item749">[749]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02980" title="Abstract">arXiv:2402.02980</a> [<a href="/pdf/2402.02980" title="Download PDF">pdf</a>, <a href="/format/2402.02980" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Review on Fault Diagnosis and Fault-Tolerant Control Scheme for Robotic  Manipulators: Recent Advances in AI, Machine Learning, and Digital Twin
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Quamar%2C+M+M">Md Muzakkir Quamar</a>, 
<a href="/search/cs?searchtype=author&query=Nasir%2C+A">Ali Nasir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">This comprehensive review article delves into the intricate realm of
fault-tolerant control (FTC) schemes tailored for robotic manipulators. Our
exploration spans the historical evolution of FTC, tracing its development over
time, and meticulously examines the recent breakthroughs fueled by the
synergistic integration of cutting-edge technologies such as artificial
intelligence (AI), machine learning (ML), and digital twin technologies (DTT).
The article places a particular emphasis on the transformative influence these
contemporary trends exert on the landscape of robotic manipulator control and
fault tolerance.
<br />By delving into the historical context, our aim is to provide a comprehensive
understanding of the evolution of FTC schemes. This journey encompasses the
transition from model-based and signal-based schemes to the role of sensors,
setting the stage for an exploration of the present-day paradigm shift enabled
by AI, ML, and DTT. The narrative unfolds as we dissect the intricate interplay
between these advanced technologies and their applications in enhancing fault
tolerance within the domain of robotic manipulators. Our review critically
evaluates the impact of these advancements, shedding light on the novel
methodologies, techniques, and applications that have emerged in recent times.
<br />The overarching goal of this article is to present a comprehensive
perspective on the current state of fault diagnosis and fault-tolerant control
within the context of robotic manipulators, positioning our exploration within
the broader framework of AI, ML, and DTT advancements. Through a meticulous
examination of both historical foundations and contemporary innovations, this
review significantly contributes to the existing body of knowledge, offering
valuable insights for researchers, practitioners, and enthusiasts navigating
the dynamic landscape of robotic manipulator control.
</p>
</div>
</dd>
<dt><a name="item750">[750]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02982" title="Abstract">arXiv:2402.02982</a> [<a href="/pdf/2402.02982" title="Download PDF">pdf</a>, <a href="/format/2402.02982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algorithms for Computing the Free Distance of Convolutional Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abreu%2C+Z">Zita Abreu</a>, 
<a href="/search/cs?searchtype=author&query=Rosenthal%2C+J">Joachim Rosenthal</a>, 
<a href="/search/cs?searchtype=author&query=Schaller%2C+M">Michael Schaller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">The free distance of a convolutional code is a reliable indicator of its
performance. However its computation is not an easy task. In this paper, we
present some algorithms to compute the free distance with good efficiency that
work for convolutional codes of all rates and over any field. Furthermore we
discuss why an algorithm which is claimed to be very efficient is incorrect.
</p>
</div>
</dd>
<dt><a name="item751">[751]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02983" title="Abstract">arXiv:2402.02983</a> [<a href="/pdf/2402.02983" title="Download PDF">pdf</a>, <a href="/ps/2402.02983" title="Download PostScript">ps</a>, <a href="/format/2402.02983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An intrinsical description of group codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bernal%2C+J+J">Jos&#xe9; Joaqu&#xed;n Bernal</a>, 
<a href="/search/cs?searchtype=author&query=del+R%C3%ADo%2C+%C3%81">&#xc1;ngel del R&#xed;o</a>, 
<a href="/search/cs?searchtype=author&query=Sim%C3%B3n%2C+J+J">Juan Jacobo Sim&#xf3;n</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">A (left) group code of length n is a linear code which is the image of a
(left) ideal of a group algebra via an isomorphism from FG to Fn which maps G
to the standard basis of Fn. Many classical linear codes have been shown to be
group codes. In this paper we obtain a criterion to decide when a linear code
is a group code in terms of its intrinsical properties in the ambient space Fn,
which does not assume an a priori group algebra structure on Fn. As an
application we provide a family of groups (including metacyclic groups) for
which every two-sided group code is an abelian group code. It is well known
that Reed-Solomon codes are cyclic and its parity check extensions are
elementary abelian group codes. These two classes of codes are included in the
class of Cauchy codes. Using our criterion we classify the Cauchy codes of some
lengths which are left group codes and the possible group code structures on
these codes.
</p>
</div>
</dd>
<dt><a name="item752">[752]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02985" title="Abstract">arXiv:2402.02985</a> [<a href="/pdf/2402.02985" title="Download PDF">pdf</a>, <a href="/format/2402.02985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised semantic segmentation of high-resolution UAV imagery for  road scene parsing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zihan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongshang Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+R">Ronggui Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+C">Chen Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Two challenges are presented when parsing road scenes in UAV images. First,
the high resolution of UAV images makes processing difficult. Second,
supervised deep learning methods require a large amount of manual annotations
to train robust and accurate models. In this paper, an unsupervised road
parsing framework that leverages recent advances in vision language models and
fundamental computer vision model is introduced.Initially, a vision language
model is employed to efficiently process ultra-large resolution UAV images to
quickly detect road regions of interest in the images. Subsequently, the vision
foundation model SAM is utilized to generate masks for the road regions without
category information. Following that, a self-supervised representation learning
network extracts feature representations from all masked regions. Finally, an
unsupervised clustering algorithm is applied to cluster these feature
representations and assign IDs to each cluster. The masked regions are combined
with the corresponding IDs to generate initial pseudo-labels, which initiate an
iterative self-training process for regular semantic segmentation. The proposed
method achieves an impressive 89.96% mIoU on the development dataset without
relying on any manual annotation. Particularly noteworthy is the extraordinary
flexibility of the proposed method, which even goes beyond the limitations of
human-defined categories and is able to acquire knowledge of new categories
from the dataset itself.
</p>
</div>
</dd>
<dt><a name="item753">[753]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02986" title="Abstract">arXiv:2402.02986</a> [<a href="/pdf/2402.02986" title="Download PDF">pdf</a>, <a href="/format/2402.02986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Safety-Adapted Loss for Pedestrian Detection in Automated Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyssenko%2C+M">Maria Lyssenko</a>, 
<a href="/search/cs?searchtype=author&query=Pimplikar%2C+P">Piyush Pimplikar</a>, 
<a href="/search/cs?searchtype=author&query=Bieshaar%2C+M">Maarten Bieshaar</a>, 
<a href="/search/cs?searchtype=author&query=Nozarian%2C+F">Farzad Nozarian</a>, 
<a href="/search/cs?searchtype=author&query=Triebel%2C+R">Rudolph Triebel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In safety-critical domains like automated driving (AD), errors by the object
detector may endanger pedestrians and other vulnerable road users (VRU). As
common evaluation metrics are not an adequate safety indicator, recent works
employ approaches to identify safety-critical VRU and back-annotate the risk to
the object detector. However, those approaches do not consider the safety
factor in the deep neural network (DNN) training process. Thus,
state-of-the-art DNN penalizes all misdetections equally irrespective of their
criticality. Subsequently, to mitigate the occurrence of critical failure
cases, i.e., false negatives, a safety-aware training strategy might be
required to enhance the detection performance for critical pedestrians. In this
paper, we propose a novel safety-aware loss variation that leverages the
estimated per-pedestrian criticality scores during training. We exploit the
reachability set-based time-to-collision (TTC-RSB) metric from the motion
domain along with distance information to account for the worst-case threat
quantifying the criticality. Our evaluation results using RetinaNet and FCOS on
the nuScenes dataset demonstrate that training the models with our safety-aware
loss function mitigates the misdetection of critical pedestrians without
sacrificing performance for the general case, i.e., pedestrians outside the
safety-critical zone.
</p>
</div>
</dd>
<dt><a name="item754">[754]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02987" title="Abstract">arXiv:2402.02987</a> [<a href="/pdf/2402.02987" title="Download PDF">pdf</a>, <a href="/format/2402.02987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conversation Reconstruction Attack Against GPT Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+J">Junjie Chu</a>, 
<a href="/search/cs?searchtype=author&query=Sha%2C+Z">Zeyang Sha</a>, 
<a href="/search/cs?searchtype=author&query=Backes%2C+M">Michael Backes</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">In recent times, significant advancements have been made in the field of
large language models (LLMs), represented by GPT series models. To optimize
task execution, users often engage in multi-round conversations with GPT models
hosted in cloud environments. These multi-round conversations, potentially
replete with private information, require transmission and storage within the
cloud. However, this operational paradigm introduces additional attack
surfaces. In this paper, we first introduce a specific Conversation
Reconstruction Attack targeting GPT models. Our introduced Conversation
Reconstruction Attack is composed of two steps: hijacking a session and
reconstructing the conversations. Subsequently, we offer an exhaustive
evaluation of the privacy risks inherent in conversations when GPT models are
subjected to the proposed attack. However, GPT-4 demonstrates certain
robustness to the proposed attacks. We then introduce two advanced attacks
aimed at better reconstructing previous conversations, specifically the UNR
attack and the PBU attack. Our experimental findings indicate that the PBU
attack yields substantial performance across all models, achieving semantic
similarity scores exceeding 0.60, while the UNR attack is effective solely on
GPT-3.5. Our results reveal the concern about privacy risks associated with
conversations involving GPT models and aim to draw the community's attention to
prevent the potential misuse of these models' remarkable capabilities. We will
responsibly disclose our findings to the suppliers of related large language
models.
</p>
</div>
</dd>
<dt><a name="item755">[755]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02989" title="Abstract">arXiv:2402.02989</a> [<a href="/pdf/2402.02989" title="Download PDF">pdf</a>, <a href="/format/2402.02989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DexDiffuser: Generating Dexterous Grasps with Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weng%2C+Z">Zehang Weng</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Haofei Lu</a>, 
<a href="/search/cs?searchtype=author&query=Kragic%2C+D">Danica Kragic</a>, 
<a href="/search/cs?searchtype=author&query=Lundell%2C+J">Jens Lundell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce DexDiffuser, a novel dexterous grasping method that generates,
evaluates, and refines grasps on partial object point clouds. DexDiffuser
includes the conditional diffusion-based grasp sampler DexSampler and the
dexterous grasp evaluator DexEvaluator. DexSampler generates high-quality
grasps conditioned on object point clouds by iterative denoising of randomly
sampled grasps. We also introduce two grasp refinement strategies:
Evaluator-Guided Diffusion (EGD) and Evaluator-based Sampling Refinement (ESR).
Our simulation and real-world experiments on the Allegro Hand consistently
demonstrate that DexDiffuser outperforms the state-of-the-art multi-finger
grasp generation method FFHNet with an, on average, 21.71--22.20\% higher grasp
success rate.
</p>
</div>
</dd>
<dt><a name="item756">[756]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02992" title="Abstract">arXiv:2402.02992</a> [<a href="/pdf/2402.02992" title="Download PDF">pdf</a>, <a href="/format/2402.02992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding-time Realignment of Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianlin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Shangmin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Bianco%2C+L">Leonardo Bianco</a>, 
<a href="/search/cs?searchtype=author&query=Calandriello%2C+D">Daniele Calandriello</a>, 
<a href="/search/cs?searchtype=author&query=Berthet%2C+Q">Quentin Berthet</a>, 
<a href="/search/cs?searchtype=author&query=Llinares%2C+F">Felipe Llinares</a>, 
<a href="/search/cs?searchtype=author&query=Hoffmann%2C+J">Jessica Hoffmann</a>, 
<a href="/search/cs?searchtype=author&query=Dixon%2C+L">Lucas Dixon</a>, 
<a href="/search/cs?searchtype=author&query=Valko%2C+M">Michal Valko</a>, 
<a href="/search/cs?searchtype=author&query=Blondel%2C+M">Mathieu Blondel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Aligning language models with human preferences is crucial for reducing
errors and biases in these models. Alignment techniques, such as reinforcement
learning from human feedback (RLHF), are typically cast as optimizing a
tradeoff between human preference rewards and a proximity regularization term
that encourages staying close to the unaligned model. Selecting an appropriate
level of regularization is critical: insufficient regularization can lead to
reduced model capabilities due to reward hacking, whereas excessive
regularization hinders alignment. Traditional methods for finding the optimal
regularization level require retraining multiple models with varying
regularization strengths. This process, however, is resource-intensive,
especially for large models. To address this challenge, we propose
decoding-time realignment (DeRa), a simple method to explore and evaluate
different regularization strengths in aligned models without retraining. DeRa
enables control over the degree of alignment, allowing users to smoothly
transition between unaligned and aligned models. It also enhances the
efficiency of hyperparameter tuning by enabling the identification of effective
regularization strengths using a validation dataset.
</p>
</div>
</dd>
<dt><a name="item757">[757]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02996" title="Abstract">arXiv:2402.02996</a> [<a href="/pdf/2402.02996" title="Download PDF">pdf</a>, <a href="/format/2402.02996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text-Guided Image Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stephan%2C+A">Andreas Stephan</a>, 
<a href="/search/cs?searchtype=author&query=Miklautz%2C+L">Lukas Miklautz</a>, 
<a href="/search/cs?searchtype=author&query=Sidak%2C+K">Kevin Sidak</a>, 
<a href="/search/cs?searchtype=author&query=Wahle%2C+J+P">Jan Philip Wahle</a>, 
<a href="/search/cs?searchtype=author&query=Gipp%2C+B">Bela Gipp</a>, 
<a href="/search/cs?searchtype=author&query=Plant%2C+C">Claudia Plant</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+B">Benjamin Roth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Image clustering divides a collection of images into meaningful groups,
typically interpreted post-hoc via human-given annotations. Those are usually
in the form of text, begging the question of using text as an abstraction for
image clustering. Current image clustering methods, however, neglect the use of
generated textual descriptions. We, therefore, propose Text-Guided Image
Clustering, i.e., generating text using image captioning and visual
question-answering (VQA) models and subsequently clustering the generated text.
Further, we introduce a novel approach to inject task- or domain knowledge for
clustering by prompting VQA models. Across eight diverse image clustering
datasets, our results show that the obtained text representations often
outperform image features. Additionally, we propose a counting-based cluster
explainability method. Our evaluations show that the derived keyword-based
explanations describe clusters better than the respective cluster accuracy
suggests. Overall, this research challenges traditional approaches and paves
the way for a paradigm shift in image clustering, using generated text.
</p>
</div>
</dd>
<dt><a name="item758">[758]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02998" title="Abstract">arXiv:2402.02998</a> [<a href="/pdf/2402.02998" title="Download PDF">pdf</a>, <a href="/format/2402.02998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Careful with that Scalpel: Improving Gradient Surgery with an EMA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+Y">Yu-Guan Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Thornton%2C+J">James Thornton</a>, 
<a href="/search/cs?searchtype=author&query=Ndiaye%2C+E">Eugene Ndiaye</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+M">Michal Klein</a>, 
<a href="/search/cs?searchtype=author&query=Cuturi%2C+M">Marco Cuturi</a>, 
<a href="/search/cs?searchtype=author&query=Ablin%2C+P">Pierre Ablin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Beyond minimizing a single training loss, many deep learning estimation
pipelines rely on an auxiliary objective to quantify and encourage desirable
properties of the model (e.g. performance on another dataset, robustness,
agreement with a prior). Although the simplest approach to incorporating an
auxiliary loss is to sum it with the training loss as a regularizer, recent
works have shown that one can improve performance by blending the gradients
beyond a simple sum; this is known as gradient surgery. We cast the problem as
a constrained minimization problem where the auxiliary objective is minimized
among the set of minimizers of the training loss. To solve this bilevel
problem, we follow a parameter update direction that combines the training loss
gradient and the orthogonal projection of the auxiliary gradient to the
training gradient. In a setting where gradients come from mini-batches, we
explain how, using a moving average of the training loss gradients, we can
carefully maintain this critical orthogonality property. We demonstrate that
our method, Bloop, can lead to much better performances on NLP and vision
experiments than other gradient surgery methods without EMA.
</p>
</div>
</dd>
<dt><a name="item759">[759]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02999" title="Abstract">arXiv:2402.02999</a> [<a href="/pdf/2402.02999" title="Download PDF">pdf</a>, <a href="/format/2402.02999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Teach Me How to ImproVISe: Co-Designing an Augmented Piano Training  System for Improvisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deja%2C+J+A">Jordan Aiko Deja</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0tor%2C+S">Sandi &#x160;tor</a>, 
<a href="/search/cs?searchtype=author&query=Pucihar%2C+I">Ilonka Pucihar</a>, 
<a href="/search/cs?searchtype=author&query=Pucihar%2C+K+%C4%8C">Klen &#x10c;opi&#x10d; Pucihar</a>, 
<a href="/search/cs?searchtype=author&query=Kljun%2C+M">Matja&#x17e; Kljun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures, 1 table, 15 references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Improvisation is a vital but often neglected aspect of traditional piano
teaching. Challenges such as difficulty in assessment and subjectivity have
hindered its effective instruction. Technological approaches, including
augmentation, aim to enhance piano instruction, but the specific application of
digital augmentation for piano improvisation is under-explored. This paper
outlines a co-design process developing an Augmented Reality (AR) Piano
Improvisation Training System, ImproVISe, involving improvisation teachers. The
prototype, featuring basic improvisation concepts, was created and refined
through expert interaction. Their insights guided the identification of
objectives, tools, interaction metaphors, and software features. The findings
offer design guidelines and recommendations to address challenges in assessing
piano improvisation in a learning context.
</p>
</div>
</dd>
<dt><a name="item760">[760]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03003" title="Abstract">arXiv:2402.03003</a> [<a href="/pdf/2402.03003" title="Download PDF">pdf</a>, <a href="/format/2402.03003" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> [Citation needed] Data usage and citation practices in medical imaging  conferences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sourget%2C+T">Th&#xe9;o Sourget</a>, 
<a href="/search/cs?searchtype=author&query=Akko%C3%A7%2C+A">Ahmet Akko&#xe7;</a>, 
<a href="/search/cs?searchtype=author&query=Winther%2C+S">Stinna Winther</a>, 
<a href="/search/cs?searchtype=author&query=Galsgaard%2C+C+L">Christine Lyngbye Galsgaard</a>, 
<a href="/search/cs?searchtype=author&query=Jim%C3%A9nez-S%C3%A1nchez%2C+A">Amelia Jim&#xe9;nez-S&#xe1;nchez</a>, 
<a href="/search/cs?searchtype=author&query=Juodelyte%2C+D">Dovile Juodelyte</a>, 
<a href="/search/cs?searchtype=author&query=Petitjean%2C+C">Caroline Petitjean</a>, 
<a href="/search/cs?searchtype=author&query=Cheplygina%2C+V">Veronika Cheplygina</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to MIDL conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Digital Libraries (cs.DL)

</div>
<p class="mathjax">Medical imaging papers often focus on methodology, but the quality of the
algorithms and the validity of the conclusions are highly dependent on the
datasets used. As creating datasets requires a lot of effort, researchers often
use publicly available datasets, there is however no adopted standard for
citing the datasets used in scientific papers, leading to difficulty in
tracking dataset usage. In this work, we present two open-source tools we
created that could help with the detection of dataset usage, a pipeline
\url{https://github.com/TheoSourget/Public_Medical_Datasets_References} using
OpenAlex and full-text analysis, and a PDF annotation software
\url{https://github.com/TheoSourget/pdf_annotator} used in our study to
manually label the presence of datasets. We applied both tools on a study of
the usage of 20 publicly available medical datasets in papers from MICCAI and
MIDL. We compute the proportion and the evolution between 2013 and 2023 of 3
types of presence in a paper: cited, mentioned in the full text, cited and
mentioned. Our findings demonstrate the concentration of the usage of a limited
set of datasets. We also highlight different citing practices, making the
automation of tracking difficult.
</p>
</div>
</dd>
<dt><a name="item761">[761]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03006" title="Abstract">arXiv:2402.03006</a> [<a href="/pdf/2402.03006" title="Download PDF">pdf</a>, <a href="/ps/2402.03006" title="Download PostScript">ps</a>, <a href="/format/2402.03006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the development of a practical Bayesian optimisation algorithm for  expensive experiments and simulations with changing environmental conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Diessner%2C+M">Mike Diessner</a>, 
<a href="/search/cs?searchtype=author&query=Wilson%2C+K+J">Kevin J. Wilson</a>, 
<a href="/search/cs?searchtype=author&query=Whalley%2C+R+D">Richard D. Whalley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Experiments in engineering are typically conducted in controlled environments
where parameters can be set to any desired value. This assumes that the same
applies in a real-world setting -- an assumption that is often incorrect as
many experiments are influenced by uncontrollable environmental conditions such
as temperature, humidity and wind speed. When optimising such experiments, the
focus should lie on finding optimal values conditionally on these
uncontrollable variables. This article extends Bayesian optimisation to the
optimisation of systems in changing environments that include controllable and
uncontrollable parameters. The extension fits a global surrogate model over all
controllable and environmental variables but optimises only the controllable
parameters conditional on measurements of the uncontrollable variables. The
method is validated on two synthetic test functions and the effects of the
noise level, the number of the environmental parameters, the parameter
fluctuation, the variability of the uncontrollable parameters, and the
effective domain size are investigated. ENVBO, the proposed algorithm resulting
from this investigation, is applied to a wind farm simulator with eight
controllable and one environmental parameter. ENVBO finds solutions for the
full domain of the environmental variable that outperforms results from
optimisation algorithms that only focus on a fixed environmental value in all
but one case while using a fraction of their evaluation budget. This makes the
proposed approach very sample-efficient and cost-effective. An off-the-shelf
open-source version of ENVBO is available via the NUBO Python package.
</p>
</div>
</dd>
<dt><a name="item762">[762]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03009" title="Abstract">arXiv:2402.03009</a> [<a href="/pdf/2402.03009" title="Download PDF">pdf</a>, <a href="/format/2402.03009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniMem: Towards a Unified View of Long-Context Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+J">Junjie Fang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+L">Likai Tang</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+H">Hongzhe Bi</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yujia Qin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Si Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haolun Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongjian Li</a>, 
<a href="/search/cs?searchtype=author&query=Cong%2C+X">Xin Cong</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yukun Yan</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xiaodong Shi</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Sen Song</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yankai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Long-context processing is a critical ability that constrains the
applicability of large language models. Although there exist various methods
devoted to enhancing the long-context processing ability of large language
models (LLMs), they are developed in an isolated manner and lack systematic
analysis and integration of their strengths, hindering further developments. In
this paper, we introduce UniMem, a unified framework that reformulates existing
long-context methods from the view of memory augmentation of LLMs. UniMem is
characterized by four key dimensions: Memory Management, Memory Writing, Memory
Reading, and Memory Injection, providing a systematic theory for understanding
various long-context methods. We reformulate 16 existing methods based on
UniMem and analyze four representative methods: Transformer-XL, Memorizing
Transformer, RMT, and Longformer into equivalent UniMem forms to reveal their
design principles and strengths. Based on these analyses, we propose UniMix, an
innovative approach that integrates the strengths of these algorithms.
Experimental results show that UniMix achieves superior performance in handling
long contexts with significantly lower perplexity than baselines.
</p>
</div>
</dd>
<dt><a name="item763">[763]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03011" title="Abstract">arXiv:2402.03011</a> [<a href="/pdf/2402.03011" title="Download PDF">pdf</a>, <a href="/format/2402.03011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Impact of Output Perturbation on Fairness in Binary Linear  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Emelianov%2C+V">Vitalii Emelianov</a>, 
<a href="/search/cs?searchtype=author&query=Perrot%2C+M">Micha&#xeb;l Perrot</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We theoretically study how differential privacy interacts with both
individual and group fairness in binary linear classification. More precisely,
we focus on the output perturbation mechanism, a classic approach in
privacy-preserving machine learning. We derive high-probability bounds on the
level of individual and group fairness that the perturbed models can achieve
compared to the original model. Hence, for individual fairness, we prove that
the impact of output perturbation on the level of fairness is bounded but grows
with the dimension of the model. For group fairness, we show that this impact
is determined by the distribution of so-called angular margins, that is signed
margins of the non-private model re-scaled by the norm of each example.
</p>
</div>
</dd>
<dt><a name="item764">[764]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03014" title="Abstract">arXiv:2402.03014</a> [<a href="/pdf/2402.03014" title="Download PDF">pdf</a>, <a href="/format/2402.03014" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Whom to Trust? Elective Learning for Distributed Gaussian Process  Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zewen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xiaobing Dai</a>, 
<a href="/search/cs?searchtype=author&query=Dubey%2C+A">Akshat Dubey</a>, 
<a href="/search/cs?searchtype=author&query=Hirche%2C+S">Sandra Hirche</a>, 
<a href="/search/cs?searchtype=author&query=Hattab%2C+G">Georges Hattab</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, conference preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper introduces an innovative approach to enhance distributed
cooperative learning using Gaussian process (GP) regression in multi-agent
systems (MASs). The key contribution of this work is the development of an
elective learning algorithm, namely prior-aware elective distributed GP
(Pri-GP), which empowers agents with the capability to selectively request
predictions from neighboring agents based on their trustworthiness. The
proposed Pri-GP effectively improves individual prediction accuracy, especially
in cases where the prior knowledge of an agent is incorrect. Moreover, it
eliminates the need for computationally intensive variance calculations for
determining aggregation weights in distributed GP. Furthermore, we establish a
prediction error bound within the Pri-GP framework, ensuring the reliability of
predictions, which is regarded as a crucial property in safety-critical MAS
applications.
</p>
</div>
</dd>
<dt><a name="item765">[765]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03017" title="Abstract">arXiv:2402.03017</a> [<a href="/pdf/2402.03017" title="Download PDF">pdf</a>, <a href="/format/2402.03017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Green and Human-Like Artificial Intelligence: A Complete Survey  on Contemporary Few-Shot Learning Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsoumplekas%2C+G">Georgios Tsoumplekas</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+V">Vladislav Li</a>, 
<a href="/search/cs?searchtype=author&query=Argyriou%2C+V">Vasileios Argyriou</a>, 
<a href="/search/cs?searchtype=author&query=Lytos%2C+A">Anastasios Lytos</a>, 
<a href="/search/cs?searchtype=author&query=Fountoukidis%2C+E">Eleftherios Fountoukidis</a>, 
<a href="/search/cs?searchtype=author&query=Goudos%2C+S+K">Sotirios K. Goudos</a>, 
<a href="/search/cs?searchtype=author&query=Moscholios%2C+I+D">Ioannis D. Moscholios</a>, 
<a href="/search/cs?searchtype=author&query=Sarigiannidis%2C+P">Panagiotis Sarigiannidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 9 figures. Submitted to ACM Computing Surveys
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Despite deep learning's widespread success, its data-hungry and
computationally expensive nature makes it impractical for many data-constrained
real-world applications. Few-Shot Learning (FSL) aims to address these
limitations by enabling rapid adaptation to novel learning tasks, seeing
significant growth in recent years. This survey provides a comprehensive
overview of the field's latest advancements. Initially, FSL is formally
defined, and its relationship with different learning fields is presented. A
novel taxonomy is introduced, extending previously proposed ones, and
real-world applications in classic and novel fields are described. Finally,
recent trends shaping the field, outstanding challenges, and promising future
research directions are discussed.
</p>
</div>
</dd>
<dt><a name="item766">[766]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03019" title="Abstract">arXiv:2402.03019</a> [<a href="/pdf/2402.03019" title="Download PDF">pdf</a>, <a href="/format/2402.03019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Taylor Videos for Action Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xiuyuan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Gedeon%2C+T">Tom Gedeon</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Liang Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Research report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Effectively extracting motions from video is a critical and long-standing
problem for action recognition. This problem is very challenging because
motions (i) do not have an explicit form, (ii) have various concepts such as
displacement, velocity, and acceleration, and (iii) often contain noise caused
by unstable pixels. Addressing these challenges, we propose the Taylor video, a
new video format that highlights the dominate motions (e.g., a waving hand) in
each of its frames named the Taylor frame. Taylor video is named after Taylor
series, which approximates a function at a given point using important terms.
In the scenario of videos, we define an implicit motion-extraction function
which aims to extract motions from video temporal block. In this block, using
the frames, the difference frames, and higher-order difference frames, we
perform Taylor expansion to approximate this function at the starting frame. We
show the summation of the higher-order terms in the Taylor series gives us
dominant motion patterns, where static objects, small and unstable motions are
removed. Experimentally we show that Taylor videos are effective inputs to
popular architectures including 2D CNNs, 3D CNNs, and transformers. When used
individually, Taylor videos yield competitive action recognition accuracy
compared to RGB videos and optical flow. When fused with RGB or optical flow
videos, further accuracy improvement is achieved.
</p>
</div>
</dd>
<dt><a name="item767">[767]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03021" title="Abstract">arXiv:2402.03021</a> [<a href="/pdf/2402.03021" title="Download PDF">pdf</a>, <a href="/format/2402.03021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-induced multiscale losses and efficient multirate gradient descent  schemes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+J">Juncai He</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liangchen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yen-Hsi">Yen-Hsi</a> (Richard)
<a href="/search/cs?searchtype=author&query=Tsai">Tsai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 4 figures, submitted under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">This paper investigates the impact of multiscale data on machine learning
algorithms, particularly in the context of deep learning. A dataset is
multiscale if its distribution shows large variations in scale across different
directions. This paper reveals multiscale structures in the loss landscape,
including its gradients and Hessians inherited from the data. Correspondingly,
it introduces a novel gradient descent approach, drawing inspiration from
multiscale algorithms used in scientific computing. This approach seeks to
transcend empirical learning rate selection, offering a more systematic,
data-informed strategy to enhance training efficiency, especially in the later
stages.
</p>
</div>
</dd>
<dt><a name="item768">[768]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03025" title="Abstract">arXiv:2402.03025</a> [<a href="/pdf/2402.03025" title="Download PDF">pdf</a>, <a href="/format/2402.03025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding and Guiding Weakly Supervised Entity Alignment with  Potential Isomorphism Propagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuanyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+W">Wei Tang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Haifeng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Z">Zirui Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+X">Xiaoyuan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Q">Qi Qi</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+J">Jianxin Liao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Weakly Supervised Entity Alignment (EA) is the task of identifying equivalent
entities across diverse knowledge graphs (KGs) using only a limited number of
seed alignments. Despite substantial advances in aggregation-based weakly
supervised EA, the underlying mechanisms in this setting remain unexplored. In
this paper, we present a propagation perspective to analyze weakly supervised
EA and explain the existing aggregation-based EA models. Our theoretical
analysis reveals that these models essentially seek propagation operators for
pairwise entity similarities. We further prove that, despite the structural
heterogeneity of different KGs, the potentially aligned entities within
aggregation-based EA models have isomorphic subgraphs, which is the core
premise of EA but has not been investigated. Leveraging this insight, we
introduce a potential isomorphism propagation operator to enhance the
propagation of neighborhood information across KGs. We develop a general EA
framework, PipEA, incorporating this operator to improve the accuracy of every
type of aggregation-based model without altering the learning process.
Extensive experiments substantiate our theoretical findings and demonstrate
PipEA's significant performance gains over state-of-the-art weakly supervised
EA methods. Our work not only advances the field but also enhances our
comprehension of aggregation-based weakly supervised EA.
</p>
</div>
</dd>
<dt><a name="item769">[769]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03028" title="Abstract">arXiv:2402.03028</a> [<a href="/pdf/2402.03028" title="Download PDF">pdf</a>, <a href="/format/2402.03028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Functional SDE approximation inspired by a deep operator network  architecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Eigel%2C+M">Martin Eigel</a>, 
<a href="/search/math?searchtype=author&query=Miranda%2C+C">Charles Miranda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">A novel approach to approximate solutions of Stochastic Differential
Equations (SDEs) by Deep Neural Networks is derived and analysed. The
architecture is inspired by the notion of Deep Operator Networks (DeepONets),
which is based on operator learning in function spaces in terms of a reduced
basis also represented in the network. In our setting, we make use of a
polynomial chaos expansion (PCE) of stochastic processes and call the
corresponding architecture SDEONet. The PCE has been used extensively in the
area of uncertainty quantification (UQ) with parametric partial differential
equations. This however is not the case with SDE, where classical sampling
methods dominate and functional approaches are seen rarely. A main challenge
with truncated PCEs occurs due to the drastic growth of the number of
components with respect to the maximum polynomial degree and the number of
basis elements. The proposed SDEONet architecture aims to alleviate the issue
of exponential complexity by learning an optimal sparse truncation of the
Wiener chaos expansion. A complete convergence and complexity analysis is
presented, making use of recent Neural Network approximation results. Numerical
experiments illustrate the promising performance of the suggested approach in
1D and higher dimensions.
</p>
</div>
</dd>
<dt><a name="item770">[770]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03030" title="Abstract">arXiv:2402.03030</a> [<a href="/pdf/2402.03030" title="Download PDF">pdf</a>, <a href="/format/2402.03030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rejection-Sampled Universal Quantization for Smaller Quantization Errors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ling%2C+C+W">Chih Wei Ling</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C+T">Cheuk Ting Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">We construct a randomized vector quantizer which has a smaller maximum error
compared to all known lattice quantizers with the same entropy for dimensions
5, 6, ..., 48, and also has a smaller mean squared error compared to known
lattice quantizers with the same entropy for dimensions 35, ..., 48, in the
high resolution limit. Moreover, our randomized quantizer has a desirable
property that the quantization error is always uniform over the ball and
independent of the input. Our construction is based on applying rejection
sampling on universal quantization, which allows us to shape the error
distribution to be any continuous distribution, not only uniform distributions
over basic cells of a lattice as in conventional dithered quantization. We also
characterize the high SNR limit of one-shot channel simulation for any additive
noise channel under a mild assumption (e.g., the AWGN channel), up to an
additive constant of 1.45 bits.
</p>
</div>
</dd>
<dt><a name="item771">[771]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03037" title="Abstract">arXiv:2402.03037</a> [<a href="/pdf/2402.03037" title="Download PDF">pdf</a>, <a href="/format/2402.03037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Investigation of the Compressed Sensing Phase in Unsourced Multiple  Access
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Clazzer%2C+F">Federico Clazzer</a>, 
<a href="/search/cs?searchtype=author&query=Amri%2C+F">Farouk Amri</a>, 
<a href="/search/cs?searchtype=author&query=Grec%2C+M">Marcel Grec</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures, Accepted for publication at IEEE WCNC 2024 WS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">A vast population of low-cost low-power transmitters sporadically sending
small amounts of data over a common wireless medium is one of the main
scenarios for Internet of things (IoT) data communications. At the medium
access, the use of grant-free solutions may be preferred to reduce overhead
even at the cost of multiple-access interference. Unsourced multiple access
(UMA) has been recently established as relevant framework for energy efficient
grant-free protocols. The use of a compressed sensing (CS) transmission phase
is key in one of the two main classes of UMA protocols, yet little attention
has been posed to sparse greedy algorithms as orthogonal matching pursuit (OMP)
and its variants. We analyze their performance and provide relevant guidance on
how to optimally setup the CS phase. Minimum average transmission power and
minimum number of channel uses are investigated together with the performance
in terms of receiver operating characteristic (ROC). Interestingly, we show how
the basic OMP and generalized OMP (gOMP) are the most competitive algorithms in
their class.
</p>
</div>
</dd>
<dt><a name="item772">[772]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03038" title="Abstract">arXiv:2402.03038</a> [<a href="/pdf/2402.03038" title="Download PDF">pdf</a>, <a href="/format/2402.03038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Combination of Sample Selection Strategies for Few-Shot  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pecher%2C+B">Branislav Pecher</a>, 
<a href="/search/cs?searchtype=author&query=Srba%2C+I">Ivan Srba</a>, 
<a href="/search/cs?searchtype=author&query=Bielikova%2C+M">Maria Bielikova</a>, 
<a href="/search/cs?searchtype=author&query=Vanschoren%2C+J">Joaquin Vanschoren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">In few-shot learning, such as meta-learning, few-shot fine-tuning or
in-context learning, the limited number of samples used to train a model have a
significant impact on the overall success. Although a large number of sample
selection strategies exist, their impact on the performance of few-shot
learning is not extensively known, as most of them have been so far evaluated
in typical supervised settings only. In this paper, we thoroughly investigate
the impact of 20 sample selection strategies on the performance of 5 few-shot
learning approaches over 8 image and 6 text datasets. In addition, we propose a
new method for automatic combination of sample selection strategies (ACSESS)
that leverages the strengths and complementary information of the individual
strategies. The experimental results show that our method consistently
outperforms the individual selection strategies, as well as the recently
proposed method for selecting support examples for in-context learning. We also
show a strong modality, dataset and approach dependence for the majority of
strategies as well as their dependence on the number of shots - demonstrating
that the sample selection strategies play a significant role for lower number
of shots, but regresses to random selection at higher number of shots.
</p>
</div>
</dd>
<dt><a name="item773">[773]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03040" title="Abstract">arXiv:2402.03040</a> [<a href="/pdf/2402.03040" title="Download PDF">pdf</a>, <a href="/format/2402.03040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InteractiveVideo: User-Centric Controllable Video Generation with  Synergistic Multimodal Instructions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yiyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Y">Yuhao Kang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhixin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+X">Xiaohan Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Sanyuan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+X">Xiangyu Yue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code, models, and demo are available at <a href="https://github.com/invictus717/InteractiveVideo">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
<p class="mathjax">We introduce $\textit{InteractiveVideo}$, a user-centric framework for video
generation. Different from traditional generative approaches that operate based
on user-provided images or text, our framework is designed for dynamic
interaction, allowing users to instruct the generative model through various
intuitive mechanisms during the whole generation process, e.g. text and image
prompts, painting, drag-and-drop, etc. We propose a Synergistic Multimodal
Instruction mechanism, designed to seamlessly integrate users' multimodal
instructions into generative models, thus facilitating a cooperative and
responsive interaction between user inputs and the generative process. This
approach enables iterative and fine-grained refinement of the generation result
through precise and effective user instructions. With
$\textit{InteractiveVideo}$, users are given the flexibility to meticulously
tailor key aspects of a video. They can paint the reference image, edit
semantics, and adjust video motions until their requirements are fully met.
Code, models, and demo are available at
https://github.com/invictus717/InteractiveVideo
</p>
</div>
</dd>
<dt><a name="item774">[774]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03041" title="Abstract">arXiv:2402.03041</a> [<a href="/pdf/2402.03041" title="Download PDF">pdf</a>, <a href="/format/2402.03041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Demystifying Datapath Accelerator Enhanced Off-path SmartNIC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xuzheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+T">Ting Fu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yifan Shen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+K">Kun Qian</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lingjun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chao Shi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Ming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zeke Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Network speeds grow quickly in the modern cloud, so SmartNICs are introduced
to offload network processing tasks, even application logic. However, typical
multicore SmartNICs such as BlueFiled-2 are only capable of processing
control-plane tasks with their embedded CPU that has limited memory bandwidth
and computing power. On the other hand, hot cloud applications evolve, such
that a limited number of fixed hardware engines in a SmartNIC cannot satisfy
the requirements of cloud applications. Therefore, SmartNIC programmers call
for a programmable datapath accelerator (DPA) to process network traffic at
line rate. However, no existing work has unveiled the performance
characteristics of the existing DPA.
<br />To this end, we present the first architectural characterization of the
latest DPA-enhanced BF3 SmartNIC. Our evaluation results indicate that BF3's
DPA is much wimpier than the off-path Arm and the host CPU. However, we still
identify that DPA has three unique architectural characteristics that unleash
the performance potential of DPA. Specifically, we demonstrate how to take
advantage of DPA's three architectural characteristics regarding computing,
networking, and memory subsystems. Then we propose three important guidelines
for programmers to fully unleash the potential of DPA. To demonstrate the
effectiveness of our approach, we conduct detailed case studies regarding each
guideline. Our case study on key-value aggregation service achieves up to
4.3$\times$ higher throughput by using our guidelines to optimize memory
combinations.
</p>
</div>
</dd>
<dt><a name="item775">[775]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03043" title="Abstract">arXiv:2402.03043</a> [<a href="/pdf/2402.03043" title="Download PDF">pdf</a>, <a href="/format/2402.03043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SIDU-TXT: An XAI Algorithm for NLP with a Holistic Assessment Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jahromi%2C+M+N+S">Mohammad N.S. Jahromi</a>, 
<a href="/search/cs?searchtype=author&query=Muddamsetty%2C+S+M">Satya. M. Muddamsetty</a>, 
<a href="/search/cs?searchtype=author&query=Jarlner%2C+A+S+S">Asta Sofie Stage Jarlner</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%B8genhaug%2C+A+M">Anna Murphy H&#xf8;genhaug</a>, 
<a href="/search/cs?searchtype=author&query=Gammeltoft-Hansen%2C+T">Thomas Gammeltoft-Hansen</a>, 
<a href="/search/cs?searchtype=author&query=Moeslund%2C+T+B">Thomas B. Moeslund</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint submitted to Elsevier on Jan 5th, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Explainable AI (XAI) aids in deciphering 'black-box' models. While several
methods have been proposed and evaluated primarily in the image domain, the
exploration of explainability in the text domain remains a growing research
area. In this paper, we delve into the applicability of XAI methods for the
text domain. In this context, the 'Similarity Difference and Uniqueness' (SIDU)
XAI method, recognized for its superior capability in localizing entire salient
regions in image-based classification is extended to textual data. The extended
method, SIDU-TXT, utilizes feature activation maps from 'black-box' models to
generate heatmaps at a granular, word-based level, thereby providing
explanations that highlight contextually significant textual elements crucial
for model predictions. Given the absence of a unified standard for assessing
XAI methods, this study applies a holistic three-tiered comprehensive
evaluation framework: Functionally-Grounded, Human-Grounded and
Application-Grounded, to assess the effectiveness of the proposed SIDU-TXT
across various experiments. We find that, in sentiment analysis task of a movie
review dataset, SIDU-TXT excels in both functionally and human-grounded
evaluations, demonstrating superior performance through quantitative and
qualitative analyses compared to benchmarks like Grad-CAM and LIME. In the
application-grounded evaluation within the sensitive and complex legal domain
of asylum decision-making, SIDU-TXT and Grad-CAM demonstrate comparable
performances, each with its own set of strengths and weaknesses. However, both
methods fall short of entirely fulfilling the sophisticated criteria of expert
expectations, highlighting the imperative need for additional research in XAI
methods suitable for such domains.
</p>
</div>
</dd>
<dt><a name="item776">[776]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03046" title="Abstract">arXiv:2402.03046</a> [<a href="/pdf/2402.03046" title="Download PDF">pdf</a>, <a href="/format/2402.03046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open RL Benchmark: Comprehensive Tracked Experiments for Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shengyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Gallou%C3%A9dec%2C+Q">Quentin Gallou&#xe9;dec</a>, 
<a href="/search/cs?searchtype=author&query=Felten%2C+F">Florian Felten</a>, 
<a href="/search/cs?searchtype=author&query=Raffin%2C+A">Antonin Raffin</a>, 
<a href="/search/cs?searchtype=author&query=Dossa%2C+R+F+J">Rousslan Fernand Julien Dossa</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yanxiao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Sullivan%2C+R">Ryan Sullivan</a>, 
<a href="/search/cs?searchtype=author&query=Makoviychuk%2C+V">Viktor Makoviychuk</a>, 
<a href="/search/cs?searchtype=author&query=Makoviichuk%2C+D">Denys Makoviichuk</a>, 
<a href="/search/cs?searchtype=author&query=Danesh%2C+M+H">Mohamad H. Danesh</a>, 
<a href="/search/cs?searchtype=author&query=Roum%C3%A9gous%2C+C">Cyril Roum&#xe9;gous</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+J">Jiayi Weng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chufan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+M">Md Masudur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Ara%C3%BAjo%2C+J+G+M">Jo&#xe3;o G. M. Ara&#xfa;jo</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+G">Guorui Quan</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+D">Daniel Tan</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+T">Timo Klein</a>, 
<a href="/search/cs?searchtype=author&query=Charakorn%2C+R">Rujikorn Charakorn</a>, 
<a href="/search/cs?searchtype=author&query=Towers%2C+M">Mark Towers</a>, 
<a href="/search/cs?searchtype=author&query=Berthelot%2C+Y">Yann Berthelot</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+K">Kinal Mehta</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+D">Dipam Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=KG%2C+A">Arjun KG</a>, 
<a href="/search/cs?searchtype=author&query=Charraut%2C+V">Valentin Charraut</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+C">Chang Ye</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zichen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Alegre%2C+L+N">Lucas N. Alegre</a>, 
<a href="/search/cs?searchtype=author&query=Nikulin%2C+A">Alexander Nikulin</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianlin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jongwook Choi</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+B">Brent Yi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In many Reinforcement Learning (RL) papers, learning curves are useful
indicators to measure the effectiveness of RL algorithms. However, the complete
raw data of the learning curves are rarely available. As a result, it is
usually necessary to reproduce the experiments from scratch, which can be
time-consuming and error-prone. We present Open RL Benchmark, a set of fully
tracked RL experiments, including not only the usual data such as episodic
return, but also all algorithm-specific and system metrics. Open RL Benchmark
is community-driven: anyone can download, use, and contribute to the data. At
the time of writing, more than 25,000 runs have been tracked, for a cumulative
duration of more than 8 years. Open RL Benchmark covers a wide range of RL
libraries and reference implementations. Special care is taken to ensure that
each experiment is precisely reproducible by providing not only the full
parameters, but also the versions of the dependencies used to generate it. In
addition, Open RL Benchmark comes with a command-line interface (CLI) for easy
fetching and generating figures to present the results. In this document, we
include two case studies to demonstrate the usefulness of Open RL Benchmark in
practice. To the best of our knowledge, Open RL Benchmark is the first RL
benchmark of its kind, and the authors hope that it will improve and facilitate
the work of researchers in the field.
</p>
</div>
</dd>
<dt><a name="item777">[777]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03047" title="Abstract">arXiv:2402.03047</a> [<a href="/pdf/2402.03047" title="Download PDF">pdf</a>, <a href="/format/2402.03047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PFDM: Parser-Free Virtual Try-on via Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niu%2C+Y">Yunfang Niu</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+D">Dong Yi</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lingxiang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+P">Pengxiang Cai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jinqiao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Virtual try-on can significantly improve the garment shopping experiences in
both online and in-store scenarios, attracting broad interest in computer
vision. However, to achieve high-fidelity try-on performance, most
state-of-the-art methods still rely on accurate segmentation masks, which are
often produced by near-perfect parsers or manual labeling. To overcome the
bottleneck, we propose a parser-free virtual try-on method based on the
diffusion model (PFDM). Given two images, PFDM can "wear" garments on the
target person seamlessly by implicitly warping without any other information.
To learn the model effectively, we synthesize many pseudo-images and construct
sample pairs by wearing various garments on persons. Supervised by the
large-scale expanded dataset, we fuse the person and garment features using a
proposed Garment Fusion Attention (GFA) mechanism. Experiments demonstrate that
our proposed PFDM can successfully handle complex cases, synthesize
high-fidelity images, and outperform both state-of-the-art parser-free and
parser-based models.
</p>
</div>
</dd>
<dt><a name="item778">[778]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03048" title="Abstract">arXiv:2402.03048</a> [<a href="/pdf/2402.03048" title="Download PDF">pdf</a>, <a href="/format/2402.03048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cooperative Learning with Gaussian Processes for Euler-Lagrange Systems  Tracking Control under Switching Topologies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zewen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+S">Songbo Dong</a>, 
<a href="/search/cs?searchtype=author&query=Lederer%2C+A">Armin Lederer</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xiaobing Dai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Siyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sosnowski%2C+S">Stefan Sosnowski</a>, 
<a href="/search/cs?searchtype=author&query=Hattab%2C+G">Georges Hattab</a>, 
<a href="/search/cs?searchtype=author&query=Hirche%2C+S">Sandra Hirche</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This work presents an innovative learning-based approach to tackle the
tracking control problem of Euler-Lagrange multi-agent systems with partially
unknown dynamics operating under switching communication topologies. The
approach leverages a correlation-aware cooperative algorithm framework built
upon Gaussian process regression, which adeptly captures inter-agent
correlations for uncertainty predictions. A standout feature is its exceptional
efficiency in deriving the aggregation weights achieved by circumventing the
computationally intensive posterior variance calculations. Through Lyapunov
stability analysis, the distributed control law ensures bounded tracking errors
with high probability. Simulation experiments validate the protocol's efficacy
in effectively managing complex scenarios, establishing it as a promising
solution for robust tracking control in multi-agent systems characterized by
uncertain dynamics and dynamic communication structures.
</p>
</div>
</dd>
<dt><a name="item779">[779]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03049" title="Abstract">arXiv:2402.03049</a> [<a href="/pdf/2402.03049" title="Download PDF">pdf</a>, <a href="/format/2402.03049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EasyInstruct: An Easy-to-use Instruction Processing Framework for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ou%2C+Y">Yixin Ou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+H">Honghao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Ziwen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+S">Shuofei Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+Z">Zhen Bi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Ongoing work; the project website is at <a href="https://zjunlp.github.io/project/EasyInstruct">this https URL</a>, code is at <a href="https://github.com/zjunlp/EasyInstruct">this https URL</a>, demo is at <a href="https://huggingface.co/spaces/zjunlp/EasyInstruct">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">In recent years, instruction tuning has gained increasing attention and
emerged as a crucial technique to enhance the capabilities of Large Language
Models (LLMs). To construct high-quality instruction datasets, many instruction
processing approaches have been proposed, aiming to achieve a delicate balance
between data quantity and data quality. Nevertheless, due to inconsistencies
that persist among various instruction processing methods, there is no standard
open-source instruction processing implementation framework available for the
community, which hinders practitioners from further developing and advancing.
To facilitate instruction processing research and development, we present
EasyInstruct, an easy-to-use instruction processing framework for LLMs, which
modularizes instruction generation, selection, and prompting, while also
considering their combination and interaction. EasyInstruct is publicly
released and actively maintained at https://github.com/zjunlp/EasyInstruct,
along with a running demo App at
https://huggingface.co/spaces/zjunlp/EasyInstruct for quick-start, calling for
broader research centered on instruction data.
</p>
</div>
</dd>
<dt><a name="item780">[780]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03050" title="Abstract">arXiv:2402.03050</a> [<a href="/pdf/2402.03050" title="Download PDF">pdf</a>, <a href="/ps/2402.03050" title="Download PostScript">ps</a>, <a href="/format/2402.03050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Study of the Current State-of-the-Art in Nepali  Automatic Speech Recognition Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghimire%2C+R+R">Rupak Raj Ghimire</a>, 
<a href="/search/cs?searchtype=author&query=Bal%2C+B+K">Bal Krishna Bal</a>, 
<a href="/search/cs?searchtype=author&query=Poudyal%2C+P">Prakash Poudyal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in International Conference on Technologies for Computer, Electrical, Electronics &amp; Communication (ICT-CEEL 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In this paper, we examine the research conducted in the field of Nepali
Automatic Speech Recognition (ASR). The primary objective of this survey is to
conduct a comprehensive review of the works on Nepali Automatic Speech
Recognition Systems completed to date, explore the different datasets used,
examine the technology utilized, and take account of the obstacles encountered
in implementing the Nepali ASR system. In tandem with the global trends of
ever-increasing research on speech recognition based research, the number of
Nepalese ASR-related projects are also growing. Nevertheless, the investigation
of language and acoustic models of the Nepali language has not received
adequate attention compared to languages that possess ample resources. In this
context, we provide a framework as well as directions for future
investigations.
</p>
</div>
</dd>
<dt><a name="item781">[781]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03053" title="Abstract">arXiv:2402.03053</a> [<a href="/pdf/2402.03053" title="Download PDF">pdf</a>, <a href="/format/2402.03053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Lingual Malaysian Embedding: Leveraging Large Language Models for  Semantic Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zolkepli%2C+H">Husein Zolkepli</a>, 
<a href="/search/cs?searchtype=author&query=Razak%2C+A">Aisyah Razak</a>, 
<a href="/search/cs?searchtype=author&query=Adha%2C+K">Kamarul Adha</a>, 
<a href="/search/cs?searchtype=author&query=Nazhan%2C+A">Ariff Nazhan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this work, we present a comprehensive exploration of finetuning Malaysian
language models, specifically Llama2 and Mistral, on embedding tasks involving
negative and positive pairs. We release two distinct models tailored for
Semantic Similarity and Retrieval-Augmented Generation (RAG).
<br />For Semantic Similarity, our 600 million parameter Llama2 model outperforms
OpenAI text-embedding-ada-002 across all recall@k metrics for b.cari.com.my,
c.cari.com.my, Malay news, and Malaysian Twitter test sets.
<br />In the realm of RAG models, our approach proves competitive with OpenAI
text-embedding-ada-002 in the Malaysian context. Notably, our 2 billion
parameter Llama2 model achieves superior Recall@5, Recall@10 for the "Melayu"
keyword research papers dataset and excels in Recall@3, Recall@5, and Recall@10
for the lom.agc.gov.my dataset.
<br />These findings underscore the effectiveness of our finetuning strategy and
highlight the performance gains in both Semantic Similarity and RAG tasks.
<br />All models released at
https://huggingface.co/collections/mesolitica/malaysian-embedding-6523612bfe5881ad35f81b99
</p>
</div>
</dd>
<dt><a name="item782">[782]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03055" title="Abstract">arXiv:2402.03055</a> [<a href="/pdf/2402.03055" title="Download PDF">pdf</a>, <a href="/format/2402.03055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Actor-Critic: Learning to Explore with PAC-Bayes  Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tasdighi%2C+B">Bahareh Tasdighi</a>, 
<a href="/search/cs?searchtype=author&query=Werge%2C+N">Nicklas Werge</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yi-Shan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Kandemir%2C+M">Melih Kandemir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 4 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We introduce Probabilistic Actor-Critic (PAC), a novel reinforcement learning
algorithm with improved continuous control performance thanks to its ability to
mitigate the exploration-exploitation trade-off. PAC achieves this by
seamlessly integrating stochastic policies and critics, creating a dynamic
synergy between the estimation of critic uncertainty and actor training. The
key contribution of our PAC algorithm is that it explicitly models and infers
epistemic uncertainty in the critic through Probably Approximately
Correct-Bayesian (PAC-Bayes) analysis. This incorporation of critic uncertainty
enables PAC to adapt its exploration strategy as it learns, guiding the actor's
decision-making process. PAC compares favorably against fixed or pre-scheduled
exploration schemes of the prior art. The synergy between stochastic policies
and critics, guided by PAC-Bayes analysis, represents a fundamental step
towards a more adaptive and effective exploration strategy in deep
reinforcement learning. We report empirical evaluations demonstrating PAC's
enhanced stability and improved performance over the state of the art in
diverse continuous control problems.
</p>
</div>
</dd>
<dt><a name="item783">[783]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03056" title="Abstract">arXiv:2402.03056</a> [<a href="/pdf/2402.03056" title="Download PDF">pdf</a>, <a href="/format/2402.03056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Note on quasi-optimal error estimates for the pressure for  shear-thickening fluids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kaltenbach%2C+A">Alex Kaltenbach</a>, 
<a href="/search/math?searchtype=author&query=R%C5%AF%C5%BEi%C4%8Dka%2C+M">Michael R&#x16f;&#x17e;i&#x10d;ka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 3 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we derive quasi-optimal a priori error estimates for the
kinematic pressure for a Local Discontinuous Galerkin (LDG) approximation of
steady systems of $p$-Navier-Stokes type in the case of shear-thickening, i.e.,
in the case $p&gt;2$, imposing a new mild Muckenhoupt regularity condition.
</p>
</div>
</dd>
<dt><a name="item784">[784]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03060" title="Abstract">arXiv:2402.03060</a> [<a href="/pdf/2402.03060" title="Download PDF">pdf</a>, <a href="/format/2402.03060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniHENN: Designing More Versatile Homomorphic Encryption-based CNNs  without im2col
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+H">Hyunmin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jihun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seungho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Seonhye Park</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jeongyong Park</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+W">Wonbin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyoungshick Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Homomorphic encryption enables computations on encrypted data without
decryption, which is crucial for privacy-preserving cloud services. However,
deploying convolutional neural networks (CNNs) with homomorphic encryption
encounters significant challenges, particularly in converting input data into a
two-dimensional matrix for convolution, typically achieved using the im2col
technique. While efficient, this method limits the variety of deployable CNN
models due to compatibility constraints with the encrypted data structure.
UniHENN, a homomorphic encryption-based CNN architecture, eliminates the need
for im2col, ensuring compatibility with a diverse range of CNN models using
homomorphic encryption. Our experiments demonstrate that UniHENN surpasses the
leading 2D CNN inference architecture, PyCrCNN, in inference time, as evidenced
by its performance on the LeNet-1 dataset, where it averages 30.090
seconds--significantly faster than PyCrCNN's 794.064 seconds. Furthermore,
UniHENN outperforms TenSEAL, which employs im2col, in processing concurrent
images, an essential feature for high-demand cloud applications. The
versatility of UniHENN is proven across various CNN architectures, including 1D
and six different 2D CNNs, highlighting its flexibility and efficiency. These
qualities establish UniHENN as a promising solution for privacy-preserving,
cloud-based CNN services, addressing the increasing demand for scalable,
secure, and efficient deep learning in cloud computing environments.
</p>
</div>
</dd>
<dt><a name="item785">[785]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03063" title="Abstract">arXiv:2402.03063</a> [<a href="/pdf/2402.03063" title="Download PDF">pdf</a>, <a href="/ps/2402.03063" title="Download PostScript">ps</a>, <a href="/format/2402.03063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Independent set reconfiguration in H-free graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bartier%2C+V">Valentin Bartier</a>, 
<a href="/search/cs?searchtype=author&query=Bousquet%2C+N">Nicolas Bousquet</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BChlenthaler%2C+M">Moritz M&#xfc;hlenthaler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Given a graph $G$ and two independent sets of $G$, the independent set
reconfiguration problem asks whether one independent set can be transformed
into the other by moving a single vertex at a time, such that at each
intermediate step we have an independent set of $G$. We study the complexity of
this problem for $H$-free graphs under the token sliding and token jumping
rule. Our contribution is twofold. First, we prove a reconfiguration analogue
of Alekseev's theorem, showing that the problem is PSPACE-complete unless $H$
is a path or a subdivision of the claw. We then show that under the token
sliding rule, the problem admits a polynomial-time algorithm if the input graph
is fork-free.
</p>
</div>
</dd>
<dt><a name="item786">[786]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03067" title="Abstract">arXiv:2402.03067</a> [<a href="/pdf/2402.03067" title="Download PDF">pdf</a>, <a href="/ps/2402.03067" title="Download PostScript">ps</a>, <a href="/format/2402.03067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilingual transformer and BERTopic for short text topic modeling: The  case of Serbian
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Medvecki%2C+D">Darija Medvecki</a>, 
<a href="/search/cs?searchtype=author&query=Ba%C5%A1aragin%2C+B">Bojana Ba&#x161;aragin</a>, 
<a href="/search/cs?searchtype=author&query=Ljaji%C4%87%2C+A">Adela Ljaji&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Milo%C5%A1evi%C4%87%2C+N">Nikola Milo&#x161;evi&#x107;</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Trajanovic, M., Filipovic, N., Zdravkovic, M. (eds) Disruptive
  Information Technologies for a Smart Society. ICIST 2023. Lecture Notes in
  Networks and Systems, vol 872. Springer, Cham
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper presents the results of the first application of BERTopic, a
state-of-the-art topic modeling technique, to short text written in a
morphologi-cally rich language. We applied BERTopic with three multilingual
embed-ding models on two levels of text preprocessing (partial and full) to
evalu-ate its performance on partially preprocessed short text in Serbian. We
also compared it to LDA and NMF on fully preprocessed text. The experiments
were conducted on a dataset of tweets expressing hesitancy toward COVID-19
vaccination. Our results show that with adequate parameter setting, BERTopic
can yield informative topics even when applied to partially pre-processed short
text. When the same parameters are applied in both prepro-cessing scenarios,
the performance drop on partially preprocessed text is minimal. Compared to LDA
and NMF, judging by the keywords, BERTopic offers more informative topics and
gives novel insights when the number of topics is not limited. The findings of
this paper can be significant for re-searchers working with other
morphologically rich low-resource languages and short text.
</p>
</div>
</dd>
<dt><a name="item787">[787]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03068" title="Abstract">arXiv:2402.03068</a> [<a href="/pdf/2402.03068" title="Download PDF">pdf</a>, <a href="/ps/2402.03068" title="Download PostScript">ps</a>, <a href="/format/2402.03068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Note on Rounding Matchings in General Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dudeja%2C+A">Aditi Dudeja</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">In this note, we revisit the rounding algorithm of Wajc. Wajc gave a
fully-adaptive randomized algorithm that rounds a dynamic fractional matching
in an unweighted bipartite graph to an integral matching of nearly the same
value in $O(\text{poly}(\log n,\frac{1}{\varepsilon}))$ update time. We give
show that the guarantees of this algorithm hold for general graphs as well.
Additionally, we show useful properties of this subroutine which have
applications in rounding weighted fractional matchings.
</p>
</div>
</dd>
<dt><a name="item788">[788]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03074" title="Abstract">arXiv:2402.03074</a> [<a href="/pdf/2402.03074" title="Download PDF">pdf</a>, <a href="/format/2402.03074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A moment-based Hermite WENO scheme with unified stencils for hyperbolic  conservation laws
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhao%2C+C+F+J+Q+Z">Chuan Fan Jianxian Qiu Zhuang Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 pages, 14 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, a fifth-order moment-based Hermite weighted essentially
non-oscillatory scheme with unified stencils (termed as HWENO-U) is proposed
for hyperbolic conservation laws. The main idea of the HWENO-U scheme is to
modify the first-order moment by a HWENO limiter only in the time
discretizations using the same information of spatial reconstructions, in which
the limiter not only overcomes spurious oscillations well, but also ensures the
stability of the fully-discrete scheme. For the HWENO reconstructions, a new
scale-invariant nonlinear weight is designed by incorporating only the integral
average values of the solution, which keeps all properties of the original one
while is more robust for simulating challenging problems with sharp scale
variations. Compared with previous HWENO schemes, the advantages of the HWENO-U
scheme are: (1) a simpler implemented process involving only a single HWENO
reconstruction applied throughout the entire procedures without any
modifications for the governing equations; (2) increased efficiency by
utilizing the same candidate stencils, reconstructed polynomials, and linear
and nonlinear weights in both the HWENO limiter and spatial reconstructions;
(3) reduced problem-specific dependencies and improved rationality, as the
nonlinear weights are identical for the function $u$ and its non-zero multiple
$\zeta u$. Besides, the proposed scheme retains the advantages of previous
HWENO schemes, including compact reconstructed stencils and the utilization of
artificial linear weights. Extensive benchmarks are carried out to validate the
accuracy, efficiency, resolution, and robustness of the proposed scheme.
</p>
</div>
</dd>
<dt><a name="item789">[789]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03077" title="Abstract">arXiv:2402.03077</a> [<a href="/pdf/2402.03077" title="Download PDF">pdf</a>, <a href="/ps/2402.03077" title="Download PostScript">ps</a>, <a href="/format/2402.03077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Markov Persuasion Processes: Learning to Persuade from Scratch
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bacchiocchi%2C+F">Francesco Bacchiocchi</a>, 
<a href="/search/cs?searchtype=author&query=Stradi%2C+F+E">Francesco Emanuele Stradi</a>, 
<a href="/search/cs?searchtype=author&query=Castiglioni%2C+M">Matteo Castiglioni</a>, 
<a href="/search/cs?searchtype=author&query=Marchesi%2C+A">Alberto Marchesi</a>, 
<a href="/search/cs?searchtype=author&query=Gatti%2C+N">Nicola Gatti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In Bayesian persuasion, an informed sender strategically discloses
information to a receiver so as to persuade them to undertake desirable
actions. Recently, a growing attention has been devoted to settings in which
sender and receivers interact sequentially. Recently, Markov persuasion
processes (MPPs) have been introduced to capture sequential scenarios where a
sender faces a stream of myopic receivers in a Markovian environment. The MPPs
studied so far in the literature suffer from issues that prevent them from
being fully operational in practice, e.g., they assume that the sender knows
receivers' rewards. We fix such issues by addressing MPPs where the sender has
no knowledge about the environment. We design a learning algorithm for the
sender, working with partial feedback. We prove that its regret with respect to
an optimal information-disclosure policy grows sublinearly in the number of
episodes, as it is the case for the loss in persuasiveness cumulated while
learning. Moreover, we provide a lower bound for our setting matching the
guarantees of our algorithm.
</p>
</div>
</dd>
<dt><a name="item790">[790]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03081" title="Abstract">arXiv:2402.03081</a> [<a href="/pdf/2402.03081" title="Download PDF">pdf</a>, <a href="/format/2402.03081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preference-Conditioned Language-Guided Abstraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+A">Andi Peng</a>, 
<a href="/search/cs?searchtype=author&query=Bobu%2C+A">Andreea Bobu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B+Z">Belinda Z. Li</a>, 
<a href="/search/cs?searchtype=author&query=Sumers%2C+T+R">Theodore R. Sumers</a>, 
<a href="/search/cs?searchtype=author&query=Sucholutsky%2C+I">Ilia Sucholutsky</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+N">Nishanth Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Griffiths%2C+T+L">Thomas L. Griffiths</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+J+A">Julie A. Shah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> HRI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Learning from demonstrations is a common way for users to teach robots, but
it is prone to spurious feature correlations. Recent work constructs state
abstractions, i.e. visual representations containing task-relevant features,
from language as a way to perform more generalizable learning. However, these
abstractions also depend on a user's preference for what matters in a task,
which may be hard to describe or infeasible to exhaustively specify using
language alone. How do we construct abstractions to capture these latent
preferences? We observe that how humans behave reveals how they see the world.
Our key insight is that changes in human behavior inform us that there are
differences in preferences for how humans see the world, i.e. their state
abstractions. In this work, we propose using language models (LMs) to query for
those preferences directly given knowledge that a change in behavior has
occurred. In our framework, we use the LM in two ways: first, given a text
description of the task and knowledge of behavioral change between states, we
query the LM for possible hidden preferences; second, given the most likely
preference, we query the LM to construct the state abstraction. In this
framework, the LM is also able to ask the human directly when uncertain about
its own estimate. We demonstrate our framework's ability to construct effective
preference-conditioned abstractions in simulated experiments, a user study, as
well as on a real Spot robot performing mobile manipulation tasks.
</p>
</div>
</dd>
<dt><a name="item791">[791]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03082" title="Abstract">arXiv:2402.03082</a> [<a href="/pdf/2402.03082" title="Download PDF">pdf</a>, <a href="/format/2402.03082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Text Meets Low-level Vision: A Comprehensive Survey on Visual  Text Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shu%2C+Y">Yan Shu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+W">Weichao Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenhang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+F">Fangmin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yu Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Visual text, a pivotal element in both document and scene images, speaks
volumes and attracts significant attention in the computer vision domain.
Beyond visual text detection and recognition, the field of visual text
processing has experienced a surge in research, driven by the advent of
fundamental generative models. However, challenges persist due to the unique
properties and features that distinguish text from general objects. Effectively
leveraging these unique textual characteristics is crucial in visual text
processing, as observed in our study. In this survey, we present a
comprehensive, multi-perspective analysis of recent advancements in this field.
Initially, we introduce a hierarchical taxonomy encompassing areas ranging from
text image enhancement and restoration to text image manipulation, followed by
different learning paradigms. Subsequently, we conduct an in-depth discussion
of how specific textual features such as structure, stroke, semantics, style,
and spatial context are seamlessly integrated into various tasks. Furthermore,
we explore available public datasets and benchmark the reviewed methods on
several widely-used datasets. Finally, we identify principal challenges and
potential avenues for future research. Our aim is to establish this survey as a
fundamental resource, fostering continued exploration and innovation in the
dynamic area of visual text processing.
</p>
</div>
</dd>
<dt><a name="item792">[792]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03084" title="Abstract">arXiv:2402.03084</a> [<a href="/pdf/2402.03084" title="Download PDF">pdf</a>, <a href="/ps/2402.03084" title="Download PostScript">ps</a>, <a href="/format/2402.03084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New constructions of MSRD codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez-Pe%C3%B1as%2C+U">Umberto Mart&#xed;nez-Pe&#xf1;as</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">In this work, we provide four methods for constructing new maximum sum-rank
distance (MSRD) codes. The first method, a variant of cartesian products,
allows faster decoding than known MSRD codes of the same parameters. The other
three methods allow us to extend or modify existing MSRD codes in order to
obtain new explicit MSRD codes for sets of matrix sizes (numbers of rows and
columns in different blocks) that were not attainable by previous
constructions. In this way, we show that MSRD codes exist (by giving explicit
constructions) for new ranges of parameters, in particular with different
numbers of rows and columns at different positions.
</p>
</div>
</dd>
<dt><a name="item793">[793]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03087" title="Abstract">arXiv:2402.03087</a> [<a href="/pdf/2402.03087" title="Download PDF">pdf</a>, <a href="/format/2402.03087" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> XNLP-hardness of Parameterized Problems on Planar Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bodlaender%2C+H+L">Hans L. Bodlaender</a>, 
<a href="/search/cs?searchtype=author&query=Szil%C3%A1gyi%2C+K">Krisztina Szil&#xe1;gyi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
<p class="mathjax">The class XNLP consists of (parameterized) problems that can be solved
nondeterministically in $f(k)n^{O(1)}$ time and $f(k)\log n$ space, where $n$
is the size of the input instance and $k$ the parameter. The class XALP
consists of problems that can be solved in the above time and space with access
to an additional stack. These two classes are a "natural home" for many
standard graph problems and their generalizations.
<br />In this paper, we show the hardness of several problems on planar graphs,
parameterized by outerplanarity, treewidth and pathwidth, thus strengthening
several existing results. In particular, we show the XNLP-hardness of the
following problems parameterized by outerplanarity: All-or-Nothing Flow, Target
Outdegree Orientation, Capacitated (Red-Blue) Dominating Set, Target Set
Selections etc. We also show the XNLP-completeness of Scattered Set
parameterized by pathwidth and XALP-completeness parameterized by treewidth and
outerplanarity.
</p>
</div>
</dd>
<dt><a name="item794">[794]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03092" title="Abstract">arXiv:2402.03092</a> [<a href="/pdf/2402.03092" title="Download PDF">pdf</a>, <a href="/ps/2402.03092" title="Download PostScript">ps</a>, <a href="/format/2402.03092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asynchronous dynamics of isomorphic Boolean networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bridoux%2C+F">Florian Bridoux</a>, 
<a href="/search/cs?searchtype=author&query=Marchetto%2C+A+P">Aymeric Picard Marchetto</a>, 
<a href="/search/cs?searchtype=author&query=Richard%2C+A">Adrien Richard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30p, submitted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">A Boolean network is a function $f:\{0,1\}^n\to\{0,1\}^n$ from which several
dynamics can be derived, depending on the context. The most classical ones are
the synchronous and asynchronous dynamics. Both are digraphs on $\{0,1\}^n$,
but the synchronous dynamics (which is identified with $f$) has an arc from $x$
to $f(x)$ while the asynchronous dynamics $\mathcal{A}(f)$ has an arc from $x$
to $x+e_i$ whenever $x_i\neq f_i(x)$. Clearly, $f$ and $\mathcal{A}(f)$ share
the same information, but what can be said on these objects up to isomorphism?
We prove that if $\mathcal{A}(f)$ is only known up to isomorphism then, with
high probability, $f$ can be fully reconstructed up to isomorphism. We then
show that the converse direction is far from being true. In particular, if $f$
is only known up to isomorphism, very little can be said on the attractors of
$\mathcal{A}(f)$. For instance, if $f$ has $p$ fixed points, then
$\mathcal{A}(f)$ has at least $\max(1,p)$ attractors, and we prove that this
trivial lower bound is tight: there always exists $h\sim f$ such that
$\mathcal{A}(h)$ has exactly $\max(1,p)$ attractors. But $\mathcal{A}(f)$ may
often have much more attractors since we prove that, with high probability,
there exists $h\sim f$ such that $\mathcal{A}(h)$ has $\Omega(2^n)$ attractors.
</p>
</div>
</dd>
<dt><a name="item795">[795]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03093" title="Abstract">arXiv:2402.03093</a> [<a href="/pdf/2402.03093" title="Download PDF">pdf</a>, <a href="/format/2402.03093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI-Enhanced Virtual Reality in Medicine: A Comprehensive Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yixuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+K">Kaiyuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D+Z">Danny Z. Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jian Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">With the rapid advance of computer graphics and artificial intelligence
technologies, the ways we interact with the world have undergone a
transformative shift. Virtual Reality (VR) technology, aided by artificial
intelligence (AI), has emerged as a dominant interaction media in multiple
application areas, thanks to its advantage of providing users with immersive
experiences. Among those applications, medicine is considered one of the most
promising areas. In this paper, we present a comprehensive examination of the
burgeoning field of AI-enhanced VR applications in medical care and services.
By introducing a systematic taxonomy, we meticulously classify the pertinent
techniques and applications into three well-defined categories based on
different phases of medical diagnosis and treatment: Visualization Enhancement,
VR-related Medical Data Processing, and VR-assisted Intervention. This
categorization enables a structured exploration of the diverse roles that
AI-powered VR plays in the medical domain, providing a framework for a more
comprehensive understanding and evaluation of these technologies. To our best
knowledge, this is the first systematic survey of AI-powered VR systems in
medical settings, laying a foundation for future research in this
interdisciplinary domain.
</p>
</div>
</dd>
<dt><a name="item796">[796]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03094" title="Abstract">arXiv:2402.03094</a> [<a href="/pdf/2402.03094" title="Download PDF">pdf</a>, <a href="/ps/2402.03094" title="Download PostScript">ps</a>, <a href="/format/2402.03094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object  Detector
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yuqian Fu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yixuan Pan</a>, 
<a href="/search/cs?searchtype=author&query=Huai%2C+L">Lian Huai</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xingyu Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Shangguan%2C+Z">Zeyu Shangguan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingjie Kong</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yanwei Fu</a>, 
<a href="/search/cs?searchtype=author&query=Van+Gool%2C+L">Luc Van Gool</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xingqun Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper addresses the challenge of cross-domain few-shot object detection
(CD-FSOD), aiming to develop an accurate object detector for novel domains with
minimal labeled examples. While transformer-based open-set detectors e.g.,
DE-ViT~\cite{zhang2023detect} have excelled in both open-vocabulary object
detection and traditional few-shot object detection, detecting categories
beyond those seen during training, we thus naturally raise two key questions:
1) can such open-set detection methods easily generalize to CD-FSOD? 2) If no,
how to enhance the results of open-set methods when faced with significant
domain gaps? To address the first question, we introduce several metrics to
quantify domain variances and establish a new CD-FSOD benchmark with diverse
domain metric values. Some State-Of-The-Art (SOTA) open-set object detection
methods are evaluated on this benchmark, with evident performance degradation
observed across out-of-domain datasets. This indicates the failure of adopting
open-set detectors directly for CD-FSOD. Sequentially, to overcome the
performance degradation issue and also to answer the second proposed question,
we endeavor to enhance the vanilla DE-ViT. With several novel components
including finetuning, a learnable prototype module, and a lightweight attention
module, we present an improved Cross-Domain Vision Transformer for CD-FSOD
(CD-ViTO). Experiments show that our CD-ViTO achieves impressive results on
both out-of-domain and in-domain target datasets, establishing new SOTAs for
both CD-FSOD and FSOD. All the datasets, codes, and models will be released to
the community.
</p>
</div>
</dd>
<dt><a name="item797">[797]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03095" title="Abstract">arXiv:2402.03095</a> [<a href="/pdf/2402.03095" title="Download PDF">pdf</a>, <a href="/format/2402.03095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transcending Adversarial Perturbations: Manifold-Aided Adversarial  Examples with Legitimate Semantics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuai Li</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xiaoyu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaoguang Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep neural networks were significantly vulnerable to adversarial examples
manipulated by malicious tiny perturbations. Although most conventional
adversarial attacks ensured the visual imperceptibility between adversarial
examples and corresponding raw images by minimizing their geometric distance,
these constraints on geometric distance led to limited attack transferability,
inferior visual quality, and human-imperceptible interpretability. In this
paper, we proposed a supervised semantic-transformation generative model to
generate adversarial examples with real and legitimate semantics, wherein an
unrestricted adversarial manifold containing continuous semantic variations was
constructed for the first time to realize a legitimate transition from
non-adversarial examples to adversarial ones. Comprehensive experiments on
MNIST and industrial defect datasets showed that our adversarial examples not
only exhibited better visual quality but also achieved superior attack
transferability and more effective explanations for model vulnerabilities,
indicating their great potential as generic adversarial examples. The code and
pre-trained models were available at https://github.com/shuaili1027/MAELS.git.
</p>
</div>
</dd>
<dt><a name="item798">[798]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03099" title="Abstract">arXiv:2402.03099</a> [<a href="/pdf/2402.03099" title="Download PDF">pdf</a>, <a href="/format/2402.03099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intent-based Prompt Calibration: Enhancing prompt optimization with  synthetic boundary cases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Levi%2C+E">Elad Levi</a>, 
<a href="/search/cs?searchtype=author&query=Brosh%2C+E">Eli Brosh</a>, 
<a href="/search/cs?searchtype=author&query=Friedmann%2C+M">Matan Friedmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Prompt engineering is a challenging and important task due to the high
sensitivity of Large Language Models (LLMs) to the given prompt and the
inherent ambiguity of a textual task instruction. Automatic prompt engineering
is essential to achieve optimized performance from LLMs. Recent studies have
demonstrated the capabilities of LLMs to automatically conduct prompt
engineering by employing a meta-prompt that incorporates the outcomes of the
last trials and proposes an improved prompt. However, this requires a
high-quality benchmark to compare different prompts, which is difficult and
expensive to acquire in many real-world use cases. In this work, we introduce a
new method for automatic prompt engineering, using a calibration process that
iteratively refines the prompt to the user intent. During the optimization
process, the system jointly generates synthetic data of boundary use cases and
optimizes the prompt according to the generated dataset. We demonstrate the
effectiveness of our method with respect to strong proprietary models on
real-world tasks such as moderation and generation. Our method outperforms
state-of-the-art methods with a limited number of annotated samples.
Furthermore, we validate the advantages of each one of the system's key
components. Our system is built in a modular way, facilitating easy adaptation
to other tasks. The code is available
$\href{https://github.com/Eladlev/AutoPrompt}{here}$.
</p>
</div>
</dd>
<dt><a name="item799">[799]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03103" title="Abstract">arXiv:2402.03103</a> [<a href="/pdf/2402.03103" title="Download PDF">pdf</a>, <a href="/ps/2402.03103" title="Download PostScript">ps</a>, <a href="/format/2402.03103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scoped Effects as Parameterized Algebraic Theories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lindley%2C+S">Sam Lindley</a>, 
<a href="/search/cs?searchtype=author&query=Matache%2C+C">Cristina Matache</a>, 
<a href="/search/cs?searchtype=author&query=Moss%2C+S">Sean Moss</a>, 
<a href="/search/cs?searchtype=author&query=Staton%2C+S">Sam Staton</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+N">Nicolas Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhixuan Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Logic in Computer Science (cs.LO); Category Theory (math.CT)

</div>
<p class="mathjax">Notions of computation can be modelled by monads. Algebraic effects offer a
characterization of monads in terms of algebraic operations and equational
axioms, where operations are basic programming features, such as reading or
updating the state, and axioms specify observably equivalent expressions.
However, many useful programming features depend on additional mechanisms such
as delimited scopes or dynamically allocated resources. Such mechanisms can be
supported via extensions to algebraic effects including scoped effects and
parameterized algebraic theories. We present a fresh perspective on scoped
effects by translation into a variation of parameterized algebraic theories.
The translation enables a new approach to equational reasoning for scoped
effects and gives rise to an alternative characterization of monads in terms of
generators and equations involving both scoped and algebraic operations. We
demonstrate the power of our fresh perspective by way of equational
characterizations of several known models of scoped effects.
</p>
</div>
</dd>
<dt><a name="item800">[800]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03106" title="Abstract">arXiv:2402.03106</a> [<a href="/pdf/2402.03106" title="Download PDF">pdf</a>, <a href="/format/2402.03106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DARTS: Diffusion Approximated Residual Time Sampling for Low Variance  Time-of-flight Rendering in Homogeneous Scattering Medium
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Q">Qianyue He</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xin Jin</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Haitian Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+D">Dongyu Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">Time-of-flight (ToF) devices have greatly propelled the advancement of
various multi-modal perception applications. However, due to complexity in both
sampling path construction and vertex connection in time domain, it is
extremely challenging to accurately render time-resolved information in ToF
device simulation, particularly in scenes involving complex geometric
structures, diverse materials and volumetric scattering media. Existing works
either exhibit significant bias or variance in ToF rendering tasks or prove
ineffective in scenes involving participating media and camera-warped settings.
To address this challenge, in this paper, we integrate the transient diffusion
theory into path construction to generate the unbiased full transport of
time-resolved radiance. Additionally, we devise an elliptical sampling method
to provide controllable vertex connection satisfying any required photon
traversal time. To our knowledge, our work is the first to explore importance
sampling according to transient radiance, enabling temporal path construction
of higher quality in multiple scattering settings. Extensive experiments show
that our sampling method can significantly improve both quality and efficiency
of ToF rendering within both path tracing and photon-based frameworks, with at
least a 5x MSE reduction versus SOTA methods in equal rendering time. Our
method introduces no memory overhead and negligible extra computation compared
to the boost in speed, providing a straightforward plug-in for various existing
rendering frameworks.
</p>
</div>
</dd>
<dt><a name="item801">[801]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03109" title="Abstract">arXiv:2402.03109</a> [<a href="/pdf/2402.03109" title="Download PDF">pdf</a>, <a href="/format/2402.03109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing with Clocks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Edwards%2C+J">Jonathan Edwards</a>, 
<a href="/search/cs?searchtype=author&query=Yakovlev%2C+A">Alex Yakovlev</a>, 
<a href="/search/cs?searchtype=author&query=O%27Keefe%2C+S">Simon O&#x27;Keefe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">Clocks are a central part of many computing paradigms, and are mainly used to
synchronise the delicate operation of switching, necessary to drive modern
computational processes. Unfortunately, this synchronisation process is
reaching a natural ``apocalypse''. No longer can clock scaling be used as a
blunt tool to accelerate computation, we are up against the natural limits of
switching and synchronisation across large processors. Therefore, we need to
rethink how time is utilised in computation, using it more naturally in the
role of representing data. This can be achieved by using a time interval
delineated by discrete start and end events, and by re-casting computational
operations into the time domain. With this, computer systems can be developed
that are naturally scaleable in time and space, and can use ambient time
references built to the best effort of the available technology.
<br />Our ambition is to better manage the energy/computation time trade-off, and
to explicitly embed the resolution of the data in the time domain. We aim to
recast calculations into the ``for free'' format that time offers, and in
addition, perform these calculations at the highest clock or oscillator
resolution possible.
</p>
</div>
</dd>
<dt><a name="item802">[802]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03110" title="Abstract">arXiv:2402.03110</a> [<a href="/pdf/2402.03110" title="Download PDF">pdf</a>, <a href="/format/2402.03110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-Stationary Latent Auto-Regressive Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Trella%2C+A+L">Anna L. Trella</a>, 
<a href="/search/cs?searchtype=author&query=Dempsey%2C+W">Walter Dempsey</a>, 
<a href="/search/cs?searchtype=author&query=Doshi-Velez%2C+F">Finale Doshi-Velez</a>, 
<a href="/search/cs?searchtype=author&query=Murphy%2C+S+A">Susan A. Murphy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We consider the stochastic multi-armed bandit problem with non-stationary
rewards. We present a novel formulation of non-stationarity in the environment
where changes in the mean reward of the arms over time are due to some unknown,
latent, auto-regressive (AR) state of order $k$. We call this new environment
the latent AR bandit. Different forms of the latent AR bandit appear in many
real-world settings, especially in emerging scientific fields such as
behavioral health or education where there are few mechanistic models of the
environment. If the AR order $k$ is known, we propose an algorithm that
achieves $\tilde{O}(k\sqrt{T})$ regret in this setting. Empirically, our
algorithm outperforms standard UCB across multiple non-stationary environments,
even if $k$ is mis-specified.
</p>
</div>
</dd>
<dt><a name="item803">[803]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03111" title="Abstract">arXiv:2402.03111</a> [<a href="/pdf/2402.03111" title="Download PDF">pdf</a>, <a href="/ps/2402.03111" title="Download PostScript">ps</a>, <a href="/format/2402.03111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing roadmaps in unbounded smooth real algebraic sets II: algorithm  and complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pr%C3%A9bet%2C+R">R&#xe9;mi Pr&#xe9;bet</a>, 
<a href="/search/cs?searchtype=author&query=Din%2C+M+S+E">Mohab Safey El Din</a>, 
<a href="/search/cs?searchtype=author&query=Schost%2C+%C3%89">&#xc9;ric Schost</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 60 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Symbolic Computation (cs.SC)</span>; Algebraic Geometry (math.AG)

</div>
<p class="mathjax">A roadmap for an algebraic set $V$ defined by polynomials with coefficients
in some real field, say $\mathbb{R}$, is an algebraic curve contained in $V$
whose intersection with all connected components of $V\cap\mathbb{R}^{n}$ is
connected. These objects, introduced by Canny, can be used to answer
connectivity queries over $V\cap \mathbb{R}^{n}$ provided that they are
required to contain the finite set of query points $\mathcal{P}\subset V$; in
this case,we say that the roadmap is associated to $(V, \mathcal{P})$.
<br />In this paper, we make effective a connectivity result we previously proved,
to design a Monte Carlo algorithm which, on input (i) a finite sequence of
polynomials defining $V$ (and satisfying some regularity assumptions) and (ii)
an algebraic representation of finitely many query points $\mathcal{P}$ in $V$,
computes a roadmap for $(V, \mathcal{P})$. This algorithm generalizes the
nearly optimal one introduced by the last two authors by dropping a boundedness
assumption on the real trace of $V$.
<br />The output size and running times of our algorithm are both polynomial in
$(nD)^{n\log d}$, where $D$ is the maximal degree of the input equations and
$d$ is the dimension of $V$. As far as we know, the best previously known
algorithm dealing with such sets has an output size and running time polynomial
in $(nD)^{n\log^2 n}$.
</p>
</div>
</dd>
<dt><a name="item804">[804]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03112" title="Abstract">arXiv:2402.03112</a> [<a href="/pdf/2402.03112" title="Download PDF">pdf</a>, <a href="/format/2402.03112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Infrared Spectra Prediction for Diazo Groups Utilizing a Machine  Learning Approach with Structural Attention Mechanism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chengchun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+F">Fanyang Mo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Chemical Physics (physics.chem-ph)

</div>
<p class="mathjax">Infrared (IR) spectroscopy is a pivotal technique in chemical research for
elucidating molecular structures and dynamics through vibrational and
rotational transitions. However, the intricate molecular fingerprints
characterized by unique vibrational and rotational patterns present substantial
analytical challenges. Here, we present a machine learning approach employing a
Structural Attention Mechanism tailored to enhance the prediction and
interpretation of infrared spectra, particularly for diazo compounds. Our model
distinguishes itself by honing in on chemical information proximal to
functional groups, thereby significantly bolstering the accuracy, robustness,
and interpretability of spectral predictions. This method not only demystifies
the correlations between infrared spectral features and molecular structures
but also offers a scalable and efficient paradigm for dissecting complex
molecular interactions.
</p>
</div>
</dd>
<dt><a name="item805">[805]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03114" title="Abstract">arXiv:2402.03114</a> [<a href="/pdf/2402.03114" title="Download PDF">pdf</a>, <a href="/format/2402.03114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Augmenting Security and Privacy in the Virtual Realm: An Analysis of  Extended Reality Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cayir%2C+D">Derin Cayir</a>, 
<a href="/search/cs?searchtype=author&query=Acar%2C+A">Abbas Acar</a>, 
<a href="/search/cs?searchtype=author&query=Lazzeretti%2C+R">Riccardo Lazzeretti</a>, 
<a href="/search/cs?searchtype=author&query=Angelini%2C+M">Marco Angelini</a>, 
<a href="/search/cs?searchtype=author&query=Conti%2C+M">Mauro Conti</a>, 
<a href="/search/cs?searchtype=author&query=Uluagac%2C+S">Selcuk Uluagac</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is the author's version of the work. It is posted here for personal/educational use only. The definitive version was published in IEEE Security &amp; Privacy Magazine Jan/Feb 2024
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> in IEEE Security &amp; Privacy, vol. 22, no. 01, pp. 10-23, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">In this work, we present a device-centric analysis of security and privacy
attacks and defenses on Extended Reality (XR) devices, highlighting the need
for robust and privacy-aware security mechanisms. Based on our analysis, we
present future research directions and propose design considerations to help
ensure the security and privacy of XR devices.
</p>
</div>
</dd>
<dt><a name="item806">[806]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03115" title="Abstract">arXiv:2402.03115</a> [<a href="/pdf/2402.03115" title="Download PDF">pdf</a>, <a href="/format/2402.03115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering interpretable models of scientific image data with deep  learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soelistyo%2C+C+J">Christopher J. Soelistyo</a>, 
<a href="/search/cs?searchtype=author&query=Lowe%2C+A+R">Alan R. Lowe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages (including appendices), 27 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">How can we find interpretable, domain-appropriate models of natural phenomena
given some complex, raw data such as images? Can we use such models to derive
scientific insight from the data? In this paper, we propose some methods for
achieving this. In particular, we implement disentangled representation
learning, sparse deep neural network training and symbolic regression, and
assess their usefulness in forming interpretable models of complex image data.
We demonstrate their relevance to the field of bioimaging using a well-studied
test problem of classifying cell states in microscopy data. We find that such
methods can produce highly parsimonious models that achieve $\sim98\%$ of the
accuracy of black-box benchmark models, with a tiny fraction of the complexity.
We explore the utility of such interpretable models in producing scientific
explanations of the underlying biological phenomenon.
</p>
</div>
</dd>
<dt><a name="item807">[807]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03116" title="Abstract">arXiv:2402.03116</a> [<a href="/pdf/2402.03116" title="Download PDF">pdf</a>, <a href="/format/2402.03116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feature-Action Design Patterns for Storytelling Visualizations with Time  Series Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+S">Saiful Khan</a>, 
<a href="/search/cs?searchtype=author&query=Jones%2C+S">Scott Jones</a>, 
<a href="/search/cs?searchtype=author&query=Bach%2C+B">Benjamin Bach</a>, 
<a href="/search/cs?searchtype=author&query=Cha%2C+J">Jaehoon Cha</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Min Chen</a>, 
<a href="/search/cs?searchtype=author&query=Meikle%2C+J">Julie Meikle</a>, 
<a href="/search/cs?searchtype=author&query=Roberts%2C+J+C">Jonathan C Roberts</a>, 
<a href="/search/cs?searchtype=author&query=Thiyagalingam%2C+J">Jeyan Thiyagalingam</a>, 
<a href="/search/cs?searchtype=author&query=Wood%2C+J">Jo Wood</a>, 
<a href="/search/cs?searchtype=author&query=Ritsos%2C+P+D">Panagiotis D. Ritsos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present a method to create storytelling visualization with time series
data. Many personal decisions nowadays rely on access to dynamic data
regularly, as we have seen during the COVID-19 pandemic. It is thus desirable
to construct storytelling visualization for dynamic data that is selected by an
individual for a specific context. Because of the need to tell data-dependent
stories, predefined storyboards based on known data cannot accommodate dynamic
data easily nor scale up to many different individuals and contexts. Motivated
initially by the need to communicate time series data during the COVID-19
pandemic, we developed a novel computer-assisted method for meta-authoring of
stories, which enables the design of storyboards that include feature-action
patterns in anticipation of potential features that may appear in dynamically
arrived or selected data. In addition to meta-storyboards involving COVID-19
data, we also present storyboards for telling stories about progress in a
machine learning workflow. Our approach is complementary to traditional methods
for authoring storytelling visualization, and provides an efficient means to
construct data-dependent storyboards for different data-streams of similar
contexts.
</p>
</div>
</dd>
<dt><a name="item808">[808]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03119" title="Abstract">arXiv:2402.03119</a> [<a href="/pdf/2402.03119" title="Download PDF">pdf</a>, <a href="/format/2402.03119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Good Teachers Explain: Explanation-Enhanced Knowledge Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parchami-Araghi%2C+A">Amin Parchami-Araghi</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%B6hle%2C+M">Moritz B&#xf6;hle</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+S">Sukrut Rao</a>, 
<a href="/search/cs?searchtype=author&query=Schiele%2C+B">Bernt Schiele</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Knowledge Distillation (KD) has proven effective for compressing large
teacher models into smaller student models. While it is well known that student
models can achieve similar accuracies as the teachers, it has also been shown
that they nonetheless often do not learn the same function. It is, however,
often highly desirable that the student's and teacher's functions share similar
properties such as basing the prediction on the same input features, as this
ensures that students learn the 'right features' from the teachers. In this
work, we explore whether this can be achieved by not only optimizing the
classic KD loss but also the similarity of the explanations generated by the
teacher and the student. Despite the idea being simple and intuitive, we find
that our proposed 'explanation-enhanced' KD (e$^2$KD) (1) consistently provides
large gains in terms of accuracy and student-teacher agreement, (2) ensures
that the student learns from the teacher to be right for the right reasons and
to give similar explanations, and (3) is robust with respect to the model
architectures, the amount of training data, and even works with 'approximate',
pre-computed explanations.
</p>
</div>
</dd>
<dt><a name="item809">[809]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03124" title="Abstract">arXiv:2402.03124</a> [<a href="/pdf/2402.03124" title="Download PDF">pdf</a>, <a href="/format/2402.03124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanbo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jian Liang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Ran He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR2024 poster The prior submission version had a bug in the image reconstruction implementation, which has been corrected without harm to the main conclusions
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Gradient inversion attacks aim to reconstruct local training data from
intermediate gradients exposed in the federated learning framework. Despite
successful attacks, all previous methods, starting from reconstructing a single
data point and then relaxing the single-image limit to batch level, are only
tested under hard label constraints. Even for single-image reconstruction, we
still lack an analysis-based algorithm to recover augmented soft labels. In
this work, we change the focus from enlarging batchsize to investigating the
hard label constraints, considering a more realistic circumstance where label
smoothing and mixup techniques are used in the training process. In particular,
we are the first to initiate a novel algorithm to simultaneously recover the
ground-truth augmented label and the input feature of the last fully-connected
layer from single-input gradients, and provide a necessary condition for any
analytical-based label recovery methods. Extensive experiments testify to the
label recovery accuracy, as well as the benefits to the following image
reconstruction. We believe soft labels in classification tasks are worth
further attention in gradient inversion attacks.
</p>
</div>
</dd>
<dt><a name="item810">[810]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03125" title="Abstract">arXiv:2402.03125</a> [<a href="/pdf/2402.03125" title="Download PDF">pdf</a>, <a href="/format/2402.03125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shape Manipulation of Bevel-Tip Needles for Prostate Biopsy Procedures:  A Comparison of Two Resolved-Rate Controllers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanzhou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Al-Zogbi%2C+L">Lidia Al-Zogbi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiawei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shepard%2C+L">Lauren Shepard</a>, 
<a href="/search/cs?searchtype=author&query=Ghazi%2C+A">Ahmed Ghazi</a>, 
<a href="/search/cs?searchtype=author&query=Tokuda%2C+J">Junichi Tokuda</a>, 
<a href="/search/cs?searchtype=author&query=Leonard%2C+S">Simon Leonard</a>, 
<a href="/search/cs?searchtype=author&query=Krieger%2C+A">Axel Krieger</a>, 
<a href="/search/cs?searchtype=author&query=Iordachita%2C+I">Iulian Iordachita</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Prostate cancer diagnosis continues to encounter challenges, often due to
imprecise needle placement in standard biopsies. Several control strategies
have been developed to compensate for needle tip prediction inaccuracies,
however none were compared against each other, and it is unclear whether any of
them can be safely and universally applied in clinical settings. This paper
compares the performance of two resolved-rate controllers, derived from a
mechanics-based and a data-driven approach, for bevel-tip needle control using
needle shape manipulation through a template. We demonstrate for a simulated
12-core biopsy procedure under model parameter uncertainty that the
mechanics-based controller can better reach desired targets when only the final
goal configuration is presented even with uncertainty on model parameters
estimation, and that providing a feasible needle path is crucial in ensuring
safe surgical outcomes when either controller is used for needle shape
manipulation.
</p>
</div>
</dd>
<dt><a name="item811">[811]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03126" title="Abstract">arXiv:2402.03126</a> [<a href="/pdf/2402.03126" title="Download PDF">pdf</a>, <a href="/format/2402.03126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Free is Parameter-Free Stochastic Optimization?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Attia%2C+A">Amit Attia</a>, 
<a href="/search/cs?searchtype=author&query=Koren%2C+T">Tomer Koren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the problem of parameter-free stochastic optimization, inquiring
whether, and under what conditions, do fully parameter-free methods exist:
these are methods that achieve convergence rates competitive with optimally
tuned methods, without requiring significant knowledge of the true problem
parameters. Existing parameter-free methods can only be considered
``partially'' parameter-free, as they require some non-trivial knowledge of the
true problem parameters, such as a bound on the stochastic gradient norms, a
bound on the distance to a minimizer, etc. In the non-convex setting, we
demonstrate that a simple hyperparameter search technique results in a fully
parameter-free method that outperforms more sophisticated state-of-the-art
algorithms. We also provide a similar result in the convex setting with access
to noisy function values under mild noise assumptions. Finally, assuming only
access to stochastic gradients, we establish a lower bound that renders fully
parameter-free stochastic convex optimization infeasible, and provide a method
which is (partially) parameter-free up to the limit indicated by our lower
bound.
</p>
</div>
</dd>
<dt><a name="item812">[812]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03130" title="Abstract">arXiv:2402.03130</a> [<a href="/pdf/2402.03130" title="Download PDF">pdf</a>, <a href="/ps/2402.03130" title="Download PostScript">ps</a>, <a href="/format/2402.03130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> User-Centric Evaluation of ChatGPT Capability of Generating R Program  Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miah%2C+T">Tanha Miah</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hong Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper has been submitted to the journal Electronics for consideration of publication. It is in the review process
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper reports an evaluation of ChatGPT's capability of generating R
programming language code from natural language input. A dataset specially
designed for generating R program code was constructed with metadata to support
scenario-based testing and evaluation of code generation capabilities in
various usage scenarios of different levels of difficulty and different types
of programs. The evaluation takes a multiple attempt process in which the
tester tries to complete the code generation task through a number of attempts
until a satisfactory solution is obtained or gives up after a fixed number of
maximal attempts. In each attempt the tester formulates a natural language
input to ChatGPT based on the previous results and the task to be completed. In
addition to the metrics of average numbers of attempts and average amount of
time taken to complete the tasks, the final generated solutions are then
assessed on a number of quality attributes, including accuracy, completeness,
conciseness, readability, well structuredness, logic clarity, depth of
ex-planation, and coverage of parameters. Our experiments demonstrated that
ChatGPT is in general highly capable of generating high quality R program code
as well as textual explanations although it may fail on hard programming tasks.
The experiment data also shows that human developers can hardly learn from
experiences naturally to improve the skill of using ChatGPT to generate code.
</p>
</div>
</dd>
<dt><a name="item813">[813]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03131" title="Abstract">arXiv:2402.03131</a> [<a href="/pdf/2402.03131" title="Download PDF">pdf</a>, <a href="/format/2402.03131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constrained Decoding for Cross-lingual Label Projection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+D+M">Duong Minh Le</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ritter%2C+A">Alan Ritter</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Zero-shot cross-lingual transfer utilizing multilingual LLMs has become a
popular learning paradigm for low-resource languages with no labeled training
data. However, for NLP tasks that involve fine-grained predictions on words and
phrases, the performance of zero-shot cross-lingual transfer learning lags far
behind supervised fine-tuning methods. Therefore, it is common to exploit
translation and label projection to further improve the performance by (1)
translating training data that is available in a high-resource language (e.g.,
English) together with the gold labels into low-resource languages, and/or (2)
translating test data in low-resource languages to a high-source language to
run inference on, then projecting the predicted span-level labels back onto the
original test data. However, state-of-the-art marker-based label projection
methods suffer from translation quality degradation due to the extra label
markers injected in the input to the translation model. In this work, we
explore a new direction that leverages constrained decoding for label
projection to overcome the aforementioned issues. Our new method not only can
preserve the quality of translated texts but also has the versatility of being
applicable to both translating training and translating test data strategies.
This versatility is crucial as our experiments reveal that translating test
data can lead to a considerable boost in performance compared to translating
only training data. We evaluate on two cross-lingual transfer tasks, namely
Named Entity Recognition and Event Argument Extraction, spanning 20 languages.
The results demonstrate that our approach outperforms the state-of-the-art
marker-based method by a large margin and also shows better performance than
other label projection methods that rely on external word alignment.
</p>
</div>
</dd>
<dt><a name="item814">[814]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03134" title="Abstract">arXiv:2402.03134</a> [<a href="/pdf/2402.03134" title="Download PDF">pdf</a>, <a href="/ps/2402.03134" title="Download PostScript">ps</a>, <a href="/format/2402.03134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Patch Topology in Univalent Foundations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arrieta%2C+I">Igor Arrieta</a>, 
<a href="/search/cs?searchtype=author&query=Escard%C3%B3%2C+M+H">Mart&#xed;n H&#xf6;tzel Escard&#xf3;</a>, 
<a href="/search/cs?searchtype=author&query=Tosun%2C+A">Ayberk Tosun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Stone locales together with continuous maps form a coreflective subcategory
of spectral locales and perfect maps. A proof in the internal language of an
elementary topos was previously given by the second-named author. This proof
can be easily translated to univalent type theory using resizing axioms. In
this work, we show how to achieve such a translation without resizing axioms,
by working with large and locally small frames with small bases. This requires
predicative reformulations of several fundamental concepts of locale theory in
predicative HoTT/UF, which we investigate systematically.
</p>
</div>
</dd>
<dt><a name="item815">[815]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03135" title="Abstract">arXiv:2402.03135</a> [<a href="/pdf/2402.03135" title="Download PDF">pdf</a>, <a href="/format/2402.03135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPU-Accelerated 3D Polygon Visibility Volumes for Synergistic Perception  and Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Willis%2C+A">Andrew Willis</a>, 
<a href="/search/cs?searchtype=author&query=Hague%2C+C">Collin Hague</a>, 
<a href="/search/cs?searchtype=author&query=Wolek%2C+A">Artur Wolek</a>, 
<a href="/search/cs?searchtype=author&query=Brink%2C+K">Kevin Brink</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computational Geometry (cs.CG); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">UAV missions often require specific geometric constraints to be satisfied
between ground locations and the vehicle location. Such requirements are
typical for contexts where line-of-sight must be maintained between the vehicle
location and the ground control location and are also important in surveillance
applications where the UAV wishes to be able to sense, e.g., with a camera
sensor, a specific region within a complex geometric environment. This problem
is further complicated when the ground location is generalized to a convex 2D
polygonal region. This article describes the theory and implementation of a
system which can quickly calculate the 3D volume that encloses all 3D
coordinates from which a 2D convex planar region can be entirely viewed;
referred to as a visibility volume. The proposed approach computes visibility
volumes using a combination of depth map computation using GPU-acceleration and
geometric boolean operations. Solutions to this problem require complex 3D
geometric analysis techniques that must execute using arbitrary precision
arithmetic on a collection of discontinuous and non-analytic surfaces.
Post-processing steps incorporate navigational constraints to further restrict
the enclosed coordinates to include both visibility and navigation constraints.
Integration of sensing visibility constraints with navigational constraints
yields a range of navigable space where a vehicle will satisfy both perceptual
sensing and navigational needs of the mission. This algorithm then provides a
synergistic perception and navigation sensitive solution yielding a volume of
coordinates in 3D that satisfy both the mission path and sensing needs.
</p>
</div>
</dd>
<dt><a name="item816">[816]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03136" title="Abstract">arXiv:2402.03136</a> [<a href="/pdf/2402.03136" title="Download PDF">pdf</a>, <a href="/format/2402.03136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mastering Zero-Shot Interactions in Cooperative and Competitive  Simultaneous Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahlau%2C+Y">Yannik Mahlau</a>, 
<a href="/search/cs?searchtype=author&query=Schubert%2C+F">Frederik Schubert</a>, 
<a href="/search/cs?searchtype=author&query=Rosenhahn%2C+B">Bodo Rosenhahn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">The combination of self-play and planning has achieved great successes in
sequential games, for instance in Chess and Go. However, adapting algorithms
such as AlphaZero to simultaneous games poses a new challenge. In these games,
missing information about concurrent actions of other agents is a limiting
factor as they may select different Nash equilibria or do not play optimally at
all. Thus, it is vital to model the behavior of the other agents when
interacting with them in simultaneous games. To this end, we propose Albatross:
AlphaZero for Learning Bounded-rational Agents and Temperature-based Response
Optimization using Simulated Self-play. Albatross learns to play the novel
equilibrium concept of a Smooth Best Response Logit Equilibrium (SBRLE), which
enables cooperation and competition with agents of any playing strength. We
perform an extensive evaluation of Albatross on a set of cooperative and
competitive simultaneous perfect-information games. In contrast to AlphaZero,
Albatross is able to exploit weak agents in the competitive game of
Battlesnake. Additionally, it yields an improvement of 37.6% compared to
previous state of the art in the cooperative Overcooked benchmark.
</p>
</div>
</dd>
<dt><a name="item817">[817]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03137" title="Abstract">arXiv:2402.03137</a> [<a href="/pdf/2402.03137" title="Download PDF">pdf</a>, <a href="/format/2402.03137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sociolinguistically Informed Interpretability: A Case Study on Hinglish  Emotion Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tatariya%2C+K">Kushal Tatariya</a>, 
<a href="/search/cs?searchtype=author&query=Lent%2C+H">Heather Lent</a>, 
<a href="/search/cs?searchtype=author&query=Bjerva%2C+J">Johannes Bjerva</a>, 
<a href="/search/cs?searchtype=author&query=de+Lhoneux%2C+M">Miryam de Lhoneux</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, Accepted to SIGTYP 2024 @ EACL
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Emotion classification is a challenging task in NLP due to the inherent
idiosyncratic and subjective nature of linguistic expression, especially with
code-mixed data. Pre-trained language models (PLMs) have achieved high
performance for many tasks and languages, but it remains to be seen whether
these models learn and are robust to the differences in emotional expression
across languages. Sociolinguistic studies have shown that Hinglish speakers
switch to Hindi when expressing negative emotions and to English when
expressing positive emotions. To understand if language models can learn these
associations, we study the effect of language on emotion prediction across 3
PLMs on a Hinglish emotion classification dataset. Using LIME and token level
language ID, we find that models do learn these associations between language
choice and emotional expression. Moreover, having code-mixed data present in
the pre-training can augment that learning when task-specific data is scarce.
We also conclude from the misclassifications that the models may overgeneralise
this heuristic to other infrequent examples where this sociolinguistic
phenomenon does not apply.
</p>
</div>
</dd>
<dt><a name="item818">[818]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03138" title="Abstract">arXiv:2402.03138</a> [<a href="/pdf/2402.03138" title="Download PDF">pdf</a>, <a href="/format/2402.03138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Just Cluster It: An Approach for Exploration in High-Dimensions using  Clustering and Pre-Trained Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wagner%2C+S+S">Stefan Sylvius Wagner</a>, 
<a href="/search/cs?searchtype=author&query=Harmeling%2C+S">Stefan Harmeling</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper we adopt a representation-centric perspective on exploration in
reinforcement learning, viewing exploration fundamentally as a density
estimation problem. We investigate the effectiveness of clustering
representations for exploration in 3-D environments, based on the observation
that the importance of pixel changes between transitions is less pronounced in
3-D environments compared to 2-D environments, where pixel changes between
transitions are typically distinct and significant. We propose a method that
performs episodic and global clustering on random representations and on
pre-trained DINO representations to count states, i.e, estimate pseudo-counts.
Surprisingly, even random features can be clustered effectively to count states
in 3-D environments, however when these become visually more complex,
pre-trained DINO representations are more effective thanks to the pre-trained
inductive biases in the representations. Overall, this presents a pathway for
integrating pre-trained biases into exploration. We evaluate our approach on
the VizDoom and Habitat environments, demonstrating that our method surpasses
other well-known exploration methods in these settings.
</p>
</div>
</dd>
<dt><a name="item819">[819]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03139" title="Abstract">arXiv:2402.03139</a> [<a href="/pdf/2402.03139" title="Download PDF">pdf</a>, <a href="/format/2402.03139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Neural Subset Selection: Integrating Background Information  into Set Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+B">Binghui Xie</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+Y">Yatao Bian</a>, 
<a href="/search/cs?searchtype=author&query=zhou%2C+K">Kaiwen zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yongqiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Peilin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+W">Wei Meng</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">James Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Learning neural subset selection tasks, such as compound selection in
AI-aided drug discovery, have become increasingly pivotal across diverse
applications. The existing methodologies in the field primarily concentrate on
constructing models that capture the relationship between utility function
values and subsets within their respective supersets. However, these approaches
tend to overlook the valuable information contained within the superset when
utilizing neural networks to model set functions. In this work, we address this
oversight by adopting a probabilistic perspective. Our theoretical findings
demonstrate that when the target value is conditioned on both the input set and
subset, it is essential to incorporate an \textit{invariant sufficient
statistic} of the superset into the subset of interest for effective learning.
This ensures that the output value remains invariant to permutations of the
subset and its corresponding superset, enabling identification of the specific
superset from which the subset originated. Motivated by these insights, we
propose a simple yet effective information aggregation module designed to merge
the representations of subsets and supersets from a permutation invariance
perspective. Comprehensive empirical evaluations across diverse tasks and
datasets validate the enhanced efficacy of our approach over conventional
methods, underscoring the practicality and potency of our proposed strategies
in real-world contexts.
</p>
</div>
</dd>
<dt><a name="item820">[820]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03141" title="Abstract">arXiv:2402.03141</a> [<a href="/pdf/2402.03141" title="Download PDF">pdf</a>, <a href="/format/2402.03141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Long-Delayed Reinforcement Learning with Auxiliary  Short-Delayed Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingyuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+S+S">Simon Sinong Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yixuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chung-Wei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+C">Chen Lv</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
<p class="mathjax">Reinforcement learning is challenging in delayed scenarios, a common
real-world situation where observations and interactions occur with delays.
State-of-the-art (SOTA) state-augmentation techniques either suffer from the
state-space explosion along with the delayed steps, or performance degeneration
in stochastic environments. To address these challenges, our novel
Auxiliary-Delayed Reinforcement Learning (AD-RL) leverages an auxiliary
short-delayed task to accelerate the learning on a long-delayed task without
compromising the performance in stochastic environments. Specifically, AD-RL
learns the value function in the short-delayed task and then employs it with
the bootstrapping and policy improvement techniques in the long-delayed task.
We theoretically show that this can greatly reduce the sample complexity
compared to directly learning on the original long-delayed task. On
deterministic and stochastic benchmarks, our method remarkably outperforms the
SOTAs in both sample efficiency and policy performance.
</p>
</div>
</dd>
<dt><a name="item821">[821]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03142" title="Abstract">arXiv:2402.03142</a> [<a href="/pdf/2402.03142" title="Download PDF">pdf</a>, <a href="/format/2402.03142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Less is KEN: a Universal and Simple Non-Parametric Pruning Algorithm for  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mastromattei%2C+M">Michele Mastromattei</a>, 
<a href="/search/cs?searchtype=author&query=Zanzotto%2C+F+M">Fabio Massimo Zanzotto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Neural network pruning has become increasingly crucial due to the complexity
of neural network models and their widespread use in various fields. Existing
pruning algorithms often suffer from limitations such as architecture
specificity, excessive complexity and reliance on complex calculations,
rendering them impractical for real-world applications. In this paper, we
propose KEN: a straightforward, universal and unstructured pruning algorithm
based on Kernel Density Estimation (KDE). KEN aims to construct optimized
transformer models by selectively preserving the most significant parameters
while restoring others to their pre-training state. This approach maintains
model performance while allowing storage of only the optimized subnetwork,
leading to significant memory savings. Extensive evaluations on seven
transformer models demonstrate that KEN achieves equal or better performance
than the original models with a minimum parameter reduction of 25%. In-depth
comparisons against other pruning and PEFT algorithms confirm KEN
effectiveness. Furthermore, we introduce KEN_viz, an explainable tool that
visualizes the optimized model composition and the subnetwork selected by KEN.
</p>
</div>
</dd>
<dt><a name="item822">[822]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03144" title="Abstract">arXiv:2402.03144</a> [<a href="/pdf/2402.03144" title="Download PDF">pdf</a>, <a href="/ps/2402.03144" title="Download PostScript">ps</a>, <a href="/format/2402.03144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing Generic Fibres of Polynomial Ideals with FGLM and Hensel  Lifting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berthomieu%2C+J">J&#xe9;r&#xe9;my Berthomieu</a>, 
<a href="/search/cs?searchtype=author&query=Mohr%2C+R">Rafael Mohr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Symbolic Computation (cs.SC)</span>; Commutative Algebra (math.AC)

</div>
<p class="mathjax">We describe a version of the FGLM algorithm that can be used to compute
generic fibers of positive-dimensional polynomial ideals. It combines the FGLM
algorithm with a Hensel lifting strategy. We show that this algorithm has a
complexity quasi-linear in the number of lifting steps. Some provided
experimental data also demonstrates the practical efficacy of our algorithm.
Additionally, we sketch a related Hensel lifting method to compute Gr\"obner
bases using so-called tracers.
</p>
</div>
</dd>
<dt><a name="item823">[823]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03145" title="Abstract">arXiv:2402.03145</a> [<a href="/pdf/2402.03145" title="Download PDF">pdf</a>, <a href="/ps/2402.03145" title="Download PostScript">ps</a>, <a href="/format/2402.03145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SafEDMD: A certified learning architecture tailored to data-driven  control of nonlinear dynamical systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Str%C3%A4sser%2C+R">Robin Str&#xe4;sser</a>, 
<a href="/search/eess?searchtype=author&query=Schaller%2C+M">Manuel Schaller</a>, 
<a href="/search/eess?searchtype=author&query=Worthmann%2C+K">Karl Worthmann</a>, 
<a href="/search/eess?searchtype=author&query=Berberich%2C+J">Julian Berberich</a>, 
<a href="/search/eess?searchtype=author&query=Allg%C3%B6wer%2C+F">Frank Allg&#xf6;wer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">The Koopman operator serves as the theoretical backbone for machine learning
of dynamical control systems, where the operator is heuristically approximated
by extended dynamic mode decomposition (EDMD). In this paper, we propose
Stability- and certificate-oriented EDMD (SafEDMD): a novel EDMD-based learning
architecture which comes along with rigorous certificates, resulting in a
reliable surrogate model generated in a data-driven fashion. To ensure
trustworthiness of SafEDMD, we derive proportional error bounds, which vanish
at the origin and are tailored for control tasks, leading to certified
controller design based on semi-definite programming. We illustrate the
developed machinery by means of several benchmark examples and highlight the
advantages over state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item824">[824]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03146" title="Abstract">arXiv:2402.03146</a> [<a href="/pdf/2402.03146" title="Download PDF">pdf</a>, <a href="/format/2402.03146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multi-step Loss Function for Robust Learning of the Dynamics in  Model-based Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Benechehab%2C+A">Abdelhakim Benechehab</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+A">Albert Thomas</a>, 
<a href="/search/cs?searchtype=author&query=Paolo%2C+G">Giuseppe Paolo</a>, 
<a href="/search/cs?searchtype=author&query=Filippone%2C+M">Maurizio Filippone</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%A9gl%2C+B">Bal&#xe1;zs K&#xe9;gl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In model-based reinforcement learning, most algorithms rely on simulating
trajectories from one-step models of the dynamics learned on data. A critical
challenge of this approach is the compounding of one-step prediction errors as
the length of the trajectory grows. In this paper we tackle this issue by using
a multi-step objective to train one-step models. Our objective is a weighted
sum of the mean squared error (MSE) loss at various future horizons. We find
that this new loss is particularly useful when the data is noisy (additive
Gaussian noise in the observations), which is often the case in real-life
environments. To support the multi-step loss, first we study its properties in
two tractable cases: i) uni-dimensional linear system, and ii) two-parameter
non-linear system. Second, we show in a variety of tasks (environments or
datasets) that the models learned with this loss achieve a significant
improvement in terms of the averaged R2-score on future prediction horizons.
Finally, in the pure batch reinforcement learning setting, we demonstrate that
one-step models serve as strong baselines when dynamics are deterministic,
while multi-step models would be more advantageous in the presence of noise,
highlighting the potential of our approach in real-world applications.
</p>
</div>
</dd>
<dt><a name="item825">[825]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03147" title="Abstract">arXiv:2402.03147</a> [<a href="/pdf/2402.03147" title="Download PDF">pdf</a>, <a href="/format/2402.03147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Scams Using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Liming Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have gained prominence in various applications,
including security. This paper explores the utility of LLMs in scam detection,
a critical aspect of cybersecurity. Unlike traditional applications, we propose
a novel use case for LLMs to identify scams, such as phishing, advance fee
fraud, and romance scams. We present notable security applications of LLMs and
discuss the unique challenges posed by scams. Specifically, we outline the key
steps involved in building an effective scam detector using LLMs, emphasizing
data collection, preprocessing, model selection, training, and integration into
target systems. Additionally, we conduct a preliminary evaluation using GPT-3.5
and GPT-4 on a duplicated email, highlighting their proficiency in identifying
common signs of phishing or scam emails. The results demonstrate the models'
effectiveness in recognizing suspicious elements, but we emphasize the need for
a comprehensive assessment across various language tasks. The paper concludes
by underlining the importance of ongoing refinement and collaboration with
cybersecurity experts to adapt to evolving threats.
</p>
</div>
</dd>
<dt><a name="item826">[826]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03148" title="Abstract">arXiv:2402.03148</a> [<a href="/pdf/2402.03148" title="Download PDF">pdf</a>, <a href="/ps/2402.03148" title="Download PostScript">ps</a>, <a href="/format/2402.03148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proof Theory and Decision Procedures for Deontic STIT Logics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyon%2C+T+S">Tim S. Lyon</a>, 
<a href="/search/cs?searchtype=author&query=van+Berkel%2C+K">Kees van Berkel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
<p class="mathjax">This paper addresses the automation of reasoning with deontic STIT logics by
means of proof theory. Our methodology consists of leveraging sound and
cut-free complete sequent-style calculi to write a proof-search algorithm
deciding deontic, multi-agent STIT logics with (un)limited choice. In order to
ensure the termination of our proof-search algorithm, we introduce a special
loop-checking mechanism. Despite the acknowledged potential for deontic
reasoning in the context of autonomous vehicles and other areas of AI, this
work is the first to provide a syntactic decision procedure for deontic STIT
logics. Our proof-search procedures are designed to provide verifiable
witnesses/certificates of the (in)validity of formulae, which permit an
analysis of the (non)theoremhood of formulae and act as explanations thereof.
We utilize our proof-search algorithm to address agent-based normative
reasoning tasks such as compliance checking.
</p>
</div>
</dd>
<dt><a name="item827">[827]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03149" title="Abstract">arXiv:2402.03149</a> [<a href="/pdf/2402.03149" title="Download PDF">pdf</a>, <a href="/format/2402.03149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparative Analysis of Microrings Based Incoherent Photonic GEMM  Accelerators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vatsavai%2C+S+S">Sairam Sri Vatsavai</a>, 
<a href="/search/cs?searchtype=author&query=Karempudi%2C+V+S+P">Venkata Sai Praneeth Karempudi</a>, 
<a href="/search/cs?searchtype=author&query=Oluwaseun%2C+A">Alo Oluwaseun</a>, 
<a href="/search/cs?searchtype=author&query=Thakkar%2C+I">Ishan Thakkar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ISQED 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Emerging Technologies (cs.ET); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Several microring resonator (MRR) based analog photonic architectures have
been proposed to accelerate general matrix-matrix multiplications (GEMMs) in
deep neural networks with exceptional throughput and energy efficiency. To
implement GEMM functions, these MRR-based architectures, in general, manipulate
optical signals in five different ways: (i) Splitting (copying) of multiple
optical signals to achieve a certain fan-out, (ii) Aggregation (multiplexing)
of multiple optical signals to achieve a certain fan-in, (iii) Modulation of
optical signals to imprint input values onto analog signal amplitude, (iv)
Weighting of modulated optical signals to achieve analog input-weight
multiplication, (v) Summation of optical signals. The MRR-based GEMM
accelerators undertake the first four ways of signal manipulation in an
arbitrary order ignoring the possible impact of the order of these
manipulations on their performance. In this paper, we conduct a detailed
analysis of accelerator organizations with three different orders of these
manipulations: (1) Modulation-Aggregation-Splitting-Weighting (MASW), (2)
Aggregation-Splitting-Modulation-Weighting (ASMW), and (3)
Splitting-Modulation-Weighting-Aggregation (SMWA). We show that these
organizations affect the crosstalk noise and optical signal losses in different
magnitudes, which renders these organizations with different levels of
processing parallelism at the circuit level, and different magnitudes of
throughput and energy-area efficiency at the system level. Our evaluation
results for four CNN models show that SMWA organization achieves up to
4.4$\times$, 5$\times$, and 5.2$\times$ better throughput, energy efficiency,
and area-energy efficiency, respectively, compared to ASMW and MASW
organizations on average.
</p>
</div>
</dd>
<dt><a name="item828">[828]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03153" title="Abstract">arXiv:2402.03153</a> [<a href="/pdf/2402.03153" title="Download PDF">pdf</a>, <a href="/ps/2402.03153" title="Download PostScript">ps</a>, <a href="/format/2402.03153" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning solutions of parametric Navier-Stokes with physics-informed  neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naderibeni%2C+M">M.Naderibeni</a> (1), 
<a href="/search/cs?searchtype=author&query=Reinders%2C+M+J+T">M. J.T. Reinders</a> (1), 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">L. Wu</a> (2), 
<a href="/search/cs?searchtype=author&query=Tax%2C+D+M+J">D. M.J. Tax</a> (1),  ((1) Pattern Recognition and Bio-informatics Group, Delft University of Technology, (2) Science, Research and Innovation, dsm-firmenich)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We leverage Physics-Informed Neural Networks (PINNs) to learn solution
functions of parametric Navier-Stokes Equations (NSE). Our proposed approach
results in a feasible optimization problem setup that bypasses PINNs'
limitations in converging to solutions of highly nonlinear parametric-PDEs like
NSE. We consider the parameter(s) of interest as inputs of PINNs along with
spatio-temporal coordinates, and train PINNs on generated numerical solutions
of parametric-PDES for instances of the parameters. We perform experiments on
the classical 2D flow past cylinder problem aiming to learn velocities and
pressure functions over a range of Reynolds numbers as parameter of interest.
Provision of training data from generated numerical simulations allows for
interpolation of the solution functions for a range of parameters. Therefore,
we compare PINNs with unconstrained conventional Neural Networks (NN) on this
problem setup to investigate the effectiveness of considering the PDEs
regularization in the loss function. We show that our proposed approach results
in optimizing PINN models that learn the solution functions while making sure
that flow predictions are in line with conservational laws of mass and
momentum. Our results show that PINN results in accurate prediction of
gradients compared to NN model, this is clearly visible in predicted vorticity
fields given that none of these models were trained on vorticity labels.
</p>
</div>
</dd>
<dt><a name="item829">[829]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03156" title="Abstract">arXiv:2402.03156</a> [<a href="/pdf/2402.03156" title="Download PDF">pdf</a>, <a href="/format/2402.03156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DogSurf: Quadruped Robot Capable of GRU-based Surface Recognition for  Blind Person Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bazhenov%2C+A">Artem Bazhenov</a>, 
<a href="/search/cs?searchtype=author&query=Berman%2C+V">Vladimir Berman</a>, 
<a href="/search/cs?searchtype=author&query=Satsevich%2C+S">Sergei Satsevich</a>, 
<a href="/search/cs?searchtype=author&query=Shalopanova%2C+O">Olga Shalopanova</a>, 
<a href="/search/cs?searchtype=author&query=Cabrera%2C+M+A">Miguel Altamirano Cabrera</a>, 
<a href="/search/cs?searchtype=author&query=Lykov%2C+A">Artem Lykov</a>, 
<a href="/search/cs?searchtype=author&query=Tsetserukou%2C+D">Dzmitry Tsetserukou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for publication at the HRI2024 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper introduces DogSurf - a newapproach of using quadruped robots to
help visually impaired people navigate in real world. The presented method
allows the quadruped robot to detect slippery surfaces, and to use audio and
haptic feedback to inform the user when to stop. A state-of-the-art GRU-based
neural network architecture with mean accuracy of 99.925% was proposed for the
task of multiclass surface classification for quadruped robots. A dataset was
collected on a Unitree Go1 Edu robot. The dataset and code have been posted to
the public domain.
</p>
</div>
</dd>
<dt><a name="item830">[830]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03158" title="Abstract">arXiv:2402.03158</a> [<a href="/pdf/2402.03158" title="Download PDF">pdf</a>, <a href="/format/2402.03158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal and Near-Optimal Adaptive Vector Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ben-Basat%2C+R">Ran Ben-Basat</a>, 
<a href="/search/cs?searchtype=author&query=Ben-Itzhak%2C+Y">Yaniv Ben-Itzhak</a>, 
<a href="/search/cs?searchtype=author&query=Mitzenmacher%2C+M">Michael Mitzenmacher</a>, 
<a href="/search/cs?searchtype=author&query=Vargaftik%2C+S">Shay Vargaftik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS); Information Theory (cs.IT); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Quantization is a fundamental optimization for many machine-learning use
cases, including compressing gradients, model weights and activations, and
datasets. The most accurate form of quantization is \emph{adaptive}, where the
error is minimized with respect to a given input, rather than optimizing for
the worst case. However, optimal adaptive quantization methods are considered
infeasible in terms of both their runtime and memory requirements.
<br />We revisit the Adaptive Vector Quantization (AVQ) problem and present
algorithms that find optimal solutions with asymptotically improved time and
space complexity. We also present an even faster near-optimal algorithm for
large inputs. Our experiments show our algorithms may open the door to using
AVQ more extensively in a variety of machine learning applications.
</p>
</div>
</dd>
<dt><a name="item831">[831]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03161" title="Abstract">arXiv:2402.03161</a> [<a href="/pdf/2402.03161" title="Download PDF">pdf</a>, <a href="/format/2402.03161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Video-LaVIT: Unified Video-Language Pre-training with Decoupled  Visual-Motional Tokenization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yang Jin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhicheng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liwei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Hao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Quzhe Huang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chengru Song</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Di Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yang Song</a>, 
<a href="/search/cs?searchtype=author&query=Gai%2C+K">Kun Gai</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+Y">Yadong Mu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">In light of recent advances in multimodal Large Language Models (LLMs), there
is increasing attention to scaling them from image-text data to more
informative real-world videos. Compared to static images, video poses unique
challenges for effective large-scale pre-training due to the modeling of its
spatiotemporal dynamics. In this paper, we address such limitations in
video-language pre-training with an efficient video decomposition that
represents each video as keyframes and temporal motions. These are then adapted
to an LLM using well-designed tokenizers that discretize visual and temporal
information as a few tokens, thus enabling unified generative pre-training of
videos, images, and text. At inference, the generated tokens from the LLM are
carefully recovered to the original continuous pixel space to create various
video content. Our proposed framework is both capable of comprehending and
generating image and video content, as demonstrated by its competitive
performance across 13 multimodal benchmarks in image and video understanding
and generation. Our code and models will be available at
https://video-lavit.github.io.
</p>
</div>
</dd>
<dt><a name="item832">[832]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03162" title="Abstract">arXiv:2402.03162</a> [<a href="/pdf/2402.03162" title="Download PDF">pdf</a>, <a href="/format/2402.03162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct-a-Video: Customized Video Generation with User-Directed Camera  Movement and Object Motion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shiyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+L">Liang Hou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Haibin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chongyang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+P">Pengfei Wan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Di Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaodong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+J">Jing Liao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent text-to-video diffusion models have achieved impressive progress. In
practice, users often desire the ability to control object motion and camera
movement independently for customized video creation. However, current methods
lack the focus on separately controlling object motion and camera movement in a
decoupled manner, which limits the controllability and flexibility of
text-to-video models. In this paper, we introduce Direct-a-Video, a system that
allows users to independently specify motions for one or multiple objects
and/or camera movements, as if directing a video. We propose a simple yet
effective strategy for the decoupled control of object motion and camera
movement. Object motion is controlled through spatial cross-attention
modulation using the model's inherent priors, requiring no additional
optimization. For camera movement, we introduce new temporal cross-attention
layers to interpret quantitative camera movement parameters. We further employ
an augmentation-based approach to train these layers in a self-supervised
manner on a small-scale dataset, eliminating the need for explicit motion
annotation. Both components operate independently, allowing individual or
combined control, and can generalize to open-domain scenarios. Extensive
experiments demonstrate the superiority and effectiveness of our method.
Project page: https://direct-a-video.github.io/.
</p>
</div>
</dd>
<dt><a name="item833">[833]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03163" title="Abstract">arXiv:2402.03163</a> [<a href="/pdf/2402.03163" title="Download PDF">pdf</a>, <a href="/format/2402.03163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linguistic features for sentence difficulty prediction in ABSA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chifu%2C+A">Adrian-Gabriel Chifu</a>, 
<a href="/search/cs?searchtype=author&query=Fournier%2C+S">S&#xe9;bastien Fournier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">One of the challenges of natural language understanding is to deal with the
subjectivity of sentences, which may express opinions and emotions that add
layers of complexity and nuance. Sentiment analysis is a field that aims to
extract and analyze these subjective elements from text, and it can be applied
at different levels of granularity, such as document, paragraph, sentence, or
aspect. Aspect-based sentiment analysis is a well-studied topic with many
available data sets and models. However, there is no clear definition of what
makes a sentence difficult for aspect-based sentiment analysis. In this paper,
we explore this question by conducting an experiment with three data sets:
"Laptops", "Restaurants", and "MTSC" (Multi-Target-dependent Sentiment
Classification), and a merged version of these three datasets. We study the
impact of domain diversity and syntactic diversity on difficulty. We use a
combination of classifiers to identify the most difficult sentences and analyze
their characteristics. We employ two ways of defining sentence difficulty. The
first one is binary and labels a sentence as difficult if the classifiers fail
to correctly predict the sentiment polarity. The second one is a six-level
scale based on how many of the top five best-performing classifiers can
correctly predict the sentiment polarity. We also define 9 linguistic features
that, combined, aim at estimating the difficulty at sentence level.
</p>
</div>
</dd>
<dt><a name="item834">[834]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03164" title="Abstract">arXiv:2402.03164</a> [<a href="/pdf/2402.03164" title="Download PDF">pdf</a>, <a href="/ps/2402.03164" title="Download PostScript">ps</a>, <a href="/format/2402.03164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decidable Reasoning About Time in Finite-Domain Situation Calculus  Theories
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hofmann%2C+T">Till Hofmann</a>, 
<a href="/search/cs?searchtype=author&query=Schupp%2C+S">Stefan Schupp</a>, 
<a href="/search/cs?searchtype=author&query=Lakemeyer%2C+G">Gerhard Lakemeyer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Representing time is crucial for cyber-physical systems and has been studied
extensively in the Situation Calculus. The most commonly used approach
represents time by adding a real-valued fluent $\mathit{time}(a)$ that attaches
a time point to each action and consequently to each situation. We show that in
this approach, checking whether there is a reachable situation that satisfies a
given formula is undecidable, even if the domain of discourse is restricted to
a finite set of objects. We present an alternative approach based on
well-established results from timed automata theory by introducing clocks as
real-valued fluents with restricted successor state axioms and comparison
operators. %that only allow comparisons against fixed rationals. With this
restriction, we can show that the reachability problem for finite-domain basic
action theories is decidable. Finally, we apply our results on Golog program
realization by presenting a decidable procedure for determining an action
sequence that is a successful execution of a given program.
</p>
</div>
</dd>
<dt><a name="item835">[835]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03165" title="Abstract">arXiv:2402.03165</a> [<a href="/pdf/2402.03165" title="Download PDF">pdf</a>, <a href="/format/2402.03165" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Risk-Aware MPC for Stochastic Systems with Runtime Temporal Logics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Engelaar%2C+M+H+W">Maico Hendrikus Wilhelmus Engelaar</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Z">Zengjie Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Lazar%2C+M">Mircea Lazar</a>, 
<a href="/search/eess?searchtype=author&query=Haesaert%2C+S">Sofie Haesaert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">This paper concerns the risk-aware control of stochastic systems with
temporal logic specifications dynamically assigned during runtime. Conventional
risk-aware control typically assumes that all specifications are predefined and
remain unchanged during runtime. In this paper, we propose a novel, provably
correct control scheme for linear systems with unbounded stochastic
disturbances that dynamically evaluates the feasibility of runtime signal
temporal logic specifications and automatically reschedules the control inputs.
The method guarantees the probabilistic satisfaction of newly accepted runtime
specifications without sacrificing the satisfaction of the previously accepted
ones. The proposed control method is validated by a robotic motion planning
case study. The idea of closed-loop control rescheduling with probabilistic
risk guarantees provides a novel solution for runtime control synthesis of
stochastic systems.
</p>
</div>
</dd>
<dt><a name="item836">[836]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03166" title="Abstract">arXiv:2402.03166</a> [<a href="/pdf/2402.03166" title="Download PDF">pdf</a>, <a href="/format/2402.03166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RRWNet: Recursive Refinement Network for Effective Retinal Artery/Vein  Segmentation and Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morano%2C+J">Jos&#xe9; Morano</a>, 
<a href="/search/cs?searchtype=author&query=Aresta%2C+G">Guilherme Aresta</a>, 
<a href="/search/cs?searchtype=author&query=Bogunovi%C4%87%2C+H">Hrvoje Bogunovi&#x107;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The caliber and configuration of retinal blood vessels serve as important
biomarkers for various diseases and medical conditions. A thorough analysis of
the retinal vasculature requires the segmentation of blood vessels and their
classification into arteries and veins, which is typically performed on color
fundus images obtained by retinography, a widely used imaging technique.
Nonetheless, manually performing these tasks is labor-intensive and prone to
human error. Various automated methods have been proposed to address this
problem. However, the current state of art in artery/vein segmentation and
classification faces challenges due to manifest classification errors that
affect the topological consistency of segmentation maps. This study presents an
innovative end-to-end framework, RRWNet, designed to recursively refine
semantic segmentation maps and correct manifest classification errors. The
framework consists of a fully convolutional neural network with a Base
subnetwork that generates base segmentation maps from input images, and a
Recursive Refinement subnetwork that iteratively and recursively improves these
maps. Evaluation on public datasets demonstrates the state-of-the-art
performance of the proposed method, yielding more topologically consistent
segmentation maps with fewer manifest classification errors than existing
approaches. In addition, the Recursive Refinement module proves effective in
post-processing segmentation maps from other methods, automatically correcting
classification errors and improving topological consistency. The model code,
weights, and predictions are publicly available at
https://github.com/j-morano/rrwnet.
</p>
</div>
</dd>
<dt><a name="item837">[837]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03170" title="Abstract">arXiv:2402.03170</a> [<a href="/pdf/2402.03170" title="Download PDF">pdf</a>, <a href="/format/2402.03170" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Mamba Capable of In-Context Learning?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grazzi%2C+R">Riccardo Grazzi</a>, 
<a href="/search/cs?searchtype=author&query=Siems%2C+J">Julien Siems</a>, 
<a href="/search/cs?searchtype=author&query=Schrodi%2C+S">Simon Schrodi</a>, 
<a href="/search/cs?searchtype=author&query=Brox%2C+T">Thomas Brox</a>, 
<a href="/search/cs?searchtype=author&query=Hutter%2C+F">Frank Hutter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This work provides empirical evidence that Mamba, a newly proposed selective
structured state space model, has similar in-context learning (ICL)
capabilities as transformers. We evaluated Mamba on tasks involving simple
function approximation as well as more complex natural language processing
problems. Our results demonstrate that across both categories of tasks, Mamba
matches the performance of transformer models for ICL. Further analysis reveals
that like transformers, Mamba appears to solve ICL problems by incrementally
optimizing its internal representations. Overall, our work suggests that Mamba
can be an efficient alternative to transformers for ICL tasks involving longer
input sequences.
</p>
</div>
</dd>
<dt><a name="item838">[838]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03171" title="Abstract">arXiv:2402.03171</a> [<a href="/pdf/2402.03171" title="Download PDF">pdf</a>, <a href="/format/2402.03171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Homograph Attacks on Maghreb Sentiment Analyzers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qachfar%2C+F+Z">Fatima Zahra Qachfar</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+R+M">Rakesh M. Verma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NAML, North Africans in Machine Leaning, NeurIPS, Neural Information Processing Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">We examine the impact of homograph attacks on the Sentiment Analysis (SA)
task of different Arabic dialects from the Maghreb North-African countries.
Homograph attacks result in a 65.3% decrease in transformer classification from
an F1-score of 0.95 to 0.33 when data is written in "Arabizi". The goal of this
study is to highlight LLMs weaknesses' and to prioritize ethical and
responsible Machine Learning.
</p>
</div>
</dd>
<dt><a name="item839">[839]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03172" title="Abstract">arXiv:2402.03172</a> [<a href="/pdf/2402.03172" title="Download PDF">pdf</a>, <a href="/format/2402.03172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accurate and Well-Calibrated ICD Code Assignment Through Attention Over  Diverse Label Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gomes%2C+G">Gon&#xe7;alo Gomes</a>, 
<a href="/search/cs?searchtype=author&query=Coutinho%2C+I">Isabel Coutinho</a>, 
<a href="/search/cs?searchtype=author&query=Martins%2C+B">Bruno Martins</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Although the International Classification of Diseases (ICD) has been adopted
worldwide, manually assigning ICD codes to clinical text is time-consuming,
error-prone, and expensive, motivating the development of automated approaches.
This paper describes a novel approach for automated ICD coding, combining
several ideas from previous related work. We specifically employ a strong
Transformer-based model as a text encoder and, to handle lengthy clinical
narratives, we explored either (a) adapting the base encoder model into a
Longformer, or (b) dividing the text into chunks and processing each chunk
independently. The representations produced by the encoder are combined with a
label embedding mechanism that explores diverse ICD code synonyms. Experiments
with different splits of the MIMIC-III dataset show that the proposed approach
outperforms the current state-of-the-art models in ICD coding, with the label
embeddings significantly contributing to the good performance. Our approach
also leads to properly calibrated classification results, which can effectively
inform downstream tasks such as quantification.
</p>
</div>
</dd>
<dt><a name="item840">[840]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03173" title="Abstract">arXiv:2402.03173</a> [<a href="/pdf/2402.03173" title="Download PDF">pdf</a>, <a href="/format/2402.03173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi: Multimodal Understanding Leaderboard with Text and Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zichen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jingkai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yichuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yiming Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+H">Hailin Wen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+J">Jinyu Cai</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yingzi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Situo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zihan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Liangtai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+K">Kai Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Details and access are available at: <a href="https://OpenDFM.github.io/MULTI-Benchmark/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Rapid progress in multimodal large language models (MLLMs) highlights the
need to introduce challenging yet realistic benchmarks to the academic
community. Existing benchmarks primarily focus on simple natural image
understanding, but Multi emerges as a cutting-edge benchmark for MLLMs,
offering a comprehensive dataset for evaluating MLLMs against understanding
complex figures and tables, and scientific questions. This benchmark,
reflecting current realistic examination styles, provides multimodal inputs and
requires responses that are either precise or open-ended, similar to real-life
school tests. It challenges MLLMs with a variety of tasks, ranging from formula
derivation to image detail analysis, and cross-modality reasoning. Multi
includes over 18,000 questions, with a focus on science-based QA in diverse
formats. We also introduce Multi-Elite, a 500-question subset for testing the
extremities of MLLMs, and Multi-Extend, which enhances In-Context Learning
research with more than 4,500 knowledge pieces. Our evaluation indicates
significant potential for MLLM advancement, with GPT-4V achieving a 63.7%
accuracy rate on Multi, in contrast to other MLLMs scoring between 31.3% and
53.7%. Multi serves not only as a robust evaluation platform but also paves the
way for the development of expert-level AI.
</p>
</div>
</dd>
<dt><a name="item841">[841]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03174" title="Abstract">arXiv:2402.03174</a> [<a href="/pdf/2402.03174" title="Download PDF">pdf</a>, <a href="/ps/2402.03174" title="Download PostScript">ps</a>, <a href="/format/2402.03174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized Event-Triggered Online Learning for Safe Consensus of  Multi-Agent Systems with Gaussian Process Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dai%2C+X">Xiaobing Dai</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+Z">Zewen Yang</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+M">Mengtian Xu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+F">Fangzhou Liu</a>, 
<a href="/search/eess?searchtype=author&query=Hattab%2C+G">Georges Hattab</a>, 
<a href="/search/eess?searchtype=author&query=Hirche%2C+S">Sandra Hirche</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Consensus control in multi-agent systems has received significant attention
and practical implementation across various domains. However, managing
consensus control under unknown dynamics remains a significant challenge for
control design due to system uncertainties and environmental disturbances. This
paper presents a novel learning-based distributed control law, augmented by an
auxiliary dynamics. Gaussian processes are harnessed to compensate for the
unknown components of the multi-agent system. For continuous enhancement in
predictive performance of Gaussian process model, a data-efficient online
learning strategy with a decentralized event-triggered mechanism is proposed.
Furthermore, the control performance of the proposed approach is ensured via
the Lyapunov theory, based on a probabilistic guarantee for prediction error
bounds. To demonstrate the efficacy of the proposed learning-based controller,
a comparative analysis is conducted, contrasting it with both conventional
distributed control laws and offline learning methodologies.
</p>
</div>
</dd>
<dt><a name="item842">[842]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03175" title="Abstract">arXiv:2402.03175</a> [<a href="/pdf/2402.03175" title="Download PDF">pdf</a>, <a href="/format/2402.03175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Matrix: A Bayesian learning model for LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dalal%2C+S">Siddhartha Dalal</a>, 
<a href="/search/cs?searchtype=author&query=Misra%2C+V">Vishal Misra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper, we introduce a Bayesian learning model to understand the
behavior of Large Language Models (LLMs). We explore the optimization metric of
LLMs, which is based on predicting the next token, and develop a novel model
grounded in this principle. Our approach involves constructing an ideal
generative text model represented by a multinomial transition probability
matrix with a prior, and we examine how LLMs approximate this matrix. We
discuss the continuity of the mapping between embeddings and multinomial
distributions, and present the Dirichlet approximation theorem to approximate
any prior. Additionally, we demonstrate how text generation by LLMs aligns with
Bayesian learning principles and delve into the implications for in-context
learning, specifically explaining why in-context learning emerges in larger
models where prompts are considered as samples to be updated. Our findings
indicate that the behavior of LLMs is consistent with Bayesian Learning,
offering new insights into their functioning and potential applications.
</p>
</div>
</dd>
<dt><a name="item843">[843]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03176" title="Abstract">arXiv:2402.03176</a> [<a href="/pdf/2402.03176" title="Download PDF">pdf</a>, <a href="/ps/2402.03176" title="Download PostScript">ps</a>, <a href="/format/2402.03176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparison of Topic Modelling Approaches in the Banking Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ogunleye%2C+B">Bayode Ogunleye</a>, 
<a href="/search/cs?searchtype=author&query=Maswera%2C+T">Tonderai Maswera</a>, 
<a href="/search/cs?searchtype=author&query=Hirsch%2C+L">Laurence Hirsch</a>, 
<a href="/search/cs?searchtype=author&query=Gaudoin%2C+J">Jotham Gaudoin</a>, 
<a href="/search/cs?searchtype=author&query=Brunsdon%2C+T">Teresa Brunsdon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, Journal of Applied Science
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Applied Sciences (2023), 13(2), 797
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Computation (stat.CO)

</div>
<p class="mathjax">Topic modelling is a prominent task for automatic topic extraction in many
applications such as sentiment analysis and recommendation systems. The
approach is vital for service industries to monitor their customer discussions.
The use of traditional approaches such as Latent Dirichlet Allocation (LDA) for
topic discovery has shown great performances, however, they are not consistent
in their results as these approaches suffer from data sparseness and inability
to model the word order in a document. Thus, this study presents the use of
Kernel Principal Component Analysis (KernelPCA) and K-means Clustering in the
BERTopic architecture. We have prepared a new dataset using tweets from
customers of Nigerian banks and we use this to compare the topic modelling
approaches. Our findings showed KernelPCA and K-means in the BERTopic
architecture-produced coherent topics with a coherence score of 0.8463.
</p>
</div>
</dd>
<dt><a name="item844">[844]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03177" title="Abstract">arXiv:2402.03177</a> [<a href="/pdf/2402.03177" title="Download PDF">pdf</a>, <a href="/format/2402.03177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CIDAR: Culturally Relevant Instruction Dataset For Arabic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alyafeai%2C+Z">Zaid Alyafeai</a>, 
<a href="/search/cs?searchtype=author&query=Almubarak%2C+K">Khalid Almubarak</a>, 
<a href="/search/cs?searchtype=author&query=Ashraf%2C+A">Ahmed Ashraf</a>, 
<a href="/search/cs?searchtype=author&query=Alnuhait%2C+D">Deema Alnuhait</a>, 
<a href="/search/cs?searchtype=author&query=Alshahrani%2C+S">Saied Alshahrani</a>, 
<a href="/search/cs?searchtype=author&query=Abdulrahman%2C+G+A+Q">Gubran A. Q. Abdulrahman</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+G">Gamil Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Gawah%2C+Q">Qais Gawah</a>, 
<a href="/search/cs?searchtype=author&query=Saleh%2C+Z">Zead Saleh</a>, 
<a href="/search/cs?searchtype=author&query=Ghaleb%2C+M">Mustafa Ghaleb</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+Y">Yousef Ali</a>, 
<a href="/search/cs?searchtype=author&query=Al-Shaibani%2C+M+S">Maged S. Al-Shaibani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Instruction tuning has emerged as a prominent methodology for teaching Large
Language Models (LLMs) to follow instructions. However, current instruction
datasets predominantly cater to English or are derived from English-dominated
LLMs, resulting in inherent biases toward Western culture. This bias
significantly impacts the linguistic structures of non-English languages such
as Arabic, which has a distinct grammar reflective of the diverse cultures
across the Arab region. This paper addresses this limitation by introducing
CIDAR: https://hf.co/datasets/arbml/CIDAR, the first open Arabic
instruction-tuning dataset culturally-aligned by human reviewers. CIDAR
contains 10,000 instruction and output pairs that represent the Arab region. We
discuss the cultural relevance of CIDAR via the analysis and comparison to
other models fine-tuned on other datasets. Our experiments show that CIDAR can
help enrich research efforts in aligning LLMs with the Arabic culture. All the
code is available at https://github.com/ARBML/CIDAR.
</p>
</div>
</dd>
<dt><a name="item845">[845]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03181" title="Abstract">arXiv:2402.03181</a> [<a href="/pdf/2402.03181" title="Download PDF">pdf</a>, <a href="/format/2402.03181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> C-RAG: Certified Generation Risks for Retrieval-Augmented Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+M">Mintong Kang</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%BCrel%2C+N+M">Nezihe Merve G&#xfc;rel</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+N">Ning Yu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+D">Dawn Song</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Despite the impressive capabilities of large language models (LLMs) across
diverse applications, they still suffer from trustworthiness issues, such as
hallucinations and misalignments. Retrieval-augmented language models (RAG)
have been proposed to enhance the credibility of generations by grounding
external knowledge, but the theoretical understandings of their generation
risks remains unexplored. In this paper, we answer: 1) whether RAG can indeed
lead to low generation risks, 2) how to provide provable guarantees on the
generation risks of RAG and vanilla LLMs, and 3) what sufficient conditions
enable RAG models to reduce generation risks. We propose C-RAG, the first
framework to certify generation risks for RAG models. Specifically, we provide
conformal risk analysis for RAG models and certify an upper confidence bound of
generation risks, which we refer to as conformal generation risk. We also
provide theoretical guarantees on conformal generation risks for general
bounded risk functions under test distribution shifts. We prove that RAG
achieves a lower conformal generation risk than that of a single LLM when the
quality of the retrieval model and transformer is non-trivial. Our intensive
empirical results demonstrate the soundness and tightness of our conformal
generation risk guarantees across four widely-used NLP datasets on four
state-of-the-art retrieval models.
</p>
</div>
</dd>
<dt><a name="item846">[846]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03182" title="Abstract">arXiv:2402.03182</a> [<a href="/pdf/2402.03182" title="Download PDF">pdf</a>, <a href="/format/2402.03182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empowering Time Series Analysis with Large Language Models: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yushan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Z">Zijie Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xikun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Sahil Garg</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+A">Anderson Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Nevmyvaka%2C+Y">Yuriy Nevmyvaka</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+D">Dongjin Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recently, remarkable progress has been made over large language models
(LLMs), demonstrating their unprecedented capability in varieties of natural
language tasks. However, completely training a large general-purpose model from
the scratch is challenging for time series analysis, due to the large volumes
and varieties of time series data, as well as the non-stationarity that leads
to concept drift impeding continuous model adaptation and re-training. Recent
advances have shown that pre-trained LLMs can be exploited to capture complex
dependencies in time series data and facilitate various applications. In this
survey, we provide a systematic overview of existing methods that leverage LLMs
for time series analysis. Specifically, we first state the challenges and
motivations of applying language models in the context of time series as well
as brief preliminaries of LLMs. Next, we summarize the general pipeline for
LLM-based time series analysis, categorize existing methods into different
groups (i.e., direct query, tokenization, prompt design, fine-tune, and model
integration), and highlight the key ideas within each group. We also discuss
the applications of LLMs for both general and spatial-temporal time series
data, tailored to specific domains. Finally, we thoroughly discuss future
research opportunities to empower time series analysis with LLMs.
</p>
</div>
</dd>
<dt><a name="item847">[847]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03183" title="Abstract">arXiv:2402.03183</a> [<a href="/pdf/2402.03183" title="Download PDF">pdf</a>, <a href="/format/2402.03183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting Configuration Performance in Multiple Environments with  Sequential Meta-learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+J">Jingzhi Gong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tao Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by FSE'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Performance (cs.PF)

</div>
<p class="mathjax">Learning and predicting the performance of given software configurations are
of high importance to many software engineering activities. While configurable
software systems will almost certainly face diverse running environments (e.g.,
version, hardware, and workload), current work often either builds performance
models under a single environment or fails to properly handle data from diverse
settings, hence restricting their accuracy for new environments. In this paper,
we target configuration performance learning under multiple environments. We do
so by designing SeMPL - a meta-learning framework that learns the common
understanding from configurations measured in distinct (meta) environments and
generalizes them to the unforeseen, target environment. What makes it unique is
that unlike common meta-learning frameworks (e.g., MAML and MetaSGD) that train
the meta environments in parallel, we train them sequentially, one at a time.
The order of training naturally allows discriminating the contributions among
meta environments in the meta-model built, which fits better with the
characteristic of configuration data that is known to dramatically differ
between different environments. Through comparing with 15 state-of-the-art
models under nine systems, our extensive experimental results demonstrate that
SeMPL performs considerably better on 89% of the systems with up to 99%
accuracy improvement, while being data-efficient, leading to a maximum of 3.86x
speedup. All code and data can be found at our repository:
https://github.com/ideas-labo/SeMPL.
</p>
</div>
</dd>
<dt><a name="item848">[848]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03186" title="Abstract">arXiv:2402.03186</a> [<a href="/pdf/2402.03186" title="Download PDF">pdf</a>, <a href="/format/2402.03186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Charting The Evolution of Solidity Error Handling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mitropoulos%2C+C">Charalambos Mitropoulos</a>, 
<a href="/search/cs?searchtype=author&query=Kechagia%2C+M">Maria Kechagia</a>, 
<a href="/search/cs?searchtype=author&query=Maschas%2C+C">Chrysostomos Maschas</a>, 
<a href="/search/cs?searchtype=author&query=Ioannidis%2C+S">Sotiris Ioannidis</a>, 
<a href="/search/cs?searchtype=author&query=Sarro%2C+F">Federica Sarro</a>, 
<a href="/search/cs?searchtype=author&query=Mitropoulos%2C+D">Dimitris Mitropoulos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">The usage of error handling in Solidity smart contracts is vital because
smart contracts perform transactions that should be verified. Transactions that
are not carefully handled, may lead to program crashes and vulnerabilities,
implying financial loss and legal consequences. While Solidity designers
attempt to constantly update the language with new features, including
error-handling (EH) features, it is necessary for developers to promptly absorb
how to use them. We conduct a large-scale empirical study on 283K unique
open-source smart contracts to identify patterns regarding the usage of
Solidity EH features over time. Overall, the usage of most EH features is
limited. However, we observe an upward trend (&gt; 60%) in the usage of a
Solidity-tailored EH feature, i.e., require. This indicates that designers of
modern programming languages may consider making error handling more tailored
to the purposes of each language. Our analysis on 102 versions of the Solidity
documentation indicates the volatile nature of Solidity, as the language
changes frequently, i.e., there are changes on EH features once or twice a
year. Such frequent releases may confuse smart contract developers,
discouraging them to carefully read the Solidity documentation, and correctly
adopt EH features. Furthermore, our findings reveal that nearly 70% of the
examined smart contracts are exposed to potential failures due to missing error
handing, e.g., unchecked external calls. Therefore, the use of EH features
should be further supported via a more informative documentation containing (1)
representative and meaningful examples and (2) details about the impact of
potential EH misuses.
</p>
</div>
</dd>
<dt><a name="item849">[849]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03187" title="Abstract">arXiv:2402.03187</a> [<a href="/pdf/2402.03187" title="Download PDF">pdf</a>, <a href="/format/2402.03187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Good is a Single Basin?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lion%2C+K">Kai Lion</a>, 
<a href="/search/cs?searchtype=author&query=Noci%2C+L">Lorenzo Noci</a>, 
<a href="/search/cs?searchtype=author&query=Hofmann%2C+T">Thomas Hofmann</a>, 
<a href="/search/cs?searchtype=author&query=Bachmann%2C+G">Gregor Bachmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The multi-modal nature of neural loss landscapes is often considered to be
the main driver behind the empirical success of deep ensembles. In this work,
we probe this belief by constructing various "connected" ensembles which are
restricted to lie in the same basin. Through our experiments, we demonstrate
that increased connectivity indeed negatively impacts performance. However,
when incorporating the knowledge from other basins implicitly through
distillation, we show that the gap in performance can be mitigated by
re-discovering (multi-basin) deep ensembles within a single basin. Thus, we
conjecture that while the extra-basin knowledge is at least partially present
in any given basin, it cannot be easily harnessed without learning it from
other basins.
</p>
</div>
</dd>
<dt><a name="item850">[850]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03188" title="Abstract">arXiv:2402.03188</a> [<a href="/pdf/2402.03188" title="Download PDF">pdf</a>, <a href="/format/2402.03188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards mitigating uncann(eye)ness in face swaps via gaze-centric loss  terms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wilson%2C+E">Ethan Wilson</a>, 
<a href="/search/cs?searchtype=author&query=Shic%2C+F">Frederick Shic</a>, 
<a href="/search/cs?searchtype=author&query=J%C3%B6rg%2C+S">Sophie J&#xf6;rg</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+E">Eakta Jain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Computers and Graphics Special Issue: Eye Gaze Visualization, Interaction, Synthesis, and Analysis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Advances in face swapping have enabled the automatic generation of highly
realistic faces. Yet face swaps are perceived differently than when looking at
real faces, with key differences in viewer behavior surrounding the eyes. Face
swapping algorithms generally place no emphasis on the eyes, relying on pixel
or feature matching losses that consider the entire face to guide the training
process. We further investigate viewer perception of face swaps, focusing our
analysis on the presence of an uncanny valley effect. We additionally propose a
novel loss equation for the training of face swapping models, leveraging a
pretrained gaze estimation network to directly improve representation of the
eyes. We confirm that viewed face swaps do elicit uncanny responses from
viewers. Our proposed improvements significant reduce viewing angle errors
between face swaps and their source material. Our method additionally reduces
the prevalence of the eyes as a deciding factor when viewers perform deepfake
detection tasks. Our findings have implications on face swapping for special
effects, as digital avatars, as privacy mechanisms, and more; negative
responses from users could limit effectiveness in said applications. Our gaze
improvements are a first step towards alleviating negative viewer perceptions
via a targeted approach.
</p>
</div>
</dd>
<dt><a name="item851">[851]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03190" title="Abstract">arXiv:2402.03190</a> [<a href="/pdf/2402.03190" title="Download PDF">pdf</a>, <a href="/format/2402.03190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified Hallucination Detection for Multimodal Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenxi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Y">Yida Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaoyan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yue Shen</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jinjie Gu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
<p class="mathjax">Despite significant strides in multimodal tasks, Multimodal Large Language
Models (MLLMs) are plagued by the critical issue of hallucination. The reliable
detection of such hallucinations in MLLMs has, therefore, become a vital aspect
of model evaluation and the safeguarding of practical application deployment.
Prior research in this domain has been constrained by a narrow focus on
singular tasks, an inadequate range of hallucination categories addressed, and
a lack of detailed granularity. In response to these challenges, our work
expands the investigative horizons of hallucination detection. We present a
novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate
the evaluation of advancements in hallucination detection methods.
Additionally, we unveil a novel unified multimodal hallucination detection
framework, UNIHD, which leverages a suite of auxiliary tools to validate the
occurrence of hallucinations robustly. We demonstrate the effectiveness of
UNIHD through meticulous evaluation and comprehensive analysis. We also provide
strategic insights on the application of specific tools for addressing various
categories of hallucinations.
</p>
</div>
</dd>
<dt><a name="item852">[852]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03191" title="Abstract">arXiv:2402.03191</a> [<a href="/pdf/2402.03191" title="Download PDF">pdf</a>, <a href="/format/2402.03191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Isotropy, Clusters, and Classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mickus%2C+T">Timothee Mickus</a>, 
<a href="/search/cs?searchtype=author&query=Gr%C3%B6nroos%2C+S">Stig-Arne Gr&#xf6;nroos</a>, 
<a href="/search/cs?searchtype=author&query=Attieh%2C+J">Joseph Attieh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Whether embedding spaces use all their dimensions equally, i.e., whether they
are isotropic, has been a recent subject of discussion. Evidence has been
accrued both for and against enforcing isotropy in embedding spaces. In the
present paper, we stress that isotropy imposes requirements on the embedding
space that are not compatible with the presence of clusters -- which also
negatively impacts linear classification objectives. We demonstrate this fact
empirically and use it to shed light on previous results from the literature.
</p>
</div>
</dd>
<dt><a name="item853">[853]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03196" title="Abstract">arXiv:2402.03196</a> [<a href="/pdf/2402.03196" title="Download PDF">pdf</a>, <a href="/format/2402.03196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lightweight Masking Against Static Power Side-Channel Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhandari%2C+J">Jitendra Bhandari</a>, 
<a href="/search/cs?searchtype=author&query=Nabeel%2C+M">Mohammed Nabeel</a>, 
<a href="/search/cs?searchtype=author&query=Mankali%2C+L">Likhitha Mankali</a>, 
<a href="/search/cs?searchtype=author&query=Sinanoglu%2C+O">Ozgur Sinanoglu</a>, 
<a href="/search/cs?searchtype=author&query=Karri%2C+R">Ramesh Karri</a>, 
<a href="/search/cs?searchtype=author&query=Knechtel%2C+J">Johann Knechtel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">This paper presents a novel defense strategy against static power
side-channel attacks (PSCAs), a critical threat to cryptographic security. Our
method is based on (1) carefully tuning high-Vth versus low-Vth cell selection
during synthesis, accounting for both security and timing impact, and (2), at
runtime, randomly switching the operation between these cells. This approach
serves to significantly obscure static power patterns, which are at the heart
of static PSCAs. Our experimental results on a commercial 28nm node show a
drastic increase in the effort required for a successful attack, namely up to
96 times more traces. When compared to prior countermeasures, ours incurs
little cost, making it a lightweight defense.
</p>
</div>
</dd>
<dt><a name="item854">[854]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03198" title="Abstract">arXiv:2402.03198</a> [<a href="/pdf/2402.03198" title="Download PDF">pdf</a>, <a href="/ps/2402.03198" title="Download PostScript">ps</a>, <a href="/format/2402.03198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blow-up Whitney forms, shadow forms, and Poisson processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Berchenko-Kogan%2C+Y">Yakov Berchenko-Kogan</a>, 
<a href="/search/math?searchtype=author&query=Gawlik%2C+E+S">Evan S. Gawlik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 pages, 8 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Differential Geometry (math.DG)

</div>
<p class="mathjax">The Whitney forms on a simplex $T$ admit high-order generalizations that have
received a great deal of attention in numerical analysis. Less well-known are
the shadow forms of Brasselet, Goresky, and MacPherson. These forms generalize
the Whitney forms, but have rational coefficients, allowing singularities near
the faces of $T$. Motivated by numerical problems that exhibit these kinds of
singularities, we introduce degrees of freedom for the shadow $k$-forms that
are well-suited for finite element implementations. In particular, we show that
the degrees of freedom for the shadow forms are given by integration over the
$k$-dimensional faces of the blow-up $\tilde T$ of the simplex $T$.
Consequently, we obtain an isomorphism between the cohomology of the complex of
shadow forms and the cellular cohomology of $\tilde T$, which vanishes except
in degree zero. Additionally, we discover a surprising probabilistic
interpretation of shadow forms in terms of Poisson processes. This perspective
simplifies several proofs and gives a way of computing bases for the shadow
forms using a straightforward combinatorial calculation.
</p>
</div>
</dd>
<dt><a name="item855">[855]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03199" title="Abstract">arXiv:2402.03199</a> [<a href="/pdf/2402.03199" title="Download PDF">pdf</a>, <a href="/format/2402.03199" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SOAP: A Social Authentication Protocol
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Linker%2C+F">Felix Linker</a>, 
<a href="/search/cs?searchtype=author&query=Basin%2C+D">David Basin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at the 33rd USENIX Security Symposium
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Social authentication has been suggested as a usable authentication ceremony
to replace manual key authentication in messaging applications. Using social
authentication, chat partners authenticate their peers using digital identities
managed by identity providers. In this paper, we formally define social
authentication, present a protocol called SOAP that largely automates social
authentication, formally prove SOAP's security, and demonstrate SOAP's
practicality in two prototypes. One prototype is web-based, and the other is
implemented in the open-source Signal messaging application.
<br />Using SOAP, users can significantly raise the bar for compromising their
messaging accounts. In contrast to the default security provided by messaging
applications such as Signal and WhatsApp, attackers must compromise both the
messaging account and all identity provider-managed identities to attack a
victim. In addition to its security and automation, SOAP is straightforward to
adopt as it is built on top of the well-established OpenID Connect protocol.
</p>
</div>
</dd>
<dt><a name="item856">[856]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03201" title="Abstract">arXiv:2402.03201</a> [<a href="/pdf/2402.03201" title="Download PDF">pdf</a>, <a href="/format/2402.03201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guidance with Spherical Gaussian Constraint for Conditional Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lingxiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+S">Shutong Ding</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Y">Yifan Cai</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jingyi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingya Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Ye Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent advances in diffusion models attempt to handle conditional generative
tasks by utilizing a differentiable loss function for guidance without the need
for additional training. While these methods achieved certain success, they
often compromise on sample quality and require small guidance step sizes,
leading to longer sampling processes. This paper reveals that the fundamental
issue lies in the manifold deviation during the sampling process when loss
guidance is employed. We theoretically show the existence of manifold deviation
by establishing a certain lower bound for the estimation error of the loss
guidance. To mitigate this problem, we propose Diffusion with Spherical
Gaussian constraint (DSG), drawing inspiration from the concentration
phenomenon in high-dimensional Gaussian distributions. DSG effectively
constrains the guidance step within the intermediate data manifold through
optimization and enables the use of larger guidance steps. Furthermore, we
present a closed-form solution for DSG denoising with the Spherical Gaussian
constraint. Notably, DSG can seamlessly integrate as a plugin module within
existing training-free conditional diffusion methods. Implementing DSG merely
involves a few lines of additional code with almost no extra computational
overhead, yet it leads to significant performance improvements. Comprehensive
experimental results in various conditional generation tasks validate the
superiority and adaptability of DSG in terms of both sample quality and time
efficiency.
</p>
</div>
</dd>
<dt><a name="item857">[857]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03202" title="Abstract">arXiv:2402.03202</a> [<a href="/pdf/2402.03202" title="Download PDF">pdf</a>, <a href="/format/2402.03202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging IRS Induced Time Delay for Enhanced Physical Layer Security  in VLC Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iqbal%2C+R">Rashid Iqbal</a>, 
<a href="/search/cs?searchtype=author&query=Biagi%2C+M">Mauro Biagi</a>, 
<a href="/search/cs?searchtype=author&query=Zoha%2C+A">Ahmed Zoha</a>, 
<a href="/search/cs?searchtype=author&query=Imran%2C+M+A">Muhammad Ali Imran</a>, 
<a href="/search/cs?searchtype=author&query=Abumarshoud%2C+H">Hanaa Abumarshoud</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Indoor visible light communication (VLC) is considered secure against
attackers outside the confined area where the light propagates, but it is still
susceptible to interception from inside the coverage area. A new technology,
intelligent reflecting surfaces (IRS), has been recently introduced, offering a
way to enhance physical layer security (PLS). Most research on IRS-assisted VLC
assumes the same time of arrival from all reflecting elements and overlooks the
effect of time delay and the associated intersymbol interference. This paper
tackles, for the first time, the effect of time delay on the secrecy rate in
VLC systems. Our results show that, at a fixed light-emitting diode (LED) power
of 3W, the secrecy rate can be enhanced by up to 253\% at random positions for
the legitimate user when the eavesdropper is located within a 1-meter radius of
the LED. Our results also show that careful allocation of the IRS elements can
lead to enhanced PLS even when the eavesdropper has a more favourable position
and, thus, a better channel gain than the legitimate user.
</p>
</div>
</dd>
<dt><a name="item858">[858]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03204" title="Abstract">arXiv:2402.03204</a> [<a href="/pdf/2402.03204" title="Download PDF">pdf</a>, <a href="/format/2402.03204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-agent Reinforcement Learning for Energy Saving in Multi-Cell  Massive MIMO Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+T">Tianzhang Cai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qichen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Demir%2C+%C3%96+T">&#xd6;zlem Tu&#x11f;fe Demir</a>, 
<a href="/search/cs?searchtype=author&query=Cavdar%2C+C">Cicek Cavdar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We develop a multi-agent reinforcement learning (MARL) algorithm to minimize
the total energy consumption of multiple massive MIMO (multiple-input
multiple-output) base stations (BSs) in a multi-cell network while preserving
the overall quality-of-service (QoS) by making decisions on the multi-level
advanced sleep modes (ASMs) and antenna switching of these BSs. The problem is
modeled as a decentralized partially observable Markov decision process
(DEC-POMDP) to enable collaboration between individual BSs, which is necessary
to tackle inter-cell interference. A multi-agent proximal policy optimization
(MAPPO) algorithm is designed to learn a collaborative BS control policy. To
enhance its scalability, a modified version called MAPPO-neighbor policy is
further proposed. Simulation results demonstrate that the trained MAPPO agent
achieves better performance compared to baseline policies. Specifically,
compared to the auto sleep mode 1 (symbol-level sleeping) algorithm, the
MAPPO-neighbor policy reduces power consumption by approximately 8.7% during
low-traffic hours and improves energy efficiency by approximately 19% during
high-traffic hours, respectively.
</p>
</div>
</dd>
<dt><a name="item859">[859]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03207" title="Abstract">arXiv:2402.03207</a> [<a href="/pdf/2402.03207" title="Download PDF">pdf</a>, <a href="/format/2402.03207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Light and Optimal Schr&#xf6;dinger Bridge Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gushchin%2C+N">Nikita Gushchin</a>, 
<a href="/search/cs?searchtype=author&query=Kholkin%2C+S">Sergei Kholkin</a>, 
<a href="/search/cs?searchtype=author&query=Burnaev%2C+E">Evgeny Burnaev</a>, 
<a href="/search/cs?searchtype=author&query=Korotin%2C+A">Alexander Korotin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Schr\"odinger Bridges (SB) have recently gained the attention of the ML
community as a promising extension of classic diffusion models which is also
interconnected to the Entropic Optimal Transport (EOT). Recent solvers for SB
exploit the pervasive bridge matching procedures. Such procedures aim to
recover a stochastic process transporting the mass between distributions given
only a transport plan between them. In particular, given the EOT plan, these
procedures can be adapted to solve SB. This fact is heavily exploited by recent
works giving rives to matching-based SB solvers. The cornerstone here is
recovering the EOT plan: recent works either use heuristical approximations
(e.g., the minibatch OT) or establish iterative matching procedures which by
the design accumulate the error during the training. We address these
limitations and propose a novel procedure to learn SB which we call the
\textbf{optimal Schr\"odinger bridge matching}. It exploits the optimal
parameterization of the diffusion process and provably recovers the SB process
\textbf{(a)} with a single bridge matching step and \textbf{(b)} with arbitrary
transport plan as the input. Furthermore, we show that the optimal bridge
matching objective coincides with the recently discovered energy-based modeling
(EBM) objectives to learn EOT/SB. Inspired by this observation, we develop a
light solver (which we call LightSB-M) to implement optimal matching in
practice using the Gaussian mixture parameterization of the Schr\"odinger
potential. We experimentally showcase the performance of our solver in a range
of practical tasks. The code for the LightSB-M solver can be found at
\url{https://github.com/SKholkin/LightSB-Matching}.
</p>
</div>
</dd>
<dt><a name="item860">[860]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03214" title="Abstract">arXiv:2402.03214</a> [<a href="/pdf/2402.03214" title="Download PDF">pdf</a>, <a href="/format/2402.03214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Organic or Diffused: Can We Distinguish Human Art from AI-generated  Images?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ha%2C+A+Y+J">Anna Yoo Jeong Ha</a>, 
<a href="/search/cs?searchtype=author&query=Passananti%2C+J">Josephine Passananti</a>, 
<a href="/search/cs?searchtype=author&query=Bhaskar%2C+R">Ronik Bhaskar</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+S">Shawn Shan</a>, 
<a href="/search/cs?searchtype=author&query=Southen%2C+R">Reid Southen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Haitao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B+Y">Ben Y. Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The advent of generative AI images has completely disrupted the art world.
Identifying AI generated images from human art is a challenging problem whose
impact is growing over time. The failure to address this problem allows bad
actors to defraud individuals paying a premium for human art, and companies
whose stated policies forbid AI imagery. This is also critical for AI model
trainers, who need to filter training data to avoid potential model collapse.
There are several different approaches to distinguishing human art from AI
images, including classifiers trained by supervised learning, research tools
targeting diffusion models, and identification by professional artists using
their knowledge of artistic techniques. In this paper, we seek to understand
how well these approaches can perform against today's modern generative models
in both benign and adversarial settings. We curate real human art across 7
styles, generate matching images from 5 generative models, and apply 8
detectors (5 automated detectors and 3 different human groups including 180
crowdworkers, 4000+ professional artists, and 13 expert artists experienced at
detecting AI). Both Hive and expert artists do very well, but make mistakes in
different ways (Hive is weaker against adversarial perturbations while Expert
artists produce higher false positives). We believe these weaknesses will
remain as models continue to evolve, and use our data to demonstrate why a
combined team of human and automated detectors provides the best combination of
accuracy and robustness.
</p>
</div>
</dd>
<dt><a name="item861">[861]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03216" title="Abstract">arXiv:2402.03216</a> [<a href="/pdf/2402.03216" title="Download PDF">pdf</a>, <a href="/format/2402.03216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity  Text Embeddings Through Self-Knowledge Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianlv Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+S">Shitao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peitian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+K">Kun Luo</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+D">Defu Lian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zheng Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we present a new embedding model, called M3-Embedding, which
is distinguished for its versatility in Multi-Linguality, Multi-Functionality,
and Multi-Granularity. It can support more than 100 working languages, leading
to new state-of-the-art performances on multi-lingual and cross-lingual
retrieval tasks. It can simultaneously perform the three common retrieval
functionalities of embedding model: dense retrieval, multi-vector retrieval,
and sparse retrieval, which provides a unified model foundation for real-world
IR applications. It is able to process inputs of different granularities,
spanning from short sentences to long documents of up to 8192 tokens. The
effective training of M3-Embedding involves the following technical
contributions. We propose a novel self-knowledge distillation approach, where
the relevance scores from different retrieval functionalities can be integrated
as the teacher signal to enhance the training quality. We also optimize the
batching strategy, enabling a large batch size and high training throughput to
ensure the discriminativeness of embeddings. To the best of our knowledge,
M3-Embedding is the first embedding model which realizes such a strong
versatility. The model and code will be publicly available at
https://github.com/FlagOpen/FlagEmbedding.
</p>
</div>
</dd>
<dt><a name="item862">[862]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03221" title="Abstract">arXiv:2402.03221</a> [<a href="/pdf/2402.03221" title="Download PDF">pdf</a>, <a href="/format/2402.03221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;Define Your Terms&quot; : Enhancing Efficient Offensive Speech  Classification with Definition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nghiem%2C+H">Huy Nghiem</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+U">Umang Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Morstatter%2C+F">Fred Morstatter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Main Conference, EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The propagation of offensive content through social media channels has
garnered attention of the research community. Multiple works have proposed
various semantically related yet subtle distinct categories of offensive
speech. In this work, we explore meta-earning approaches to leverage the
diversity of offensive speech corpora to enhance their reliable and efficient
detection. We propose a joint embedding architecture that incorporates the
input's label and definition for classification via Prototypical Network. Our
model achieves at least 75% of the maximal F1-score while using less than 10%
of the available training data across 4 datasets. Our experimental findings
also provide a case study of training strategies valuable to combat resource
scarcity.
</p>
</div>
</dd>
<dt><a name="item863">[863]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03223" title="Abstract">arXiv:2402.03223</a> [<a href="/pdf/2402.03223" title="Download PDF">pdf</a>, <a href="/format/2402.03223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> English Prompts are Better for NLI-based Zero-Shot Emotion  Classification than Target-Language Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barrei%C3%9F%2C+P">Patrick Barrei&#xdf;</a>, 
<a href="/search/cs?searchtype=author&query=Klinger%2C+R">Roman Klinger</a>, 
<a href="/search/cs?searchtype=author&query=Barnes%2C+J">Jeremy Barnes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to the PromptEng workshop at The Web Conf
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Emotion classification in text is a challenging and subjective task, due to
the involved cognitive inference processes that are required to interpret a
textual stimulus. In addition, the set of emotion categories is highly
domain-specific. For instance, literature analysis might require the use of
aesthetic emotions (e.g., finding something beautiful), and social media
analysis could benefit from fine-grained sets (e.g., separating anger from
annoyance) in contrast to basic emotion categories. This renders the task an
interesting field for zero-shot classifications, in which the label set is not
known at model development time. Unfortunately, most resources for emotion
analysis are English, and therefore, most studies on emotion analysis have been
performed in English, including those that involve prompting language models
for text labels. This leaves us with a research gap that we address in this
paper: In which language should we prompt for emotion labels on non-English
texts? This is particularly of interest when we have access to a multilingual
large language model, because we could request labels with English prompts even
for non-English data. Our experiments with natural language inference-based
language models show that it is consistently better to use English prompts even
if the data is in a different language.
</p>
</div>
</dd>
<dt><a name="item864">[864]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03226" title="Abstract">arXiv:2402.03226</a> [<a href="/pdf/2402.03226" title="Download PDF">pdf</a>, <a href="/format/2402.03226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xing Han</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+H">Huy Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Harris%2C+C">Carl Harris</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+N">Nhat Ho</a>, 
<a href="/search/cs?searchtype=author&query=Saria%2C+S">Suchi Saria</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 8 tables, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">As machine learning models in critical fields increasingly grapple with
multimodal data, they face the dual challenges of handling a wide array of
modalities, often incomplete due to missing elements, and the temporal
irregularity and sparsity of collected samples. Successfully leveraging this
complex data, while overcoming the scarcity of high-quality training samples,
is key to improving these models' predictive performance. We introduce
``FuseMoE'', a mixture-of-experts framework incorporated with an innovative
gating function. Designed to integrate a diverse number of modalities, FuseMoE
is effective in managing scenarios with missing modalities and irregularly
sampled data trajectories. Theoretically, our unique gating function
contributes to enhanced convergence rates, leading to better performance in
multiple downstream tasks. The practical utility of FuseMoE in real world is
validated by a challenging set of clinical risk prediction tasks.
</p>
</div>
</dd>
<dt><a name="item865">[865]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03227" title="Abstract">arXiv:2402.03227</a> [<a href="/pdf/2402.03227" title="Download PDF">pdf</a>, <a href="/format/2402.03227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of  brain MR images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roca%2C+V">Vincent Roca</a>, 
<a href="/search/cs?searchtype=author&query=Kuchcinski%2C+G">Gr&#xe9;gory Kuchcinski</a>, 
<a href="/search/cs?searchtype=author&query=Pruvo%2C+J">Jean-Pierre Pruvo</a>, 
<a href="/search/cs?searchtype=author&query=Manouvriez%2C+D">Dorian Manouvriez</a>, 
<a href="/search/cs?searchtype=author&query=Lopes%2C+R">Renaud Lopes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In MRI studies, the aggregation of imaging data from multiple acquisition
sites enhances sample size but may introduce site-related variabilities that
hinder consistency in subsequent analyses. Deep learning methods for image
translation have emerged as a solution for harmonizing MR images across sites.
In this study, we introduce IGUANe (Image Generation with Unified Adversarial
Networks), an original 3D model that leverages the strengths of domain
translation and straightforward application of style transfer methods for
multicenter brain MR image harmonization. IGUANe extends CycleGAN architecture
by integrating an arbitrary number of domains for training through a
many-to-one strategy. During inference, the model can be applied to any image,
even from an unknown acquisition site, making it a universal generator for
harmonization. Trained on a dataset comprising T1-weighted images from 11
different scanners, IGUANe was evaluated on data from unseen sites. The
assessments included the transformation of MR images with traveling subjects,
the preservation of pairwise distances between MR images within domains, the
evolution of volumetric patterns related to age and Alzheimer$^\prime$s disease
(AD), and the performance in age regression and patient classification tasks.
Comparisons with other harmonization and normalization methods suggest that
IGUANe better preserves individual information in MR images and is more
suitable for maintaining and reinforcing variabilities related to age and AD.
Future studies may further assess IGUANe in other multicenter contexts, either
using the same model or retraining it for applications to different image
modalities.
</p>
</div>
</dd>
<dt><a name="item866">[866]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03230" title="Abstract">arXiv:2402.03230</a> [<a href="/pdf/2402.03230" title="Download PDF">pdf</a>, <a href="/format/2402.03230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CT-based Anatomical Segmentation for Thoracic Surgical Planning: A  Benchmark Study for 3D U-shaped Deep Learning Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Harirpoush%2C+A">Arash Harirpoush</a>, 
<a href="/search/cs?searchtype=author&query=Rasoulian%2C+A">Amirhossein Rasoulian</a>, 
<a href="/search/cs?searchtype=author&query=Kersten-Oertel%2C+M">Marta Kersten-Oertel</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yiming Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent rising interests in patient-specific thoracic surgical planning and
simulation require efficient and robust creation of digital anatomical models
from automatic medical image segmentation algorithms. Deep learning (DL) is now
state-of-the-art in various radiological tasks, and U-shaped DL models have
particularly excelled in medical image segmentation since the inception of the
2D UNet. To date, many variants of U-shaped models have been proposed by the
integration of different attention mechanisms and network configurations.
Leveraging the recent development of large multi-label databases, systematic
benchmark studies for these models can provide valuable insights for clinical
deployment and future model designs, but such studies are still rare. We
conduct the first benchmark study for variants of 3D U-shaped models (3DUNet,
STUNet, AttentionUNet, SwinUNETR, FocalSegNet, and a novel 3D SwinUnet with
four variants) with a focus on CT-based anatomical segmentation for thoracic
surgery. Our study systematically examines the impact of different attention
mechanisms, number of resolution stages, and network configurations on
segmentation accuracy and computational complexity. To allow cross-reference
with other recent benchmarking studies, we also included a performance
assessment of the BTCV abdominal structural segmentation. With the STUNet
ranking at the top, our study demonstrated the value of CNN-based U-shaped
models for the investigated tasks and the benefit of residual blocks in network
configuration designs to boost segmentation performance.
</p>
</div>
</dd>
<dt><a name="item867">[867]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03232" title="Abstract">arXiv:2402.03232</a> [<a href="/pdf/2402.03232" title="Download PDF">pdf</a>, <a href="/format/2402.03232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smart Flow Matching: On The Theory of Flow Matching Algorithms with  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ryzhakov%2C+G">Gleb Ryzhakov</a>, 
<a href="/search/cs?searchtype=author&query=Pavlova%2C+S">Svetlana Pavlova</a>, 
<a href="/search/cs?searchtype=author&query=Sevriugov%2C+E">Egor Sevriugov</a>, 
<a href="/search/cs?searchtype=author&query=Oseledets%2C+I">Ivan Oseledets</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The paper presents the exact formula for the vector field that minimizes the
loss for the standard flow. This formula depends analytically on a given
distribution \rho_0 and an unknown one \rho_1. Based on the presented formula,
a new loss and algorithm for training a vector field model in the style of
Conditional Flow Matching are provided. Our loss, in comparison to the standard
Conditional Flow Matching approach, exhibits smaller variance when evaluated
through Monte Carlo sampling methods. Numerical experiments on synthetic models
and models on tabular data of large dimensions demonstrate better learning
results with the use of the presented algorithm.
</p>
</div>
</dd>
<dt><a name="item868">[868]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03235" title="Abstract">arXiv:2402.03235</a> [<a href="/pdf/2402.03235" title="Download PDF">pdf</a>, <a href="/format/2402.03235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ActiveAnno3D -- An Active Learning Framework for Multi-Modal 3D Object  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghita%2C+A">Ahmed Ghita</a>, 
<a href="/search/cs?searchtype=author&query=Antoniussen%2C+B">Bj&#xf8;rk Antoniussen</a>, 
<a href="/search/cs?searchtype=author&query=Zimmer%2C+W">Walter Zimmer</a>, 
<a href="/search/cs?searchtype=author&query=Greer%2C+R">Ross Greer</a>, 
<a href="/search/cs?searchtype=author&query=Cre%C3%9F%2C+C">Christian Cre&#xdf;</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%B8gelmose%2C+A">Andreas M&#xf8;gelmose</a>, 
<a href="/search/cs?searchtype=author&query=Trivedi%2C+M+M">Mohan M. Trivedi</a>, 
<a href="/search/cs?searchtype=author&query=Knoll%2C+A+C">Alois C. Knoll</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The curation of large-scale datasets is still costly and requires much time
and resources. Data is often manually labeled, and the challenge of creating
high-quality datasets remains. In this work, we fill the research gap using
active learning for multi-modal 3D object detection. We propose ActiveAnno3D,
an active learning framework to select data samples for labeling that are of
maximum informativeness for training. We explore various continuous training
methods and integrate the most efficient method regarding computational demand
and detection performance. Furthermore, we perform extensive experiments and
ablation studies with BEVFusion and PV-RCNN on the nuScenes and TUM Traffic
Intersection dataset. We show that we can achieve almost the same performance
with PV-RCNN and the entropy-based query strategy when using only half of the
training data (77.25 mAP compared to 83.50 mAP) of the TUM Traffic Intersection
dataset. BEVFusion achieved an mAP of 64.31 when using half of the training
data and 75.0 mAP when using the complete nuScenes dataset. We integrate our
active learning framework into the proAnno labeling tool to enable AI-assisted
data selection and labeling and minimize the labeling costs. Finally, we
provide code, weights, and visualization results on our website:
https://active3d-framework.github.io/active3d-framework.
</p>
</div>
</dd>
<dt><a name="item869">[869]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03239" title="Abstract">arXiv:2402.03239</a> [<a href="/pdf/2402.03239" title="Download PDF">pdf</a>, <a href="/format/2402.03239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bluesky and the AT Protocol: Usable Decentralized Social Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kleppmann%2C+M">Martin Kleppmann</a>, 
<a href="/search/cs?searchtype=author&query=Frazee%2C+P">Paul Frazee</a>, 
<a href="/search/cs?searchtype=author&query=Gold%2C+J">Jake Gold</a>, 
<a href="/search/cs?searchtype=author&query=Graber%2C+J">Jay Graber</a>, 
<a href="/search/cs?searchtype=author&query=Holmgren%2C+D">Daniel Holmgren</a>, 
<a href="/search/cs?searchtype=author&query=Ivy%2C+D">Devin Ivy</a>, 
<a href="/search/cs?searchtype=author&query=Johnson%2C+J">Jeromy Johnson</a>, 
<a href="/search/cs?searchtype=author&query=Newbold%2C+B">Bryan Newbold</a>, 
<a href="/search/cs?searchtype=author&query=Volpert%2C+J">Jaz Volpert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Bluesky is a new social network built upon the AT Protocol, a decentralized
foundation for public social media. It was launched in private beta in February
2023, and has grown to over 3 million registered users in the following year.
In this paper we introduce the architecture of Bluesky and the AT Protocol,
which is inspired by the web itself, but modernized to include streams of
real-time updates and cryptographic authentication. We explain how the
technical design of Bluesky is informed by our goals: to enable
decentralization by having multiple interoperable providers for every part of
the system; to make it easy for users to switch providers; to give users agency
over the content they see; and to provide a simple user experience that does
not burden users with complexity arising from the system's decentralized
nature. The system's openness allows anybody to contribute to content
moderation and community management, and we invite the research community to
use Bluesky as a dataset and testing ground for new approaches in social media
moderation.
</p>
</div>
</dd>
<dt><a name="item870">[870]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03241" title="Abstract">arXiv:2402.03241</a> [<a href="/pdf/2402.03241" title="Download PDF">pdf</a>, <a href="/format/2402.03241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaohu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+K">Kun Yao</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+K">Kai Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we introduce FROSTER, an effective framework for
open-vocabulary action recognition. The CLIP model has achieved remarkable
success in a range of image-based tasks, benefiting from its strong
generalization capability stemming from pretaining on massive image-text pairs.
However, applying CLIP directly to the open-vocabulary action recognition task
is challenging due to the absence of temporal information in CLIP's
pretraining. Further, fine-tuning CLIP on action recognition datasets may lead
to overfitting and hinder its generalizability, resulting in unsatisfactory
results when dealing with unseen actions.
<br />To address these issues, FROSTER employs a residual feature distillation
approach to ensure that CLIP retains its generalization capability while
effectively adapting to the action recognition task. Specifically, the residual
feature distillation treats the frozen CLIP model as a teacher to maintain the
generalizability exhibited by the original CLIP and supervises the feature
learning for the extraction of video-specific features to bridge the gap
between images and videos. Meanwhile, it uses a residual sub-network for
feature distillation to reach a balance between the two distinct objectives of
learning generalizable and video-specific features.
<br />We extensively evaluate FROSTER on open-vocabulary action recognition
benchmarks under both base-to-novel and cross-dataset settings. FROSTER
consistently achieves state-of-the-art performance on all datasets across the
board. Project page: https://visual-ai.github.io/froster.
</p>
</div>
</dd>
<dt><a name="item871">[871]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03242" title="Abstract">arXiv:2402.03242</a> [<a href="/pdf/2402.03242" title="Download PDF">pdf</a>, <a href="/format/2402.03242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JOBSKAPE: A Framework for Generating Synthetic Job Postings to Enhance  Skill Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Magron%2C+A">Antoine Magron</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+A">Anna Dai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mike Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Montariol%2C+S">Syrielle Montariol</a>, 
<a href="/search/cs?searchtype=author&query=Bosselut%2C+A">Antoine Bosselut</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NLP4HR 2024 (EACL Workshop)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent approaches in skill matching, employing synthetic training data for
classification or similarity model training, have shown promising results,
reducing the need for time-consuming and expensive annotations. However,
previous synthetic datasets have limitations, such as featuring only one skill
per sentence and generally comprising short sentences. In this paper, we
introduce JobSkape, a framework to generate synthetic data that tackles these
limitations, specifically designed to enhance skill-to-taxonomy matching.
Within this framework, we create SkillSkape, a comprehensive open-source
synthetic dataset of job postings tailored for skill-matching tasks. We
introduce several offline metrics that show that our dataset resembles
real-world data. Additionally, we present a multi-step pipeline for skill
extraction and matching tasks using large language models (LLMs), benchmarking
against known supervised methodologies. We outline that the downstream
evaluation results on real-world data can beat baselines, underscoring its
efficacy and adaptability.
</p>
</div>
</dd>
<dt><a name="item872">[872]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03243" title="Abstract">arXiv:2402.03243</a> [<a href="/pdf/2402.03243" title="Download PDF">pdf</a>, <a href="/format/2402.03243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PINN-BO: A Black-box Optimization Algorithm using Physics-Informed  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Phan-Trong%2C+D">Dat Phan-Trong</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+H+T">Hung The Tran</a>, 
<a href="/search/cs?searchtype=author&query=Shilton%2C+A">Alistair Shilton</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Sunil Gupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Black-box optimization is a powerful approach for discovering global optima
in noisy and expensive black-box functions, a problem widely encountered in
real-world scenarios. Recently, there has been a growing interest in leveraging
domain knowledge to enhance the efficacy of machine learning methods. Partial
Differential Equations (PDEs) often provide an effective means for elucidating
the fundamental principles governing the black-box functions. In this paper, we
propose PINN-BO, a black-box optimization algorithm employing Physics-Informed
Neural Networks that integrates the knowledge from Partial Differential
Equations (PDEs) to improve the sample efficiency of the optimization. We
analyze the theoretical behavior of our algorithm in terms of regret bound
using advances in NTK theory and prove that the use of the PDE alongside the
black-box function evaluations, PINN-BO leads to a tighter regret bound. We
perform several experiments on a variety of optimization tasks and show that
our algorithm is more sample-efficient compared to existing methods.
</p>
</div>
</dd>
<dt><a name="item873">[873]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03244" title="Abstract">arXiv:2402.03244</a> [<a href="/pdf/2402.03244" title="Download PDF">pdf</a>, <a href="/format/2402.03244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Skill Set Optimization: Reinforcing Language Model Behavior via  Transferable Skills
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nottingham%2C+K">Kolby Nottingham</a>, 
<a href="/search/cs?searchtype=author&query=Majumder%2C+B+P">Bodhisattwa Prasad Majumder</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+B+D">Bhavana Dalvi Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Sameer Singh</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+P">Peter Clark</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+R">Roy Fox</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models (LLMs) have recently been used for sequential decision
making in interactive environments. However, leveraging environment reward
signals for continual LLM actor improvement is not straightforward. We propose
Skill Set Optimization (SSO) for improving LLM actor performance through
constructing and refining sets of transferable skills. SSO constructs skills by
extracting common subtrajectories with high rewards and generating subgoals and
instructions to represent each skill. These skills are provided to the LLM
actor in-context to reinforce behaviors with high rewards. Then, SSO further
refines the skill set by pruning skills that do not continue to result in high
rewards. We evaluate our method in the classic videogame NetHack and the text
environment ScienceWorld to demonstrate SSO's ability to optimize a set of
skills and perform in-context policy improvement. SSO outperforms baselines by
40% in our custom NetHack task and outperforms the previous state-of-the-art in
ScienceWorld by 35%.
</p>
</div>
</dd>
<dt><a name="item874">[874]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03246" title="Abstract">arXiv:2402.03246</a> [<a href="/pdf/2402.03246" title="Download PDF">pdf</a>, <a href="/format/2402.03246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingrui Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuhong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Heng Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">Semantic understanding plays a crucial role in Dense Simultaneous
Localization and Mapping (SLAM), facilitating comprehensive scene
interpretation. Recent advancements that integrate Gaussian Splatting into SLAM
systems have demonstrated its effectiveness in generating high-quality
renderings through the use of explicit 3D Gaussian representations. Building on
this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system
grounded in 3D Gaussians, which provides precise 3D semantic segmentation
alongside high-fidelity reconstructions. Specifically, we propose to employ
multi-channel optimization during the mapping process, integrating appearance,
geometric, and semantic constraints with key-frame optimization to enhance
reconstruction quality. Extensive experiments demonstrate that SGS-SLAM
delivers state-of-the-art performance in camera pose estimation, map
reconstruction, and semantic segmentation, outperforming existing methods
meanwhile preserving real-time rendering ability.
</p>
</div>
</dd>
<dt><a name="item875">[875]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03247" title="Abstract">arXiv:2402.03247</a> [<a href="/pdf/2402.03247" title="Download PDF">pdf</a>, <a href="/format/2402.03247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HEANA: A Hybrid Time-Amplitude Analog Optical Accelerator with Flexible  Dataflows for Energy-Efficient CNN Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vatsavai%2C+S+S">Sairam Sri Vatsavai</a>, 
<a href="/search/cs?searchtype=author&query=Karempudi%2C+V+S+P">Venkata Sai Praneeth Karempudi</a>, 
<a href="/search/cs?searchtype=author&query=Thakkar%2C+I">Ishan Thakkar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper is under review at ACM TODAES
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Several photonic microring resonators (MRRs) based analog accelerators have
been proposed to accelerate the inference of integer-quantized CNNs with
remarkably higher throughput and energy efficiency compared to their electronic
counterparts. However, the existing analog photonic accelerators suffer from
three shortcomings: (i) severe hampering of wavelength parallelism due to
various crosstalk effects, (ii) inflexibility of supporting various dataflows
other than the weight-stationary dataflow, and (iii) failure in fully
leveraging the ability of photodetectors to perform in-situ accumulations.
These shortcomings collectively hamper the performance and energy efficiency of
prior accelerators. To tackle these shortcomings, we present a novel Hybrid
timE Amplitude aNalog optical Accelerator, called HEANA. HEANA employs hybrid
time-amplitude analog optical multipliers (TAOMs) that increase the flexibility
of HEANA to support multiple dataflows. A spectrally hitless arrangement of
TAOMs significantly reduces the crosstalk effects, thereby increasing the
wavelength parallelism in HEANA. Moreover, HEANA employs our invented balanced
photo-charge accumulators (BPCAs) that enable buffer-less, in-situ, temporal
accumulations to eliminate the need to use reduction networks in HEANA,
relieving it from related latency and energy overheads. Our evaluation for the
inference of four modern CNNs indicates that HEANA provides improvements of
atleast 66x and 84x in frames-per-second (FPS) and FPS/W (energy-efficiency),
respectively, for equal-area comparisons, on gmean over two MRR-based analog
CNN accelerators from prior work.
</p>
</div>
</dd>
<dt><a name="item876">[876]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03251" title="Abstract">arXiv:2402.03251</a> [<a href="/pdf/2402.03251" title="Download PDF">pdf</a>, <a href="/format/2402.03251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLIP Can Understand Depth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Dunam Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seokju Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent studies on generalizing CLIP for monocular depth estimation reveal
that CLIP pre-trained on web-crawled data is inefficient for deriving proper
similarities between image patches and depth-related prompts. In this paper, we
adapt CLIP for meaningful quality of monocular depth estimation with dense
prediction, without fine-tuning its original vision-language alignment. By
jointly training a compact deconvolutional decoder with a tiny learnable
embedding matrix named mirror, as a static prompt for its text encoder, CLIP is
enabled to understand depth. With this approach, our model exhibits impressive
performance matching several previous state-of-the-art vision-only models on
the NYU Depth v2 and KITTI datasets, outperforming every CLIP-based depth
estimation model with a large margin. Experiments on temporal depth consistency
and spatial continuity demonstrate that the prior knowledge of CLIP can be
effectively refined by our proposed framework. Furthermore, an ablation study
on mirror proves that the resulting model estimates depth utilizing knowledge
not only from the image encoder but also text encoder despite not being given
any prompt written in a human way. This research demonstrates that through
minimal adjustments, the prior knowledge of vision-language foundation models,
such as CLIP, can be generalized even to domains where learning during
pretraining is challenging. We facilitate future works focused on methods to
adjust suboptimal prior knowledge of vision-language models using non-human
language prompts, achieving performance on par with task-specific
state-of-the-art methodologies.
</p>
</div>
</dd>
<dt><a name="item877">[877]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03252" title="Abstract">arXiv:2402.03252</a> [<a href="/pdf/2402.03252" title="Download PDF">pdf</a>, <a href="/format/2402.03252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair Active Ranking from Pairwise Preferences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gorantla%2C+S">Sruthi Gorantla</a>, 
<a href="/search/cs?searchtype=author&query=Ahmadian%2C+S">Sara Ahmadian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages, 3.1 MB
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We investigate the problem of probably approximately correct and fair (PACF)
ranking of items by adaptively evoking pairwise comparisons. Given a set of $n$
items that belong to disjoint groups, our goal is to find an $(\epsilon,
\delta)$-PACF-Ranking according to a fair objective function that we propose.
We assume access to an oracle, wherein, for each query, the learner can choose
a pair of items and receive stochastic winner feedback from the oracle. Our
proposed objective function asks to minimize the $\ell_q$ norm of the error of
the groups, where the error of a group is the $\ell_p$ norm of the error of all
the items within that group, for $p, q \geq 1$. This generalizes the objective
function of $\epsilon$-Best-Ranking, proposed by Saha &amp; Gopalan (2019).
<br />By adopting our objective function, we gain the flexibility to explore
fundamental fairness concepts like equal or proportionate errors within a
unified framework. Adjusting parameters $p$ and $q$ allows tailoring to
specific fairness preferences. We present both group-blind and group-aware
algorithms and analyze their sample complexity. We provide matching lower
bounds up to certain logarithmic factors for group-blind algorithms. For a
restricted class of group-aware algorithms, we show that we can get reasonable
lower bounds. We conduct comprehensive experiments on both real-world and
synthetic datasets to complement our theoretical findings.
</p>
</div>
</dd>
<dt><a name="item878">[878]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03253" title="Abstract">arXiv:2402.03253</a> [<a href="/pdf/2402.03253" title="Download PDF">pdf</a>, <a href="/format/2402.03253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semitopology: distributed collaborative action via topology, algebra,  and logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gabbay%2C+M+J">Murdoch J. Gabbay</a>, 
<a href="/search/cs?searchtype=author&query=Losa%2C+G">Giuliano Losa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This document is a preprint of a book, which integrates material from papers arxiv.org/abs/<a href="/abs/2303.0928">2303.0928</a> (for point-set semitopologies) and <a href="/abs/2310.00956">arXiv:2310.00956</a> (for algebra)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); General Topology (math.GN)

</div>
<p class="mathjax">We introduce semitopologies, a generalisation of point-set topology that
removes the restriction that intersections of open sets need necessarily be
open.
<br />The intuition is that points are participants in some distributed system, and
an open set is a collection of participants that can collaborate to update
their local state by taking a distributed collaborative action; we call this an
actionable coalition. What constitutes an actionable coalition depends on what
actions we want to model. Intuitive examples include 'a group of people that is
collectively strong enough to lift a rock', where the state update is very
simply 'holding rock low' to 'holding rock high' and this update is common to
all participants in the actionable coalition. Or, consider 'two people wishing
to barter a can of juice for a bar of chocolate', in which case the coalition
is any such pair and the state updates differ between participants to flip them
between 'has/has no juice' and 'has/has no chocolate'. A characteristic of
these systems is that state updates are local to the coalition, voluntary, may
vary between participants, and are not assumed subject to permission or
synchronisation by a central authority. Peer-to-peer computer networks,
including filesharing and blockchain systems, provide motivating examples from
computing.
<br />This paper presents a comprehensive view of semitopologies which includes
point-set semitopology, algebra, and logic inspired by these considerations.
This is interesting in and of itself and it provides a conceptual framework
within which to understand a useful class of distributed systems.
</p>
</div>
</dd>
<dt><a name="item879">[879]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03255" title="Abstract">arXiv:2402.03255</a> [<a href="/pdf/2402.03255" title="Download PDF">pdf</a>, <a href="/format/2402.03255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Security Advice for Parents and Children About Content Filtering and  Circumvention as Found on YouTube and TikTok
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elgedawy%2C+R">Ran Elgedawy</a>, 
<a href="/search/cs?searchtype=author&query=Sadik%2C+J">John Sadik</a>, 
<a href="/search/cs?searchtype=author&query=Gautam%2C+A">Anuj Gautam</a>, 
<a href="/search/cs?searchtype=author&query=Bissahoyo%2C+T">Trinity Bissahoyo</a>, 
<a href="/search/cs?searchtype=author&query=Childress%2C+C">Christopher Childress</a>, 
<a href="/search/cs?searchtype=author&query=Leonard%2C+J">Jacob Leonard</a>, 
<a href="/search/cs?searchtype=author&query=Shubert%2C+C">Clay Shubert</a>, 
<a href="/search/cs?searchtype=author&query=Ruoti%2C+S">Scott Ruoti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 5 figures, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">In today's digital age, concerns about online security and privacy have
become paramount. However, addressing these issues can be difficult, especially
within the context of family relationships, wherein parents and children may
have conflicting interests. In this environment, parents and children may turn
to online security advice to determine how to proceed. In this paper, we
examine the advice available to parents and children regarding content
filtering and circumvention as found on YouTube and TikTok. In an analysis of
839 videos returned from queries on these topics, we found that half (n=399)
provide relevant advice. Our results show that of these videos, roughly
three-quarters are accurate, with the remaining one-fourth containing factually
incorrect advice. We find that videos targeting children are both more likely
to be incorrect and actionable than videos targeting parents, leaving children
at increased risk of taking harmful action. Moreover, we find that while advice
videos targeting parents will occasionally discuss the ethics of content
filtering and device monitoring (including recommendations to respect
children's autonomy) no such discussion of the ethics or risks of circumventing
content filtering is given to children, leaving them unaware of any risks that
may be involved with doing so. Ultimately, our research indicates that
video-based social media sites are already effective sources of security advice
propagation and that the public would benefit from security researchers and
practitioners engaging more with these platforms, both for the creation of
content and of tools designed to help with more effective filtering.
</p>
</div>
</dd>
<dt><a name="item880">[880]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03256" title="Abstract">arXiv:2402.03256</a> [<a href="/pdf/2402.03256" title="Download PDF">pdf</a>, <a href="/ps/2402.03256" title="Download PostScript">ps</a>, <a href="/format/2402.03256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Best-in-Class Policies for the Predict-then-Optimize Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Michael Huang</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+V">Vishal Gupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">We propose a novel family of decision-aware surrogate losses, called
Perturbation Gradient (PG) losses, for the predict-then-optimize framework.
These losses directly approximate the downstream decision loss and can be
optimized using off-the-shelf gradient-based methods. Importantly, unlike
existing surrogate losses, the approximation error of our PG losses vanishes as
the number of samples grows. This implies that optimizing our surrogate loss
yields a best-in-class policy asymptotically, even in misspecified settings.
This is the first such result in misspecified settings and we provide numerical
evidence confirming our PG losses substantively outperform existing proposals
when the underlying model is misspecified and the noise is not centrally
symmetric. Insofar as misspecification is commonplace in practice -- especially
when we might prefer a simpler, more interpretable model -- PG losses offer a
novel, theoretically justified, method for computationally tractable
decision-aware learning.
</p>
</div>
</dd>
<dt><a name="item881">[881]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03258" title="Abstract">arXiv:2402.03258</a> [<a href="/pdf/2402.03258" title="Download PDF">pdf</a>, <a href="/format/2402.03258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Freeze-Tag in $L_1$ has Wake-up Time Five
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bonichon%2C+N">Nicolas Bonichon</a>, 
<a href="/search/cs?searchtype=author&query=Casteigts%2C+A">Arnaud Casteigts</a>, 
<a href="/search/cs?searchtype=author&query=Gavoille%2C+C">Cyril Gavoille</a>, 
<a href="/search/cs?searchtype=author&query=Hanusse%2C+N">Nicolas Hanusse</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Geometry (cs.CG); Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">The Freeze-Tag Problem, introduced in Arkin et al. (SODA'02) consists of
waking up a swarm of $n$ robots, starting from a single active robot. In the
basic geometric version, every robot is given coordinates in the plane. As soon
as a robot is awakened, it can move towards inactive robots to wake them up.
The goal is to minimize the wake-up time of the last robot, the makespan.
<br />Despite significant progress on the computational complexity of this problem
and on approximation algorithms, the characterization of exact bounds on the
makespan remains one of the main open questions. In this paper, we settle this
question for the $\ell_1$-norm, showing that a makespan of at most $5r$ can
always be achieved, where $r$ is the maximum distance between the initial
active robot and any sleeping robot. Moreover, a schedule achieving a makespan
of at most $5r$ can be computed in optimal time $O(n)$. Both bounds, the time
and the makespan are optimal. This implies a new upper bound of $5\sqrt{2}r
\approx 7.07r$ on the makespan in the $\ell_2$-norm, improving the best known
bound so far $(5+2\sqrt{2}+\sqrt{5})r \approx 10.06r$.
</p>
</div>
</dd>
<dt><a name="item882">[882]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03259" title="Abstract">arXiv:2402.03259</a> [<a href="/pdf/2402.03259" title="Download PDF">pdf</a>, <a href="/format/2402.03259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meeting Bridges: Designing Information Artifacts that Bridge from  Synchronous Meetings to Asynchronous Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruotong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+L">Lin Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Cranshaw%2C+J">Justin Cranshaw</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A+X">Amy X. Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to CSCW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">A recent surge in remote meetings has led to complaints of ``Zoom fatigue''
and ``collaboration overload,'' negatively impacting worker productivity and
well-being. One way to alleviate the burden of meetings is to de-emphasize
their synchronous participation by shifting work to and enabling sensemaking
during post-meeting asynchronous activities. Towards this goal, we propose the
design concept of meeting bridges, or information artifacts that can
encapsulate meeting information towards bridging to and facilitating
post-meeting activities. Through 13 interviews and a survey of 198 information
workers, we learn how people use online meeting information after meetings are
over, finding five main uses: as an archive, as task reminders, to onboard or
support inclusion, for group sensemaking, and as a launching point for
follow-on collaboration. However, we also find that current common meeting
artifacts, such as notes and recordings, present challenges in serving as
meeting bridges. After conducting co-design sessions with 16 participants, we
distill key principles for the design of meeting bridges to optimally support
asynchronous collaboration goals. Overall, our findings point to the
opportunity of designing information artifacts that not only support users to
access but also continue to transform and engage in meeting information
post-meeting.
</p>
</div>
</dd>
<dt><a name="item883">[883]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03264" title="Abstract">arXiv:2402.03264</a> [<a href="/pdf/2402.03264" title="Download PDF">pdf</a>, <a href="/format/2402.03264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MobilityGPT: Enhanced Human Mobility Modeling with a GPT model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haydari%2C+A">Ammar Haydari</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dongjie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Z">Zhengfeng Lai</a>, 
<a href="/search/cs?searchtype=author&query=Chuah%2C+C">Chen-Nee Chuah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Generative models have shown promising results in capturing human mobility
characteristics and generating synthetic trajectories. However, it remains
challenging to ensure that the generated geospatial mobility data is
semantically realistic, including consistent location sequences, and reflects
real-world characteristics, such as constraining on geospatial limits. To
address these issues, we reformat human mobility modeling as an autoregressive
generation task, leveraging Generative Pre-trained Transformer (GPT). To ensure
its controllable generation to alleviate the above challenges, we propose a
geospatially-aware generative model, MobilityGPT. We propose a gravity-based
sampling method to train a transformer for semantic sequence similarity. Then,
we constrained the training process via a road connectivity matrix that
provides the connectivity of sequences in trajectory generation, thereby
keeping generated trajectories in geospatial limits. Lastly, we constructed a
Reinforcement Learning from Trajectory Feedback (RLTF) to minimize the travel
distance between training and the synthetically generated trajectories. Our
experiments on real-world datasets demonstrate that MobilityGPT outperforms
state-of-the-art methods in generating high-quality mobility trajectories that
are closest to real data in terms of origin-destination similarity, trip
length, travel radius, link, and gravity distributions.
</p>
</div>
</dd>
<dt><a name="item884">[884]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03268" title="Abstract">arXiv:2402.03268</a> [<a href="/pdf/2402.03268" title="Download PDF">pdf</a>, <a href="/format/2402.03268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Reasoning Ability of Language Models From the  Perspective of Reasoning Paths Aggregation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Amayuelas%2C+A">Alfonso Amayuelas</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kexun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Liangming Pan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Pre-trained language models (LMs) are able to perform complex reasoning
without explicit fine-tuning. To understand how pre-training with a next-token
prediction objective contributes to the emergence of such reasoning capability,
we propose that we can view an LM as deriving new conclusions by aggregating
indirect reasoning paths seen at pre-training time. We found this perspective
effective in two important cases of reasoning: logic reasoning with knowledge
graphs (KGs) and math reasoning with math word problems (MWPs). More
specifically, we formalize the reasoning paths as random walk paths on the
knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a
weighted sum of relevant random walk path probabilities is a reasonable way to
explain how LMs reason. Experiments and analysis on multiple KG and MWP
datasets reveal the effect of training on random walk paths and suggest that
augmenting unlabeled random walk reasoning paths can improve real-world
multi-step reasoning performance.
</p>
</div>
</dd>
<dt><a name="item885">[885]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03269" title="Abstract">arXiv:2402.03269</a> [<a href="/pdf/2402.03269" title="Download PDF">pdf</a>, <a href="/format/2402.03269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ISPA: Inter-Species Phonetic Alphabet for Transcribing Animal Sounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hagiwara%2C+M">Masato Hagiwara</a>, 
<a href="/search/cs?searchtype=author&query=Miron%2C+M">Marius Miron</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jen-Yu Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at XAI-AI Workshop (IEEEXplore track) @ ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Traditionally, bioacoustics has relied on spectrograms and continuous,
per-frame audio representations for the analysis of animal sounds, also serving
as input to machine learning models. Meanwhile, the International Phonetic
Alphabet (IPA) system has provided an interpretable, language-independent
method for transcribing human speech sounds. In this paper, we introduce ISPA
(Inter-Species Phonetic Alphabet), a precise, concise, and interpretable system
designed for transcribing animal sounds into text. We compare acoustics-based
and feature-based methods for transcribing and classifying animal sounds,
demonstrating their comparable performance with baseline methods utilizing
continuous, dense audio representations. By representing animal sounds with
text, we effectively treat them as a "foreign language," and we show that
established human language ML paradigms and models, such as language models,
can be successfully applied to improve performance.
</p>
</div>
</dd>
<dt><a name="item886">[886]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03270" title="Abstract">arXiv:2402.03270</a> [<a href="/pdf/2402.03270" title="Download PDF">pdf</a>, <a href="/ps/2402.03270" title="Download PostScript">ps</a>, <a href="/format/2402.03270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiclass Classification Procedure for Detecting Attacks on MQTT-IoT  Protocol
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alaiz-Moreton%2C+H">Hector Alaiz-Moreton</a> (1), 
<a href="/search/cs?searchtype=author&query=Aveleira-Mata%2C+J">Jose Aveleira-Mata</a> (2), 
<a href="/search/cs?searchtype=author&query=Ondicol-Garcia%2C+J">Jorge Ondicol-Garcia</a> (2), 
<a href="/search/cs?searchtype=author&query=Mu%C3%B1oz-Casta%C3%B1eda%2C+A+L">Angel Luis Mu&#xf1;oz-Casta&#xf1;eda</a> (2), 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa%2C+I">Isa&#xed;as Garc&#xed;a</a> (1), 
<a href="/search/cs?searchtype=author&query=Benavides%2C+C">Carmen Benavides</a> (1) ((1) Escuela de Ingenier&#xed;as, Universidad de Le&#xf3;n, (2) Research Institute of Applied Sciences in Cybersecurity, Universidad de Le&#xf3;n)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Complexity (New York, N.Y.), 2019, Vol.2019, p.1-11
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The large number of sensors and actuators that make up the Internet of Things
obliges these systems to use diverse technologies and protocols. This means
that IoT networks are more heterogeneous than traditional networks. This gives
rise to new challenges in cybersecurity to protect these systems and devices
which are characterized by being connected continuously to the Internet.
Intrusion detection systems (IDS) are used to protect IoT systems from the
various anomalies and attacks at the network level. Intrusion Detection Systems
(IDS) can be improved through machine learning techniques. Our work focuses on
creating classification models that can feed an IDS using a dataset containing
frames under attacks of an IoT system that uses the MQTT protocol. We have
addressed two types of method for classifying the attacks, ensemble methods and
deep learning models, more specifically recurrent networks with very
satisfactory results.
</p>
</div>
</dd>
<dt><a name="item887">[887]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03271" title="Abstract">arXiv:2402.03271</a> [<a href="/pdf/2402.03271" title="Download PDF">pdf</a>, <a href="/format/2402.03271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information  Seeking in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhiyuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chumin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+X">Xidong Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yilun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+S">See-Kiong Ng</a>, 
<a href="/search/cs?searchtype=author&query=Luu%2C+A+T">Anh Tuan Luu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Junxian He</a>, 
<a href="/search/cs?searchtype=author&query=Koh%2C+P+W">Pang Wei Koh</a>, 
<a href="/search/cs?searchtype=author&query=Hooi%2C+B">Bryan Hooi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In the face of uncertainty, the ability to seek information is of fundamental
importance. In many practical applications, such as medical diagnosis and
troubleshooting, the information needed to solve the task is not initially
given, and has to be actively sought by asking follow-up questions (for
example, a doctor asking a patient for more details about their symptoms). In
this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment
large language models with the ability to actively seek information by asking
effective questions. UoT combines 1) an uncertainty-aware simulation approach
which enables the model to simulate possible future scenarios and how likely
they are to occur, 2) uncertainty-based rewards motivated by information gain
which incentivizes the model to seek information, and 3) a reward propagation
scheme to select the optimal question to ask in a way that maximizes the
expected reward. In experiments on medical diagnosis, troubleshooting and the
'20 Questions' game, UoT achieves an average performance improvement of 57.8%
in the rate of successful task completion across multiple LLMs compared with
direct prompting, and also improves efficiency (i.e., the number of questions
needed to complete the task).
</p>
</div>
</dd>
<dt><a name="item888">[888]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03273" title="Abstract">arXiv:2402.03273</a> [<a href="/pdf/2402.03273" title="Download PDF">pdf</a>, <a href="/ps/2402.03273" title="Download PostScript">ps</a>, <a href="/format/2402.03273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algorithms and Complexity of Difference Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dabrowski%2C+K+K">Konrad K. Dabrowski</a>, 
<a href="/search/cs?searchtype=author&query=Jonsson%2C+P">Peter Jonsson</a>, 
<a href="/search/cs?searchtype=author&query=Ordyniak%2C+S">Sebastian Ordyniak</a>, 
<a href="/search/cs?searchtype=author&query=Osipov%2C+G">George Osipov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is an strongly extended version of two conference papers with the same authors that appeared at KR 2020 (Title: Fine-Grained Complexity of Temporal Problems) and AAAI 2021 (Title: Disjunctive Temporal Problems under Structural Restrictions)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Difference Logic (DL) is a fragment of linear arithmetics where atoms are
constraints x+k &lt;= y for variables x,y (ranging over Q or Z) and integer k. We
study the complexity of deciding the truth of existential DL sentences. This
problem appears in many contexts: examples include verification,
bioinformatics, telecommunications, and spatio-temporal reasoning in AI. We
begin by considering sentences in CNF with rational-valued variables. We
restrict the allowed clauses via two natural parameters: arity and coefficient
bounds. The problem is NP-hard for most choices of these parameters. As a
response to this, we refine our understanding by analyzing the time complexity
and the parameterized complexity (with respect to well-studied parameters such
as primal and incidence treewidth). We obtain a comprehensive picture of the
complexity landscape in both cases. Finally, we generalize our results to
integer domains and sentences that are not in CNF.
</p>
</div>
</dd>
<dt><a name="item889">[889]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03274" title="Abstract">arXiv:2402.03274</a> [<a href="/pdf/2402.03274" title="Download PDF">pdf</a>, <a href="/format/2402.03274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bounding the Weisfeiler-Leman Dimension via a Depth Analysis of  I/R-Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kiefer%2C+S">Sandra Kiefer</a>, 
<a href="/search/cs?searchtype=author&query=Neuen%2C+D">Daniel Neuen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Logic in Computer Science (cs.LO); Combinatorics (math.CO)

</div>
<p class="mathjax">The Weisfeiler-Leman (WL) dimension is an established measure for the
inherent descriptive complexity of graphs and relational structures. It
corresponds to the number of variables that are needed and sufficient to define
the object of interest in a counting version of first-order logic (FO). These
bounded-variable counting logics were even candidates to capture graph
isomorphism, until a celebrated construction due to Cai, F\"urer, and Immerman
[Combinatorica 1992] showed that $\Omega(n)$ variables are required to
distinguish all non-isomorphic $n$-vertex graphs.
<br />Still, very little is known about the precise number of variables required
and sufficient to define every $n$-vertex graph. For the bounded-variable
(non-counting) FO fragments, Pikhurko, Veith, and Verbitsky [Discret. Appl.
Math. 2006] provided an upper bound of $\frac{n+3}{2}$ and showed that it is
essentially tight. Our main result yields that, in the presence of counting
quantifiers, $\frac{n}{4} + o(n)$ variables suffice. This shows that counting
does allow us to save variables when defining graphs. As an application of our
techniques, we also show new bounds in terms of the vertex cover number of the
graph.
<br />To obtain the results, we introduce a new concept called the WL depth of a
graph. We use it to analyze branching trees within the
Individualization/Refinement (I/R) paradigm from the domain of isomorphism
algorithms. We extend the recursive procedure from the I/R paradigm by the
possibility of splitting the graphs into independent parts. Then we bound the
depth of the obtained branching trees, which translates into bounds on the WL
dimension and thereby on the number of variables that suffice to define the
graphs.
</p>
</div>
</dd>
<dt><a name="item890">[890]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03277" title="Abstract">arXiv:2402.03277</a> [<a href="/pdf/2402.03277" title="Download PDF">pdf</a>, <a href="/format/2402.03277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Event-based Product Carousel Recommendation with Query-Click Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Luyi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+N">Nimesh Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Vajge%2C+P">Parth Vajge</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+J+H">Jason HD Cho</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sushant Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Achan%2C+K">Kannan Achan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2 figures, 2021 IEEE International Conference on Big Data (Big Data)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Many current recommender systems mainly focus on the product-to-product
recommendations and user-to-product recommendations even during the time of
events rather than modeling the typical recommendations for the target event
(e.g., festivals, seasonal activities, or social activities) without addressing
the multiple aspects of the shopping demands for the target event. Product
recommendations for the multiple aspects of the target event are usually
generated by human curators who manually identify the aspects and select a list
of aspect-related products (i.e., product carousel) for each aspect as
recommendations. However, building a recommender system with machine learning
is non-trivial due to the lack of both the ground truth of event-related
aspects and the aspect-related products. To fill this gap, we define the novel
problem as the event-based product carousel recommendations in e-commerce and
propose an effective recommender system based on the query-click bipartite
graph. We apply the iterative clustering algorithm over the query-click
bipartite graph and infer the event-related aspects by the clusters of queries.
The aspect-related recommendations are powered by the click-through rate of
products regarding each aspect. We show through experiments that this approach
effectively mines product carousels for the target event.
</p>
</div>
</dd>
<dt><a name="item891">[891]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03279" title="Abstract">arXiv:2402.03279</a> [<a href="/pdf/2402.03279" title="Download PDF">pdf</a>, <a href="/format/2402.03279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stepping into the Right Shoes: The Effects of User-Matched Avatar  Ethnicity and Gender on Sense of Embodiment in Virtual Reality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Do%2C+T+D">Tiffany D. Do</a>, 
<a href="/search/cs?searchtype=author&query=Isabella%2C+C">Camille Isabella</a>, 
<a href="/search/cs?searchtype=author&query=McMahan%2C+R+P">Ryan P. McMahan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in IEEE Transactions on Visualization and Computer Graphics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">In many consumer virtual reality (VR) applications, users embody predefined
characters that offer minimal customization options, frequently emphasizing
storytelling over user choice. We explore whether matching a user's physical
characteristics, specifically ethnicity and gender, with their virtual
self-avatar affects their sense of embodiment in VR. We conducted a 2 x 2
within-subjects experiment (n=32) with a diverse user population to explore the
impact of matching or not matching a user's self-avatar to their ethnicity and
gender on their sense of embodiment. Our results indicate that matching the
ethnicity of the user and their self-avatar significantly enhances sense of
embodiment regardless of gender, extending across various aspects, including
appearance, response, and ownership. We also found that matching gender
significantly enhanced ownership, suggesting that this aspect is influenced by
matching both ethnicity and gender. Interestingly, we found that matching
ethnicity specifically affects self-location while matching gender specifically
affects one's body ownership.
</p>
</div>
</dd>
<dt><a name="item892">[892]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03282" title="Abstract">arXiv:2402.03282</a> [<a href="/pdf/2402.03282" title="Download PDF">pdf</a>, <a href="/ps/2402.03282" title="Download PostScript">ps</a>, <a href="/format/2402.03282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Framework for Partially Observed Reward-States in RLHF
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kausik%2C+C">Chinmaya Kausik</a>, 
<a href="/search/cs?searchtype=author&query=Mutti%2C+M">Mirco Mutti</a>, 
<a href="/search/cs?searchtype=author&query=Pacchiano%2C+A">Aldo Pacchiano</a>, 
<a href="/search/cs?searchtype=author&query=Tewari%2C+A">Ambuj Tewari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 47 pages. 13 pages for the main paper, 34 pages for the references and appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">The study of reinforcement learning from human feedback (RLHF) has gained
prominence in recent years due to its role in the development of LLMs.
Neuroscience research shows that human responses to stimuli are known to depend
on partially-observed "internal states." Unfortunately current models of RLHF
do not take take this into consideration. Moreover most RLHF models do not
account for intermediate feedback, which is gaining importance in empirical
work and can help improve both sample complexity and alignment. To address
these limitations, we model RLHF as reinforcement learning with partially
observed reward-states (PORRL). We show reductions from the the two dominant
forms of human feedback in RLHF - cardinal and dueling feedback to PORRL. For
cardinal feedback, we develop generic statistically efficient algorithms and
instantiate them to present POR-UCRL and POR-UCBVI. For dueling feedback, we
show that a naive reduction to cardinal feedback fails to achieve sublinear
dueling regret. We then present the first explicit reduction that converts
guarantees for cardinal regret to dueling regret. We show that our models and
guarantees in both settings generalize and extend existing ones. Finally, we
identify a recursive structure on our model that could improve the statistical
and computational tractability of PORRL, giving examples from past work on RLHF
as well as learning perfect reward machines, which PORRL subsumes.
</p>
</div>
</dd>
<dt><a name="item893">[893]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03283" title="Abstract">arXiv:2402.03283</a> [<a href="/pdf/2402.03283" title="Download PDF">pdf</a>, <a href="/format/2402.03283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Flexible Scale-out Framework for Efficient Visual Data Query  Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verma%2C+R">Rohit Verma</a>, 
<a href="/search/cs?searchtype=author&query=Raghunath%2C+A">Arun Raghunath</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">There is growing interest in visual data management systems that support
queries with specialized operations ranging from resizing an image to running
complex machine learning models. With a plethora of such operations, the basic
need to receive query responses in minimal time takes a hit, especially when
the client desires to run multiple such operations in a single query. Existing
systems provide an ad-hoc approach where different solutions are clubbed
together to provide an end-to-end visual data management system. Unlike such
solutions, the Visual Data Management System (VDMS) natively executes queries
with multiple operations, thus providing an end-to-end solution. However, a
fixed subset of native operations and a synchronous threading architecture
limit its generality and scalability.
<br />In this paper, we develop VDMS-Async that adds the capability to run
user-defined operations with VDMS and execute operations within a query on a
remote server. VDMS-Async utilizes an event-driven architecture to create an
efficient pipeline for executing operations within a query. Our experiments
have shown that VDMS-Async reduces the query execution time by 2-3X compared to
existing state-of-the-art systems. Further, remote operations coupled with an
event-driven architecture enables VDMS-Async to scale query execution time
linearly with the addition of every new remote server. We demonstrate a 64X
reduction in query execution time when adding 64 remote servers.
</p>
</div>
</dd>
<dt><a name="item894">[894]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03284" title="Abstract">arXiv:2402.03284</a> [<a href="/pdf/2402.03284" title="Download PDF">pdf</a>, <a href="/format/2402.03284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deal, or no deal (or who knows)? Forecasting Uncertainty in  Conversations using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sicilia%2C+A">Anthony Sicilia</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyunwoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Chandu%2C+K+R">Khyathi Raghavi Chandu</a>, 
<a href="/search/cs?searchtype=author&query=Alikhani%2C+M">Malihe Alikhani</a>, 
<a href="/search/cs?searchtype=author&query=Hessel%2C+J">Jack Hessel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 Figures; 7 Tables; 27 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Effective interlocutors account for the uncertain goals, beliefs, and
emotions of others. But even the best human conversationalist cannot perfectly
anticipate the trajectory of a dialogue. How well can language models represent
inherent uncertainty in conversations? We propose FortUne Dial, an expansion of
the long-standing "conversation forecasting" task: instead of just accuracy,
evaluation is conducted with uncertainty-aware metrics, effectively enabling
abstention on individual instances. We study two ways in which language models
potentially represent outcome uncertainty (internally, using scores and
directly, using tokens) and propose fine-tuning strategies to improve
calibration of both representations. Experiments on eight difficult negotiation
corpora demonstrate that our proposed fine-tuning strategies (a traditional
supervision strategy and an off-policy reinforcement learning strategy) can
calibrate smaller open-source models to compete with pre-trained models 10x
their size.
</p>
</div>
</dd>
<dt><a name="item895">[895]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03286" title="Abstract">arXiv:2402.03286</a> [<a href="/pdf/2402.03286" title="Download PDF">pdf</a>, <a href="/format/2402.03286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training-Free Consistent Text-to-Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tewel%2C+Y">Yoad Tewel</a>, 
<a href="/search/cs?searchtype=author&query=Kaduri%2C+O">Omri Kaduri</a>, 
<a href="/search/cs?searchtype=author&query=Gal%2C+R">Rinon Gal</a>, 
<a href="/search/cs?searchtype=author&query=Kasten%2C+Y">Yoni Kasten</a>, 
<a href="/search/cs?searchtype=author&query=Wolf%2C+L">Lior Wolf</a>, 
<a href="/search/cs?searchtype=author&query=Chechik%2C+G">Gal Chechik</a>, 
<a href="/search/cs?searchtype=author&query=Atzmon%2C+Y">Yuval Atzmon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page is in <a href="https://consistory-paper.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Text-to-image models offer a new level of creative flexibility by allowing
users to guide the image generation process through natural language. However,
using these models to consistently portray the same subject across diverse
prompts remains challenging. Existing approaches fine-tune the model to teach
it new words that describe specific user-provided subjects or add image
conditioning to the model. These methods require lengthy per-subject
optimization or large-scale pre-training. Moreover, they struggle to align
generated images with text prompts and face difficulties in portraying multiple
subjects. Here, we present ConsiStory, a training-free approach that enables
consistent subject generation by sharing the internal activations of the
pretrained model. We introduce a subject-driven shared attention block and
correspondence-based feature injection to promote subject consistency between
images. Additionally, we develop strategies to encourage layout diversity while
maintaining subject consistency. We compare ConsiStory to a range of baselines,
and demonstrate state-of-the-art performance on subject consistency and text
alignment, without requiring a single optimization step. Finally, ConsiStory
can naturally extend to multi-subject scenarios, and even enable training-free
personalization for common objects.
</p>
</div>
</dd>
<dt><a name="item896">[896]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03287" title="Abstract">arXiv:2402.03287</a> [<a href="/pdf/2402.03287" title="Download PDF">pdf</a>, <a href="/format/2402.03287" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Lennard-Jones Layer for Distribution Normalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Na%2C+M">Mulun Na</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+J">Jonathan Klein</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Biao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pa%C5%82ubicki%2C+W">Wojtek Pa&#x142;ubicki</a>, 
<a href="/search/cs?searchtype=author&query=Pirk%2C+S">S&#xf6;ren Pirk</a>, 
<a href="/search/cs?searchtype=author&query=Michels%2C+D+L">Dominik L. Michels</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Upon request, we are happy to share the source code to generate the results presented in this paper. Please contact the first or the last author of this manuscript
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">We introduce the Lennard-Jones layer (LJL) for the equalization of the
density of 2D and 3D point clouds through systematically rearranging points
without destroying their overall structure (distribution normalization). LJL
simulates a dissipative process of repulsive and weakly attractive interactions
between individual points by considering the nearest neighbor of each point at
a given moment in time. This pushes the particles into a potential valley,
reaching a well-defined stable configuration that approximates an equidistant
sampling after the stabilization process. We apply LJLs to redistribute
randomly generated point clouds into a randomized uniform distribution.
Moreover, LJLs are embedded in the generation process of point cloud networks
by adding them at later stages of the inference process. The improvements in 3D
point cloud generation utilizing LJLs are evaluated qualitatively and
quantitatively. Finally, we apply LJLs to improve the point distribution of a
score-based 3D point cloud denoising network. In general, we demonstrate that
LJLs are effective for distribution normalization which can be applied at
negligible cost without retraining the given neural network.
</p>
</div>
</dd>
<dt><a name="item897">[897]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03289" title="Abstract">arXiv:2402.03289</a> [<a href="/pdf/2402.03289" title="Download PDF">pdf</a>, <a href="/format/2402.03289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Make Every Move Count: LLM-based High-Quality RTL Code Generation Using  MCTS
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=DeLorenzo%2C+M">Matthew DeLorenzo</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+A+B">Animesh Basak Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Gohil%2C+V">Vasudev Gohil</a>, 
<a href="/search/cs?searchtype=author&query=Thakur%2C+S">Shailja Thakur</a>, 
<a href="/search/cs?searchtype=author&query=Karri%2C+R">Ramesh Karri</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Siddharth Garg</a>, 
<a href="/search/cs?searchtype=author&query=Rajendran%2C+J">Jeyavijayan Rajendran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR)

</div>
<p class="mathjax">Existing large language models (LLMs) for register transfer level code
generation face challenges like compilation failures and suboptimal power,
performance, and area (PPA) efficiency. This is due to the lack of PPA
awareness in conventional transformer decoding algorithms. In response, we
present an automated transformer decoding algorithm that integrates Monte Carlo
tree-search for lookahead, guiding the transformer to produce compilable,
functionally correct, and PPA-optimized code. Empirical evaluation with a
fine-tuned language model on RTL codesets shows that our proposed technique
consistently generates functionally correct code compared to prompting-only
methods and effectively addresses the PPA-unawareness drawback of naive large
language models. For the largest design generated by the state-of-the-art LLM
(16-bit adder), our technique can achieve a 31.8% improvement in the area-delay
product.
</p>
</div>
</dd>
<dt><a name="item898">[898]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03290" title="Abstract">arXiv:2402.03290</a> [<a href="/pdf/2402.03290" title="Download PDF">pdf</a>, <a href="/format/2402.03290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstanceDiffusion: Instance-level Control for Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xudong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Darrell%2C+T">Trevor Darrell</a>, 
<a href="/search/cs?searchtype=author&query=Rambhatla%2C+S+S">Sai Saketh Rambhatla</a>, 
<a href="/search/cs?searchtype=author&query=Girdhar%2C+R">Rohit Girdhar</a>, 
<a href="/search/cs?searchtype=author&query=Misra%2C+I">Ishan Misra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint; Project page: <a href="https://people.eecs.berkeley.edu/~xdwang/projects/InstDiff/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Text-to-image diffusion models produce high quality images but do not offer
control over individual instances in the image. We introduce InstanceDiffusion
that adds precise instance-level control to text-to-image diffusion models.
InstanceDiffusion supports free-form language conditions per instance and
allows flexible ways to specify instance locations such as simple single
points, scribbles, bounding boxes or intricate instance segmentation masks, and
combinations thereof. We propose three major changes to text-to-image models
that enable precise instance-level control. Our UniFusion block enables
instance-level conditions for text-to-image models, the ScaleU block improves
image fidelity, and our Multi-instance Sampler improves generations for
multiple instances. InstanceDiffusion significantly surpasses specialized
state-of-the-art models for each location condition. Notably, on the COCO
dataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\text{box}$
for box inputs, and 25.4% IoU for mask inputs.
</p>
</div>
</dd>
<dt><a name="item899">[899]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03291" title="Abstract">arXiv:2402.03291</a> [<a href="/pdf/2402.03291" title="Download PDF">pdf</a>, <a href="/format/2402.03291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Acquisition and Integration with Expert-in-the-loop
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahman%2C+S">Sajjadur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+F">Frederick Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hannah Kim</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hruschka%2C+E">Estevam Hruschka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Databases (cs.DB)

</div>
<p class="mathjax">Constructing and serving knowledge graphs (KGs) is an iterative and
human-centered process involving on-demand programming and analysis. In this
paper, we present Kyurem, a programmable and interactive widget library that
facilitates human-in-the-loop knowledge acquisition and integration to enable
continuous curation a knowledge graph (KG). Kyurem provides a seamless
environment within computational notebooks where data scientists explore a KG
to identify opportunities for acquiring new knowledge and verify
recommendations provided by AI agents for integrating the acquired knowledge in
the KG. We refined Kyurem through participatory design and conducted case
studies in a real-world setting for evaluation. The case-studies show that
introduction of Kyurem within an existing HR knowledge graph construction and
serving platform improved the user experience of the experts and helped
eradicate inefficiencies related to knowledge acquisition and integration tasks
</p>
</div>
</dd>
<dt><a name="item900">[900]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03292" title="Abstract">arXiv:2402.03292</a> [<a href="/pdf/2402.03292" title="Download PDF">pdf</a>, <a href="/format/2402.03292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-shot Object-Level OOD Detection with Context-Aware Inpainting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+Q">Quang-Huy Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J+P">Jin Peng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenzhen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bui%2C+K">Khanh-Huyen Bui</a>, 
<a href="/search/cs?searchtype=author&query=Weinberger%2C+K+Q">Kilian Q. Weinberger</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+D+D">Dung D. Le</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Machine learning algorithms are increasingly provided as black-box cloud
services or pre-trained models, without access to their training data. This
motivates the problem of zero-shot out-of-distribution (OOD) detection.
Concretely, we aim to detect OOD objects that do not belong to the classifier's
label set but are erroneously classified as in-distribution (ID) objects. Our
approach, RONIN, uses an off-the-shelf diffusion model to replace detected
objects with inpainting. RONIN conditions the inpainting process with the
predicted ID label, drawing the input object closer to the in-distribution
domain. As a result, the reconstructed object is very close to the original in
the ID cases and far in the OOD cases, allowing RONIN to effectively
distinguish ID and OOD samples. Throughout extensive experiments, we
demonstrate that RONIN achieves competitive results compared to previous
approaches across several datasets, both in zero-shot and non-zero-shot
settings.
</p>
</div>
</dd>
<dt><a name="item901">[901]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03293" title="Abstract">arXiv:2402.03293</a> [<a href="/pdf/2402.03293" title="Download PDF">pdf</a>, <a href="/format/2402.03293" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flora: Low-Rank Adapters Are Secretly Gradient Compressors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+Y">Yongchang Hao</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yanshuai Cao</a>, 
<a href="/search/cs?searchtype=author&query=Mou%2C+L">Lili Mou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Despite large neural networks demonstrating remarkable abilities to complete
different tasks, they require excessive memory usage to store the optimization
states for training. To alleviate this, the low-rank adaptation (LoRA) is
proposed to reduce the optimization states by training fewer parameters.
However, LoRA restricts overall weight update matrices to be low-rank, limiting
the model performance. In this work, we investigate the dynamics of LoRA and
identify that it can be approximated by a random projection. Based on this
observation, we propose Flora, which is able to achieve high-rank updates by
resampling the projection matrices while enjoying the sublinear space
complexity of optimization states. We conduct experiments across different
tasks and model architectures to verify the effectiveness of our approach.
</p>
</div>
</dd>
<dt><a name="item902">[902]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03295" title="Abstract">arXiv:2402.03295</a> [<a href="/pdf/2402.03295" title="Download PDF">pdf</a>, <a href="/format/2402.03295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ginger: An Efficient Curvature Approximation with Linear Complexity for  General Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hao%2C+Y">Yongchang Hao</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yanshuai Cao</a>, 
<a href="/search/cs?searchtype=author&query=Mou%2C+L">Lili Mou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">Second-order optimization approaches like the generalized Gauss-Newton method
are considered more powerful as they utilize the curvature information of the
objective function with preconditioning matrices. Albeit offering tempting
theoretical benefits, they are not easily applicable to modern deep learning.
The major reason is due to the quadratic memory and cubic time complexity to
compute the inverse of the matrix. These requirements are infeasible even with
state-of-the-art hardware. In this work, we propose Ginger, an
eigendecomposition for the inverse of the generalized Gauss-Newton matrix. Our
method enjoys efficient linear memory and time complexity for each iteration.
Instead of approximating the conditioning matrix, we directly maintain its
inverse to make the approximation more accurate. We provide the convergence
result of Ginger for non-convex objectives. Our experiments on different tasks
with different model architectures verify the effectiveness of our method. Our
code is publicly available.
</p>
</div>
</dd>
<dt><a name="item903">[903]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03299" title="Abstract">arXiv:2402.03299</a> [<a href="/pdf/2402.03299" title="Download PDF">pdf</a>, <a href="/format/2402.03299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GUARD: Role-playing to Generate Natural-language Jailbreakings to Test  Guideline Adherence of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+H">Haibo Jin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ruoxi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Andy Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jinyin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haohan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 papges
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The discovery of "jailbreaks" to bypass safety filters of Large Language
Models (LLMs) and harmful responses have encouraged the community to implement
safety measures. One major safety measure is to proactively test the LLMs with
jailbreaks prior to the release. Therefore, such testing will require a method
that can generate jailbreaks massively and efficiently. In this paper, we
follow a novel yet intuitive strategy to generate jailbreaks in the style of
the human generation. We propose a role-playing system that assigns four
different roles to the user LLMs to collaborate on new jailbreaks. Furthermore,
we collect existing jailbreaks and split them into different independent
characteristics using clustering frequency and semantic patterns sentence by
sentence. We organize these characteristics into a knowledge graph, making them
more accessible and easier to retrieve. Our system of different roles will
leverage this knowledge graph to generate new jailbreaks, which have proved
effective in inducing LLMs to generate unethical or guideline-violating
responses. In addition, we also pioneer a setting in our system that will
automatically follow the government-issued guidelines to generate jailbreaks to
test whether LLMs follow the guidelines accordingly. We refer to our system as
GUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have
empirically validated the effectiveness of GUARD on three cutting-edge
open-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a
widely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the
realm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing
GUARD's versatility and contributing valuable insights for the development of
safer, more reliable LLM-based applications across diverse modalities.
</p>
</div>
</dd>
<dt><a name="item904">[904]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03300" title="Abstract">arXiv:2402.03300</a> [<a href="/pdf/2402.03300" title="Download PDF">pdf</a>, <a href="/format/2402.03300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+Z">Zhihong Shao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peiyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qihao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Runxin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Junxiao Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingchuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y+K">Y.K. Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Y. Wu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+D">Daya Guo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Mathematical reasoning poses a significant challenge for language models due
to its complex and structured nature. In this paper, we introduce DeepSeekMath
7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B
math-related tokens sourced from Common Crawl, together with natural language
and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the
competition-level MATH benchmark without relying on external toolkits and
voting techniques, approaching the performance level of Gemini-Ultra and GPT-4.
Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH.
The mathematical reasoning capability of DeepSeekMath is attributed to two key
factors: First, we harness the significant potential of publicly available web
data through a meticulously engineered data selection pipeline. Second, we
introduce Group Relative Policy Optimization (GRPO), a variant of Proximal
Policy Optimization (PPO), that enhances mathematical reasoning abilities while
concurrently optimizing the memory usage of PPO.
</p>
</div>
</dd>
<dt><a name="item905">[905]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03302" title="Abstract">arXiv:2402.03302</a> [<a href="/pdf/2402.03302" title="Download PDF">pdf</a>, <a href="/format/2402.03302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Swin-UMamba: Mamba-based UNet with ImageNet-based pretraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiarun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hong-Yu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+Y">Yan Xi</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lequan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yizhou Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yong Liang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+G">Guangming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaoting Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Hairong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shanshan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Accurate medical image segmentation demands the integration of multi-scale
information, spanning from local features to global dependencies. However, it
is challenging for existing methods to model long-range global information,
where convolutional neural networks (CNNs) are constrained by their local
receptive fields, and vision transformers (ViTs) suffer from high quadratic
complexity of their attention mechanism. Recently, Mamba-based models have
gained great attention for their impressive ability in long sequence modeling.
Several studies have demonstrated that these models can outperform popular
vision models in various tasks, offering higher accuracy, lower memory
consumption, and less computational burden. However, existing Mamba-based
models are mostly trained from scratch and do not explore the power of
pretraining, which has been proven to be quite effective for data-efficient
medical image analysis. This paper introduces a novel Mamba-based model,
Swin-UMamba, designed specifically for medical image segmentation tasks,
leveraging the advantages of ImageNet-based pretraining. Our experimental
results reveal the vital role of ImageNet-based training in enhancing the
performance of Mamba-based models. Swin-UMamba demonstrates superior
performance with a large margin compared to CNNs, ViTs, and latest Mamba-based
models. Notably, on AbdomenMRI, Encoscopy, and Microscopy datasets, Swin-UMamba
outperforms its closest counterpart U-Mamba by an average score of 3.58%. The
code and models of Swin-UMamba are publicly available at:
https://github.com/JiarunLiu/Swin-UMamba
</p>
</div>
</dd>
<dt><a name="item906">[906]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03303" title="Abstract">arXiv:2402.03303</a> [<a href="/pdf/2402.03303" title="Download PDF">pdf</a>, <a href="/format/2402.03303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nevermind: Instruction Override and Moderation in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+E">Edward Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Given the impressive capabilities of recent Large Language Models (LLMs), we
investigate and benchmark the most popular proprietary and different sized open
source models on the task of explicit instruction following in conflicting
situations, e.g. overrides. These include the ability of the model to override
the knowledge within the weights of the model, the ability to override (or
moderate) extracted knowledge in the prompt, and lastly the ability to perform
a full jailbreak. Experimentation performed suggest several key findings to
improve instruction following - larger models perform the best in following
instructions that override internal and contextual instructions, and are
obedient, even to a fault. When scaling to longer contexts via rope scaling, a
significant buffer needs to be maintained from the edge of the perplexity cliff
in order to maintain instruction following capabilities. Finally, we observe
improving instruction following, and subsequently instruction
overrides/jailbreaks, is fundamentally at odds with the ability of a language
model to follow given safety filters or guidelines. Thus, we postulate the most
effective approach for safe, trustworthy AI should be dealt external to the LLM
itself.
</p>
</div>
</dd>
<dt><a name="item907">[907]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03305" title="Abstract">arXiv:2402.03305</a> [<a href="/pdf/2402.03305" title="Download PDF">pdf</a>, <a href="/format/2402.03305" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Diffusion Models Learn Semantically Meaningful and Efficient  Representations?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+Q">Qiyao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fiete%2C+I">Ila Fiete</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Diffusion models are capable of impressive feats of image generation with
uncommon juxtapositions such as astronauts riding horses on the moon with
properly placed shadows. These outputs indicate the ability to perform
compositional generalization, but how do the models do so? We perform
controlled experiments on conditional DDPMs learning to generate 2D spherical
Gaussian bumps centered at specified $x$- and $y$-positions. Our results show
that the emergence of semantically meaningful latent representations is key to
achieving high performance. En route to successful performance over learning,
the model traverses three distinct phases of latent representations: (phase A)
no latent structure, (phase B) a 2D manifold of disordered states, and (phase
C) a 2D ordered manifold. Corresponding to each of these phases, we identify
qualitatively different generation behaviors: 1) multiple bumps are generated,
2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is
generated at the correct $x$ and y location. Furthermore, we show that even
under imbalanced datasets where features ($x$- versus $y$-positions) are
represented with skewed frequencies, the learning process for $x$ and $y$ is
coupled rather than factorized, demonstrating that simple vanilla-flavored
diffusion models cannot learn efficient representations in which localization
in $x$ and $y$ are factorized into separate 1D tasks. These findings suggest
the need for future work to find inductive biases that will push generative
models to discover and exploit factorizable independent structures in their
inputs, which will be required to vault these models into more data-efficient
regimes.
</p>
</div>
</dd>
<dt><a name="item908">[908]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03306" title="Abstract">arXiv:2402.03306</a> [<a href="/pdf/2402.03306" title="Download PDF">pdf</a>, <a href="/format/2402.03306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast solutions to k-parity and k-synchronisation using parallel automata  networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Perrotin%2C+P">Pac&#xf4;me Perrotin</a>, 
<a href="/search/cs?searchtype=author&query=Ruivo%2C+E">Eurico Ruivo</a>, 
<a href="/search/cs?searchtype=author&query=Balbi%2C+P+P">Pedro Paulo Balbi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>

</div>
<p class="mathjax">We present a family of automata networks that solve the k-parity problem when
run in parallel. These solutions are constructed by connecting cliques in a
non-cyclical fashion. The size of the local neighbourhood is linear in the size
of the alphabet, and the convergence time is proven to always be the diameter
of the interaction graph. We show that this family of solutions can be slightly
altered to obtain an equivalent family of solutions to the k-synchronisation
problem, which means that these solutions converge from any initial
configuration to the cycle which contains all the uniform configurations over
the alphabet, in order.
</p>
</div>
</dd>
<dt><a name="item909">[909]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03307" title="Abstract">arXiv:2402.03307</a> [<a href="/pdf/2402.03307" title="Download PDF">pdf</a>, <a href="/format/2402.03307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 4D Gaussian Splatting: Towards Efficient Novel View Synthesis for  Dynamic Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+Y">Yuanxing Duan</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Fangyin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Q">Qiyu Dai</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yuhang He</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenzheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Baoquan Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We consider the problem of novel view synthesis (NVS) for dynamic scenes.
Recent neural approaches have accomplished exceptional NVS results for static
3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior
efforts often encode dynamics by learning a canonical space plus implicit or
explicit deformation fields, which struggle in challenging scenarios like
sudden movements or capturing high-fidelity renderings. In this paper, we
introduce 4D Gaussian Splatting (4DGS), a novel method that represents dynamic
scenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D
Gaussian Splatting in static scenes. We model dynamics at each timestamp by
temporally slicing the 4D Gaussians, which naturally compose dynamic 3D
Gaussians and can be seamlessly projected into images. As an explicit
spatial-temporal representation, 4DGS demonstrates powerful capabilities for
modeling complicated dynamics and fine details, especially for scenes with
abrupt motions. We further implement our temporal slicing and splatting
techniques in a highly optimized CUDA acceleration framework, achieving
real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and
583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions
showcase the superior efficiency and effectiveness of 4DGS, which consistently
outperforms existing methods both quantitatively and qualitatively.
</p>
</div>
</dd>
<dt><a name="item910">[910]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03309" title="Abstract">arXiv:2402.03309</a> [<a href="/pdf/2402.03309" title="Download PDF">pdf</a>, <a href="/format/2402.03309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qadri%2C+M">Mohamad Qadri</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kevin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hinduja%2C+A">Akshay Hinduja</a>, 
<a href="/search/cs?searchtype=author&query=Kaess%2C+M">Michael Kaess</a>, 
<a href="/search/cs?searchtype=author&query=Pediredla%2C+A">Adithya Pediredla</a>, 
<a href="/search/cs?searchtype=author&query=Metzler%2C+C+A">Christopher A. Metzler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First two authors contributed equally. Paper website: <a href="https://aoneus.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Underwater perception and 3D surface reconstruction are challenging problems
with broad applications in construction, security, marine archaeology, and
environmental monitoring. Treacherous operating conditions, fragile
surroundings, and limited navigation control often dictate that submersibles
restrict their range of motion and, thus, the baseline over which they can
capture measurements. In the context of 3D scene reconstruction, it is
well-known that smaller baselines make reconstruction more challenging. Our
work develops a physics-based multimodal acoustic-optical neural surface
reconstruction framework (AONeuS) capable of effectively integrating
high-resolution RGB measurements with low-resolution depth-resolved imaging
sonar measurements. By fusing these complementary modalities, our framework can
reconstruct accurate high-resolution 3D surfaces from measurements captured
over heavily-restricted baselines. Through extensive simulations and in-lab
experiments, we demonstrate that AONeuS dramatically outperforms recent
RGB-only and sonar-only inverse-differentiable-rendering--based surface
reconstruction methods. A website visualizing the results of our paper is
located at this address: https://aoneus.github.io/
</p>
</div>
</dd>
<dt><a name="item911">[911]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03310" title="Abstract">arXiv:2402.03310</a> [<a href="/pdf/2402.03310" title="Download PDF">pdf</a>, <a href="/format/2402.03310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> V-IRL: Grounding Virtual Intelligence in Real Life
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jihan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+R">Runyu Ding</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+E">Ellis Brown</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+X">Xiaojuan Qi</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Saining Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://virl-platform.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">There is a sensory gulf between the Earth that humans inhabit and the digital
realms in which modern AI agents are created. To develop AI agents that can
sense, think, and act as flexibly as humans in real-world settings, it is
imperative to bridge the realism gap between the digital and physical worlds.
How can we embody agents in an environment as rich and diverse as the one we
inhabit, without the constraints imposed by real hardware and control? Towards
this end, we introduce V-IRL: a platform that enables agents to scalably
interact with the real world in a virtual yet realistic environment. Our
platform serves as a playground for developing agents that can accomplish
various practical tasks and as a vast testbed for measuring progress in
capabilities spanning perception, decision-making, and interaction with
real-world data across the entire globe.
</p>
</div>
</dd>
<dt><a name="item912">[912]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03311" title="Abstract">arXiv:2402.03311</a> [<a href="/pdf/2402.03311" title="Download PDF">pdf</a>, <a href="/format/2402.03311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HASSOD: Hierarchical Adaptive Self-Supervised Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+S">Shengcao Cao</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+D">Dhiraj Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+L">Liang-Yan Gui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu-Xiong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The human visual perception system demonstrates exceptional capabilities in
learning without explicit supervision and understanding the part-to-whole
composition of objects. Drawing inspiration from these two abilities, we
propose Hierarchical Adaptive Self-Supervised Object Detection (HASSOD), a
novel approach that learns to detect objects and understand their compositions
without human supervision. HASSOD employs a hierarchical adaptive clustering
strategy to group regions into object masks based on self-supervised visual
representations, adaptively determining the number of objects per image.
Furthermore, HASSOD identifies the hierarchical levels of objects in terms of
composition, by analyzing coverage relations between masks and constructing
tree structures. This additional self-supervised learning task leads to
improved detection performance and enhanced interpretability. Lastly, we
abandon the inefficient multi-round self-training process utilized in prior
methods and instead adapt the Mean Teacher framework from semi-supervised
learning, which leads to a smoother and more efficient training process.
Through extensive experiments on prevalent image datasets, we demonstrate the
superiority of HASSOD over existing methods, thereby advancing the state of the
art in self-supervised object detection. Notably, we improve Mask AR from 20.2
to 22.5 on LVIS, and from 17.0 to 26.0 on SA-1B. Project page:
https://HASSOD-NeurIPS23.github.io.
</p>
</div>
</dd>
<dt><a name="item913">[913]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03312" title="Abstract">arXiv:2402.03312</a> [<a href="/pdf/2402.03312" title="Download PDF">pdf</a>, <a href="/format/2402.03312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Test-Time Adaptation for Depth Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+H">Hyoungseob Park</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Anjali Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+A">Alex Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">It is common to observe performance degradation when transferring models
trained on some (source) datasets to target testing data due to a domain gap
between them. Existing methods for bridging this gap, such as domain adaptation
(DA), may require the source data on which the model was trained (often not
available), while others, i.e., source-free DA, require many passes through the
testing data. We propose an online test-time adaptation method for depth
completion, the task of inferring a dense depth map from a single image and
associated sparse depth map, that closes the performance gap in a single pass.
We first present a study on how the domain shift in each data modality affects
model performance. Based on our observations that the sparse depth modality
exhibits a much smaller covariate shift than the image, we design an embedding
module trained in the source domain that preserves a mapping from features
encoding only sparse depth to those encoding image and sparse depth. During
test time, sparse depth features are projected using this map as a proxy for
source domain features and are used as guidance to train a set of auxiliary
parameters (i.e., adaptation layer) to align image and sparse depth features
from the target test domain to that of the source domain. We evaluate our
method on indoor and outdoor scenarios and show that it improves over baselines
by an average of 21.1%.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Tue,  6 Feb 24</h3>
<dl>
<dt><a name="item914">[914]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.05057" title="Abstract">arXiv:2304.05057</a> (cross-list from eess.IV) [<a href="/pdf/2304.05057" title="Download PDF">pdf</a>, <a href="/format/2304.05057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SFT-KD-Recon: Learning a Student-friendly Teacher for Knowledge  Distillation in Magnetic Resonance Image Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gayathri%2C+M+N">Matcha Naga Gayathri</a>, 
<a href="/search/eess?searchtype=author&query=Ramanarayanan%2C+S">Sriprabha Ramanarayanan</a>, 
<a href="/search/eess?searchtype=author&query=Fahim%2C+M+A">Mohammad Al Fahim</a>, 
<a href="/search/eess?searchtype=author&query=S%2C+R+G">Rahul G S</a>, 
<a href="/search/eess?searchtype=author&query=Ram%2C+K">Keerthi Ram</a>, 
<a href="/search/eess?searchtype=author&query=Sivaprakasam%2C+M">Mohanasankar Sivaprakasam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 8 figures. Accepted for publication at MIDL 2023. Code for our proposed method is available at <a href="https://github.com/GayathriMatcha/SFT-KD-Recon">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Deep cascaded architectures for magnetic resonance imaging (MRI) acceleration
have shown remarkable success in providing high-quality reconstruction.
However, as the number of cascades increases, the improvements in
reconstruction tend to become marginal, indicating possible excess model
capacity. Knowledge distillation (KD) is an emerging technique to compress
these models, in which a trained deep teacher network is used to distill
knowledge to a smaller student network such that the student learns to mimic
the behavior of the teacher. Most KD methods focus on effectively training the
student with a pre-trained teacher unaware of the student model. We propose
SFT-KD-Recon, a student-friendly teacher training approach along with the
student as a prior step to KD to make the teacher aware of the structure and
capacity of the student and enable aligning the representations of the teacher
with the student. In SFT, the teacher is jointly trained with the unfolded
branch configurations of the student blocks using three loss terms -
teacher-reconstruction loss, student-reconstruction loss, and teacher-student
imitation loss, followed by KD of the student. We perform extensive experiments
for MRI acceleration in 4x and 5x under-sampling on the brain and cardiac
datasets on five KD methods using the proposed approach as a prior step. We
consider the DC-CNN architecture and setup teacher as D5C5 (141765 parameters),
and student as D3C5 (49285 parameters), denoting a compression of 2.87:1.
Results show that (i) our approach consistently improves the KD methods with
improved reconstruction performance and image quality, and (ii) the student
distilled using our approach is competitive with the teacher, with the
performance gap reduced from 0.53 dB to 0.03 dB.
</p>
</div>
</dd>
<dt><a name="item915">[915]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15018" title="Abstract">arXiv:2401.15018</a> (cross-list from eess.AS) [<a href="/pdf/2401.15018" title="Download PDF">pdf</a>, <a href="/ps/2401.15018" title="Download PostScript">ps</a>, <a href="/format/2401.15018" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancement of a Text-Independent Speaker Verification System by using  Feature Combination and Parallel-Structure Classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Abdalmalak%2C+K+A">Kerlos Atia Abdalmalak</a>, 
<a href="/search/eess?searchtype=author&query=Gallardo-Antol%27in%2C+A">Ascensi&#xf3;n Gallardo-Antol&#x27;in</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Neural Computing and Applications 29 (2018) 637-651
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Sound (cs.SD)

</div>
<p class="mathjax">Speaker Verification (SV) systems involve mainly two individual stages:
feature extraction and classification. In this paper, we explore these two
modules with the aim of improving the performance of a speaker verification
system under noisy conditions. On the one hand, the choice of the most
appropriate acoustic features is a crucial factor for performing robust speaker
verification. The acoustic parameters used in the proposed system are: Mel
Frequency Cepstral Coefficients (MFCC), their first and second derivatives
(Deltas and Delta- Deltas), Bark Frequency Cepstral Coefficients (BFCC),
Perceptual Linear Predictive (PLP), and Relative Spectral Transform -
Perceptual Linear Predictive (RASTA-PLP). In this paper, a complete comparison
of different combinations of the previous features is discussed. On the other
hand, the major weakness of a conventional Support Vector Machine (SVM)
classifier is the use of generic traditional kernel functions to compute the
distances among data points. However, the kernel function of an SVM has great
influence on its performance. In this work, we propose the combination of two
SVM-based classifiers with different kernel functions: Linear kernel and
Gaussian Radial Basis Function (RBF) kernel with a Logistic Regression (LR)
classifier. The combination is carried out by means of a parallel structure
approach, in which different voting rules to take the final decision are
considered. Results show that significant improvement in the performance of the
SV system is achieved by using the combined features with the combined
classifiers either with clean speech or in the presence of noise. Finally, to
enhance the system more in noisy environments, the inclusion of the multiband
noise removal technique as a preprocessing stage is proposed.
</p>
</div>
</dd>
<dt><a name="item916">[916]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01699" title="Abstract">arXiv:2402.01699</a> (cross-list from econ.TH) [<a href="/pdf/2402.01699" title="Download PDF">pdf</a>, <a href="/ps/2402.01699" title="Download PostScript">ps</a>, <a href="/format/2402.01699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intergenerational Preferences and Continuity: Reconciling Order and  Topology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Estevan%2C+A">Asier Estevan</a>, 
<a href="/search/econ?searchtype=author&query=Maura%2C+R">Roberto Maura</a>, 
<a href="/search/econ?searchtype=author&query=Valero%2C+O">Oscar Valero</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">In this paper we focus our efforts on studying how a preorder and topology
can be made compatible. Thus we provide a characterization of those that are
continuous-compatible. Such a characterization states that such topologies must
be finer than the so-called upper topology induced by the preorder and, thus,
it clarifies which topology is the smallest one among those that make the
preorder continuous. Moreover, we provide sufficient conditions that allows us
to discard in an easy way the continuity of a preference. In the light of the
obtained results, we provide possibility counterparts of the a few celebrate
impossibility theorems for continuous social social intergenerational
preferences due to P. Diamond, L.G. Svensson and T. Sakai. Furthermore, we
suggest quasi-pseudo-metrics as appropriate quantitative tool for reconciling
topology and social intergenerational preferences. Thus, we develop a metric
type method which is able to guarantee possibility counterparts of the
aforesaid impossibility theorems and, in addition, it is able to give numerical
quantifications of the improvement of welfare. We also show that our method
makes always the intergenerational preferences semi-continuous multi-utility
representables in the sense of \"{O}zg\"{u} Evern and Efe O. Ok. Finally, in
order to keep close to the classical way of measuring in the literature, a
refinement of the previous method is presented in such a way that metrics are
involved.
</p>
</div>
</dd>
<dt><a name="item917">[917]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01744" title="Abstract">arXiv:2402.01744</a> (cross-list from q-bio.QM) [<a href="/pdf/2402.01744" title="Download PDF">pdf</a>, <a href="/format/2402.01744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling Molecular Moieties through Hierarchical Graph Explainability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Sortino%2C+P">Paolo Sortino</a>, 
<a href="/search/q-bio?searchtype=author&query=Contino%2C+S">Salvatore Contino</a>, 
<a href="/search/q-bio?searchtype=author&query=Perricone%2C+U">Ugo Perricone</a>, 
<a href="/search/q-bio?searchtype=author&query=Pirrone%2C+R">Roberto Pirrone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Molecular Networks (q-bio.MN)

</div>
<p class="mathjax">Background: Graph Neural Networks (GNN) have emerged in very recent years as
a powerful tool for supporting in silico Virtual Screening. In this work we
present a GNN which uses Graph Convolutional architectures to achieve very
accurate multi-target screening. We also devised a hierarchical Explainable
Artificial Intelligence (XAI) technique to catch information directly at atom,
ring, and whole molecule level by leveraging the message passing mechanism. In
this way, we find the most relevant moieties involved in bioactivity
prediction. Results: We report a state-of-the-art GNN classifier on twenty
Cyclin-dependent Kinase targets in support of VS. Our classifier outperforms
previous SOTA approaches proposed by the authors. Moreover, a CDK1-only
high-sensitivity version of the GNN has been designed to use our explainer in
order to avoid the inherent bias of multi-class models. The hierarchical
explainer has been validated by an expert chemist on 19 approved drugs on CDK1.
Our explainer provided information in accordance to the docking analysis for 17
out of the 19 test drugs. Conclusion: Our approach is a valid support for
shortening both the screening and the hit-to-lead phase. Detailed knowledge
about the molecular substructures that play a role in the inhibitory action,
can help the computational chemist to gain insights into the pharmacophoric
function of the molecule also for repurposing purposes.
</p>
</div>
</dd>
<dt><a name="item918">[918]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01752" title="Abstract">arXiv:2402.01752</a> (cross-list from eess.AS) [<a href="/pdf/2402.01752" title="Download PDF">pdf</a>, <a href="/ps/2402.01752" title="Download PostScript">ps</a>, <a href="/format/2402.01752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying False Content and Hate Speech in Sinhala YouTube Videos by  Analyzing the Audio
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wickramaarachchi%2C+W+A+K+M">W. A. K. M. Wickramaarachchi</a>, 
<a href="/search/eess?searchtype=author&query=Subasinghe%2C+S+S">Sameeri Sathsara Subasinghe</a>, 
<a href="/search/eess?searchtype=author&query=Wijerathna%2C+K+K+R+T">K. K. Rashani Tharushika Wijerathna</a>, 
<a href="/search/eess?searchtype=author&query=Athukorala%2C+A+S+U">A. Sahashra Udani Athukorala</a>, 
<a href="/search/eess?searchtype=author&query=Abeywardhana%2C+L">Lakmini Abeywardhana</a>, 
<a href="/search/eess?searchtype=author&query=Karunasena%2C+A">A. Karunasena</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD)

</div>
<p class="mathjax">YouTube faces a global crisis with the dissemination of false information and
hate speech. To counter these issues, YouTube has implemented strict rules
against uploading content that includes false information or promotes hate
speech. While numerous studies have been conducted to reduce offensive
English-language content, there's a significant lack of research on Sinhala
content. This study aims to address the aforementioned gap by proposing a
solution to minimize the spread of violence and misinformation in Sinhala
YouTube videos. The approach involves developing a rating system that assesses
whether a video contains false information by comparing the title and
description with the audio content and evaluating whether the video includes
hate speech. The methodology encompasses several steps, including audio
extraction using the Pytube library, audio transcription via the fine-tuned
Whisper model, hate speech detection employing the distilroberta-base model and
a text classification LSTM model, and text summarization through the fine-tuned
BART-Large- XSUM model. Notably, the Whisper model achieved a 48.99\% word
error rate, while the distilroberta-base model demonstrated an F1 score of
0.856 and a recall value of 0.861 in comparison to the LSTM model, which
exhibited signs of overfitting.
</p>
</div>
</dd>
<dt><a name="item919">[919]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01778" title="Abstract">arXiv:2402.01778</a> (cross-list from eess.AS) [<a href="/pdf/2402.01778" title="Download PDF">pdf</a>, <a href="/format/2402.01778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Introduction to speech recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Dauphin%2C+G">Gabriel Dauphin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> in French language
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This document contains lectures and practical experimentations using Matlab
and implementing a system which is actually correctly classifying three words
(one, two and three) with the help of a very small database. To achieve this
performance, it uses speech modeling specificities, powerful computer
algorithms (dynamic time warping and Dijktra's algorithm) and machine learning
(nearest neighbor). This document introduces also some machine learning
evaluation metrics.
</p>
</div>
</dd>
<dt><a name="item920">[920]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01779" title="Abstract">arXiv:2402.01779</a> (cross-list from eess.IV) [<a href="/pdf/2402.01779" title="Download PDF">pdf</a>, <a href="/format/2402.01779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Plug-and-Play image restoration with Stochastic deNOising REgularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Renaud%2C+M">Marien Renaud</a>, 
<a href="/search/eess?searchtype=author&query=Prost%2C+J">Jean Prost</a>, 
<a href="/search/eess?searchtype=author&query=Leclaire%2C+A">Arthur Leclaire</a>, 
<a href="/search/eess?searchtype=author&query=Papadakis%2C+N">Nicolas Papadakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Plug-and-Play (PnP) algorithms are a class of iterative algorithms that
address image inverse problems by combining a physical model and a deep neural
network for regularization. Even if they produce impressive image restoration
results, these algorithms rely on a non-standard use of a denoiser on images
that are less and less noisy along the iterations, which contrasts with recent
algorithms based on Diffusion Models (DM), where the denoiser is applied only
on re-noised images. We propose a new PnP framework, called Stochastic
deNOising REgularization (SNORE), which applies the denoiser only on images
with noise of the adequate level. It is based on an explicit stochastic
regularization, which leads to a stochastic gradient descent algorithm to solve
ill-posed inverse problems. A convergence analysis of this algorithm and its
annealing extension is provided. Experimentally, we prove that SNORE is
competitive with respect to state-of-the-art methods on deblurring and
inpainting tasks, both quantitatively and qualitatively.
</p>
</div>
</dd>
<dt><a name="item921">[921]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01791" title="Abstract">arXiv:2402.01791</a> (cross-list from quant-ph) [<a href="/pdf/2402.01791" title="Download PDF">pdf</a>, <a href="/format/2402.01791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Quantum Circuits Enhanced Generative Adversarial Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Shu%2C+R">Runqiu Shu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Xu%2C+X">Xusheng Xu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yung%2C+M">Man-Hong Yung</a>, 
<a href="/search/quant-ph?searchtype=author&query=Cui%2C+W">Wei Cui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Machine Learning (cs.LG)

</div>
<p class="mathjax">Generative adversarial network (GAN) is one of the widely-adopted
machine-learning frameworks for a wide range of applications such as generating
high-quality images, video, and audio contents. However, training a GAN could
become computationally expensive for large neural networks. In this work, we
propose a hybrid quantum-classical architecture for improving GAN (denoted as
QC-GAN). The performance was examed numerically by benchmarking with a
classical GAN using MindSpore Quantum on the task of hand-written image
generation. The generator of the QC-GAN consists of a quantum variational
circuit together with a one-layer neural network, and the discriminator
consists of a traditional neural network. Leveraging the entangling and
expressive power of quantum circuits, our hybrid architecture achieved better
performance (Frechet Inception Distance) than the classical GAN, with much
fewer training parameters and number of iterations for convergence. We have
also demonstrated the superiority of QC-GAN over an alternative quantum GAN,
namely pathGAN, which could hardly generate 16$\times$16 or larger images. This
work demonstrates the value of combining ideas from quantum computing with
machine learning for both areas of Quantum-for-AI and AI-for-Quantum.
</p>
</div>
</dd>
<dt><a name="item922">[922]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01796" title="Abstract">arXiv:2402.01796</a> (cross-list from eess.AS) [<a href="/pdf/2402.01796" title="Download PDF">pdf</a>, <a href="/format/2402.01796" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring transfer learning for pathological speech feature prediction:  Impact of layer selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wiepert%2C+D+A">Daniela A. Wiepert</a>, 
<a href="/search/eess?searchtype=author&query=Utianski%2C+R+L">Rene L. Utianski</a>, 
<a href="/search/eess?searchtype=author&query=Duffy%2C+J+R">Joseph R. Duffy</a>, 
<a href="/search/eess?searchtype=author&query=Stricker%2C+J+L">John L. Stricker</a>, 
<a href="/search/eess?searchtype=author&query=Barnard%2C+L+R">Leland R. Barnard</a>, 
<a href="/search/eess?searchtype=author&query=Jones%2C+D+T">David T. Jones</a>, 
<a href="/search/eess?searchtype=author&query=Botha%2C+H">Hugo Botha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">There is interest in leveraging AI to conduct automatic, objective
assessments of clinical speech, in turn facilitating diagnosis and treatment of
speech disorders. We explore transfer learning, focusing on the impact of layer
selection, for the downstream task of predicting the presence of pathological
speech. We find that selecting an optimal layer offers large performance
improvements (12.4% average increase in balanced accuracy), though the best
layer varies by predicted feature and does not always generalize well to unseen
data. A learned weighted sum offers comparable performance to the average best
layer in-distribution and has better generalization for out-of-distribution
data.
</p>
</div>
</dd>
<dt><a name="item923">[923]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01809" title="Abstract">arXiv:2402.01809</a> (cross-list from q-bio.GN) [<a href="/pdf/2402.01809" title="Download PDF">pdf</a>, <a href="/format/2402.01809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PhenoLinker: Phenotype-Gene Link Prediction and Explanation using  Heterogeneous Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Andreu%2C+J+L+M">Jose L. Mellina Andreu</a>, 
<a href="/search/q-bio?searchtype=author&query=Bernal%2C+L">Luis Bernal</a>, 
<a href="/search/q-bio?searchtype=author&query=Skarmeta%2C+A+F">Antonio F. Skarmeta</a>, 
<a href="/search/q-bio?searchtype=author&query=Ryten%2C+M">Mina Ryten</a>, 
<a href="/search/q-bio?searchtype=author&query=%C3%81lvarez%2C+S">Sara &#xc1;lvarez</a>, 
<a href="/search/q-bio?searchtype=author&query=Garc%C3%ADa%2C+A+C">Alejandro Cisterna Garc&#xed;a</a>, 
<a href="/search/q-bio?searchtype=author&query=Bot%C3%ADa%2C+J+A">Juan A. Bot&#xed;a</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The association of a given human phenotype to a genetic variant remains a
critical challenge for biology. We present a novel system called PhenoLinker
capable of associating a score to a phenotype-gene relationship by using
heterogeneous information networks and a convolutional neural network-based
model for graphs, which can provide an explanation for the predictions. This
system can aid in the discovery of new associations and in the understanding of
the consequences of human genetic variation.
</p>
</div>
</dd>
<dt><a name="item924">[924]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01810" title="Abstract">arXiv:2402.01810</a> (cross-list from stat.ML) [<a href="/pdf/2402.01810" title="Download PDF">pdf</a>, <a href="/format/2402.01810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Misspecification uncertainties in near-deterministic regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Swinburne%2C+T+D">Thomas D Swinburne</a>, 
<a href="/search/stat?searchtype=author&query=Perez%2C+D">Danny Perez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an)

</div>
<p class="mathjax">The expected loss is an upper bound to the model generalization error which
admits robust PAC-Bayes bounds for learning. However, loss minimization is
known to ignore misspecification, where models cannot exactly reproduce
observations. This leads to significant underestimates of parameter
uncertainties in the large data, or underparameterized, limit. We analyze the
generalization error of near-deterministic, misspecified and underparametrized
surrogate models, a regime of broad relevance in science and engineering. We
show posterior distributions must cover every training point to avoid a
divergent generalization error and derive an ensemble {ansatz} that respects
this constraint, which for linear models incurs minimal overhead. The efficient
approach is demonstrated on model problems before application to high
dimensional datasets in atomistic machine learning. Parameter uncertainties
from misspecification survive in the underparametrized limit, giving accurate
prediction and bounding of test errors.
</p>
</div>
</dd>
<dt><a name="item925">[925]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01829" title="Abstract">arXiv:2402.01829</a> (cross-list from q-bio.BM) [<a href="/pdf/2402.01829" title="Download PDF">pdf</a>, <a href="/format/2402.01829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting ATP binding sites in protein sequences using Deep Learning  and Natural Language Processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=V%2C+S">Shreyas V</a>, 
<a href="/search/q-bio?searchtype=author&query=Agarwal%2C+S">Swati Agarwal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at 3rd Annual AAAI Workshop on AI to Accelerate Science and Engineering (AI2ASE)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Predicting ATP-Protein Binding sites in genes is of great significance in the
field of Biology and Medicine. The majority of research in this field has been
conducted through time- and resource-intensive 'wet experiments' in
laboratories. Over the years, researchers have been investigating computational
methods computational methods to accomplish the same goals, utilising the
strength of advanced Deep Learning and NLP algorithms. In this paper, we
propose to develop methods to classify ATP-Protein binding sites. We conducted
various experiments mainly using PSSMs and several word embeddings as features.
We used 2D CNNs and LightGBM classifiers as our chief Deep Learning Algorithms.
The MP3Vec and BERT models have also been subjected to testing in our study.
The outcomes of our experiments demonstrated improvement over the
state-of-the-art benchmarks.
</p>
</div>
</dd>
<dt><a name="item926">[926]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01855" title="Abstract">arXiv:2402.01855</a> (cross-list from stat.ML) [<a href="/pdf/2402.01855" title="Download PDF">pdf</a>, <a href="/format/2402.01855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPDE priors for uncertainty quantification of end-to-end neural data  assimilation schemes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Beauchamp%2C+M">Maxime Beauchamp</a>, 
<a href="/search/stat?searchtype=author&query=Desassis%2C+N">Nicolas Desassis</a>, 
<a href="/search/stat?searchtype=author&query=Johnson%2C+J+E">J. Emmanuel Johnson</a>, 
<a href="/search/stat?searchtype=author&query=Benaichouche%2C+S">Simon Benaichouche</a>, 
<a href="/search/stat?searchtype=author&query=Tandeo%2C+P">Pierre Tandeo</a>, 
<a href="/search/stat?searchtype=author&query=Fablet%2C+R">Ronan Fablet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">The spatio-temporal interpolation of large geophysical datasets has
historically been adressed by Optimal Interpolation (OI) and more sophisticated
model-based or data-driven DA techniques. In the last ten years, the link
established between Stochastic Partial Differential Equations (SPDE) and
Gaussian Markov Random Fields (GMRF) opened a new way of handling both large
datasets and physically-induced covariance matrix in Optimal Interpolation.
Recent advances in the deep learning community also enables to adress this
problem as neural architecture embedding data assimilation variational
framework. The reconstruction task is seen as a joint learning problem of the
prior involved in the variational inner cost and the gradient-based
minimization of the latter: both prior models and solvers are stated as neural
networks with automatic differentiation which can be trained by minimizing a
loss function, typically stated as the mean squared error between some ground
truth and the reconstruction. In this work, we draw from the SPDE-based
Gaussian Processes to estimate complex prior models able to handle
non-stationary covariances in both space and time and provide a stochastic
framework for interpretability and uncertainty quantification. Our neural
variational scheme is modified to embed an augmented state formulation with
both state and SPDE parametrization to estimate. Instead of a neural prior, we
use a stochastic PDE as surrogate model along the data assimilation window. The
training involves a loss function for both reconstruction task and SPDE prior
model, where the likelihood of the SPDE parameters given the true states is
involved in the training. Because the prior is stochastic, we can easily draw
samples in the prior distribution before conditioning to provide a flexible way
to estimate the posterior distribution based on thousands of members.
</p>
</div>
</dd>
<dt><a name="item927">[927]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01876" title="Abstract">arXiv:2402.01876</a> (cross-list from hep-ex) [<a href="/pdf/2402.01876" title="Download PDF">pdf</a>, <a href="/format/2402.01876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sets are all you need: Ultrafast jet classification on FPGAs for HL-LHC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-ex?searchtype=author&query=Odagiu%2C+P">Patrick Odagiu</a>, 
<a href="/search/hep-ex?searchtype=author&query=Que%2C+Z">Zhiqiang Que</a>, 
<a href="/search/hep-ex?searchtype=author&query=Duarte%2C+J">Javier Duarte</a>, 
<a href="/search/hep-ex?searchtype=author&query=Haller%2C+J">Johannes Haller</a>, 
<a href="/search/hep-ex?searchtype=author&query=Kasieczka%2C+G">Gregor Kasieczka</a>, 
<a href="/search/hep-ex?searchtype=author&query=Lobanov%2C+A">Artur Lobanov</a>, 
<a href="/search/hep-ex?searchtype=author&query=Loncar%2C+V">Vladimir Loncar</a>, 
<a href="/search/hep-ex?searchtype=author&query=Luk%2C+W">Wayne Luk</a>, 
<a href="/search/hep-ex?searchtype=author&query=Ngadiuba%2C+J">Jennifer Ngadiuba</a>, 
<a href="/search/hep-ex?searchtype=author&query=Pierini%2C+M">Maurizio Pierini</a>, 
<a href="/search/hep-ex?searchtype=author&query=Rincke%2C+P">Philipp Rincke</a>, 
<a href="/search/hep-ex?searchtype=author&query=Seksaria%2C+A">Arpita Seksaria</a>, 
<a href="/search/hep-ex?searchtype=author&query=Summers%2C+S">Sioni Summers</a>, 
<a href="/search/hep-ex?searchtype=author&query=Sznajder%2C+A">Andre Sznajder</a>, 
<a href="/search/hep-ex?searchtype=author&query=Tapper%2C+A">Alexander Tapper</a>, 
<a href="/search/hep-ex?searchtype=author&query=Aarrestad%2C+T+K">Thea K. Aarrestad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 3 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Experiment (hep-ex)</span>; Machine Learning (cs.LG); Instrumentation and Detectors (physics.ins-det)

</div>
<p class="mathjax">We study various machine learning based algorithms for performing accurate
jet flavor classification on field-programmable gate arrays and demonstrate how
latency and resource consumption scale with the input size and choice of
algorithm. These architectures provide an initial design for models that could
be used for tagging at the CERN LHC during its high-luminosity phase. The
high-luminosity upgrade will lead to a five-fold increase in its instantaneous
luminosity for proton-proton collisions and, in turn, higher data volume and
complexity, such as the availability of jet constituents. Through
quantization-aware training and efficient hardware implementations, we show
that O(100) ns inference of complex architectures such as deep sets and
interaction networks is feasible at a low computational resource cost.
</p>
</div>
</dd>
<dt><a name="item928">[928]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01887" title="Abstract">arXiv:2402.01887</a> (cross-list from stat.ML) [<a href="/pdf/2402.01887" title="Download PDF">pdf</a>, <a href="/format/2402.01887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On f-Divergence Principled Domain Adaptation: An Improved Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wang%2C+Z">Ziqiao Wang</a>, 
<a href="/search/stat?searchtype=author&query=Mao%2C+Y">Yongyi Mao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Unsupervised domain adaptation (UDA) plays a crucial role in addressing
distribution shifts in machine learning. In this work, we improve the
theoretical foundations of UDA proposed by Acuna et al. (2021) by refining
their f-divergence-based discrepancy and additionally introducing a new
measure, f-domain discrepancy (f-DD). By removing the absolute value function
and incorporating a scaling parameter, f-DD yields novel target error and
sample complexity bounds, allowing us to recover previous KL-based results and
bridging the gap between algorithms and theory presented in Acuna et al.
(2021). Leveraging a localization technique, we also develop a fast-rate
generalization bound. Empirical results demonstrate the superior performance of
f-DD-based domain learning algorithms over previous works in popular UDA
benchmarks.
</p>
</div>
</dd>
<dt><a name="item929">[929]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01900" title="Abstract">arXiv:2402.01900</a> (cross-list from stat.ML) [<a href="/pdf/2402.01900" title="Download PDF">pdf</a>, <a href="/format/2402.01900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributional Off-policy Evaluation with Bellman Residual Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hong%2C+S">Sungee Hong</a>, 
<a href="/search/stat?searchtype=author&query=Qi%2C+Z">Zhengling Qi</a>, 
<a href="/search/stat?searchtype=author&query=Wong%2C+R+K+W">Raymond K. W. Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We consider the problem of distributional off-policy evaluation which serves
as the foundation of many distributional reinforcement learning (DRL)
algorithms. In contrast to most existing works (that rely on supremum-extended
statistical distances such as supremum-Wasserstein distance), we study the
expectation-extended statistical distance for quantifying the distributional
Bellman residuals and show that it can upper bound the expected error of
estimating the return distribution. Based on this appealing property, by
extending the framework of Bellman residual minimization to DRL, we propose a
method called Energy Bellman Residual Minimizer (EBRM) to estimate the return
distribution. We establish a finite-sample error bound for the EBRM estimator
under the realizability assumption. Furthermore, we introduce a variant of our
method based on a multi-step bootstrapping procedure to enable multi-step
extension. By selecting an appropriate step level, we obtain a better error
bound for this variant of EBRM compared to a single-step EBRM, under some
non-realizability settings. Finally, we demonstrate the superior performance of
our method through simulation studies, comparing with several existing methods.
</p>
</div>
</dd>
<dt><a name="item930">[930]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01942" title="Abstract">arXiv:2402.01942</a> (cross-list from q-bio.GN) [<a href="/pdf/2402.01942" title="Download PDF">pdf</a>, <a href="/ps/2402.01942" title="Download PostScript">ps</a>, <a href="/format/2402.01942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pairwise Rearrangement is Fixed-Parameter Tractable in the Single  Cut-and-Join Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Bailey%2C+L">Lora Bailey</a>, 
<a href="/search/q-bio?searchtype=author&query=Blake%2C+H+S">Heather Smith Blake</a>, 
<a href="/search/q-bio?searchtype=author&query=Cochran%2C+G">Garner Cochran</a>, 
<a href="/search/q-bio?searchtype=author&query=Fox%2C+N">Nathan Fox</a>, 
<a href="/search/q-bio?searchtype=author&query=Levet%2C+M">Michael Levet</a>, 
<a href="/search/q-bio?searchtype=author&query=Mahmoud%2C+R">Reem Mahmoud</a>, 
<a href="/search/q-bio?searchtype=author&query=Singgih%2C+I">Inne Singgih</a>, 
<a href="/search/q-bio?searchtype=author&query=Stadnyk%2C+G">Grace Stadnyk</a>, 
<a href="/search/q-bio?searchtype=author&query=Wiedemann%2C+A">Alexander Wiedemann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2305.01851">arXiv:2305.01851</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Data Structures and Algorithms (cs.DS); Combinatorics (math.CO)

</div>
<p class="mathjax">Genome rearrangement is a common model for molecular evolution. In this
paper, we consider the Pairwise Rearrangement problem, which takes as input two
genomes and asks for the number of minimum-length sequences of permissible
operations transforming the first genome into the second. In the Single
Cut-and-Join model (Bergeron, Medvedev, &amp; Styoe, J. Comput. Biol. 2010),
Pairwise Rearrangement is $\#\textsf{P}$-complete (Bailey, et. al., COCOON
2023), which implies that exact sampling is intractable. In order to cope with
this intractability, we investigate the parameterized complexity of this
problem. We exhibit a fixed-parameter tractable algorithm with respect to the
number of components in the adjacency graph that are not cycles of length $2$
or paths of length $1$. As a consequence, we obtain that Pairwise Rearrangement
in the Single Cut-and-Join model is fixed-parameter tractable by distance. Our
results suggest that the number of nontrivial components in the adjacency graph
serves as the key obstacle for efficient sampling.
</p>
</div>
</dd>
<dt><a name="item931">[931]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01972" title="Abstract">arXiv:2402.01972</a> (cross-list from stat.ML) [<a href="/pdf/2402.01972" title="Download PDF">pdf</a>, <a href="/format/2402.01972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining T-learning and DR-learning: a framework for oracle-efficient  estimation of causal contrasts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=van+der+Laan%2C+L">Lars van der Laan</a>, 
<a href="/search/stat?searchtype=author&query=Carone%2C+M">Marco Carone</a>, 
<a href="/search/stat?searchtype=author&query=Luedtke%2C+A">Alex Luedtke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">We introduce efficient plug-in (EP) learning, a novel framework for the
estimation of heterogeneous causal contrasts, such as the conditional average
treatment effect and conditional relative risk. The EP-learning framework
enjoys the same oracle-efficiency as Neyman-orthogonal learning strategies,
such as DR-learning and R-learning, while addressing some of their primary
drawbacks, including that (i) their practical applicability can be hindered by
loss function non-convexity; and (ii) they may suffer from poor performance and
instability due to inverse probability weighting and pseudo-outcomes that
violate bounds. To avoid these drawbacks, EP-learner constructs an efficient
plug-in estimator of the population risk function for the causal contrast,
thereby inheriting the stability and robustness properties of plug-in
estimation strategies like T-learning. Under reasonable conditions, EP-learners
based on empirical risk minimization are oracle-efficient, exhibiting
asymptotic equivalence to the minimizer of an oracle-efficient one-step
debiased estimator of the population risk function. In simulation experiments,
we illustrate that EP-learners of the conditional average treatment effect and
conditional relative risk outperform state-of-the-art competitors, including
T-learner, R-learner, and DR-learner. Open-source implementations of the
proposed methods are available in our R package hte3.
</p>
</div>
</dd>
<dt><a name="item932">[932]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02041" title="Abstract">arXiv:2402.02041</a> (cross-list from stat.ML) [<a href="/pdf/2402.02041" title="Download PDF">pdf</a>, <a href="/ps/2402.02041" title="Download PostScript">ps</a>, <a href="/format/2402.02041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $&#x3b1;$-Divergence Loss Function for Neural Density Ratio Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kitazawa%2C+Y">Yoshiaki Kitazawa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recently, neural networks have produced state-of-the-art results for
density-ratio estimation (DRE), a fundamental technique in machine learning.
However, existing methods bear optimization issues that arise from the loss
functions of DRE: a large sample requirement of Kullback--Leibler
(KL)-divergence, vanishing of train loss gradients, and biased gradients of the
loss functions. Thus, an $\alpha$-divergence loss function ($\alpha$-Div) that
offers concise implementation and stable optimization is proposed in this
paper. Furthermore, technical justifications for the proposed loss function are
presented. The stability of the proposed loss function is empirically
demonstrated and the estimation accuracy of DRE tasks is investigated.
Additionally, this study presents a sample requirement for DRE using the
proposed loss function in terms of the upper bound of $L_1$ error, which
connects a curse of dimensionality as a common problem in high-dimensional DRE
tasks.
</p>
</div>
</dd>
<dt><a name="item933">[933]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02059" title="Abstract">arXiv:2402.02059</a> (cross-list from math.ST) [<a href="/pdf/2402.02059" title="Download PDF">pdf</a>, <a href="/format/2402.02059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiple sequences Prophet Inequality Under Observation Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Tsopelakos%2C+A">Aristomenis Tsopelakos</a>, 
<a href="/search/math?searchtype=author&query=Milenkovic%2C+O">Olgica Milenkovic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Information Theory (cs.IT); Probability (math.PR)

</div>
<p class="mathjax">In our problem, we are given access to a number of sequences of nonnegative
i.i.d. random variables, whose realizations are observed sequentially. All
sequences are of the same finite length. The goal is to pick one element from
each sequence in order to maximize a reward equal to the expected value of the
sum of the selections from all sequences. The decision on which element to pick
is irrevocable, i.e., rejected observations cannot be revisited. Furthermore,
the procedure terminates upon having a single selection from each sequence. Our
observation constraint is that we cannot observe the current realization of all
sequences at each time instant. Instead, we can observe only a smaller, yet
arbitrary, subset of them. Thus, together with a stopping rule that determines
whether we choose or reject the sample, the solution requires a sampling rule
that determines which sequence to observe at each instant. The problem can be
solved via dynamic programming, but with an exponential complexity in the
length of the sequences. In order to make the solution computationally
tractable, we introduce a decoupling approach and determine each stopping time
using either a single-sequence dynamic programming, or a Prophet Inequality
inspired threshold method, with polynomial complexity in the length of the
sequences. We prove that the decoupling approach guarantees at least 0.745 of
the optimal expected reward of the joint problem. In addition, we describe how
to efficiently compute the optimal number of samples for each sequence, and
its' dependence on the variances.
</p>
</div>
</dd>
<dt><a name="item934">[934]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02098" title="Abstract">arXiv:2402.02098</a> (cross-list from stat.ML) [<a href="/pdf/2402.02098" title="Download PDF">pdf</a>, <a href="/format/2402.02098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-attention Networks Localize When QK-eigenspectrum Concentrates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Bao%2C+H">Han Bao</a>, 
<a href="/search/stat?searchtype=author&query=Hataya%2C+R">Ryuichiro Hataya</a>, 
<a href="/search/stat?searchtype=author&query=Karakida%2C+R">Ryo Karakida</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The self-attention mechanism prevails in modern machine learning. It has an
interesting functionality of adaptively selecting tokens from an input sequence
by modulating the degree of attention localization, which many researchers
speculate is the basis of the powerful model performance but complicates the
underlying mechanism of the learning dynamics. In recent years, mainly two
arguments have connected attention localization to the model performances. One
is the rank collapse, where the embedded tokens by a self-attention block
become very similar across different tokens, leading to a less expressive
network. The other is the entropy collapse, where the attention probability
approaches non-uniform and entails low entropy, making the learning dynamics
more likely to be trapped in plateaus. These two failure modes may apparently
contradict each other because the rank and entropy collapses are relevant to
uniform and non-uniform attention, respectively. To this end, we characterize
the notion of attention localization by the eigenspectrum of query-key
parameter matrices and reveal that a small eigenspectrum variance leads
attention to be localized. Interestingly, the small eigenspectrum variance
prevents both rank and entropy collapse, leading to better model expressivity
and trainability.
</p>
</div>
</dd>
<dt><a name="item935">[935]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02111" title="Abstract">arXiv:2402.02111</a> (cross-list from stat.ML) [<a href="/pdf/2402.02111" title="Download PDF">pdf</a>, <a href="/format/2402.02111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Look-ahead in Bayesian Optimization: Multilevel Monte Carlo  is All you Need
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Yang%2C+S">Shangda Yang</a>, 
<a href="/search/stat?searchtype=author&query=Zankin%2C+V">Vitaly Zankin</a>, 
<a href="/search/stat?searchtype=author&query=Balandat%2C+M">Maximilian Balandat</a>, 
<a href="/search/stat?searchtype=author&query=Scherer%2C+S">Stefan Scherer</a>, 
<a href="/search/stat?searchtype=author&query=Carlberg%2C+K">Kevin Carlberg</a>, 
<a href="/search/stat?searchtype=author&query=Walton%2C+N">Neil Walton</a>, 
<a href="/search/stat?searchtype=author&query=Law%2C+K+J+H">Kody J. H. Law</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC); Probability (math.PR); Computation (stat.CO); Methodology (stat.ME)

</div>
<p class="mathjax">We leverage multilevel Monte Carlo (MLMC) to improve the performance of
multi-step look-ahead Bayesian optimization (BO) methods that involve nested
expectations and maximizations. The complexity rate of naive Monte Carlo
degrades for nested operations, whereas MLMC is capable of achieving the
canonical Monte Carlo convergence rate for this type of problem, independently
of dimension and without any smoothness assumptions. Our theoretical study
focuses on the approximation improvements for one- and two-step look-ahead
acquisition functions, but, as we discuss, the approach is generalizable in
various ways, including beyond the context of BO. Findings are verified
numerically and the benefits of MLMC for BO are illustrated on several
benchmark examples. Code is available here
https://github.com/Shangda-Yang/MLMCBO.
</p>
</div>
</dd>
<dt><a name="item936">[936]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02162" title="Abstract">arXiv:2402.02162</a> (cross-list from stat.ML) [<a href="/pdf/2402.02162" title="Download PDF">pdf</a>, <a href="/format/2402.02162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Bayesian cluster validity index
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wiroonsri%2C+N">Nathakhun Wiroonsri</a>, 
<a href="/search/stat?searchtype=author&query=Preedasawakul%2C+O">Onthada Preedasawakul</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
<p class="mathjax">Selecting the number of clusters is one of the key processes when applying
clustering algorithms. To fulfill this task, various cluster validity indices
(CVIs) have been introduced. Most of the cluster validity indices are defined
to detect the optimal number of clusters hidden in a dataset. However, users
sometimes do not expect to get the optimal number of groups but a secondary one
which is more reasonable for their applications. This has motivated us to
introduce a Bayesian cluster validity index (BCVI) based on existing underlying
indices. This index is defined based on either Dirichlet or Generalized
Dirichlet priors which result in the same posterior distribution. Our BCVI is
then tested based on the Wiroonsri index (WI), and the Wiroonsri-Preedasawakul
index (WP) as underlying indices for hard and soft clustering, respectively. We
compare their outcomes with the original underlying indices, as well as a few
more existing CVIs including Davies and Bouldin (DB), Starczewski (STR), Xie
and Beni (XB), and KWON2 indices. Our proposed BCVI clearly benefits the use of
CVIs when experiences matter where users can specify their expected range of
the final number of clusters. This aspect is emphasized by our experiment
classified into three different cases. Finally, we present some applications to
real-world datasets including MRI brain tumor images. Our tools will be added
to a new version of the recently developed R package ``UniversalCVI''.
</p>
</div>
</dd>
<dt><a name="item937">[937]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02171" title="Abstract">arXiv:2402.02171</a> (cross-list from stat.ML) [<a href="/pdf/2402.02171" title="Download PDF">pdf</a>, <a href="/format/2402.02171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Off-Policy Evaluation of Slate Bandit Policies via Optimizing  Abstraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kiyohara%2C+H">Haruka Kiyohara</a>, 
<a href="/search/stat?searchtype=author&query=Nomura%2C+M">Masahiro Nomura</a>, 
<a href="/search/stat?searchtype=author&query=Saito%2C+Y">Yuta Saito</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WebConf2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We study off-policy evaluation (OPE) in the problem of slate contextual
bandits where a policy selects multi-dimensional actions known as slates. This
problem is widespread in recommender systems, search engines, marketing, to
medical applications, however, the typical Inverse Propensity Scoring (IPS)
estimator suffers from substantial variance due to large action spaces, making
effective OPE a significant challenge. The PseudoInverse (PI) estimator has
been introduced to mitigate the variance issue by assuming linearity in the
reward function, but this can result in significant bias as this assumption is
hard-to-verify from observed data and is often substantially violated. To
address the limitations of previous estimators, we develop a novel estimator
for OPE of slate bandits, called Latent IPS (LIPS), which defines importance
weights in a low-dimensional slate abstraction space where we optimize slate
abstractions to minimize the bias and variance of LIPS in a data-driven way. By
doing so, LIPS can substantially reduce the variance of IPS without imposing
restrictive assumptions on the reward function structure like linearity.
Through empirical evaluation, we demonstrate that LIPS substantially
outperforms existing estimators, particularly in scenarios with non-linear
rewards and large slate spaces.
</p>
</div>
</dd>
<dt><a name="item938">[938]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02173" title="Abstract">arXiv:2402.02173</a> (cross-list from physics.soc-ph) [<a href="/pdf/2402.02173" title="Download PDF">pdf</a>, <a href="/format/2402.02173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding the News Media Diet of Disinformation Spreaders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Bertani%2C+A">Anna Bertani</a>, 
<a href="/search/physics?searchtype=author&query=Mazzeo%2C+V">Valeria Mazzeo</a>, 
<a href="/search/physics?searchtype=author&query=Gallotti%2C+R">Riccardo Gallotti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">In the digital era, information consumption is predominantly channeled
through online news media disseminated on social media platforms. Understanding
the complex dynamics of the news media environment and users habits within the
digital ecosystem is a challenging task that requires at the same time large
bases of data and accurate methodological approaches. This study contributes to
this expanding research landscape by employing network science methodologies
and entropic measures to analyze the behavioural patterns of social media users
sharing news pieces and dig into the diverse news consumption habits within
different online social media user groups. Our analyses reveal that users are
more inclined to share news classified as fake when they have previously posted
conspiracy or junk science content, and vice versa, creating a series of
misinformation hot streaks. To better understand these dynamics, we used three
different measures of entropy to gain insights into the news media habits of
each user, finding that the patterns of news consumption significantly differ
among users when focusing on disinformation spreaders, as opposed to accounts
sharing reliable or low-risk content. Thanks to these entropic measures, we
quantify the variety and the regularity of the news media diet, finding that
those disseminating unreliable content exhibit a more varied and at the same
time a more regular choice of web domains. This quantitative insight into the
nuances of news consumption behaviours exhibited by disinformation spreaders
holds the potential to significantly inform the strategic formulation of more
robust and adaptive social media moderation policies.
</p>
</div>
</dd>
<dt><a name="item939">[939]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02190" title="Abstract">arXiv:2402.02190</a> (cross-list from stat.ML) [<a href="/pdf/2402.02190" title="Download PDF">pdf</a>, <a href="/format/2402.02190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continuous Tensor Relaxation for Finding Diverse Solutions in  Combinatorial Optimization Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ichikawa%2C+Y">Yuma Ichikawa</a>, 
<a href="/search/stat?searchtype=author&query=Iwashita%2C+H">Hiroaki Iwashita</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO); Methodology (stat.ME)

</div>
<p class="mathjax">Finding the best solution is the most common objective in combinatorial
optimization (CO) problems. However, a single solution may not be suitable in
practical scenarios, as the objective functions and constraints are only
approximations of original real-world situations. To tackle this, finding (i)
"heterogeneous solutions", diverse solutions with distinct characteristics, and
(ii) "penalty-diversified solutions", variations in constraint severity, are
natural directions. This strategy provides the flexibility to select a suitable
solution during post-processing. However, discovering these diverse solutions
is more challenging than identifying a single solution. To overcome this
challenge, this study introduces Continual Tensor Relaxation Annealing (CTRA)
for unsupervised-learning-based CO solvers. CTRA addresses various problems
simultaneously by extending the continual relaxation approach, which transforms
discrete decision variables into continual tensors. This method finds
heterogeneous and penalty-diversified solutions through mutual interactions,
where the choice of one solution affects the other choices. Numerical
experiments show that CTRA enables UL-based solvers to find heterogeneous and
penalty-diversified solutions much faster than existing UL-based solvers.
Moreover, these experiments reveal that CTRA enhances the exploration ability.
</p>
</div>
</dd>
<dt><a name="item940">[940]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02193" title="Abstract">arXiv:2402.02193</a> (cross-list from math.OC) [<a href="/pdf/2402.02193" title="Download PDF">pdf</a>, <a href="/ps/2402.02193" title="Download PostScript">ps</a>, <a href="/format/2402.02193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Extended ADMM for Three-block Nonconvex Nonseparable Problem with  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liu%2C+Z">Zekun Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 1 figure, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">We consider a three-block alternating direction method of multipliers (ADMM)
for solving the nonconvex nonseparable optimization problem with linear
constraint. Inspired by [1], the third variable is updated twice in each
iteration to ensure the global convergence. Based on the powerful
Kurdyka-Lojasiewicz property, we prove that the sequence generated by the ADMM
converges globally to the critical point of the augmented Lagrangian function.
We also point out the convergence of proposed ADMM with swapping the update
order of the first and second variables, and with adding a proximal term to the
first variable for more general nonseparable problems, respectively. Moreover,
we make numerical experiments on three nonconvex problems: multiple measurement
vector (MMV), robust PCA (RPCA) and nonnegative matrix completion (NMC). The
results show the efficiency and outperformance of proposed ADMM.
</p>
</div>
</dd>
<dt><a name="item941">[941]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02196" title="Abstract">arXiv:2402.02196</a> (cross-list from stat.ME) [<a href="/pdf/2402.02196" title="Download PDF">pdf</a>, <a href="/format/2402.02196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sample-Efficient Clustering and Conquer Procedures for Parallel  Large-Scale Ranking and Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhang%2C+Z">Zishi Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Peng%2C+Y">Yijie Peng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose novel "clustering and conquer" procedures for the parallel
large-scale ranking and selection (R&amp;S) problem, which leverage correlation
information for clustering to break the bottleneck of sample efficiency. In
parallel computing environments, correlation-based clustering can achieve an
$\mathcal{O}(p)$ sample complexity reduction rate, which is the optimal
reduction rate theoretically attainable. Our proposed framework is versatile,
allowing for seamless integration of various prevalent R&amp;S methods under both
fixed-budget and fixed-precision paradigms. It can achieve improvements without
the necessity of highly accurate correlation estimation and precise clustering.
In large-scale AI applications such as neural architecture search, a
screening-free version of our procedure surprisingly surpasses fully-sequential
benchmarks in terms of sample efficiency. This suggests that leveraging
valuable structural information, such as correlation, is a viable path to
bypassing the traditional need for screening via pairwise comparison--a step
previously deemed essential for high sample efficiency but problematic for
parallelization. Additionally, we propose a parallel few-shot clustering
algorithm tailored for large-scale problems.
</p>
</div>
</dd>
<dt><a name="item942">[942]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02198" title="Abstract">arXiv:2402.02198</a> (cross-list from cond-mat.mtrl-sci) [<a href="/pdf/2402.02198" title="Download PDF">pdf</a>, <a href="/ps/2402.02198" title="Download PostScript">ps</a>, <a href="/format/2402.02198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Co-orchestration for Exploring Structure-Property  Relationships in Combinatorial Libraries via Multi-Task Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Slautin%2C+B+N">Boris N. Slautin</a>, 
<a href="/search/cond-mat?searchtype=author&query=Pratiush%2C+U">Utkarsh Pratiush</a>, 
<a href="/search/cond-mat?searchtype=author&query=Ivanov%2C+I+N">Ilia N. Ivanov</a>, 
<a href="/search/cond-mat?searchtype=author&query=Liu%2C+Y">Yongtao Liu</a>, 
<a href="/search/cond-mat?searchtype=author&query=Pant%2C+R">Rohit Pant</a>, 
<a href="/search/cond-mat?searchtype=author&query=Zhang%2C+X">Xiaohang Zhang</a>, 
<a href="/search/cond-mat?searchtype=author&query=Takeuchi%2C+I">Ichiro Takeuchi</a>, 
<a href="/search/cond-mat?searchtype=author&query=Ziatdinov%2C+M+A">Maxim A. Ziatdinov</a>, 
<a href="/search/cond-mat?searchtype=author&query=Kalinin%2C+S+V">Sergei V. Kalinin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The rapid growth of automated and autonomous instrumentations brings forth an
opportunity for the co-orchestration of multimodal tools, equipped with
multiple sequential detection methods, or several characterization tools to
explore identical samples. This can be exemplified by the combinatorial
libraries that can be explored in multiple locations by multiple tools
simultaneously, or downstream characterization in automated synthesis systems.
In the co-orchestration approaches, information gained in one modality should
accelerate the discovery of other modalities. Correspondingly, the
orchestrating agent should select the measurement modality based on the
anticipated knowledge gain and measurement cost. Here, we propose and implement
a co-orchestration approach for conducting measurements with complex
observables such as spectra or images. The method relies on combining
dimensionality reduction by variational autoencoders with representation
learning for control over the latent space structure, and integrated into
iterative workflow via multi-task Gaussian Processes (GP). This approach
further allows for the native incorporation of the system's physics via a
probabilistic model as a mean function of the GP. We illustrated this method
for different modalities of piezoresponse force microscopy and micro-Raman on
combinatorial $Sm-BiFeO_3$ library. However, the proposed framework is general
and can be extended to multiple measurement modalities and arbitrary
dimensionality of measured signals. The analysis code that supports the funding
is publicly available at https://github.com/Slautin/2024_Co-orchestration.
</p>
</div>
</dd>
<dt><a name="item943">[943]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02279" title="Abstract">arXiv:2402.02279</a> (cross-list from quant-ph) [<a href="/pdf/2402.02279" title="Download PDF">pdf</a>, <a href="/format/2402.02279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bosehedral: Compiler Optimization for Bosonic Quantum Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Zhou%2C+J">Junyu Zhou</a>, 
<a href="/search/quant-ph?searchtype=author&query=Liu%2C+Y">Yuhao Liu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Shi%2C+Y">Yunong Shi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Javadi-Abhari%2C+A">Ali Javadi-Abhari</a>, 
<a href="/search/quant-ph?searchtype=author&query=Li%2C+G">Gushu Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Bosonic quantum computing, based on the infinite-dimensional qumodes, has
shown promise for various practical applications that are classically hard.
However, the lack of compiler optimizations has hindered its full potential.
This paper introduces Bosehedral, an efficient compiler optimization framework
for (Gaussian) Boson sampling on Bosonic quantum hardware. Bosehedral overcomes
the challenge of handling infinite-dimensional qumode gate matrices by
performing all its program analysis and optimizations at a higher algorithmic
level, using a compact unitary matrix representation. It optimizes qumode gate
decomposition and logical-to-physical qumode mapping, and introduces a tunable
probabilistic gate dropout method. Overall, Bosehedral significantly improves
the performance by accurately approximating the original program with much
fewer gates. Our evaluation shows that Bosehedral can largely reduce the
program size but still maintain a high approximation fidelity, which can
translate to significant end-to-end application performance improvement.
</p>
</div>
</dd>
<dt><a name="item944">[944]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02290" title="Abstract">arXiv:2402.02290</a> (cross-list from stat.CO) [<a href="/pdf/2402.02290" title="Download PDF">pdf</a>, <a href="/format/2402.02290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Goodness-of-Fit and Clustering of Spherical Data: the QuadratiK package  in R and Python
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Saraceno%2C+G">Giovanni Saraceno</a>, 
<a href="/search/stat?searchtype=author&query=Markatou%2C+M">Marianthi Markatou</a>, 
<a href="/search/stat?searchtype=author&query=Mukhopadhyay%2C+R">Raktim Mukhopadhyay</a>, 
<a href="/search/stat?searchtype=author&query=Golzy%2C+M">Mojgan Golzy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 54 pages, 26 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Machine Learning (cs.LG); Mathematical Software (cs.MS); Applications (stat.AP); Machine Learning (stat.ML)

</div>
<p class="mathjax">We introduce the QuadratiK package that incorporates innovative data analysis
methodologies. The presented software, implemented in both R and Python, offers
a comprehensive set of goodness-of-fit tests and clustering techniques using
kernel-based quadratic distances, thereby bridging the gap between the
statistical and machine learning literatures. Our software implements one, two
and k-sample tests for goodness of fit, providing an efficient and
mathematically sound way to assess the fit of probability distributions.
Expanded capabilities of our software include supporting tests for uniformity
on the $d$-dimensional Sphere based on Poisson kernel densities, and algorithms
for generating random samples from Poisson kernel densities. Particularly
noteworthy is the incorporation of a unique clustering algorithm specifically
tailored for spherical data that leverages a mixture of Poisson-kernel-based
densities on the sphere. Alongside this, our software includes additional
graphical functions, aiding the users in validating, as well as visualizing and
representing clustering results. This enhances interpretability and usability
of the analysis. In summary, our R and Python packages serve as a powerful
suite of tools, offering researchers and practitioners the means to delve
deeper into their data, draw robust inference, and conduct potentially
impactful analyses and inference across a wide array of disciplines.
</p>
</div>
</dd>
<dt><a name="item945">[945]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02297" title="Abstract">arXiv:2402.02297</a> (cross-list from math.OC) [<a href="/pdf/2402.02297" title="Download PDF">pdf</a>, <a href="/format/2402.02297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising Diffusion-Based Control of Nonlinear Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Elamvazhuthi%2C+K">Karthik Elamvazhuthi</a>, 
<a href="/search/math?searchtype=author&query=Gadginmath%2C+D">Darshan Gadginmath</a>, 
<a href="/search/math?searchtype=author&query=Pasqualetti%2C+F">Fabio Pasqualetti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Robotics (cs.RO); Systems and Control (eess.SY)

</div>
<p class="mathjax">We propose a novel approach based on Denoising Diffusion Probabilistic Models
(DDPMs) to control nonlinear dynamical systems. DDPMs are the state-of-art of
generative models that have achieved success in a wide variety of sampling
tasks. In our framework, we pose the feedback control problem as a generative
task of drawing samples from a target set under control system constraints. The
forward process of DDPMs constructs trajectories originating from a target set
by adding noise. We learn to control a dynamical system in reverse such that
the terminal state belongs to the target set. For control-affine systems
without drift, we prove that the control system can exactly track the
trajectory of the forward process in reverse, whenever the the Lie bracket
based condition for controllability holds. We numerically study our approach on
various nonlinear systems and verify our theoretical results. We also conduct
numerical experiments for cases beyond our theoretical results on a
physics-engine.
</p>
</div>
</dd>
<dt><a name="item946">[946]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02302" title="Abstract">arXiv:2402.02302</a> (cross-list from eess.AS) [<a href="/pdf/2402.02302" title="Download PDF">pdf</a>, <a href="/format/2402.02302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting positive transfer for improved low-resource speech  recognition using acoustic pseudo-tokens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=San%2C+N">Nay San</a>, 
<a href="/search/eess?searchtype=author&query=Paraskevopoulos%2C+G">Georgios Paraskevopoulos</a>, 
<a href="/search/eess?searchtype=author&query=Arora%2C+A">Aryaman Arora</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+X">Xiluo He</a>, 
<a href="/search/eess?searchtype=author&query=Kaur%2C+P">Prabhjot Kaur</a>, 
<a href="/search/eess?searchtype=author&query=Adams%2C+O">Oliver Adams</a>, 
<a href="/search/eess?searchtype=author&query=Jurafsky%2C+D">Dan Jurafsky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for SIGTYP2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">While massively multilingual speech models like wav2vec 2.0 XLSR-128 can be
directly fine-tuned for automatic speech recognition (ASR), downstream
performance can still be relatively poor on languages that are
under-represented in the pre-training data. Continued pre-training on 70-200
hours of untranscribed speech in these languages can help -- but what about
languages without that much recorded data? For such cases, we show that
supplementing the target language with data from a similar, higher-resource
'donor' language can help. For example, continued pre-training on only 10 hours
of low-resource Punjabi supplemented with 60 hours of donor Hindi is almost as
good as continued pretraining on 70 hours of Punjabi. By contrast, sourcing
data from less similar donors like Bengali does not improve ASR performance. To
inform donor language selection, we propose a novel similarity metric based on
the sequence distribution of induced acoustic units: the Acoustic Token
Distribution Similarity (ATDS). Across a set of typologically different target
languages (Punjabi, Galician, Iban, Setswana), we show that the ATDS between
the target language and its candidate donors precisely predicts target language
ASR performance.
</p>
</div>
</dd>
<dt><a name="item947">[947]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02303" title="Abstract">arXiv:2402.02303</a> (cross-list from math.ST) [<a href="/pdf/2402.02303" title="Download PDF">pdf</a>, <a href="/format/2402.02303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bootstrapping Fisher Market Equilibrium and First-Price Pacing  Equilibrium
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Luofeng%2C+L">Liao Luofeng</a>, 
<a href="/search/math?searchtype=author&query=Christian%2C+K">Kroer Christian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Computer Science and Game Theory (cs.GT); Econometrics (econ.EM); Applications (stat.AP)

</div>
<p class="mathjax">The linear Fisher market (LFM) is a basic equilibrium model from economics,
which also has applications in fair and efficient resource allocation.
First-price pacing equilibrium (FPPE) is a model capturing budget-management
mechanisms in first-price auctions. In certain practical settings such as
advertising auctions, there is an interest in performing statistical inference
over these models. A popular methodology for general statistical inference is
the bootstrap procedure. Yet, for LFM and FPPE there is no existing theory for
the valid application of bootstrap procedures. In this paper, we introduce and
devise several statistically valid bootstrap inference procedures for LFM and
FPPE. The most challenging part is to bootstrap general FPPE, which reduces to
bootstrapping constrained M-estimators, a largely unexplored problem. We devise
a bootstrap procedure for FPPE under mild degeneracy conditions by using the
powerful tool of epi-convergence theory. Experiments with synthetic and
semi-real data verify our theory.
</p>
</div>
</dd>
<dt><a name="item948">[948]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02304" title="Abstract">arXiv:2402.02304</a> (cross-list from math.AP) [<a href="/pdf/2402.02304" title="Download PDF">pdf</a>, <a href="/format/2402.02304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Numerical Wave Propagation Enhanced by an End-to-End Deep  Learning Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kaiser%2C+L">Luis Kaiser</a>, 
<a href="/search/math?searchtype=author&query=Tsai%2C+R">Richard Tsai</a>, 
<a href="/search/math?searchtype=author&query=Klingenberg%2C+C">Christian Klingenberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In a variety of scientific and engineering domains, ranging from seismic
modeling to medical imaging, the need for high-fidelity and efficient solutions
for high-frequency wave propagation holds great significance. Recent advances
in wave modeling use sufficiently accurate fine solver outputs to train neural
networks that enhance the accuracy of a fast but inaccurate coarse solver. A
stable and fast solver further allows the use of Parareal, a parallel-in-time
algorithm to retrieve and correct high-frequency wave components. In this paper
we build upon the work of Nguyen and Tsai (2023) and present a novel unified
system that integrates a numerical solver with deep learning components into an
end-to-end framework. In the proposed setting, we investigate refinements to
the neural network architecture, data generation algorithm and Parareal scheme.
Our results show that the cohesive structure significantly improves performance
without sacrificing speed, and demonstrate the importance of temporal dynamics,
as well as Parareal iterations, for accurate wave propagation.
</p>
</div>
</dd>
<dt><a name="item949">[949]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02356" title="Abstract">arXiv:2402.02356</a> (cross-list from math.OC) [<a href="/pdf/2402.02356" title="Download PDF">pdf</a>, <a href="/format/2402.02356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized Sum-of-Nonconvex Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liu%2C+Z">Zhuanghua Liu</a>, 
<a href="/search/math?searchtype=author&query=Low%2C+B+K+H">Bryan Kian Hsiang Low</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We consider the optimization problem of minimizing the sum-of-nonconvex
function, i.e., a convex function that is the average of nonconvex components.
The existing stochastic algorithms for such a problem only focus on a single
machine and the centralized scenario. In this paper, we study the
sum-of-nonconvex optimization in the decentralized setting. We present a new
theoretical analysis of the PMGT-SVRG algorithm for this problem and prove the
linear convergence of their approach. However, the convergence rate of the
PMGT-SVRG algorithm has a linear dependency on the condition number, which is
undesirable for the ill-conditioned problem. To remedy this issue, we propose
an accelerated stochastic decentralized first-order algorithm by incorporating
the techniques of acceleration, gradient tracking, and multi-consensus mixing
into the SVRG algorithm. The convergence rate of the proposed method has a
square-root dependency on the condition number. The numerical experiments
validate the theoretical guarantee of our proposed algorithms on both synthetic
and real-world datasets.
</p>
</div>
</dd>
<dt><a name="item950">[950]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02359" title="Abstract">arXiv:2402.02359</a> (cross-list from math.OC) [<a href="/pdf/2402.02359" title="Download PDF">pdf</a>, <a href="/format/2402.02359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incremental Quasi-Newton Methods with Faster Superlinear Convergence  Rates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liu%2C+Z">Zhuanghua Liu</a>, 
<a href="/search/math?searchtype=author&query=Luo%2C+L">Luo Luo</a>, 
<a href="/search/math?searchtype=author&query=Low%2C+B+K+H">Bryan Kian Hsiang Low</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We consider the finite-sum optimization problem, where each component
function is strongly convex and has Lipschitz continuous gradient and Hessian.
The recently proposed incremental quasi-Newton method is based on BFGS update
and achieves a local superlinear convergence rate that is dependent on the
condition number of the problem. This paper proposes a more efficient
quasi-Newton method by incorporating the symmetric rank-1 update into the
incremental framework, which results in the condition-number-free local
superlinear convergence rate. Furthermore, we can boost our method by applying
the block update on the Hessian approximation, which leads to an even faster
local convergence rate. The numerical experiments show the proposed methods
significantly outperform the baseline methods.
</p>
</div>
</dd>
<dt><a name="item951">[951]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02384" title="Abstract">arXiv:2402.02384</a> (cross-list from eess.SP) [<a href="/pdf/2402.02384" title="Download PDF">pdf</a>, <a href="/ps/2402.02384" title="Download PostScript">ps</a>, <a href="/format/2402.02384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Acoustic Local Positioning With Encoded Emission Beacons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Urena%2C+J">Jesus Urena</a>, 
<a href="/search/eess?searchtype=author&query=Hernandez%2C+A">Alvaro Hernandez</a>, 
<a href="/search/eess?searchtype=author&query=Garcia%2C+J+J">Juan Jesus Garcia</a>, 
<a href="/search/eess?searchtype=author&query=Villadangos%2C+J+M">Jose Manuel Villadangos</a>, 
<a href="/search/eess?searchtype=author&query=del+Carmen+Perez%2C+M">Maria del Carmen Perez</a>, 
<a href="/search/eess?searchtype=author&query=Gualda%2C+D">David Gualda</a>, 
<a href="/search/eess?searchtype=author&query=Alvarez%2C+F+J">Fernando J. Alvarez</a>, 
<a href="/search/eess?searchtype=author&query=Aguilera%2C+T">Teodoro Aguilera</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the IEEE, vol. 106, no. 6, pp. 1042-1062, Jun. 2018
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">Acoustic local positioning systems (ALPSs) are an interesting alternative for
indoor positioning due to certain advantages over other approaches, including
their relatively high accuracy, low cost, and room-level signal propagation.
Centimeter-level or fine-grained indoor positioning can be an asset for robot
navigation, guiding a person to, for instance, a particular piece in a museum
or to a specific product in a shop, targeted advertising, or augmented reality.
In airborne system applications, acoustic positioning can be based on using
opportunistic signals or sounds produced by the person or object to be located
(e.g., noise from appliances or the speech from a speaker) or from encoded
emission beacons (or anchors) specifically designed for this purpose. This work
presents a review of the different challenges that designers of systems based
on encoded emission beacons must address in order to achieve suitable
performance. At low-level processing, the waveform design (coding and
modulation) and the processing of the received signal are key factors to
address such drawbacks as multipath propagation, multiple-access interference,
nearfar effect, or Doppler shifting. With regards to high-level system design,
the issues to be addressed are related to the distribution of beacons, ease of
deployment, and calibration and positioning algorithms, including the possible
fusion of information. Apart from theoretical discussions, this work also
includes the description of an ALPS that was implemented, installed in a large
area and tested for mobile robot navigation. In addition to practical interest
for real applications, airborne ALPSs can also be used as an excellent platform
to test complex algorithms, which can be subsequently adapted for other
positioning systems, such as underwater acoustic systems or ultrawideband
radiofrequency (UWB RF) systems.
</p>
</div>
</dd>
<dt><a name="item952">[952]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02397" title="Abstract">arXiv:2402.02397</a> (cross-list from physics.optics) [<a href="/pdf/2402.02397" title="Download PDF">pdf</a>, <a href="/ps/2402.02397" title="Download PostScript">ps</a>, <a href="/format/2402.02397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiplexed all-optical permutation operations using a reconfigurable  diffractive optical network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Ma%2C+G">Guangdong Ma</a>, 
<a href="/search/physics?searchtype=author&query=Yang%2C+X">Xilin Yang</a>, 
<a href="/search/physics?searchtype=author&query=Bai%2C+B">Bijie Bai</a>, 
<a href="/search/physics?searchtype=author&query=Li%2C+J">Jingxi Li</a>, 
<a href="/search/physics?searchtype=author&query=Li%2C+Y">Yuhang Li</a>, 
<a href="/search/physics?searchtype=author&query=Gan%2C+T">Tianyi Gan</a>, 
<a href="/search/physics?searchtype=author&query=Shen%2C+C">Che-Yung Shen</a>, 
<a href="/search/physics?searchtype=author&query=Zhang%2C+Y">Yijie Zhang</a>, 
<a href="/search/physics?searchtype=author&query=Li%2C+Y">Yuzhu Li</a>, 
<a href="/search/physics?searchtype=author&query=Jarrahi%2C+M">Mona Jarrahi</a>, 
<a href="/search/physics?searchtype=author&query=Ozcan%2C+A">Aydogan Ozcan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 Pages, 10 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optics (physics.optics)</span>; Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Large-scale and high-dimensional permutation operations are important for
various applications in e.g., telecommunications and encryption. Here, we
demonstrate the use of all-optical diffractive computing to execute a set of
high-dimensional permutation operations between an input and output
field-of-view through layer rotations in a diffractive optical network. In this
reconfigurable multiplexed material designed by deep learning, every
diffractive layer has four orientations: 0, 90, 180, and 270 degrees. Each
unique combination of these rotatable layers represents a distinct rotation
state of the diffractive design tailored for a specific permutation operation.
Therefore, a K-layer rotatable diffractive material is capable of all-optically
performing up to 4^K independent permutation operations. The original input
information can be decrypted by applying the specific inverse permutation
matrix to output patterns, while applying other inverse operations will lead to
loss of information. We demonstrated the feasibility of this reconfigurable
multiplexed diffractive design by approximating 256 randomly selected
permutation matrices using K=4 rotatable diffractive layers. We also
experimentally validated this reconfigurable diffractive network using
terahertz radiation and 3D-printed diffractive layers, providing a decent match
to our numerical results. The presented rotation-multiplexed diffractive
processor design is particularly useful due to its mechanical
reconfigurability, offering multifunctional representation through a single
fabrication process.
</p>
</div>
</dd>
<dt><a name="item953">[953]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02459" title="Abstract">arXiv:2402.02459</a> (cross-list from stat.ML) [<a href="/pdf/2402.02459" title="Download PDF">pdf</a>, <a href="/format/2402.02459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Minimum Trace Factor Analysis -- An Old Song Sung to a New Tune
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+C">C. Li</a>, 
<a href="/search/stat?searchtype=author&query=Shkolnik%2C+A">A. Shkolnik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">Dimensionality reduction methods, such as principal component analysis (PCA)
and factor analysis, are central to many problems in data science. There are,
however, serious and well-understood challenges to finding robust low
dimensional approximations for data with significant heteroskedastic noise.
This paper introduces a relaxed version of Minimum Trace Factor Analysis
(MTFA), a convex optimization method with roots dating back to the work of
Ledermann in 1940. This relaxation is particularly effective at not overfitting
to heteroskedastic perturbations and addresses the commonly cited Heywood cases
in factor analysis and the recently identified "curse of ill-conditioning" for
existing spectral methods. We provide theoretical guarantees on the accuracy of
the resulting low rank subspace and the convergence rate of the proposed
algorithm to compute that matrix. We develop a number of interesting
connections to existing methods, including HeteroPCA, Lasso, and Soft-Impute,
to fill an important gap in the already large literature on low rank matrix
estimation. Numerical experiments benchmark our results against several recent
proposals for dealing with heteroskedastic noise.
</p>
</div>
</dd>
<dt><a name="item954">[954]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02487" title="Abstract">arXiv:2402.02487</a> (cross-list from physics.soc-ph) [<a href="/pdf/2402.02487" title="Download PDF">pdf</a>, <a href="/format/2402.02487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interplay between tie strength and neighbourhood topology in complex  networks: Granovetter&#x27;s theory and beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Mrowinski%2C+M+J">Maciej J Mrowinski</a>, 
<a href="/search/physics?searchtype=author&query=Orzechowski%2C+K+P">Kamil P. Orzechowski</a>, 
<a href="/search/physics?searchtype=author&query=Fronczak%2C+A">Agata Fronczak</a>, 
<a href="/search/physics?searchtype=author&query=Fronczak%2C+P">Piotr Fronczak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Computers and Society (cs.CY)

</div>
<p class="mathjax">Granovetter's weak ties theory is a very important sociological theory
according to which a correlation between edge weight and the network's topology
should exist. More specifically, the neighbourhood overlap of two nodes
connected by an edge should be positively correlated with edge weight (tie
strength). However, some real social networks exhibit a negative correlation -
the most prominent example is the scientific collaboration network, for which
overlap decreases with edge weight. It has been demonstrated that the
aforementioned inconsistency with Granovetter's theory can be alleviated in the
scientific collaboration network through the use of asymmetric measures. In
this paper, we explain that while asymmetric measures are often necessary to
describe complex networks and to confirm Granovetter's theory, their
interpretation is not simple, and there are pitfalls that one must be wary of.
The definitions of asymmetric weights and overlaps introduce structural
correlations that must be filtered out. We show that correlation profiles can
be used to overcome this problem. Using this technique, not only do we confirm
Granovetter's theory in various real and artificial social networks, but we
also show that Granovetter-like weight-topology correlations are present in
other complex networks (e.g. metabolic and neural networks). Our results
suggest that Granovetter's theory is a sociological manifestation of more
general principles governing various types of complex networks.
</p>
</div>
</dd>
<dt><a name="item955">[955]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02498" title="Abstract">arXiv:2402.02498</a> (cross-list from eess.IV) [<a href="/pdf/2402.02498" title="Download PDF">pdf</a>, <a href="/format/2402.02498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fully Differentiable Correlation-driven 2D/3D Registration for X-ray to  CT Image Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+M">Minheng Chen</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Z">Zhirun Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Gu%2C+S">Shuheng Gu</a>, 
<a href="/search/eess?searchtype=author&query=Ge%2C+Z">Zhangyang Ge</a>, 
<a href="/search/eess?searchtype=author&query=Kong%2C+Y">Youyong Kong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Image-based rigid 2D/3D registration is a critical technique for fluoroscopic
guided surgical interventions. In recent years, some learning-based fully
differentiable methods have produced beneficial outcomes while the process of
feature extraction and gradient flow transmission still lack controllability
and interpretability. To alleviate these problems, in this work, we propose a
novel fully differentiable correlation-driven network using a dual-branch
CNN-transformer encoder which enables the network to extract and separate
low-frequency global features from high-frequency local features. A
correlation-driven loss is further proposed for low-frequency feature and
high-frequency feature decomposition based on embedded information. Besides, a
training strategy that learns to approximate a convex-shape similarity function
is applied in our work. We test our approach on a in-house datasetand show that
it outperforms both existing fully differentiable learning-based registration
approaches and the conventional optimization-based baseline.
</p>
</div>
</dd>
<dt><a name="item956">[956]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02520" title="Abstract">arXiv:2402.02520</a> (cross-list from q-bio.NC) [<a href="/pdf/2402.02520" title="Download PDF">pdf</a>, <a href="/format/2402.02520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A minimal model of cognition based on oscillatory and reinforcement  processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Gyllingberg%2C+L">Linn&#xe9;a Gyllingberg</a>, 
<a href="/search/q-bio?searchtype=author&query=Tian%2C+Y">Yu Tian</a>, 
<a href="/search/q-bio?searchtype=author&query=Sumpter%2C+D+J+T">David J.T. Sumpter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Social and Information Networks (cs.SI); Dynamical Systems (math.DS); Adaptation and Self-Organizing Systems (nlin.AO); Biological Physics (physics.bio-ph)

</div>
<p class="mathjax">Building mathematical models of brains is difficult because of the sheer
complexity of the problem. One potential approach is to start by identifying
models of basal cognition, which give an abstract representation of a range
organisms without central nervous systems, including fungi, slime moulds and
bacteria. We propose one such model, demonstrating how a combination of
oscillatory and current-based reinforcement processes can be used to couple
resources in an efficient manner. We first show that our model connects
resources in an efficient manner when the environment is constant. We then show
that in an oscillatory environment our model builds efficient solutions,
provided the environmental oscillations are sufficiently out of phase. We show
that amplitude differences can promote efficient solutions and that the system
is robust to frequency differences. We identify connections between our model
and basal cognition in biological systems and slime moulds, in particular,
showing how oscillatory and problem-solving properties of these systems are
captured by our model.
</p>
</div>
</dd>
<dt><a name="item957">[957]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02552" title="Abstract">arXiv:2402.02552</a> (cross-list from math.OC) [<a href="/pdf/2402.02552" title="Download PDF">pdf</a>, <a href="/format/2402.02552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neur2BiLO: Neural Bilevel Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dumouchelle%2C+J">Justin Dumouchelle</a>, 
<a href="/search/math?searchtype=author&query=Julien%2C+E">Esther Julien</a>, 
<a href="/search/math?searchtype=author&query=Kurtz%2C+J">Jannis Kurtz</a>, 
<a href="/search/math?searchtype=author&query=Khalil%2C+E+B">Elias B. Khalil</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Bilevel optimization deals with nested problems in which a leader takes the
first decision to minimize their objective function while accounting for a
follower's best-response reaction. Constrained bilevel problems with integer
variables are particularly notorious for their hardness. While exact solvers
have been proposed for mixed-integer linear bilevel optimization, they tend to
scale poorly with problem size and are hard to generalize to the non-linear
case. On the other hand, problem-specific algorithms (exact and heuristic) are
limited in scope. Under a data-driven setting in which similar instances of a
bilevel problem are solved routinely, our proposed framework, Neur2BiLO, embeds
a neural network approximation of the leader's or follower's value function,
trained via supervised regression, into an easy-to-solve mixed-integer program.
Neur2BiLO serves as a heuristic that produces high-quality solutions extremely
fast for the bilevel knapsack interdiction problem, the "critical node game"
from network security, a donor-recipient healthcare problem, and discrete
network design from transportation planning. These problems are diverse in that
they have linear or non-linear objectives/constraints and integer or
mixed-integer variables, making Neur2BiLO unique in its versatility.
</p>
</div>
</dd>
<dt><a name="item958">[958]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02569" title="Abstract">arXiv:2402.02569</a> (cross-list from math.OC) [<a href="/pdf/2402.02569" title="Download PDF">pdf</a>, <a href="/format/2402.02569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Complexity of Finite-Sum Smooth Optimization under the  Polyak-&#x141;ojasiewicz Condition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bai%2C+Y">Yunyan Bai</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+Y">Yuxing Liu</a>, 
<a href="/search/math?searchtype=author&query=Luo%2C+L">Luo Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper considers the optimization problem of the form $\min_{{\bf
x}\in{\mathbb R}^d} f({\bf x})\triangleq \frac{1}{n}\sum_{i=1}^n f_i({\bf x})$,
where $f(\cdot)$ satisfies the Polyak--{\L}ojasiewicz (PL) condition with
parameter $\mu$ and $\{f_i(\cdot)\}_{i=1}^n$ is $L$-mean-squared smooth. We
show that any gradient method requires at least
$\Omega(n+\kappa\sqrt{n}\log(1/\epsilon))$ incremental first-order oracle (IFO)
calls to find an $\epsilon$-suboptimal solution, where $\kappa\triangleq L/\mu$
is the condition number of the problem. This result nearly matches upper bounds
of IFO complexity for best-known first-order methods. We also study the problem
of minimizing the PL function in the distributed setting such that the
individuals $f_1(\cdot),\dots,f_n(\cdot)$ are located on a connected network of
$n$ agents. We provide lower bounds of
$\Omega(\kappa/\sqrt{\gamma}\,\log(1/\epsilon))$,
$\Omega((\kappa+\tau\kappa/\sqrt{\gamma}\,)\log(1/\epsilon))$ and
$\Omega\big(n+\kappa\sqrt{n}\log(1/\epsilon)\big)$ for communication rounds,
time cost and local first-order oracle calls respectively, where
$\gamma\in(0,1]$ is the spectral gap of the mixing matrix associated with the
network and~$\tau&gt;0$ is the time cost of per communication round. Furthermore,
we propose a decentralized first-order method that nearly matches above lower
bounds in expectation.
</p>
</div>
</dd>
<dt><a name="item959">[959]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02578" title="Abstract">arXiv:2402.02578</a> (cross-list from astro-ph.CO) [<a href="/pdf/2402.02578" title="Download PDF">pdf</a>, <a href="/ps/2402.02578" title="Download PostScript">ps</a>, <a href="/format/2402.02578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impact of PSF misestimation and galaxy population bias on precision  shear measurement using a CNN
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Voigt%2C+L">Lisa Voigt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 10 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2024, MNRAS, 528, 3217
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cosmology and Nongalactic Astrophysics (astro-ph.CO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Weak gravitational lensing of distant galaxies provides a powerful probe of
dark energy. The aim of this study is to investigate the application of
convolutional neural networks (CNNs) to precision shear estimation. In
particular, using a shallow CNN, we explore the impact of point spread function
(PSF) misestimation and `galaxy population bias' (including `distribution bias'
and `morphology bias'), focusing on the accuracy requirements of next
generation surveys. We simulate a population of noisy disk and elliptical
galaxies and adopt a PSF that is representative of a Euclid-like survey. We
quantify the accuracy achieved by the CNN assuming a linear relationship
between the estimated and true shears and measure the multiplicative ($m$) and
additive ($c$) biases. We make use of an unconventional loss function to
mitigate the effects of noise bias and measure $m$ and $c$ when we use either:
(i) an incorrect galaxy ellipticity distribution or size-magnitude relation, or
the wrong ratio of morphological types, to describe the population of galaxies
(distribution bias); (ii) an incorrect galaxy light profile (morphology bias);
or (iii) a PSF with size or ellipticity offset from its true value (PSF
misestimation). We compare our results to the Euclid requirements on the
knowledge of the PSF model shape and size. Finally, we outline further work to
build on the promising potential of CNNs in precision shear estimation.
</p>
</div>
</dd>
<dt><a name="item960">[960]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02672" title="Abstract">arXiv:2402.02672</a> (cross-list from stat.ME) [<a href="/pdf/2402.02672" title="Download PDF">pdf</a>, <a href="/format/2402.02672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimation of conditional average treatment effects on distributed data:  A privacy-preserving approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kawamata%2C+Y">Yuji Kawamata</a>, 
<a href="/search/stat?searchtype=author&query=Motai%2C+R">Ryoki Motai</a>, 
<a href="/search/stat?searchtype=author&query=Okada%2C+Y">Yukihiko Okada</a>, 
<a href="/search/stat?searchtype=author&query=Imakura%2C+A">Akira Imakura</a>, 
<a href="/search/stat?searchtype=author&query=Sakurai%2C+T">Tetsuya Sakurai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Estimation of conditional average treatment effects (CATEs) is an important
topic in various fields such as medical and social sciences. CATEs can be
estimated with high accuracy if distributed data across multiple parties can be
centralized. However, it is difficult to aggregate such data if they contain
privacy information. To address this issue, we proposed data collaboration
double machine learning (DC-DML), a method that can estimate CATE models with
privacy preservation of distributed data, and evaluated the method through
numerical experiments. Our contributions are summarized in the following three
points. First, our method enables estimation and testing of semi-parametric
CATE models without iterative communication on distributed data.
Semi-parametric or non-parametric CATE models enable estimation and testing
that is more robust to model mis-specification than parametric models. However,
to our knowledge, no communication-efficient method has been proposed for
estimating and testing semi-parametric or non-parametric CATE models on
distributed data. Second, our method enables collaborative estimation between
different parties as well as multiple time points because the
dimensionality-reduced intermediate representations can be accumulated. Third,
our method performed as well or better than other methods in evaluation
experiments using synthetic, semi-synthetic and real-world datasets.
</p>
</div>
</dd>
<dt><a name="item961">[961]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02686" title="Abstract">arXiv:2402.02686</a> (cross-list from q-bio.NC) [<a href="/pdf/2402.02686" title="Download PDF">pdf</a>, <a href="/format/2402.02686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Region Markovian Gaussian Process: An Efficient Method to Discover  Directional Communications Across Multiple Brain Regions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Li%2C+W">Weihan Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+C">Chengrui Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+Y">Yule Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Wu%2C+A">Anqi Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Studying the complex interactions between different brain regions is crucial
in neuroscience. Various statistical methods have explored the latent
communication across multiple brain regions. Two main categories are the
Gaussian Process (GP) and Linear Dynamical System (LDS), each with unique
strengths. The GP-based approach effectively discovers latent variables such as
frequency bands and communication directions. Conversely, the LDS-based
approach is computationally efficient but lacks powerful expressiveness in
latent representation. In this study, we merge both methodologies by creating
an LDS mirroring a multi-output GP, termed Multi-Region Markovian Gaussian
Process (MRM-GP). Our work is the first to establish a connection between an
LDS and a multi-output GP that explicitly models frequencies and phase delays
within the latent space of neural recordings. Consequently, the model achieves
a linear inference cost over time points and provides an interpretable
low-dimensional representation, revealing communication directions across brain
regions and separating oscillatory communications into different frequency
bands.
</p>
</div>
</dd>
<dt><a name="item962">[962]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02694" title="Abstract">arXiv:2402.02694</a> (cross-list from eess.AS) [<a href="/pdf/2402.02694" title="Download PDF">pdf</a>, <a href="/format/2402.02694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Description on IEEE ICME 2024 Grand Challenge: Semi-supervised Acoustic  Scene Classification under Domain Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bai%2C+J">Jisheng Bai</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+M">Mou Wang</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+H">Haohe Liu</a>, 
<a href="/search/eess?searchtype=author&query=Yin%2C+H">Han Yin</a>, 
<a href="/search/eess?searchtype=author&query=Jia%2C+Y">Yafei Jia</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+S">Siwei Huang</a>, 
<a href="/search/eess?searchtype=author&query=Du%2C+Y">Yutong Du</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+D">Dongzhe Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Plumbley%2C+M+D">Mark D. Plumbley</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+D">Dongyuan Shi</a>, 
<a href="/search/eess?searchtype=author&query=Gan%2C+W">Woon-Seng Gan</a>, 
<a href="/search/eess?searchtype=author&query=Rahardja%2C+S">Susanto Rahardja</a>, 
<a href="/search/eess?searchtype=author&query=Xiang%2C+B">Bin Xiang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+J">Jianfeng Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Sound (cs.SD)

</div>
<p class="mathjax">Acoustic scene classification (ASC) is a crucial research problem in
computational auditory scene analysis, and it aims to recognize the unique
acoustic characteristics of an environment. One of the challenges of the ASC
task is domain shift caused by a distribution gap between training and testing
data. Since 2018, ASC challenges have focused on the generalization of ASC
models across different recording devices. Although this task in recent years
has achieved substantial progress in device generalization, the challenge of
domain shift between different regions, involving characteristics such as time,
space, culture, and language, remains insufficiently explored at present. In
addition, considering the abundance of unlabeled acoustic scene data in the
real world, it is important to study the possible ways to utilize these
unlabelled data. Therefore, we introduce the task Semi-supervised Acoustic
Scene Classification under Domain Shift in the ICME 2024 Grand Challenge. We
encourage participants to innovate with semi-supervised learning techniques,
aiming to develop more robust ASC models under domain shift.
</p>
</div>
</dd>
<dt><a name="item963">[963]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02721" title="Abstract">arXiv:2402.02721</a> (cross-list from quant-ph) [<a href="/pdf/2402.02721" title="Download PDF">pdf</a>, <a href="/format/2402.02721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Switches for Gottesman-Kitaev-Preskill Qubit-based All-Photonic  Quantum Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Azari%2C+M">Mohadeseh Azari</a>, 
<a href="/search/quant-ph?searchtype=author&query=Polakos%2C+P">Paul Polakos</a>, 
<a href="/search/quant-ph?searchtype=author&query=Seshadreesan%2C+K+P">Kaushik P. Seshadreesan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">The Gottesman-Kitaev-Preskill (GKP) code, being information theoretically
near optimal for quantum communication over Gaussian thermal-loss optical
channels, is likely to be the encoding of choice for advanced quantum networks
of the future. Quantum repeaters based on GKP-encoded light have been shown to
support high end-to-end entanglement rates across large distances despite
realistic finite squeezing in GKP code preparation and homodyne detection
inefficiencies. Here, we introduce a quantum switch for GKP-qubit-based quantum
networks, whose architecture involves multiplexed GKP-qubit-based entanglement
link generation with clients, and their all-photonic storage, together enabled
by GKP-qubit graph state resources. For bipartite entanglement distribution
between clients via entanglement swapping, the switch uses a multi-client
generalization of a recently introduced $\textit{entanglement-ranking-based
link matching}$ protocol heuristic. Since generating the GKP-qubit graph state
resource is hardware intensive, given a total resource budget and an arbitrary
layout of clients, we address the question of their optimal allocation towards
the different client-pair connections served by the switch such that the sum
throughput of the switch is maximized while also being fair in terms of the
individual entanglement rates. We illustrate our results for an exemplary data
center network, where the data center is a client of a switch and all of its
other clients aim to connect to the data center alone -- a scenario that also
captures the general case of a gateway router connecting a local area network
to a global network. Together with compatible quantum repeaters, our quantum
switch provides a way to realize quantum networks of arbitrary topology.
</p>
</div>
</dd>
<dt><a name="item964">[964]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02817" title="Abstract">arXiv:2402.02817</a> (cross-list from stat.ML) [<a href="/pdf/2402.02817" title="Download PDF">pdf</a>, <a href="/format/2402.02817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayes-Optimal Fair Classification with Linear Disparity Constraints via  Pre-, In-, and Post-processing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zeng%2C+X">Xianli Zeng</a>, 
<a href="/search/stat?searchtype=author&query=Cheng%2C+G">Guang Cheng</a>, 
<a href="/search/stat?searchtype=author&query=Dobriban%2C+E">Edgar Dobriban</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Machine learning algorithms may have disparate impacts on protected groups.
To address this, we develop methods for Bayes-optimal fair classification,
aiming to minimize classification error subject to given group fairness
constraints. We introduce the notion of \emph{linear disparity measures}, which
are linear functions of a probabilistic classifier; and \emph{bilinear
disparity measures}, which are also linear in the group-wise regression
functions. We show that several popular disparity measures -- the deviations
from demographic parity, equality of opportunity, and predictive equality --
are bilinear.
<br />We find the form of Bayes-optimal fair classifiers under a single linear
disparity measure, by uncovering a connection with the Neyman-Pearson lemma.
For bilinear disparity measures, Bayes-optimal fair classifiers become
group-wise thresholding rules. Our approach can also handle multiple fairness
constraints (such as equalized odds), and the common scenario when the
protected attribute cannot be used at the prediction phase.
<br />Leveraging our theoretical results, we design methods that learn fair
Bayes-optimal classifiers under bilinear disparity constraints. Our methods
cover three popular approaches to fairness-aware classification, via
pre-processing (Fair Up- and Down-Sampling), in-processing (Fair Cost-Sensitive
Classification) and post-processing (a Fair Plug-In Rule). Our methods control
disparity directly while achieving near-optimal fairness-accuracy tradeoffs. We
show empirically that our methods compare favorably to existing algorithms.
</p>
</div>
</dd>
<dt><a name="item965">[965]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02836" title="Abstract">arXiv:2402.02836</a> (cross-list from eess.IV) [<a href="/pdf/2402.02836" title="Download PDF">pdf</a>, <a href="/ps/2402.02836" title="Download PostScript">ps</a>, <a href="/format/2402.02836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perceptual Learned Image Compression via End-to-End JND-Based  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Pakdaman%2C+F">Farhad Pakdaman</a>, 
<a href="/search/eess?searchtype=author&query=Nami%2C+S">Sanaz Nami</a>, 
<a href="/search/eess?searchtype=author&query=Gabbouj%2C+M">Moncef Gabbouj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Copyright 2024 IEEE - Submitted to IEEE ICIP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)

</div>
<p class="mathjax">Emerging Learned image Compression (LC) achieves significant improvements in
coding efficiency by end-to-end training of neural networks for compression. An
important benefit of this approach over traditional codecs is that any
optimization criteria can be directly applied to the encoder-decoder networks
during training. Perceptual optimization of LC to comply with the Human Visual
System (HVS) is among such criteria, which has not been fully explored yet.
This paper addresses this gap by proposing a novel framework to integrate Just
Noticeable Distortion (JND) principles into LC. Leveraging existing JND
datasets, three perceptual optimization methods are proposed to integrate JND
into the LC training process: (1) Pixel-Wise JND Loss (PWL) prioritizes
pixel-by-pixel fidelity in reproducing JND characteristics, (2) Image-Wise JND
Loss (IWL) emphasizes on overall imperceptible degradation levels, and (3)
Feature-Wise JND Loss (FWL) aligns the reconstructed image features with
perceptually significant features. Experimental evaluations demonstrate the
effectiveness of JND integration, highlighting improvements in rate-distortion
performance and visual quality, compared to baseline methods. The proposed
methods add no extra complexity after training.
</p>
</div>
</dd>
<dt><a name="item966">[966]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02846" title="Abstract">arXiv:2402.02846</a> (cross-list from physics.optics) [<a href="/pdf/2402.02846" title="Download PDF">pdf</a>, <a href="/format/2402.02846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning Resistant Amorphous Silicon Physically Unclonable  Functions (PUFs)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Kilic%2C+V">Velat Kilic</a>, 
<a href="/search/physics?searchtype=author&query=Macfarlane%2C+N">Neil Macfarlane</a>, 
<a href="/search/physics?searchtype=author&query=Stround%2C+J">Jasper Stround</a>, 
<a href="/search/physics?searchtype=author&query=Metais%2C+S">Samuel Metais</a>, 
<a href="/search/physics?searchtype=author&query=Alemohammad%2C+M">Milad Alemohammad</a>, 
<a href="/search/physics?searchtype=author&query=Cooper%2C+A+B">A. Brinton Cooper</a>, 
<a href="/search/physics?searchtype=author&query=Foster%2C+A+C">Amy C. Foster</a>, 
<a href="/search/physics?searchtype=author&query=Foster%2C+M+A">Mark A. Foster</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optics (physics.optics)</span>; Machine Learning (cs.LG); Applied Physics (physics.app-ph)

</div>
<p class="mathjax">We investigate usage of nonlinear wave chaotic amorphous silicon (a-Si)
cavities as physically unclonable functions (PUF). Machine learning attacks on
integrated electronic PUFs have been demonstrated to be very effective at
modeling PUF behavior. Such attacks on integrated a-Si photonic PUFs are
investigated through application of algorithms including linear regression,
k-nearest neighbor, decision tree ensembles (random forests and gradient
boosted trees), and deep neural networks (DNNs). We found that DNNs performed
the best among all the algorithms studied but still failed to completely break
the a-Si PUF security which we quantify through a private information metric.
Furthermore, machine learning resistance of a-Si PUFs were found to be directly
related to the strength of their nonlinear response.
</p>
</div>
</dd>
<dt><a name="item967">[967]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02850" title="Abstract">arXiv:2402.02850</a> (cross-list from eess.AS) [<a href="/pdf/2402.02850" title="Download PDF">pdf</a>, <a href="/format/2402.02850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Attention Long Short-Term Memory based system for automatic  classification of speech intelligibility
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fern%C3%A1ndez-D%C3%ADaz%2C+M">Miguel Fern&#xe1;ndez-D&#xed;az</a>, 
<a href="/search/eess?searchtype=author&query=Gallardo-Antol%C3%ADn%2C+A">Ascensi&#xf3;n Gallardo-Antol&#xed;n</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Miguel Fernandez-Diaz and Ascension Gallardo-Antolin Engineering
  Applications of Artificial Intelligence 96 (2020) 103976
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Speech intelligibility can be degraded due to multiple factors, such as noisy
environments, technical difficulties or biological conditions. This work is
focused on the development of an automatic non-intrusive system for predicting
the speech intelligibility level in this latter case. The main contribution of
our research on this topic is the use of Long Short-Term Memory (LSTM) networks
with log-mel spectrograms as input features for this purpose. In addition, this
LSTM-based system is further enhanced by the incorporation of a simple
attention mechanism that is able to determine the more relevant frames to this
task. The proposed models are evaluated with the UA-Speech database that
contains dysarthric speech with different degrees of severity. Results show
that the attention LSTM architecture outperforms both, a reference Support
Vector Machine (SVM)-based system with hand-crafted features and a LSTM-based
system with Mean-Pooling.
</p>
</div>
</dd>
<dt><a name="item968">[968]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02857" title="Abstract">arXiv:2402.02857</a> (cross-list from stat.ML) [<a href="/pdf/2402.02857" title="Download PDF">pdf</a>, <a href="/format/2402.02857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-asymptotic Analysis of Biased Adaptive Stochastic Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Surendran%2C+S">Sobihan Surendran</a> (LPSM (UMR\_8001)), 
<a href="/search/stat?searchtype=author&query=Godichon-Baggioni%2C+A">Antoine Godichon-Baggioni</a> (LPSM (UMR\_8001)), 
<a href="/search/stat?searchtype=author&query=Fermanian%2C+A">Adeline Fermanian</a>, 
<a href="/search/stat?searchtype=author&query=Corff%2C+S+L">Sylvain Le Corff</a> (LPSM (UMR\_8001))
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Stochastic Gradient Descent (SGD) with adaptive steps is now widely used for
training deep neural networks. Most theoretical results assume access to
unbiased gradient estimators, which is not the case in several recent deep
learning and reinforcement learning applications that use Monte Carlo methods.
This paper provides a comprehensive non-asymptotic analysis of SGD with biased
gradients and adaptive steps for convex and non-convex smooth functions. Our
study incorporates time-dependent bias and emphasizes the importance of
controlling the bias and Mean Squared Error (MSE) of the gradient estimator. In
particular, we establish that Adagrad and RMSProp with biased gradients
converge to critical points for smooth non-convex functions at a rate similar
to existing results in the literature for the unbiased case. Finally, we
provide experimental results using Variational Autoenconders (VAE) that
illustrate our convergence results and show how the effect of bias can be
reduced by appropriate hyperparameter tuning.
</p>
</div>
</dd>
<dt><a name="item969">[969]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02862" title="Abstract">arXiv:2402.02862</a> (cross-list from stat.ML) [<a href="/pdf/2402.02862" title="Download PDF">pdf</a>, <a href="/format/2402.02862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Machine: A New Model for Learning with Tabular Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Nikolentzos%2C+G">Giannis Nikolentzos</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+S">Siyun Wang</a>, 
<a href="/search/stat?searchtype=author&query=Lutzeyer%2C+J">Johannes Lutzeyer</a>, 
<a href="/search/stat?searchtype=author&query=Vazirgiannis%2C+M">Michalis Vazirgiannis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In recent years, there has been a growing interest in mapping data from
different domains to graph structures. Among others, neural network models such
as the multi-layer perceptron (MLP) can be modeled as graphs. In fact, MLPs can
be represented as directed acyclic graphs. Graph neural networks (GNNs) have
recently become the standard tool for performing machine learning tasks on
graphs. In this work, we show that an MLP is equivalent to an asynchronous
message passing GNN model which operates on the MLP's graph representation. We
then propose a new machine learning model for tabular data, the so-called Graph
Neural Machine (GNM), which replaces the MLP's directed acyclic graph with a
nearly complete graph and which employs a synchronous message passing scheme.
We show that a single GNM model can simulate multiple MLP models. We evaluate
the proposed model in several classification and regression datasets. In most
cases, the GNM model outperforms the MLP architecture.
</p>
</div>
</dd>
<dt><a name="item970">[970]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02865" title="Abstract">arXiv:2402.02865</a> (cross-list from eess.AS) [<a href="/pdf/2402.02865" title="Download PDF">pdf</a>, <a href="/format/2402.02865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On combining acoustic and modulation spectrograms in an attention  LSTM-based system for speech intelligibility level classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gallardo-Antol%C3%ADn%2C+A">Ascensi&#xf3;n Gallardo-Antol&#xed;n</a>, 
<a href="/search/eess?searchtype=author&query=Montero%2C+J+M">Juan M. Montero</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Ascension Gallardo-Antolin and Juan M. Montero Neurocomputing 456
  (2021) 49-60
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Speech intelligibility can be affected by multiple factors, such as noisy
environments, channel distortions or physiological issues. In this work, we
deal with the problem of automatic prediction of the speech intelligibility
level in this latter case. Starting from our previous work, a non-intrusive
system based on LSTM networks with attention mechanism designed for this task,
we present two main contributions. In the first one, it is proposed the use of
per-frame modulation spectrograms as input features, instead of compact
representations derived from them that discard important temporal information.
In the second one, two different strategies for the combination of per-frame
acoustic log-mel and modulation spectrograms into the LSTM framework are
explored: at decision level or late fusion and at utterance level or
Weighted-Pooling (WP) fusion. The proposed models are evaluated with the
UA-Speech database that contains dysarthric speech with different degrees of
severity. On the one hand, results show that attentional LSTM networks are able
to adequately modeling the modulation spectrograms sequences producing similar
classification rates as in the case of log-mel spectrograms. On the other hand,
both combination strategies, late and WP fusion, outperform the single-feature
systems, suggesting that per-frame log-mel and modulation spectrograms carry
complementary information for the task of speech intelligibility prediction,
than can be effectively exploited by the LSTM-based architectures, being the
system with the WP fusion strategy and Attention-Pooling the one that achieves
best results.
</p>
</div>
</dd>
<dt><a name="item971">[971]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02866" title="Abstract">arXiv:2402.02866</a> (cross-list from quant-ph) [<a href="/pdf/2402.02866" title="Download PDF">pdf</a>, <a href="/format/2402.02866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Normalizing Flows for Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Rosenhahn%2C+B">Bodo Rosenhahn</a>, 
<a href="/search/quant-ph?searchtype=author&query=Hirche%2C+C">Christoph Hirche</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">A Normalizing Flow computes a bijective mapping from an arbitrary
distribution to a predefined (e.g. normal) distribution. Such a flow can be
used to address different tasks, e.g. anomaly detection, once such a mapping
has been learned. In this work we introduce Normalizing Flows for Quantum
architectures, describe how to model and optimize such a flow and evaluate our
method on example datasets. Our proposed models show competitive performance
for anomaly detection compared to classical methods, e.g. based on isolation
forests, the local outlier factor (LOF) or single-class SVMs, while being fully
executable on a quantum computer.
</p>
</div>
</dd>
<dt><a name="item972">[972]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02880" title="Abstract">arXiv:2402.02880</a> (cross-list from quant-ph) [<a href="/pdf/2402.02880" title="Download PDF">pdf</a>, <a href="/format/2402.02880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unleashing the Expressive Power of Pulse-Based Quantum Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Tao%2C+H">Han-Xiao Tao</a>, 
<a href="/search/quant-ph?searchtype=author&query=Hu%2C+J">Jiaqi Hu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wu%2C+R">Re-Bing Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages; 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET); Machine Learning (cs.LG)

</div>
<p class="mathjax">Quantum machine learning (QML) based on Noisy Intermediate-Scale Quantum
(NISQ) devices requires the optimal utilization of limited quantum resources.
The commonly used gate-based QML models are convenient for software engineers,
but their expressivity is restricted by the permissible circuit depth within a
finite coherence time. In contrast, pulse-based models enable the construction
of "infinitely" deep quantum neural networks within the same coherence time,
which may unleash greater expressive power for complex learning tasks. In this
paper, we investigate this potential from the perspective of quantum control
theory. We first indicate that the nonlinearity of pulse-based models comes
from the encoding process that can be viewed as the continuous limit of
data-reuploading in gate-based models. Subsequently, we prove that the
pulse-based model can approximate arbitrary nonlinear functions when the
underlying physical system is ensemble controllable. Under this condition,
numerical simulations show that the expressivity can be enhanced by either
increasing the pulse length or the number of qubits. As anticipated, we
demonstrate through numerical examples that the pulse-based model can unleash
more expressive power compared to the gate-based model. These findings
establish a theoretical foundation for understanding and designing expressive
QML models using NISQ devices.
</p>
</div>
</dd>
<dt><a name="item973">[973]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02894" title="Abstract">arXiv:2402.02894</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2402.02894" title="Download PDF">pdf</a>, <a href="/ps/2402.02894" title="Download PostScript">ps</a>, <a href="/format/2402.02894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shock equations and jump conditions for the 2D Adjoint Euler equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Lozano%2C+C">Carlos Lozano</a>, 
<a href="/search/physics?searchtype=author&query=Ponsin%2C+J">Jorge Ponsin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Aerospace 2023, 10, 267
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">This paper considers the formulation of the adjoint problem in two dimensions
when there are shocks in the flow solution. For typical cost functions, the
adjoint variables are continuous at shocks, where they have to obey an internal
boundary condition, but their derivatives may be discontinuous. The derivation
of the adjoint shock equations is reviewed and detailed predictions for the
behavior of the gradients of the adjoint variables at shocks are obtained as
jump conditions for the normal adjoint gradients in terms of the tangent
gradients. Several numerical computations on a very fine mesh are used to
illustrate the behavior of numerical adjoint solutions at shocks.
</p>
</div>
</dd>
<dt><a name="item974">[974]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02897" title="Abstract">arXiv:2402.02897</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2402.02897" title="Download PDF">pdf</a>, <a href="/ps/2402.02897" title="Download PostScript">ps</a>, <a href="/format/2402.02897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explaining the Lack of Mesh Convergence of Inviscid Adjoint Solutions  Near Solid Walls for Subcritical Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Lozano%2C+C">Carlos Lozano</a>, 
<a href="/search/physics?searchtype=author&query=Ponsin%2C+J">Jorge Ponsin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, accepted version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Aerospace 2023, 10, 392
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Numerical solutions to the adjoint Euler equations have been found to diverge
with mesh refinement near walls for a variety of flow conditions and geometry
configurations. The issue is reviewed and an explanation is provided by
comparing a numerical incompressible adjoint solution with an analytic adjoint
solution, showing that the anomaly observed in numerical computations is caused
by a divergence of the analytic solution at the wall. The singularity causing
this divergence is of the same type as the well-known singularity along the
incoming stagnation streamline and both originate at the adjoint singularity at
the trailing edge. The argument is extended to cover the fully compressible
case, in subcritical flow conditions, by presenting an analytic solution that
follows the same structure as the incompressible one.
</p>
</div>
</dd>
<dt><a name="item975">[975]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02909" title="Abstract">arXiv:2402.02909</a> (cross-list from stat.AP) [<a href="/pdf/2402.02909" title="Download PDF">pdf</a>, <a href="/ps/2402.02909" title="Download PostScript">ps</a>, <a href="/format/2402.02909" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digital Twin for Grey Box modeling of Multistory residential building  thermal dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Morkunaite%2C+L">Lina Morkunaite</a>, 
<a href="/search/stat?searchtype=author&query=Kardoka%2C+J">Justas Kardoka</a>, 
<a href="/search/stat?searchtype=author&query=Pupeikis%2C+D">Darius Pupeikis</a>, 
<a href="/search/stat?searchtype=author&query=Fokaides%2C+P">Paris Fokaides</a>, 
<a href="/search/stat?searchtype=author&query=Angelakis%2C+V">Vangelis Angelakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Buildings energy efficiency is a widely researched topic, which is rapidly
gaining popularity due to rising environmental concerns and the need for energy
independence. In Northern Europe heating energy alone accounts for up to 70
percent of the total building energy consumption. Industry 4.0 technologies
such as IoT, big data, cloud computing and machine learning, along with the
creation of predictive and proactive digital twins, can help to reduce this
number. However, buildings thermal dynamics is a very complex process that
depends on many variables. As a result, commonly used physics-based white box
models are time-consuming and require vast expertise. On the contrary, black
box forecasting models, which rely primarily on building energy consumption
data, lack fundamental insights and hinder re-use. In this study we propose an
architecture to facilitate grey box modelling of building thermal dynamics
while integrating real time IoT data with 3D representation of buildings. The
architecture is validated in a case study creating a digital twin platform that
enables users to define the thermal dynamics of buildings based on physical
laws and real data, thus facilitating informed decision making for the best
heating energy optimization strategy. Also, the created user interface enables
stakeholders such as facility managers, energy providers or governing bodies to
analyse, compare and evaluate buildings thermal dynamics without extensive
expertise or time resources.
</p>
</div>
</dd>
<dt><a name="item976">[976]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02936" title="Abstract">arXiv:2402.02936</a> (cross-list from eess.IV) [<a href="/pdf/2402.02936" title="Download PDF">pdf</a>, <a href="/format/2402.02936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Panoramic Image Inpainting With Gated Convolution And Contextual  Reconstruction Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yu%2C+L">Li Yu</a>, 
<a href="/search/eess?searchtype=author&query=Gao%2C+Y">Yanjun Gao</a>, 
<a href="/search/eess?searchtype=author&query=Pakdaman%2C+F">Farhad Pakdaman</a>, 
<a href="/search/eess?searchtype=author&query=Gabbouj%2C+M">Moncef Gabbouj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Copyright 2024 IEEE - to appear in IEEE ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
<p class="mathjax">Deep learning-based methods have demonstrated encouraging results in tackling
the task of panoramic image inpainting. However, it is challenging for existing
methods to distinguish valid pixels from invalid pixels and find suitable
references for corrupted areas, thus leading to artifacts in the inpainted
results. In response to these challenges, we propose a panoramic image
inpainting framework that consists of a Face Generator, a Cube Generator, a
side branch, and two discriminators. We use the Cubemap Projection (CMP) format
as network input. The generator employs gated convolutions to distinguish valid
pixels from invalid ones, while a side branch is designed utilizing contextual
reconstruction (CR) loss to guide the generators to find the most suitable
reference patch for inpainting the missing region. The proposed method is
compared with state-of-the-art (SOTA) methods on SUN360 Street View dataset in
terms of PSNR and SSIM. Experimental results and ablation study demonstrate
that the proposed method outperforms SOTA both quantitatively and
qualitatively.
</p>
</div>
</dd>
<dt><a name="item977">[977]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02952" title="Abstract">arXiv:2402.02952</a> (cross-list from stat.ML) [<a href="/pdf/2402.02952" title="Download PDF">pdf</a>, <a href="/ps/2402.02952" title="Download PostScript">ps</a>, <a href="/format/2402.02952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Least Squares Estimation in Softmax Gating Mixture of Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Nguyen%2C+H">Huy Nguyen</a>, 
<a href="/search/stat?searchtype=author&query=Ho%2C+N">Nhat Ho</a>, 
<a href="/search/stat?searchtype=author&query=Rinaldo%2C+A">Alessandro Rinaldo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Mixture of experts (MoE) model is a statistical machine learning design that
aggregates multiple expert networks using a softmax gating function in order to
form a more intricate and expressive model. Despite being commonly used in
several applications owing to their scalability, the mathematical and
statistical properties of MoE models are complex and difficult to analyze. As a
result, previous theoretical works have primarily focused on probabilistic MoE
models by imposing the impractical assumption that the data are generated from
a Gaussian MoE model. In this work, we investigate the performance of the least
squares estimators (LSE) under a deterministic MoE model where the data are
sampled according to a regression model, a setting that has remained largely
unexplored. We establish a condition called strong identifiability to
characterize the convergence behavior of various types of expert functions. We
demonstrate that the rates for estimating strongly identifiable experts, namely
the widely used feed forward networks with activation functions
$\mathrm{sigmoid}(\cdot)$ and $\tanh(\cdot)$, are substantially faster than
those of polynomial experts, which we show to exhibit a surprising slow
estimation rate. Our findings have important practical implications for expert
selection.
</p>
</div>
</dd>
<dt><a name="item978">[978]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02963" title="Abstract">arXiv:2402.02963</a> (cross-list from eess.IV) [<a href="/pdf/2402.02963" title="Download PDF">pdf</a>, <a href="/format/2402.02963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-class anomaly detection through color-to-thermal AI for building  envelope inspection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kurtser%2C+P">Polina Kurtser</a>, 
<a href="/search/eess?searchtype=author&query=Feng%2C+K">Kailun Feng</a>, 
<a href="/search/eess?searchtype=author&query=Olofsson%2C+T">Thomas Olofsson</a>, 
<a href="/search/eess?searchtype=author&query=De+Andres%2C+A">Aitor De Andres</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">We present a label-free method for detecting anomalies during thermographic
inspection of building envelopes. It is based on the AI-driven prediction of
thermal distributions from color images. Effectively the method performs as a
one-class classifier of the thermal image regions with high mismatch between
the predicted and actual thermal distributions. The algorithm can learn to
identify certain features as normal or anomalous by selecting the target sample
used for training. We demonstrated this principle by training the algorithm
with data collected at different outdoors temperature, which lead to the
detection of thermal bridges. The method can be implemented to assist human
professionals during routine building inspections or combined with mobile
platforms for automating examination of large areas.
</p>
</div>
</dd>
<dt><a name="item979">[979]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02969" title="Abstract">arXiv:2402.02969</a> (cross-list from stat.ML) [<a href="/pdf/2402.02969" title="Download PDF">pdf</a>, <a href="/format/2402.02969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Understanding the Word Sensitivity of Attention Layers: A Study  via Random Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Bombari%2C+S">Simone Bombari</a>, 
<a href="/search/stat?searchtype=author&query=Mondelli%2C+M">Marco Mondelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Unveiling the reasons behind the exceptional success of transformers requires
a better understanding of why attention layers are suitable for NLP tasks. In
particular, such tasks require predictive models to capture contextual meaning
which often depends on one or few words, even if the sentence is long. Our work
studies this key property, dubbed word sensitivity (WS), in the prototypical
setting of random features. We show that attention layers enjoy high WS,
namely, there exists a vector in the space of embeddings that largely perturbs
the random attention features map. The argument critically exploits the role of
the softmax in the attention layer, highlighting its benefit compared to other
activations (e.g., ReLU). In contrast, the WS of standard random features is of
order $1/\sqrt{n}$, $n$ being the number of words in the textual sample, and
thus it decays with the length of the context. We then translate these results
on the word sensitivity into generalization bounds: due to their low WS, random
features provably cannot learn to distinguish between two sentences that differ
only in a single word; in contrast, due to their high WS, random attention
features have higher generalization capabilities. We validate our theoretical
results with experimental evidence over the BERT-Base word embeddings of the
imdb review dataset.
</p>
</div>
</dd>
<dt><a name="item980">[980]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02997" title="Abstract">arXiv:2402.02997</a> (cross-list from quant-ph) [<a href="/pdf/2402.02997" title="Download PDF">pdf</a>, <a href="/format/2402.02997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Profunctorial Semantics for Quantum Supermaps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Hefford%2C+J">James Hefford</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wilson%2C+M">Matt Wilson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Logic in Computer Science (cs.LO); Category Theory (math.CT)

</div>
<p class="mathjax">We identify morphisms of strong profunctors as a categorification of quantum
supermaps. These black-box generalisations of diagrams-with-holes are hence
placed within the broader field of profunctor optics, as morphisms in the
category of copresheaves on concrete networks. This enables the first
construction of abstract logical connectives such as tensor products and
negations for supermaps in a totally theory-independent setting. These logical
connectives are found to be all that is needed to abstractly model the key
structural features of the quantum theory of supermaps: black-box indefinite
causal order, black-box definite causal order, and the factorisation of
definitely causally ordered supermaps into concrete circuit diagrams. We
demonstrate that at the heart of these factorisation theorems lies the Yoneda
lemma and the notion of representability.
</p>
</div>
</dd>
<dt><a name="item981">[981]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03008" title="Abstract">arXiv:2402.03008</a> (cross-list from stat.ML) [<a href="/pdf/2402.03008" title="Download PDF">pdf</a>, <a href="/format/2402.03008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusive Gibbs Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chen%2C+W">Wenlin Chen</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+M">Mingtian Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Paige%2C+B">Brooks Paige</a>, 
<a href="/search/stat?searchtype=author&query=Hern%C3%A1ndez-Lobato%2C+J+M">Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato</a>, 
<a href="/search/stat?searchtype=author&query=Barber%2C+D">David Barber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 11 figures, 4 tables, 1 algorithm
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods
for multi-modal distributions presents a significant challenge in practical
applications such as Bayesian inference and molecular dynamics. Addressing
this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of
sampling methods designed for effective sampling from distributions
characterized by distant and disconnected modes. DiGS integrates recent
developments in diffusion models, leveraging Gaussian convolution to create an
auxiliary noisy distribution that bridges isolated modes in the original space
and applying Gibbs sampling to alternately draw samples from both spaces. Our
approach exhibits a better mixing property for sampling multi-modal
distributions than state-of-the-art methods such as parallel tempering. We
demonstrate that our sampler attains substantially improved results across
various tasks, including mixtures of Gaussians, Bayesian neural networks and
molecular dynamics.
</p>
</div>
</dd>
<dt><a name="item982">[982]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03015" title="Abstract">arXiv:2402.03015</a> (cross-list from math.CO) [<a href="/pdf/2402.03015" title="Download PDF">pdf</a>, <a href="/format/2402.03015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-separating dominating codes in graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chakraborty%2C+D">Dipayan Chakraborty</a>, 
<a href="/search/math?searchtype=author&query=Wagler%2C+A+K">Annegret K. Wagler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">Using dominating sets to separate vertices of graphs is a well-studied
problem in the larger domain of identification problems. In such problems, the
objective is typically to separate any two vertices of a graph by their unique
neighbourhoods in a suitably chosen dominating set of the graph. Such a
dominating and separating set is often referred to as a \emph{code} in the
literature. Depending on the types of dominating and separating sets used,
various problems arise under various names in the literature. In this paper, we
introduce a new problem in the same realm of identification problems whereby
the code, called the \emph{open-separating dominating code}, or the
\emph{OSD-code} for short, is a dominating set and uses open neighbourhoods for
separating vertices. The paper studies the fundamental properties concerning
the existence, hardness and minimality of OSD-codes. Due to the emergence of a
close and yet difficult to establish relation of the OSD-codes with another
well-studied code in the literature called the open locating dominating codes,
or OLD-codes for short, we compare the two on various graph classes. Finally,
we also provide an equivalent reformulation of the problem of finding OSD-codes
of a graph as a covering problem in a suitable hypergraph and discuss the
polyhedra associated with OSD-codes, again in relation to OLD-codes of some
graph classes already studied in this context.
</p>
</div>
</dd>
<dt><a name="item983">[983]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03058" title="Abstract">arXiv:2402.03058</a> (cross-list from eess.AS) [<a href="/pdf/2402.03058" title="Download PDF">pdf</a>, <a href="/format/2402.03058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Array Geometry-Robust Attention-Based Neural Beamformer for Moving  Speakers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tammen%2C+M">Marvin Tammen</a>, 
<a href="/search/eess?searchtype=author&query=Ochiai%2C+T">Tsubasa Ochiai</a>, 
<a href="/search/eess?searchtype=author&query=Delcroix%2C+M">Marc Delcroix</a>, 
<a href="/search/eess?searchtype=author&query=Nakatani%2C+T">Tomohiro Nakatani</a>, 
<a href="/search/eess?searchtype=author&query=Araki%2C+S">Shoko Araki</a>, 
<a href="/search/eess?searchtype=author&query=Doclo%2C+S">Simon Doclo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Recently, a mask-based beamformer with attention-based spatial covariance
matrix aggregator (ASA) was proposed, which was demonstrated to track moving
sources accurately. However, the deep neural network model used in this
algorithm is limited to a specific channel configuration, requiring a different
model in case a different channel permutation, channel count, or microphone
array geometry is considered. Addressing this limitation, in this paper, we
investigate three approaches to improve the robustness of the ASA-based
tracking method against such variations: incorporating random channel
configurations during the training process, employing the
transform-average-concatenate (TAC) method to process multi-channel input
features (allowing for any channel count and enabling permutation invariance),
and utilizing input features that are robust against variations of the channel
configuration. Our experiments, conducted using the CHiME-3 and DEMAND
datasets, demonstrate improved robustness against mismatches in channel
permutations, channel counts, and microphone array geometries compared to the
conventional ASA-based tracking method without compromising performance in
matched conditions, suggesting that the mask-based beamformer with ASA
integrating the proposed approaches has the potential to track moving sources
for arbitrary microphone arrays.
</p>
</div>
</dd>
<dt><a name="item984">[984]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03069" title="Abstract">arXiv:2402.03069</a> (cross-list from math.LO) [<a href="/pdf/2402.03069" title="Download PDF">pdf</a>, <a href="/ps/2402.03069" title="Download PostScript">ps</a>, <a href="/format/2402.03069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fixed Point Theorems in Computability Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Terwijn%2C+S+A">Sebastiaan A. Terwijn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">We give a quick survey of the various fixed point theorems in computability
theory, partial combinatory algebra, and the theory of numberings, as well as
generalizations based on those. We also point out several open problems
connected to these.
</p>
</div>
</dd>
<dt><a name="item985">[985]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03072" title="Abstract">arXiv:2402.03072</a> (cross-list from q-bio.NC) [<a href="/pdf/2402.03072" title="Download PDF">pdf</a>, <a href="/format/2402.03072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Abstract Visuomotor Mappings using Meta-Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Velazquez-Vargas%2C+C+A">Carlos A. Velazquez-Vargas</a>, 
<a href="/search/q-bio?searchtype=author&query=Christian%2C+I+R">Isaac Ray Christian</a>, 
<a href="/search/q-bio?searchtype=author&query=Taylor%2C+J+A">Jordan A. Taylor</a>, 
<a href="/search/q-bio?searchtype=author&query=Kumar%2C+S">Sreejan Kumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We investigated the human capacity to acquire multiple visuomotor mappings
for de novo skills. Using a grid navigation paradigm, we tested whether
contextual cues implemented as different "grid worlds", allow participants to
learn two distinct key-mappings more efficiently. Our results indicate that
when contextual information is provided, task performance is significantly
better. The same held true for meta-reinforcement learning agents that differed
in whether or not they receive contextual information when performing the task.
We evaluated their accuracy in predicting human performance in the task and
analyzed their internal representations. The results indicate that contextual
cues allow the formation of separate representations in space and time when
using different visuomotor mappings, whereas the absence of them favors sharing
one representation. While both strategies can allow learning of multiple
visuomotor mappings, we showed contextual cues provide a computational
advantage in terms of how many mappings can be learned.
</p>
</div>
</dd>
<dt><a name="item986">[986]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03086" title="Abstract">arXiv:2402.03086</a> (cross-list from math.OC) [<a href="/pdf/2402.03086" title="Download PDF">pdf</a>, <a href="/format/2402.03086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual Lagrangian Learning for Conic Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Tanneau%2C+M">Mathieu Tanneau</a>, 
<a href="/search/math?searchtype=author&query=Van+Hentenryck%2C+P">Pascal Van Hentenryck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper presents Dual Lagrangian Learning (DLL), a principled learning
methodology that combines conic duality theory with the representation power of
ML models. DLL leverages conic duality to provide dual-feasible solutions, and
therefore valid Lagrangian dual bounds, for parametric linear and nonlinear
conic optimization problems. The paper introduces differentiable conic
projection layers, a systematic dual completion procedure, and a
self-supervised learning framework. The effectiveness of DLL is demonstrated on
linear and nonlinear parametric optimization problems for which DLL provides
valid dual bounds within 0.5% of optimality.
</p>
</div>
</dd>
<dt><a name="item987">[987]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03091" title="Abstract">arXiv:2402.03091</a> (cross-list from math.AP) [<a href="/pdf/2402.03091" title="Download PDF">pdf</a>, <a href="/format/2402.03091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal rate of convergence in periodic homogenization of viscous  Hamilton-Jacobi equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Qian%2C+J">Jianliang Qian</a>, 
<a href="/search/math?searchtype=author&query=Sprekeler%2C+T">Timo Sprekeler</a>, 
<a href="/search/math?searchtype=author&query=Tran%2C+H+V">Hung V. Tran</a>, 
<a href="/search/math?searchtype=author&query=Yu%2C+Y">Yifeng Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We study the optimal rate of convergence in periodic homogenization of the
viscous Hamilton-Jacobi equation $u^\varepsilon_t +
H(\frac{x}{\varepsilon},Du^\varepsilon) = \varepsilon \Delta u^\varepsilon$ in
$\mathbb R^n\times (0,\infty)$ subject to a given initial datum. We prove that
$\|u^\varepsilon-u\|_{L^\infty(\mathbb R^n \times [0,T])} \leq C(1+T)
\sqrt{\varepsilon}$ for any given $T&gt;0$. Moreover, we show that the
$O(\sqrt{\varepsilon})$ rate is optimal in general, both theoretically and
through numerical experiments. Finally, we propose a numerical scheme for the
approximation of the effective Hamiltonian based on a finite element
approximation of approximate corrector problems.
</p>
</div>
</dd>
<dt><a name="item988">[988]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03104" title="Abstract">arXiv:2402.03104</a> (cross-list from stat.ML) [<a href="/pdf/2402.03104" title="Download PDF">pdf</a>, <a href="/format/2402.03104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-dimensional Bayesian Optimization via Covariance Matrix Adaptation  Strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ngo%2C+L">Lam Ngo</a>, 
<a href="/search/stat?searchtype=author&query=Ha%2C+H">Huong Ha</a>, 
<a href="/search/stat?searchtype=author&query=Chan%2C+J">Jeffrey Chan</a>, 
<a href="/search/stat?searchtype=author&query=Nguyen%2C+V">Vu Nguyen</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+H">Hongyu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 17 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Transactions on Machine Learning Research 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Bayesian Optimization (BO) is an effective method for finding the global
optimum of expensive black-box functions. However, it is well known that
applying BO to high-dimensional optimization problems is challenging. To
address this issue, a promising solution is to use a local search strategy that
partitions the search domain into local regions with high likelihood of
containing the global optimum, and then use BO to optimize the objective
function within these regions. In this paper, we propose a novel technique for
defining the local regions using the Covariance Matrix Adaptation (CMA)
strategy. Specifically, we use CMA to learn a search distribution that can
estimate the probabilities of data points being the global optimum of the
objective function. Based on this search distribution, we then define the local
regions consisting of data points with high probabilities of being the global
optimum. Our approach serves as a meta-algorithm as it can incorporate existing
black-box BO optimizers, such as BO, TuRBO, and BAxUS, to find the global
optimum of the objective function within our derived local regions. We evaluate
our proposed method on various benchmark synthetic and real-world problems. The
results demonstrate that our method outperforms existing state-of-the-art
techniques.
</p>
</div>
</dd>
<dt><a name="item989">[989]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03167" title="Abstract">arXiv:2402.03167</a> (cross-list from math.OC) [<a href="/pdf/2402.03167" title="Download PDF">pdf</a>, <a href="/format/2402.03167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic  Update and Transient Iteration Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kong%2C+B">Boao Kong</a>, 
<a href="/search/math?searchtype=author&query=Zhu%2C+S">Shuchen Zhu</a>, 
<a href="/search/math?searchtype=author&query=Lu%2C+S">Songtao Lu</a>, 
<a href="/search/math?searchtype=author&query=Huang%2C+X">Xinmeng Huang</a>, 
<a href="/search/math?searchtype=author&query=Yuan%2C+K">Kun Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Stochastic bilevel optimization (SBO) is becoming increasingly essential in
machine learning due to its versatility in handling nested structures. To
address large-scale SBO, decentralized approaches have emerged as effective
paradigms in which nodes communicate with immediate neighbors without a central
server, thereby improving communication efficiency and enhancing algorithmic
robustness. However, current decentralized SBO algorithms face challenges,
including expensive inner-loop updates and unclear understanding of the
influence of network topology, data heterogeneity, and the nested bilevel
algorithmic structures. In this paper, we introduce a single-loop decentralized
SBO (D-SOBA) algorithm and establish its transient iteration complexity, which,
for the first time, clarifies the joint influence of network topology and data
heterogeneity on decentralized bilevel algorithms. D-SOBA achieves the
state-of-the-art asymptotic rate, asymptotic gradient/Hessian complexity, and
transient iteration complexity under more relaxed assumptions compared to
existing methods. Numerical experiments validate our theoretical findings.
</p>
</div>
</dd>
<dt><a name="item990">[990]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03169" title="Abstract">arXiv:2402.03169</a> (cross-list from stat.ML) [<a href="/pdf/2402.03169" title="Download PDF">pdf</a>, <a href="/ps/2402.03169" title="Download PostScript">ps</a>, <a href="/format/2402.03169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Random Matrix Approach to Low-Multilinear-Rank Tensor Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lebeau%2C+H">Hugo Lebeau</a>, 
<a href="/search/stat?searchtype=author&query=Chatelain%2C+F">Florent Chatelain</a>, 
<a href="/search/stat?searchtype=author&query=Couillet%2C+R">Romain Couillet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Probability (math.PR)

</div>
<p class="mathjax">This work presents a comprehensive understanding of the estimation of a
planted low-rank signal from a general spiked tensor model near the
computational threshold. Relying on standard tools from the theory of large
random matrices, we characterize the large-dimensional spectral behavior of the
unfoldings of the data tensor and exhibit relevant signal-to-noise ratios
governing the detectability of the principal directions of the signal. These
results allow to accurately predict the reconstruction performance of truncated
multilinear SVD (MLSVD) in the non-trivial regime. This is particularly
important since it serves as an initialization of the higher-order orthogonal
iteration (HOOI) scheme, whose convergence to the best low-multilinear-rank
approximation depends entirely on its initialization. We give a sufficient
condition for the convergence of HOOI and show that the number of iterations
before convergence tends to $1$ in the large-dimensional limit.
</p>
</div>
</dd>
<dt><a name="item991">[991]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03179" title="Abstract">arXiv:2402.03179</a> (cross-list from eess.IV) [<a href="/pdf/2402.03179" title="Download PDF">pdf</a>, <a href="/format/2402.03179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cool-chic video: Learned video coding with 800 parameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Leguay%2C+T">Thomas Leguay</a>, 
<a href="/search/eess?searchtype=author&query=Ladune%2C+T">Th&#xe9;o Ladune</a>, 
<a href="/search/eess?searchtype=author&query=Philippe%2C+P">Pierrick Philippe</a>, 
<a href="/search/eess?searchtype=author&query=D%C3%A9forges%2C+O">Olivier D&#xe9;forges</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, published in Data Compression Conference 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose a lightweight learned video codec with 900 multiplications per
decoded pixel and 800 parameters overall. To the best of our knowledge, this is
one of the neural video codecs with the lowest decoding complexity. It is built
upon the overfitted image codec Cool-chic and supplements it with an inter
coding module to leverage the video's temporal redundancies. The proposed model
is able to compress videos using both low-delay and random access
configurations and achieves rate-distortion close to AVC while out-performing
other overfitted codecs such as FFNeRV. The system is made open-source:
orange-opensource.github.io/Cool-Chic.
</p>
</div>
</dd>
<dt><a name="item992">[992]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03211" title="Abstract">arXiv:2402.03211</a> (cross-list from quant-ph) [<a href="/pdf/2402.03211" title="Download PDF">pdf</a>, <a href="/format/2402.03211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast classical simulation of Harvard/QuEra IQP circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Maslov%2C+D">Dmitri Maslov</a>, 
<a href="/search/quant-ph?searchtype=author&query=Bravyi%2C+S">Sergey Bravyi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Tripier%2C+F">Felix Tripier</a>, 
<a href="/search/quant-ph?searchtype=author&query=Maksymov%2C+A">Andrii Maksymov</a>, 
<a href="/search/quant-ph?searchtype=author&query=Latone%2C+J">Joe Latone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Establishing an advantage for (white-box) computations by a quantum computer
against its classical counterpart is currently a key goal for the quantum
computation community. A quantum advantage is achieved once a certain
computational capability of a quantum computer is so complex that it can no
longer be reproduced by classical means, and as such, the quantum advantage can
be seen as a continued negotiation between classical simulations and quantum
computational experiments.
<br />A recent publication (Bluvstein et al., Nature 626:58-65, 2024) introduces a
type of Instantaneous Quantum Polynomial-Time (IQP) computation complemented by
a $48$-qubit (logical) experimental demonstration using quantum hardware. The
authors state that the ``simulation of such logical circuits is challenging''
and project the simulation time to grow rapidly with the number of CNOT layers
added, see Figure 5d/bottom therein. However, we report a classical simulation
algorithm that takes only $0.00257947$ seconds to compute an amplitude for the
$48$-qubit computation, which is roughly $10^3$ times faster than that reported
by the original authors. Our algorithm is furthermore not subject to a
significant decline in performance due to the additional CNOT layers. We
simulated these types of IQP computations for up to $96$ qubits, taking an
average of $4.16629$ seconds to compute a single amplitude, and estimated that
a $192$-qubit simulation should be tractable for computations relying on Tensor
Processing Units.
</p>
</div>
</dd>
<dt><a name="item993">[993]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03220" title="Abstract">arXiv:2402.03220</a> (cross-list from stat.ML) [<a href="/pdf/2402.03220" title="Download PDF">pdf</a>, <a href="/format/2402.03220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Benefits of Reusing Batches for Gradient Descent in Two-Layer  Networks: Breaking the Curse of Information and Leap Exponents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Dandi%2C+Y">Yatin Dandi</a>, 
<a href="/search/stat?searchtype=author&query=Troiani%2C+E">Emanuele Troiani</a>, 
<a href="/search/stat?searchtype=author&query=Arnaboldi%2C+L">Luca Arnaboldi</a>, 
<a href="/search/stat?searchtype=author&query=Pesce%2C+L">Luca Pesce</a>, 
<a href="/search/stat?searchtype=author&query=Zdeborov%C3%A1%2C+L">Lenka Zdeborov&#xe1;</a>, 
<a href="/search/stat?searchtype=author&query=Krzakala%2C+F">Florent Krzakala</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We investigate the training dynamics of two-layer neural networks when
learning multi-index target functions. We focus on multi-pass gradient descent
(GD) that reuses the batches multiple times and show that it significantly
changes the conclusion about which functions are learnable compared to
single-pass gradient descent. In particular, multi-pass GD with finite stepsize
is found to overcome the limitations of gradient flow and single-pass GD given
by the information exponent (Ben Arous et al., 2021) and leap exponent (Abbe et
al., 2023) of the target function. We show that upon re-using batches, the
network achieves in just two time steps an overlap with the target subspace
even for functions not satisfying the staircase property (Abbe et al., 2021).
We characterize the (broad) class of functions efficiently learned in finite
time. The proof of our results is based on the analysis of the Dynamical
Mean-Field Theory (DMFT). We further provide a closed-form description of the
dynamical process of the low-dimensional projections of the weights, and
numerical experiments illustrating the theory.
</p>
</div>
</dd>
<dt><a name="item994">[994]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03231" title="Abstract">arXiv:2402.03231</a> (cross-list from stat.ME) [<a href="/pdf/2402.03231" title="Download PDF">pdf</a>, <a href="/format/2402.03231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved prediction of future user activity in online A/B testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Masoero%2C+L">Lorenzo Masoero</a>, 
<a href="/search/stat?searchtype=author&query=Beraha%2C+M">Mario Beraha</a>, 
<a href="/search/stat?searchtype=author&query=Richardson%2C+T">Thomas Richardson</a>, 
<a href="/search/stat?searchtype=author&query=Favaro%2C+S">Stefano Favaro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Applications (stat.AP)

</div>
<p class="mathjax">In online randomized experiments or A/B tests, accurate predictions of
participant inclusion rates are of paramount importance. These predictions not
only guide experimenters in optimizing the experiment's duration but also
enhance the precision of treatment effect estimates. In this paper we present a
novel, straightforward, and scalable Bayesian nonparametric approach for
predicting the rate at which individuals will be exposed to interventions
within the realm of online A/B testing. Our approach stands out by offering
dual prediction capabilities: it forecasts both the quantity of new customers
expected in future time windows and, unlike available alternative methods, the
number of times they will be observed. We derive closed-form expressions for
the posterior distributions of the quantities needed to form predictions about
future user activity, thereby bypassing the need for numerical algorithms such
as Markov chain Monte Carlo. After a comprehensive exposition of our model, we
test its performance on experiments on real and simulated data, where we show
its superior performance with respect to existing alternatives in the
literature.
</p>
</div>
</dd>
<dt><a name="item995">[995]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03237" title="Abstract">arXiv:2402.03237</a> (cross-list from math.FA) [<a href="/pdf/2402.03237" title="Download PDF">pdf</a>, <a href="/format/2402.03237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Declipping and the recovery of vectors from saturated measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Alharbi%2C+W">Wedad Alharbi</a>, 
<a href="/search/math?searchtype=author&query=Freeman%2C+D">Daniel Freeman</a>, 
<a href="/search/math?searchtype=author&query=Ghoreishi%2C+D">Dorsa Ghoreishi</a>, 
<a href="/search/math?searchtype=author&query=Johnson%2C+B">Brody Johnson</a>, 
<a href="/search/math?searchtype=author&query=Randrianarivony%2C+N+L">N. Lovasoa Randrianarivony</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Functional Analysis (math.FA)</span>; Metric Geometry (math.MG); Numerical Analysis (math.NA)

</div>
<p class="mathjax">A frame $(x_j)_{j\in J}$ for a Hilbert space $H$ allows for a linear and
stable reconstruction of any vector $x\in H$ from the linear measurements
$(\langle x,x_j\rangle)_{j\in J}$. However, there are many situations where
some information in the frame coefficients is lost. In applications where one
is using sensors with a fixed dynamic range, any measurement above that range
is registered as the maximum, and any measurement below that range is
registered as the minimum. Depending on the context, recovering a vector from
such measurements is called either declipping or saturation recovery. We
initiate a frame theoretic approach to saturation recovery in a similar way to
what [BCE06] did for phase retrieval. We characterize when saturation recovery
is possible, show optimal frames for use with saturation recovery correspond to
minimal multi-fold packings in projective space, and prove that the classical
frame algorithm may be adapted to this non-linear problem to provide a
reconstruction algorithm.
</p>
</div>
</dd>
<dt><a name="item996">[996]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03245" title="Abstract">arXiv:2402.03245</a> (cross-list from math.OC) [<a href="/pdf/2402.03245" title="Download PDF">pdf</a>, <a href="/format/2402.03245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Popov-Belevitch-Hautus tests for functional observability and  output controllability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Montanari%2C+A+N">Arthur N. Montanari</a>, 
<a href="/search/math?searchtype=author&query=Duan%2C+C">Chao Duan</a>, 
<a href="/search/math?searchtype=author&query=Motter%2C+A+E">Adilson E. Motter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Systems and Control (eess.SY); Dynamical Systems (math.DS); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Functional observability and output controllability are properties that
establish the conditions respectively for the partial estimation and partial
control of the system state. In the special case of full-state observability
and controllability, the Popov-Belevitch-Hautus (PBH) tests provide conditions
for the properties to hold based on the system eigenspace. Generalizations of
the Popov-Belevitch-Hautus (PBH) test have been recently proposed for
functional observability and output controllability but were proved to be valid
only for diagonalizable systems thus far. Here, we rigorously establish a more
general class of systems based on their Jordan decomposition under which a
generalized PBH test for functional observability is valid. Likewise, we
determine the class of systems under which the generalized PBH test is
sufficient and necessary for output controllability. These results have
immediate implications for observer and controller design, pole assignment, and
optimal placement of sensors and drivers.
</p>
</div>
</dd>
<dt><a name="item997">[997]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03254" title="Abstract">arXiv:2402.03254</a> (cross-list from stat.ML) [<a href="/pdf/2402.03254" title="Download PDF">pdf</a>, <a href="/format/2402.03254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimum Description Length and Generalization Guarantees for  Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Sefidgaran%2C+M">Milad Sefidgaran</a>, 
<a href="/search/stat?searchtype=author&query=Zaidi%2C+A">Abdellatif Zaidi</a>, 
<a href="/search/stat?searchtype=author&query=Krasnowski%2C+P">Piotr Krasnowski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted and presented at NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
<p class="mathjax">A major challenge in designing efficient statistical supervised learning
algorithms is finding representations that perform well not only on available
training samples but also on unseen data. While the study of representation
learning has spurred much interest, most existing such approaches are
heuristic; and very little is known about theoretical generalization
guarantees.
<br />In this paper, we establish a compressibility framework that allows us to
derive upper bounds on the generalization error of a representation learning
algorithm in terms of the "Minimum Description Length" (MDL) of the labels or
the latent variables (representations). Rather than the mutual information
between the encoder's input and the representation, which is often believed to
reflect the algorithm's generalization capability in the related literature but
in fact, falls short of doing so, our new bounds involve the "multi-letter"
relative entropy between the distribution of the representations (or labels) of
the training and test sets and a fixed prior. In particular, these new bounds
reflect the structure of the encoder and are not vacuous for deterministic
algorithms. Our compressibility approach, which is information-theoretic in
nature, builds upon that of Blum-Langford for PAC-MDL bounds and introduces two
essential ingredients: block-coding and lossy-compression. The latter allows
our approach to subsume the so-called geometrical compressibility as a special
case. To the best knowledge of the authors, the established generalization
bounds are the first of their kind for Information Bottleneck (IB) type
encoders and representation learning. Finally, we partly exploit the
theoretical results by introducing a new data-dependent prior. Numerical
simulations illustrate the advantages of well-chosen such priors over classical
priors used in IB.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Tue,  6 Feb 24</h3>
<dl>
<dt><a name="item998">[998]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1704.01142" title="Abstract">arXiv:1704.01142</a> (replaced) [<a href="/pdf/1704.01142" title="Download PDF">pdf</a>, <a href="/ps/1704.01142" title="Download PostScript">ps</a>, <a href="/format/1704.01142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Structured Approach to the development of Solutions in Excel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bartholomew%2C+P">Peter Bartholomew</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures. This version updated email address
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the EuSpRIG 2016 Conference "Spreadsheet Risk
  Management" ISBN : 978-1-905404-53-7 pp 25-36
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item999">[999]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1711.01820" title="Abstract">arXiv:1711.01820</a> (replaced) [<a href="/pdf/1711.01820" title="Download PDF">pdf</a>, <a href="/format/1711.01820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Game Theoretic Semi-Distributed D2D Resource Allocation Underlaying an  LTE Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neogi%2C+A">Anushree Neogi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Computer Science and Game Theory (cs.GT); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1000">[1000]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1810.01864" title="Abstract">arXiv:1810.01864</a> (replaced) [<a href="/pdf/1810.01864" title="Download PDF">pdf</a>, <a href="/format/1810.01864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Agnostic Sample Compression Schemes for Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Attias%2C+I">Idan Attias</a>, 
<a href="/search/cs?searchtype=author&query=Hanneke%2C+S">Steve Hanneke</a>, 
<a href="/search/cs?searchtype=author&query=Kontorovich%2C+A">Aryeh Kontorovich</a>, 
<a href="/search/cs?searchtype=author&query=Sadigurschi%2C+M">Menachem Sadigurschi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> New results in this version: (1) Approximate agnostic sample compression scheme for function classes with finite fat-shattering dimension and the $\ell_p$ loss (section 3), (2) Near-optimal approximate compression for linear functions and the $\ell_p$ loss (section 4.1) The results in sections 4.2 and 4.3 appear in the previous version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1001">[1001]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1910.06002" title="Abstract">arXiv:1910.06002</a> (replaced) [<a href="/pdf/1910.06002" title="Download PDF">pdf</a>, <a href="/format/1910.06002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Clustering from Noisy Binary Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ariu%2C+K">Kaito Ariu</a>, 
<a href="/search/stat?searchtype=author&query=Ok%2C+J">Jungseul Ok</a>, 
<a href="/search/stat?searchtype=author&query=Proutiere%2C+A">Alexandre Proutiere</a>, 
<a href="/search/stat?searchtype=author&query=Yun%2C+S">Se-Young Yun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1002">[1002]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2001.07488" title="Abstract">arXiv:2001.07488</a> (replaced) [<a href="/pdf/2001.07488" title="Download PDF">pdf</a>, <a href="/format/2001.07488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Profunctor Optics, a Categorical Update
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Clarke%2C+B">Bryce Clarke</a>, 
<a href="/search/cs?searchtype=author&query=Elkins%2C+D">Derek Elkins</a>, 
<a href="/search/cs?searchtype=author&query=Gibbons%2C+J">Jeremy Gibbons</a>, 
<a href="/search/cs?searchtype=author&query=Loregian%2C+F">Fosco Loregian</a>, 
<a href="/search/cs?searchtype=author&query=Milewski%2C+B">Bartosz Milewski</a>, 
<a href="/search/cs?searchtype=author&query=Pillmore%2C+E">Emily Pillmore</a>, 
<a href="/search/cs?searchtype=author&query=Rom%C3%A1n%2C+M">Mario Rom&#xe1;n</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, Compositionality journal formatting
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Category Theory (math.CT)

</div>
</div>
</dd>
<dt><a name="item1003">[1003]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2003.01154" title="Abstract">arXiv:2003.01154</a> (replaced) [<a href="/pdf/2003.01154" title="Download PDF">pdf</a>, <a href="/format/2003.01154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient algorithms for the Potts model on small-set expanders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carlson%2C+C">Charles Carlson</a>, 
<a href="/search/cs?searchtype=author&query=Davies%2C+E">Ewan Davies</a>, 
<a href="/search/cs?searchtype=author&query=Kolla%2C+A">Alexandra Kolla</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item1004">[1004]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2006.14706" title="Abstract">arXiv:2006.14706</a> (replaced) [<a href="/pdf/2006.14706" title="Download PDF">pdf</a>, <a href="/ps/2006.14706" title="Download PostScript">ps</a>, <a href="/format/2006.14706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Will Dynamic Arrays finally change the way Models are built?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bartholomew%2C+P">Peter Bartholomew</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 Pages, 5 Figures, Numerous Spreadsheet Formulae. This version email address update
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the EuSpRIG 2019 Conference "Spreadsheet Risk
  Management", Browns, Covent Garden, London, pp149-160, ISBN:
  978-1-905404-56-8
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item1005">[1005]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.10268" title="Abstract">arXiv:2011.10268</a> (replaced) [<a href="/pdf/2011.10268" title="Download PDF">pdf</a>, <a href="/format/2011.10268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hyperparameter Optimization for AST Differencing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Martinez%2C+M">Matias Martinez</a>, 
<a href="/search/cs?searchtype=author&query=Falleri%2C+J">Jean-R&#xe9;my Falleri</a>, 
<a href="/search/cs?searchtype=author&query=Monperrus%2C+M">Martin Monperrus</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Software Engineering, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item1006">[1006]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.14872" title="Abstract">arXiv:2103.14872</a> (replaced) [<a href="/pdf/2103.14872" title="Download PDF">pdf</a>, <a href="/format/2103.14872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning Techniques for In-Crop Weed Identification: A Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+K">Kun Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhiyong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Coleman%2C+G">Guy Coleman</a>, 
<a href="/search/cs?searchtype=author&query=Bender%2C+A">Asher Bender</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+T">Tingting Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+S">Shan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+D">Dezhen Song</a>, 
<a href="/search/cs?searchtype=author&query=Schumann%2C+A">Arnold Schumann</a>, 
<a href="/search/cs?searchtype=author&query=Walsh%2C+M">Michael Walsh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1007">[1007]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.04310" title="Abstract">arXiv:2104.04310</a> (replaced) [<a href="/pdf/2104.04310" title="Download PDF">pdf</a>, <a href="/format/2104.04310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context-self contrastive pretraining for crop type semantic segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tarasiou%2C+M">Michail Tarasiou</a>, 
<a href="/search/cs?searchtype=author&query=Guler%2C+R+A">Riza Alp Guler</a>, 
<a href="/search/cs?searchtype=author&query=Zafeiriou%2C+S">Stefanos Zafeiriou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1008">[1008]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.04240" title="Abstract">arXiv:2105.04240</a> (replaced) [<a href="/pdf/2105.04240" title="Download PDF">pdf</a>, <a href="/format/2105.04240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A rigorous introduction to linear models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jun Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1009">[1009]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.07123" title="Abstract">arXiv:2105.07123</a> (replaced) [<a href="/pdf/2105.07123" title="Download PDF">pdf</a>, <a href="/ps/2105.07123" title="Download PostScript">ps</a>, <a href="/format/2105.07123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Byzantine-Resilient Population Protocols
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Busch%2C+C">Costas Busch</a>, 
<a href="/search/cs?searchtype=author&query=Kowalski%2C+D+R">Dariusz R. Kowalski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item1010">[1010]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.09254" title="Abstract">arXiv:2105.09254</a> (replaced) [<a href="/pdf/2105.09254" title="Download PDF">pdf</a>, <a href="/format/2105.09254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiply Robust Causal Mediation Analysis with Continuous Treatments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Sani%2C+N">Numair Sani</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+Y">Yizhen Xu</a>, 
<a href="/search/math?searchtype=author&query=Ghassami%2C+A">AmirEmad Ghassami</a>, 
<a href="/search/math?searchtype=author&query=Shpitser%2C+I">Ilya Shpitser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Econometrics (econ.EM); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1011">[1011]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.12699" title="Abstract">arXiv:2110.12699</a> (replaced) [<a href="/pdf/2110.12699" title="Download PDF">pdf</a>, <a href="/format/2110.12699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporal Team Semantics Revisited
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gutsfeld%2C+J+O">Jens Oliver Gutsfeld</a>, 
<a href="/search/cs?searchtype=author&query=Meier%2C+A">Arne Meier</a>, 
<a href="/search/cs?searchtype=author&query=Ohrem%2C+C">Christoph Ohrem</a>, 
<a href="/search/cs?searchtype=author&query=Virtema%2C+J">Jonni Virtema</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> extended version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item1012">[1012]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.15587" title="Abstract">arXiv:2110.15587</a> (replaced) [<a href="/pdf/2110.15587" title="Download PDF">pdf</a>, <a href="/ps/2110.15587" title="Download PostScript">ps</a>, <a href="/format/2110.15587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A sublinear query quantum algorithm for s-t minimum cut on dense simple  graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Apers%2C+S">Simon Apers</a>, 
<a href="/search/quant-ph?searchtype=author&query=Auza%2C+A">Arinta Auza</a>, 
<a href="/search/quant-ph?searchtype=author&query=Lee%2C+T">Troy Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The proof of the upper bound on the time complexity in the first arXiv version contained a fatal flaw. In this version we remove the claim about time complexity and prove the result only for query complexity
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item1013">[1013]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.04746" title="Abstract">arXiv:2111.04746</a> (replaced) [<a href="/pdf/2111.04746" title="Download PDF">pdf</a>, <a href="/format/2111.04746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Realizable Learning is All You Need
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hopkins%2C+M">Max Hopkins</a>, 
<a href="/search/cs?searchtype=author&query=Kane%2C+D+M">Daniel M. Kane</a>, 
<a href="/search/cs?searchtype=author&query=Lovett%2C+S">Shachar Lovett</a>, 
<a href="/search/cs?searchtype=author&query=Mahajan%2C+G">Gaurav Mahajan</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> TheoretiCS, Volume 3 (2024), Article 2, 1-62
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1014">[1014]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.13424" title="Abstract">arXiv:2112.13424</a> (replaced) [<a href="/pdf/2112.13424" title="Download PDF">pdf</a>, <a href="/ps/2112.13424" title="Download PostScript">ps</a>, <a href="/format/2112.13424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explicit non-special divisors of small degree, algebraic geometric  hulls, and LCD codes from Kummer extensions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Camps%2C+E">Eduardo Camps</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%B3pez%2C+H+H">Hiram H. L&#xf3;pez</a>, 
<a href="/search/cs?searchtype=author&query=Matthews%2C+G+L">Gretchen L. Matthews</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SIAM Journal on Applied Algebra and Geometry, to appear
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Algebraic Geometry (math.AG)

</div>
</div>
</dd>
<dt><a name="item1015">[1015]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.14968" title="Abstract">arXiv:2112.14968</a> (replaced) [<a href="/pdf/2112.14968" title="Download PDF">pdf</a>, <a href="/format/2112.14968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Generator with Auxiliary Branch for Improving GAN Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Seung Park</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+Y">Yong-Goo Shin</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE transactions on neural networks and learning systems 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1016">[1016]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.03482" title="Abstract">arXiv:2202.03482</a> (replaced) [<a href="/pdf/2202.03482" title="Download PDF">pdf</a>, <a href="/format/2202.03482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Navigating Neural Space: Revisiting Concept Activation Vectors to  Overcome Directional Divergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pahde%2C+F">Frederik Pahde</a>, 
<a href="/search/cs?searchtype=author&query=Dreyer%2C+M">Maximilian Dreyer</a>, 
<a href="/search/cs?searchtype=author&query=Weber%2C+L">Leander Weber</a>, 
<a href="/search/cs?searchtype=author&query=Weckbecker%2C+M">Moritz Weckbecker</a>, 
<a href="/search/cs?searchtype=author&query=Anders%2C+C+J">Christopher J. Anders</a>, 
<a href="/search/cs?searchtype=author&query=Wiegand%2C+T">Thomas Wiegand</a>, 
<a href="/search/cs?searchtype=author&query=Samek%2C+W">Wojciech Samek</a>, 
<a href="/search/cs?searchtype=author&query=Lapuschkin%2C+S">Sebastian Lapuschkin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1017">[1017]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.03844" title="Abstract">arXiv:2202.03844</a> (replaced) [<a href="/pdf/2202.03844" title="Download PDF">pdf</a>, <a href="/format/2202.03844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EvoPruneDeepTL: An Evolutionary Pruning Model for Transfer Learning  based Deep Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Poyatos%2C+J">Javier Poyatos</a>, 
<a href="/search/cs?searchtype=author&query=Molina%2C+D">Daniel Molina</a>, 
<a href="/search/cs?searchtype=author&query=Martinez%2C+A+D">Aritz. D. Martinez</a>, 
<a href="/search/cs?searchtype=author&query=Del+Ser%2C+J">Javier Del Ser</a>, 
<a href="/search/cs?searchtype=author&query=Herrera%2C+F">Francisco Herrera</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Neural Networks, 158, (2023), 59-82
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item1018">[1018]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.03603" title="Abstract">arXiv:2204.03603</a> (replaced) [<a href="/pdf/2204.03603" title="Download PDF">pdf</a>, <a href="/ps/2204.03603" title="Download PostScript">ps</a>, <a href="/format/2204.03603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algebraic Structure of the Weak Stage Order Conditions for Runge-Kutta  Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Biswas%2C+A">Abhijit Biswas</a>, 
<a href="/search/math?searchtype=author&query=Ketcheson%2C+D">David Ketcheson</a>, 
<a href="/search/math?searchtype=author&query=Seibold%2C+B">Benjamin Seibold</a>, 
<a href="/search/math?searchtype=author&query=Shirokoff%2C+D">David Shirokoff</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> SIAM Journal on Numerical Analysis Vol. 62, Iss. 1, Pg. 48-72
  (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1019">[1019]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.04510" title="Abstract">arXiv:2204.04510</a> (replaced) [<a href="/pdf/2204.04510" title="Download PDF">pdf</a>, <a href="/format/2204.04510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Translating Subgraphs to Nodes Makes Simple GNNs Strong and Efficient  for Subgraph Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Dongkwan Kim</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+A">Alice Oh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item1020">[1020]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.13070" title="Abstract">arXiv:2204.13070</a> (replaced) [<a href="/pdf/2204.13070" title="Download PDF">pdf</a>, <a href="/format/2204.13070" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Control for Cooperative Teams in Competitive Autonomous  Racing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thakkar%2C+R+S">Rishabh Saumil Thakkar</a>, 
<a href="/search/cs?searchtype=author&query=Samyal%2C+A+S">Aryaman Singh Samyal</a>, 
<a href="/search/cs?searchtype=author&query=Fridovich-Keil%2C+D">David Fridovich-Keil</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhe Xu</a>, 
<a href="/search/cs?searchtype=author&query=Topcu%2C+U">Ufuk Topcu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item1021">[1021]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.05248" title="Abstract">arXiv:2206.05248</a> (replaced) [<a href="/pdf/2206.05248" title="Download PDF">pdf</a>, <a href="/ps/2206.05248" title="Download PostScript">ps</a>, <a href="/format/2206.05248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerated Algorithms for Constrained Nonconvex-Nonconcave Min-Max  Optimization and Comonotone Inclusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cai%2C+Y">Yang Cai</a>, 
<a href="/search/math?searchtype=author&query=Oikonomou%2C+A">Argyris Oikonomou</a>, 
<a href="/search/math?searchtype=author&query=Zheng%2C+W">Weiqiang Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This version includes new results on point convergence
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1022">[1022]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.01327" title="Abstract">arXiv:2207.01327</a> (replaced) [<a href="/pdf/2207.01327" title="Download PDF">pdf</a>, <a href="/format/2207.01327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BoAT v2 -- A Web-Based Dependency Annotation Tool with Focus on  Agglutinative Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akkurt%2C+S+F">Salih Furkan Akkurt</a>, 
<a href="/search/cs?searchtype=author&query=Mar%C5%9Fan%2C+B">B&#xfc;&#x15f;ra Mar&#x15f;an</a>, 
<a href="/search/cs?searchtype=author&query=Uskudarli%2C+S">Susan Uskudarli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented in The International Conference and Workshop on Agglutinative Language Technologies as a challenge of Natural Language Processing (ALTNLP), June 7-8, 2022, Koper, Slovenia
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1023">[1023]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.02697" title="Abstract">arXiv:2207.02697</a> (replaced) [<a href="/pdf/2207.02697" title="Download PDF">pdf</a>, <a href="/format/2207.02697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Home-Space Problem for Petri Nets and its Ackermannian Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jan%C4%8Dar%2C+P">Petr Jan&#x10d;ar</a>, 
<a href="/search/cs?searchtype=author&query=Leroux%2C+J">J&#xe9;r&#xf4;me Leroux</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item1024">[1024]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.04764" title="Abstract">arXiv:2207.04764</a> (replaced) [<a href="/pdf/2207.04764" title="Download PDF">pdf</a>, <a href="/format/2207.04764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical modeling and open-source implementation of variational  partition-of-unity localizations of space-time dual-weighted residual  estimators for parabolic problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Thiele%2C+J+P">Jan Philipp Thiele</a>, 
<a href="/search/math?searchtype=author&query=Wick%2C+T">Thomas Wick</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Changes in v3: Polished errors, added some further scientific details
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1025">[1025]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.04923" title="Abstract">arXiv:2207.04923</a> (replaced) [<a href="/pdf/2207.04923" title="Download PDF">pdf</a>, <a href="/format/2207.04923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Killing a Vortex
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Thilikos%2C+D+M">Dimitrios M. Thilikos</a>, 
<a href="/search/math?searchtype=author&query=Wiederrecht%2C+S">Sebastian Wiederrecht</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> An earlier version of this paper has appeared at FOCS 2022 We also changed the term "vga-hierarchy" with the more appropriate term "vga-lattice". arXiv admin note: text overlap with <a href="/abs/2010.12397">arXiv:2010.12397</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item1026">[1026]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.10250" title="Abstract">arXiv:2207.10250</a> (replaced) [<a href="/pdf/2207.10250" title="Download PDF">pdf</a>, <a href="/format/2207.10250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Hardness Results for the Guided Local Hamiltonian Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Cade%2C+C">Chris Cade</a>, 
<a href="/search/quant-ph?searchtype=author&query=Folkertsma%2C+M">Marten Folkertsma</a>, 
<a href="/search/quant-ph?searchtype=author&query=Gharibian%2C+S">Sevag Gharibian</a>, 
<a href="/search/quant-ph?searchtype=author&query=Hayakawa%2C+R">Ryu Hayakawa</a>, 
<a href="/search/quant-ph?searchtype=author&query=Gall%2C+F+L">Fran&#xe7;ois Le Gall</a>, 
<a href="/search/quant-ph?searchtype=author&query=Morimae%2C+T">Tomoyuki Morimae</a>, 
<a href="/search/quant-ph?searchtype=author&query=Weggemans%2C+J">Jordi Weggemans</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages; v3: This article merges and supersedes <a href="/abs/2207.10097">arXiv:2207.10097</a> and the previous version of <a href="/abs/2207.10250">arXiv:2207.10250</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 50th EATCS International Colloquium on
  Automata, Languages and Programming (ICALP 2023), pp. 32:1-32.19, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item1027">[1027]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.12382" title="Abstract">arXiv:2207.12382</a> (replaced) [<a href="/pdf/2207.12382" title="Download PDF">pdf</a>, <a href="/format/2207.12382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Confidence Sequences for Bounded Random Processes via Universal  Gambling Strategies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ryu%2C+J+J">J. Jon Ryu</a>, 
<a href="/search/math?searchtype=author&query=Bhatt%2C+A">Alankrita Bhatt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Information Theory (cs.IT); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item1028">[1028]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.13842" title="Abstract">arXiv:2207.13842</a> (replaced) [<a href="/pdf/2207.13842" title="Download PDF">pdf</a>, <a href="/format/2207.13842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dive into Machine Learning Algorithms for Influenza Virus Host  Prediction with Hemagglutinin Sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yanhua Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wojtczak%2C+D">Dominik Wojtczak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at BioSystems; V1: minor typo correction; V2: minor typo correction and add more clarification in "Cross-validation" section
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1029">[1029]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.05553" title="Abstract">arXiv:2208.05553</a> (replaced) [<a href="/pdf/2208.05553" title="Download PDF">pdf</a>, <a href="/format/2208.05553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Neighborhood Interference with Low Order Interactions under  Unit Randomized Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cortez-Rodriguez%2C+M">Mayleen Cortez-Rodriguez</a>, 
<a href="/search/stat?searchtype=author&query=Eichhorn%2C+M">Matthew Eichhorn</a>, 
<a href="/search/stat?searchtype=author&query=Yu%2C+C+L">Christina Lee Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 42 pages including citations and appendix, 2 figures (total of 12 subfigures)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Causal Inference, vol. 11, no. 1, 2023, pp. 20220051
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item1030">[1030]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.09125" title="Abstract">arXiv:2208.09125</a> (replaced) [<a href="/pdf/2208.09125" title="Download PDF">pdf</a>, <a href="/ps/2208.09125" title="Download PostScript">ps</a>, <a href="/format/2208.09125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Open Radio Access Networks: Challenges, Research Directions,  and Open Source Approaches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azariah%2C+W">Wilfrid Azariah</a>, 
<a href="/search/cs?searchtype=author&query=Bimo%2C+F+A">Fransiscus Asisi Bimo</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chih-Wei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+R">Ray-Guang Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Nikaein%2C+N">Navid Nikaein</a>, 
<a href="/search/cs?searchtype=author&query=Jana%2C+R">Rittwik Jana</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted and to be published by MPDI Sensors, 2024
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Sensors 2024, 24, 1038
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item1031">[1031]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.13315" title="Abstract">arXiv:2208.13315</a> (replaced) [<a href="/pdf/2208.13315" title="Download PDF">pdf</a>, <a href="/format/2208.13315" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ANAct: Adaptive Normalization for Activation Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peiwen%2C+Y">Yuan Peiwen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Henan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Changsheng%2C+Z">Zhu Changsheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuyi Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1032">[1032]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.03275" title="Abstract">arXiv:2209.03275</a> (replaced) [<a href="/pdf/2209.03275" title="Download PDF">pdf</a>, <a href="/format/2209.03275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Speech Enhancement Using Burst Propagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raza%2C+M">Mohsin Raza</a>, 
<a href="/search/cs?searchtype=author&query=Passos%2C+L+A">Leandro A. Passos</a>, 
<a href="/search/cs?searchtype=author&query=Khubaib%2C+A">Ahmed Khubaib</a>, 
<a href="/search/cs?searchtype=author&query=Adeel%2C+A">Ahsan Adeel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1033">[1033]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.01283" title="Abstract">arXiv:2210.01283</a> (replaced) [<a href="/pdf/2210.01283" title="Download PDF">pdf</a>, <a href="/format/2210.01283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Persistent Homology Guided Monte-Carlo Tree Search for Effective  Non-Prehensile Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vieira%2C+E+R">Ewerton R. Vieira</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+K">Kai Gao</a>, 
<a href="/search/cs?searchtype=author&query=Nakhimovich%2C+D">Daniel Nakhimovich</a>, 
<a href="/search/cs?searchtype=author&query=Bekris%2C+K+E">Kostas E. Bekris</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jingjin Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1034">[1034]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.01595" title="Abstract">arXiv:2210.01595</a> (replaced) [<a href="/pdf/2210.01595" title="Download PDF">pdf</a>, <a href="/format/2210.01595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FreDSNet: Joint Monocular Depth and Semantic Segmentation with Fast  Fourier Convolutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berenguel-Baeta%2C+B">Bruno Berenguel-Baeta</a>, 
<a href="/search/cs?searchtype=author&query=Bermudez-Cameo%2C+J">Jesus Bermudez-Cameo</a>, 
<a href="/search/cs?searchtype=author&query=Guerrero%2C+J+J">Jose J. Guerrero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1035">[1035]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.11388" title="Abstract">arXiv:2210.11388</a> (replaced) [<a href="/pdf/2210.11388" title="Download PDF">pdf</a>, <a href="/ps/2210.11388" title="Download PostScript">ps</a>, <a href="/format/2210.11388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-informed Deep Diffusion MRI Reconstruction with Synthetic Data:  Break Training Data Bottleneck in Artificial Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Qian%2C+C">Chen Qian</a>, 
<a href="/search/eess?searchtype=author&query=Gao%2C+Y">Yuncheng Gao</a>, 
<a href="/search/eess?searchtype=author&query=Han%2C+M">Mingyang Han</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Zi Wang</a>, 
<a href="/search/eess?searchtype=author&query=Ruan%2C+D">Dan Ruan</a>, 
<a href="/search/eess?searchtype=author&query=Shen%2C+Y">Yu Shen</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+Y">Yaping Wu</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+Y">Yirong Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+C">Chengyan Wang</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+B">Boyu Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Tao%2C+R">Ran Tao</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+Z">Zhigang Wu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jiazheng Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+L">Liuhong Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+Y">Yi Guo</a>, 
<a href="/search/eess?searchtype=author&query=Kang%2C+T">Taishan Kang</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+J">Jianzhong Lin</a>, 
<a href="/search/eess?searchtype=author&query=Gong%2C+T">Tao Gong</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+C">Chen Yang</a>, 
<a href="/search/eess?searchtype=author&query=Fei%2C+G">Guoqiang Fei</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+M">Meijin Lin</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+D">Di Guo</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+J">Jianjun Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+M">Meiyun Wang</a>, 
<a href="/search/eess?searchtype=author&query=Qu%2C+X">Xiaobo Qu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1036">[1036]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.04684" title="Abstract">arXiv:2211.04684</a> (replaced) [<a href="/pdf/2211.04684" title="Download PDF">pdf</a>, <a href="/format/2211.04684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Few-Shot Character Understanding in Movies as an Assessment to  Meta-Learning of Theory-of-Mind
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+M">Mo Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qiujing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shunchi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sang%2C+Y">Yisi Sang</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+K">Kangsheng Pu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zekai Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Han Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Liyan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jing Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1037">[1037]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.05006" title="Abstract">arXiv:2211.05006</a> (replaced) [<a href="/pdf/2211.05006" title="Download PDF">pdf</a>, <a href="/format/2211.05006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Almost Tight Error Bounds on Differentially Private Continual Counting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Henzinger%2C+M">Monika Henzinger</a>, 
<a href="/search/cs?searchtype=author&query=Upadhyay%2C+J">Jalaj Upadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Upadhyay%2C+S">Sarvagya Upadhyay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated the citations to include two papers we learned about since version 01
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item1038">[1038]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.10636" title="Abstract">arXiv:2211.10636</a> (replaced) [<a href="/pdf/2211.10636" title="Download PDF">pdf</a>, <a href="/format/2211.10636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EVEREST: Efficient Masked Video Autoencoder by Removing Redundant  Spatiotemporal Tokens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S">Sunil Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+J">Jaehong Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Youngwan Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S+J">Sung Ju Hwang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1039">[1039]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.11419" title="Abstract">arXiv:2211.11419</a> (replaced) [<a href="/pdf/2211.11419" title="Download PDF">pdf</a>, <a href="/format/2211.11419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SSCFormer: Push the Limit of Chunk-wise Conformer for Streaming ASR  Using Sequentially Sampled Chunks and Chunked Causal Convolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fangyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Bo Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Bo Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This manuscript has been accepted by SPL
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1040">[1040]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.12104" title="Abstract">arXiv:2211.12104</a> (replaced) [<a href="/pdf/2211.12104" title="Download PDF">pdf</a>, <a href="/ps/2211.12104" title="Download PostScript">ps</a>, <a href="/format/2211.12104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy Consumption of Automated Program Repair
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Martinez%2C+M">Matias Martinez</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez-Fern%C3%A1ndez%2C+S">Silverio Mart&#xed;nez-Fern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Franch%2C+X">Xavier Franch</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2024 IEEE/ACM 46th International Conference on Software
  Engineering: Companion Proceedings (ICSE-Companion '24), April 14--20, 2024,
  Lisbon, Portugal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item1041">[1041]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.15335" title="Abstract">arXiv:2211.15335</a> (replaced) [<a href="/pdf/2211.15335" title="Download PDF">pdf</a>, <a href="/format/2211.15335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> You Can Have Better Graph Neural Networks by Not Training Weights at  All: Finding Untrained GNNs Tickets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tianjin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianlong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+M">Meng Fang</a>, 
<a href="/search/cs?searchtype=author&query=Menkovski%2C+V">Vlado Menkovski</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jiaxu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+L">Lu Yin</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+Y">Yulong Pei</a>, 
<a href="/search/cs?searchtype=author&query=Mocanu%2C+D+C">Decebal Constantin Mocanu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pechenizkiy%2C+M">Mykola Pechenizkiy</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shiwei Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the LoG conference 2022 as a spotlight
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> LoG 2022 (Oral &amp; Best Paper Award)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1042">[1042]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.06808" title="Abstract">arXiv:2212.06808</a> (replaced) [<a href="/pdf/2212.06808" title="Download PDF">pdf</a>, <a href="/format/2212.06808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incentive-Aware Models of Financial Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jalan%2C+A">Akhil Jalan</a>, 
<a href="/search/math?searchtype=author&query=Chakrabarti%2C+D">Deepayan Chakrabarti</a>, 
<a href="/search/math?searchtype=author&query=Sarkar%2C+P">Purnamrita Sarkar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item1043">[1043]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.08515" title="Abstract">arXiv:2212.08515</a> (replaced) [<a href="/pdf/2212.08515" title="Download PDF">pdf</a>, <a href="/format/2212.08515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Formal Theory of Monads, Univalently
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+der+Weide%2C+N">Niels van der Weide</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Category Theory (math.CT)

</div>
</div>
</dd>
<dt><a name="item1044">[1044]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.08639" title="Abstract">arXiv:2212.08639</a> (replaced) [<a href="/pdf/2212.08639" title="Download PDF">pdf</a>, <a href="/format/2212.08639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An annotated instance segmentation XXL-CT data-set from a historic  airplane
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gruber%2C+R">Roland Gruber</a>, 
<a href="/search/cs?searchtype=author&query=Reims%2C+N">Nils Reims</a>, 
<a href="/search/cs?searchtype=author&query=Hempfer%2C+A">Andreas Hempfer</a>, 
<a href="/search/cs?searchtype=author&query=Gerth%2C+S">Stefan Gerth</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%B6hnel%2C+M">Michael B&#xf6;hnel</a>, 
<a href="/search/cs?searchtype=author&query=Fuchs%2C+T">Theobald Fuchs</a>, 
<a href="/search/cs?searchtype=author&query=Salamon%2C+M">Michael Salamon</a>, 
<a href="/search/cs?searchtype=author&query=Wittenberg%2C+T">Thomas Wittenberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1045">[1045]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10923" title="Abstract">arXiv:2212.10923</a> (replaced) [<a href="/pdf/2212.10923" title="Download PDF">pdf</a>, <a href="/format/2212.10923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Models as Inductive Reasoners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zonglin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+L">Li Dong</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+X">Xinya Du</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Cambria%2C+E">Erik Cambria</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaodong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1046">[1046]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.02996" title="Abstract">arXiv:2301.02996</a> (replaced) [<a href="/pdf/2301.02996" title="Download PDF">pdf</a>, <a href="/format/2301.02996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A note on the rate of convergence of integration schemes for closed  surfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zavalani%2C+G">Gentian Zavalani</a>, 
<a href="/search/math?searchtype=author&query=Shehu%2C+E">Elima Shehu</a>, 
<a href="/search/math?searchtype=author&query=Hecht%2C+M">Michael Hecht</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1047">[1047]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11117" title="Abstract">arXiv:2301.11117</a> (replaced) [<a href="/pdf/2301.11117" title="Download PDF">pdf</a>, <a href="/format/2301.11117" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthesizing Specifications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+K">Kanghee Park</a>, 
<a href="/search/cs?searchtype=author&query=D%27Antoni%2C+L">Loris D&#x27;Antoni</a>, 
<a href="/search/cs?searchtype=author&query=Reps%2C+T">Thomas Reps</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
</div>
</dd>
<dt><a name="item1048">[1048]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.12407" title="Abstract">arXiv:2301.12407</a> (replaced) [<a href="/pdf/2301.12407" title="Download PDF">pdf</a>, <a href="/format/2301.12407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedEBA+: Towards Fair and Effective Federated Learning via Entropy-Based  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhichao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Karimireddy%2C+S+P">Sai Praneeth Karimireddy</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiaoying Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1049">[1049]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.13336" title="Abstract">arXiv:2301.13336</a> (replaced) [<a href="/pdf/2301.13336" title="Download PDF">pdf</a>, <a href="/format/2301.13336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Fair Value of Data Under Heterogeneous Privacy Constraints in  Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Justin Kang</a>, 
<a href="/search/cs?searchtype=author&query=Pedarsani%2C+R">Ramtin Pedarsani</a>, 
<a href="/search/cs?searchtype=author&query=Ramchandran%2C+K">Kannan Ramchandran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 5 figures, Accepted to TMLR
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item1050">[1050]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00253" title="Abstract">arXiv:2302.00253</a> (replaced) [<a href="/pdf/2302.00253" title="Download PDF">pdf</a>, <a href="/ps/2302.00253" title="Download PostScript">ps</a>, <a href="/format/2302.00253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Attractor of the Replicator Dynamic in Zero-Sum Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Biggar%2C+O">Oliver Biggar</a>, 
<a href="/search/cs?searchtype=author&query=Shames%2C+I">Iman Shames</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 2 figures. Accepted version, to appear at ALT 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item1051">[1051]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02450" title="Abstract">arXiv:2302.02450</a> (replaced) [<a href="/pdf/2302.02450" title="Download PDF">pdf</a>, <a href="/format/2302.02450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regularization and Optimization in Model-Based Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sampaio%2C+R+A">Raphael Araujo Sampaio</a>, 
<a href="/search/cs?searchtype=author&query=Garcia%2C+J+D">Joaquim Dias Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Poggi%2C+M">Marcus Poggi</a>, 
<a href="/search/cs?searchtype=author&query=Vidal%2C+T">Thibaut Vidal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1052">[1052]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03596" title="Abstract">arXiv:2302.03596</a> (replaced) [<a href="/pdf/2302.03596" title="Download PDF">pdf</a>, <a href="/format/2302.03596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Generation with Diffusion Mixture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jo%2C+J">Jaehyeong Jo</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Dongki Kim</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S+J">Sung Ju Hwang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1053">[1053]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04313" title="Abstract">arXiv:2302.04313</a> (replaced) [<a href="/pdf/2302.04313" title="Download PDF">pdf</a>, <a href="/format/2302.04313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geometry-Complete Diffusion for 3D Molecule Generation and Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morehead%2C+A">Alex Morehead</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">Jianlin Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 5 figures, 5 tables. Under review. Also presented at ICLR 2023's MLDD workshop. Code available at <a href="https://github.com/BioinfoMachineLearning/Bio-Diffusion">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Biomolecules (q-bio.BM); Quantitative Methods (q-bio.QM); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1054">[1054]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.05737" title="Abstract">arXiv:2302.05737</a> (replaced) [<a href="/pdf/2302.05737" title="Download PDF">pdf</a>, <a href="/format/2302.05737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Reparameterized Discrete Diffusion Model for Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Lin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jianbo Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingpeng Kong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code available at <a href="https://github.com/hkunlp/reparam-discrete-diffusion">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1055">[1055]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.05793" title="Abstract">arXiv:2302.05793</a> (replaced) [<a href="/pdf/2302.05793" title="Download PDF">pdf</a>, <a href="/format/2302.05793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributional GFlowNets with Quantile Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dinghuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Ling Pan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R+T+Q">Ricky T. Q. Chen</a>, 
<a href="/search/cs?searchtype=author&query=Courville%2C+A">Aaron Courville</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by TMLR
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation (stat.CO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1056">[1056]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06279" title="Abstract">arXiv:2302.06279</a> (replaced) [<a href="/pdf/2302.06279" title="Download PDF">pdf</a>, <a href="/format/2302.06279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural  Networks with Neuromorphic Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abad%2C+G">Gorka Abad</a>, 
<a href="/search/cs?searchtype=author&query=Ersoy%2C+O">Oguzhan Ersoy</a>, 
<a href="/search/cs?searchtype=author&query=Picek%2C+S">Stjepan Picek</a>, 
<a href="/search/cs?searchtype=author&query=Urbieta%2C+A">Aitor Urbieta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in Network and Distributed System Security (NDSS) Symposium 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1057">[1057]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08263" title="Abstract">arXiv:2302.08263</a> (replaced) [<a href="/pdf/2302.08263" title="Download PDF">pdf</a>, <a href="/format/2302.08263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta-Auto-Decoder: A Meta-Learning Based Reduced Order Model for Solving  Parametric Partial Differential Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ye%2C+Z">Zhanhong Ye</a>, 
<a href="/search/math?searchtype=author&query=Huang%2C+X">Xiang Huang</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+H">Hongsheng Liu</a>, 
<a href="/search/math?searchtype=author&query=Dong%2C+B">Bin Dong</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Communications on Applied Mathematics and Computation (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1058">[1058]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.09167" title="Abstract">arXiv:2302.09167</a> (replaced) [<a href="/pdf/2302.09167" title="Download PDF">pdf</a>, <a href="/format/2302.09167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixed Traffic Control and Coordination from Pixels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Villarreal%2C+M">Michael Villarreal</a>, 
<a href="/search/cs?searchtype=author&query=Poudel%2C+B">Bibek Poudel</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Jia Pan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weizi Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE International Conference on Robotics and Automation (ICRA), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1059">[1059]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.09168" title="Abstract">arXiv:2302.09168</a> (replaced) [<a href="/pdf/2302.09168" title="Download PDF">pdf</a>, <a href="/ps/2302.09168" title="Download PostScript">ps</a>, <a href="/format/2302.09168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Screening Signal-Manipulating Agents via Contests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Li%2C+Y">Yingkai Li</a>, 
<a href="/search/econ?searchtype=author&query=Qiu%2C+X">Xiaoyun Qiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item1060">[1060]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.10253" title="Abstract">arXiv:2302.10253</a> (replaced) [<a href="/pdf/2302.10253" title="Download PDF">pdf</a>, <a href="/format/2302.10253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiobjective Evolutionary Pruning of Deep Neural Networks with  Transfer Learning for improving their Performance and Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Poyatos%2C+J">Javier Poyatos</a>, 
<a href="/search/cs?searchtype=author&query=Molina%2C+D">Daniel Molina</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez%2C+A">Aitor Mart&#xed;nez</a>, 
<a href="/search/cs?searchtype=author&query=Del+Ser%2C+J">Javier Del Ser</a>, 
<a href="/search/cs?searchtype=author&query=Herrera%2C+F">Francisco Herrera</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 11 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Applied Soft Computing, 147 (2023), 110757
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1061">[1061]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01549" title="Abstract">arXiv:2303.01549</a> (replaced) [<a href="/pdf/2303.01549" title="Download PDF">pdf</a>, <a href="/format/2303.01549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-Tuned Convex Approximations of Probabilistic Reachable Sets under  Data-driven Uncertainties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Pengcheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Martinez%2C+S">Sonia Martinez</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jun Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1062">[1062]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01704" title="Abstract">arXiv:2303.01704</a> (replaced) [<a href="/pdf/2303.01704" title="Download PDF">pdf</a>, <a href="/format/2303.01704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feature Importance Disparities for Data Bias Investigations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+P+W">Peter W. Chang</a>, 
<a href="/search/cs?searchtype=author&query=Fishman%2C+L">Leor Fishman</a>, 
<a href="/search/cs?searchtype=author&query=Neel%2C+S">Seth Neel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures. Appendix: 15 pages, 12 figures. Edits: Restructuring of paper with more emphasis on usefulness as a hypothesis generator for biased subgroups and features. Additional experiment tying our work to existing fairness work by observing fairness metrics of our discovered subgroups and measuring the FID of subgroups found using prior biased subgroup discovery methods
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1063">[1063]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06070" title="Abstract">arXiv:2303.06070</a> (replaced) [<a href="/pdf/2303.06070" title="Download PDF">pdf</a>, <a href="/ps/2303.06070" title="Download PostScript">ps</a>, <a href="/format/2303.06070" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thinness and its variations on some graph families and coloring graphs  of bounded thinness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bonomo-Braberman%2C+F">Flavia Bonomo-Braberman</a>, 
<a href="/search/math?searchtype=author&query=Brandwein%2C+E">Eric Brandwein</a>, 
<a href="/search/math?searchtype=author&query=Oliveira%2C+F+S">Fabiano S. Oliveira</a>, 
<a href="/search/math?searchtype=author&query=Sampaio%2C+M+S">Moys&#xe9;s S. Sampaio Jr.</a>, 
<a href="/search/math?searchtype=author&query=Sansone%2C+A">Agustin Sansone</a>, 
<a href="/search/math?searchtype=author&query=Szwarcfiter%2C+J+L">Jayme L. Szwarcfiter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item1064">[1064]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.06943" title="Abstract">arXiv:2303.06943</a> (replaced) [<a href="/pdf/2303.06943" title="Download PDF">pdf</a>, <a href="/format/2303.06943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The joint bidiagonalization of a matrix pair with inaccurate inner  iterations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+H">Haibo Li</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> SIAM Journal on Matrix Analysis and Applications, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1065">[1065]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.07106" title="Abstract">arXiv:2303.07106</a> (replaced) [<a href="/pdf/2303.07106" title="Download PDF">pdf</a>, <a href="/format/2303.07106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design, Control, and Motion Strategy of TRADY: Tilted-Rotor-Equipped  Aerial Robot With Autonomous In-Flight Assembly and Disassembly Ability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sugihara%2C+J">Junichiro Sugihara</a>, 
<a href="/search/cs?searchtype=author&query=Nishio%2C+T">Takuzumi Nishio</a>, 
<a href="/search/cs?searchtype=author&query=Nagato%2C+K">Keisuke Nagato</a>, 
<a href="/search/cs?searchtype=author&query=Nakao%2C+M">Masayuki Nakao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+M">Moju Zhao</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Adv. Intell. Syst. 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1066">[1066]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.07651" title="Abstract">arXiv:2303.07651</a> (replaced) [<a href="/pdf/2303.07651" title="Download PDF">pdf</a>, <a href="/format/2303.07651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context Normalization Layer with Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Faye%2C+B">Bilal Faye</a>, 
<a href="/search/cs?searchtype=author&query=Dilmi%2C+M">Mohamed-Djallel Dilmi</a>, 
<a href="/search/cs?searchtype=author&query=Azzag%2C+H">Hanane Azzag</a>, 
<a href="/search/cs?searchtype=author&query=Lebbah%2C+M">Mustapha Lebbah</a>, 
<a href="/search/cs?searchtype=author&query=Bouchaffra%2C+D">Djamel Bouchaffra</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Fifth-icdm-workshop-dlc-2023 Fifth-icdm-workshop-dlc-2023
  Fifth-icdm-workshop-dlc-2023
  https://sites.google.com/view/fifth-icdm-workshop-dlc-2023/home
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1067">[1067]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.07885" title="Abstract">arXiv:2303.07885</a> (replaced) [<a href="/pdf/2303.07885" title="Download PDF">pdf</a>, <a href="/format/2303.07885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Role Assignment for Multiplayer Reach-Avoid Differential Games  in 3D Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Agasti%2C+A">Abinash Agasti</a>, 
<a href="/search/math?searchtype=author&query=Reddy%2C+P+V">Puduru Viswanadha Reddy</a>, 
<a href="/search/math?searchtype=author&query=Bhikkaji%2C+B">Bharath Bhikkaji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1068">[1068]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10181" title="Abstract">arXiv:2303.10181</a> (replaced) [<a href="/pdf/2303.10181" title="Download PDF">pdf</a>, <a href="/format/2303.10181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Operating critical machine learning models in resource constrained  regimes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Selvan%2C+R">Raghavendra Selvan</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6n%2C+J">Julian Sch&#xf6;n</a>, 
<a href="/search/cs?searchtype=author&query=Dam%2C+E+B">Erik B Dam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Resource Efficient Medical Image Analysis workshop at MICCAI-2023. Source code available at <a href="https://github.com/raghavian/redl">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Lecture Notes Comp. Sci.14394 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1069">[1069]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10383" title="Abstract">arXiv:2303.10383</a> (replaced) [<a href="/pdf/2303.10383" title="Download PDF">pdf</a>, <a href="/format/2303.10383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Multi-source Predictor for Zero-shot Video Object Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiaoqi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+S">Shijie Chang</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+Y">Youwei Pang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jiaxing Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lihe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Huchuan Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IJCV 2024. Code is available at: <a href="https://github.com/Xiaoqi-Zhao-DLUT/Multi-Source-APS-ZVOS.">this https URL</a> arXiv admin note: substantial text overlap with <a href="/abs/2108.05076">arXiv:2108.05076</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1070">[1070]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.11831" title="Abstract">arXiv:2303.11831</a> (replaced) [<a href="/pdf/2303.11831" title="Download PDF">pdf</a>, <a href="/format/2303.11831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLADE: Cycle Loss Augmented Degradation Enhancement for Unpaired  Super-Resolution of Anisotropic Medical Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pascale%2C+M">Michele Pascale</a>, 
<a href="/search/cs?searchtype=author&query=Muthurangu%2C+V">Vivek Muthurangu</a>, 
<a href="/search/cs?searchtype=author&query=Tordera%2C+J+M">Javier Montalt Tordera</a>, 
<a href="/search/cs?searchtype=author&query=Fitzke%2C+H+E">Heather E Fitzke</a>, 
<a href="/search/cs?searchtype=author&query=Bhatnagar%2C+G">Gauraang Bhatnagar</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+S">Stuart Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Steeden%2C+J">Jennifer Steeden</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV); Medical Physics (physics.med-ph)

</div>
</div>
</dd>
<dt><a name="item1071">[1071]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12678" title="Abstract">arXiv:2303.12678</a> (replaced) [<a href="/pdf/2303.12678" title="Download PDF">pdf</a>, <a href="/format/2303.12678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uni-Fusion: Universal Continuous Mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yijun Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Nuechter%2C+A">Andreas Nuechter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published on IEEE Transactions on Robotics. Project page: <a href="https://jarrome.github.io/Uni-Fusion/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1072">[1072]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13077" title="Abstract">arXiv:2303.13077</a> (replaced) [<a href="/pdf/2303.13077" title="Download PDF">pdf</a>, <a href="/format/2303.13077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Efficient Knowledge Transfer Strategy for Spiking Neural Networks  from Static to Event Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiang He</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dongcheng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+G">Guobin Shen</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+Q">Qingqun Kong</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Yi Zeng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2024 (Oral). Code is available at <a href="https://github.com/Brain-Cog-Lab/Transfer-for-DVS">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1073">[1073]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00206" title="Abstract">arXiv:2304.00206</a> (replaced) [<a href="/pdf/2304.00206" title="Download PDF">pdf</a>, <a href="/format/2304.00206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MonoPIC -- A Monocular Low-Latency Pedestrian Intention Classification  Framework for IoT Edges Using ID3 Modelled Decision Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Radhakrishna%2C+S">Sriram Radhakrishna</a>, 
<a href="/search/cs?searchtype=author&query=Balasubramanyam%2C+A">Adithya Balasubramanyam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1074">[1074]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01285" title="Abstract">arXiv:2304.01285</a> (replaced) [<a href="/pdf/2304.01285" title="Download PDF">pdf</a>, <a href="/format/2304.01285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> X-TIME: An in-memory engine for accelerating machine learning on tabular  data with CAMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pedretti%2C+G">Giacomo Pedretti</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+J">John Moon</a>, 
<a href="/search/cs?searchtype=author&query=Bruel%2C+P">Pedro Bruel</a>, 
<a href="/search/cs?searchtype=author&query=Serebryakov%2C+S">Sergey Serebryakov</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+R+M">Ron M. Roth</a>, 
<a href="/search/cs?searchtype=author&query=Buonanno%2C+L">Luca Buonanno</a>, 
<a href="/search/cs?searchtype=author&query=Gajjar%2C+A">Archit Gajjar</a>, 
<a href="/search/cs?searchtype=author&query=Ziegler%2C+T">Tobias Ziegler</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Cong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Foltin%2C+M">Martin Foltin</a>, 
<a href="/search/cs?searchtype=author&query=Faraboschi%2C+P">Paolo Faraboschi</a>, 
<a href="/search/cs?searchtype=author&query=Ignowski%2C+J">Jim Ignowski</a>, 
<a href="/search/cs?searchtype=author&query=Graves%2C+C+E">Catherine E. Graves</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1075">[1075]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01624" title="Abstract">arXiv:2304.01624</a> (replaced) [<a href="/pdf/2304.01624" title="Download PDF">pdf</a>, <a href="/format/2304.01624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On a family of low-rank algorithms for large-scale algebraic Riccati  equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bertram%2C+C">Christian Bertram</a>, 
<a href="/search/math?searchtype=author&query=Fa%C3%9Fbender%2C+H">Heike Fa&#xdf;bender</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1076">[1076]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01904" title="Abstract">arXiv:2304.01904</a> (replaced) [<a href="/pdf/2304.01904" title="Download PDF">pdf</a>, <a href="/format/2304.01904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> REFINER: Reasoning Feedback on Intermediate Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paul%2C+D">Debjit Paul</a>, 
<a href="/search/cs?searchtype=author&query=Ismayilzada%2C+M">Mete Ismayilzada</a>, 
<a href="/search/cs?searchtype=author&query=Peyrard%2C+M">Maxime Peyrard</a>, 
<a href="/search/cs?searchtype=author&query=Borges%2C+B">Beatriz Borges</a>, 
<a href="/search/cs?searchtype=author&query=Bosselut%2C+A">Antoine Bosselut</a>, 
<a href="/search/cs?searchtype=author&query=West%2C+R">Robert West</a>, 
<a href="/search/cs?searchtype=author&query=Faltings%2C+B">Boi Faltings</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1077">[1077]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.05622" title="Abstract">arXiv:2304.05622</a> (replaced) [<a href="/pdf/2304.05622" title="Download PDF">pdf</a>, <a href="/format/2304.05622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAMM (Segment Any Medical Model): A 3D Slicer Integration to SAM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yihao Liu</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+J">Jiaming Zhang</a>, 
<a href="/search/eess?searchtype=author&query=She%2C+Z">Zhangcong She</a>, 
<a href="/search/eess?searchtype=author&query=Kheradmand%2C+A">Amir Kheradmand</a>, 
<a href="/search/eess?searchtype=author&query=Armand%2C+M">Mehran Armand</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures. We added editorial changes in the text
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1078">[1078]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07944" title="Abstract">arXiv:2304.07944</a> (replaced) [<a href="/pdf/2304.07944" title="Download PDF">pdf</a>, <a href="/format/2304.07944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An In-depth Investigation of User Response Simulation for Conversational  Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhenduo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhichao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ai%2C+Q">Qingyao Ai</a>, 
<a href="/search/cs?searchtype=author&query=Srikumar%2C+V">Vivek Srikumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in The Web Conference 2024, 8 pages with Appendices
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item1079">[1079]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08172" title="Abstract">arXiv:2304.08172</a> (replaced) [<a href="/pdf/2304.08172" title="Download PDF">pdf</a>, <a href="/ps/2304.08172" title="Download PostScript">ps</a>, <a href="/format/2304.08172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pointwise convergence of Fourier series and deep neural network for the  indicator function of d-dimensional ball
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kawasumi%2C+R">Ryota Kawasumi</a>, 
<a href="/search/cs?searchtype=author&query=Yoneda%2C+T">Tsuyoshi Yoneda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> When the version 2 was rejected (where I submitted it to an AI journal), I realized I needed to further clarify the key point, and also realized the field is rather Fourier analysis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Analysis of PDEs (math.AP)

</div>
</div>
</dd>
<dt><a name="item1080">[1080]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09067" title="Abstract">arXiv:2304.09067</a> (replaced) [<a href="/pdf/2304.09067" title="Download PDF">pdf</a>, <a href="/format/2304.09067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance of GAN-based augmentation for deep learning COVID-19 image  classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fedoruk%2C+O">Oleksandr Fedoruk</a>, 
<a href="/search/eess?searchtype=author&query=Klimaszewski%2C+K">Konrad Klimaszewski</a>, 
<a href="/search/eess?searchtype=author&query=Ogonowski%2C+A">Aleksander Ogonowski</a>, 
<a href="/search/eess?searchtype=author&query=Mo%C5%BCd%C5%BConek%2C+R">Rafa&#x142; Mo&#x17c;d&#x17c;onek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in prceedings of WMLQ2022 International Workshop on Machine Learning and Quantum Computing Applications in Medicine and Physics. Version updated after editorial review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Medical Physics (physics.med-ph)

</div>
</div>
</dd>
<dt><a name="item1081">[1081]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09571" title="Abstract">arXiv:2304.09571</a> (replaced) [<a href="/pdf/2304.09571" title="Download PDF">pdf</a>, <a href="/format/2304.09571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLIC: Large Receptive Field Transform Coding with Adaptive Weights for  Learned Image Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+P">Peirong Ning</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jiayu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+Y">Yongqi Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+F">Feng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ronggang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Fix typos
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item1082">[1082]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.11082" title="Abstract">arXiv:2304.11082</a> (replaced) [<a href="/pdf/2304.11082" title="Download PDF">pdf</a>, <a href="/format/2304.11082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fundamental Limitations of Alignment in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wolf%2C+Y">Yotam Wolf</a>, 
<a href="/search/cs?searchtype=author&query=Wies%2C+N">Noam Wies</a>, 
<a href="/search/cs?searchtype=author&query=Avnery%2C+O">Oshri Avnery</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+Y">Yoav Levine</a>, 
<a href="/search/cs?searchtype=author&query=Shashua%2C+A">Amnon Shashua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1083">[1083]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12290" title="Abstract">arXiv:2304.12290</a> (replaced) [<a href="/pdf/2304.12290" title="Download PDF">pdf</a>, <a href="/format/2304.12290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Message Detection and Channel Estimation for Unsourced Random  Access in Cell-Free User-Centric Wireless Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C3%87akmak%2C+B">Burak &#xc7;akmak</a>, 
<a href="/search/cs?searchtype=author&query=Gkiouzepi%2C+E">Eleni Gkiouzepi</a>, 
<a href="/search/cs?searchtype=author&query=Opper%2C+M">Manfred Opper</a>, 
<a href="/search/cs?searchtype=author&query=Caire%2C+G">Giuseppe Caire</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 45 pages, 9 figures, submitted to the IEEE Transactions on Information Theory
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item1084">[1084]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.12770" title="Abstract">arXiv:2304.12770</a> (replaced) [<a href="/pdf/2304.12770" title="Download PDF">pdf</a>, <a href="/format/2304.12770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controlling Posterior Collapse by an Inverse Lipschitz Constraint on the  Decoder Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kinoshita%2C+Y">Yuri Kinoshita</a>, 
<a href="/search/cs?searchtype=author&query=Oono%2C+K">Kenta Oono</a>, 
<a href="/search/cs?searchtype=author&query=Fukumizu%2C+K">Kenji Fukumizu</a>, 
<a href="/search/cs?searchtype=author&query=Yoshida%2C+Y">Yuichi Yoshida</a>, 
<a href="/search/cs?searchtype=author&query=Maeda%2C+S">Shin-ichi Maeda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to ICML 2023, some notations adjusted from the submitted version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1085">[1085]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.13861" title="Abstract">arXiv:2304.13861</a> (replaced) [<a href="/pdf/2304.13861" title="Download PDF">pdf</a>, <a href="/format/2304.13861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Parrot Dilemma: Human-Labeled vs. LLM-augmented Data in  Classification Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%B8ller%2C+A+G">Anders Giovanni M&#xf8;ller</a>, 
<a href="/search/cs?searchtype=author&query=Dalsgaard%2C+J+A">Jacob Aarup Dalsgaard</a>, 
<a href="/search/cs?searchtype=author&query=Pera%2C+A">Arianna Pera</a>, 
<a href="/search/cs?searchtype=author&query=Aiello%2C+L+M">Luca Maria Aiello</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EACL 2024. 14 pages, 4 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item1086">[1086]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14065" title="Abstract">arXiv:2304.14065</a> (replaced) [<a href="/pdf/2304.14065" title="Download PDF">pdf</a>, <a href="/format/2304.14065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lightweight, Pre-trained Transformers for Remote Sensing Timeseries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tseng%2C+G">Gabriel Tseng</a>, 
<a href="/search/cs?searchtype=author&query=Cartuyvels%2C+R">Ruben Cartuyvels</a>, 
<a href="/search/cs?searchtype=author&query=Zvonkov%2C+I">Ivan Zvonkov</a>, 
<a href="/search/cs?searchtype=author&query=Purohit%2C+M">Mirali Purohit</a>, 
<a href="/search/cs?searchtype=author&query=Rolnick%2C+D">David Rolnick</a>, 
<a href="/search/cs?searchtype=author&query=Kerner%2C+H">Hannah Kerner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1087">[1087]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14922" title="Abstract">arXiv:2304.14922</a> (replaced) [<a href="/pdf/2304.14922" title="Download PDF">pdf</a>, <a href="/format/2304.14922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supervised and Unsupervised Deep Learning Approaches for EEG Seizure  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Georgis-Yap%2C+Z">Zakary Georgis-Yap</a>, 
<a href="/search/eess?searchtype=author&query=Popovic%2C+M+R">Milos R. Popovic</a>, 
<a href="/search/eess?searchtype=author&query=Khan%2C+S+S">Shehroz S. Khan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 figures, 9 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Health Informatics Research, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1088">[1088]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.02614" title="Abstract">arXiv:2305.02614</a> (replaced) [<a href="/pdf/2305.02614" title="Download PDF">pdf</a>, <a href="/format/2305.02614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-Dimensional Bayesian Optimization via Semi-Supervised Learning with  Optimized Unlabeled Data Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yuxuan Yin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1089">[1089]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03614" title="Abstract">arXiv:2305.03614</a> (replaced) [<a href="/pdf/2305.03614" title="Download PDF">pdf</a>, <a href="/format/2305.03614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising-Diffusion Alignment for Continuous Sign Language Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Leming Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+W">Wanli Xue</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Z">Ze Kang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuxi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+T">Tiantian Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shengyong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1090">[1090]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04228" title="Abstract">arXiv:2305.04228</a> (replaced) [<a href="/pdf/2305.04228" title="Download PDF">pdf</a>, <a href="/ps/2305.04228" title="Download PostScript">ps</a>, <a href="/format/2305.04228" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Heterogeneous Directed Hypergraph Neural Network over abstract syntax  tree (AST) for Code Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Guang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+T">Tiancheng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+L">Liang Dou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in the 35th International Conference on Software Engineering and Knowledge Engineering (SEKE 2023) as a regular paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1091">[1091]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04435" title="Abstract">arXiv:2305.04435</a> (replaced) [<a href="/pdf/2305.04435" title="Download PDF">pdf</a>, <a href="/ps/2305.04435" title="Download PostScript">ps</a>, <a href="/format/2305.04435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Communication complexity of entanglement assisted multi-party  computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Meng%2C+R">Ruoyu Meng</a>, 
<a href="/search/quant-ph?searchtype=author&query=Ramamoorthy%2C+A">Aditya Ramamoorthy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Modified layout so that the reference is shown correctly
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item1092">[1092]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04721" title="Abstract">arXiv:2305.04721</a> (replaced) [<a href="/pdf/2305.04721" title="Download PDF">pdf</a>, <a href="/ps/2305.04721" title="Download PostScript">ps</a>, <a href="/format/2305.04721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Menger-type theorem for two induced paths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Albrechtsen%2C+S">Sandra Albrechtsen</a>, 
<a href="/search/math?searchtype=author&query=Huynh%2C+T">Tony Huynh</a>, 
<a href="/search/math?searchtype=author&query=Jacobs%2C+R+W">Raphael W. Jacobs</a>, 
<a href="/search/math?searchtype=author&query=Knappe%2C+P">Paul Knappe</a>, 
<a href="/search/math?searchtype=author&query=Wollan%2C+P">Paul Wollan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item1093">[1093]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04881" title="Abstract">arXiv:2305.04881</a> (replaced) [<a href="/pdf/2305.04881" title="Download PDF">pdf</a>, <a href="/ps/2305.04881" title="Download PostScript">ps</a>, <a href="/format/2305.04881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Skolem and Positivity Completeness of Ergodic Markov Chains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vahanwala%2C+M">Mihir Vahanwala</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item1094">[1094]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05214" title="Abstract">arXiv:2305.05214</a> (replaced) [<a href="/pdf/2305.05214" title="Download PDF">pdf</a>, <a href="/format/2305.05214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CharSpan: Utilizing Lexical Similarity to Enable Zero-Shot Machine  Translation for Extremely Low-resource Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maurya%2C+K+K">Kaushal Kumar Maurya</a>, 
<a href="/search/cs?searchtype=author&query=Kejriwal%2C+R">Rahul Kejriwal</a>, 
<a href="/search/cs?searchtype=author&query=Desarkar%2C+M+S">Maunendra Sankar Desarkar</a>, 
<a href="/search/cs?searchtype=author&query=Kunchukuttan%2C+A">Anoop Kunchukuttan</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1095">[1095]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05875" title="Abstract">arXiv:2305.05875</a> (replaced) [<a href="/pdf/2305.05875" title="Download PDF">pdf</a>, <a href="/format/2305.05875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantization Aware Attack: Enhancing Transferable Adversarial Attacks by  Model Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yulong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenhao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qian Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhengyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+H">Haoran Fan</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Dawei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Nannan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transactions on Information Forensics and Security in 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item1096">[1096]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06773" title="Abstract">arXiv:2305.06773</a> (replaced) [<a href="/pdf/2305.06773" title="Download PDF">pdf</a>, <a href="/format/2305.06773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Better Understanding of the Computer Vision Research Community  in Africa
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Omotayo%2C+A">Abdul-Hakeem Omotayo</a>, 
<a href="/search/cs?searchtype=author&query=Gamal%2C+M">Mai Gamal</a>, 
<a href="/search/cs?searchtype=author&query=Ehab%2C+E">Eman Ehab</a>, 
<a href="/search/cs?searchtype=author&query=Dovonon%2C+G">Gbetondji Dovonon</a>, 
<a href="/search/cs?searchtype=author&query=Akinjobi%2C+Z">Zainab Akinjobi</a>, 
<a href="/search/cs?searchtype=author&query=Lukman%2C+I">Ismaila Lukman</a>, 
<a href="/search/cs?searchtype=author&query=Turki%2C+H">Houcemeddine Turki</a>, 
<a href="/search/cs?searchtype=author&query=Abdien%2C+M">Mahmod Abdien</a>, 
<a href="/search/cs?searchtype=author&query=Tondji%2C+I">Idriss Tondji</a>, 
<a href="/search/cs?searchtype=author&query=Oppong%2C+A">Abigail Oppong</a>, 
<a href="/search/cs?searchtype=author&query=Pimi%2C+Y">Yvan Pimi</a>, 
<a href="/search/cs?searchtype=author&query=Gamal%2C+K">Karim Gamal</a>, 
<a href="/search/cs?searchtype=author&query=Ro%27ya-CV4Africa">Ro&#x27;ya-CV4Africa</a>, 
<a href="/search/cs?searchtype=author&query=Siam%2C+M">Mennatullah Siam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in EAAMO'23 under ACM License. This work is part of our African computer vision grassroots research in Ro'ya - CV4Africa, <a href="https://ro-ya-cv4africa.github.io/homepage/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1097">[1097]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08029" title="Abstract">arXiv:2305.08029</a> (replaced) [<a href="/pdf/2305.08029" title="Download PDF">pdf</a>, <a href="/format/2305.08029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> REMAST: Real-time Emotion-based Music Arrangement with Soft Transition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zihao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Le Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yunfei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yikai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+H">HaoRong Hong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenbo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xinda Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kejun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1098">[1098]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08514" title="Abstract">arXiv:2305.08514</a> (replaced) [<a href="/pdf/2305.08514" title="Download PDF">pdf</a>, <a href="/format/2305.08514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Adversarial Networks for Spatio-Spectral Compression of  Hyperspectral Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Byju%2C+A+P">Akshara Preethy Byju</a>, 
<a href="/search/cs?searchtype=author&query=Fuchs%2C+M+H+P">Martin Hermann Paul Fuchs</a>, 
<a href="/search/cs?searchtype=author&query=Walda%2C+A">Alisa Walda</a>, 
<a href="/search/cs?searchtype=author&query=Demir%2C+B">Beg&#xfc;m Demir</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item1099">[1099]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09859" title="Abstract">arXiv:2305.09859</a> (replaced) [<a href="/pdf/2305.09859" title="Download PDF">pdf</a>, <a href="/format/2305.09859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smaller Language Models are Better Black-box Machine-Generated Text  Detectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mireshghallah%2C+N">Niloofar Mireshghallah</a>, 
<a href="/search/cs?searchtype=author&query=Mattern%2C+J">Justus Mattern</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Sicun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Shokri%2C+R">Reza Shokri</a>, 
<a href="/search/cs?searchtype=author&query=Berg-Kirkpatrick%2C+T">Taylor Berg-Kirkpatrick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1100">[1100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10110" title="Abstract">arXiv:2305.10110</a> (replaced) [<a href="/pdf/2305.10110" title="Download PDF">pdf</a>, <a href="/ps/2305.10110" title="Download PostScript">ps</a>, <a href="/format/2305.10110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive aggregation of Monte Carlo augmented decomposed filters for  efficient group-equivariant convolutional neural network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wenzhao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wichtmann%2C+B+D">Barbara D. Wichtmann</a>, 
<a href="/search/cs?searchtype=author&query=Albert%2C+S">Steffen Albert</a>, 
<a href="/search/cs?searchtype=author&query=Maurer%2C+A">Angelika Maurer</a>, 
<a href="/search/cs?searchtype=author&query=Z%C3%B6llner%2C+F+G">Frank G. Z&#xf6;llner</a>, 
<a href="/search/cs?searchtype=author&query=Attenberger%2C+U">Ulrike Attenberger</a>, 
<a href="/search/cs?searchtype=author&query=Hesser%2C+J">J&#xfc;rgen Hesser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1101">[1101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10267" title="Abstract">arXiv:2305.10267</a> (replaced) [<a href="/pdf/2305.10267" title="Download PDF">pdf</a>, <a href="/format/2305.10267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> State Representation Learning Using an Unbalanced Atlas
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meng%2C+L">Li Meng</a>, 
<a href="/search/cs?searchtype=author&query=Goodwin%2C+M">Morten Goodwin</a>, 
<a href="/search/cs?searchtype=author&query=Yazidi%2C+A">Anis Yazidi</a>, 
<a href="/search/cs?searchtype=author&query=Engelstad%2C+P">Paal Engelstad</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1102">[1102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11857" title="Abstract">arXiv:2305.11857</a> (replaced) [<a href="/pdf/2305.11857" title="Download PDF">pdf</a>, <a href="/format/2305.11857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing high-dimensional optimal transport by flow neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Xu%2C+C">Chen Xu</a>, 
<a href="/search/stat?searchtype=author&query=Cheng%2C+X">Xiuyuan Cheng</a>, 
<a href="/search/stat?searchtype=author&query=Xie%2C+Y">Yao Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item1103">[1103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12793" title="Abstract">arXiv:2305.12793</a> (replaced) [<a href="/pdf/2305.12793" title="Download PDF">pdf</a>, <a href="/format/2305.12793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot End-to-End Spoken Language Understanding via Cross-Modal  Selective Self-Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=He%2C+J">Jianfeng He</a>, 
<a href="/search/eess?searchtype=author&query=Salazar%2C+J">Julian Salazar</a>, 
<a href="/search/eess?searchtype=author&query=Yao%2C+K">Kaisheng Yao</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Haoqi Li</a>, 
<a href="/search/eess?searchtype=author&query=Cai%2C+J">Jinglun Cai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Multimedia (cs.MM); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item1104">[1104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12809" title="Abstract">arXiv:2305.12809</a> (replaced) [<a href="/pdf/2305.12809" title="Download PDF">pdf</a>, <a href="/format/2305.12809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relabeling Minimal Training Subset to Flip a Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jinghan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Linjie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lequan Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1105">[1105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13015" title="Abstract">arXiv:2305.13015</a> (replaced) [<a href="/pdf/2305.13015" title="Download PDF">pdf</a>, <a href="/format/2305.13015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D Rotation and Translation for Hyperbolic Knowledge Graph Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yihua Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Shimodaira%2C+H">Hidetoshi Shimodaira</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, EACL2024 main
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1106">[1106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13654" title="Abstract">arXiv:2305.13654</a> (replaced) [<a href="/pdf/2305.13654" title="Download PDF">pdf</a>, <a href="/format/2305.13654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding and Mitigating Spurious Correlations in Text  Classification with Neighborhood Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chew%2C+O">Oscar Chew</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hsuan-Tien Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kuan-Hao Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EACL-Findings 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1107">[1107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13954" title="Abstract">arXiv:2305.13954</a> (replaced) [<a href="/pdf/2305.13954" title="Download PDF">pdf</a>, <a href="/format/2305.13954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Prompt Optimization for Large Language Models Against  Distribution Shifts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Moxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+F">Fuli Feng</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yixin Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jizhi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 Main
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1108">[1108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14195" title="Abstract">arXiv:2305.14195</a> (replaced) [<a href="/pdf/2305.14195" title="Download PDF">pdf</a>, <a href="/format/2305.14195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HumBEL: A Human-in-the-Loop Approach for Evaluating Demographic Factors  of Language Models in Human-Machine Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sicilia%2C+A">Anthony Sicilia</a>, 
<a href="/search/cs?searchtype=author&query=Gates%2C+J+C">Jennifer C. Gates</a>, 
<a href="/search/cs?searchtype=author&query=Alikhani%2C+M">Malihe Alikhani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 9 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1109">[1109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14383" title="Abstract">arXiv:2305.14383</a> (replaced) [<a href="/pdf/2305.14383" title="Download PDF">pdf</a>, <a href="/format/2305.14383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Rational Model of Dimension-reduced Human Categorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+Y">Yifan Hong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1110">[1110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14982" title="Abstract">arXiv:2305.14982</a> (replaced) [<a href="/pdf/2305.14982" title="Download PDF">pdf</a>, <a href="/format/2305.14982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LAraBench: Benchmarking Arabic AI with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdelali%2C+A">Ahmed Abdelali</a>, 
<a href="/search/cs?searchtype=author&query=Mubarak%2C+H">Hamdy Mubarak</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+S+A">Shammur Absar Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Hasanain%2C+M">Maram Hasanain</a>, 
<a href="/search/cs?searchtype=author&query=Mousi%2C+B">Basel Mousi</a>, 
<a href="/search/cs?searchtype=author&query=Boughorbel%2C+S">Sabri Boughorbel</a>, 
<a href="/search/cs?searchtype=author&query=Kheir%2C+Y+E">Yassine El Kheir</a>, 
<a href="/search/cs?searchtype=author&query=Izham%2C+D">Daniel Izham</a>, 
<a href="/search/cs?searchtype=author&query=Dalvi%2C+F">Fahim Dalvi</a>, 
<a href="/search/cs?searchtype=author&query=Hawasly%2C+M">Majd Hawasly</a>, 
<a href="/search/cs?searchtype=author&query=Nazar%2C+N">Nizi Nazar</a>, 
<a href="/search/cs?searchtype=author&query=Elshahawy%2C+Y">Yousseif Elshahawy</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+A">Ahmed Ali</a>, 
<a href="/search/cs?searchtype=author&query=Durrani%2C+N">Nadir Durrani</a>, 
<a href="/search/cs?searchtype=author&query=Milic-Frayling%2C+N">Natasa Milic-Frayling</a>, 
<a href="/search/cs?searchtype=author&query=Alam%2C+F">Firoj Alam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Foundation Models, Large Language Models, Arabic NLP, Arabic Speech, Arabic AI, GPT3.5 Evaluation, USM Evaluation, Whisper Evaluation, GPT-4, BLOOMZ, Jais13b
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1111">[1111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15244" title="Abstract">arXiv:2305.15244</a> (replaced) [<a href="/pdf/2305.15244" title="Download PDF">pdf</a>, <a href="/format/2305.15244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Lyapunov and Optimal Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Layeghi%2C+D">Daniel Layeghi</a>, 
<a href="/search/cs?searchtype=author&query=Tonneau%2C+S">Steve Tonneau</a>, 
<a href="/search/cs?searchtype=author&query=Mistry%2C+M">Michael Mistry</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1112">[1112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16368" title="Abstract">arXiv:2305.16368</a> (replaced) [<a href="/pdf/2305.16368" title="Download PDF">pdf</a>, <a href="/format/2305.16368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural incomplete factorization: learning preconditioners for the  conjugate gradient method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=H%C3%A4usner%2C+P">Paul H&#xe4;usner</a>, 
<a href="/search/math?searchtype=author&query=%C3%96ktem%2C+O">Ozan &#xd6;ktem</a>, 
<a href="/search/math?searchtype=author&query=Sj%C3%B6lund%2C+J">Jens Sj&#xf6;lund</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review. 18 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1113">[1113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16905" title="Abstract">arXiv:2305.16905</a> (replaced) [<a href="/pdf/2305.16905" title="Download PDF">pdf</a>, <a href="/format/2305.16905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Neural Additive Models with Bayesian Principles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Bouchiat%2C+K">Kouroche Bouchiat</a>, 
<a href="/search/stat?searchtype=author&query=Immer%2C+A">Alexander Immer</a>, 
<a href="/search/stat?searchtype=author&query=Y%C3%A8che%2C+H">Hugo Y&#xe8;che</a>, 
<a href="/search/stat?searchtype=author&query=R%C3%A4tsch%2C+G">Gunnar R&#xe4;tsch</a>, 
<a href="/search/stat?searchtype=author&query=Fortuin%2C+V">Vincent Fortuin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1114">[1114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17267" title="Abstract">arXiv:2305.17267</a> (replaced) [<a href="/pdf/2305.17267" title="Download PDF">pdf</a>, <a href="/format/2305.17267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CODET: A Benchmark for Contrastive Dialectal Evaluation of Machine  Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alam%2C+M+M+I">Md Mahfuz Ibn Alam</a>, 
<a href="/search/cs?searchtype=author&query=Ahmadi%2C+S">Sina Ahmadi</a>, 
<a href="/search/cs?searchtype=author&query=Anastasopoulos%2C+A">Antonios Anastasopoulos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1115">[1115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17326" title="Abstract">arXiv:2305.17326</a> (replaced) [<a href="/pdf/2305.17326" title="Download PDF">pdf</a>, <a href="/format/2305.17326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Matrix Information Theory for Self-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhiquan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jingqin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weiran Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yang Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1116">[1116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18435" title="Abstract">arXiv:2305.18435</a> (replaced) [<a href="/pdf/2305.18435" title="Download PDF">pdf</a>, <a href="/format/2305.18435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistically Efficient Bayesian Sequential Experiment Design via  Reinforcement Learning with Cross-Entropy Estimators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blau%2C+T">Tom Blau</a>, 
<a href="/search/cs?searchtype=author&query=Chades%2C+I">Iadine Chades</a>, 
<a href="/search/cs?searchtype=author&query=Dezfouli%2C+A">Amir Dezfouli</a>, 
<a href="/search/cs?searchtype=author&query=Steinberg%2C+D">Daniel Steinberg</a>, 
<a href="/search/cs?searchtype=author&query=Bonilla%2C+E+V">Edwin V. Bonilla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item1117">[1117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18466" title="Abstract">arXiv:2305.18466</a> (replaced) [<a href="/pdf/2305.18466" title="Download PDF">pdf</a>, <a href="/format/2305.18466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Test-Time Training on Nearest Neighbors for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hardt%2C+M">Moritz Hardt</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yu Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR final version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1118">[1118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00937" title="Abstract">arXiv:2306.00937</a> (replaced) [<a href="/pdf/2306.00937" title="Download PDF">pdf</a>, <a href="/format/2306.00937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STEVE-1: A Generative Model for Text-to-Behavior in Minecraft
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lifshitz%2C+S">Shalev Lifshitz</a>, 
<a href="/search/cs?searchtype=author&query=Paster%2C+K">Keiran Paster</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+H">Harris Chan</a>, 
<a href="/search/cs?searchtype=author&query=Ba%2C+J">Jimmy Ba</a>, 
<a href="/search/cs?searchtype=author&query=McIlraith%2C+S">Sheila McIlraith</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1119">[1119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01271" title="Abstract">arXiv:2306.01271</a> (replaced) [<a href="/pdf/2306.01271" title="Download PDF">pdf</a>, <a href="/format/2306.01271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Understanding Clean Generalization and Robust Overfitting in  Adversarial Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Binghui Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuanzhi Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, comments welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1120">[1120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01276" title="Abstract">arXiv:2306.01276</a> (replaced) [<a href="/pdf/2306.01276" title="Download PDF">pdf</a>, <a href="/format/2306.01276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symmetric Replay Training: Enhancing Sample Efficiency in Deep  Reinforcement Learning for Combinatorial Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyeonah Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minsu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+S">Sungsoo Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jinkyoo Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages (including 12 pages of the appendix)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1121">[1121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01992" title="Abstract">arXiv:2306.01992</a> (replaced) [<a href="/pdf/2306.01992" title="Download PDF">pdf</a>, <a href="/format/2306.01992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Size-Independent Sample Complexity of ReLU Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sellke%2C+M">Mark Sellke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1122">[1122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02689" title="Abstract">arXiv:2306.02689</a> (replaced) [<a href="/pdf/2306.02689" title="Download PDF">pdf</a>, <a href="/format/2306.02689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equity-Transformer: Solving NP-hard Min-Max Routing Problems as  Sequential Generation with Equity Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Son%2C+J">Jiwoo Son</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minsu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Sanghyeok Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyeonah Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jinkyoo Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 2024, 16 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1123">[1123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03163" title="Abstract">arXiv:2306.03163</a> (replaced) [<a href="/pdf/2306.03163" title="Download PDF">pdf</a>, <a href="/format/2306.03163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Can We Train Deep Learning Models Across Clouds and Continents? An  Experimental Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Erben%2C+A">Alexander Erben</a>, 
<a href="/search/cs?searchtype=author&query=Mayer%2C+R">Ruben Mayer</a>, 
<a href="/search/cs?searchtype=author&query=Jacobsen%2C+H">Hans-Arno Jacobsen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at VLDB 2024. Artifacts and Code: <a href="https://github.com/cirquit/hivemind-multi-cloud">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Networking and Internet Architecture (cs.NI); Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item1124">[1124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04498" title="Abstract">arXiv:2306.04498</a> (replaced) [<a href="/pdf/2306.04498" title="Download PDF">pdf</a>, <a href="/format/2306.04498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair Multi-Agent Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leshem%2C+A">Amir Leshem</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item1125">[1125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04775" title="Abstract">arXiv:2306.04775</a> (replaced) [<a href="/pdf/2306.04775" title="Download PDF">pdf</a>, <a href="/format/2306.04775" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting Observation Bias to Improve Matrix Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jedra%2C+Y">Yassir Jedra</a>, 
<a href="/search/cs?searchtype=author&query=Mann%2C+S">Sean Mann</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+C">Charlotte Park</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+D">Devavrat Shah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1126">[1126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04940" title="Abstract">arXiv:2306.04940</a> (replaced) [<a href="/pdf/2306.04940" title="Download PDF">pdf</a>, <a href="/format/2306.04940" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LayerAct: Advanced activation mechanism utilizing layer-direction  normalization for CNNs with BatchNorm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoon%2C+K">Kihyuk Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+C">Chiehyeon Lim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures, 3 tables except appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item1127">[1127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05035" title="Abstract">arXiv:2306.05035</a> (replaced) [<a href="/pdf/2306.05035" title="Download PDF">pdf</a>, <a href="/format/2306.05035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does Long-Term Series Forecasting Need Complex Attention and Extra Long  Inputs?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+D">Daojun Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haixia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+D">Dongfeng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaoyan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Minggao Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1128">[1128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06347" title="Abstract">arXiv:2306.06347</a> (replaced) [<a href="/pdf/2306.06347" title="Download PDF">pdf</a>, <a href="/format/2306.06347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DocChecker: Bootstrapping Code Large Language Model for Detecting and  Resolving Code-Comment Inconsistencies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dau%2C+A+T+V">Anh T. V. Dau</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J+L+C">Jin L. C. Guo</a>, 
<a href="/search/cs?searchtype=author&query=Bui%2C+N+D+Q">Nghi D. Q. Bui</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> EACL 2024 - Demonstration track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item1129">[1129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.06651" title="Abstract">arXiv:2306.06651</a> (replaced) [<a href="/pdf/2306.06651" title="Download PDF">pdf</a>, <a href="/format/2306.06651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting Software Performance with Divide-and-Learn
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+J">Jingzhi Gong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tao Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by The ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item1130">[1130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07392" title="Abstract">arXiv:2306.07392</a> (replaced) [<a href="/pdf/2306.07392" title="Download PDF">pdf</a>, <a href="/format/2306.07392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Any-View 6DoF Robotic Grasping in Cluttered Scenes via Neural  Surface Rendering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jauhri%2C+S">Snehal Jauhri</a>, 
<a href="/search/cs?searchtype=author&query=Lunawat%2C+I">Ishikaa Lunawat</a>, 
<a href="/search/cs?searchtype=author&query=Chalvatzaki%2C+G">Georgia Chalvatzaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1131">[1131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07623" title="Abstract">arXiv:2306.07623</a> (replaced) [<a href="/pdf/2306.07623" title="Download PDF">pdf</a>, <a href="/format/2306.07623" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Invariants and Home Spaces in Transition Systems and Petri Nets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Memmi%2C+G">Gerard Memmi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 94 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
</div>
</dd>
<dt><a name="item1132">[1132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07629" title="Abstract">arXiv:2306.07629</a> (replaced) [<a href="/pdf/2306.07629" title="Download PDF">pdf</a>, <a href="/format/2306.07629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SqueezeLLM: Dense-and-Sparse Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sehoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Hooper%2C+C">Coleman Hooper</a>, 
<a href="/search/cs?searchtype=author&query=Gholami%2C+A">Amir Gholami</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiuyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Sheng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Mahoney%2C+M+W">Michael W. Mahoney</a>, 
<a href="/search/cs?searchtype=author&query=Keutzer%2C+K">Kurt Keutzer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1133">[1133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08489" title="Abstract">arXiv:2306.08489</a> (replaced) [<a href="/pdf/2306.08489" title="Download PDF">pdf</a>, <a href="/ps/2306.08489" title="Download PostScript">ps</a>, <a href="/format/2306.08489" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis and Approximate Inference of Large Random Kronecker Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Liao%2C+Z">Zhenyu Liao</a>, 
<a href="/search/stat?searchtype=author&query=Xia%2C+Y">Yuanqian Xia</a>, 
<a href="/search/stat?searchtype=author&query=Niu%2C+C">Chengmei Niu</a>, 
<a href="/search/stat?searchtype=author&query=Xiao%2C+Y">Yong Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 5 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Spectral Theory (math.SP)

</div>
</div>
</dd>
<dt><a name="item1134">[1134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08838" title="Abstract">arXiv:2306.08838</a> (replaced) [<a href="/pdf/2306.08838" title="Download PDF">pdf</a>, <a href="/format/2306.08838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Domain Adaptation with Theoretical Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bassily%2C+R">Raef Bassily</a>, 
<a href="/search/cs?searchtype=author&query=Cortes%2C+C">Corinna Cortes</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+A">Anqi Mao</a>, 
<a href="/search/cs?searchtype=author&query=Mohri%2C+M">Mehryar Mohri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1135">[1135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09152" title="Abstract">arXiv:2306.09152</a> (replaced) [<a href="/pdf/2306.09152" title="Download PDF">pdf</a>, <a href="/ps/2306.09152" title="Download PostScript">ps</a>, <a href="/format/2306.09152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Schematic Unification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cerna%2C+D+M">David M. Cerna</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted for review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item1136">[1136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09215" title="Abstract">arXiv:2306.09215</a> (replaced) [<a href="/pdf/2306.09215" title="Download PDF">pdf</a>, <a href="/format/2306.09215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Effects and Optimal Design of Redundant Sensors in Collaborative  State Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ren%2C+Y">Yunxiao Ren</a>, 
<a href="/search/eess?searchtype=author&query=Duan%2C+Z">Zhisheng Duan</a>, 
<a href="/search/eess?searchtype=author&query=Duan%2C+P">Peihu Duan</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+L">Ling Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item1137">[1137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09912" title="Abstract">arXiv:2306.09912</a> (replaced) [<a href="/pdf/2306.09912" title="Download PDF">pdf</a>, <a href="/ps/2306.09912" title="Download PostScript">ps</a>, <a href="/format/2306.09912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Quantum Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+C">Chao Ren</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Han Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+R">Rudai Yan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Minrui Xu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yuan Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Huihui Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z+Y">Zhao Yang Dong</a>, 
<a href="/search/cs?searchtype=author&query=Kwek%2C+L+C">Leong Chuan Kwek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Survey of quantum federated learning (QFL)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item1138">[1138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10224" title="Abstract">arXiv:2306.10224</a> (replaced) [<a href="/pdf/2306.10224" title="Download PDF">pdf</a>, <a href="/format/2306.10224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bloated Disclosures: Can ChatGPT Help Investors Process Information?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Kim%2C+A">Alex Kim</a>, 
<a href="/search/econ?searchtype=author&query=Muhn%2C+M">Maximilian Muhn</a>, 
<a href="/search/econ?searchtype=author&query=Nikolaev%2C+V">Valeri Nikolaev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Economics (econ.GN)</span>; Artificial Intelligence (cs.AI); General Finance (q-fin.GN)

</div>
</div>
</dd>
<dt><a name="item1139">[1139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10374" title="Abstract">arXiv:2306.10374</a> (replaced) [<a href="/pdf/2306.10374" title="Download PDF">pdf</a>, <a href="/ps/2306.10374" title="Download PostScript">ps</a>, <a href="/format/2306.10374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Contextual Optimization Methods for Decision Making under  Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Sadana%2C+U">Utsav Sadana</a>, 
<a href="/search/math?searchtype=author&query=Chenreddy%2C+A">Abhilash Chenreddy</a>, 
<a href="/search/math?searchtype=author&query=Delage%2C+E">Erick Delage</a>, 
<a href="/search/math?searchtype=author&query=Forel%2C+A">Alexandre Forel</a>, 
<a href="/search/math?searchtype=author&query=Frejinger%2C+E">Emma Frejinger</a>, 
<a href="/search/math?searchtype=author&query=Vidal%2C+T">Thibaut Vidal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1140">[1140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10409" title="Abstract">arXiv:2306.10409</a> (replaced) [<a href="/e-print/2306.10409" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Iterative Hierarchy and Ranking Process (IHRP): A Novel Effective  Hierarchy Method for Densely Connected Systems and Case Study in Student  Performance Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dhara%2C+S">Suvojit Dhara</a>, 
<a href="/search/cs?searchtype=author&query=Goswami%2C+A">Adrijit Goswami</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper has been rejected from the mentioned journal "International Journal of Information Technology and Decision Making" and also some of the results in this version was mis-calculated
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item1141">[1141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13339" title="Abstract">arXiv:2306.13339</a> (replaced) [<a href="/pdf/2306.13339" title="Download PDF">pdf</a>, <a href="/format/2306.13339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TrustGuard: GNN-based Robust and Explainable Trust Evaluation with  Dynamicity Support
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zheng Yan</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+J">Jiahe Lan</a>, 
<a href="/search/cs?searchtype=author&query=Bertino%2C+E">Elisa Bertino</a>, 
<a href="/search/cs?searchtype=author&query=Pedrycz%2C+W">Witold Pedrycz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE TDSC. Code: <a href="https://github.com/Jieerbobo/TrustGuard">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1142">[1142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13588" title="Abstract">arXiv:2306.13588</a> (replaced) [<a href="/pdf/2306.13588" title="Download PDF">pdf</a>, <a href="/format/2306.13588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> System-Level Natural Language Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+W">Weizhe Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+K">Kyunghyun Cho</a>, 
<a href="/search/cs?searchtype=author&query=Weston%2C+J">Jason Weston</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1143">[1143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14233" title="Abstract">arXiv:2306.14233</a> (replaced) [<a href="/pdf/2306.14233" title="Download PDF">pdf</a>, <a href="/format/2306.14233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention-Refined Unrolling for Sparse Sequential micro-Doppler  Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mazzieri%2C+R">Riccardo Mazzieri</a>, 
<a href="/search/eess?searchtype=author&query=Pegoraro%2C+J">Jacopo Pegoraro</a>, 
<a href="/search/eess?searchtype=author&query=Rossi%2C+M">Michele Rossi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 10 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1144">[1144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14275" title="Abstract">arXiv:2306.14275</a> (replaced) [<a href="/pdf/2306.14275" title="Download PDF">pdf</a>, <a href="/format/2306.14275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Adversarial Training via Reweighting Optimization Trajectory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tianjin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianlong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+M">Meng Fang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Menkovski%2C+V">Vlaod Menkovski</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+L">Lu Yin</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+Y">Yulong Pei</a>, 
<a href="/search/cs?searchtype=author&query=Pechenizkiy%2C+M">Mykola Pechenizkiy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ECML 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ECML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1145">[1145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.17107" title="Abstract">arXiv:2306.17107</a> (replaced) [<a href="/pdf/2306.17107" title="Download PDF">pdf</a>, <a href="/format/2306.17107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image  Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanzhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruiyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jiuxiang Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yufan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lipka%2C+N">Nedim Lipka</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Diyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tong Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1146">[1146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00270" title="Abstract">arXiv:2307.00270</a> (replaced) [<a href="/pdf/2307.00270" title="Download PDF">pdf</a>, <a href="/format/2307.00270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time High-Resolution Neural Network with Semantic Guidance for  Crack Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongshang Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+R">Ronggui Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Han Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+G">Gaoli Cheng</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Automation in Construction, Volume 156, December 2023, 105112
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1147">[1147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00494" title="Abstract">arXiv:2307.00494</a> (replaced) [<a href="/pdf/2307.00494" title="Download PDF">pdf</a>, <a href="/format/2307.00494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Protein Optimization with Smoothed Fitness Landscapes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Kirjner%2C+A">Andrew Kirjner</a>, 
<a href="/search/q-bio?searchtype=author&query=Yim%2C+J">Jason Yim</a>, 
<a href="/search/q-bio?searchtype=author&query=Samusevich%2C+R">Raman Samusevich</a>, 
<a href="/search/q-bio?searchtype=author&query=Bracha%2C+S">Shahar Bracha</a>, 
<a href="/search/q-bio?searchtype=author&query=Jaakkola%2C+T">Tommi Jaakkola</a>, 
<a href="/search/q-bio?searchtype=author&query=Barzilay%2C+R">Regina Barzilay</a>, 
<a href="/search/q-bio?searchtype=author&query=Fiete%2C+I">Ila Fiete</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024. Code: <a href="https://github.com/kirjner/GGS">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Machine Learning (cs.LG); Quantitative Methods (q-bio.QM); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1148">[1148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01171" title="Abstract">arXiv:2307.01171</a> (replaced) [<a href="/pdf/2307.01171" title="Download PDF">pdf</a>, <a href="/format/2307.01171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Neural Estimation of Entropies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Goldfeld%2C+Z">Ziv Goldfeld</a>, 
<a href="/search/quant-ph?searchtype=author&query=Patel%2C+D">Dhrumil Patel</a>, 
<a href="/search/quant-ph?searchtype=author&query=Sreekumar%2C+S">Sreejith Sreekumar</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wilde%2C+M+M">Mark M. Wilde</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 2 figures; see also independent works of Shin, Lee, and Jeong at <a href="/abs/2306.14566">arXiv:2306.14566v1</a> and Lee, Kwon, and Lee at <a href="/abs/2307.13511">arXiv:2307.13511v2</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Statistical Mechanics (cond-mat.stat-mech); Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1149">[1149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01645" title="Abstract">arXiv:2307.01645</a> (replaced) [<a href="/pdf/2307.01645" title="Download PDF">pdf</a>, <a href="/format/2307.01645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Domain Self-Supervised Learning Improves Remote Sensing Image Scene  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dimitrovski%2C+I">Ivica Dimitrovski</a>, 
<a href="/search/cs?searchtype=author&query=Kitanovski%2C+I">Ivan Kitanovski</a>, 
<a href="/search/cs?searchtype=author&query=Simidjievski%2C+N">Nikola Simidjievski</a>, 
<a href="/search/cs?searchtype=author&query=Kocev%2C+D">Dragi Kocev</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Geoscience and Remote Sensing Letters (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1150">[1150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03108" title="Abstract">arXiv:2307.03108</a> (replaced) [<a href="/pdf/2307.03108" title="Download PDF">pdf</a>, <a href="/format/2307.03108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DIAGNOSIS: Detecting Unauthorized Data Usages in Text-to-image Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhenting Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+L">Lingjuan Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Metaxas%2C+D+N">Dimitris N. Metaxas</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shiqing Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1151">[1151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03343" title="Abstract">arXiv:2307.03343</a> (replaced) [<a href="/pdf/2307.03343" title="Download PDF">pdf</a>, <a href="/format/2307.03343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified Modeling and Rate Coverage Analysis for Satellite-Terrestrial  Integrated Networks: Coverage Extension or Data Offloading?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jeonghun Park</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jinseok Choi</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+N">Namyoon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Baccelli%2C+F">Fran&#xe7;ois Baccelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to IEEE journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1152">[1152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03927" title="Abstract">arXiv:2307.03927</a> (replaced) [<a href="/pdf/2307.03927" title="Download PDF">pdf</a>, <a href="/format/2307.03927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Empirical Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Multerer%2C+M">Michael Multerer</a>, 
<a href="/search/stat?searchtype=author&query=Schneider%2C+P">Paul Schneider</a>, 
<a href="/search/stat?searchtype=author&query=Sen%2C+R">Rohan Sen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA); Risk Management (q-fin.RM)

</div>
</div>
</dd>
<dt><a name="item1153">[1153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05591" title="Abstract">arXiv:2307.05591</a> (replaced) [<a href="/pdf/2307.05591" title="Download PDF">pdf</a>, <a href="/format/2307.05591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear Alignment of Vision-language Models for Image Captioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paischer%2C+F">Fabian Paischer</a>, 
<a href="/search/cs?searchtype=author&query=Hofmarcher%2C+M">Markus Hofmarcher</a>, 
<a href="/search/cs?searchtype=author&query=Hochreiter%2C+S">Sepp Hochreiter</a>, 
<a href="/search/cs?searchtype=author&query=Adler%2C+T">Thomas Adler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages (+ references and appendix)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1154">[1154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06095" title="Abstract">arXiv:2307.06095</a> (replaced) [<a href="/pdf/2307.06095" title="Download PDF">pdf</a>, <a href="/format/2307.06095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact Resource Allocation for Fair Wireless Relay
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arribas%2C+E">Edgar Arribas</a>, 
<a href="/search/cs?searchtype=author&query=Cholvi%2C+V">Vicent Cholvi</a>, 
<a href="/search/cs?searchtype=author&query=Mancuso%2C+V">Vincenzo Mancuso</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item1155">[1155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06324" title="Abstract">arXiv:2307.06324</a> (replaced) [<a href="/pdf/2307.06324" title="Download PDF">pdf</a>, <a href="/format/2307.06324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provably Faster Gradient Descent via Long Steps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Grimmer%2C+B">Benjamin Grimmer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item1156">[1156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06908" title="Abstract">arXiv:2307.06908</a> (replaced) [<a href="/pdf/2307.06908" title="Download PDF">pdf</a>, <a href="/format/2307.06908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Benchmarks for Factuality Evaluation of Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Muhlgay%2C+D">Dor Muhlgay</a>, 
<a href="/search/cs?searchtype=author&query=Ram%2C+O">Ori Ram</a>, 
<a href="/search/cs?searchtype=author&query=Magar%2C+I">Inbal Magar</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+Y">Yoav Levine</a>, 
<a href="/search/cs?searchtype=author&query=Ratner%2C+N">Nir Ratner</a>, 
<a href="/search/cs?searchtype=author&query=Belinkov%2C+Y">Yonatan Belinkov</a>, 
<a href="/search/cs?searchtype=author&query=Abend%2C+O">Omri Abend</a>, 
<a href="/search/cs?searchtype=author&query=Leyton-Brown%2C+K">Kevin Leyton-Brown</a>, 
<a href="/search/cs?searchtype=author&query=Shashua%2C+A">Amnon Shashua</a>, 
<a href="/search/cs?searchtype=author&query=Shoham%2C+Y">Yoav Shoham</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1157">[1157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06924" title="Abstract">arXiv:2307.06924</a> (replaced) [<a href="/pdf/2307.06924" title="Download PDF">pdf</a>, <a href="/format/2307.06924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual  Language Grounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuijing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+A">Aamir Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+K">Kaiwen Hong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Runxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+P">Peixin Chang</a>, 
<a href="/search/cs?searchtype=author&query=Mizrachi%2C+Z">Zachary Mizrachi</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Justin Lin</a>, 
<a href="/search/cs?searchtype=author&query=McPherson%2C+D+L">D. Livingston McPherson</a>, 
<a href="/search/cs?searchtype=author&query=Rogers%2C+W+A">Wendy A. Rogers</a>, 
<a href="/search/cs?searchtype=author&query=Driggs-Campbell%2C+K">Katherine Driggs-Campbell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in IEEE Robotics and Automation Letters (RA-L)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1158">[1158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07262" title="Abstract">arXiv:2307.07262</a> (replaced) [<a href="/pdf/2307.07262" title="Download PDF">pdf</a>, <a href="/format/2307.07262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MorphPiece : A Linguistic Tokenizer for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jabbar%2C+H">Haris Jabbar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Manuscript under review. Patent pending
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1159">[1159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07354" title="Abstract">arXiv:2307.07354</a> (replaced) [<a href="/pdf/2307.07354" title="Download PDF">pdf</a>, <a href="/format/2307.07354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PG-Triggers: Triggers for Property Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ceri%2C+S">Stefano Ceri</a>, 
<a href="/search/cs?searchtype=author&query=Bernasconi%2C+A">Anna Bernasconi</a>, 
<a href="/search/cs?searchtype=author&query=Gagliardi%2C+A">Alessia Gagliardi</a>, 
<a href="/search/cs?searchtype=author&query=Martinenghi%2C+D">Davide Martinenghi</a>, 
<a href="/search/cs?searchtype=author&query=Bellomarini%2C+L">Luigi Bellomarini</a>, 
<a href="/search/cs?searchtype=author&query=Magnanimi%2C+D">Davide Magnanimi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
</div>
</dd>
<dt><a name="item1160">[1160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07604" title="Abstract">arXiv:2307.07604</a> (replaced) [<a href="/pdf/2307.07604" title="Download PDF">pdf</a>, <a href="/ps/2307.07604" title="Download PostScript">ps</a>, <a href="/format/2307.07604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smooth Lower Bounds for Differentially Private Algorithms via  Padding-and-Permuting Fingerprinting Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peter%2C+N">Naty Peter</a>, 
<a href="/search/cs?searchtype=author&query=Tsfadia%2C+E">Eliad Tsfadia</a>, 
<a href="/search/cs?searchtype=author&query=Ullman%2C+J">Jonathan Ullman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1161">[1161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07941" title="Abstract">arXiv:2307.07941</a> (replaced) [<a href="/pdf/2307.07941" title="Download PDF">pdf</a>, <a href="/ps/2307.07941" title="Download PostScript">ps</a>, <a href="/format/2307.07941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Compression of Unit Norm Vectors in the High Distortion Regime
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Heng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+A">Avishek Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Mazumdar%2C+A">Arya Mazumdar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appeared in ISIT 2023; Correct the proof of Theorem 1
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1162">[1162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08430" title="Abstract">arXiv:2307.08430</a> (replaced) [<a href="/pdf/2307.08430" title="Download PDF">pdf</a>, <a href="/format/2307.08430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Long-range Meta-path Search on Large-scale Heterogeneous Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chao Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zijie Guo</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Q">Qiuting He</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hao Xu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+K">Kun He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1163">[1163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09653" title="Abstract">arXiv:2307.09653</a> (replaced) [<a href="/pdf/2307.09653" title="Download PDF">pdf</a>, <a href="/format/2307.09653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HAT-CL: A Hard-Attention-to-the-Task PyTorch Library for Continual  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+X">Xiaotian Duan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1164">[1164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10172" title="Abstract">arXiv:2307.10172</a> (replaced) [<a href="/pdf/2307.10172" title="Download PDF">pdf</a>, <a href="/format/2307.10172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DialogStudio: Towards Richest and Most Diverse Unified Dataset  Collection for Conversational AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianguo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+K">Kun Qian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Heinecke%2C+S">Shelby Heinecke</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+R">Rui Meng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Ye Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhou Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Savarese%2C+S">Silvio Savarese</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Caiming Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, accepted by EACL 2024 Findings as a long paper. All datasets, licenses, codes, and models are available at at <a href="https://github.com/salesforce/DialogStudio">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1165">[1165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10249" title="Abstract">arXiv:2307.10249</a> (replaced) [<a href="/pdf/2307.10249" title="Download PDF">pdf</a>, <a href="/format/2307.10249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RCM-Fusion: Radar-Camera Multi-Level Fusion for 3D Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jisong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Seong%2C+M">Minjae Seong</a>, 
<a href="/search/cs?searchtype=author&query=Bang%2C+G">Geonho Bang</a>, 
<a href="/search/cs?searchtype=author&query=Kum%2C+D">Dongsuk Kum</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J+W">Jun Won Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE International Conference on Robotics and Automation (ICRA 2024), 7 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1166">[1166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10843" title="Abstract">arXiv:2307.10843</a> (replaced) [<a href="/pdf/2307.10843" title="Download PDF">pdf</a>, <a href="/format/2307.10843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Global Precipitation Nowcasting of Integrated Multi-satellitE Retrievals  for GPM: A U-Net Convolutional LSTM Architecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahimi%2C+R">Reyhaneh Rahimi</a>, 
<a href="/search/cs?searchtype=author&query=Ravirathinam%2C+P">Praveen Ravirathinam</a>, 
<a href="/search/cs?searchtype=author&query=Ebtehaj%2C+A">Ardeshir Ebtehaj</a>, 
<a href="/search/cs?searchtype=author&query=Behrangi%2C+A">Ali Behrangi</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+J">Jackson Tan</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+V">Vipin Kumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
</div>
</dd>
<dt><a name="item1167">[1167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12971" title="Abstract">arXiv:2307.12971</a> (replaced) [<a href="/pdf/2307.12971" title="Download PDF">pdf</a>, <a href="/format/2307.12971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Big Data -- Supply Chain Management Framework for Forecasting: Data  Preprocessing and Machine Learning Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jahin%2C+M+A">Md Abrar Jahin</a>, 
<a href="/search/cs?searchtype=author&query=Shovon%2C+M+S+H">Md Sakib Hossain Shovon</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jungpil Shin</a>, 
<a href="/search/cs?searchtype=author&query=Ridoy%2C+I+A">Istiyaque Ahmed Ridoy</a>, 
<a href="/search/cs?searchtype=author&query=Mridha%2C+M+F">M. F. Mridha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1168">[1168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13147" title="Abstract">arXiv:2307.13147</a> (replaced) [<a href="/pdf/2307.13147" title="Download PDF">pdf</a>, <a href="/format/2307.13147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extending Path-Dependent NJ-ODEs to Noisy Observations and a Dependent  Observation Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Andersson%2C+W">William Andersson</a>, 
<a href="/search/stat?searchtype=author&query=Heiss%2C+J">Jakob Heiss</a>, 
<a href="/search/stat?searchtype=author&query=Krach%2C+F">Florian Krach</a>, 
<a href="/search/stat?searchtype=author&query=Teichmann%2C+J">Josef Teichmann</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Transactions on Machine Learning Research (TMLR) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item1169">[1169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15299" title="Abstract">arXiv:2307.15299</a> (replaced) [<a href="/pdf/2307.15299" title="Download PDF">pdf</a>, <a href="/format/2307.15299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differential Evolution Algorithm based Hyper-Parameters Selection of  Transformer Neural Network Model for Load Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sen%2C+A">Anuvab Sen</a>, 
<a href="/search/cs?searchtype=author&query=Mazumder%2C+A+R">Arul Rhik Mazumder</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+U">Udayon Sen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 Pages, 6 Figures, 2 Tables, Accepted by the 14th IEEE International Symposium Series on Computational Intelligence (SSCI 2023), December 5-8, 2023, Mexico City, Mexico
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 14th IEEE International Symposium Series on Computational
  Intelligence (SSCI 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1170">[1170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16375" title="Abstract">arXiv:2307.16375</a> (replaced) [<a href="/pdf/2307.16375" title="Download PDF">pdf</a>, <a href="/format/2307.16375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed  Integer Quadratic Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Ke Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jie Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wu-Jun Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1171">[1171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01222" title="Abstract">arXiv:2308.01222</a> (replaced) [<a href="/pdf/2308.01222" title="Download PDF">pdf</a>, <a href="/format/2308.01222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Calibration in Deep Learning: A Survey of the State-of-the-Art
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cheng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1172">[1172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01580" title="Abstract">arXiv:2308.01580</a> (replaced) [<a href="/pdf/2308.01580" title="Download PDF">pdf</a>, <a href="/format/2308.01580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite element approximation of the Hardy constant
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Della+Pietra%2C+F">Francesco Della Pietra</a>, 
<a href="/search/math?searchtype=author&query=Fantuzzi%2C+G">Giovanni Fantuzzi</a>, 
<a href="/search/math?searchtype=author&query=Ignat%2C+L+I">Liviu I. Ignat</a>, 
<a href="/search/math?searchtype=author&query=Masiello%2C+A+L">Alba Lia Masiello</a>, 
<a href="/search/math?searchtype=author&query=Paoli%2C+G">Gloria Paoli</a>, 
<a href="/search/math?searchtype=author&query=Zuazua%2C+E">Enrique Zuazua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Review: Significantly improved estimates compared to the original version (23 pages, 6 figures)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
</div>
</dd>
<dt><a name="item1173">[1173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02068" title="Abstract">arXiv:2308.02068</a> (replaced) [<a href="/pdf/2308.02068" title="Download PDF">pdf</a>, <a href="/format/2308.02068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Specious Sites: Tracking the Spread and Sway of Spurious News Stories at  Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hanley%2C+H+W+A">Hans W. A. Hanley</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+D">Deepak Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Durumeric%2C+Z">Zakir Durumeric</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE S&amp;P 2024. Updated Emails
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1174">[1174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04215" title="Abstract">arXiv:2308.04215</a> (replaced) [<a href="/pdf/2308.04215" title="Download PDF">pdf</a>, <a href="/format/2308.04215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid Retrieval-Augmented Generation for Real-time Composition  Assistance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+M">Menglin Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuchao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Couturier%2C+C">Camille Couturier</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+G">Guoqing Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Rajmohan%2C+S">Saravan Rajmohan</a>, 
<a href="/search/cs?searchtype=author&query=Ruhle%2C+V">Victor Ruhle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item1175">[1175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04369" title="Abstract">arXiv:2308.04369</a> (replaced) [<a href="/pdf/2308.04369" title="Download PDF">pdf</a>, <a href="/format/2308.04369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SSTFormer: Bridging Spiking Neural Network and Memory Support  Transformer for Frame-Event based Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zongzhen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Rong%2C+Y">Yao Rong</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+B">Bo Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jin Tang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yonghong Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Peer Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item1176">[1176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05502" title="Abstract">arXiv:2308.05502</a> (replaced) [<a href="/pdf/2308.05502" title="Download PDF">pdf</a>, <a href="/ps/2308.05502" title="Download PostScript">ps</a>, <a href="/format/2308.05502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bringing order into the realm of Transformer-based language models for  artificial intelligence and law
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Greco%2C+C+M">Candida M. Greco</a>, 
<a href="/search/cs?searchtype=author&query=Tagarelli%2C+A">Andrea Tagarelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Please refer to the published version: Greco, C.M., Tagarelli, A. (2023) Bringing order into the realm of Transformer-based language models for artificial intelligence and law. Artif Intell Law, Springer Nature. November 2023. <a href="https://doi.org/10.1007/s10506-023-09374-7">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Artif Intell Law, Springer Nature. November 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Neural and Evolutionary Computing (cs.NE); Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item1177">[1177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07037" title="Abstract">arXiv:2308.07037</a> (replaced) [<a href="/pdf/2308.07037" title="Download PDF">pdf</a>, <a href="/format/2308.07037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Flow Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Graves%2C+A">Alex Graves</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+R+K">Rupesh Kumar Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Atkinson%2C+T">Timothy Atkinson</a>, 
<a href="/search/cs?searchtype=author&query=Gomez%2C+F">Faustino Gomez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1178">[1178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07134" title="Abstract">arXiv:2308.07134</a> (replaced) [<a href="/pdf/2308.07134" title="Download PDF">pdf</a>, <a href="/format/2308.07134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language is All a Graph Needs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+R">Ruosong Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Caiqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Runhui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shuyuan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongfeng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1179">[1179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08168" title="Abstract">arXiv:2308.08168</a> (replaced) [<a href="/pdf/2308.08168" title="Download PDF">pdf</a>, <a href="/format/2308.08168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emergent Software Service Platform and its Application in a Smart  Mobility Setting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wilken%2C+N">Nils Wilken</a>, 
<a href="/search/cs?searchtype=author&query=Knieke%2C+C">Christoph Knieke</a>, 
<a href="/search/cs?searchtype=author&query=Nyakam%2C+E">Eric Nyakam</a>, 
<a href="/search/cs?searchtype=author&query=Rausch%2C+A">Andreas Rausch</a>, 
<a href="/search/cs?searchtype=author&query=Schindler%2C+C">Christian Schindler</a>, 
<a href="/search/cs?searchtype=author&query=Bartelt%2C+C">Christian Bartelt</a>, 
<a href="/search/cs?searchtype=author&query=Ziebura%2C+N">Nikolaus Ziebura</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper was presented on The Fifteenth International Conference on Adaptive and Self-Adaptive Systems and Applications (ADAPTIVE 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item1180">[1180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08630" title="Abstract">arXiv:2308.08630</a> (replaced) [<a href="/pdf/2308.08630" title="Download PDF">pdf</a>, <a href="/ps/2308.08630" title="Download PostScript">ps</a>, <a href="/format/2308.08630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cooperation and interdependence in global science funding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Miao%2C+L">Lili Miao</a>, 
<a href="/search/econ?searchtype=author&query=Larivi%C3%A8re%2C+V">Vincent Larivi&#xe8;re</a>, 
<a href="/search/econ?searchtype=author&query=Wang%2C+F">Feifei Wang</a>, 
<a href="/search/econ?searchtype=author&query=Ahn%2C+Y">Yong-Yeol Ahn</a>, 
<a href="/search/econ?searchtype=author&query=Sugimoto%2C+C+R">Cassidy R. Sugimoto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 48 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Economics (econ.GN)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item1181">[1181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10058" title="Abstract">arXiv:2308.10058</a> (replaced) [<a href="/pdf/2308.10058" title="Download PDF">pdf</a>, <a href="/ps/2308.10058" title="Download PostScript">ps</a>, <a href="/format/2308.10058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> R-C-P Method: An Autonomous Volume Calculation Method Using Image  Processing and Machine Vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Muktadir%2C+M">MA Muktadir</a>, 
<a href="/search/cs?searchtype=author&query=Parker%2C+S">Sydney Parker</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+S">Sun Yi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1182">[1182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10699" title="Abstract">arXiv:2308.10699</a> (replaced) [<a href="/pdf/2308.10699" title="Download PDF">pdf</a>, <a href="/format/2308.10699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cost-Efficient Online Decision Making: A Combinatorial Multi-Armed  Bandit Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahbar%2C+A">Arman Rahbar</a>, 
<a href="/search/cs?searchtype=author&query=%C3%85kerblom%2C+N">Niklas &#xc5;kerblom</a>, 
<a href="/search/cs?searchtype=author&query=Chehreghani%2C+M+H">Morteza Haghir Chehreghani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1183">[1183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11129" title="Abstract">arXiv:2308.11129</a> (replaced) [<a href="/pdf/2308.11129" title="Download PDF">pdf</a>, <a href="/format/2308.11129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Graph Transformers with Hierarchical Distance Structural  Encoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yuankai Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item1184">[1184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11767" title="Abstract">arXiv:2308.11767</a> (replaced) [<a href="/pdf/2308.11767" title="Download PDF">pdf</a>, <a href="/format/2308.11767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detection of ChatGPT Fake Science with the xFakeBibs Learning Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hamed%2C+A+A">Ahmed Abdeen Hamed</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xindong Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 6 figures, 4 tables, 2 algorithms
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item1185">[1185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12038" title="Abstract">arXiv:2308.12038</a> (replaced) [<a href="/pdf/2308.12038" title="Download PDF">pdf</a>, <a href="/format/2308.12038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Multilingual Models Pivot Zero-Shot Multimodal Learning across  Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jinyi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuan Yao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chongyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yinxu Pan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qianyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tianyu Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hanghao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yue Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haoye Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yankai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+J">Jiao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dahai Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://github.com/OpenBMB/VisCPM.git">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1186">[1186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12252" title="Abstract">arXiv:2308.12252</a> (replaced) [<a href="/pdf/2308.12252" title="Download PDF">pdf</a>, <a href="/format/2308.12252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Safe Am I Given What I See? Calibrated Prediction of Safety Chances  for Image-Controlled Autonomy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+Z">Zhenjiang Mao</a>, 
<a href="/search/cs?searchtype=author&query=Sobolewski%2C+C">Carson Sobolewski</a>, 
<a href="/search/cs?searchtype=author&query=Ruchkin%2C+I">Ivan Ruchkin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1187">[1187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12420" title="Abstract">arXiv:2308.12420</a> (replaced) [<a href="/pdf/2308.12420" title="Download PDF">pdf</a>, <a href="/format/2308.12420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hernandez%2C+W">Walter Hernandez</a>, 
<a href="/search/cs?searchtype=author&query=Tylinski%2C+K">Kamil Tylinski</a>, 
<a href="/search/cs?searchtype=author&query=Moore%2C+A">Alastair Moore</a>, 
<a href="/search/cs?searchtype=author&query=Roche%2C+N">Niall Roche</a>, 
<a href="/search/cs?searchtype=author&query=Vadgama%2C+N">Nikhil Vadgama</a>, 
<a href="/search/cs?searchtype=author&query=Treiblmaier%2C+H">Horst Treiblmaier</a>, 
<a href="/search/cs?searchtype=author&query=Shangguan%2C+J">Jiangbo Shangguan</a>, 
<a href="/search/cs?searchtype=author&query=Tasca%2C+P">Paolo Tasca</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiahua Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1188">[1188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12714" title="Abstract">arXiv:2308.12714</a> (replaced) [<a href="/pdf/2308.12714" title="Download PDF">pdf</a>, <a href="/format/2308.12714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VIGC: Visual Instruction Generation and Correction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiao Han</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+J">Jiahui Peng</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+H">Huaping Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xiaoyi Dong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weijia Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+C">Conghui He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2024, Project Website: <a href="https://opendatalab.github.io/VIGC">this https URL</a>, Code and Pretrained Model: <a href="https://github.com/opendatalab/VIGC">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1189">[1189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13320" title="Abstract">arXiv:2308.13320</a> (replaced) [<a href="/pdf/2308.13320" title="Download PDF">pdf</a>, <a href="/format/2308.13320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-tuning can cripple your foundation model; preserving features may  be the solution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mukhoti%2C+J">Jishnu Mukhoti</a>, 
<a href="/search/cs?searchtype=author&query=Gal%2C+Y">Yarin Gal</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P+H+S">Philip H.S. Torr</a>, 
<a href="/search/cs?searchtype=author&query=Dokania%2C+P+K">Puneet K. Dokania</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1190">[1190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14731" title="Abstract">arXiv:2308.14731</a> (replaced) [<a href="/pdf/2308.14731" title="Download PDF">pdf</a>, <a href="/format/2308.14731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distilled GPT for Source Code Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+C">Chia-Yi Su</a>, 
<a href="/search/cs?searchtype=author&query=McMillan%2C+C">Collin McMillan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages + 6 figures. Accepted to Automated Software Engineering Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1191">[1191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14920" title="Abstract">arXiv:2308.14920</a> (replaced) [<a href="/pdf/2308.14920" title="Download PDF">pdf</a>, <a href="/format/2308.14920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Matbench Discovery -- A framework to evaluate machine learning crystal  stability predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Riebesell%2C+J">Janosh Riebesell</a>, 
<a href="/search/cond-mat?searchtype=author&query=Goodall%2C+R+E+A">Rhys E. A. Goodall</a>, 
<a href="/search/cond-mat?searchtype=author&query=Benner%2C+P">Philipp Benner</a>, 
<a href="/search/cond-mat?searchtype=author&query=Chiang%2C+Y">Yuan Chiang</a>, 
<a href="/search/cond-mat?searchtype=author&query=Deng%2C+B">Bowen Deng</a>, 
<a href="/search/cond-mat?searchtype=author&query=Lee%2C+A+A">Alpha A. Lee</a>, 
<a href="/search/cond-mat?searchtype=author&query=Jain%2C+A">Anubhav Jain</a>, 
<a href="/search/cond-mat?searchtype=author&query=Persson%2C+K+A">Kristin A. Persson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 18 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1192">[1192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15154" title="Abstract">arXiv:2308.15154</a> (replaced) [<a href="/pdf/2308.15154" title="Download PDF">pdf</a>, <a href="/ps/2308.15154" title="Download PostScript">ps</a>, <a href="/format/2308.15154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Anatomy of Conspirators: Unveiling Traits using a Comprehensive  Twitter Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gambini%2C+M">Margherita Gambini</a>, 
<a href="/search/cs?searchtype=author&query=Tardelli%2C+S">Serena Tardelli</a>, 
<a href="/search/cs?searchtype=author&query=Tesconi%2C+M">Maurizio Tesconi</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computer Communications, 217 (2024), 25-40
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1193">[1193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15828" title="Abstract">arXiv:2308.15828</a> (replaced) [<a href="/pdf/2308.15828" title="Download PDF">pdf</a>, <a href="/format/2308.15828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Round-Trip Energy Efficiency and Energy-Efficiency Fade Estimation for  Battery Passport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Beckers%2C+C">Camiel Beckers</a> (1), 
<a href="/search/eess?searchtype=author&query=Hoedemaekers%2C+E">Erik Hoedemaekers</a> (1), 
<a href="/search/eess?searchtype=author&query=Dagkilic%2C+A">Arda Dagkilic</a> (2), 
<a href="/search/eess?searchtype=author&query=Bergveld%2C+H+J">Henk Jan Bergveld</a> (3 and 4) ((1) TNO - Powertrains Dept., (2) VDL Enabling Transport Solutions, (3) Eindhoven University of Technology - Dept. of Electrical Engineering, (4) NXP Semiconductors)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, presented at VPPC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item1194">[1194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16262" title="Abstract">arXiv:2308.16262</a> (replaced) [<a href="/pdf/2308.16262" title="Download PDF">pdf</a>, <a href="/format/2308.16262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Strategic Learning with Competitive Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vo%2C+K+Q+H">Kiet Q. H. Vo</a>, 
<a href="/search/cs?searchtype=author&query=Aadil%2C+M">Muneeb Aadil</a>, 
<a href="/search/cs?searchtype=author&query=Chau%2C+S+L">Siu Lun Chau</a>, 
<a href="/search/cs?searchtype=author&query=Muandet%2C+K">Krikamol Muandet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Added more discussions on assumptions and the algorithm, and expand the Conclusion
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1195">[1195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16617" title="Abstract">arXiv:2308.16617</a> (replaced) [<a href="/pdf/2308.16617" title="Download PDF">pdf</a>, <a href="/format/2308.16617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bi-level iterative regularization for inverse problems in nonlinear PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Nguyen%2C+T+T+N">Tram Thi Ngoc Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1196">[1196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16681" title="Abstract">arXiv:2308.16681</a> (replaced) [<a href="/pdf/2308.16681" title="Download PDF">pdf</a>, <a href="/format/2308.16681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One Model Many Scores: Using Multiverse Analysis to Prevent Fairness  Hacking and Evaluate the Influence of Model Design Decisions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Simson%2C+J">Jan Simson</a>, 
<a href="/search/stat?searchtype=author&query=Pfisterer%2C+F">Florian Pfisterer</a>, 
<a href="/search/stat?searchtype=author&query=Kern%2C+C">Christoph Kern</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1197">[1197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01098" title="Abstract">arXiv:2309.01098</a> (replaced) [<a href="/pdf/2309.01098" title="Download PDF">pdf</a>, <a href="/format/2309.01098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> martFL: Enabling Utility-Driven Data Marketplace with a Robust and  Verifiable Federated Learning Architecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qi Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhuotao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qi Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Ke Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Another version of this paper is published in the proceedings of ACM Conference on Computer and Communications Security (CCS) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item1198">[1198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01365" title="Abstract">arXiv:2309.01365</a> (replaced) [<a href="/pdf/2309.01365" title="Download PDF">pdf</a>, <a href="/format/2309.01365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Refined Temporal Pyramidal Compression-and-Amplification Transformer for  3D Human Pose Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hanbing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+W">Wangmeng Xiang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jun-Yan He</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zhi-Qi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+B">Bin Luo</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+Y">Yifeng Geng</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xuansong Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1199">[1199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01922" title="Abstract">arXiv:2309.01922</a> (replaced) [<a href="/pdf/2309.01922" title="Download PDF">pdf</a>, <a href="/ps/2309.01922" title="Download PostScript">ps</a>, <a href="/format/2309.01922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regret Analysis of Policy Gradient Algorithm for Infinite Horizon  Average Reward Markov Decision Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+Q">Qinbo Bai</a>, 
<a href="/search/cs?searchtype=author&query=Mondal%2C+W+U">Washim Uddin Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+V">Vaneet Aggarwal</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1200">[1200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03162" title="Abstract">arXiv:2309.03162</a> (replaced) [<a href="/pdf/2309.03162" title="Download PDF">pdf</a>, <a href="/format/2309.03162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Line-Separable Unit-Disk Coverage and Related Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Gang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haitao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This version improves the results in the previous version. The algorithm idea is the same as before, but this version provides a more efficient implementation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item1201">[1201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05649" title="Abstract">arXiv:2309.05649</a> (replaced) [<a href="/pdf/2309.05649" title="Download PDF">pdf</a>, <a href="/ps/2309.05649" title="Download PostScript">ps</a>, <a href="/format/2309.05649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data efficiency, dimensionality reduction, and the generalized symmetric  information bottleneck
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Martini%2C+K+M">K. Michael Martini</a>, 
<a href="/search/cs?searchtype=author&query=Nemenman%2C+I">Ilya Nemenman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Statistical Mechanics (cond-mat.stat-mech); Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an)

</div>
</div>
</dd>
<dt><a name="item1202">[1202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06256" title="Abstract">arXiv:2309.06256</a> (replaced) [<a href="/pdf/2309.06256" title="Download PDF">pdf</a>, <a href="/format/2309.06256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating the Alignment Tax of RLHF
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yong Lin</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hangyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+W">Wei Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Diao%2C+S">Shizhe Diao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jianmeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+R">Rui Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoxiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wenbin Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanning Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Hanze Dong</a>, 
<a href="/search/cs?searchtype=author&query=Pi%2C+R">Renjie Pi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Han Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+N">Nan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuan Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1203">[1203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06364" title="Abstract">arXiv:2309.06364</a> (replaced) [<a href="/pdf/2309.06364" title="Download PDF">pdf</a>, <a href="/format/2309.06364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Framework-Based Qualitative Analysis of Free Responses of Large Language  Models: Algorithmic Fidelity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amirova%2C+A">Aliya Amirova</a>, 
<a href="/search/cs?searchtype=author&query=Fteropoulli%2C+T">Theodora Fteropoulli</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+N">Nafiso Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Cowie%2C+M+R">Martin R. Cowie</a>, 
<a href="/search/cs?searchtype=author&query=Leibo%2C+J+Z">Joel Z. Leibo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 52 pages, 5 tables, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1204">[1204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06642" title="Abstract">arXiv:2309.06642</a> (replaced) [<a href="/pdf/2309.06642" title="Download PDF">pdf</a>, <a href="/format/2309.06642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adapt and Diffuse: Sample-adaptive Reconstruction via Latent Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fabian%2C+Z">Zalan Fabian</a>, 
<a href="/search/eess?searchtype=author&query=Tinaz%2C+B">Berk Tinaz</a>, 
<a href="/search/eess?searchtype=author&query=Soltanolkotabi%2C+M">Mahdi Soltanolkotabi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 21 figures, preliminary version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1205">[1205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06847" title="Abstract">arXiv:2309.06847</a> (replaced) [<a href="/pdf/2309.06847" title="Download PDF">pdf</a>, <a href="/ps/2309.06847" title="Download PostScript">ps</a>, <a href="/format/2309.06847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Undetectable Selfish Mining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bahrani%2C+M">Maryam Bahrani</a>, 
<a href="/search/cs?searchtype=author&query=Weinberg%2C+S+M">S. Matthew Weinberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item1206">[1206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06902" title="Abstract">arXiv:2309.06902</a> (replaced) [<a href="/pdf/2309.06902" title="Download PDF">pdf</a>, <a href="/format/2309.06902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CCSPNet-Joint: Efficient Joint Training Method for Traffic Sign  Detection Under Extreme Conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+H">Haoqin Hong</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yue Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+X">Xiangyu Shu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiaofang Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1207">[1207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06991" title="Abstract">arXiv:2309.06991</a> (replaced) [<a href="/pdf/2309.06991" title="Download PDF">pdf</a>, <a href="/format/2309.06991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Contrast-Consistent Ranking with Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stoehr%2C+N">Niklas Stoehr</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+P">Pengxiang Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Preotiuc-Pietro%2C+D">Daniel Preotiuc-Pietro</a>, 
<a href="/search/cs?searchtype=author&query=Bhowmik%2C+R">Rajarshi Bhowmik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Long Paper at EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1208">[1208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07002" title="Abstract">arXiv:2309.07002</a> (replaced) [<a href="/pdf/2309.07002" title="Download PDF">pdf</a>, <a href="/format/2309.07002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Evolutionary Algorithms to Find Cache-Friendly Generalized Morton  Layouts for Arrays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Swatman%2C+S+N">Stephen Nicholas Swatman</a>, 
<a href="/search/cs?searchtype=author&query=Varbanescu%2C+A">Ana-Lucia Varbanescu</a>, 
<a href="/search/cs?searchtype=author&query=Pimentel%2C+A+D">Andy D. Pimentel</a>, 
<a href="/search/cs?searchtype=author&query=Salzburger%2C+A">Andreas Salzburger</a>, 
<a href="/search/cs?searchtype=author&query=Krasznahorkay%2C+A">Attila Krasznahorkay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Minor textual changes, update of Figure 3 to incorporate a bugfix, notational changes to Table 1, Figure 4, Table 2 (no effect on veracity), addition of artifact information
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item1209">[1209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07445" title="Abstract">arXiv:2309.07445</a> (replaced) [<a href="/pdf/2309.07445" title="Download PDF">pdf</a>, <a href="/format/2309.07445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic  Classification in 200+ Languages and Dialects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adelani%2C+D+I">David Ifeoluwa Adelani</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hannah Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xiaoyu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Vassilyev%2C+N">Nikita Vassilyev</a>, 
<a href="/search/cs?searchtype=author&query=Alabi%2C+J+O">Jesujoba O. Alabi</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yanke Mao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Haonan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+A+E">Annie En-Shiun Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024 (main conference)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1210">[1210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07794" title="Abstract">arXiv:2309.07794</a> (replaced) [<a href="/pdf/2309.07794" title="Download PDF">pdf</a>, <a href="/format/2309.07794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Multimodal Classification of Social Media Posts by Leveraging  Image-Text Auxiliary Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Villegas%2C+D+S">Danae S&#xe1;nchez Villegas</a>, 
<a href="/search/cs?searchtype=author&query=Preo%C5%A3iuc-Pietro%2C+D">Daniel Preo&#x163;iuc-Pietro</a>, 
<a href="/search/cs?searchtype=author&query=Aletras%2C+N">Nikolaos Aletras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EACL 2024 Findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item1211">[1211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08777" title="Abstract">arXiv:2309.08777</a> (replaced) [<a href="/pdf/2309.08777" title="Download PDF">pdf</a>, <a href="/format/2309.08777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-training Strategies for Sentiment Analysis: An Empirical Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haochen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Rallabandi%2C+S+K">Sai Krishna Rallabandi</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yijing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Dakle%2C+P+P">Parag Pravin Dakle</a>, 
<a href="/search/cs?searchtype=author&query=Raghavan%2C+P">Preethi Raghavan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EACL Findings 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1212">[1212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08889" title="Abstract">arXiv:2309.08889</a> (replaced) [<a href="/pdf/2309.08889" title="Download PDF">pdf</a>, <a href="/format/2309.08889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SafeShift: Safety-Informed Distribution Shifts for Robust Trajectory  Prediction in Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stoler%2C+B">Benjamin Stoler</a>, 
<a href="/search/cs?searchtype=author&query=Navarro%2C+I">Ingrid Navarro</a>, 
<a href="/search/cs?searchtype=author&query=Jana%2C+M">Meghdeep Jana</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S">Soonmin Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Francis%2C+J">Jonathan Francis</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+J">Jean Oh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1213">[1213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08969" title="Abstract">arXiv:2309.08969</a> (replaced) [<a href="/pdf/2309.08969" title="Download PDF">pdf</a>, <a href="/format/2309.08969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking STS and NLI in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuxia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Minghan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Nakov%2C+P">Preslav Nakov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2212.13138">arXiv:2212.13138</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1214">[1214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08999" title="Abstract">arXiv:2309.08999</a> (replaced) [<a href="/pdf/2309.08999" title="Download PDF">pdf</a>, <a href="/format/2309.08999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context-aware Adversarial Attack on Named Entity Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shuguang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Neves%2C+L">Leonardo Neves</a>, 
<a href="/search/cs?searchtype=author&query=Solorio%2C+T">Thamar Solorio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to W-NUT at EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1215">[1215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09357" title="Abstract">arXiv:2309.09357</a> (replaced) [<a href="/pdf/2309.09357" title="Download PDF">pdf</a>, <a href="/format/2309.09357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Talk2Care: Facilitating Asynchronous Patient-Provider Communication with  Large-Language-Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Ziqi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xuhai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+B">Bingsheng Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Rogers%2C+E">Ethan Rogers</a>, 
<a href="/search/cs?searchtype=author&query=Intille%2C+S">Stephen Intille</a>, 
<a href="/search/cs?searchtype=author&query=Shara%2C+N">Nawar Shara</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+G+G">Guodong Gordon Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dakuo Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under submission to IMWUT'23, 26 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item1216">[1216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09582" title="Abstract">arXiv:2309.09582</a> (replaced) [<a href="/pdf/2309.09582" title="Download PDF">pdf</a>, <a href="/format/2309.09582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fabricator: An Open Source Toolkit for Generating Labeled Training Data  with Teacher LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Golde%2C+J">Jonas Golde</a>, 
<a href="/search/cs?searchtype=author&query=Haller%2C+P">Patrick Haller</a>, 
<a href="/search/cs?searchtype=author&query=Hamborg%2C+F">Felix Hamborg</a>, 
<a href="/search/cs?searchtype=author&query=Risch%2C+J">Julian Risch</a>, 
<a href="/search/cs?searchtype=author&query=Akbik%2C+A">Alan Akbik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 Figures and 2 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1217">[1217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09944" title="Abstract">arXiv:2309.09944</a> (replaced) [<a href="/pdf/2309.09944" title="Download PDF">pdf</a>, <a href="/format/2309.09944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffusionWorldViewer: Exposing and Broadening the Worldview Reflected by  Generative Text-to-Image Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=De+Simone%2C+Z">Zoe De Simone</a>, 
<a href="/search/cs?searchtype=author&query=Boggust%2C+A">Angie Boggust</a>, 
<a href="/search/cs?searchtype=author&query=Satyanarayan%2C+A">Arvind Satyanarayan</a>, 
<a href="/search/cs?searchtype=author&query=Wilson%2C+A">Ashia Wilson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1218">[1218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10426" title="Abstract">arXiv:2309.10426</a> (replaced) [<a href="/pdf/2309.10426" title="Download PDF">pdf</a>, <a href="/format/2309.10426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Object Graph Affordance Network: Enabling Goal-Oriented Planning  through Compound Object Affordances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Girgin%2C+T">Tuba Girgin</a>, 
<a href="/search/cs?searchtype=author&query=Ugur%2C+E">Emre Ugur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. Submitted to Robotics and Automation Letters on February 2, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1219">[1219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10456" title="Abstract">arXiv:2309.10456</a> (replaced) [<a href="/pdf/2309.10456" title="Download PDF">pdf</a>, <a href="/format/2309.10456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Speaker Diarization using Semantic Information: Joint Pairwise  Constraints Propagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Luyao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Siqi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qinglin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yafeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shiliang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1220">[1220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10625" title="Abstract">arXiv:2309.10625</a> (replaced) [<a href="/pdf/2309.10625" title="Download PDF">pdf</a>, <a href="/format/2309.10625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NoisyNN: Exploring the Influence of Information Entropy Change in  Learning Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xiaowei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhe Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Y">Yao Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Li Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+D">Dajiang Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Information Entropy, NoisyNN, ViT, CNN
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1221">[1221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11243" title="Abstract">arXiv:2309.11243</a> (replaced) [<a href="/pdf/2309.11243" title="Download PDF">pdf</a>, <a href="/format/2309.11243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Minimum Processing Beamforming and Near-end Listening Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fuglsig%2C+A+J">Andreas J. Fuglsig</a>, 
<a href="/search/eess?searchtype=author&query=Jensen%2C+J">Jesper Jensen</a>, 
<a href="/search/eess?searchtype=author&query=Tan%2C+Z">Zheng-Hua Tan</a>, 
<a href="/search/eess?searchtype=author&query=Bertelsen%2C+L+S">Lars S. Bertelsen</a>, 
<a href="/search/eess?searchtype=author&query=Lindof%2C+J+C">Jens Christian Lindof</a>, 
<a href="/search/eess?searchtype=author&query=%C3%98stergaard%2C+J">Jan &#xd8;stergaard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at IEEE ICASSP 2024 Workshop on Hands-free Speech Communication and Microphone Arrays (HSCMA) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item1222">[1222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11864" title="Abstract">arXiv:2309.11864</a> (replaced) [<a href="/pdf/2309.11864" title="Download PDF">pdf</a>, <a href="/ps/2309.11864" title="Download PostScript">ps</a>, <a href="/format/2309.11864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Golub-Welsch version for simultaneous Gaussian quadrature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Van+Assche%2C+W">Walter Van Assche</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Classical Analysis and ODEs (math.CA)

</div>
</div>
</dd>
<dt><a name="item1223">[1223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12476" title="Abstract">arXiv:2309.12476</a> (replaced) [<a href="/pdf/2309.12476" title="Download PDF">pdf</a>, <a href="/format/2309.12476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Reward Functions for Markov Decision Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Benvenuti%2C+A">Alexander Benvenuti</a>, 
<a href="/search/eess?searchtype=author&query=Hawkins%2C+C">Calvin Hawkins</a>, 
<a href="/search/eess?searchtype=author&query=Fallin%2C+B">Brandon Fallin</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+B">Bo Chen</a>, 
<a href="/search/eess?searchtype=author&query=Bialy%2C+B">Brendan Bialy</a>, 
<a href="/search/eess?searchtype=author&query=Dennis%2C+M">Miriam Dennis</a>, 
<a href="/search/eess?searchtype=author&query=Hale%2C+M">Matthew Hale</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 Pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item1224">[1224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13567" title="Abstract">arXiv:2309.13567</a> (replaced) [<a href="/pdf/2309.13567" title="Download PDF">pdf</a>, <a href="/format/2309.13567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MentaLLaMA: Interpretable Mental Health Analysis on Social Media with  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kailai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianlin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+Z">Ziyan Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qianqian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jimin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ananiadou%2C+S">Sophia Ananiadou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1225">[1225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13672" title="Abstract">arXiv:2309.13672</a> (replaced) [<a href="/pdf/2309.13672" title="Download PDF">pdf</a>, <a href="/format/2309.13672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Reinforcement Learning for Image-to-Image Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Ziwei Luo</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jing Hu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Chengming Feng</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+S">Siwei Lyu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1226">[1226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13950" title="Abstract">arXiv:2309.13950</a> (replaced) [<a href="/pdf/2309.13950" title="Download PDF">pdf</a>, <a href="/format/2309.13950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local and Global Trend Bayesian Exponential Smoothing Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Smyl%2C+S">Slawek Smyl</a>, 
<a href="/search/cs?searchtype=author&query=Bergmeir%2C+C">Christoph Bergmeir</a>, 
<a href="/search/cs?searchtype=author&query=Dokumentov%2C+A">Alexander Dokumentov</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+X">Xueying Long</a>, 
<a href="/search/cs?searchtype=author&query=Wibowo%2C+E">Erwin Wibowo</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+D">Daniel Schmidt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1227">[1227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14016" title="Abstract">arXiv:2309.14016</a> (replaced) [<a href="/pdf/2309.14016" title="Download PDF">pdf</a>, <a href="/format/2309.14016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Virtuoso: High Resource Utilization and &#x3bc;s-scale Performance  Isolation in a Shared Virtual Machine TCP Network Stack
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stolet%2C+M">Matheus Stolet</a>, 
<a href="/search/cs?searchtype=author&query=Arzola%2C+L">Liam Arzola</a>, 
<a href="/search/cs?searchtype=author&query=Peter%2C+S">Simon Peter</a>, 
<a href="/search/cs?searchtype=author&query=Kaufmann%2C+A">Antoine Kaufmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under submission for conference peer review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Operating Systems (cs.OS)

</div>
</div>
</dd>
<dt><a name="item1228">[1228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15136" title="Abstract">arXiv:2309.15136</a> (replaced) [<a href="/pdf/2309.15136" title="Download PDF">pdf</a>, <a href="/format/2309.15136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A multi-modal approach for identifying schizophrenia using cross-modal  attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Premananth%2C+G">Gowtham Premananth</a>, 
<a href="/search/eess?searchtype=author&query=Siriwardena%2C+Y+M">Yashish M.Siriwardena</a>, 
<a href="/search/eess?searchtype=author&query=Resnik%2C+P">Philip Resnik</a>, 
<a href="/search/eess?searchtype=author&query=Espy-Wilson%2C+C">Carol Espy-Wilson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Annual International Conference of the IEEE Engineering in Medicine and Biology Society 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Multimedia (cs.MM); Sound (cs.SD); Audio and Speech Processing (eess.AS); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item1229">[1229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15395" title="Abstract">arXiv:2309.15395</a> (replaced) [<a href="/pdf/2309.15395" title="Download PDF">pdf</a>, <a href="/format/2309.15395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-Free, Regret-Optimal Best Policy Identification in Online CMDPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zihan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Honghao Wei</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+L">Lei Ying</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1230">[1230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16519" title="Abstract">arXiv:2309.16519</a> (replaced) [<a href="/pdf/2309.16519" title="Download PDF">pdf</a>, <a href="/format/2309.16519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AtomSurf : Surface Representation for Learning on Protein Structures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mallet%2C+V">Vincent Mallet</a>, 
<a href="/search/cs?searchtype=author&query=Attaiki%2C+S">Souhaib Attaiki</a>, 
<a href="/search/cs?searchtype=author&query=Ovsjanikov%2C+M">Maks Ovsjanikov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Biomolecules (q-bio.BM)

</div>
</div>
</dd>
<dt><a name="item1231">[1231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16965" title="Abstract">arXiv:2309.16965</a> (replaced) [<a href="/pdf/2309.16965" title="Download PDF">pdf</a>, <a href="/format/2309.16965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controlling Continuous Relaxation for Combinatorial Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ichikawa%2C+Y">Yuma Ichikawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item1232">[1232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17102" title="Abstract">arXiv:2309.17102</a> (replaced) [<a href="/pdf/2309.17102" title="Download PDF">pdf</a>, <a href="/format/2309.17102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guiding Instruction-based Image Editing via Multimodal Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+T">Tsu-Jui Fu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wenze Hu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+X">Xianzhi Du</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yinfei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+Z">Zhe Gan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR'24 (Spotlight) ; Project at <a href="https://mllm-ie.github.io">this https URL</a> ; Code at <a href="https://github.com/tsujuifu/pytorch_mgie">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1233">[1233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17196" title="Abstract">arXiv:2309.17196</a> (replaced) [<a href="/pdf/2309.17196" title="Download PDF">pdf</a>, <a href="/format/2309.17196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ResBit: Residual Bit Vector for Categorical Values
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fuchi%2C+M">Masane Fuchi</a>, 
<a href="/search/cs?searchtype=author&query=Zanashir%2C+A">Amar Zanashir</a>, 
<a href="/search/cs?searchtype=author&query=Minami%2C+H">Hiroto Minami</a>, 
<a href="/search/cs?searchtype=author&query=Takagi%2C+T">Tomohiro Takagi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20pages, 7 figures, 24 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1234">[1234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00229" title="Abstract">arXiv:2310.00229</a> (replaced) [<a href="/pdf/2310.00229" title="Download PDF">pdf</a>, <a href="/format/2310.00229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consciousness-Inspired Spatio-Temporal Abstractions for Better  Generalization in Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+M">Mingde Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Alver%2C+S">Safa Alver</a>, 
<a href="/search/cs?searchtype=author&query=van+Seijen%2C+H">Harm van Seijen</a>, 
<a href="/search/cs?searchtype=author&query=Laroche%2C+R">Romain Laroche</a>, 
<a href="/search/cs?searchtype=author&query=Precup%2C+D">Doina Precup</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> updated post-acceptance version for ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1235">[1235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01812" title="Abstract">arXiv:2310.01812</a> (replaced) [<a href="/pdf/2310.01812" title="Download PDF">pdf</a>, <a href="/format/2310.01812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PPT: Token Pruning and Pooling for Efficient Vision Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xinjian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+F">Fanhu Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiudong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinghao Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1236">[1236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01886" title="Abstract">arXiv:2310.01886</a> (replaced) [<a href="/pdf/2310.01886" title="Download PDF">pdf</a>, <a href="/format/2310.01886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BYOM: Building Your Own Multi-Task Model For Free
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Weisen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Baijiong Lin</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Han Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Kwok%2C+J+T">James T. Kwok</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1237">[1237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02025" title="Abstract">arXiv:2310.02025</a> (replaced) [<a href="/pdf/2310.02025" title="Download PDF">pdf</a>, <a href="/format/2310.02025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Aochuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yimeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jinghan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Diffenderfer%2C+J">James Diffenderfer</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiancheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Parasyris%2C+K">Konstantinos Parasyris</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yihua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kailkhura%2C+B">Bhavya Kailkhura</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sijia Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1238">[1238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02054" title="Abstract">arXiv:2310.02054</a> (replaced) [<a href="/pdf/2310.02054" title="Download PDF">pdf</a>, <a href="/format/2310.02054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable  Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zibin Dong</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yifu Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+J">Jianye Hao</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+F">Fei Ni</a>, 
<a href="/search/cs?searchtype=author&query=Mu%2C+Y">Yao Mu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yujing Hu</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+T">Tangjie Lv</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+C">Changjie Fan</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhipeng Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1239">[1239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02299" title="Abstract">arXiv:2310.02299</a> (replaced) [<a href="/pdf/2310.02299" title="Download PDF">pdf</a>, <a href="/format/2310.02299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering Symmetry Breaking in Physical Systems with Relaxed Group  Convolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hofgard%2C+E">Elyssa Hofgard</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Han Gao</a>, 
<a href="/search/cs?searchtype=author&query=Walters%2C+R">Robin Walters</a>, 
<a href="/search/cs?searchtype=author&query=Smidt%2C+T+E">Tess E.Smidt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1240">[1240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02698" title="Abstract">arXiv:2310.02698</a> (replaced) [<a href="/pdf/2310.02698" title="Download PDF">pdf</a>, <a href="/format/2310.02698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhanced Federated Optimization: Adaptive Unbiased Sampling with Reduced  Variance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+D">Dun Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zenglin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yu Pan</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiaoying Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1241">[1241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02702" title="Abstract">arXiv:2310.02702</a> (replaced) [<a href="/pdf/2310.02702" title="Download PDF">pdf</a>, <a href="/format/2310.02702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tackling Hybrid Heterogeneity on Federated Optimization via Gradient  Diversity Maximization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+D">Dun Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zenglin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yu Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiaoying Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1242">[1242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02823" title="Abstract">arXiv:2310.02823</a> (replaced) [<a href="/pdf/2310.02823" title="Download PDF">pdf</a>, <a href="/format/2310.02823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Scale Logits for Temperature-Conditional GFlowNets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Minsu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+J">Joohwan Ko</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+T">Taeyoung Yun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dinghuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Ling Pan</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+W">Woochang Kim</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jinkyoo Park</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+E">Emmanuel Bengio</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 21 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1243">[1243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03052" title="Abstract">arXiv:2310.03052</a> (replaced) [<a href="/pdf/2310.03052" title="Download PDF">pdf</a>, <a href="/format/2310.03052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memoria: Resolving Fateful Forgetting Problem through Human-Inspired  Memory Architecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Sangjun Park</a>, 
<a href="/search/cs?searchtype=author&query=Bak%2C+J">JinYeong Bak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. 27 pages, 13 figures, 10 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item1244">[1244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03512" title="Abstract">arXiv:2310.03512</a> (replaced) [<a href="/pdf/2310.03512" title="Download PDF">pdf</a>, <a href="/format/2310.03512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Otago Exercises Monitoring for Older Adults by a Single IMU and  Hierarchical Machine Learning Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shang%2C+M">Meng Shang</a>, 
<a href="/search/cs?searchtype=author&query=Dedeyne%2C+L">Lenore Dedeyne</a>, 
<a href="/search/cs?searchtype=author&query=Dupont%2C+J">Jolan Dupont</a>, 
<a href="/search/cs?searchtype=author&query=Vercauteren%2C+L">Laura Vercauteren</a>, 
<a href="/search/cs?searchtype=author&query=Amini%2C+N">Nadjia Amini</a>, 
<a href="/search/cs?searchtype=author&query=Lapauw%2C+L">Laurence Lapauw</a>, 
<a href="/search/cs?searchtype=author&query=Gielen%2C+E">Evelien Gielen</a>, 
<a href="/search/cs?searchtype=author&query=Verschueren%2C+S">Sabine Verschueren</a>, 
<a href="/search/cs?searchtype=author&query=Varon%2C+C">Carolina Varon</a>, 
<a href="/search/cs?searchtype=author&query=De+Raedt%2C+W">Walter De Raedt</a>, 
<a href="/search/cs?searchtype=author&query=Vanrumste%2C+B">Bart Vanrumste</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Neural Systems and Rehabilitation
  Engineering, vol. 32, pp. 462-471, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1245">[1245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04861" title="Abstract">arXiv:2310.04861</a> (replaced) [<a href="/pdf/2310.04861" title="Download PDF">pdf</a>, <a href="/format/2310.04861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncovering hidden geometry in Transformers via disentangling position  and context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jiajun Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yiqiao Zhong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 34 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1246">[1246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05212" title="Abstract">arXiv:2310.05212</a> (replaced) [<a href="/pdf/2310.05212" title="Download PDF">pdf</a>, <a href="/format/2310.05212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Semiotics Networks Representing Awareness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kupeev%2C+D">David Kupeev</a>, 
<a href="/search/cs?searchtype=author&query=Nitcany%2C+E">Eyal Nitcany</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item1247">[1247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05401" title="Abstract">arXiv:2310.05401</a> (replaced) [<a href="/pdf/2310.05401" title="Download PDF">pdf</a>, <a href="/format/2310.05401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entropy-MCMC: Sampling from Flat Basins with Ease
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bolian Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruqi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1248">[1248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05453" title="Abstract">arXiv:2310.05453</a> (replaced) [<a href="/pdf/2310.05453" title="Download PDF">pdf</a>, <a href="/format/2310.05453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lai%2C+Y">Yuxiang Lai</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yi Zhou</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinghong Liu</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tao Zhou</a> (3) ((1) School of Computer Science and Engineering, Southeast University, China (2) Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China (3) School of Computer Science and Engineering, Nanjing University of Science and Technology, China)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1249">[1249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05707" title="Abstract">arXiv:2310.05707</a> (replaced) [<a href="/pdf/2310.05707" title="Download PDF">pdf</a>, <a href="/format/2310.05707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guiding Language Model Math Reasoning with Planning Tokens
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Caccia%2C+L">Lucas Caccia</a>, 
<a href="/search/cs?searchtype=author&query=Ostapenko%2C+O">Oleksiy Ostapenko</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xingdi Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sordoni%2C+A">Alessandro Sordoni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1250">[1250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06112" title="Abstract">arXiv:2310.06112</a> (replaced) [<a href="/pdf/2310.06112" title="Download PDF">pdf</a>, <a href="/format/2310.06112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+S">Shaopeng Fu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Di Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Twelfth International Conference on Learning Representations (ICLR 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1251">[1251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06187" title="Abstract">arXiv:2310.06187</a> (replaced) [<a href="/pdf/2310.06187" title="Download PDF">pdf</a>, <a href="/ps/2310.06187" title="Download PostScript">ps</a>, <a href="/format/2310.06187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quasi-Monte Carlo sparse grid Galerkin finite element methods for linear  elasticity equations with uncertainties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dick%2C+J">J. Dick</a>, 
<a href="/search/math?searchtype=author&query=Gia%2C+Q+T+L">Q. T. Le Gia</a>, 
<a href="/search/math?searchtype=author&query=Mustapha%2C+K">K. Mustapha</a>, 
<a href="/search/math?searchtype=author&query=Tran%2C+T">T. Tran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1252">[1252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06644" title="Abstract">arXiv:2310.06644</a> (replaced) [<a href="/pdf/2310.06644" title="Download PDF">pdf</a>, <a href="/format/2310.06644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Level-Set Encoder for Neural Distance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeske%2C+S+R">Stefan Rhys Jeske</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+J">Jonathan Klein</a>, 
<a href="/search/cs?searchtype=author&query=Michels%2C+D+L">Dominik L. Michels</a>, 
<a href="/search/cs?searchtype=author&query=Bender%2C+J">Jan Bender</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item1253">[1253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07129" title="Abstract">arXiv:2310.07129</a> (replaced) [<a href="/pdf/2310.07129" title="Download PDF">pdf</a>, <a href="/format/2310.07129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The impact when neural min-sum variant meets ordered statistics decoding  of LDPC codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guangwen Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xiao Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 8 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item1254">[1254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07684" title="Abstract">arXiv:2310.07684</a> (replaced) [<a href="/pdf/2310.07684" title="Download PDF">pdf</a>, <a href="/format/2310.07684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hypergraph Neural Networks through the Lens of Message Passing: A Common  Perspective to Homophily and Architecture Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Telyatnikov%2C+L">Lev Telyatnikov</a>, 
<a href="/search/cs?searchtype=author&query=Bucarelli%2C+M+S">Maria Sofia Bucarelli</a>, 
<a href="/search/cs?searchtype=author&query=Bernardez%2C+G">Guillermo Bernardez</a>, 
<a href="/search/cs?searchtype=author&query=Zaghen%2C+O">Olga Zaghen</a>, 
<a href="/search/cs?searchtype=author&query=Scardapane%2C+S">Simone Scardapane</a>, 
<a href="/search/cs?searchtype=author&query=Lio%2C+P">Pietro Lio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item1255">[1255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07891" title="Abstract">arXiv:2310.07891</a> (replaced) [<a href="/pdf/2310.07891" title="Download PDF">pdf</a>, <a href="/format/2310.07891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Theory of Non-Linear Feature Learning with One Gradient Step in  Two-Layer Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Moniri%2C+B">Behrad Moniri</a>, 
<a href="/search/stat?searchtype=author&query=Lee%2C+D">Donghwan Lee</a>, 
<a href="/search/stat?searchtype=author&query=Hassani%2C+H">Hamed Hassani</a>, 
<a href="/search/stat?searchtype=author&query=Dobriban%2C+E">Edgar Dobriban</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1256">[1256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08164" title="Abstract">arXiv:2310.08164</a> (replaced) [<a href="/pdf/2310.08164" title="Download PDF">pdf</a>, <a href="/format/2310.08164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Training Objectives: Interpreting Reward Model Divergence in  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marks%2C+L">Luke Marks</a>, 
<a href="/search/cs?searchtype=author&query=Abdullah%2C+A">Amir Abdullah</a>, 
<a href="/search/cs?searchtype=author&query=Mendez%2C+L">Luna Mendez</a>, 
<a href="/search/cs?searchtype=author&query=Arike%2C+R">Rauno Arike</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P">Philip Torr</a>, 
<a href="/search/cs?searchtype=author&query=Barez%2C+F">Fazl Barez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1257">[1257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09486" title="Abstract">arXiv:2310.09486</a> (replaced) [<a href="/pdf/2310.09486" title="Download PDF">pdf</a>, <a href="/format/2310.09486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mirage: Model-Agnostic Graph Distillation for Graph Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+M">Mridul Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Manchanda%2C+S">Sahil Manchanda</a>, 
<a href="/search/cs?searchtype=author&query=Kodamana%2C+H">Hariprasad Kodamana</a>, 
<a href="/search/cs?searchtype=author&query=Ranu%2C+S">Sayan Ranu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1258">[1258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09824" title="Abstract">arXiv:2310.09824</a> (replaced) [<a href="/pdf/2310.09824" title="Download PDF">pdf</a>, <a href="/format/2310.09824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overconstrained Robotic Limb with Energy-Efficient, Omni-directional  Locomotion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ronghan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+J">Jiayi Yin</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shihao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+B">Bangchao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Haoran Sun</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Jia Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+F">Fang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chaoyang Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 13 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1259">[1259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10477" title="Abstract">arXiv:2310.10477</a> (replaced) [<a href="/pdf/2310.10477" title="Download PDF">pdf</a>, <a href="/format/2310.10477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chunwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kuo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jianhua Han</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lanqing Hong</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+F">Fei Mi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengying Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenyong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Yeung%2C+D">Dit-Yan Yeung</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+L">Lifeng Shang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qun Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1260">[1260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10565" title="Abstract">arXiv:2310.10565</a> (replaced) [<a href="/pdf/2310.10565" title="Download PDF">pdf</a>, <a href="/format/2310.10565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HelmFluid: Learning Helmholtz Dynamics for Interpretable Fluid  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xing%2C+L">Lanxiang Xing</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haixu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yuezhou Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianmin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+M">Mingsheng Long</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1261">[1261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10688" title="Abstract">arXiv:2310.10688</a> (replaced) [<a href="/pdf/2310.10688" title="Download PDF">pdf</a>, <a href="/format/2310.10688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A decoder-only foundation model for time-series forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Abhimanyu Das</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+W">Weihao Kong</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+R">Rajat Sen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yichen Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1262">[1262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11334" title="Abstract">arXiv:2310.11334</a> (replaced) [<a href="/pdf/2310.11334" title="Download PDF">pdf</a>, <a href="/format/2310.11334" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Agent-Specific Effects: A Causal Effect Propagation Analysis in  Multi-Agent MDPs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Triantafyllou%2C+S">Stelios Triantafyllou</a>, 
<a href="/search/cs?searchtype=author&query=Sukovic%2C+A">Aleksa Sukovic</a>, 
<a href="/search/cs?searchtype=author&query=Mandal%2C+D">Debmalya Mandal</a>, 
<a href="/search/cs?searchtype=author&query=Radanovic%2C+G">Goran Radanovic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1263">[1263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11677" title="Abstract">arXiv:2310.11677</a> (replaced) [<a href="/pdf/2310.11677" title="Download PDF">pdf</a>, <a href="/ps/2310.11677" title="Download PostScript">ps</a>, <a href="/format/2310.11677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Sample Complexity Analysis of Natural Policy Gradient Algorithm  with General Parameterization for Infinite Horizon Discounted Reward Markov  Decision Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mondal%2C+W+U">Washim Uddin Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+V">Vaneet Aggarwal</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1264">[1264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12531" title="Abstract">arXiv:2310.12531</a> (replaced) [<a href="/pdf/2310.12531" title="Download PDF">pdf</a>, <a href="/format/2310.12531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ICU: Conquering Language Barriers in Vision-and-Language Modeling by  Dividing the Tasks into Image Captioning and Language Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+G">Guojun Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023 (Findings)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1265">[1265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12536" title="Abstract">arXiv:2310.12536</a> (replaced) [<a href="/pdf/2310.12536" title="Download PDF">pdf</a>, <a href="/format/2310.12536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fully Onboard Low-Power Localization with Semantic Sensor Fusion on a  Nano-UAV using Floor Plans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zimmerman%2C+N">Nicky Zimmerman</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+H">Hanna M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Magno%2C+M">Michele Magno</a>, 
<a href="/search/cs?searchtype=author&query=Benini%2C+L">Luca Benini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for ICRA 2024, 7 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1266">[1266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13397" title="Abstract">arXiv:2310.13397</a> (replaced) [<a href="/pdf/2310.13397" title="Download PDF">pdf</a>, <a href="/format/2310.13397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Equivariant Deep Weight Space Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Navon%2C+A">Aviv Navon</a>, 
<a href="/search/cs?searchtype=author&query=Shamsian%2C+A">Aviv Shamsian</a>, 
<a href="/search/cs?searchtype=author&query=Fetaya%2C+E">Ethan Fetaya</a>, 
<a href="/search/cs?searchtype=author&query=Chechik%2C+G">Gal Chechik</a>, 
<a href="/search/cs?searchtype=author&query=Dym%2C+N">Nadav Dym</a>, 
<a href="/search/cs?searchtype=author&query=Maron%2C+H">Haggai Maron</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1267">[1267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13681" title="Abstract">arXiv:2310.13681</a> (replaced) [<a href="/pdf/2310.13681" title="Download PDF">pdf</a>, <a href="/format/2310.13681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RealFM: A Realistic Mechanism to Incentivize Federated Participation and  Contribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bornstein%2C+M">Marco Bornstein</a>, 
<a href="/search/cs?searchtype=author&query=Bedi%2C+A+S">Amrit Singh Bedi</a>, 
<a href="/search/cs?searchtype=author&query=Sahu%2C+A+K">Anit Kumar Sahu</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+F">Furqan Khan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Computers and Society (cs.CY); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG); Theoretical Economics (econ.TH)

</div>
</div>
</dd>
<dt><a name="item1268">[1268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13920" title="Abstract">arXiv:2310.13920</a> (replaced) [<a href="/pdf/2310.13920" title="Download PDF">pdf</a>, <a href="/format/2310.13920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New low-order mixed finite element methods for linear elasticity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Huang%2C+X">Xuehai Huang</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+Y">Yaqian Zhou</a>, 
<a href="/search/math?searchtype=author&query=Zhu%2C+Y">Yangxing Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1269">[1269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14345" title="Abstract">arXiv:2310.14345</a> (replaced) [<a href="/pdf/2310.14345" title="Download PDF">pdf</a>, <a href="/format/2310.14345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum-walk search in motion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Sahu%2C+H">Himanshu Sahu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Sen%2C+K">Kallol Sen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Finalized version : Accepted in Scientific Reports
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Scientific Reports 14, 2815 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item1270">[1270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15330" title="Abstract">arXiv:2310.15330</a> (replaced) [<a href="/pdf/2310.15330" title="Download PDF">pdf</a>, <a href="/format/2310.15330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards the Theory of Unsupervised Federated Learning: Non-asymptotic  Analysis of Federated EM Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Tian%2C+Y">Ye Tian</a>, 
<a href="/search/stat?searchtype=author&query=Weng%2C+H">Haolei Weng</a>, 
<a href="/search/stat?searchtype=author&query=Feng%2C+Y">Yang Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 50 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1271">[1271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15393" title="Abstract">arXiv:2310.15393</a> (replaced) [<a href="/pdf/2310.15393" title="Download PDF">pdf</a>, <a href="/format/2310.15393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DoGE: Domain Reweighting with Generalization Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+S">Simin Fan</a>, 
<a href="/search/cs?searchtype=author&query=Pagliardini%2C+M">Matteo Pagliardini</a>, 
<a href="/search/cs?searchtype=author&query=Jaggi%2C+M">Martin Jaggi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1272">[1272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15524" title="Abstract">arXiv:2310.15524</a> (replaced) [<a href="/pdf/2310.15524" title="Download PDF">pdf</a>, <a href="/format/2310.15524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Inherent Privacy Properties of Discrete Denoising Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+R">Rongzhe Wei</a>, 
<a href="/search/cs?searchtype=author&query=Krea%C4%8Di%C4%87%2C+E">Eleonora Krea&#x10d;i&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Haoteng Yin</a>, 
<a href="/search/cs?searchtype=author&query=Chien%2C+E">Eli Chien</a>, 
<a href="/search/cs?searchtype=author&query=Potluru%2C+V+K">Vamsi K. Potluru</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pan Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 52 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1273">[1273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16144" title="Abstract">arXiv:2310.16144</a> (replaced) [<a href="/pdf/2310.16144" title="Download PDF">pdf</a>, <a href="/ps/2310.16144" title="Download PostScript">ps</a>, <a href="/format/2310.16144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ROM-Based Stochastic Optimization for a Continuous Manufacturing Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cruz-Oliver%2C+R">Raul Cruz-Oliver</a> (1), 
<a href="/search/eess?searchtype=author&query=Monzon%2C+L">Luis Monzon</a> (2), 
<a href="/search/eess?searchtype=author&query=Ramirez-Laboreo%2C+E">Edgar Ramirez-Laboreo</a> (3), 
<a href="/search/eess?searchtype=author&query=Rodriguez-Fortun%2C+J">Jose-Manuel Rodriguez-Fortun</a> (2) ((1) ETH Zurich, (2) Instituto Tecnologico de Aragon, (3) Universidad de Zaragoza)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 10 figures. Revised version submitted to the journal ISA Transactions after first round of peer review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item1274">[1274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16401" title="Abstract">arXiv:2310.16401</a> (replaced) [<a href="/pdf/2310.16401" title="Download PDF">pdf</a>, <a href="/format/2310.16401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Networks with a Distribution of Parametrized Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S+H">See Hian Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+F">Feng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+K">Kelin Xia</a>, 
<a href="/search/cs?searchtype=author&query=Tay%2C+W+P">Wee Peng Tay</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1275">[1275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16920" title="Abstract">arXiv:2310.16920</a> (replaced) [<a href="/pdf/2310.16920" title="Download PDF">pdf</a>, <a href="/format/2310.16920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smoothed Gradient Clipping and Error Feedback for Distributed  Optimization under Heavy-Tailed Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yu%2C+S">Shuhua Yu</a>, 
<a href="/search/math?searchtype=author&query=Jakovetic%2C+D">Dusan Jakovetic</a>, 
<a href="/search/math?searchtype=author&query=Kar%2C+S">Soummya Kar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item1276">[1276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17451" title="Abstract">arXiv:2310.17451</a> (replaced) [<a href="/pdf/2310.17451" title="Download PDF">pdf</a>, <a href="/format/2310.17451" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating by Understanding: Neural Visual Generation with Logical  Symbol Groundings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yifei Peng</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yu Jin</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zhexu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yao-Xiang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+W">Wang-Zhou Dai</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhong Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kun Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item1277">[1277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17496" title="Abstract">arXiv:2310.17496</a> (replaced) [<a href="/pdf/2310.17496" title="Download PDF">pdf</a>, <a href="/format/2310.17496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tackling Interference Induced by Data Training Loops in A/B Tests: A  Weighted Training Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Si%2C+N">Nian Si</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Econometrics (econ.EM)

</div>
</div>
</dd>
<dt><a name="item1278">[1278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17846" title="Abstract">arXiv:2310.17846</a> (replaced) [<a href="/pdf/2310.17846" title="Download PDF">pdf</a>, <a href="/format/2310.17846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Awareness to Action: Exploring End-User Empowerment Interventions  for Dark Patterns in UX
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yuwen Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuewen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yaxing Yao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T+J">Toby Jia-Jun Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CSCW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item1279">[1279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18384" title="Abstract">arXiv:2310.18384</a> (replaced) [<a href="/pdf/2310.18384" title="Download PDF">pdf</a>, <a href="/format/2310.18384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MicroNAS: Memory and Latency Constrained Hardware-Aware Neural  Architecture Search for Time Series Classification on Microcontrollers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=King%2C+T">Tobias King</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yexu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%B6ddiger%2C+T">Tobias R&#xf6;ddiger</a>, 
<a href="/search/cs?searchtype=author&query=Beigl%2C+M">Michael Beigl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1280">[1280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18765" title="Abstract">arXiv:2310.18765</a> (replaced) [<a href="/pdf/2310.18765" title="Download PDF">pdf</a>, <a href="/format/2310.18765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Semi-Supervised Imbalanced Node Classification from  Bias-Variance Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+D">Divin Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+G">Gengchen Wei</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shengzhong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zengfeng Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Thirty-seventh Conference on Neural Information Processing
  Systems. (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1281">[1281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18936" title="Abstract">arXiv:2310.18936</a> (replaced) [<a href="/pdf/2310.18936" title="Download PDF">pdf</a>, <a href="/ps/2310.18936" title="Download PostScript">ps</a>, <a href="/format/2310.18936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Examples Are Not Real Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Ang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yifei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yiwen Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yisen Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1282">[1282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18961" title="Abstract">arXiv:2310.18961</a> (replaced) [<a href="/pdf/2310.18961" title="Download PDF">pdf</a>, <a href="/format/2310.18961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qihang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+G">Guansong Pang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yu Tian</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shibo He</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiming Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1283">[1283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19923" title="Abstract">arXiv:2310.19923</a> (replaced) [<a href="/pdf/2310.19923" title="Download PDF">pdf</a>, <a href="/format/2310.19923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long  Documents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=G%C3%BCnther%2C+M">Michael G&#xfc;nther</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+J">Jackmin Ong</a>, 
<a href="/search/cs?searchtype=author&query=Mohr%2C+I">Isabelle Mohr</a>, 
<a href="/search/cs?searchtype=author&query=Abdessalem%2C+A">Alaeddine Abdessalem</a>, 
<a href="/search/cs?searchtype=author&query=Abel%2C+T">Tanguy Abel</a>, 
<a href="/search/cs?searchtype=author&query=Akram%2C+M+K">Mohammad Kalim Akram</a>, 
<a href="/search/cs?searchtype=author&query=Guzman%2C+S">Susana Guzman</a>, 
<a href="/search/cs?searchtype=author&query=Mastrapas%2C+G">Georgios Mastrapas</a>, 
<a href="/search/cs?searchtype=author&query=Sturua%2C+S">Saba Sturua</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Werk%2C+M">Maximilian Werk</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Nan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+H">Han Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1284">[1284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20689" title="Abstract">arXiv:2310.20689</a> (replaced) [<a href="/pdf/2310.20689" title="Download PDF">pdf</a>, <a href="/format/2310.20689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning From Mistakes Makes LLM Better Reasoner
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+S">Shengnan An</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zexiong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zeqi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+N">Nanning Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+J">Jian-Guang Lou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weizhu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 13 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1285">[1285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00099" title="Abstract">arXiv:2311.00099</a> (replaced) [<a href="/pdf/2311.00099" title="Download PDF">pdf</a>, <a href="/ps/2311.00099" title="Download PostScript">ps</a>, <a href="/format/2311.00099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convex quadratic sets and the complexity of mixed integer convex  quadratic programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Del+Pia%2C+A">Alberto Del Pia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item1286">[1286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01223" title="Abstract">arXiv:2311.01223</a> (replaced) [<a href="/pdf/2311.01223" title="Download PDF">pdf</a>, <a href="/format/2311.01223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Models for Reinforcement Learning: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhengbang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hanye Zhao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+H">Haoran He</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yichao Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shenyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Haoquan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tingting Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weinan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 2 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1287">[1287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01256" title="Abstract">arXiv:2311.01256</a> (replaced) [<a href="/pdf/2311.01256" title="Download PDF">pdf</a>, <a href="/ps/2311.01256" title="Download PostScript">ps</a>, <a href="/format/2311.01256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An energy-based comparative analysis of common approaches to text  classification in the Legal domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gultekin%2C+S">Sinan Gultekin</a>, 
<a href="/search/cs?searchtype=author&query=Globo%2C+A">Achille Globo</a>, 
<a href="/search/cs?searchtype=author&query=Zugarini%2C+A">Andrea Zugarini</a>, 
<a href="/search/cs?searchtype=author&query=Ernandes%2C+M">Marco Ernandes</a>, 
<a href="/search/cs?searchtype=author&query=Rigutini%2C+L">Leonardo Rigutini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at The 4th International Conference on NLP &amp; Text Mining (NLTM 2024), January 27-28 2024, Copenhagen, Denmark - 12 pages, 1 figure, 7 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computer Science &amp; Information Technology (CS &amp; IT) ISSN 2231-5403
  Volume 14, Number 02, January 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item1288">[1288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01378" title="Abstract">arXiv:2311.01378</a> (replaced) [<a href="/pdf/2311.01378" title="Download PDF">pdf</a>, <a href="/format/2311.01378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision-Language Foundation Models as Effective Robot Imitators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinghang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Minghuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanbo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Cunjun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hongtao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Cheang%2C+C">Chilam Cheang</a>, 
<a href="/search/cs?searchtype=author&query=Jing%2C+Y">Ya Jing</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weinan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huaping Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hang Li</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+T">Tao Kong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Fix typos. Project page: <a href="https://roboflamingo.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1289">[1289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01386" title="Abstract">arXiv:2311.01386</a> (replaced) [<a href="/pdf/2311.01386" title="Download PDF">pdf</a>, <a href="/format/2311.01386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Language Models Be Tricked by Language Illusions? Easier with  Syntax, Harder with Semantics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuhan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gibson%2C+E">Edward Gibson</a>, 
<a href="/search/cs?searchtype=author&query=Davis%2C+F">Forrest Davis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by The SIGNLL Conference on Computational Natural Language Learning 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1290">[1290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01441" title="Abstract">arXiv:2311.01441</a> (replaced) [<a href="/pdf/2311.01441" title="Download PDF">pdf</a>, <a href="/format/2311.01441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distilling Out-of-Distribution Robustness from Vision-Language  Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Andy Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu-Xiong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haohan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1291">[1291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01568" title="Abstract">arXiv:2311.01568</a> (replaced) [<a href="/pdf/2311.01568" title="Download PDF">pdf</a>, <a href="/format/2311.01568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anytime-Competitive Reinforcement Learning with Policy Prior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jianyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pengfei Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tongxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Wierman%2C+A">Adam Wierman</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Shaolei Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1292">[1292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02467" title="Abstract">arXiv:2311.02467</a> (replaced) [<a href="/pdf/2311.02467" title="Download PDF">pdf</a>, <a href="/format/2311.02467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Individualized Policy Evaluation and Learning under Clustered Network  Interference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhang%2C+Y">Yi Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Imai%2C+K">Kosuke Imai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Econometrics (econ.EM)

</div>
</div>
</dd>
<dt><a name="item1293">[1293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02945" title="Abstract">arXiv:2311.02945</a> (replaced) [<a href="/pdf/2311.02945" title="Download PDF">pdf</a>, <a href="/ps/2311.02945" title="Download PostScript">ps</a>, <a href="/format/2311.02945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PhoGPT: Generative Pre-training for Vietnamese
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D+Q">Dat Quoc Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+L+T">Linh The Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+C">Chi Tran</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D+N">Dung Ngoc Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Phung%2C+D">Dinh Phung</a>, 
<a href="/search/cs?searchtype=author&query=Bui%2C+H">Hung Bui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PhoGPT-4B Technical Report - 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1294">[1294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.03079" title="Abstract">arXiv:2311.03079</a> (replaced) [<a href="/pdf/2311.03079" title="Download PDF">pdf</a>, <a href="/format/2311.03079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CogVLM: Visual Expert for Pretrained Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weihan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+Q">Qingsong Lv</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenmeng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+W">Wenyi Hong</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+J">Ji Qi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Junhui Ji</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhuoyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Lei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xixuan Song</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiazheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Bin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juanzi Li</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yuxiao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Ming Ding</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jie Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1295">[1295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.03099" title="Abstract">arXiv:2311.03099</a> (replaced) [<a href="/pdf/2311.03099" title="Download PDF">pdf</a>, <a href="/format/2311.03099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Models are Super Mario: Absorbing Abilities from Homologous  Models as a Free Lunch
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Le Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bowen Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haiyang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongbin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 21 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1296">[1296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.03340" title="Abstract">arXiv:2311.03340</a> (replaced) [<a href="/pdf/2311.03340" title="Download PDF">pdf</a>, <a href="/format/2311.03340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multitask Kernel-based Learning with First-Order Logic Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Diligenti%2C+M">Michelangelo Diligenti</a>, 
<a href="/search/cs?searchtype=author&query=Gori%2C+M">Marco Gori</a>, 
<a href="/search/cs?searchtype=author&query=Maggini%2C+M">Marco Maggini</a>, 
<a href="/search/cs?searchtype=author&query=Rigutini%2C+L">Leonardo Rigutini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 20th International Conference on Inductive Logic Programming (ILP 2010). Florence, Italy. June 27-30 2010
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of The 20th International Conference on Inductive
  Logic Programming (ILP 2010)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item1297">[1297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.03415" title="Abstract">arXiv:2311.03415</a> (replaced) [<a href="/pdf/2311.03415" title="Download PDF">pdf</a>, <a href="/format/2311.03415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PowerFlowNet: Power Flow Approximation Using Message Passing Graph  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+N">Nan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Orfanoudakis%2C+S">Stavros Orfanoudakis</a>, 
<a href="/search/cs?searchtype=author&query=Cardenas%2C+N+O">Nathan Ordonez Cardenas</a>, 
<a href="/search/cs?searchtype=author&query=Giraldo%2C+J+S">Juan S. Giraldo</a>, 
<a href="/search/cs?searchtype=author&query=Vergara%2C+P+P">Pedro P. Vergara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1298">[1298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.04076" title="Abstract">arXiv:2311.04076</a> (replaced) [<a href="/pdf/2311.04076" title="Download PDF">pdf</a>, <a href="/format/2311.04076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do LLMs exhibit human-like response biases? A case study in survey  design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tjuatja%2C+L">Lindia Tjuatja</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+V">Valerie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S+T">Sherry Tongshuang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Talwalkar%2C+A">Ameet Talwalkar</a>, 
<a href="/search/cs?searchtype=author&query=Neubig%2C+G">Graham Neubig</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1299">[1299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.04534" title="Abstract">arXiv:2311.04534</a> (replaced) [<a href="/pdf/2311.04534" title="Download PDF">pdf</a>, <a href="/format/2311.04534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Loss Masking Is Not Needed in Decoder-only Transformer for  Discrete-token-based ASR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qinglin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Siqi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shiliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+C">Chong Deng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yukun Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hai Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, accepted by ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1300">[1300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.05644" title="Abstract">arXiv:2311.05644</a> (replaced) [<a href="/pdf/2311.05644" title="Download PDF">pdf</a>, <a href="/format/2311.05644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the tractability of Nash equilibrium
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Avramopoulos%2C+I">Ioannis Avramopoulos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item1301">[1301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06233" title="Abstract">arXiv:2311.06233</a> (replaced) [<a href="/pdf/2311.06233" title="Download PDF">pdf</a>, <a href="/ps/2311.06233" title="Download PostScript">ps</a>, <a href="/format/2311.06233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Contamination Quiz: A Tool to Detect and Estimate Contamination in  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Golchin%2C+S">Shahriar Golchin</a>, 
<a href="/search/cs?searchtype=author&query=Surdeanu%2C+M">Mihai Surdeanu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2 preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1302">[1302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06911" title="Abstract">arXiv:2311.06911</a> (replaced) [<a href="/pdf/2311.06911" title="Download PDF">pdf</a>, <a href="/format/2311.06911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling Human Factors and Message Attributes in a Smishing Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Timko%2C+D">Daniel Timko</a>, 
<a href="/search/cs?searchtype=author&query=Castillo%2C+D+H">Daniel Hernandez Castillo</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+L">Muhammad Lutfor Rahman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item1303">[1303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07554" title="Abstract">arXiv:2311.07554</a> (replaced) [<a href="/pdf/2311.07554" title="Download PDF">pdf</a>, <a href="/format/2311.07554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast and Space-Efficient Parallel Algorithms for Influence Maximization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Letong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+X">Xiangyun Ding</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yan Gu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yihan Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item1304">[1304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07908" title="Abstract">arXiv:2311.07908</a> (replaced) [<a href="/pdf/2311.07908" title="Download PDF">pdf</a>, <a href="/format/2311.07908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Bayes-Optimal Channel Estimation for Holographic MIMO in  Unknown EM Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yu%2C+W">Wentao Yu</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+H">Hengtao He</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+X">Xianghao Yu</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+S">Shenghui Song</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Murch%2C+R+D">Ross D. Murch</a>, 
<a href="/search/eess?searchtype=author&query=Letaief%2C+K+B">Khaled B. Letaief</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures, 1 table, accepted for presentation at IEEE ICC 2024, Denver, CO, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item1305">[1305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08316" title="Abstract">arXiv:2311.08316</a> (replaced) [<a href="/pdf/2311.08316" title="Download PDF">pdf</a>, <a href="/format/2311.08316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CholeskyQR with Randomization and Pivoting for Tall Matrices (CQRRPT)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Melnichenko%2C+M">Maksim Melnichenko</a>, 
<a href="/search/math?searchtype=author&query=Balabanov%2C+O">Oleg Balabanov</a>, 
<a href="/search/math?searchtype=author&query=Murray%2C+R">Riley Murray</a>, 
<a href="/search/math?searchtype=author&query=Demmel%2C+J">James Demmel</a>, 
<a href="/search/math?searchtype=author&query=Mahoney%2C+M+W">Michael W. Mahoney</a>, 
<a href="/search/math?searchtype=author&query=Luszczek%2C+P">Piotr Luszczek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v1: 26 pages in the body, 10 pages in the appendices, 10 figures. v2: performance experiments now use a larger sketch size for CQRRPT
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1306">[1306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09308" title="Abstract">arXiv:2311.09308</a> (replaced) [<a href="/pdf/2311.09308" title="Download PDF">pdf</a>, <a href="/format/2311.09308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Divergences between Language Models and Human Brains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuchen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+E">Emmy Liu</a>, 
<a href="/search/cs?searchtype=author&query=Neubig%2C+G">Graham Neubig</a>, 
<a href="/search/cs?searchtype=author&query=Tarr%2C+M+J">Michael J. Tarr</a>, 
<a href="/search/cs?searchtype=author&query=Wehbe%2C+L">Leila Wehbe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item1307">[1307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09631" title="Abstract">arXiv:2311.09631</a> (replaced) [<a href="/pdf/2311.09631" title="Download PDF">pdf</a>, <a href="/format/2311.09631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Pauli Spectrum of QAC0
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Nadimpalli%2C+S">Shivam Nadimpalli</a>, 
<a href="/search/quant-ph?searchtype=author&query=Parham%2C+N">Natalie Parham</a>, 
<a href="/search/quant-ph?searchtype=author&query=Vasconcelos%2C+F">Francisca Vasconcelos</a>, 
<a href="/search/quant-ph?searchtype=author&query=Yuen%2C+H">Henry Yuen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item1308">[1308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10246" title="Abstract">arXiv:2311.10246</a> (replaced) [<a href="/pdf/2311.10246" title="Download PDF">pdf</a>, <a href="/format/2311.10246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Surprisal Driven $k$-NN for Robust and Interpretable Nonparametric  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+A">Amartya Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Hazard%2C+C+J">Christopher J. Hazard</a>, 
<a href="/search/cs?searchtype=author&query=Beel%2C+J">Jacob Beel</a>, 
<a href="/search/cs?searchtype=author&query=Mack%2C+C">Cade Mack</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+J">Jack Xia</a>, 
<a href="/search/cs?searchtype=author&query=Resnick%2C+M">Michael Resnick</a>, 
<a href="/search/cs?searchtype=author&query=Goddin%2C+W">Will Goddin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1309">[1309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10642" title="Abstract">arXiv:2311.10642</a> (replaced) [<a href="/pdf/2311.10642" title="Download PDF">pdf</a>, <a href="/format/2311.10642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as  an Alternative to Attention Layers in Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bozic%2C+V">Vukasin Bozic</a>, 
<a href="/search/cs?searchtype=author&query=Dordevic%2C+D">Danilo Dordevic</a>, 
<a href="/search/cs?searchtype=author&query=Coppola%2C+D">Daniele Coppola</a>, 
<a href="/search/cs?searchtype=author&query=Thommes%2C+J">Joseph Thommes</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S+P">Sidak Pal Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAAI24(<a href="https://aaai.org/aaai-conference/">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1310">[1310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10755" title="Abstract">arXiv:2311.10755</a> (replaced) [<a href="/pdf/2311.10755" title="Download PDF">pdf</a>, <a href="/ps/2311.10755" title="Download PostScript">ps</a>, <a href="/format/2311.10755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robotic Pollination of Apples in Commercial Orchards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sapkota%2C+R">Ranjan Sapkota</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+D">Dawood Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Khanal%2C+S+R">Salik Ram Khanal</a>, 
<a href="/search/cs?searchtype=author&query=Bhattarai%2C+U">Uddhav Bhattarai</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+C">Changki Mo</a>, 
<a href="/search/cs?searchtype=author&query=Whiting%2C+M+D">Matthew D. Whiting</a>, 
<a href="/search/cs?searchtype=author&query=Karkee%2C+M">Manoj Karkee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 Page, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1311">[1311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.11514" title="Abstract">arXiv:2311.11514</a> (replaced) [<a href="/pdf/2311.11514" title="Download PDF">pdf</a>, <a href="/format/2311.11514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HexGen: Generative Inference of Large-Scale Foundation Model over  Heterogeneous Decentralized Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Youhe Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+R">Ran Yan</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+X">Xiaozhe Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Beidi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+B">Binhang Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item1312">[1312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12267" title="Abstract">arXiv:2311.12267</a> (replaced) [<a href="/pdf/2311.12267" title="Download PDF">pdf</a>, <a href="/format/2311.12267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Causal Representations from General Environments:  Identifiability and Intrinsic Ambiguity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+J">Jikai Jin</a>, 
<a href="/search/cs?searchtype=author&query=Syrgkanis%2C+V">Vasilis Syrgkanis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 42 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Econometrics (econ.EM); Applications (stat.AP); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1313">[1313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12495" title="Abstract">arXiv:2311.12495</a> (replaced) [<a href="/pdf/2311.12495" title="Download PDF">pdf</a>, <a href="/format/2311.12495" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Objective Reinforcement Learning Based on Decomposition: A  Taxonomy and Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Felten%2C+F">Florian Felten</a>, 
<a href="/search/cs?searchtype=author&query=Talbi%2C+E">El-Ghazali Talbi</a>, 
<a href="/search/cs?searchtype=author&query=Danoy%2C+G">Gr&#xe9;goire Danoy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at JAIR
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1314">[1314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12664" title="Abstract">arXiv:2311.12664</a> (replaced) [<a href="/pdf/2311.12664" title="Download PDF">pdf</a>, <a href="/format/2311.12664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The DURel Annotation Tool: Human and Computational Measurement of  Semantic Proximity, Sense Clusters and Semantic Change
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schlechtweg%2C+D">Dominik Schlechtweg</a>, 
<a href="/search/cs?searchtype=author&query=Virk%2C+S+M">Shafqat Mumtaz Virk</a>, 
<a href="/search/cs?searchtype=author&query=Sander%2C+P">Pauline Sander</a>, 
<a href="/search/cs?searchtype=author&query=Sk%C3%B6ldberg%2C+E">Emma Sk&#xf6;ldberg</a>, 
<a href="/search/cs?searchtype=author&query=Linke%2C+L+T">Lukas Theuer Linke</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tuo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tahmasebi%2C+N">Nina Tahmasebi</a>, 
<a href="/search/cs?searchtype=author&query=Kuhn%2C+J">Jonas Kuhn</a>, 
<a href="/search/cs?searchtype=author&query=Walde%2C+S+S+i">Sabine Schulte im Walde</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL Demo, 7 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1315">[1315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12832" title="Abstract">arXiv:2311.12832</a> (replaced) [<a href="/pdf/2311.12832" title="Download PDF">pdf</a>, <a href="/format/2311.12832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward effective protection against diffusion based mimicry through  score distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+H">Haotian Xue</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+C">Chumeng Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaoyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yongxin Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 Conference Paper, the code is available in <a href="https://github.com/xavihart/Diff-Protect">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1316">[1316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.13049" title="Abstract">arXiv:2311.13049</a> (replaced) [<a href="/pdf/2311.13049" title="Download PDF">pdf</a>, <a href="/ps/2311.13049" title="Download PostScript">ps</a>, <a href="/format/2311.13049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fourier pseudospectral methods for the spatial variable-order fractional  wave equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+Y">Yanzhi Zhang</a>, 
<a href="/search/math?searchtype=author&query=Zhao%2C+X">Xiaofei Zhao</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+S">Shiping Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1317">[1317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.13380" title="Abstract">arXiv:2311.13380</a> (replaced) [<a href="/pdf/2311.13380" title="Download PDF">pdf</a>, <a href="/format/2311.13380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing the Evolution and Maintenance of ML Models on Hugging Face
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Casta%C3%B1o%2C+J">Joel Casta&#xf1;o</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez-Fern%C3%A1ndez%2C+S">Silverio Mart&#xed;nez-Fern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Franch%2C+X">Xavier Franch</a>, 
<a href="/search/cs?searchtype=author&query=Bogner%2C+J">Justus Bogner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 2024 IEEE/ACM 21th International Conference on Mining Software Repositories (MSR)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item1318">[1318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.13544" title="Abstract">arXiv:2311.13544</a> (replaced) [<a href="/pdf/2311.13544" title="Download PDF">pdf</a>, <a href="/format/2311.13544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Piecewise Polynomial Regression of Tame Functions via Integer  Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bareilles%2C+G">Gilles Bareilles</a>, 
<a href="/search/math?searchtype=author&query=Aspman%2C+J">Johannes Aspman</a>, 
<a href="/search/math?searchtype=author&query=Nemecek%2C+J">Jiri Nemecek</a>, 
<a href="/search/math?searchtype=author&query=Marecek%2C+J">Jakub Marecek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item1319">[1319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.13664" title="Abstract">arXiv:2311.13664</a> (replaced) [<a href="/pdf/2311.13664" title="Download PDF">pdf</a>, <a href="/format/2311.13664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sample as You Infer: Predictive Coding With Langevin Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zahid%2C+U">Umais Zahid</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qinghai Guo</a>, 
<a href="/search/cs?searchtype=author&query=Fountas%2C+Z">Zafeirios Fountas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> FID values updated to use a fixed 50,000 samples for all experiments - Jeffrey's divergence now consistently best performing. Dynov2 based metrics removed due to inconsistency of results - and since not industry standard. Multiple beta values tested in Fig 4. Theta LR for VAEs; beta and inf LR for LPC now tuned for results. Figure 5B updated; curves now correspond to results in Table 1
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item1320">[1320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.14220" title="Abstract">arXiv:2311.14220</a> (replaced) [<a href="/pdf/2311.14220" title="Download PDF">pdf</a>, <a href="/format/2311.14220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assumption-lean and Data-adaptive Post-Prediction Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Miao%2C+J">Jiacheng Miao</a>, 
<a href="/search/stat?searchtype=author&query=Miao%2C+X">Xinran Miao</a>, 
<a href="/search/stat?searchtype=author&query=Wu%2C+Y">Yixuan Wu</a>, 
<a href="/search/stat?searchtype=author&query=Zhao%2C+J">Jiwei Zhao</a>, 
<a href="/search/stat?searchtype=author&query=Lu%2C+Q">Qiongshi Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1321">[1321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.14652" title="Abstract">arXiv:2311.14652</a> (replaced) [<a href="/pdf/2311.14652" title="Download PDF">pdf</a>, <a href="/format/2311.14652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One Pass Streaming Algorithm for Super Long Token Attention  Approximation in Sublinear Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Addanki%2C+R">Raghav Addanki</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhao Song</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chiwun Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1322">[1322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.14736" title="Abstract">arXiv:2311.14736</a> (replaced) [<a href="/pdf/2311.14736" title="Download PDF">pdf</a>, <a href="/format/2311.14736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Diversity Matters for Robust Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bukharin%2C+A">Alexander Bukharin</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tuo Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1323">[1323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.15852" title="Abstract">arXiv:2311.15852</a> (replaced) [<a href="/pdf/2311.15852" title="Download PDF">pdf</a>, <a href="/format/2311.15852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exponential Auto-Tuning Fault-Tolerant Control of N Degrees-of-Freedom  Manipulators Subject to Torque Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shahna%2C+M+H">Mehdi Heydari Shahna</a>, 
<a href="/search/cs?searchtype=author&query=Mattila%2C+J">Jouni Mattila</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted for possible publication in the IEEE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1324">[1324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.16054" title="Abstract">arXiv:2311.16054</a> (replaced) [<a href="/pdf/2311.16054" title="Download PDF">pdf</a>, <a href="/format/2311.16054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Metric Space Magnitude for Evaluating the Diversity of Latent  Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Limbeck%2C+K">Katharina Limbeck</a>, 
<a href="/search/cs?searchtype=author&query=Andreeva%2C+R">Rayna Andreeva</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+R">Rik Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Rieck%2C+B">Bastian Rieck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Geometric Topology (math.GT); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1325">[1325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.16148" title="Abstract">arXiv:2311.16148</a> (replaced) [<a href="/pdf/2311.16148" title="Download PDF">pdf</a>, <a href="/format/2311.16148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Univariate Radial Basis Function Layers: Brain-inspired Deep Neural  Layers for Low-Dimensional Inputs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jost%2C+D">Daniel Jost</a>, 
<a href="/search/cs?searchtype=author&query=Patil%2C+B">Basavasagar Patil</a>, 
<a href="/search/cs?searchtype=author&query=Alameda-Pineda%2C+X">Xavier Alameda-Pineda</a>, 
<a href="/search/cs?searchtype=author&query=Reinke%2C+C">Chris Reinke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1326">[1326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.16203" title="Abstract">arXiv:2311.16203</a> (replaced) [<a href="/pdf/2311.16203" title="Download PDF">pdf</a>, <a href="/format/2311.16203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatTraffic: Text-to-Traffic Generation via Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chengyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Q">Qitan Shao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+Y">Yisheng Lv</a>, 
<a href="/search/cs?searchtype=author&query=Piao%2C+X">Xinglin Piao</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+B">Baocai Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1327">[1327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.17515" title="Abstract">arXiv:2311.17515</a> (replaced) [<a href="/pdf/2311.17515" title="Download PDF">pdf</a>, <a href="/ps/2311.17515" title="Download PostScript">ps</a>, <a href="/format/2311.17515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fusion of Single and Integral Multispectral Aerial Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Youssef%2C+M">Mohamed Youssef</a>, 
<a href="/search/eess?searchtype=author&query=Bimber%2C+O">Oliver Bimber</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1328">[1328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.17539" title="Abstract">arXiv:2311.17539</a> (replaced) [<a href="/pdf/2311.17539" title="Download PDF">pdf</a>, <a href="/format/2311.17539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Sharpness-aware Minimization under Overparameterization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shin%2C+S">Sungbin Shin</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongyeop Lee</a>, 
<a href="/search/cs?searchtype=author&query=Andriushchenko%2C+M">Maksym Andriushchenko</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+N">Namhoon Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1329">[1329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.17936" title="Abstract">arXiv:2311.17936</a> (replaced) [<a href="/pdf/2311.17936" title="Download PDF">pdf</a>, <a href="/ps/2311.17936" title="Download PostScript">ps</a>, <a href="/format/2311.17936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diagnostics Using Nuclear Plant Cyber Attack Analysis Toolkit
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Patel%2C+J+K">Japan K. Patel</a>, 
<a href="/search/eess?searchtype=author&query=Varuttamaseni%2C+A">Athi Varuttamaseni</a>, 
<a href="/search/eess?searchtype=author&query=Youngblood%2C+R+W">Robert W. Youngblood III</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+J+C">John C. Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper has been submitted to ANS for review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item1330">[1330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.18022" title="Abstract">arXiv:2311.18022</a> (replaced) [<a href="/pdf/2311.18022" title="Download PDF">pdf</a>, <a href="/format/2311.18022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compelling ReLU Network Initialization and Training to Leverage  Exponential Scaling with Depth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Milkert%2C+M">Max Milkert</a>, 
<a href="/search/cs?searchtype=author&query=Hyde%2C+D">David Hyde</a>, 
<a href="/search/cs?searchtype=author&query=Laine%2C+F">Forrest Laine</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1331">[1331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.18373" title="Abstract">arXiv:2311.18373</a> (replaced) [<a href="/pdf/2311.18373" title="Download PDF">pdf</a>, <a href="/format/2311.18373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Deep Learning for Polyp Segmentation: Techniques, Challenges  and Future Trends
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mei%2C+J">Jiaxin Mei</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaiwen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yizhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Ye Wu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Huazhu Fu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1332">[1332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.18703" title="Abstract">arXiv:2311.18703</a> (replaced) [<a href="/pdf/2311.18703" title="Download PDF">pdf</a>, <a href="/format/2311.18703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predictable Reinforcement Learning Dynamics through Entropy Rate  Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ornia%2C+D+J">Daniel Jarne Ornia</a>, 
<a href="/search/cs?searchtype=author&query=Delimpaltadakis%2C+G">Giannis Delimpaltadakis</a>, 
<a href="/search/cs?searchtype=author&query=Kober%2C+J">Jens Kober</a>, 
<a href="/search/cs?searchtype=author&query=Alonso-Mora%2C+J">Javier Alonso-Mora</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1333">[1333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.18751" title="Abstract">arXiv:2311.18751</a> (replaced) [<a href="/pdf/2311.18751" title="Download PDF">pdf</a>, <a href="/format/2311.18751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exposing Limitations of Language Model Agents in Sequential-Task  Compositions on the Web
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Furuta%2C+H">Hiroki Furuta</a>, 
<a href="/search/cs?searchtype=author&query=Matsuo%2C+Y">Yutaka Matsuo</a>, 
<a href="/search/cs?searchtype=author&query=Faust%2C+A">Aleksandra Faust</a>, 
<a href="/search/cs?searchtype=author&query=Gur%2C+I">Izzeddin Gur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code: <a href="https://github.com/google-research/google-research/tree/master/compositional_rl/compwob">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1334">[1334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.00050" title="Abstract">arXiv:2312.00050</a> (replaced) [<a href="/pdf/2312.00050" title="Download PDF">pdf</a>, <a href="/format/2312.00050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Elijah: Eliminating Backdoors Injected in Diffusion Models via  Distribution Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+S">Shengwei An</a>, 
<a href="/search/cs?searchtype=author&query=Chou%2C+S">Sheng-Yen Chou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaiyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qiuling Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+G">Guanhong Tao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+G">Guangyu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Siyuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shiqing Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pin-Yu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+T">Tsung-Yi Ho</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiangyu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1335">[1335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.01187" title="Abstract">arXiv:2312.01187</a> (replaced) [<a href="/pdf/2312.01187" title="Download PDF">pdf</a>, <a href="/format/2312.01187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SASSL: Enhancing Self-Supervised Learning via Neural Style Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rojas-Gomez%2C+R+A">Renan A. Rojas-Gomez</a>, 
<a href="/search/cs?searchtype=author&query=Singhal%2C+K">Karan Singhal</a>, 
<a href="/search/cs?searchtype=author&query=Etemad%2C+A">Ali Etemad</a>, 
<a href="/search/cs?searchtype=author&query=Bijamov%2C+A">Alex Bijamov</a>, 
<a href="/search/cs?searchtype=author&query=Morningstar%2C+W+R">Warren R. Morningstar</a>, 
<a href="/search/cs?searchtype=author&query=Mansfield%2C+P+A">Philip Andrew Mansfield</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1336">[1336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.03885" title="Abstract">arXiv:2312.03885</a> (replaced) [<a href="/pdf/2312.03885" title="Download PDF">pdf</a>, <a href="/format/2312.03885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adapting Newton&#x27;s Method to Neural Networks through a Summary of  Higher-Order Derivatives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wolinski%2C+P">Pierre Wolinski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1337">[1337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.04293" title="Abstract">arXiv:2312.04293</a> (replaced) [<a href="/pdf/2312.04293" title="Download PDF">pdf</a>, <a href="/format/2312.04293" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT-4V with Emotion: A Zero-shot Benchmark for Generalized Emotion  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lian%2C+Z">Zheng Lian</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Licai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Haiyang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Z">Zhuofan Wen</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+H">Hao Gu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+J">Jianhua Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item1338">[1338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.04693" title="Abstract">arXiv:2312.04693</a> (replaced) [<a href="/pdf/2312.04693" title="Download PDF">pdf</a>, <a href="/format/2312.04693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraphMETRO: Mitigating Complex Graph Distribution Shifts via Mixture of  Aligned Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shirley Wu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+K">Kaidi Cao</a>, 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+B">Bruno Ribeiro</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">James Zou</a>, 
<a href="/search/cs?searchtype=author&query=Leskovec%2C+J">Jure Leskovec</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Graph Neural Networks, Mixture-of-experts, Distribution Shifts, Generalization
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1339">[1339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.04704" title="Abstract">arXiv:2312.04704</a> (replaced) [<a href="/pdf/2312.04704" title="Download PDF">pdf</a>, <a href="/format/2312.04704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Parallel Reinforcement Learning Framework using the Reactor  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kwok%2C+J">Jacky Kwok</a>, 
<a href="/search/cs?searchtype=author&query=Lohstroh%2C+M">Marten Lohstroh</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+E+A">Edward A. Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1340">[1340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.04792" title="Abstract">arXiv:2312.04792</a> (replaced) [<a href="/pdf/2312.04792" title="Download PDF">pdf</a>, <a href="/format/2312.04792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Playing Large Games with Oracles and AI Debate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Angelica Chen</a>, 
<a href="/search/cs?searchtype=author&query=Foster%2C+D">Dean Foster</a>, 
<a href="/search/cs?searchtype=author&query=Hazan%2C+E">Elad Hazan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1341">[1341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.05332" title="Abstract">arXiv:2312.05332</a> (replaced) [<a href="/pdf/2312.05332" title="Download PDF">pdf</a>, <a href="/format/2312.05332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging the Gaps: Learning Verifiable Model-Free Quadratic Programming  Controllers Inspired by Model Predictive Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lu%2C+Y">Yiwen Lu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Z">Zishuo Li</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+Y">Yihan Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+N">Na Li</a>, 
<a href="/search/eess?searchtype=author&query=Mo%2C+Y">Yilin Mo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG); Robotics (cs.RO); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1342">[1342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.05647" title="Abstract">arXiv:2312.05647</a> (replaced) [<a href="/pdf/2312.05647" title="Download PDF">pdf</a>, <a href="/format/2312.05647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Avoiding matrix exponentials for large transition rate matrices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Pessoa%2C+P">Pedro Pessoa</a>, 
<a href="/search/physics?searchtype=author&query=Schweiger%2C+M">Max Schweiger</a>, 
<a href="/search/physics?searchtype=author&query=Presse%2C+S">Steve Presse</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Numerical Analysis (math.NA); Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item1343">[1343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.06825" title="Abstract">arXiv:2312.06825</a> (replaced) [<a href="/pdf/2312.06825" title="Download PDF">pdf</a>, <a href="/ps/2312.06825" title="Download PostScript">ps</a>, <a href="/format/2312.06825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Utilization of Non-verbal Behaviour and Social Gaze in Classroom  Human-Robot Interaction Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shaghaghi%2C+S">Sahand Shaghaghi</a>, 
<a href="/search/cs?searchtype=author&query=Aliasghari%2C+P">Pourya Aliasghari</a>, 
<a href="/search/cs?searchtype=author&query=Tripp%2C+B">Bryan Tripp</a>, 
<a href="/search/cs?searchtype=author&query=Dautenhahn%2C+K">Kerstin Dautenhahn</a>, 
<a href="/search/cs?searchtype=author&query=Nehaniv%2C+C">Chrystopher Nehaniv</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In WTF Workshop Proceedings (<a href="/abs/2401.04108">arXiv:2401.04108</a>) held in conjunction with the ACM conference on Conversational User Interfaces (CUI), 19 - 21/07 2023, in Eindhoven, The Netherlands
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1344">[1344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.06837" title="Abstract">arXiv:2312.06837</a> (replaced) [<a href="/pdf/2312.06837" title="Download PDF">pdf</a>, <a href="/format/2312.06837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral State Space Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+N">Naman Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Suo%2C+D">Daniel Suo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hazan%2C+E">Elad Hazan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1345">[1345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07125" title="Abstract">arXiv:2312.07125</a> (replaced) [<a href="/pdf/2312.07125" title="Download PDF">pdf</a>, <a href="/format/2312.07125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TransMed: Large Language Models Enhance Vision Transformer for  Biomedical Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+K">Kaipeng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weiran Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lichao Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1346">[1346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08983" title="Abstract">arXiv:2312.08983</a> (replaced) [<a href="/pdf/2312.08983" title="Download PDF">pdf</a>, <a href="/format/2312.08983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interactive Humanoid: Online Full-Body Motion Reaction Synthesis with  Social Affordance Canonicalization and Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yunze Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Changxi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+L">Li Yi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1347">[1347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.09397" title="Abstract">arXiv:2312.09397</a> (replaced) [<a href="/pdf/2312.09397" title="Download PDF">pdf</a>, <a href="/format/2312.09397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models for Autonomous Driving: Real-World Experiments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+C">Can Cui</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zichong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yupeng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yunsheng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Juanwu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lingxi Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yaobin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Panchal%2C+J">Jitesh Panchal</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziran Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1348">[1348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.10807" title="Abstract">arXiv:2312.10807</a> (replaced) [<a href="/pdf/2312.10807" title="Download PDF">pdf</a>, <a href="/format/2312.10807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language-conditioned Learning for Robotic Manipulation: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hongkuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+X">Xiangtong Yao</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Y">Yuan Meng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Siming Sun</a>, 
<a href="/search/cs?searchtype=author&query=Bing%2C+Z">Zhenshan Bing</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kai Huang</a>, 
<a href="/search/cs?searchtype=author&query=Knoll%2C+A">Alois Knoll</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1349">[1349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11282" title="Abstract">arXiv:2312.11282</a> (replaced) [<a href="/pdf/2312.11282" title="Download PDF">pdf</a>, <a href="/format/2312.11282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating and Enhancing Large Language Models for Conversational  Reasoning on Knowledge Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuxuan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+L">Lida Shi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Anqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hao Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1350">[1350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11540" title="Abstract">arXiv:2312.11540</a> (replaced) [<a href="/pdf/2312.11540" title="Download PDF">pdf</a>, <a href="/ps/2312.11540" title="Download PostScript">ps</a>, <a href="/format/2312.11540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Trade-off between the Number of Nodes and the Number of Trees in  a Random Forest
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akutsu%2C+T">Tatsuya Akutsu</a>, 
<a href="/search/cs?searchtype=author&query=Melkman%2C+A+A">Avraham A. Melkman</a>, 
<a href="/search/cs?searchtype=author&query=Takasu%2C+A">Atsuhiro Takasu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1351">[1351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11872" title="Abstract">arXiv:2312.11872</a> (replaced) [<a href="/pdf/2312.11872" title="Download PDF">pdf</a>, <a href="/format/2312.11872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Prototypes: Semantic Anchor Regularization for Better  Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ge%2C+Y">Yanqi Ge</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+Q">Qiang Nie</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Ye Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+F">Feng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wen Li</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+L">Lixin Duan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1352">[1352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12964" title="Abstract">arXiv:2312.12964</a> (replaced) [<a href="/pdf/2312.12964" title="Download PDF">pdf</a>, <a href="/format/2312.12964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Far- and Near-Field Channel Measurements and Characterization in the  Terahertz Band Using a Virtual Antenna Array
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiqin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Shu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Chong Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1353">[1353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.13434" title="Abstract">arXiv:2312.13434</a> (replaced) [<a href="/pdf/2312.13434" title="Download PDF">pdf</a>, <a href="/format/2312.13434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-1-to-3: Domain-level Zero-shot Cognitive Diagnosis via One Batch of  Early-bird Students towards Three Diagnostic Objectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+W">Weibo Gao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+L">Linan Yue</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+H">Haoyang Bi</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yin Gu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+F">Fangzhou Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yuanjing He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item1354">[1354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.13508" title="Abstract">arXiv:2312.13508</a> (replaced) [<a href="/pdf/2312.13508" title="Download PDF">pdf</a>, <a href="/format/2312.13508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Federated Learning with Missing Modality via Prototype Mask  and Contrast
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bao%2C+G">Guangyin Bao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+D">Duoqian Miao</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Z">Zixuan Gong</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+L">Liang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Ke Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chongyang Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item1355">[1355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.13560" title="Abstract">arXiv:2312.13560</a> (replaced) [<a href="/pdf/2312.13560" title="Download PDF">pdf</a>, <a href="/format/2312.13560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> kNN-CTC: Enhancing ASR via Retrieval of CTC Pseudo Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiaming Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shiwan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yaqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+W">Wenjia Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yong Qin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1356">[1356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.13933" title="Abstract">arXiv:2312.13933</a> (replaced) [<a href="/pdf/2312.13933" title="Download PDF">pdf</a>, <a href="/format/2312.13933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structured Probabilistic Coding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+D">Dou Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+L">Lingwei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yaxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Songlin Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, accepted by AAAI 2024 (Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1357">[1357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15574" title="Abstract">arXiv:2312.15574</a> (replaced) [<a href="/pdf/2312.15574" title="Download PDF">pdf</a>, <a href="/format/2312.15574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faster Rates for Switchback Experiments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jia%2C+S">Su Jia</a>, 
<a href="/search/math?searchtype=author&query=Kallus%2C+N">Nathan Kallus</a>, 
<a href="/search/math?searchtype=author&query=Yu%2C+C+L">Christina Lee Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1358">[1358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15740" title="Abstract">arXiv:2312.15740</a> (replaced) [<a href="/pdf/2312.15740" title="Download PDF">pdf</a>, <a href="/format/2312.15740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BiSwift: Bandwidth Orchestrator for Multi-Stream Video Analytics on Edge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weijun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+T">Tingting Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+L">Liang Mi</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+H">Haipeng Dai</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yunxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+X">Xiaoming Fu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by 2024 IEEE INFOCOM
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1359">[1359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.16043" title="Abstract">arXiv:2312.16043</a> (replaced) [<a href="/pdf/2312.16043" title="Download PDF">pdf</a>, <a href="/format/2312.16043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An extended asymmetric sigmoid with Perceptron (SIGTRON) for imbalanced  linear classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Woo%2C+H">Hyenkyun Woo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 9 figures, a typo is corrected
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1360">[1360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.16046" title="Abstract">arXiv:2312.16046</a> (replaced) [<a href="/pdf/2312.16046" title="Download PDF">pdf</a>, <a href="/format/2312.16046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaNAS: Adaptively Post-processing with Self-supervised Neural  Architecture Search for Ensemble Rainfall Forecasts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+Y">Yingpeng Wen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Weijiang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+F">Fudan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Dan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+N">Nong Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
</div>
</dd>
<dt><a name="item1361">[1361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.16127" title="Abstract">arXiv:2312.16127</a> (replaced) [<a href="/pdf/2312.16127" title="Download PDF">pdf</a>, <a href="/format/2312.16127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-SAP: Large Language Model Situational Awareness Based Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liman Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+H">Hanyang Zhong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages including appendix. Website:<a href="https://github.com/HanyangZhong/Situational_Planning_datasets">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1362">[1362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.16314" title="Abstract">arXiv:2312.16314</a> (replaced) [<a href="/pdf/2312.16314" title="Download PDF">pdf</a>, <a href="/format/2312.16314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mathematical LoRE: Local Recovery of Erasures using Polynomials, Curves,  Surfaces, and Liftings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haymaker%2C+K">Kathryn Haymaker</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%B3pez%2C+H+H">Hiram H. L&#xf3;pez</a>, 
<a href="/search/cs?searchtype=author&query=Malmskog%2C+B">Beth Malmskog</a>, 
<a href="/search/cs?searchtype=author&query=Matthews%2C+G+L">Gretchen L. Matthews</a>, 
<a href="/search/cs?searchtype=author&query=Pi%C3%B1ero%2C+F">Fernando Pi&#xf1;ero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE BITS the Information Theory Magazine, to appear
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Commutative Algebra (math.AC); Algebraic Geometry (math.AG)

</div>
</div>
</dd>
<dt><a name="item1363">[1363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.17296" title="Abstract">arXiv:2312.17296</a> (replaced) [<a href="/pdf/2312.17296" title="Download PDF">pdf</a>, <a href="/format/2312.17296" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structured Packing in LLM Training Improves Long Context Utilization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Staniszewski%2C+K">Konrad Staniszewski</a>, 
<a href="/search/cs?searchtype=author&query=Tworkowski%2C+S">Szymon Tworkowski</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Jaszczur%2C+S">Sebastian Jaszczur</a>, 
<a href="/search/cs?searchtype=author&query=Michalewski%2C+H">Henryk Michalewski</a>, 
<a href="/search/cs?searchtype=author&query=Kuci%C5%84ski%2C+%C5%81">&#x141;ukasz Kuci&#x144;ski</a>, 
<a href="/search/cs?searchtype=author&query=Mi%C5%82o%C5%9B%2C+P">Piotr Mi&#x142;o&#x15b;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1364">[1364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.17612" title="Abstract">arXiv:2312.17612</a> (replaced) [<a href="/pdf/2312.17612" title="Download PDF">pdf</a>, <a href="/format/2312.17612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bespoke Approximation of Multiplication-Accumulation and Activation  Targeting Printed Multilayer Perceptrons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Afentaki%2C+F">Florentia Afentaki</a>, 
<a href="/search/cs?searchtype=author&query=Saglam%2C+G">Gurol Saglam</a>, 
<a href="/search/cs?searchtype=author&query=Kokkinis%2C+A">Argyris Kokkinis</a>, 
<a href="/search/cs?searchtype=author&query=Siozios%2C+K">Kostas Siozios</a>, 
<a href="/search/cs?searchtype=author&query=Zervakis%2C+G">Georgios Zervakis</a>, 
<a href="/search/cs?searchtype=author&query=Tahoori%2C+M+B">Mehdi B Tahoori</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at the 42th IEEE/ACM International Conference on Computer Aided Design (ICCAD) 2023, San Francisco, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1365">[1365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.17683" title="Abstract">arXiv:2312.17683</a> (replaced) [<a href="/pdf/2312.17683" title="Download PDF">pdf</a>, <a href="/ps/2312.17683" title="Download PostScript">ps</a>, <a href="/format/2312.17683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Malware Detection in IOT Systems Using Machine Learning Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mehrban%2C+A">Ali Mehrban</a>, 
<a href="/search/cs?searchtype=author&query=Ahadian%2C+P">Pegah Ahadian</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Wireless &amp; Mobile Networks (IJWMN),
  Vol.15, No.6, December 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item1366">[1366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.00006" title="Abstract">arXiv:2401.00006</a> (replaced) [<a href="/pdf/2401.00006" title="Download PDF">pdf</a>, <a href="/format/2401.00006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Building Open-Ended Embodied Agent via Language-Policy Bidirectional  Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhai%2C+S">Shaopeng Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fuxian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Ming Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+J">Jing Hou</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1367">[1367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.01325" title="Abstract">arXiv:2401.01325</a> (replaced) [<a href="/pdf/2401.01325" title="Download PDF">pdf</a>, <a href="/format/2401.01325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+H">Hongye Jin</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xiaotian Han</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jingfeng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhimeng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zirui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+C">Chia-Yuan Chang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huiyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xia Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1368">[1368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.01836" title="Abstract">arXiv:2401.01836</a> (replaced) [<a href="/pdf/2401.01836" title="Download PDF">pdf</a>, <a href="/format/2401.01836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Control: Concurrent System Identification and Control Learning  with Neural ODE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chi%2C+C">Cheng Chi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, code open sourced in format of Google Colab notebooks; Resubmitted for adding missed references in the last submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1369">[1369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.02130" title="Abstract">arXiv:2401.02130</a> (replaced) [<a href="/pdf/2401.02130" title="Download PDF">pdf</a>, <a href="/format/2401.02130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral-Based Graph Neural Networks for Complementary Item  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haitong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+X">Xuying Meng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Suhang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+H">Hanyun Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weiyao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yequan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yujun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI-24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item1370">[1370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.02378" title="Abstract">arXiv:2401.02378</a> (replaced) [<a href="/pdf/2401.02378" title="Download PDF">pdf</a>, <a href="/format/2401.02378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Opinion formation in the world trade network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Coquid%C3%A9%2C+C">C&#xe9;lestin Coquid&#xe9;</a>, 
<a href="/search/q-fin?searchtype=author&query=Lages%2C+J">Jos&#xe9; Lages</a>, 
<a href="/search/q-fin?searchtype=author&query=Shepelyansky%2C+D+L">Dima L. Shepelyansky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 19 figures (including 9 figures present in Appendix section) and 1 table
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Coquid\'{e} C., Lages J. and Shepelyansky D.L., Opinion formation
  in the world trade network. Entropy 2024, 26(2), 141
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Trading and Market Microstructure (q-fin.TR)</span>; Statistical Mechanics (cond-mat.stat-mech); Social and Information Networks (cs.SI); Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item1371">[1371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.02647" title="Abstract">arXiv:2401.02647</a> (replaced) [<a href="/pdf/2401.02647" title="Download PDF">pdf</a>, <a href="/format/2401.02647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Technical Report: Modeling Average False Positive Rates of Recycling  Bloom Filters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dozier%2C+K">Kahlil Dozier</a>, 
<a href="/search/cs?searchtype=author&query=Salamatian%2C+L">Loqman Salamatian</a>, 
<a href="/search/cs?searchtype=author&query=Rubenstein%2C+D">Dan Rubenstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item1372">[1372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.03735" title="Abstract">arXiv:2401.03735</a> (replaced) [<a href="/pdf/2401.03735" title="Download PDF">pdf</a>, <a href="/format/2401.03735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Models Understand Numbers, at Least Partially
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+F">Fangwei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+D">Damai Dai</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+Z">Zhifang Sui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1373">[1373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04403" title="Abstract">arXiv:2401.04403</a> (replaced) [<a href="/pdf/2401.04403" title="Download PDF">pdf</a>, <a href="/format/2401.04403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MST: Adaptive Multi-Scale Tokens Guided Interactive Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Long Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shanghong Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yongquan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jun Luo</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+S">Shiwu Lai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1374">[1374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04700" title="Abstract">arXiv:2401.04700</a> (replaced) [<a href="/pdf/2401.04700" title="Download PDF">pdf</a>, <a href="/format/2401.04700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Editing Can Hurt General Abilities of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jia-Chen Gu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hao-Xiang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jun-Yu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+P">Pan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+Z">Zhen-Hua Ling</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+N">Nanyun Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Add new results on LLaMA-2 (7B)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1375">[1375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04883" title="Abstract">arXiv:2401.04883</a> (replaced) [<a href="/pdf/2401.04883" title="Download PDF">pdf</a>, <a href="/format/2401.04883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-User Chat Assistant (MUCA): a Framework Using LLMs to Facilitate  Group Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+M">Manqing Mao</a>, 
<a href="/search/cs?searchtype=author&query=Ting%2C+P">Paishun Ting</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Y">Yijian Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mingyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Julia Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jianzhe Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1376">[1376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05043" title="Abstract">arXiv:2401.05043</a> (replaced) [<a href="/pdf/2401.05043" title="Download PDF">pdf</a>, <a href="/format/2401.05043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CreINNs: Credal-Set Interval Neural Networks for Uncertainty Estimation  in Classification Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaizheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shariatmadar%2C+K">Keivan Shariatmadar</a>, 
<a href="/search/cs?searchtype=author&query=Manchingal%2C+S+K">Shireen Kudukkil Manchingal</a>, 
<a href="/search/cs?searchtype=author&query=Cuzzolin%2C+F">Fabio Cuzzolin</a>, 
<a href="/search/cs?searchtype=author&query=Moens%2C+D">David Moens</a>, 
<a href="/search/cs?searchtype=author&query=Hallez%2C+H">Hans Hallez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1377">[1377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05441" title="Abstract">arXiv:2401.05441</a> (replaced) [<a href="/pdf/2401.05441" title="Download PDF">pdf</a>, <a href="/ps/2401.05441" title="Download PostScript">ps</a>, <a href="/format/2401.05441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An adaptive network-based approach for advanced forecasting of  cryptocurrency values
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Mehrban%2C+A">Ali Mehrban</a>, 
<a href="/search/q-fin?searchtype=author&query=Ahadian%2C+P">Pegah Ahadian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Computer Science and Information
  Technology (IJCSIT), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Finance (q-fin.ST)</span>; Computational Engineering, Finance, and Science (cs.CE); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1378">[1378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05631" title="Abstract">arXiv:2401.05631</a> (replaced) [<a href="/pdf/2401.05631" title="Download PDF">pdf</a>, <a href="/format/2401.05631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DrawTalking: Building Interactive Worlds by Sketching and Speaking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rosenberg%2C+K+T">Karl Toby Rosenberg</a>, 
<a href="/search/cs?searchtype=author&query=Kazi%2C+R+H">Rubaiat Habib Kazi</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+L">Li-Yi Wei</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+H">Haijun Xia</a>, 
<a href="/search/cs?searchtype=author&query=Perlin%2C+K">Ken Perlin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item1379">[1379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05657" title="Abstract">arXiv:2401.05657</a> (replaced) [<a href="/pdf/2401.05657" title="Download PDF">pdf</a>, <a href="/ps/2401.05657" title="Download PostScript">ps</a>, <a href="/format/2401.05657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An impossibility theorem concerning positive involvement in voting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Holliday%2C+W+H">Wesley H. Holliday</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 2 table, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item1380">[1380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05738" title="Abstract">arXiv:2401.05738</a> (replaced) [<a href="/pdf/2401.05738" title="Download PDF">pdf</a>, <a href="/format/2401.05738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LKCA: Large Kernel Convolutional Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenghao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+B">Boheng Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yi Lu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+P">Pengbo Shi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qingzi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jirui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lingyun Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1381">[1381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05779" title="Abstract">arXiv:2401.05779</a> (replaced) [<a href="/pdf/2401.05779" title="Download PDF">pdf</a>, <a href="/format/2401.05779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EraseDiff: Erasing Data Influence in Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Trung Le</a>, 
<a href="/search/cs?searchtype=author&query=Hayat%2C+M">Munawar Hayat</a>, 
<a href="/search/cs?searchtype=author&query=Harandi%2C+M">Mehrtash Harandi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Diffusion Model, Machine Unlearning
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1382">[1382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07242" title="Abstract">arXiv:2401.07242</a> (replaced) [<a href="/pdf/2401.07242" title="Download PDF">pdf</a>, <a href="/ps/2401.07242" title="Download PostScript">ps</a>, <a href="/format/2401.07242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Testing Sumsets is Hard
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Nadimpalli%2C+S">Shivam Nadimpalli</a>, 
<a href="/search/cs?searchtype=author&query=Randolph%2C+T">Tim Randolph</a>, 
<a href="/search/cs?searchtype=author&query=Servedio%2C+R+A">Rocco A. Servedio</a>, 
<a href="/search/cs?searchtype=author&query=Zamir%2C+O">Or Zamir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item1383">[1383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07353" title="Abstract">arXiv:2401.07353</a> (replaced) [<a href="/pdf/2401.07353" title="Download PDF">pdf</a>, <a href="/format/2401.07353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Engineering Fair and Equitable Software Systems for Managing  Low-Altitude Airspace Authorizations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gohar%2C+U">Usman Gohar</a>, 
<a href="/search/cs?searchtype=author&query=Hunter%2C+M+C">Michael C. Hunter</a>, 
<a href="/search/cs?searchtype=author&query=Marczak-Czajka%2C+A">Agnieszka Marczak-Czajka</a>, 
<a href="/search/cs?searchtype=author&query=Lutz%2C+R+R">Robyn R. Lutz</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+M+B">Myra B. Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Cleland-Huang%2C+J">Jane Cleland-Huang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICSE-SEIS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1384">[1384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07402" title="Abstract">arXiv:2401.07402</a> (replaced) [<a href="/pdf/2401.07402" title="Download PDF">pdf</a>, <a href="/format/2401.07402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Implicit Neural Representation with Fourier Bases  Reparameterized Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+K">Kexuan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xingyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+S">Shuhang Gu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1385">[1385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07964" title="Abstract">arXiv:2401.07964</a> (replaced) [<a href="/pdf/2401.07964" title="Download PDF">pdf</a>, <a href="/ps/2401.07964" title="Download PostScript">ps</a>, <a href="/format/2401.07964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI-as-exploration: Navigating intelligence space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mollo%2C+D+C">Dimitri Coelho Mollo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1386">[1386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08035" title="Abstract">arXiv:2401.08035</a> (replaced) [<a href="/pdf/2401.08035" title="Download PDF">pdf</a>, <a href="/format/2401.08035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BanglaNet: Bangla Handwritten Character Recognition using Ensembling of  Convolutional Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saha%2C+C">Chandrika Saha</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+M">Md Mostafijur Rahman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1387">[1387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08224" title="Abstract">arXiv:2401.08224</a> (replaced) [<a href="/pdf/2401.08224" title="Download PDF">pdf</a>, <a href="/format/2401.08224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy Preserving Adaptive Experiment Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+J">Jiachun Li</a>, 
<a href="/search/stat?searchtype=author&query=Shi%2C+K">Kaining Shi</a>, 
<a href="/search/stat?searchtype=author&query=Simchi-Levi%2C+D">David Simchi-Levi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Add a table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1388">[1388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08572" title="Abstract">arXiv:2401.08572</a> (replaced) [<a href="/pdf/2401.08572" title="Download PDF">pdf</a>, <a href="/format/2401.08572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The illusion of artificial inclusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agnew%2C+W">William Agnew</a>, 
<a href="/search/cs?searchtype=author&query=Bergman%2C+A+S">A. Stevie Bergman</a>, 
<a href="/search/cs?searchtype=author&query=Chien%2C+J">Jennifer Chien</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%ADaz%2C+M">Mark D&#xed;az</a>, 
<a href="/search/cs?searchtype=author&query=El-Sayed%2C+S">Seliem El-Sayed</a>, 
<a href="/search/cs?searchtype=author&query=Pittman%2C+J">Jaylen Pittman</a>, 
<a href="/search/cs?searchtype=author&query=Mohamed%2C+S">Shakir Mohamed</a>, 
<a href="/search/cs?searchtype=author&query=McKee%2C+K+R">Kevin R. McKee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item1389">[1389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08702" title="Abstract">arXiv:2401.08702</a> (replaced) [<a href="/pdf/2401.08702" title="Download PDF">pdf</a>, <a href="/format/2401.08702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do We Really Even Need Data?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hoffman%2C+K">Kentaro Hoffman</a>, 
<a href="/search/stat?searchtype=author&query=Salerno%2C+S">Stephen Salerno</a>, 
<a href="/search/stat?searchtype=author&query=Afiaz%2C+A">Awan Afiaz</a>, 
<a href="/search/stat?searchtype=author&query=Leek%2C+J+T">Jeffrey T. Leek</a>, 
<a href="/search/stat?searchtype=author&query=McCormick%2C+T+H">Tyler H. McCormick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1390">[1390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08875" title="Abstract">arXiv:2401.08875</a> (replaced) [<a href="/pdf/2401.08875" title="Download PDF">pdf</a>, <a href="/ps/2401.08875" title="Download PostScript">ps</a>, <a href="/format/2401.08875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DCRMTA: Unbiased Causal Representation for Multi-touch Attribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jiaming Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item1391">[1391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.09486" title="Abstract">arXiv:2401.09486</a> (replaced) [<a href="/pdf/2401.09486" title="Download PDF">pdf</a>, <a href="/format/2401.09486" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoMA: Lossless Compressed Memory Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yumeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Zhenyang Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1392">[1392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10244" title="Abstract">arXiv:2401.10244</a> (replaced) [<a href="/pdf/2401.10244" title="Download PDF">pdf</a>, <a href="/ps/2401.10244" title="Download PostScript">ps</a>, <a href="/format/2401.10244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Graph Driven Recommendation System Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chaoyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yanan Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+S">Siwei Fan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wei Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1393">[1393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10521" title="Abstract">arXiv:2401.10521</a> (replaced) [<a href="/pdf/2401.10521" title="Download PDF">pdf</a>, <a href="/format/2401.10521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-lingual Editing in Multilingual Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beniwal%2C+H">Himanshu Beniwal</a>, 
<a href="/search/cs?searchtype=author&query=D%2C+K+N">Kowsik Nandagopan D</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Mayank Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1394">[1394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10845" title="Abstract">arXiv:2401.10845</a> (replaced) [<a href="/pdf/2401.10845" title="Download PDF">pdf</a>, <a href="/format/2401.10845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emotion Classification In Software Engineering Texts: A Comparative  Analysis of Pre-trained Transformers Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Imran%2C+M+M">Mia Mohammad Imran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item1395">[1395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11268" title="Abstract">arXiv:2401.11268</a> (replaced) [<a href="/pdf/2401.11268" title="Download PDF">pdf</a>, <a href="/format/2401.11268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Word-Level ASR Quality Estimation for Efficient Corpus Sampling and  Post-Editing through Analyzing Attentions of a Reference-Free Metric
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Javadi%2C+G">Golara Javadi</a>, 
<a href="/search/cs?searchtype=author&query=Yuksel%2C+K+A">Kamer Ali Yuksel</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yunsu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ferreira%2C+T+C">Thiago Castro Ferreira</a>, 
<a href="/search/cs?searchtype=author&query=Al-Badrashiny%2C+M">Mohamed Al-Badrashiny</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2024 IEEE International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP 2024), Seoul, Korea
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1396">[1396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11373" title="Abstract">arXiv:2401.11373</a> (replaced) [<a href="/pdf/2401.11373" title="Download PDF">pdf</a>, <a href="/format/2401.11373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding a Needle in the Adversarial Haystack: A Targeted Paraphrasing  Approach For Uncovering Edge Cases with Minimal Distribution Distortion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kassem%2C+A+M">Aly M. Kassem</a>, 
<a href="/search/cs?searchtype=author&query=Saad%2C+S">Sherif Saad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024 - Main conference - Camera ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1397">[1397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11511" title="Abstract">arXiv:2401.11511</a> (replaced) [<a href="/pdf/2401.11511" title="Download PDF">pdf</a>, <a href="/format/2401.11511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MobileARLoc: On-device Robust Absolute Localisation for Pervasive  Markerless Mobile AR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Changkun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yukun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Braud%2C+T">Tristan Braud</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at the 3rd edition of the Pervasive and Resource-Constrained AI (PerConAI) workshop (co-located with PerCom 2024). This article supersedes <a href="/abs/2308.05394">arXiv:2308.05394</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1398">[1398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11617" title="Abstract">arXiv:2401.11617</a> (replaced) [<a href="/pdf/2401.11617" title="Download PDF">pdf</a>, <a href="/format/2401.11617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on African Computer Vision Datasets, Topics and Researchers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Omotayo%2C+A">Abdul-Hakeem Omotayo</a>, 
<a href="/search/cs?searchtype=author&query=Mbilinyi%2C+A">Ashery Mbilinyi</a>, 
<a href="/search/cs?searchtype=author&query=Ismaila%2C+L">Lukman Ismaila</a>, 
<a href="/search/cs?searchtype=author&query=Turki%2C+H">Houcemeddine Turki</a>, 
<a href="/search/cs?searchtype=author&query=Abdien%2C+M">Mahmoud Abdien</a>, 
<a href="/search/cs?searchtype=author&query=Gamal%2C+K">Karim Gamal</a>, 
<a href="/search/cs?searchtype=author&query=Tondji%2C+I">Idriss Tondji</a>, 
<a href="/search/cs?searchtype=author&query=Pimi%2C+Y">Yvan Pimi</a>, 
<a href="/search/cs?searchtype=author&query=Etori%2C+N+A">Naome A. Etori</a>, 
<a href="/search/cs?searchtype=author&query=Matar%2C+M+M">Marwa M. Matar</a>, 
<a href="/search/cs?searchtype=author&query=Broni-Bediako%2C+C">Clifford Broni-Bediako</a>, 
<a href="/search/cs?searchtype=author&query=Oppong%2C+A">Abigail Oppong</a>, 
<a href="/search/cs?searchtype=author&query=Gamal%2C+M">Mai Gamal</a>, 
<a href="/search/cs?searchtype=author&query=Ehab%2C+E">Eman Ehab</a>, 
<a href="/search/cs?searchtype=author&query=Dovonon%2C+G">Gbetondji Dovonon</a>, 
<a href="/search/cs?searchtype=author&query=Akinjobi%2C+Z">Zainab Akinjobi</a>, 
<a href="/search/cs?searchtype=author&query=Ajisafe%2C+D">Daniel Ajisafe</a>, 
<a href="/search/cs?searchtype=author&query=Adegboro%2C+O+G">Oluwabukola G. Adegboro</a>, 
<a href="/search/cs?searchtype=author&query=Siam%2C+M">Mennatullah Siam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review, Community Work of Ro'ya Grassroots, <a href="https://ro-ya-cv4africa.github.io/homepage/.Journal">this https URL</a> extension of our conference paper, arXiv admin note: text overlap with <a href="/abs/2305.06773">arXiv:2305.06773</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1399">[1399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11782" title="Abstract">arXiv:2401.11782</a> (replaced) [<a href="/pdf/2401.11782" title="Download PDF">pdf</a>, <a href="/format/2401.11782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporal Interaction and its Role in the Evolution of Cooperation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=He%2C+Y">Yujie He</a>, 
<a href="/search/physics?searchtype=author&query=Ren%2C+T">Tianyu Ren</a>, 
<a href="/search/physics?searchtype=author&query=Zeng%2C+X">Xiao-Jun Zeng</a>, 
<a href="/search/physics?searchtype=author&query=Liang%2C+H">Huawen Liang</a>, 
<a href="/search/physics?searchtype=author&query=Yu%2C+L">Liukai Yu</a>, 
<a href="/search/physics?searchtype=author&query=Zheng%2C+J">Junjun Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI); Populations and Evolution (q-bio.PE)

</div>
</div>
</dd>
<dt><a name="item1400">[1400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11929" title="Abstract">arXiv:2401.11929</a> (replaced) [<a href="/pdf/2401.11929" title="Download PDF">pdf</a>, <a href="/format/2401.11929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Bigger the Better? Rethinking the Effective Model Scale in Long-term  Time Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jinliang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xuan Song</a>, 
<a href="/search/cs?searchtype=author&query=Tsang%2C+I+W">Ivor W. Tsang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Hui Xiong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1401">[1401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11940" title="Abstract">arXiv:2401.11940</a> (replaced) [<a href="/pdf/2401.11940" title="Download PDF">pdf</a>, <a href="/format/2401.11940" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Tubal-Rank Tensor Recovery via Factorized Gradient Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zhi Han</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yandong Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xi-Le Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1402">[1402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12480" title="Abstract">arXiv:2401.12480</a> (replaced) [<a href="/pdf/2401.12480" title="Download PDF">pdf</a>, <a href="/format/2401.12480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explore Synergistic Interaction Across Frames for Interactive Video  Object Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kexin Li</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+T">Tao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zongxin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Y">Yueting Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jun Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1403">[1403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12585" title="Abstract">arXiv:2401.12585</a> (replaced) [<a href="/pdf/2401.12585" title="Download PDF">pdf</a>, <a href="/format/2401.12585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SLANG: New Concept Comprehension of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mei%2C+L">Lingrui Mei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shenghua Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+B">Baolong Bi</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1404">[1404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12768" title="Abstract">arXiv:2401.12768</a> (replaced) [<a href="/pdf/2401.12768" title="Download PDF">pdf</a>, <a href="/format/2401.12768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Can Self-Admitted Technical Debt Tell Us About Security? A  Mixed-Methods Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferreyra%2C+N+E+D">Nicol&#xe1;s E. D&#xed;az Ferreyra</a>, 
<a href="/search/cs?searchtype=author&query=Shahin%2C+M">Mojtaba Shahin</a>, 
<a href="/search/cs?searchtype=author&query=Zahedi%2C+M">Mansooreh Zahedi</a>, 
<a href="/search/cs?searchtype=author&query=Quadri%2C+S">Sodiq Quadri</a>, 
<a href="/search/cs?searchtype=author&query=Scandariato%2C+R">Ricardo Scandariato</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in the 21th International Conference on Mining Software Repositories (MSR '24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item1405">[1405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12945" title="Abstract">arXiv:2401.12945</a> (replaced) [<a href="/pdf/2401.12945" title="Download PDF">pdf</a>, <a href="/format/2401.12945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lumiere: A Space-Time Diffusion Model for Video Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bar-Tal%2C+O">Omer Bar-Tal</a>, 
<a href="/search/cs?searchtype=author&query=Chefer%2C+H">Hila Chefer</a>, 
<a href="/search/cs?searchtype=author&query=Tov%2C+O">Omer Tov</a>, 
<a href="/search/cs?searchtype=author&query=Herrmann%2C+C">Charles Herrmann</a>, 
<a href="/search/cs?searchtype=author&query=Paiss%2C+R">Roni Paiss</a>, 
<a href="/search/cs?searchtype=author&query=Zada%2C+S">Shiran Zada</a>, 
<a href="/search/cs?searchtype=author&query=Ephrat%2C+A">Ariel Ephrat</a>, 
<a href="/search/cs?searchtype=author&query=Hur%2C+J">Junhwa Hur</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Guanghui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+A">Amit Raj</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuanzhen Li</a>, 
<a href="/search/cs?searchtype=author&query=Rubinstein%2C+M">Michael Rubinstein</a>, 
<a href="/search/cs?searchtype=author&query=Michaeli%2C+T">Tomer Michaeli</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+O">Oliver Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+D">Deqing Sun</a>, 
<a href="/search/cs?searchtype=author&query=Dekel%2C+T">Tali Dekel</a>, 
<a href="/search/cs?searchtype=author&query=Mosseri%2C+I">Inbar Mosseri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Webpage: <a href="https://lumiere-video.github.io/">this https URL</a> | Video: <a href="https://www.youtube.com/watch?v=wxLr02Dz2Sc">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1406">[1406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13138" title="Abstract">arXiv:2401.13138</a> (replaced) [<a href="/pdf/2401.13138" title="Download PDF">pdf</a>, <a href="/format/2401.13138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visibility into AI Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chan%2C+A">Alan Chan</a>, 
<a href="/search/cs?searchtype=author&query=Ezell%2C+C">Carson Ezell</a>, 
<a href="/search/cs?searchtype=author&query=Kaufmann%2C+M">Max Kaufmann</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+K">Kevin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Hammond%2C+L">Lewis Hammond</a>, 
<a href="/search/cs?searchtype=author&query=Bradley%2C+H">Herbie Bradley</a>, 
<a href="/search/cs?searchtype=author&query=Bluemke%2C+E">Emma Bluemke</a>, 
<a href="/search/cs?searchtype=author&query=Rajkumar%2C+N">Nitarshan Rajkumar</a>, 
<a href="/search/cs?searchtype=author&query=Krueger%2C+D">David Krueger</a>, 
<a href="/search/cs?searchtype=author&query=Kolt%2C+N">Noam Kolt</a>, 
<a href="/search/cs?searchtype=author&query=Heim%2C+L">Lennart Heim</a>, 
<a href="/search/cs?searchtype=author&query=Anderljung%2C+M">Markus Anderljung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1407">[1407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13421" title="Abstract">arXiv:2401.13421</a> (replaced) [<a href="/pdf/2401.13421" title="Download PDF">pdf</a>, <a href="/format/2401.13421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated learning with distributed fixed design quantum chips and  quantum channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Daskin%2C+A">Ammar Daskin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> a few typos are corrected
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1408">[1408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13442" title="Abstract">arXiv:2401.13442</a> (replaced) [<a href="/pdf/2401.13442" title="Download PDF">pdf</a>, <a href="/format/2401.13442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite-Precision Arithmetic Transceiver for Massive MIMO Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yiming Fang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Li Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yunfei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Huarui Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 8 figures. Submitted to IEEE JSAC for possible publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1409">[1409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13565" title="Abstract">arXiv:2401.13565</a> (replaced) [<a href="/pdf/2401.13565" title="Download PDF">pdf</a>, <a href="/format/2401.13565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Malaysian Language Model Based on Mistral for Enhanced Local  Language Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zolkepli%2C+H">Husein Zolkepli</a>, 
<a href="/search/cs?searchtype=author&query=Razak%2C+A">Aisyah Razak</a>, 
<a href="/search/cs?searchtype=author&query=Adha%2C+K">Kamarul Adha</a>, 
<a href="/search/cs?searchtype=author&query=Nazhan%2C+A">Ariff Nazhan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1410">[1410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14005" title="Abstract">arXiv:2401.14005</a> (replaced) [<a href="/pdf/2401.14005" title="Download PDF">pdf</a>, <a href="/format/2401.14005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cyber-Twin: Digital Twin-boosted Autonomous Attack Detection for  Vehicular Ad-Hoc Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yigit%2C+Y">Yagmur Yigit</a>, 
<a href="/search/cs?searchtype=author&query=Panitsas%2C+I">Ioannis Panitsas</a>, 
<a href="/search/cs?searchtype=author&query=Maglaras%2C+L">Leandros Maglaras</a>, 
<a href="/search/cs?searchtype=author&query=Tassiulas%2C+L">Leandros Tassiulas</a>, 
<a href="/search/cs?searchtype=author&query=Canberk%2C+B">Berk Canberk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures, IEEE International Conference on Communications (ICC) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item1411">[1411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14194" title="Abstract">arXiv:2401.14194</a> (replaced) [<a href="/pdf/2401.14194" title="Download PDF">pdf</a>, <a href="/format/2401.14194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameter-Efficient Conversational Recommender System as a Language  Processing Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ravaut%2C+M">Mathieu Ravaut</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+A">Aixin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures, 8 tables, EACL 2024 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1412">[1412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14377" title="Abstract">arXiv:2401.14377</a> (replaced) [<a href="/pdf/2401.14377" title="Download PDF">pdf</a>, <a href="/ps/2401.14377" title="Download PostScript">ps</a>, <a href="/format/2401.14377" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bonding Grammars
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pshenitsyn%2C+T">Tikhon Pshenitsyn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to UCNC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>

</div>
</div>
</dd>
<dt><a name="item1413">[1413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14580" title="Abstract">arXiv:2401.14580</a> (replaced) [<a href="/pdf/2401.14580" title="Download PDF">pdf</a>, <a href="/format/2401.14580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design Your Own Universe: A Physics-Informed Agnostic Method for  Enhancing Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+D">Dai Shi</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+A">Andi Han</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Lequan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhiyong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Junbin Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1414">[1414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14591" title="Abstract">arXiv:2401.14591</a> (replaced) [<a href="/pdf/2401.14591" title="Download PDF">pdf</a>, <a href="/format/2401.14591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ricci flow-guided autoencoders in learning time-dependent dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gracyk%2C+A">Andrew Gracyk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1415">[1415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14705" title="Abstract">arXiv:2401.14705</a> (replaced) [<a href="/pdf/2401.14705" title="Download PDF">pdf</a>, <a href="/format/2401.14705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Additional Look into GAN-based Augmentation for Deep Learning COVID-19  Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fedoruk%2C+O">Oleksandr Fedoruk</a>, 
<a href="/search/eess?searchtype=author&query=Klimaszewski%2C+K">Konrad Klimaszewski</a>, 
<a href="/search/eess?searchtype=author&query=Ogonowski%2C+A">Aleksander Ogonowski</a>, 
<a href="/search/eess?searchtype=author&query=Kruk%2C+M">Micha&#x142; Kruk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Machine Graphics &amp; Vision. Version with updated acknowledgments
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1416">[1416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14831" title="Abstract">arXiv:2401.14831</a> (replaced) [<a href="/pdf/2401.14831" title="Download PDF">pdf</a>, <a href="/format/2401.14831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Machine Vision Iceberg Explained: Advancing Dynamic Testing by  Considering Holistic Environmental Circumstances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Padusinski%2C+H">Hubert Padusinski</a>, 
<a href="/search/cs?searchtype=author&query=Braun%2C+T">Thilo Braun</a>, 
<a href="/search/cs?searchtype=author&query=Steinhauser%2C+C">Christian Steinhauser</a>, 
<a href="/search/cs?searchtype=author&query=Ries%2C+L">Lennart Ries</a>, 
<a href="/search/cs?searchtype=author&query=Sax%2C+E">Eric Sax</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted at IEEE IV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Software Engineering (cs.SE); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item1417">[1417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14999" title="Abstract">arXiv:2401.14999</a> (replaced) [<a href="/pdf/2401.14999" title="Download PDF">pdf</a>, <a href="/format/2401.14999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The causal role of the Reddit collective action on the GameStop short  squeeze
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Desiderio%2C+A">Antonio Desiderio</a>, 
<a href="/search/physics?searchtype=author&query=Aiello%2C+L+M">Luca Maria Aiello</a>, 
<a href="/search/physics?searchtype=author&query=Cimini%2C+G">Giulio Cimini</a>, 
<a href="/search/physics?searchtype=author&query=Alessandretti%2C+L">Laura Alessandretti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Computers and Society (cs.CY); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item1418">[1418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15022" title="Abstract">arXiv:2401.15022</a> (replaced) [<a href="/pdf/2401.15022" title="Download PDF">pdf</a>, <a href="/ps/2401.15022" title="Download PostScript">ps</a>, <a href="/format/2401.15022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Applications of artificial intelligence in the analysis of  histopathology images of gliomas: a review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Redlich%2C+J">Jan-Philipp Redlich</a>, 
<a href="/search/eess?searchtype=author&query=Feuerhake%2C+F">Friedrich Feuerhake</a>, 
<a href="/search/eess?searchtype=author&query=Weis%2C+J">Joachim Weis</a>, 
<a href="/search/eess?searchtype=author&query=Schaadt%2C+N+S">Nadine S. Schaadt</a>, 
<a href="/search/eess?searchtype=author&query=Teuber-Hanselmann%2C+S">Sarah Teuber-Hanselmann</a>, 
<a href="/search/eess?searchtype=author&query=Buck%2C+C">Christoph Buck</a>, 
<a href="/search/eess?searchtype=author&query=Luttmann%2C+S">Sabine Luttmann</a>, 
<a href="/search/eess?searchtype=author&query=Eberle%2C+A">Andrea Eberle</a>, 
<a href="/search/eess?searchtype=author&query=Nikolin%2C+S">Stefan Nikolin</a>, 
<a href="/search/eess?searchtype=author&query=Appenzeller%2C+A">Arno Appenzeller</a>, 
<a href="/search/eess?searchtype=author&query=Portmann%2C+A">Andreas Portmann</a>, 
<a href="/search/eess?searchtype=author&query=Homeyer%2C+A">Andr&#xe9; Homeyer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1419">[1419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15077" title="Abstract">arXiv:2401.15077</a> (replaced) [<a href="/pdf/2401.15077" title="Download PDF">pdf</a>, <a href="/format/2401.15077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuhui Li</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Fangyun Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongyang Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1420">[1420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15222" title="Abstract">arXiv:2401.15222</a> (replaced) [<a href="/pdf/2401.15222" title="Download PDF">pdf</a>, <a href="/format/2401.15222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transfer Learning for the Prediction of Entity Modifiers in Clinical  Text: Application to Opioid Use Disorder Case Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Almudaifer%2C+A+I">Abdullateef I. Almudaifer</a>, 
<a href="/search/cs?searchtype=author&query=Covington%2C+W">Whitney Covington</a>, 
<a href="/search/cs?searchtype=author&query=Hairston%2C+J">JaMor Hairston</a>, 
<a href="/search/cs?searchtype=author&query=Deitch%2C+Z">Zachary Deitch</a>, 
<a href="/search/cs?searchtype=author&query=Anand%2C+A">Ankit Anand</a>, 
<a href="/search/cs?searchtype=author&query=Carroll%2C+C+M">Caleb M. Carroll</a>, 
<a href="/search/cs?searchtype=author&query=Crisan%2C+E">Estera Crisan</a>, 
<a href="/search/cs?searchtype=author&query=Bradford%2C+W">William Bradford</a>, 
<a href="/search/cs?searchtype=author&query=Walter%2C+L">Lauren Walter</a>, 
<a href="/search/cs?searchtype=author&query=Ellen%2C+E">Eaton Ellen</a>, 
<a href="/search/cs?searchtype=author&query=Feldman%2C+S+S">Sue S. Feldman</a>, 
<a href="/search/cs?searchtype=author&query=Osborne%2C+J+D">John D. Osborne</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 2 figures, 6 tables. To be submitted to the Journal of Biomedical Semantics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1421">[1421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15268" title="Abstract">arXiv:2401.15268</a> (replaced) [<a href="/e-print/2401.15268" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Stable Preferences for Stakeholder-aligned Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sheraz%2C+H">Haleema Sheraz</a>, 
<a href="/search/cs?searchtype=author&query=Kremer%2C+S+C">Stefan C. Kremer</a>, 
<a href="/search/cs?searchtype=author&query=Skorburg%2C+J+A">Joshua August Skorburg</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+G">Graham Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Sinnott-Armstrong%2C+W">Walter Sinnott-Armstrong</a>, 
<a href="/search/cs?searchtype=author&query=Boerstler%2C+K">Kyle Boerstler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1422">[1422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15285" title="Abstract">arXiv:2401.15285</a> (replaced) [<a href="/pdf/2401.15285" title="Download PDF">pdf</a>, <a href="/ps/2401.15285" title="Download PostScript">ps</a>, <a href="/format/2401.15285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ransomware threat mitigation through network traffic analysis and  machine learning techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mehrban%2C+A">Ali Mehrban</a>, 
<a href="/search/cs?searchtype=author&query=Geransayeh%2C+S+K">Shirin Karimi Geransayeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1423">[1423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15313" title="Abstract">arXiv:2401.15313</a> (replaced) [<a href="/pdf/2401.15313" title="Download PDF">pdf</a>, <a href="/format/2401.15313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Robot Relative Pose Estimation in SE(2) with Observability  Analysis: A Comparison of Extended Kalman Filtering and Robust Pose Graph  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shin%2C+K">Kihoon Shin</a>, 
<a href="/search/cs?searchtype=author&query=Sim%2C+H">Hyunjae Sim</a>, 
<a href="/search/cs?searchtype=author&query=Nam%2C+S">Seungwon Nam</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yonghee Kim</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jae Hu</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K+K">Kwang-Ki K. Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 21 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item1424">[1424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15638" title="Abstract">arXiv:2401.15638</a> (replaced) [<a href="/pdf/2401.15638" title="Download PDF">pdf</a>, <a href="/format/2401.15638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cyto R-CNN and CytoNuke Dataset: Towards reliable whole-cell  segmentation in bright-field histological images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Raufeisen%2C+J">Johannes Raufeisen</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+K">Kunpeng Xie</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%B6rst%2C+F">Fabian H&#xf6;rst</a>, 
<a href="/search/cs?searchtype=author&query=Braunschweig%2C+T">Till Braunschweig</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianning Li</a>, 
<a href="/search/cs?searchtype=author&query=Kleesiek%2C+J">Jens Kleesiek</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%B6hrig%2C+R">Rainer R&#xf6;hrig</a>, 
<a href="/search/cs?searchtype=author&query=Egger%2C+J">Jan Egger</a>, 
<a href="/search/cs?searchtype=author&query=Leibe%2C+B">Bastian Leibe</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%B6lzle%2C+F">Frank H&#xf6;lzle</a>, 
<a href="/search/cs?searchtype=author&query=Hermans%2C+A">Alexander Hermans</a>, 
<a href="/search/cs?searchtype=author&query=Puladi%2C+B">Behrus Puladi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1425">[1425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15896" title="Abstract">arXiv:2401.15896</a> (replaced) [<a href="/pdf/2401.15896" title="Download PDF">pdf</a>, <a href="/format/2401.15896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> M2-Encoder: Advancing Bilingual Image-Text Understanding by Large-scale  Efficient Pretraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qingpei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+F">Furong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanxiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+W">Wang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Ziping Ma</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+L">Lin Ju</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jingdong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1426">[1426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15906" title="Abstract">arXiv:2401.15906</a> (replaced) [<a href="/pdf/2401.15906" title="Download PDF">pdf</a>, <a href="/format/2401.15906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mean Estimation with User-Level Privacy for Spatio-Temporal IoT Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rameshwar%2C+V+A">V. Arvind Rameshwar</a>, 
<a href="/search/cs?searchtype=author&query=Tandon%2C+A">Anshoo Tandon</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+P">Prajjwal Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+N">Novoneel Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Abhay Sharma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Information Theory (cs.IT); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item1427">[1427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15934" title="Abstract">arXiv:2401.15934</a> (replaced) [<a href="/pdf/2401.15934" title="Download PDF">pdf</a>, <a href="/format/2401.15934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HICH Image/Text (HICH-IT): Comprehensive Text and Image Datasets for  Hypertensive Intracerebral Hemorrhage Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jie Li</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yulong Xia</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tongxin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+F">Fenglin Cai</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+M">Miao Wei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Li Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1428">[1428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15947" title="Abstract">arXiv:2401.15947</a> (replaced) [<a href="/pdf/2401.15947" title="Download PDF">pdf</a>, <a href="/format/2401.15947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MoE-LLaVA: Mixture of Experts for Large Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Bin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zhenyu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yang Ye</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+J">Jiaxi Cui</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+P">Peng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jinfa Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junwu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+M">Munan Ning</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Li Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> update latest results and fix typo
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1429">[1429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16116" title="Abstract">arXiv:2401.16116</a> (replaced) [<a href="/pdf/2401.16116" title="Download PDF">pdf</a>, <a href="/ps/2401.16116" title="Download PostScript">ps</a>, <a href="/format/2401.16116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Cheques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Barhoush%2C+M">Mohammed Barhoush</a>, 
<a href="/search/quant-ph?searchtype=author&query=Salvail%2C+L">Louis Salvail</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item1430">[1430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16332" title="Abstract">arXiv:2401.16332</a> (replaced) [<a href="/pdf/2401.16332" title="Download PDF">pdf</a>, <a href="/format/2401.16332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tradeoffs Between Alignment and Helpfulness in Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wolf%2C+Y">Yotam Wolf</a>, 
<a href="/search/cs?searchtype=author&query=Wies%2C+N">Noam Wies</a>, 
<a href="/search/cs?searchtype=author&query=Shteyman%2C+D">Dorin Shteyman</a>, 
<a href="/search/cs?searchtype=author&query=Rothberg%2C+B">Binyamin Rothberg</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+Y">Yoav Levine</a>, 
<a href="/search/cs?searchtype=author&query=Shashua%2C+A">Amnon Shashua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1431">[1431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16452" title="Abstract">arXiv:2401.16452</a> (replaced) [<a href="/pdf/2401.16452" title="Download PDF">pdf</a>, <a href="/format/2401.16452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context-Former: Stitching via Latent Conditioned Sequence Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Ziqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jingzehua Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Z">Zifeng Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Donglin Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1432">[1432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16653" title="Abstract">arXiv:2401.16653</a> (replaced) [<a href="/pdf/2401.16653" title="Download PDF">pdf</a>, <a href="/format/2401.16653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ILBiT: Imitation Learning for Robot Using Position and Torque  Information based on Bilateral Control with Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kobayashi%2C+M">Masato Kobayashi</a>, 
<a href="/search/cs?searchtype=author&query=Buamanee%2C+T">Thanpimon Buamanee</a>, 
<a href="/search/cs?searchtype=author&query=Uranishi%2C+Y">Yuki Uranishi</a>, 
<a href="/search/cs?searchtype=author&query=Takemura%2C+H">Haruo Takemura</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1433">[1433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16736" title="Abstract">arXiv:2401.16736</a> (replaced) [<a href="/pdf/2401.16736" title="Download PDF">pdf</a>, <a href="/format/2401.16736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Engineering A Large Language Model From Scratch
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oketunji%2C+A+F">Abiodun Finbarrs Oketunji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item1434">[1434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16808" title="Abstract">arXiv:2401.16808</a> (replaced) [<a href="/pdf/2401.16808" title="Download PDF">pdf</a>, <a href="/format/2401.16808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Encoding Temporal Statistical-space Priors via Augmented Representation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+I">Insu Choi</a>, 
<a href="/search/cs?searchtype=author&query=Koh%2C+W">Woosung Koh</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+G">Gimin Kang</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+Y">Yuntae Jang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+W+C">Woo Chang Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> pre-print
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1435">[1435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16822" title="Abstract">arXiv:2401.16822</a> (replaced) [<a href="/pdf/2401.16822" title="Download PDF">pdf</a>, <a href="/format/2401.16822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor  Image Comprehension in Remote Sensing Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+M">Miaoxin Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Y">Yin Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+X">Xuerui Mao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1436">[1436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16866" title="Abstract">arXiv:2401.16866</a> (replaced) [<a href="/pdf/2401.16866" title="Download PDF">pdf</a>, <a href="/ps/2401.16866" title="Download PostScript">ps</a>, <a href="/format/2401.16866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New Centralized MSR Codes With Small Sub-packetization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yaqian Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item1437">[1437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16895" title="Abstract">arXiv:2401.16895</a> (replaced) [<a href="/pdf/2401.16895" title="Download PDF">pdf</a>, <a href="/format/2401.16895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Lingual Transfer from Related Languages: Treating Low-Resource  Maltese as Multilingual Code-Switching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Micallef%2C+K">Kurt Micallef</a>, 
<a href="/search/cs?searchtype=author&query=Habash%2C+N">Nizar Habash</a>, 
<a href="/search/cs?searchtype=author&query=Borg%2C+C">Claudia Borg</a>, 
<a href="/search/cs?searchtype=author&query=Eryani%2C+F">Fadhl Eryani</a>, 
<a href="/search/cs?searchtype=author&query=Bouamor%2C+H">Houda Bouamor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024 camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1438">[1438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17072" title="Abstract">arXiv:2401.17072</a> (replaced) [<a href="/pdf/2401.17072" title="Download PDF">pdf</a>, <a href="/format/2401.17072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SemScore: Automated Evaluation of Instruction-Tuned LLMs based on  Semantic Textual Similarity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aynetdinov%2C+A">Ansar Aynetdinov</a>, 
<a href="/search/cs?searchtype=author&query=Akbik%2C+A">Alan Akbik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1439">[1439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17212" title="Abstract">arXiv:2401.17212</a> (replaced) [<a href="/pdf/2401.17212" title="Download PDF">pdf</a>, <a href="/format/2401.17212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ContactGen: Contact-Guided Interactive 3D Human Generation for Partners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+D">Dongjun Gu</a>, 
<a href="/search/cs?searchtype=author&query=Shim%2C+J">Jaehyeok Shim</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+J">Jaehoon Jang</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+C">Changwoo Kang</a>, 
<a href="/search/cs?searchtype=author&query=Joo%2C+K">Kyungdon Joo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1440">[1440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17219" title="Abstract">arXiv:2401.17219</a> (replaced) [<a href="/pdf/2401.17219" title="Download PDF">pdf</a>, <a href="/ps/2401.17219" title="Download PostScript">ps</a>, <a href="/format/2401.17219" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faster coloring and embedding in dense hypergraphs via stability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hou%2C+J">Jianfeng Hou</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+X">Xizhi Liu</a>, 
<a href="/search/math?searchtype=author&query=Zhao%2C+H">Hongbin Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> added a new statement in THM 1.2, fixed some typo, added a new reference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item1441">[1441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17256" title="Abstract">arXiv:2401.17256</a> (replaced) [<a href="/pdf/2401.17256" title="Download PDF">pdf</a>, <a href="/format/2401.17256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weak-to-Strong Jailbreaking on Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xuandong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xianjun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+T">Tianyu Pang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+C">Chao Du</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu-Xiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1442">[1442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17263" title="Abstract">arXiv:2401.17263</a> (replaced) [<a href="/pdf/2401.17263" title="Download PDF">pdf</a>, <a href="/format/2401.17263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Prompt Optimization for Defending Language Models Against  Jailbreaking Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Andy Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haohan Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> website and code available at <a href="https://andyz245.github.io/rpo/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1443">[1443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17314" title="Abstract">arXiv:2401.17314</a> (replaced) [<a href="/pdf/2401.17314" title="Download PDF">pdf</a>, <a href="/ps/2401.17314" title="Download PostScript">ps</a>, <a href="/format/2401.17314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Labeled random finite sets vs. trajectory random finite sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mahler%2C+R">Ronald Mahler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 1 figur4e
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item1444">[1444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17555" title="Abstract">arXiv:2401.17555</a> (replaced) [<a href="/pdf/2401.17555" title="Download PDF">pdf</a>, <a href="/format/2401.17555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> opML: Optimistic Machine Learning on Blockchain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Conway%2C+K">KD Conway</a>, 
<a href="/search/cs?searchtype=author&query=So%2C+C">Cathie So</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xiaohang Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kartin Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item1445">[1445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17612" title="Abstract">arXiv:2401.17612</a> (replaced) [<a href="/pdf/2401.17612" title="Download PDF">pdf</a>, <a href="/format/2401.17612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IGCN: Integrative Graph Convolutional Networks for Multi-modal Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ozdemir%2C+C">Cagri Ozdemir</a>, 
<a href="/search/cs?searchtype=author&query=Olaimat%2C+M+A">Mohammad Al Olaimat</a>, 
<a href="/search/cs?searchtype=author&query=Vashishath%2C+Y">Yashu Vashishath</a>, 
<a href="/search/cs?searchtype=author&query=Bozdag%2C+S">Serdar Bozdag</a>, 
<a href="/search/cs?searchtype=author&query=Initiative%2C+A+D+N">Alzheimer&#x27;s Disease Neuroimaging Initiative</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1446">[1446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17686" title="Abstract">arXiv:2401.17686</a> (replaced) [<a href="/pdf/2401.17686" title="Download PDF">pdf</a>, <a href="/format/2401.17686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deductive Beam Search: Decoding Deducible Rationale for Chain-of-Thought  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Tinghui Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yu Su</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1447">[1447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17766" title="Abstract">arXiv:2401.17766</a> (replaced) [<a href="/pdf/2401.17766" title="Download PDF">pdf</a>, <a href="/format/2401.17766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-Grained Zero-Shot Learning: Advances, Challenges, and Prospects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jingcai Guo</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+Z">Zhijie Rao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jingren Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 1 figure, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1448">[1448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17786" title="Abstract">arXiv:2401.17786</a> (replaced) [<a href="/pdf/2401.17786" title="Download PDF">pdf</a>, <a href="/format/2401.17786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Graph-Native Query Optimization Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+B">Bingqing Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiaoli Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+L">Longbin Lai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yufan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+Y">Yunkai Lou</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenyuan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jingren Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item1449">[1449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17799" title="Abstract">arXiv:2401.17799</a> (replaced) [<a href="/pdf/2401.17799" title="Download PDF">pdf</a>, <a href="/format/2401.17799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI-enabled Cyber-Physical In-Orbit Factory -- AI approaches based on  digital twin technology for robotic small satellite production
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leutert%2C+F">Florian Leutert</a>, 
<a href="/search/cs?searchtype=author&query=Bohlig%2C+D">David Bohlig</a>, 
<a href="/search/cs?searchtype=author&query=Kempf%2C+F">Florian Kempf</a>, 
<a href="/search/cs?searchtype=author&query=Schilling%2C+K">Klaus Schilling</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BChlbauer%2C+M">Maximilian M&#xfc;hlbauer</a>, 
<a href="/search/cs?searchtype=author&query=Ayan%2C+B">Bengisu Ayan</a>, 
<a href="/search/cs?searchtype=author&query=Hulin%2C+T">Thomas Hulin</a>, 
<a href="/search/cs?searchtype=author&query=Stulp%2C+F">Freek Stulp</a>, 
<a href="/search/cs?searchtype=author&query=Albu-Sch%C3%A4ffer%2C+A">Alin Albu-Sch&#xe4;ffer</a>, 
<a href="/search/cs?searchtype=author&query=Kutscher%2C+V">Vladimir Kutscher</a>, 
<a href="/search/cs?searchtype=author&query=Plesker%2C+C">Christian Plesker</a>, 
<a href="/search/cs?searchtype=author&query=Dasbach%2C+T">Thomas Dasbach</a>, 
<a href="/search/cs?searchtype=author&query=Damm%2C+S">Stephan Damm</a>, 
<a href="/search/cs?searchtype=author&query=Anderl%2C+R">Reiner Anderl</a>, 
<a href="/search/cs?searchtype=author&query=Schleich%2C+B">Benjamin Schleich</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Acta Astronautica (2024), vol. 217, page 1-17
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1450">[1450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17870" title="Abstract">arXiv:2401.17870</a> (replaced) [<a href="/pdf/2401.17870" title="Download PDF">pdf</a>, <a href="/format/2401.17870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Subseasonal Weather Forecast using Teleconnection-informed  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zhitong Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X+X">Xiao Xiang Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IGARSS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1451">[1451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17984" title="Abstract">arXiv:2401.17984</a> (replaced) [<a href="/pdf/2401.17984" title="Download PDF">pdf</a>, <a href="/format/2401.17984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Makinote: An FPGA-Based HW/SW Platform for Pre-Silicon Emulation of  RISC-V Designs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Perdomo%2C+E">Elias Perdomo</a>, 
<a href="/search/cs?searchtype=author&query=Kropotov%2C+A">Alexander Kropotov</a>, 
<a href="/search/cs?searchtype=author&query=Cano%2C+F">Francelly Cano</a>, 
<a href="/search/cs?searchtype=author&query=Zafar%2C+S">Syed Zafar</a>, 
<a href="/search/cs?searchtype=author&query=Cervero%2C+T">Teresa Cervero</a>, 
<a href="/search/cs?searchtype=author&query=Martorell%2C+X">Xavier Martorell</a>, 
<a href="/search/cs?searchtype=author&query=Salami%2C+B">Behzad Salami</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, presented in Rapid Simulation and Performance Evaluation for Design 2024 (RAPIDO24) and published in ACM Proceedings of Rapid Simulation and Performance Evaluation for Design
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item1452">[1452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.18006" title="Abstract">arXiv:2401.18006</a> (replaced) [<a href="/pdf/2401.18006" title="Download PDF">pdf</a>, <a href="/format/2401.18006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EEG-GPT: Exploring Capabilities of Large Language Models for EEG  Classification and Interpretation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Kim%2C+J+W">Jonathan W. Kim</a>, 
<a href="/search/q-bio?searchtype=author&query=Alaa%2C+A">Ahmed Alaa</a>, 
<a href="/search/q-bio?searchtype=author&query=Bernardo%2C+D">Danilo Bernardo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1453">[1453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00024" title="Abstract">arXiv:2402.00024</a> (replaced) [<a href="/pdf/2402.00024" title="Download PDF">pdf</a>, <a href="/format/2402.00024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparative Analysis of LLaMA and ChatGPT Embeddings for Molecule  Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Sadeghi%2C+S">Shaghayegh Sadeghi</a>, 
<a href="/search/q-bio?searchtype=author&query=Bui%2C+A">Alan Bui</a>, 
<a href="/search/q-bio?searchtype=author&query=Forooghi%2C+A">Ali Forooghi</a>, 
<a href="/search/q-bio?searchtype=author&query=Lu%2C+J">Jianguo Lu</a>, 
<a href="/search/q-bio?searchtype=author&query=Ngom%2C+A">Alioune Ngom</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1454">[1454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00042" title="Abstract">arXiv:2402.00042</a> (replaced) [<a href="/pdf/2402.00042" title="Download PDF">pdf</a>, <a href="/ps/2402.00042" title="Download PostScript">ps</a>, <a href="/format/2402.00042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimized Task Assignment and Predictive Maintenance for Industrial  Machines using Markov Decision Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nasir%2C+A">Ali Nasir</a>, 
<a href="/search/cs?searchtype=author&query=Mekid%2C+S">Samir Mekid</a>, 
<a href="/search/cs?searchtype=author&query=Sawlan%2C+Z">Zaid Sawlan</a>, 
<a href="/search/cs?searchtype=author&query=Alsawafy%2C+O">Omar Alsawafy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 11 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1455">[1455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00129" title="Abstract">arXiv:2402.00129</a> (replaced) [<a href="/pdf/2402.00129" title="Download PDF">pdf</a>, <a href="/format/2402.00129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CMRNext: Camera to LiDAR Matching in the Wild for Localization and  Extrinsic Calibration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cattaneo%2C+D">Daniele Cattaneo</a>, 
<a href="/search/cs?searchtype=author&query=Valada%2C+A">Abhinav Valada</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1456">[1456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00263" title="Abstract">arXiv:2402.00263</a> (replaced) [<a href="/pdf/2402.00263" title="Download PDF">pdf</a>, <a href="/format/2402.00263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does DetectGPT Fully Utilize Perturbation? Selective Perturbation on  Model-Based Contrastive Learning Detector would be Better
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shengchao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yichen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zehua Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengzhengxu Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaohan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Y">Yu Lan</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chao Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1457">[1457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00267" title="Abstract">arXiv:2402.00267</a> (replaced) [<a href="/pdf/2402.00267" title="Download PDF">pdf</a>, <a href="/ps/2402.00267" title="Download PostScript">ps</a>, <a href="/format/2402.00267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Not All Learnable Distribution Classes are Privately Learnable
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bun%2C+M">Mark Bun</a>, 
<a href="/search/cs?searchtype=author&query=Kamath%2C+G">Gautam Kamath</a>, 
<a href="/search/cs?searchtype=author&query=Mouzakis%2C+A">Argyris Mouzakis</a>, 
<a href="/search/cs?searchtype=author&query=Singhal%2C+V">Vikrant Singhal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in ALT 2024. Added a minor clarification to the construction and an acknowledgement of the Fields Institute
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Cryptography and Security (cs.CR); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1458">[1458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00515" title="Abstract">arXiv:2402.00515</a> (replaced) [<a href="/pdf/2402.00515" title="Download PDF">pdf</a>, <a href="/format/2402.00515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Developing A Multi-Agent and Self-Adaptive Framework with Deep  Reinforcement Learning for Dynamic Portfolio Risk Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Li%2C+Z">Zhenglong Li</a>, 
<a href="/search/q-fin?searchtype=author&query=Tam%2C+V">Vincent Tam</a>, 
<a href="/search/q-fin?searchtype=author&query=Yeung%2C+K+L">Kwan L. Yeung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by The 23rd International Conference on Autonomous Agents and Multi-Agent Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Portfolio Management (q-fin.PM)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1459">[1459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00670" title="Abstract">arXiv:2402.00670</a> (replaced) [<a href="/pdf/2402.00670" title="Download PDF">pdf</a>, <a href="/format/2402.00670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ECALL: Expectation-calibrated learning for unsupervised blind  deconvolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Haltmeier%2C+M">Markus Haltmeier</a>, 
<a href="/search/math?searchtype=author&query=Hwang%2C+G">Gyeongha Hwang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1460">[1460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00672" title="Abstract">arXiv:2402.00672</a> (replaced) [<a href="/pdf/2402.00672" title="Download PDF">pdf</a>, <a href="/format/2402.00672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Homogeneous and Heterogeneous Consistent Label Associations  for Unsupervised Visible-Infrared Person ReID
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+L">Lingfeng He</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+D">De Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Nannan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xinbo Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1461">[1461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00680" title="Abstract">arXiv:2402.00680</a> (replaced) [<a href="/pdf/2402.00680" title="Download PDF">pdf</a>, <a href="/format/2402.00680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LVC-LGMC: Joint Local and Global Motion Compensation for Learned Video  Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jiang%2C+W">Wei Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Junru Li</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+L">Li Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Fix typos and Fig.1 and Fig.2. Accepted at ICASSP 2024. The first attempt to use cross attention for bits-free motion estimation and motion compensation
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICASSP (International Conference on Acoustics, Speech, and Signal
  Processing) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1462">[1462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00715" title="Abstract">arXiv:2402.00715</a> (replaced) [<a href="/pdf/2402.00715" title="Download PDF">pdf</a>, <a href="/format/2402.00715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intent Assurance using LLMs guided by Intent Drift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dzeparoska%2C+K">Kristina Dzeparoska</a>, 
<a href="/search/cs?searchtype=author&query=Tizghadam%2C+A">Ali Tizghadam</a>, 
<a href="/search/cs?searchtype=author&query=Leon-Garcia%2C+A">Alberto Leon-Garcia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Networking and Internet Architecture (cs.NI); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item1463">[1463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00798" title="Abstract">arXiv:2402.00798</a> (replaced) [<a href="/pdf/2402.00798" title="Download PDF">pdf</a>, <a href="/format/2402.00798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Formal-LLM: Integrating Formal Language and Natural Language for  Controllable LLM-based Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zelong Li</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+W">Wenyue Hua</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">He Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongfeng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 6 figures; comments and suggestions are welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Formal Languages and Automata Theory (cs.FL)

</div>
</div>
</dd>
<dt><a name="item1464">[1464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00861" title="Abstract">arXiv:2402.00861</a> (replaced) [<a href="/pdf/2402.00861" title="Download PDF">pdf</a>, <a href="/format/2402.00861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Large Language Models for Generalization and Robustness via  Data Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yucheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yunhao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Guerin%2C+F">Frank Guerin</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenghua Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1465">[1465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00876" title="Abstract">arXiv:2402.00876</a> (replaced) [<a href="/pdf/2402.00876" title="Download PDF">pdf</a>, <a href="/format/2402.00876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Building Blocks to Empower Cognitive Internet with Hybrid Edge Cloud
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alamouti%2C+S">Siavash Alamouti</a>, 
<a href="/search/cs?searchtype=author&query=Arjomandi%2C+F">Fay Arjomandi</a>, 
<a href="/search/cs?searchtype=author&query=Burger%2C+M">Michel Burger</a>, 
<a href="/search/cs?searchtype=author&query=Altakrouri%2C+D+B">Dr. Bashar Altakrouri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1466">[1466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01007" title="Abstract">arXiv:2402.01007</a> (replaced) [<a href="/pdf/2402.01007" title="Download PDF">pdf</a>, <a href="/ps/2402.01007" title="Download PostScript">ps</a>, <a href="/format/2402.01007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Municipal cyber risk modeling using cryptographic computing to inform  cyber policymaking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baral%2C+A">Avital Baral</a>, 
<a href="/search/cs?searchtype=author&query=Reynolds%2C+T">Taylor Reynolds</a>, 
<a href="/search/cs?searchtype=author&query=Susskind%2C+L">Lawrence Susskind</a>, 
<a href="/search/cs?searchtype=author&query=Weitzner%2C+D+J">Daniel J. Weitzner</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+A">Angelina Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working Draft for Presentation at the Cybersecurity Law and Policy Scholars Conference - September 29, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; General Economics (econ.GN)

</div>
</div>
</dd>
<dt><a name="item1467">[1467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01152" title="Abstract">arXiv:2402.01152</a> (replaced) [<a href="/pdf/2402.01152" title="Download PDF">pdf</a>, <a href="/format/2402.01152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AccentFold: A Journey through African Accents for Zero-Shot ASR  Adaptation to Target Accents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Owodunni%2C+A+T">Abraham Toluwase Owodunni</a>, 
<a href="/search/cs?searchtype=author&query=Yadavalli%2C+A">Aditya Yadavalli</a>, 
<a href="/search/cs?searchtype=author&query=Emezue%2C+C+C">Chris Chinenye Emezue</a>, 
<a href="/search/cs?searchtype=author&query=Olatunji%2C+T">Tobi Olatunji</a>, 
<a href="/search/cs?searchtype=author&query=Mbataku%2C+C+C">Clinton C Mbataku</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL Findings 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1468">[1468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01155" title="Abstract">arXiv:2402.01155</a> (replaced) [<a href="/pdf/2402.01155" title="Download PDF">pdf</a>, <a href="/format/2402.01155" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CABINET: Content Relevance based Noise Reduction for Table Question  Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patnaik%2C+S">Sohan Patnaik</a>, 
<a href="/search/cs?searchtype=author&query=Changwal%2C+H">Heril Changwal</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+M">Milan Aggarwal</a>, 
<a href="/search/cs?searchtype=author&query=Bhatia%2C+S">Sumit Bhatia</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+Y">Yaman Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+B">Balaji Krishnamurthy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICLR 2024 (spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1469">[1469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01204" title="Abstract">arXiv:2402.01204</a> (replaced) [<a href="/pdf/2402.01204" title="Download PDF">pdf</a>, <a href="/format/2402.01204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Self-Supervised Learning for Non-Sequential Tabular Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei-Yao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+W">Wei-Wei Du</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Derek Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+W">Wen-Chih Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper list can be found at <a href="https://github.com/wwweiwei/awesome-self-supervised-learning-for-tabular-data">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1470">[1470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01207" title="Abstract">arXiv:2402.01207</a> (replaced) [<a href="/pdf/2402.01207" title="Download PDF">pdf</a>, <a href="/format/2402.01207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Causal Graph Discovery Using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiralerspong%2C+T">Thomas Jiralerspong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaoyin Chen</a>, 
<a href="/search/cs?searchtype=author&query=More%2C+Y">Yash More</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+V">Vedant Shah</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item1471">[1471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01253" title="Abstract">arXiv:2402.01253</a> (replaced) [<a href="/pdf/2402.01253" title="Download PDF">pdf</a>, <a href="/format/2402.01253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RimiRec: Modeling Refined Multi-interest in Hierarchical Structure for  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pei%2C+H">Haolei Pei</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuanyuan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yangping Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+Y">Yuan Nie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item1472">[1472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01262" title="Abstract">arXiv:2402.01262</a> (replaced) [<a href="/pdf/2402.01262" title="Download PDF">pdf</a>, <a href="/format/2402.01262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cascaded Scaling Classifier: class incremental learning with probability  scaling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pomponi%2C+J">Jary Pomponi</a>, 
<a href="/search/cs?searchtype=author&query=Devoto%2C+A">Alessio Devoto</a>, 
<a href="/search/cs?searchtype=author&query=Scardapane%2C+S">Simone Scardapane</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper under review. The official code is available <a href="https://github.com/jaryP/Cascaded-Scaling-Classifier">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1473">[1473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01276" title="Abstract">arXiv:2402.01276</a> (replaced) [<a href="/pdf/2402.01276" title="Download PDF">pdf</a>, <a href="/format/2402.01276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Unlearning: a Perspective of Stability and Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+J">Jiaqi Shao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xuanyu Cao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+B">Bing Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1474">[1474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01297" title="Abstract">arXiv:2402.01297</a> (replaced) [<a href="/pdf/2402.01297" title="Download PDF">pdf</a>, <a href="/format/2402.01297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterizing Overfitting in Kernel Ridgeless Regression Through the  Eigenspectrum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+T+S">Tin Sum Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Lucchi%2C+A">Aurelien Lucchi</a>, 
<a href="/search/cs?searchtype=author&query=Kratsios%2C+A">Anastasis Kratsios</a>, 
<a href="/search/cs?searchtype=author&query=Belius%2C+D">David Belius</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1475">[1475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01304" title="Abstract">arXiv:2402.01304</a> (replaced) [<a href="/pdf/2402.01304" title="Download PDF">pdf</a>, <a href="/format/2402.01304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Phrase Grounding-based Style Transfer for Single-Domain Generalized  Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zhigang Luo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinwang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kenli Li</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xiaochun Cao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1476">[1476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01313" title="Abstract">arXiv:2402.01313</a> (replaced) [<a href="/pdf/2402.01313" title="Download PDF">pdf</a>, <a href="/format/2402.01313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoGCN -- Towards Generic Human Activity Recognition with Neural  Architecture Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tempel%2C+F">Felix Tempel</a>, 
<a href="/search/cs?searchtype=author&query=Str%C3%BCmke%2C+I">Inga Str&#xfc;mke</a>, 
<a href="/search/cs?searchtype=author&query=Ihlen%2C+E+A+F">Espen Alexander F. Ihlen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1477">[1477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01327" title="Abstract">arXiv:2402.01327</a> (replaced) [<a href="/pdf/2402.01327" title="Download PDF">pdf</a>, <a href="/format/2402.01327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supervised Algorithmic Fairness in Distribution Shifts: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yujie Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dong Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xintao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Q">Qin Tian</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+M">Minglai Shao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1478">[1478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01376" title="Abstract">arXiv:2402.01376</a> (replaced) [<a href="/pdf/2402.01376" title="Download PDF">pdf</a>, <a href="/ps/2402.01376" title="Download PostScript">ps</a>, <a href="/format/2402.01376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoTR: Low Tensor Rank Weight Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bershatsky%2C+D">Daniel Bershatsky</a>, 
<a href="/search/cs?searchtype=author&query=Cherniuk%2C+D">Daria Cherniuk</a>, 
<a href="/search/cs?searchtype=author&query=Daulbaev%2C+T">Talgat Daulbaev</a>, 
<a href="/search/cs?searchtype=author&query=Mikhalev%2C+A">Aleksandr Mikhalev</a>, 
<a href="/search/cs?searchtype=author&query=Oseledets%2C+I">Ivan Oseledets</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted; missing author and sections were added;
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1479">[1479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01391" title="Abstract">arXiv:2402.01391</a> (replaced) [<a href="/pdf/2402.01391" title="Download PDF">pdf</a>, <a href="/format/2402.01391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StepCoder: Improve Code Generation with Reinforcement Learning from  Compiler Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dou%2C+S">Shihan Dou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+H">Haoxiang Jia</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+L">Limao Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+E">Enyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+W">Wei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+J">Junjie Shan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Caishuang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+X">Xiaoran Fan</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+Z">Zhiheng Xi</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuhao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+T">Tao Ji</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Rui Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1480">[1480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01401" title="Abstract">arXiv:2402.01401</a> (replaced) [<a href="/pdf/2402.01401" title="Download PDF">pdf</a>, <a href="/format/2402.01401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Foster%2C+J">Jack Foster</a>, 
<a href="/search/cs?searchtype=author&query=Fogarty%2C+K">Kyle Fogarty</a>, 
<a href="/search/cs?searchtype=author&query=Schoepf%2C+S">Stefan Schoepf</a>, 
<a href="/search/cs?searchtype=author&query=%C3%96ztireli%2C+C">Cengiz &#xd6;ztireli</a>, 
<a href="/search/cs?searchtype=author&query=Brintrup%2C+A">Alexandra Brintrup</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1481">[1481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01481" title="Abstract">arXiv:2402.01481</a> (replaced) [<a href="/pdf/2402.01481" title="Download PDF">pdf</a>, <a href="/format/2402.01481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-level protein pre-training with Vabs-Net
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jiale Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+W">Wanru Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jia Song</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shuqi Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Biomolecules (q-bio.BM)

</div>
</div>
</dd>
<dt><a name="item1482">[1482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01622" title="Abstract">arXiv:2402.01622</a> (replaced) [<a href="/pdf/2402.01622" title="Download PDF">pdf</a>, <a href="/format/2402.01622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TravelPlanner: A Benchmark for Real-World Planning with Language Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+J">Jian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiangjie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Tinghui Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+R">Renze Lou</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuandong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yanghua Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yu Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item914">Cross-lists</a></li>
<li><a href="#item998">Replacements</a></li>
</ul>
<small>[ total of 1482 entries:  <b>1-1482</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2402">2402</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
