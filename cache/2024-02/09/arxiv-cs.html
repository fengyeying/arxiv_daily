<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Wed  7 Feb 24  to  Thu  8 Feb 24, announced Fri,  9 Feb 24</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item370">Cross-lists</a></li>
<li><a href="#item425">Replacements</a></li>
</ul>
<small>[ total of 642 entries:  <b>1-642</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Fri,  9 Feb 24</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05114" title="Abstract">arXiv:2402.05114</a> [<a href="/pdf/2402.05114" title="Download PDF">pdf</a>, <a href="/ps/2402.05114" title="Download PostScript">ps</a>, <a href="/format/2402.05114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Light-weight and Unsupervised Method for Near Real-time Behavioral  Analysis using Operational Data Measurement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vargis%2C+T+R">Tom Richard Vargis</a>, 
<a href="/search/cs?searchtype=author&query=Ghiasvand%2C+S">Siavash Ghiasvand</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Monitoring the status of large computing systems is essential to identify
unexpected behavior and improve their performance and uptime. However, due to
the large-scale and distributed design of such computing systems as well as a
large number of monitoring parameters, automated monitoring methods should be
applied. Such automatic monitoring methods should also have the ability to
adapt themselves to the continuous changes in the computing system. In
addition, they should be able to identify behavioral anomalies in useful time,
to perform appropriate reactions. This work proposes a general lightweight and
unsupervised method for near real-time anomaly detection using operational data
measurement on large computing systems. The proposed model requires as little
as 4 hours of data and 50 epochs for each training process to accurately
resemble the behavioral pattern of computing systems.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05115" title="Abstract">arXiv:2402.05115</a> [<a href="/pdf/2402.05115" title="Download PDF">pdf</a>, <a href="/format/2402.05115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Motion Retargeting for Human-Robot Imitation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Annabi%2C+L">Louis Annabi</a> (Flowers, U2IS), 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Ziqi Ma</a> (U2IS), 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+S+M">Sao Mai Nguyen</a> (Lab-STICC_RAMBO, U2IS, Flowers, IMT Atlantique - INFO)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interactio, Mar 2024, Boulder (CO), United States
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This early-stage research work aims to improve online human-robot imitation
by translating sequences of joint positions from the domain of human motions to
a domain of motions achievable by a given robot, thus constrained by its
embodiment. Leveraging the generalization capabilities of deep learning
methods, we address this problem by proposing an encoder-decoder neural network
model performing domain-to-domain translation. In order to train such a model,
one could use pairs of associated robot and human motions. Though, such paired
data is extremely rare in practice, and tedious to collect. Therefore, we turn
towards deep learning methods for unpaired domain-to-domain translation, that
we adapt in order to perform human-robot imitation.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05116" title="Abstract">arXiv:2402.05116</a> [<a href="/pdf/2402.05116" title="Download PDF">pdf</a>, <a href="/format/2402.05116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying Similarity: Text-Mining Approaches to Evaluate ChatGPT and  Google Bard Content in Relation to BioMedical Literature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Klimczak%2C+J">Jakub Klimczak</a>, 
<a href="/search/cs?searchtype=author&query=Hamed%2C+A+A">Ahmed Abdeen Hamed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 10 figures, 4 tables; and 1 algorithm
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Digital Libraries (cs.DL); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Background: The emergence of generative AI tools, empowered by Large Language
Models (LLMs), has shown powerful capabilities in generating content. To date,
the assessment of the usefulness of such content, generated by what is known as
prompt engineering, has become an interesting research question. Objectives
Using the mean of prompt engineering, we assess the similarity and closeness of
such contents to real literature produced by scientists. Methods In this
exploratory analysis, (1) we prompt-engineer ChatGPT and Google Bard to
generate clinical content to be compared with literature counterparts, (2) we
assess the similarities of the contents generated by comparing them with
counterparts from biomedical literature. Our approach is to use text-mining
approaches to compare documents and associated bigrams and to use network
analysis to assess the terms' centrality. Results The experiments demonstrated
that ChatGPT outperformed Google Bard in cosine document similarity (38% to
34%), Jaccard document similarity (23% to 19%), TF-IDF bigram similarity (47%
to 41%), and term network centrality (degree and closeness). We also found new
links that emerged in ChatGPT bigram networks that did not exist in literature
bigram networks. Conclusions: The obtained similarity results show that ChatGPT
outperformed Google Bard in document similarity, bigrams, and degree and
closeness centrality. We also observed that ChatGPT offers linkage to terms
that are connected in the literature. Such connections could inspire asking
interesting questions and generate new hypotheses.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05118" title="Abstract">arXiv:2402.05118</a> [<a href="/pdf/2402.05118" title="Download PDF">pdf</a>, <a href="/format/2402.05118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Accessibility of Rural Populations through Vehicle-based  Services
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pizzinini%2C+C">Clemens Pizzinini</a>, 
<a href="/search/cs?searchtype=author&query=Justen%2C+N">Nils Justen</a>, 
<a href="/search/cs?searchtype=author&query=Ziegler%2C+D">David Ziegler</a>, 
<a href="/search/cs?searchtype=author&query=Lienkamp%2C+M">Markus Lienkamp</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Improving access to essential public services like healthcare and education
is crucial for human development, particularly in rural Sub-Saharan Africa.
However, limited reliable transportation and sparse public facilities present
significant challenges. Mobile facilities like mobile clinics offer a
cost-effective solution to enhance spatial accessibility for the rural
population.Public authorities require detailed demand distribution data to
allocate resources efficiently and maximize the impact of mobile facilities.
This includes determining optimal vehicle service stop locations and estimating
operational costs. Our integrated approach utilizes GIS data and an
accessibility scaling factor to assess spatial accessibility for rural
populations. We tailor demand structures to account for remote and underserved
populations. To reduce average travel distances to 5 km, we apply a clustering
algorithm and optimize vehicle service stop locations. In a case study in rural
Ethiopia, focusing on four key public services, our analysis demonstrates that
mobile facilities can address 39-62\% of unmet demand, even in areas with
widely dispersed populations. This approach aids decision-makers, including
fleet operators, policymakers, and public authorities in Sub-Saharan Africa,
during project evaluation and planning for mobile facilities. By enhancing
spatial accessibility and optimizing resource allocation, our methodology
contributes to the effective delivery of essential public services to
underserved populations.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05119" title="Abstract">arXiv:2402.05119</a> [<a href="/pdf/2402.05119" title="Download PDF">pdf</a>, <a href="/format/2402.05119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Closer Look at the Limitations of Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Sreyan Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Evuru%2C+C+K+R">Chandra Kiran Reddy Evuru</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sonal Kumar</a>, 
<a href="/search/cs?searchtype=author&query=S%2C+R">Ramaneswaran S</a>, 
<a href="/search/cs?searchtype=author&query=Aneja%2C+D">Deepali Aneja</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zeyu Jin</a>, 
<a href="/search/cs?searchtype=author&query=Duraiswami%2C+R">Ramani Duraiswami</a>, 
<a href="/search/cs?searchtype=author&query=Manocha%2C+D">Dinesh Manocha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Instruction Tuning (IT), the process of training large language models (LLMs)
using instruction-response pairs, has emerged as the predominant method for
transforming base pre-trained LLMs into open-domain conversational agents.
While IT has achieved notable success and widespread adoption, its limitations
and shortcomings remain underexplored. In this paper, through rigorous
experiments and an in-depth analysis of the changes LLMs undergo through IT, we
reveal various limitations of IT. In particular, we show that (1) IT fails to
enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning
response initiation and style tokens, and full-parameter fine-tuning leads to
knowledge degradation. (2) Copying response patterns from IT datasets derived
from knowledgeable sources leads to a decline in response quality. (3)
Full-parameter fine-tuning increases hallucination by inaccurately borrowing
tokens from conceptually similar instances in the IT dataset for generating
responses. (4) Popular methods to improve IT do not lead to performance
improvements over a simple LoRA fine-tuned model. Our findings reveal that
responses generated solely from pre-trained knowledge consistently outperform
responses by models that learn any form of new knowledge from IT on open-source
datasets. We hope the insights and challenges revealed inspire future work.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05120" title="Abstract">arXiv:2402.05120</a> [<a href="/pdf/2402.05120" title="Download PDF">pdf</a>, <a href="/format/2402.05120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> More Agents Is All You Need
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junyou Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yangbin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Q">Qiang Fu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+D">Deheng Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We find that, simply via a sampling-and-voting method, the performance of
large language models (LLMs) scales with the number of agents instantiated.
Also, this method is orthogonal to existing complicated methods to further
enhance LLMs, while the degree of enhancement is correlated to the task
difficulty. We conduct comprehensive experiments on a wide range of LLM
benchmarks to verify the presence of our finding, and to study the properties
that can facilitate its occurrence. Our code is publicly available at:
\url{https://anonymous.4open.science/r/more_agent_is_all_you_need}.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05121" title="Abstract">arXiv:2402.05121</a> [<a href="/pdf/2402.05121" title="Download PDF">pdf</a>, <a href="/format/2402.05121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model for Table Processing: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Weizheng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yueguo Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Tables, typically two-dimensional and structured to store large amounts of
data, are essential in daily activities like database queries, spreadsheet
calculations, and generating reports from web tables. Automating these
table-centric tasks with Large Language Models (LLMs) offers significant public
benefits, garnering interest from academia and industry. This survey provides
an extensive overview of table tasks, encompassing not only the traditional
areas like table question answering (Table QA) and fact verification, but also
newly emphasized aspects such as table manipulation and advanced table data
analysis. Additionally, it goes beyond the early strategies of pre-training and
fine-tuning small language models, to include recent paradigms in LLM usage.
The focus here is particularly on instruction-tuning, prompting, and
agent-based approaches within the realm of LLMs. Finally, we highlight several
challenges, ranging from private deployment and efficient inference to the
development of extensive benchmarks for table manipulation and advanced data
analysis.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05122" title="Abstract">arXiv:2402.05122</a> [<a href="/pdf/2402.05122" title="Download PDF">pdf</a>, <a href="/ps/2402.05122" title="Download PostScript">ps</a>, <a href="/format/2402.05122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> History of generative Artificial Intelligence (AI) chatbots: past,  present, and future development
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Al-Amin%2C+M">Md. Al-Amin</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+M+S">Mohammad Shazed Ali</a>, 
<a href="/search/cs?searchtype=author&query=Salam%2C+A">Abdus Salam</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+A">Arif Khan</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+A">Ashraf Ali</a>, 
<a href="/search/cs?searchtype=author&query=Ullah%2C+A">Ahsan Ullah</a>, 
<a href="/search/cs?searchtype=author&query=Alam%2C+M+N">Md Nur Alam</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+S+K">Shamsul Kabir Chowdhury</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Literature (cs.GL)</span>; Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">This research provides an in-depth comprehensive review of the progress of
chatbot technology over time, from the initial basic systems relying on rules
to today's advanced conversational bots powered by artificial intelligence.
Spanning many decades, the paper explores the major milestones, innovations,
and paradigm shifts that have driven the evolution of chatbots. Looking back at
the very basic statistical model in 1906 via the early chatbots, such as ELIZA
and ALICE in the 1960s and 1970s, the study traces key innovations leading to
today's advanced conversational agents, such as ChatGPT and Google Bard. The
study synthesizes insights from academic literature and industry sources to
highlight crucial milestones, including the introduction of Turing tests,
influential projects such as CALO, and recent transformer-based models. Tracing
the path forward, the paper highlights how natural language processing and
machine learning have been integrated into modern chatbots for more
sophisticated capabilities. This chronological survey of the chatbot landscape
provides a holistic reference to understand the technological and historical
factors propelling conversational AI. By synthesizing learnings from this
historical analysis, the research offers important context about the
developmental trajectory of chatbots and their immense future potential across
various field of application which could be the potential take ways for the
respective research community and stakeholders.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05123" title="Abstract">arXiv:2402.05123</a> [<a href="/pdf/2402.05123" title="Download PDF">pdf</a>, <a href="/ps/2402.05123" title="Download PostScript">ps</a>, <a href="/format/2402.05123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Data Selection for LLM Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiahao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bolin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Q">Qianlong Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiajun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+D">Dianhui Chu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Instruction tuning is a vital step of training large language models (LLM),
so how to enhance the effect of instruction tuning has received increased
attention. Existing works indicate that the quality of the dataset is more
crucial than the quantity during instruction tuning of LLM. Therefore, recently
a lot of studies focus on exploring the methods of selecting high-quality
subset from instruction datasets, aiming to reduce training costs and enhance
the instruction-following capabilities of LLMs. This paper presents a
comprehensive survey on data selection for LLM instruction tuning. Firstly, we
introduce the wildly used instruction datasets. Then, we propose a new taxonomy
of the data selection methods and provide a detailed introduction of recent
advances,and the evaluation strategies and results of data selection methods
are also elaborated in detail. Finally, we emphasize the open challenges and
present new frontiers of this task.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05125" title="Abstract">arXiv:2402.05125</a> [<a href="/pdf/2402.05125" title="Download PDF">pdf</a>, <a href="/format/2402.05125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot Clinical Trial Patient Matching with LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wornow%2C+M">Michael Wornow</a>, 
<a href="/search/cs?searchtype=author&query=Lozano%2C+A">Alejandro Lozano</a>, 
<a href="/search/cs?searchtype=author&query=Dash%2C+D">Dev Dash</a>, 
<a href="/search/cs?searchtype=author&query=Jindal%2C+J">Jenelle Jindal</a>, 
<a href="/search/cs?searchtype=author&query=Mahaffey%2C+K+W">Kenneth W. Mahaffey</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+N+H">Nigam H. Shah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Matching patients to clinical trials is a key unsolved challenge in bringing
new drugs to market. Today, identifying patients who meet a trial's eligibility
criteria is highly manual, taking up to 1 hour per patient. Automated screening
is challenging, however, as it requires understanding unstructured clinical
text. Large language models (LLMs) offer a promising solution. In this work, we
explore their application to trial matching. First, we design an LLM-based
system which, given a patient's medical history as unstructured clinical text,
evaluates whether that patient meets a set of inclusion criteria (also
specified as free text). Our zero-shot system achieves state-of-the-art scores
on the n2c2 2018 cohort selection benchmark. Second, we improve the data and
cost efficiency of our method by identifying a prompting strategy which matches
patients an order of magnitude faster and more cheaply than the status quo, and
develop a two-stage retrieval pipeline that reduces the number of tokens
processed by up to a third while retaining high performance. Third, we evaluate
the interpretability of our system by having clinicians evaluate the natural
language justifications generated by the LLM for each eligibility decision, and
show that it can output coherent explanations for 97% of its correct decisions
and 75% of its incorrect ones. Our results establish the feasibility of using
LLMs to accelerate clinical trial operations.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05126" title="Abstract">arXiv:2402.05126</a> [<a href="/pdf/2402.05126" title="Download PDF">pdf</a>, <a href="/format/2402.05126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Network and NER-Based Text Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+I+Z">Imaad Zaffar Khan</a>, 
<a href="/search/cs?searchtype=author&query=Sheikh%2C+A+A">Amaan Aijaz Sheikh</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+U">Utkarsh Sinha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">With the abundance of data and information in todays time, it is nearly
impossible for man, or, even machine, to go through all of the data line by
line. What one usually does is to try to skim through the lines and retain the
absolutely important information, that in a more formal term is called
summarization. Text summarization is an important task that aims to compress
lengthy documents or articles into shorter, coherent representations while
preserving the core information and meaning. This project introduces an
innovative approach to text summarization, leveraging the capabilities of Graph
Neural Networks (GNNs) and Named Entity Recognition (NER) systems. GNNs, with
their exceptional ability to capture and process the relational data inherent
in textual information, are adept at understanding the complex structures
within large documents. Meanwhile, NER systems contribute by identifying and
emphasizing key entities, ensuring that the summarization process maintains a
focus on the most critical aspects of the text. By integrating these two
technologies, our method aims to enhances the efficiency of summarization and
also tries to ensures a high degree relevance in the condensed content. This
project, therefore, offers a promising direction for handling the ever
increasing volume of textual data in an information-saturated world.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05127" title="Abstract">arXiv:2402.05127</a> [<a href="/pdf/2402.05127" title="Download PDF">pdf</a>, <a href="/ps/2402.05127" title="Download PostScript">ps</a>, <a href="/format/2402.05127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Illuminate: A novel approach for depression detection with explainable  analysis and proactive therapy using prompt engineering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+A">Aryan Agrawal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 9 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper introduces a novel paradigm for depression detection and treatment
using advanced Large Language Models (LLMs): Generative Pre-trained Transformer
4 (GPT-4), Llama 2 chat, and Gemini. These LLMs are fine-tuned with specialized
prompts to diagnose, explain, and suggest therapeutic interventions for
depression. A unique few-shot prompting method enhances the models' ability to
analyze and explain depressive symptoms based on the DSM-5 criteria. In the
interaction phase, the models engage in empathetic dialogue management, drawing
from resources like PsychDB and a Cognitive Behavioral Therapy (CBT) Guide,
fostering supportive interactions with individuals experiencing major
depressive disorders. Additionally, the research introduces the Illuminate
Database, enriched with various CBT modules, aiding in personalized therapy
recommendations. The study evaluates LLM performance using metrics such as F1
scores, Precision, Recall, Cosine similarity, and Recall-Oriented Understudy
for Gisting Evaluation (ROUGE) across different test sets, demonstrating their
effectiveness. This comprehensive approach blends cutting-edge AI with
established psychological methods, offering new possibilities in mental health
care and showcasing the potential of LLMs in revolutionizing depression
diagnosis and treatment strategies.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05128" title="Abstract">arXiv:2402.05128</a> [<a href="/pdf/2402.05128" title="Download PDF">pdf</a>, <a href="/format/2402.05128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Textbook Question Answering Task with Large Language Models  and Retrieval Augmented Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alawwad%2C+H+A">Hessa Abdulrahman Alawwad</a>, 
<a href="/search/cs?searchtype=author&query=Alhothali%2C+A">Areej Alhothali</a>, 
<a href="/search/cs?searchtype=author&query=Naseem%2C+U">Usman Naseem</a>, 
<a href="/search/cs?searchtype=author&query=Alkhathlan%2C+A">Ali Alkhathlan</a>, 
<a href="/search/cs?searchtype=author&query=Jamal%2C+A">Amani Jamal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Textbook question answering (TQA) is a challenging task in artificial
intelligence due to the complex nature of context and multimodal data. Although
previous research has significantly improved the task, there are still some
limitations including the models' weak reasoning and inability to capture
contextual information in the lengthy context. The introduction of large
language models (LLMs) has revolutionized the field of AI, however, directly
applying LLMs often leads to inaccurate answers. This paper proposes a
methodology that handle the out-of-domain scenario in TQA where concepts are
spread across different lessons by incorporating the retrieval augmented
generation (RAG) technique and utilize transfer learning to handle the long
context and enhance reasoning abilities. Through supervised fine-tuning of the
LLM model Llama-2 and the incorporation of RAG, our architecture outperforms
the baseline, achieving a 4.12% accuracy improvement on validation set and
9.84% on test set for non-diagram multiple-choice questions.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05129" title="Abstract">arXiv:2402.05129</a> [<a href="/pdf/2402.05129" title="Download PDF">pdf</a>, <a href="/ps/2402.05129" title="Download PostScript">ps</a>, <a href="/format/2402.05129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Best Practices for Text Annotation with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=T%C3%B6rnberg%2C+P">Petter T&#xf6;rnberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have ushered in a new era of text annotation, as
their ease-of-use, high accuracy, and relatively low costs have meant that
their use has exploded in recent months. However, the rapid growth of the field
has meant that LLM-based annotation has become something of an academic Wild
West: the lack of established practices and standards has led to concerns about
the quality and validity of research. Researchers have warned that the
ostensible simplicity of LLMs can be misleading, as they are prone to bias,
misunderstandings, and unreliable results. Recognizing the transformative
potential of LLMs, this paper proposes a comprehensive set of standards and
best practices for their reliable, reproducible, and ethical use. These
guidelines span critical areas such as model selection, prompt engineering,
structured prompting, prompt stability analysis, rigorous model validation, and
the consideration of ethical and legal implications. The paper emphasizes the
need for a structured, directed, and formalized approach to using LLMs, aiming
to ensure the integrity and robustness of text annotation practices, and
advocates for a nuanced and critical engagement with LLMs in social scientific
research.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05130" title="Abstract">arXiv:2402.05130</a> [<a href="/pdf/2402.05130" title="Download PDF">pdf</a>, <a href="/format/2402.05130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LB-KBQA: Large-language-model and BERT based Knowledge-Based Question  and Answering System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhongyun Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaxing Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Generative Artificial Intelligence (AI), because of its emergent abilities,
has empowered various fields, one typical of which is large language models
(LLMs). One of the typical application fields of Generative AI is large
language models (LLMs), and the natural language understanding capability of
LLM is dramatically improved when compared with conventional AI-based methods.
The natural language understanding capability has always been a barrier to the
intent recognition performance of the Knowledge-Based-Question-and-Answer
(KBQA) system, which arises from linguistic diversity and the newly appeared
intent. Conventional AI-based methods for intent recognition can be divided
into semantic parsing-based and model-based approaches. However, both of the
methods suffer from limited resources in intent recognition. To address this
issue, we propose a novel KBQA system based on a Large Language Model(LLM) and
BERT (LB-KBQA). With the help of generative AI, our proposed method could
detect newly appeared intent and acquire new knowledge. In experiments on
financial domain question answering, our model has demonstrated superior
effectiveness.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05131" title="Abstract">arXiv:2402.05131</a> [<a href="/pdf/2402.05131" title="Download PDF">pdf</a>, <a href="/ps/2402.05131" title="Download PostScript">ps</a>, <a href="/format/2402.05131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Financial Report Chunking for Effective Retrieval Augmented Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yepes%2C+A+J">Antonio Jimeno Yepes</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yao You</a>, 
<a href="/search/cs?searchtype=author&query=Milczek%2C+J">Jan Milczek</a>, 
<a href="/search/cs?searchtype=author&query=Laverde%2C+S">Sebastian Laverde</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Leah Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Chunking information is a key step in Retrieval Augmented Generation (RAG).
Current research primarily centers on paragraph-level chunking. This approach
treats all texts as equal and neglects the information contained in the
structure of documents. We propose an expanded approach to chunk documents by
moving beyond mere paragraph-level chunking to chunk primary by structural
element components of documents. Dissecting documents into these constituent
elements creates a new way to chunk documents that yields the best chunk size
without tuning. We introduce a novel framework that evaluates how chunking
based on element types annotated by document understanding models contributes
to the overall context and accuracy of the information retrieved. We also
demonstrate how this approach impacts RAG assisted Question &amp; Answer task
performance. Our research includes a comprehensive analysis of various element
types, their role in effective information retrieval, and the impact they have
on the quality of RAG outputs. Findings support that element type based
chunking largely improve RAG results on financial reporting. Through this
research, we are also able to answer how to uncover highly accurate RAG.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05132" title="Abstract">arXiv:2402.05132</a> [<a href="/pdf/2402.05132" title="Download PDF">pdf</a>, <a href="/format/2402.05132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TexShape: Information Theoretic Sentence Embedding for Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kale%2C+H+K">H. Kaan Kale</a>, 
<a href="/search/cs?searchtype=author&query=Esfahanizadeh%2C+H">Homa Esfahanizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Elias%2C+N">Noel Elias</a>, 
<a href="/search/cs?searchtype=author&query=Baser%2C+O">Oguzhan Baser</a>, 
<a href="/search/cs?searchtype=author&query=Medard%2C+M">Muriel Medard</a>, 
<a href="/search/cs?searchtype=author&query=Vishwanath%2C+S">Sriram Vishwanath</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the 2024 IEEE International Symposium on Information Theory
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">With the exponential growth in data volume and the emergence of
data-intensive applications, particularly in the field of machine learning,
concerns related to resource utilization, privacy, and fairness have become
paramount. This paper focuses on the textual domain of data and addresses
challenges regarding encoding sentences to their optimized representations
through the lens of information-theory. In particular, we use empirical
estimates of mutual information, using the Donsker-Varadhan definition of
Kullback-Leibler divergence. Our approach leverages this estimation to train an
information-theoretic sentence embedding, called TexShape, for (task-based)
data compression or for filtering out sensitive information, enhancing privacy
and fairness. In this study, we employ a benchmark language model for initial
text representation, complemented by neural networks for information-theoretic
compression and mutual information estimations. Our experiments demonstrate
significant advancements in preserving maximal targeted information and minimal
sensitive information over adverse compression ratios, in terms of predictive
accuracy of downstream models that are trained using the compressed data.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05133" title="Abstract">arXiv:2402.05133</a> [<a href="/pdf/2402.05133" title="Download PDF">pdf</a>, <a href="/format/2402.05133" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalized Language Modeling from Personalized Human Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Lipton%2C+Z+C">Zachary C. Lipton</a>, 
<a href="/search/cs?searchtype=author&query=Leqi%2C+L">Liu Leqi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Reinforcement Learning from Human Feedback (RLHF) is the current dominating
framework to fine-tune large language models to better align with human
preferences. However, the underlying premise of algorithms developed under this
framework can be problematic when user preferences encoded in human feedback
are diverse. In this work, we aim to address this problem by developing methods
for building personalized language models. We first formally introduce the task
of learning from personalized human feedback and explain why vanilla RLHF can
be problematic in this context. We then propose a general Personalized-RLHF
(P-RLHF) framework, which requires one to jointly learn a user model and a
language (or reward) model. The user model takes in user information and
outputs user representations. Its structure encodes our assumptions about user
preferences underlying the feedback data. We develop new learning objectives
for personalized reward modeling and personalized Direct Preference
Optimization. To demonstrate the efficacy of our method, we test it on
real-world text summarization data with annotated preferences and annotator
information. We fine-tune GPT-J 6B to obtain personalized language (and reward)
models, which outperform non-personalized models in terms of aligning with
individual preferences.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05135" title="Abstract">arXiv:2402.05135</a> [<a href="/pdf/2402.05135" title="Download PDF">pdf</a>, <a href="/format/2402.05135" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CADReN: Contextual Anchor-Driven Relational Network for Controllable  Cross-Graphs Node Importance Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Z">Zijie Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunhui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Z">Ziyi Chang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zengchang Qin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Node Importance Estimation (NIE) is crucial for integrating external
information into Large Language Models through Retriever-Augmented Generation.
Traditional methods, focusing on static, single-graph characteristics, lack
adaptability to new graphs and user-specific requirements. CADReN, our proposed
method, addresses these limitations by introducing a Contextual Anchor (CA)
mechanism. This approach enables the network to assess node importance relative
to the CA, considering both structural and semantic features within Knowledge
Graphs (KGs). Extensive experiments show that CADReN achieves better
performance in cross-graph NIE task, with zero-shot prediction ability. CADReN
is also proven to match the performance of previous models on single-graph NIE
task. Additionally, we introduce and opensource two new datasets, RIC200 and
WK1K, specifically designed for cross-graph NIE research, providing a valuable
resource for future developments in this domain.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05136" title="Abstract">arXiv:2402.05136</a> [<a href="/pdf/2402.05136" title="Download PDF">pdf</a>, <a href="/format/2402.05136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to  256K
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+T">Tao Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+X">Xuefei Ning</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Dong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhijie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shiyao Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+M">Minghui Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zheyue Tan</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Zhuyu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Boxun Li</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+G">Guohao Dai</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shengen Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">State-of-the-art large language models (LLMs) are now claiming remarkable
supported context lengths of 256k or even more. In contrast, the average
context lengths of mainstream benchmarks are insufficient (5k-21k), and they
suffer from potential knowledge leakage and inaccurate metrics, resulting in
biased evaluation. This paper introduces LV-Eval, a challenging long-context
benchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up
to 256k words. LV-Eval features two main tasks, single-hop QA and multi-hop QA,
comprising 11 bilingual datasets. The design of LV-Eval has incorporated three
key techniques, namely confusing facts insertion, keyword and phrase
replacement, and keyword-recall-based metric design. The advantages of LV-Eval
include controllable evaluation across different context lengths, challenging
test instances with confusing facts, mitigated knowledge leakage, and more
objective evaluations. We evaluate 10 LLMs on LV-Eval and conduct ablation
studies on the techniques used in LV-Eval construction. The results reveal
that: (i) Commercial LLMs generally outperform open-source LLMs when evaluated
within length levels shorter than their claimed context length. However, their
overall performance is surpassed by open-source LLMs with longer context
lengths. (ii) Extremely long-context LLMs, such as Yi-6B-200k, exhibit a
relatively gentle degradation of performance, but their absolute performances
may not necessarily be higher than those of LLMs with shorter context lengths.
(iii) LLMs' performances can significantly degrade in the presence of confusing
information, especially in the pressure test of "needle in a haystack". (iv)
Issues related to knowledge leakage and inaccurate metrics introduce bias in
evaluation, and these concerns are alleviated in LV-Eval. All datasets and
evaluation codes are released at: https://github.com/infinigence/LVEval.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05138" title="Abstract">arXiv:2402.05138</a> [<a href="/pdf/2402.05138" title="Download PDF">pdf</a>, <a href="/format/2402.05138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SceMQA: A Scientific College Entrance Level Multimodal Question  Answering Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+Z">Zhenwen Liang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+K">Kehan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Gang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+T">Taicheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yujun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tianyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+J">Jiajun Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Pi%2C+R">Renjie Pi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiangliang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">The paper introduces SceMQA, a novel benchmark for scientific multimodal
question answering at the college entrance level. It addresses a critical
educational phase often overlooked in existing benchmarks, spanning high school
to pre-college levels. SceMQA focuses on core science subjects including
Mathematics, Physics, Chemistry, and Biology. It features a blend of
multiple-choice and free-response formats, ensuring a comprehensive evaluation
of AI models' abilities. Additionally, our benchmark provides specific
knowledge points for each problem and detailed explanations for each answer.
SceMQA also uniquely presents problems with identical contexts but varied
questions to facilitate a more thorough and accurate assessment of reasoning
capabilities. In the experiment, we evaluate both open-source and close-source
state-of-the-art Multimodal Large Language Models (MLLMs), across various
experimental settings. The results show that further research and development
are needed in developing more capable MLLM, as highlighted by only 50% to 60%
accuracy achieved by the strongest models. Our benchmark and analysis will be
available at https://scemqa.github.io/
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05140" title="Abstract">arXiv:2402.05140</a> [<a href="/pdf/2402.05140" title="Download PDF">pdf</a>, <a href="/format/2402.05140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Junhong Shen</a>, 
<a href="/search/cs?searchtype=author&query=Tenenholtz%2C+N">Neil Tenenholtz</a>, 
<a href="/search/cs?searchtype=author&query=Hall%2C+J+B">James Brian Hall</a>, 
<a href="/search/cs?searchtype=author&query=Alvarez-Melis%2C+D">David Alvarez-Melis</a>, 
<a href="/search/cs?searchtype=author&query=Fusi%2C+N">Nicolo Fusi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated remarkable proficiency in
understanding and generating natural language. However, their capabilities wane
in highly specialized domains underrepresented in the pretraining corpus, such
as physical and biomedical sciences. This work explores how to repurpose
general LLMs into effective task solvers for specialized domains. We introduce
a novel, model-agnostic framework for learning custom input tags, which are
parameterized as continuous vectors appended to the LLM's embedding layer, to
condition the LLM. We design two types of input tags: domain tags are used to
delimit specialized representations (e.g., chemical formulas) and provide
domain-relevant context; function tags are used to represent specific functions
(e.g., predicting molecular properties) and compress function-solving
instructions. We develop a three-stage protocol to learn these tags using
auxiliary data and domain knowledge. By explicitly disentangling task domains
from task functions, our method enables zero-shot generalization to unseen
problems through diverse combinations of the input tags. It also boosts LLM's
performance in various specialized domains, such as predicting protein or
chemical properties and modeling drug-target interactions, outperforming expert
models tailored to these tasks.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05142" title="Abstract">arXiv:2402.05142</a> [<a href="/pdf/2402.05142" title="Download PDF">pdf</a>, <a href="/ps/2402.05142" title="Download PostScript">ps</a>, <a href="/format/2402.05142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Foundations of Computational Management: A Systematic Approach to  Task Automation for the Integration of Artificial Intelligence into Existing  Workflows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jadad-Garcia%2C+T">Tamen Jadad-Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Jadad%2C+A+R">Alejandro R. Jadad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 3 appendices
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Driven by the rapid ascent of artificial intelligence (AI), organizations are
at the epicenter of a seismic shift, facing a crucial question: How can AI be
successfully integrated into existing operations? To help answer it, manage
expectations and mitigate frustration, this article introduces Computational
Management, a systematic approach to task automation for enhancing the ability
of organizations to harness AI's potential within existing workflows.
Computational Management acts as a bridge between the strategic insights of
management science with the analytical rigor of computational thinking. The
article offers three easy step-by-step procedures to begin the process of
implementing AI within a workflow. Such procedures focus on task
(re)formulation, on the assessment of the automation potential of tasks, on the
completion of task specification templates for AI selection and adaptation.
Included in the article there are manual and automated methods, with prompt
suggestions for publicly available LLMs, to complete these three procedures.
The first procedure, task (re)formulation, focuses on breaking down work
activities into basic units, so they can be completed by one agent, involve a
single well-defined action, and produce a distinct outcome. The second, allows
the assessment of the granular task and its suitability for automation, using
the Task Automation Index to rank tasks based on whether they have standardized
input, well-defined rules, repetitiveness, data dependency, and objective
outputs. The third, focuses on a task specification template which details
information on 16 critical components of tasks, and can be used as a checklist
to select or adapt the most suitable AI solution for integration into existing
workflows. Computational Management provides a roadmap and a toolkit for humans
and AI to thrive together, while enhancing organizational efficiency and
innovation.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05144" title="Abstract">arXiv:2402.05144</a> [<a href="/pdf/2402.05144" title="Download PDF">pdf</a>, <a href="/format/2402.05144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Bandit Approach with Evolutionary Operators for Model Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Br%C3%A9g%C3%A8re%2C+M">Margaux Br&#xe9;g&#xe8;re</a> (LPSM (UMR_8001), EDF R&amp;D), 
<a href="/search/cs?searchtype=author&query=Keisler%2C+J">Julie Keisler</a> (CRIStAL, EDF R&amp;D)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper formulates model selection as an infinite-armed bandit problem.
The models are arms, and picking an arm corresponds to a partial training of
the model (resource allocation). The reward is the accuracy of the selected
model after its partial training. In this best arm identification problem,
regret is the gap between the expected accuracy of the optimal model and that
of the model finally chosen. We first consider a straightforward generalization
of UCB-E to the stochastic infinite-armed bandit problem and show that, under
basic assumptions, the expected regret order is $T^{-\alpha}$ for some $\alpha
\in (0,1/5)$ and $T$ the number of resources to allocate. From this vanilla
algorithm, we introduce the algorithm Mutant-UCB that incorporates operators
from evolutionary algorithms. Tests carried out on three open source image
classification data sets attest to the relevance of this novel combining
approach, which outperforms the state-of-the-art for a fixed budget.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05145" title="Abstract">arXiv:2402.05145</a> [<a href="/pdf/2402.05145" title="Download PDF">pdf</a>, <a href="/format/2402.05145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Learning Approach for Survival Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fernandez%2C+C">Camila Fernandez</a> (LPSM), 
<a href="/search/cs?searchtype=author&query=Gaillard%2C+P">Pierre Gaillard</a> (Thoth), 
<a href="/search/cs?searchtype=author&query=de+Vilmarest%2C+J">Joseph de Vilmarest</a>, 
<a href="/search/cs?searchtype=author&query=Wintenberger%2C+O">Olivier Wintenberger</a> (LPSM (UMR\_8001))
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Analysis, Statistics and Probability (physics.data-an); Machine Learning (stat.ML)

</div>
<p class="mathjax">We introduce an online mathematical framework for survival analysis, allowing
real time adaptation to dynamic environments and censored data. This framework
enables the estimation of event time distributions through an optimal second
order online convex optimization algorithm-Online Newton Step (ONS). This
approach, previously unexplored, presents substantial advantages, including
explicit algorithms with non-asymptotic convergence guarantees. Moreover, we
analyze the selection of ONS hyperparameters, which depends on the
exp-concavity property and has a significant influence on the regret bound. We
propose a stochastic approach that guarantees logarithmic stochastic regret for
ONS. Additionally, we introduce an adaptive aggregation method that ensures
robustness in hyperparameter selection while maintaining fast regret bounds.
The findings of this paper can extend beyond the survival analysis field, and
are relevant for any case characterized by poor exp-concavity and unstable ONS.
Finally, these assertions are illustrated by simulation experiments.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05146" title="Abstract">arXiv:2402.05146</a> [<a href="/pdf/2402.05146" title="Download PDF">pdf</a>, <a href="/format/2402.05146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compressing Deep Reinforcement Learning Networks with a Dynamic  Structured Pruning Method for Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+W">Wensheng Su</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenni Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Minrui Xu</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jiawen Kang</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Shengli Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">Deep reinforcement learning (DRL) has shown remarkable success in complex
autonomous driving scenarios. However, DRL models inevitably bring high memory
consumption and computation, which hinders their wide deployment in
resource-limited autonomous driving devices. Structured Pruning has been
recognized as a useful method to compress and accelerate DRL models, but it is
still challenging to estimate the contribution of a parameter (i.e., neuron) to
DRL models. In this paper, we introduce a novel dynamic structured pruning
approach that gradually removes a DRL model's unimportant neurons during the
training stage. Our method consists of two steps, i.e. training DRL models with
a group sparse regularizer and removing unimportant neurons with a dynamic
pruning threshold. To efficiently train the DRL model with a small number of
important neurons, we employ a neuron-importance group sparse regularizer. In
contrast to conventional regularizers, this regularizer imposes a penalty on
redundant groups of neurons that do not significantly influence the output of
the DRL model. Furthermore, we design a novel structured pruning strategy to
dynamically determine the pruning threshold and gradually remove unimportant
neurons with a binary mask. Therefore, our method can remove not only redundant
groups of neurons of the DRL model but also achieve high and robust
performance. Experimental results show that the proposed method is competitive
with existing DRL pruning methods on discrete control environments (i.e.,
CartPole-v1 and LunarLander-v2) and MuJoCo continuous environments (i.e.,
Hopper-v3 and Walker2D-v3). Specifically, our method effectively compresses
$93\%$ neurons and $96\%$ weights of the DRL model in four challenging DRL
environments with slight accuracy degradation.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05147" title="Abstract">arXiv:2402.05147</a> [<a href="/pdf/2402.05147" title="Download PDF">pdf</a>, <a href="/format/2402.05147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ApiQ: Finetuning of 2-Bit Quantized Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+B">Baohao Liao</a>, 
<a href="/search/cs?searchtype=author&query=Monz%2C+C">Christof Monz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Memory-efficient finetuning of large language models (LLMs) has recently
attracted huge attention with the increasing size of LLMs, primarily due to the
constraints posed by GPU memory limitations and the comparable results of these
methods with full finetuning. Despite the advancements, current strategies for
memory-efficient finetuning, such as QLoRA, exhibit inconsistent performance
across diverse bit-width quantizations and multifaceted tasks. This
inconsistency largely stems from the detrimental impact of the quantization
process on preserved knowledge, leading to catastrophic forgetting and
undermining the utilization of pretrained models for finetuning purposes. In
this work, we introduce a novel quantization framework named ApiQ, designed to
restore the lost information from quantization by concurrently initializing
LoRA components and quantizing the weights of LLMs. This approach ensures the
maintenance of the original LLM's activation precision while mitigating the
error propagation from shallower into deeper layers. Through comprehensive
evaluations conducted on a spectrum of language tasks with various models, ApiQ
demonstrably minimizes activation error during quantization. Consequently, it
consistently achieves superior finetuning outcomes across various bit-widths of
quantization.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05148" title="Abstract">arXiv:2402.05148</a> [<a href="/pdf/2402.05148" title="Download PDF">pdf</a>, <a href="/format/2402.05148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cost Optimized Scheduling in Modular Electrolysis Plants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Henkel%2C+V">Vincent Henkel</a>, 
<a href="/search/cs?searchtype=author&query=Kilthau%2C+M">Maximilian Kilthau</a>, 
<a href="/search/cs?searchtype=author&query=Gehlhoff%2C+F">Felix Gehlhoff</a>, 
<a href="/search/cs?searchtype=author&query=Wagner%2C+L">Lukas Wagner</a>, 
<a href="/search/cs?searchtype=author&query=Fay%2C+A">Alexander Fay</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In response to the global shift towards renewable energy resources, the
production of green hydrogen through electrolysis is emerging as a promising
solution. Modular electrolysis plants, designed for flexibility and
scalability, offer a dynamic response to the increasing demand for hydrogen
while accommodating the fluctuations inherent in renewable energy sources.
However, optimizing their operation is challenging, especially when a large
number of electrolysis modules needs to be coordinated, each with potentially
different characteristics.
<br />To address these challenges, this paper presents a decentralized scheduling
model to optimize the operation of modular electrolysis plants using the
Alternating Direction Method of Multipliers. The model aims to balance hydrogen
production with fluctuating demand, to minimize the marginal Levelized Cost of
Hydrogen (mLCOH), and to ensure adaptability to operational disturbances. A
case study validates the accuracy of the model in calculating mLCOH values
under nominal load conditions and demonstrates its responsiveness to dynamic
changes, such as electrolyzer module malfunctions and scale-up scenarios.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05149" title="Abstract">arXiv:2402.05149</a> [<a href="/pdf/2402.05149" title="Download PDF">pdf</a>, <a href="/format/2402.05149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FlowPG: Action-constrained Policy Gradient with Normalizing Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brahmanage%2C+J+C">Janaka Chathuranga Brahmanage</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+J">Jiajing Ling</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Akshat Kumar</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Thirty-seventh Conference on Neural Information Processing
  Systems. 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Action-constrained reinforcement learning (ACRL) is a popular approach for
solving safety-critical and resource-allocation related decision making
problems. A major challenge in ACRL is to ensure agent taking a valid action
satisfying constraints in each RL step. Commonly used approach of using a
projection layer on top of the policy network requires solving an optimization
program which can result in longer training time, slow convergence, and zero
gradient problem. To address this, first we use a normalizing flow model to
learn an invertible, differentiable mapping between the feasible action space
and the support of a simple distribution on a latent variable, such as
Gaussian. Second, learning the flow model requires sampling from the feasible
action space, which is also challenging. We develop multiple methods, based on
Hamiltonian Monte-Carlo and probabilistic sentential decision diagrams for such
action sampling for convex and non-convex constraints. Third, we integrate the
learned normalizing flow with the DDPG algorithm. By design, a well-trained
normalizing flow will transform policy output into a valid action without
requiring an optimization solver. Empirically, our approach results in
significantly fewer constraint violations (upto an order-of-magnitude for
several instances) and is multiple times faster on a variety of continuous
control tasks.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05150" title="Abstract">arXiv:2402.05150</a> [<a href="/pdf/2402.05150" title="Download PDF">pdf</a>, <a href="/format/2402.05150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Designing deep neural networks for driver intention recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vellenga%2C+K">Koen Vellenga</a>, 
<a href="/search/cs?searchtype=author&query=Steinhauer%2C+H+J">H. Joe Steinhauer</a>, 
<a href="/search/cs?searchtype=author&query=Karlsson%2C+A">Alexander Karlsson</a>, 
<a href="/search/cs?searchtype=author&query=Falkman%2C+G">G&#xf6;ran Falkman</a>, 
<a href="/search/cs?searchtype=author&query=Rhodin%2C+A">Asli Rhodin</a>, 
<a href="/search/cs?searchtype=author&query=Koppisetty%2C+A">Ashok Koppisetty</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Driver intention recognition studies increasingly rely on deep neural
networks. Deep neural networks have achieved top performance for many different
tasks, but it is not a common practice to explicitly analyse the complexity and
performance of the network's architecture. Therefore, this paper applies neural
architecture search to investigate the effects of the deep neural network
architecture on a real-world safety critical application with limited
computational capabilities. We explore a pre-defined search space for three
deep neural network layer types that are capable to handle sequential data (a
long-short term memory, temporal convolution, and a time-series transformer
layer), and the influence of different data fusion strategies on the driver
intention recognition performance. A set of eight search strategies are
evaluated for two driver intention recognition datasets. For the two datasets,
we observed that there is no search strategy clearly sampling better deep
neural network architectures. However, performing an architecture search does
improve the model performance compared to the original manually designed
networks. Furthermore, we observe no relation between increased model
complexity and higher driver intention recognition performance. The result
indicate that multiple architectures yield similar performance, regardless of
the deep neural network layer type or fusion strategy.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05151" title="Abstract">arXiv:2402.05151</a> [<a href="/pdf/2402.05151" title="Download PDF">pdf</a>, <a href="/format/2402.05151" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CrashFormer: A Multimodal Architecture to Predict the Risk of Crash
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Monsefi%2C+A+K">Amin Karimi Monsefi</a>, 
<a href="/search/cs?searchtype=author&query=Shiri%2C+P">Pouya Shiri</a>, 
<a href="/search/cs?searchtype=author&query=Mohammadshirazi%2C+A">Ahmad Mohammadshirazi</a>, 
<a href="/search/cs?searchtype=author&query=Monsefi%2C+N+K">Nastaran Karimi Monsefi</a>, 
<a href="/search/cs?searchtype=author&query=Davies%2C+R">Ron Davies</a>, 
<a href="/search/cs?searchtype=author&query=Moosavi%2C+S">Sobhan Moosavi</a>, 
<a href="/search/cs?searchtype=author&query=Ramnath%2C+R">Rajiv Ramnath</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper is accepted In 1st ACM SIGSPATIAL International Workshop on Advances in Urban-AI (UrbanAI 23), November 13, 2023, Hamburg, Germany
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Reducing traffic accidents is a crucial global public safety concern.
Accident prediction is key to improving traffic safety, enabling proactive
measures to be taken before a crash occurs, and informing safety policies,
regulations, and targeted interventions. Despite numerous studies on accident
prediction over the past decades, many have limitations in terms of
generalizability, reproducibility, or feasibility for practical use due to
input data or problem formulation. To address existing shortcomings, we propose
CrashFormer, a multi-modal architecture that utilizes comprehensive (but
relatively easy to obtain) inputs such as the history of accidents, weather
information, map images, and demographic information. The model predicts the
future risk of accidents on a reasonably acceptable cadence (i.e., every six
hours) for a geographical location of 5.161 square kilometers. CrashFormer is
composed of five components: a sequential encoder to utilize historical
accidents and weather data, an image encoder to use map imagery data, a raw
data encoder to utilize demographic information, a feature fusion module for
aggregating the encoded features, and a classifier that accepts the aggregated
data and makes predictions accordingly. Results from extensive real-world
experiments in 10 major US cities show that CrashFormer outperforms
state-of-the-art sequential and non-sequential models by 1.8% in F1-score on
average when using ``sparse'' input data.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05153" title="Abstract">arXiv:2402.05153</a> [<a href="/pdf/2402.05153" title="Download PDF">pdf</a>, <a href="/format/2402.05153" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating On-road Transportation Carbon Emissions from Open Data of  Road Network and Origin-destination Flow Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+J">Jinwei Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+J">Jingtao Ding</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jian Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Accounting for over 20% of the total carbon emissions, the precise estimation
of on-road transportation carbon emissions is crucial for carbon emission
monitoring and efficient mitigation policy formulation. However, existing
estimation methods typically depend on hard-to-collect individual statistics of
vehicle miles traveled to calculate emissions, thereby suffering from high data
collection difficulty. To relieve this issue by utilizing the strong pattern
recognition of artificial intelligence, we incorporate two sources of open data
representative of the transportation demand and capacity factors, the
origin-destination (OD) flow data and the road network data, to build a
hierarchical heterogeneous graph learning method for on-road carbon emission
estimation (HENCE). Specifically, a hierarchical graph consisting of the road
network level, community level, and region level is constructed to model the
multi-scale road network-based connectivity and travel connection between
spatial areas. Heterogeneous graphs consisting of OD links and spatial links
are further built at both the community level and region level to capture the
intrinsic interactions between travel demand and road network accessibility.
Extensive experiments on two large-scale real-world datasets demonstrate
HENCE's effectiveness and superiority with R-squared exceeding 0.75 and
outperforming baselines by 9.60% on average, validating its success in
pioneering the use of artificial intelligence to empower carbon emission
management and sustainability development. The implementation codes are
available at this link: https://github.com/tsinghua-fib-lab/HENCE.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05154" title="Abstract">arXiv:2402.05154</a> [<a href="/pdf/2402.05154" title="Download PDF">pdf</a>, <a href="/format/2402.05154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Hypergraph Network for Trust Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Rongwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Guanfeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuyun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+K">Kai Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiaofang Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Trust plays an essential role in an individual's decision-making. Traditional
trust prediction models rely on pairwise correlations to infer potential
relationships between users. However, in the real world, interactions between
users are usually complicated rather than pairwise only. Hypergraphs offer a
flexible approach to modeling these complex high-order correlations (not just
pairwise connections), since hypergraphs can leverage hyperedeges to link more
than two nodes. However, most hypergraph-based methods are generic and cannot
be well applied to the trust prediction task. In this paper, we propose an
Adaptive Hypergraph Network for Trust Prediction (AHNTP), a novel approach that
improves trust prediction accuracy by using higher-order correlations. AHNTP
utilizes Motif-based PageRank to capture high-order social influence
information. In addition, it constructs hypergroups from both node-level and
structure-level attributes to incorporate complex correlation information.
Furthermore, AHNTP leverages adaptive hypergraph Graph Convolutional Network
(GCN) layers and multilayer perceptrons (MLPs) to generate comprehensive user
embeddings, facilitating trust relationship prediction. To enhance model
generalization and robustness, we introduce a novel supervised contrastive
learning loss for optimization. Extensive experiments demonstrate the
superiority of our model over the state-of-the-art approaches in terms of trust
prediction accuracy. The source code of this work can be accessed via
https://github.com/Sherry-XU1995/AHNTP.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05156" title="Abstract">arXiv:2402.05156</a> [<a href="/pdf/2402.05156" title="Download PDF">pdf</a>, <a href="/format/2402.05156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What About the Data? A Mapping Study on Data Engineering for AI Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heck%2C+P">Petra Heck</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, accepted for CAIN24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB)

</div>
<p class="mathjax">AI systems cannot exist without data. Now that AI models (data science and
AI) have matured and are readily available to apply in practice, most
organizations struggle with the data infrastructure to do so. There is a
growing need for data engineers that know how to prepare data for AI systems or
that can setup enterprise-wide data architectures for analytical projects. But
until now, the data engineering part of AI engineering has not been getting
much attention, in favor of discussing the modeling part. In this paper we aim
to change this by perform a mapping study on data engineering for AI systems,
i.e., AI data engineering. We found 25 relevant papers between January 2019 and
June 2023, explaining AI data engineering activities. We identify which life
cycle phases are covered, which technical solutions or architectures are
proposed and which lessons learned are presented. We end by an overall
discussion of the papers with implications for practitioners and researchers.
This paper creates an overview of the body of knowledge on data engineering for
AI. This overview is useful for practitioners to identify solutions and best
practices as well as for researchers to identify gaps.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05158" title="Abstract">arXiv:2402.05158</a> [<a href="/pdf/2402.05158" title="Download PDF">pdf</a>, <a href="/format/2402.05158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancement of Bengali OCR by Specialized Models and Advanced Techniques  for Diverse Document Types
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rabby%2C+A+S+A">AKM Shahariar Azad Rabby</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+H">Hasmot Ali</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+M">Md. Majedul Islam</a>, 
<a href="/search/cs?searchtype=author&query=Abujar%2C+S">Sheikh Abujar</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+F">Fuad Rahman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures, 4 table Link of the paper <a href="https://openaccess.thecvf.com/content/WACV2024W/WVLL/html/Rabby_Enhancement_of_Bengali_OCR_by_Specialized_Models_and_Advanced_Techniques_WACVW_2024_paper.html">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV) Workshops, 2024, pp. 1102-1109
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This research paper presents a unique Bengali OCR system with some
capabilities. The system excels in reconstructing document layouts while
preserving structure, alignment, and images. It incorporates advanced image and
signature detection for accurate extraction. Specialized models for word
segmentation cater to diverse document types, including computer-composed,
letterpress, typewriter, and handwritten documents. The system handles static
and dynamic handwritten inputs, recognizing various writing styles.
Furthermore, it has the ability to recognize compound characters in Bengali.
Extensive data collection efforts provide a diverse corpus, while advanced
technical components optimize character and word recognition. Additional
contributions include image, logo, signature and table recognition, perspective
correction, layout reconstruction, and a queuing module for efficient and
scalable processing. The system demonstrates outstanding performance in
efficient and accurate text extraction and analysis.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05159" title="Abstract">arXiv:2402.05159</a> [<a href="/pdf/2402.05159" title="Download PDF">pdf</a>, <a href="/ps/2402.05159" title="Download PostScript">ps</a>, <a href="/format/2402.05159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Threats and Limitations of Terrestrial Broadcast Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Michele%2C+B">Benjamin Michele</a>, 
<a href="/search/cs?searchtype=author&query=Pena%2C+I">Ivan Pena</a>, 
<a href="/search/cs?searchtype=author&query=Angueira%2C+P">Pablo Angueira</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> B. Michele, I. Pena, P. Angueira, Threats and Limitations of
  Terrestrial Broadcast Attacks, IEEEE Transactions on Broadcasting, Vol. 64,
  No. 1, pp. 105-118, March 2018
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">The DVB standard does not mandate the use of authentication and integrity
protection for transport streams. This allows malicious third parties to
replace legitimate broadcasts by overpowering terrestrial transmissions. The
rogue signal can then deliver a malicious broadcast stream to exploit security
vulnerabilities on Smart TVs (STVs) in range. We implemented a proof-of-concept
attack based on a malicious Hybrid Broadcast Broadband TV app, able to acquire
permanent system-level access to an STV over the air, in less than 10 s. These
attacks, however, are severely limited in range due to required co-channel
protection ratios (CCPRs), which is in direct contradiction to previous
publications. We present evidence for these limitations in form of laboratory
experiments, extensive simulations, and field measurements. To this end, we
developed an automated, low-cost method for CCPR determination, as well as a
method for non-disruptive attack range measurements based on a gap filler and
the resulting channel impulse response.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05160" title="Abstract">arXiv:2402.05160</a> [<a href="/pdf/2402.05160" title="Download PDF">pdf</a>, <a href="/format/2402.05160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What&#x27;s documented in AI? Systematic Analysis of 32K AI Model Cards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+W">Weixin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Rajani%2C+N">Nazneen Rajani</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xinyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ozoani%2C+E">Ezinwanne Ozoani</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+E">Eric Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiqun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+D+S">Daniel Scott Smith</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">James Zou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The rapid proliferation of AI models has underscored the importance of
thorough documentation, as it enables users to understand, trust, and
effectively utilize these models in various applications. Although developers
are encouraged to produce model cards, it's not clear how much information or
what information these cards contain. In this study, we conduct a comprehensive
analysis of 32,111 AI model documentations on Hugging Face, a leading platform
for distributing and deploying AI models. Our investigation sheds light on the
prevailing model card documentation practices. Most of the AI models with
substantial downloads provide model cards, though the cards have uneven
informativeness. We find that sections addressing environmental impact,
limitations, and evaluation exhibit the lowest filled-out rates, while the
training section is the most consistently filled-out. We analyze the content of
each section to characterize practitioners' priorities. Interestingly, there
are substantial discussions of data, sometimes with equal or even greater
emphasis than the model itself. To evaluate the impact of model cards, we
conducted an intervention study by adding detailed model cards to 42 popular
models which had no or sparse model cards previously. We find that adding model
cards is moderately correlated with an increase weekly download rates. Our
study opens up a new perspective for analyzing community norms and practices
for model documentation through large-scale data science and linguistics
analysis.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05161" title="Abstract">arXiv:2402.05161</a> [<a href="/pdf/2402.05161" title="Download PDF">pdf</a>, <a href="/ps/2402.05161" title="Download PostScript">ps</a>, <a href="/format/2402.05161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximate Keys and Functional Dependencies in Incomplete Databases  With Limited Domains-Algorithmic Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Al-atar%2C+M">Munqath Al-atar</a>, 
<a href="/search/cs?searchtype=author&query=Sali%2C+A">Attila Sali</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2402.05057">arXiv:2402.05057</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">A possible world of an incomplete database table is obtained by imputing
values from the attributes (infinite) domain to the place of \texttt{NULL} s. A
table satisfies a possible key or possible functional dependency constraint if
there exists a possible world of the table that satisfies the given key or
functional dependency constraint. A certain key or functional dependency is
satisfied by a table if all of its possible worlds satisfy the constraint.
Recently, an intermediate concept was introduced. A strongly possible key or
functional dependency is satisfied by a table if there exists a strongly
possible world that satisfies the key or functional dependency. A strongly
possible world is obtained by imputing values from the active domain of the
attributes, that is from the values appearing in the table. In the present
paper, we study approximation measures of strongly possible keys and FDs.
Measure $g_3$ is the ratio of the minimum number of tuples to be removed in
order that the remaining table satisfies the constraint. We introduce a new
measure $g_5$, the ratio of the minimum number of tuples to be added to the
table so the result satisfies the constraint. $g_5$ is meaningful because the
addition of tuples may extend the active domains. We prove that if $g_5$ can be
defined for a table and a constraint, then the $g_3$ value is always an upper
bound of the $g_5$ value. However, the two measures are independent of each
other in the sense that for any rational number $0\le\frac{p}{q}&lt;1$ there are
tables of an arbitrarily large number of rows and a constant number of columns
that satisfy $g_3-g_5=\frac{p}{q}$. A possible world is obtained usually by
adding many new values not occurring in the table before. The measure $g_5$
measures the smallest possible distortion of the active domains. We study
complexity of determining these approximate measures.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05162" title="Abstract">arXiv:2402.05162</a> [<a href="/pdf/2402.05162" title="Download PDF">pdf</a>, <a href="/format/2402.05162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank  Modifications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+B">Boyi Wei</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaixuan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yangsibo Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+T">Tinghao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+X">Xiangyu Qi</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+M">Mengzhou Xia</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+P">Prateek Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengdi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Henderson%2C+P">Peter Henderson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 9 figures. Project page is available at <a href="https://boyiwei.com/alignment-attribution/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models (LLMs) show inherent brittleness in their safety
mechanisms, as evidenced by their susceptibility to jailbreaking and even
non-malicious fine-tuning. This study explores this brittleness of safety
alignment by leveraging pruning and low-rank modifications. We develop methods
to identify critical regions that are vital for safety guardrails, and that are
disentangled from utility-relevant regions at both the neuron and rank levels.
Surprisingly, the isolated regions we find are sparse, comprising about $3\%$
at the parameter level and $2.5\%$ at the rank level. Removing these regions
compromises safety without significantly impacting utility, corroborating the
inherent brittleness of the model's safety mechanisms. Moreover, we show that
LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications
to the safety-critical regions are restricted. These findings underscore the
urgent need for more robust safety strategies in LLMs.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05164" title="Abstract">arXiv:2402.05164</a> [<a href="/pdf/2402.05164" title="Download PDF">pdf</a>, <a href="/format/2402.05164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Resource Model For Neural Scaling Law
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jinyeop Song</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tegmark%2C+M">Max Tegmark</a>, 
<a href="/search/cs?searchtype=author&query=Gore%2C+J">Jeff Gore</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 8 figures, Under review as a workshop paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Neural scaling laws characterize how model performance improves as the model
size scales up. Inspired by empirical observations, we introduce a resource
model of neural scaling. A task is usually composite hence can be decomposed
into many subtasks, which compete for resources (measured by the number of
neurons allocated to subtasks). On toy problems, we empirically find that: (1)
The loss of a subtask is inversely proportional to its allocated neurons. (2)
When multiple subtasks are present in a composite task, the resources acquired
by each subtask uniformly grow as models get larger, keeping the ratios of
acquired resources constants. We hypothesize these findings to be generally
true and build a model to predict neural scaling laws for general composite
tasks, which successfully replicates the neural scaling law of Chinchilla
models reported in <a href="/abs/2203.15556">arXiv:2203.15556</a>. We believe that the notion of resource
used in this paper will be a useful tool for characterizing and diagnosing
neural networks.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05173" title="Abstract">arXiv:2402.05173</a> [<a href="/pdf/2402.05173" title="Download PDF">pdf</a>, <a href="/format/2402.05173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Understanding Inductive Bias in Transformers: A View From  Infinity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lavie%2C+I">Itay Lavie</a>, 
<a href="/search/cs?searchtype=author&query=Gur-Ari%2C+G">Guy Gur-Ari</a>, 
<a href="/search/cs?searchtype=author&query=Ringel%2C+Z">Zohar Ringel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study inductive bias in Transformers in the infinitely over-parameterized
Gaussian process limit and argue transformers tend to be biased towards more
permutation symmetric functions in sequence space. We show that the
representation theory of the symmetric group can be used to give quantitative
analytical predictions when the dataset is symmetric to permutations between
tokens. We present a simplified transformer block and solve the model at the
limit, including accurate predictions for the learning curves and network
outputs. We show that in common setups, one can derive tight bounds in the form
of a scaling law for the learnability as a function of the context length.
Finally, we argue WikiText dataset, does indeed possess a degree of permutation
symmetry.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05188" title="Abstract">arXiv:2402.05188</a> [<a href="/pdf/2402.05188" title="Download PDF">pdf</a>, <a href="/format/2402.05188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InCoRo: In-Context Learning for Robotics Control with Feedback Loops
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J+Y">Jiaqiang Ye Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Cano%2C+C+G">Carla Gomez Cano</a>, 
<a href="/search/cs?searchtype=author&query=Bermudez%2C+D+V">David Vazquez Bermudez</a>, 
<a href="/search/cs?searchtype=author&query=Drozdzal%2C+M">Michal Drozdzal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">One of the challenges in robotics is to enable robotic units with the
reasoning capability that would be robust enough to execute complex tasks in
dynamic environments. Recent advances in LLMs have positioned them as go-to
tools for simple reasoning tasks, motivating the pioneering work of Liang et
al. [35] that uses an LLM to translate natural language commands into low-level
static execution plans for robotic units. Using LLMs inside robotics systems
brings their generalization to a new level, enabling zero-shot generalization
to new tasks. This paper extends this prior work to dynamic environments. We
propose InCoRo, a system that uses a classical robotic feedback loop composed
of an LLM controller, a scene understanding unit, and a robot. Our system
continuously analyzes the state of the environment and provides adapted
execution commands, enabling the robot to adjust to changing environmental
conditions and correcting for controller errors. Our system does not require
any iterative optimization to learn to accomplish a task as it leverages
in-context learning with an off-the-shelf LLM model. Through an extensive
validation process involving two standardized industrial robotic units -- SCARA
and DELTA types -- we contribute knowledge about these robots, not popular in
the community, thereby enriching it. We highlight the generalization
capabilities of our system and show that (1) in-context learning in combination
with the current state-of-the-art LLMs is an effective way to implement a
robotic controller; (2) in static environments, InCoRo surpasses the prior art
in terms of the success rate; (3) in dynamic environments, we establish new
state-of-the-art for the SCARA and DELTA units, respectively. This research
paves the way towards building reliable, efficient, intelligent autonomous
systems that adapt to dynamic environments.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05195" title="Abstract">arXiv:2402.05195</a> [<a href="/pdf/2402.05195" title="Download PDF">pdf</a>, <a href="/format/2402.05195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $&#x3bb;$-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion  Models by Leveraging CLIP Latent Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patel%2C+M">Maitreya Patel</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+S">Sangmin Jung</a>, 
<a href="/search/cs?searchtype=author&query=Baral%2C+C">Chitta Baral</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yezhou Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://eclipse-t2i.github.io/Lambda-ECLIPSE/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Despite the recent advances in personalized text-to-image (P-T2I) generative
models, subject-driven T2I remains challenging. The primary bottlenecks include
1) Intensive training resource requirements, 2) Hyper-parameter sensitivity
leading to inconsistent outputs, and 3) Balancing the intricacies of novel
visual concept and composition alignment. We start by re-iterating the core
philosophy of T2I diffusion models to address the above limitations.
Predominantly, contemporary subject-driven T2I approaches hinge on Latent
Diffusion Models (LDMs), which facilitate T2I mapping through cross-attention
layers. While LDMs offer distinct advantages, P-T2I methods' reliance on the
latent space of these diffusion models significantly escalates resource
demands, leading to inconsistent results and necessitating numerous iterations
for a single desired image. Recently, ECLIPSE has demonstrated a more
resource-efficient pathway for training UnCLIP-based T2I models, circumventing
the need for diffusion text-to-image priors. Building on this, we introduce
$\lambda$-ECLIPSE. Our method illustrates that effective P-T2I does not
necessarily depend on the latent space of diffusion models. $\lambda$-ECLIPSE
achieves single, multi-subject, and edge-guided T2I personalization with just
34M parameters and is trained on a mere 74 GPU hours using 1.6M image-text
interleaved data. Through extensive experiments, we also establish that
$\lambda$-ECLIPSE surpasses existing baselines in composition alignment while
preserving concept alignment performance, even with significantly lower
resource utilization.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05197" title="Abstract">arXiv:2402.05197</a> [<a href="/pdf/2402.05197" title="Download PDF">pdf</a>, <a href="/format/2402.05197" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Geometric Slosh-Free Tracking for Robotic Manipulators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arrizabalaga%2C+J">Jon Arrizabalaga</a>, 
<a href="/search/cs?searchtype=author&query=Pries%2C+L">Lukas Pries</a>, 
<a href="/search/cs?searchtype=author&query=Laha%2C+R">Riddhiman Laha</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Runkang Li</a>, 
<a href="/search/cs?searchtype=author&query=Haddadin%2C+S">Sami Haddadin</a>, 
<a href="/search/cs?searchtype=author&query=Ryll%2C+M">Markus Ryll</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for publication at the IEEE International Conference on Robotics and Automation (ICRA), Yokohama, Japan, May 2024. Copyright @ IEEE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This work focuses on the agile transportation of liquids with robotic
manipulators. In contrast to existing methods that are either computationally
heavy, system/container specific or dependant on a singularity-prone pendulum
model, we present a real-time slosh-free tracking technique. This method solely
requires the reference trajectory and the robot's kinematic constraints to
output kinematically feasible joint space commands. The crucial element
underlying this approach consists on mimicking the end-effector's motion
through a virtual quadrotor, which is inherently slosh-free and differentially
flat, thereby allowing us to calculate a slosh-free reference orientation.
Through the utilization of a cascaded proportional-derivative (PD) controller,
this slosh-free reference is transformed into task space acceleration commands,
which, following the resolution of a Quadratic Program (QP) based on Resolved
Acceleration Control (RAC), are translated into a feasible joint configuration.
The validity of the proposed approach is demonstrated by simulated and
real-world experiments on a 7 DoF Franka Emika Panda robot.
<br />Code: https://github.com/jonarriza96/gsft Video: https://youtu.be/4kitqYVS9n8
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05201" title="Abstract">arXiv:2402.05201</a> [<a href="/pdf/2402.05201" title="Download PDF">pdf</a>, <a href="/format/2402.05201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Effect of Sampling Temperature on Problem Solving in Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Renze%2C+M">Matthew Renze</a>, 
<a href="/search/cs?searchtype=author&query=Guven%2C+E">Erhan Guven</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this research study, we empirically investigate the effect of sampling
temperature on the performance of Large Language Models (LLMs) on various
problem-solving tasks. We created a multiple-choice question-and-answer (MCQA)
exam by randomly sampling problems from standard LLM benchmarks. Then, we used
four popular LLMs with five prompt-engineering techniques to solve the MCQA
problems while increasing the sampling temperature from 0.0 to 1.0. Despite
anecdotal reports to the contrary, our empirical results indicate that changes
in temperature in the range 0.0 to 1.0 do not have a statistically significant
impact on LLM performance for problem-solving tasks. In addition, these results
appear to hold regardless of the LLM, the prompt-engineering technique, or the
problem domain. All code, data, and supplemental materials are available on
GitHub at: https://github.com/matthewrenze/jhu-llm-temperature.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05202" title="Abstract">arXiv:2402.05202</a> [<a href="/pdf/2402.05202" title="Download PDF">pdf</a>, <a href="/format/2402.05202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UEyes: An Eye-Tracking Dataset across User Interface Types
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yue Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Leiva%2C+L+A">Luis A. Leiva</a>, 
<a href="/search/cs?searchtype=author&query=Houssel%2C+P+R+B">Paul R. B. Houssel</a>, 
<a href="/search/cs?searchtype=author&query=Tavakoli%2C+H+R">Hamed R. Tavakoli</a>, 
<a href="/search/cs?searchtype=author&query=Kylm%C3%A4l%C3%A4%2C+J">Julia Kylm&#xe4;l&#xe4;</a>, 
<a href="/search/cs?searchtype=author&query=Oulasvirta%2C+A">Antti Oulasvirta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a CHI2023 workshop paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Different types of user interfaces differ significantly in the number of
elements and how they are displayed. To examine how such differences affect the
way users look at UIs, we collected and analyzed a large eye-tracking-based
dataset, UEyes (62 participants, 1,980 UI screenshots, near 20K eye movement
sequences), covering four major UI types: webpage, desktop UI, mobile UI, and
poster. Furthermore, we analyze and discuss the differences in important
factors, such as color, location, and gaze direction across UI types,
individual viewing strategies and potential future directions. This position
paper is a derivative of our recent paper with a particular focus on the UEyes
dataset.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05203" title="Abstract">arXiv:2402.05203</a> [<a href="/pdf/2402.05203" title="Download PDF">pdf</a>, <a href="/format/2402.05203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bellman Conformal Inference: Calibrating Prediction Intervals For Time  Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zitong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cand%C3%A8s%2C+E">Emmanuel Cand&#xe8;s</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+L">Lihua Lei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We introduce Bellman Conformal Inference (BCI), a framework that wraps around
any time series forecasting models and provides calibrated prediction
intervals. Unlike the existing methods, BCI is able to leverage multi-step
ahead forecasts and explicitly optimize the average interval lengths by solving
a one-dimensional stochastic control problem (SCP) at each time step. In
particular, we use the dynamic programming algorithm to find the optimal policy
for the SCP. We prove that BCI achieves long-term coverage under arbitrary
distribution shifts and temporal dependence, even with poor multi-step ahead
forecasts. We find empirically that BCI avoids uninformative intervals that
have infinite lengths and generates substantially shorter prediction intervals
on volatility forecasting problems when compared with existing methods.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05206" title="Abstract">arXiv:2402.05206</a> [<a href="/pdf/2402.05206" title="Download PDF">pdf</a>, <a href="/format/2402.05206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Giving Robots a Voice: Human-in-the-Loop Voice Creation and open-ended  Labeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Rijn%2C+P">Pol van Rijn</a>, 
<a href="/search/cs?searchtype=author&query=Mertes%2C+S">Silvan Mertes</a>, 
<a href="/search/cs?searchtype=author&query=Janowski%2C+K">Kathrin Janowski</a>, 
<a href="/search/cs?searchtype=author&query=Weitz%2C+K">Katharina Weitz</a>, 
<a href="/search/cs?searchtype=author&query=Jacoby%2C+N">Nori Jacoby</a>, 
<a href="/search/cs?searchtype=author&query=Andr%C3%A9%2C+E">Elisabeth Andr&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to CHI 2024, May 11 to 16, 2024, Honolulu, HI, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Speech is a natural interface for humans to interact with robots. Yet,
aligning a robot's voice to its appearance is challenging due to the rich
vocabulary of both modalities. Previous research has explored a few labels to
describe robots and tested them on a limited number of robots and existing
voices. Here, we develop a robot-voice creation tool followed by large-scale
behavioral human experiments (N=2,505). First, participants collectively tune
robotic voices to match 175 robot images using an adaptive human-in-the-loop
pipeline. Then, participants describe their impression of the robot or their
matched voice using another human-in-the-loop paradigm for open-ended labeling.
The elicited taxonomy is then used to rate robot attributes and to predict the
best voice for an unseen robot. We offer a web interface to aid engineers in
customizing robot voices, demonstrating the synergy between cognitive science
and machine learning for engineering tools.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05211" title="Abstract">arXiv:2402.05211</a> [<a href="/pdf/2402.05211" title="Download PDF">pdf</a>, <a href="/ps/2402.05211" title="Download PostScript">ps</a>, <a href="/format/2402.05211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Maturity Model for Urban Dataset Meta-data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fox%2C+M+S">Mark S. Fox</a>, 
<a href="/search/cs?searchtype=author&query=Gajderowicz%2C+B">Bart Gajderowicz</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+D">Dishu Lyu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
<p class="mathjax">In the current environment of data generation and publication, there is an
ever-growing number of datasets available for download. This growth
precipitates an existing challenge: sourcing and integrating relevant datasets
for analysis is becoming more complex. Despite efforts by open data platforms,
obstacles remain, predominantly rooted in inadequate metadata, unsuitable data
presentation, complications in pinpointing desired data, and data integration.
This paper delves into the intricacies of dataset retrieval, emphasizing the
pivotal role of metadata in aligning datasets with user queries. Through an
exploration of existing literature, it underscores prevailing issues such as
the identification of valuable metadata and the development of tools to
maintain and annotate them effectively. The central contribution of this
research is the proposition of a dataset metadata maturity model. Deriving
inspiration from software engineering maturity models, this framework
delineates a progression from rudimentary metadata documentation to advanced
levels, aiding dataset creators in their documentation efforts. The model
encompasses seven pivotal dimensions, spanning content to quality information,
each stratified across six maturity levels to guide the optimal documentation
of datasets, ensuring ease of discovery, relevance assessment, and
comprehensive dataset understanding. This paper also incorporates the maturity
model into a data cataloguing tool called CKAN through a custom plugin,
CKANext-udc. The plugin introduces custom fields based on different maturity
levels, allows for user interface customisation, and integrates with a graph
database, converting catalogue data into a knowledge graph based on the
Maturity Model ontology.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05212" title="Abstract">arXiv:2402.05212</a> [<a href="/pdf/2402.05212" title="Download PDF">pdf</a>, <a href="/format/2402.05212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Investigation of Patch Porting Practices of the Linux Kernel  Ecosystem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xingyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Z">Zhiyun Qian</a>, 
<a href="/search/cs?searchtype=author&query=Jaeger%2C+T">Trent Jaeger</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+C">Chengyu Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Open-source software is increasingly reused, complicating the process of
patching to repair bugs. In the case of Linux, a distinct ecosystem has formed,
with Linux mainline serving as the upstream, stable or long-term-support (LTS)
systems forked from mainline, and Linux distributions, such as Ubuntu and
Android, as downstreams forked from stable or LTS systems for end-user use.
Ideally, when a patch is committed in the Linux upstream, it should not
introduce new bugs and be ported to all the applicable downstream branches in a
timely fashion. However, several concerns have been expressed in prior work
about the responsiveness of patch porting in this Linux ecosystem. In this
paper, we mine the software repositories to investigate a range of Linux
distributions in combination with Linux stable and LTS, and find diverse patch
porting strategies and competence levels that help explain the phenomenon.
Furthermore, we show concretely using three metrics, i.e., patch delay, patch
rate, and bug inheritance ratio, that different porting strategies have
different tradeoffs. We find that hinting tags(e.g., Cc stable tags and fixes
tags) are significantly important to the prompt patch porting, but it is
noteworthy that a substantial portion of patches remain devoid of these
indicative tags. Finally, we offer recommendations based on our analysis of the
general patch flow, e.g., interactions among various stakeholders in the
ecosystem and automatic generation of hinting tags, as well as tailored
suggestions for specific porting strategies.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05223" title="Abstract">arXiv:2402.05223</a> [<a href="/pdf/2402.05223" title="Download PDF">pdf</a>, <a href="/format/2402.05223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Taming Timeout Flakiness: An Empirical Study of SAP HANA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berndt%2C+A">Alexander Berndt</a>, 
<a href="/search/cs?searchtype=author&query=Baltes%2C+S">Sebastian Baltes</a>, 
<a href="/search/cs?searchtype=author&query=Bach%2C+T">Thomas Bach</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 9 figures, 3 tables, Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice (ICSE SEIP 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Regression testing aims to prevent code changes from breaking existing
features. Flaky tests negatively affect regression testing because they result
in test failures that are not necessarily caused by code changes, thus
providing an ambiguous signal. Test timeouts are one contributing factor to
such flaky test failures. With the goal of reducing test flakiness in SAP HANA,
we empirically study the impact of test timeouts on flakiness in system tests.
We evaluate different approaches to automatically adjust timeout values,
assessing their suitability for reducing execution time costs and improving
build turnaround times. We collect metadata on SAP HANA's test executions by
repeatedly executing tests on the same code revision over a period of six
months. We analyze the test flakiness rate, investigate the evolution of test
timeout values, and evaluate different approaches for optimizing timeout
values. The test flakiness rate ranges from 49% to 70%, depending on the number
of repeated test executions. Test timeouts account for 70% of flaky test
failures. Developers typically react to flaky timeouts by manually increasing
timeout values or splitting long-running tests. However, manually adjusting
timeout values is a tedious task. Our approach for timeout optimization reduces
timeout-related flaky failures by 80% and reduces the overall median timeout
value by 25%, i.e., blocked tests are identified faster. Test timeouts are a
major contributing factor to flakiness in system tests. It is challenging for
developers to effectively mitigate this problem manually. Our technique for
optimizing timeout values reduces flaky failures while minimizing test costs.
Practitioners working on large-scale industrial software systems can use our
findings to increase the effectiveness of their system tests while reducing the
burden on developers to manually maintain appropriate timeout values.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05224" title="Abstract">arXiv:2402.05224</a> [<a href="/pdf/2402.05224" title="Download PDF">pdf</a>, <a href="/format/2402.05224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VerAs: Verify then Assess STEM Lab Reports
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Atil%2C+B">Berk Atil</a>, 
<a href="/search/cs?searchtype=author&query=Karizaki%2C+M+S">Mahsa Sheikhi Karizaki</a>, 
<a href="/search/cs?searchtype=author&query=Passonneau%2C+R+J">Rebecca J. Passonneau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">With an increasing focus in STEM education on critical thinking skills,
science writing plays an ever more important role in curricula that stress
inquiry skills. A recently published dataset of two sets of college level lab
reports from an inquiry-based physics curriculum relies on analytic assessment
rubrics that utilize multiple dimensions, specifying subject matter knowledge
and general components of good explanations. Each analytic dimension is
assessed on a 6-point scale, to provide detailed feedback to students that can
help them improve their science writing skills. Manual assessment can be slow,
and difficult to calibrate for consistency across all students in large
classes. While much work exists on automated assessment of open-ended questions
in STEM subjects, there has been far less work on long-form writing such as lab
reports. We present an end-to-end neural architecture that has separate
verifier and assessment modules, inspired by approaches to Open Domain Question
Answering (OpenQA). VerAs first verifies whether a report contains any content
relevant to a given rubric dimension, and if so, assesses the relevant
sentences. On the lab reports, VerAs outperforms multiple baselines based on
OpenQA systems or Automated Essay Scoring (AES). VerAs also performs well on an
analytic rubric for middle school physics essays.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05226" title="Abstract">arXiv:2402.05226</a> [<a href="/pdf/2402.05226" title="Download PDF">pdf</a>, <a href="/ps/2402.05226" title="Download PostScript">ps</a>, <a href="/format/2402.05226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Analysis of Secondary Coexistence in a Real-World CBRS  Deployment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tusha%2C+A">Armed Tusha</a> (1), 
<a href="/search/cs?searchtype=author&query=Dogan-Tusha%2C+S">Seda Dogan-Tusha</a> (1), 
<a href="/search/cs?searchtype=author&query=Nasiri%2C+H">Hossein Nasiri</a> (1), 
<a href="/search/cs?searchtype=author&query=Rochman%2C+M+I">Muhammad I. Rochman</a> (2), 
<a href="/search/cs?searchtype=author&query=McGuire%2C+P">Patrick McGuire</a> (3), 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+M">Monisha Ghosh</a> (1) ((1) Department of Electrical and Electronics Engineering, University of Notre Dame, South Bend, IN, USA, (2) Department of Computer Science, University of Chicago, Chicago, IL, USA, (3) South Bend Community School Corporation, South Bend, IN, USA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The Federal Communications Commission (FCC) in the U.S. has made the Citizens
Broadband Radio Service (CBRS) band (3.55 - 3.7 GHz) available for commercial
wireless usage under a shared approach using a three-tier hierarchical
architecture, where the federal incumbent is the highest priority Tier 1 user,
Priority Access License (PAL) holders, who have paid for licenses, are Tier 2
users and Tier 3 users operate under General Authorized Access (GAA), without
license fees or protection from higher priority users. The Spectrum Access
System (SAS) ensures that higher priority users are protected from interference
from lower priority users. However, the lowest priority GAA users are not given
any protection from each other by the SAS and are expected to not cause any
harmful interference to Tier 1 and Tier 2 users. As the deployments of GAA
devices grow, the potential for secondary interference between GAA users
increases, especially since the SAS architecture does not allow dynamic channel
switching when faced with interference. In this paper, we present a
first-of-its-kind extensive measurement campaign of a commercial CBRS network
deployed in the city of South Bend, IN, that quantifies both co-channel
interference (CCI) and adjacent channel interference (ACI) caused by competing
GAA devices and C-band 5G, respectively. We (i) identify a particular CCI
scenario and improve performance by changing the frequency allocation based on
our study of other allocations in the vicinity and (ii) quantify ACI from 5G in
C-band (3.7 GHz) on CBRS throughput. We conclude that (i) CCI and ACI for GAA
users is not handled well by the SAS, (ii) proper frequency allocation for GAA
requires additional analysis of interference from other GAA users followed by
dynamical channel selection, and (iii) utilization of immediate adjacent
channels by high power 5G deployments limits the performance of CBRS.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05229" title="Abstract">arXiv:2402.05229</a> [<a href="/pdf/2402.05229" title="Download PDF">pdf</a>, <a href="/ps/2402.05229" title="Download PostScript">ps</a>, <a href="/format/2402.05229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Long time numerical stability of implicit schemes for stochastic heat  equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Yang%2C+X">Xiaochen Yang</a>, 
<a href="/search/math?searchtype=author&query=Hu%2C+Y">Yaozhong Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
<p class="mathjax">This paper studies the long time stability of both stochastic heat equations
on a bounded domain driven by a correlated noise and their approximations. It
is popular for researchers to prove the intermittency of the solution which
means that the moments of solution to stochastic heat equation usually grow
exponentially to infinite and this hints that the solution to stochastic heat
equation is generally not stable in long time. However, quite surprisingly in
this paper we show that when the domain is bounded and when the noise is not
singular in spatial variables, the system can be long time stable and we also
prove that we can approximate the solution by its finite dimensional spectral
approximation which is also long time stable. The idea is to use eigenfunction
expansion of the Laplacian on bounded domain. We also present numerical
experiments which are consistent with our theoretical results.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05232" title="Abstract">arXiv:2402.05232</a> [<a href="/pdf/2402.05232" title="Download PDF">pdf</a>, <a href="/format/2402.05232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Neural Functionals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Allan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Finn%2C+C">Chelsea Finn</a>, 
<a href="/search/cs?searchtype=author&query=Harrison%2C+J">James Harrison</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A challenging problem in many modern machine learning tasks is to process
weight-space features, i.e., to transform or extract information from the
weights and gradients of a neural network. Recent works have developed
promising weight-space models that are equivariant to the permutation
symmetries of simple feedforward networks. However, they are not applicable to
general architectures, since the permutation symmetries of a weight space can
be complicated by recurrence or residual connections. This work proposes an
algorithm that automatically constructs permutation equivariant models, which
we refer to as universal neural functionals (UNFs), for any weight space. Among
other applications, we demonstrate how UNFs can be substituted into existing
learned optimizer designs, and find promising improvements over prior methods
when optimizing small image classifiers and language models. Our results
suggest that learned optimizers can benefit from considering the (symmetry)
structure of the weight space they optimize. We open-source our library for
constructing UNFs at
https://github.com/AllanYangZhou/universal_neural_functional.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05234" title="Abstract">arXiv:2402.05234</a> [<a href="/pdf/2402.05234" title="Download PDF">pdf</a>, <a href="/format/2402.05234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QGFN: Controllable Greediness with Action Values
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lau%2C+E">Elaine Lau</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S+Z">Stephen Zhewen Lu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Ling Pan</a>, 
<a href="/search/cs?searchtype=author&query=Precup%2C+D">Doina Precup</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+E">Emmanuel Bengio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Generative Flow Networks (GFlowNets; GFNs) are a family of
reward/energy-based generative methods for combinatorial objects, capable of
generating diverse and high-utility samples. However, biasing GFNs towards
producing high-utility samples is non-trivial. In this work, we leverage
connections between GFNs and reinforcement learning (RL) and propose to combine
the GFN policy with an action-value estimate, $Q$, to create greedier sampling
policies which can be controlled by a mixing parameter. We show that several
variants of the proposed method, QGFN, are able to improve on the number of
high-reward samples generated in a variety of tasks without sacrificing
diversity.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05235" title="Abstract">arXiv:2402.05235</a> [<a href="/pdf/2402.05235" title="Download PDF">pdf</a>, <a href="/format/2402.05235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPAD : Spatially Aware Multiview Diffusers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kant%2C+Y">Yash Kant</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Ziyi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Vasilkovsky%2C+M">Michael Vasilkovsky</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+G">Guocheng Qian</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jian Ren</a>, 
<a href="/search/cs?searchtype=author&query=Guler%2C+R+A">Riza Alp Guler</a>, 
<a href="/search/cs?searchtype=author&query=Ghanem%2C+B">Bernard Ghanem</a>, 
<a href="/search/cs?searchtype=author&query=Tulyakov%2C+S">Sergey Tulyakov</a>, 
<a href="/search/cs?searchtype=author&query=Gilitschenski%2C+I">Igor Gilitschenski</a>, 
<a href="/search/cs?searchtype=author&query=Siarohin%2C+A">Aliaksandr Siarohin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Webpage: <a href="https://yashkant.github.io/spad">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present SPAD, a novel approach for creating consistent multi-view images
from text prompts or single images. To enable multi-view generation, we
repurpose a pretrained 2D diffusion model by extending its self-attention
layers with cross-view interactions, and fine-tune it on a high quality subset
of Objaverse. We find that a naive extension of the self-attention proposed in
prior work (e.g. MVDream) leads to content copying between views. Therefore, we
explicitly constrain the cross-view attention based on epipolar geometry. To
further enhance 3D consistency, we utilize Plucker coordinates derived from
camera rays and inject them as positional encoding. This enables SPAD to reason
over spatial proximity in 3D well. In contrast to recent works that can only
generate views at fixed azimuth and elevation, SPAD offers full camera control
and achieves state-of-the-art results in novel view synthesis on unseen objects
from the Objaverse and Google Scanned Objects datasets. Finally, we demonstrate
that text-to-3D generation using SPAD prevents the multi-face Janus issue. See
more details at our webpage: https://yashkant.github.io/spad
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05236" title="Abstract">arXiv:2402.05236</a> [<a href="/pdf/2402.05236" title="Download PDF">pdf</a>, <a href="/format/2402.05236" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-Time Line-Based Room Segmentation and Continuous Euclidean Distance  Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Warberg%2C+E">Erik Warberg</a> (1), 
<a href="/search/cs?searchtype=author&query=Miksits%2C+A">Adam Miksits</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Barbosa%2C+F+S">Fernando S. Barbosa</a> (2) ((1) KTH Royal Institute of Technology, (2) Ericsson Research)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Open-source code: <a href="https://github.com/EricssonResearch/Line-Based-Room-Segmentation-and-EDF">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Continuous maps representations, as opposed to traditional discrete ones such
as grid maps, have been gaining traction in the research community. However,
current approaches still suffer from high computation costs, making them unable
to be used in large environments without sacrificing precision. In this paper,
a scalable method building upon Gaussian Process-based Euclidean Distance
Fields (GP-EDFs) is proposed. By leveraging structure inherent to indoor
environments, namely walls and rooms, we achieve an accurate continuous map
representation that is fast enough to be updated and used in real-time. This is
possible thanks to a novel line-based room segmentation algorithm, enabling the
creation of smaller local GP-EDFs for each room, which in turn also use line
segments as its shape priors, thus representing the map more efficiently with
fewer data points. We evaluate this method in simulation experiments, and make
the code available open-source.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05238" title="Abstract">arXiv:2402.05238</a> [<a href="/pdf/2402.05238" title="Download PDF">pdf</a>, <a href="/ps/2402.05238" title="Download PostScript">ps</a>, <a href="/format/2402.05238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Data-Driven Discovery of Material Models Based on Symbolic  Regression: A Case Study on Human Brain Cortex
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hou%2C+J">Jixin Hou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xianyan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Taotao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Kuhl%2C+E">Ellen Kuhl</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xianqiao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 53 pages, 17 figures, and 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Symbolic Computation (cs.SC)</span>; Quantitative Methods (q-bio.QM); Tissues and Organs (q-bio.TO)

</div>
<p class="mathjax">We introduce a data-driven framework to automatically identify interpretable
and physically meaningful hyperelastic constitutive models from sparse data.
Leveraging symbolic regression, an algorithm based on genetic programming, our
approach generates elegant hyperelastic models that achieve accurate data
fitting through parsimonious mathematic formulae, while strictly adhering to
hyperelasticity constraints such as polyconvexity. Our investigation spans
three distinct hyperelastic models -- invariant-based, principal stretch-based,
and normal strain-based -- and highlights the versatility of symbolic
regression. We validate our new approach using synthetic data from five classic
hyperelastic models and experimental data from the human brain to demonstrate
algorithmic efficacy. Our results suggest that our symbolic regression robustly
discovers accurate models with succinct mathematic expressions in
invariant-based, stretch-based, and strain-based scenarios. Strikingly, the
strain-based model exhibits superior accuracy, while both stretch- and
strain-based models effectively capture the nonlinearity and
tension-compression asymmetry inherent to human brain tissue. Polyconvexity
examinations affirm the rigor of convexity within the training regime and
demonstrate excellent extrapolation capabilities beyond this regime for all
three models. However, the stretch-based models raise concerns regarding
potential convexity loss under large deformations. Finally, robustness tests on
noise-embedded data underscore the reliability of our symbolic regression
algorithms. Our study confirms the applicability and accuracy of symbolic
regression in the automated discovery of hyperelastic models for the human
brain and gives rise to a wide variety of applications in other soft matter
systems.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05244" title="Abstract">arXiv:2402.05244</a> [<a href="/pdf/2402.05244" title="Download PDF">pdf</a>, <a href="/ps/2402.05244" title="Download PostScript">ps</a>, <a href="/format/2402.05244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CRIU -- Checkpoint Restore in Userspace for computational simulations  and scientific applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Andrijauskas%2C+F">Fabio Andrijauskas</a>, 
<a href="/search/cs?searchtype=author&query=Sfiligoi%2C+I">Igor Sfiligoi</a>, 
<a href="/search/cs?searchtype=author&query=Davila%2C+D">Diego Davila</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+A">Aashay Arora</a>, 
<a href="/search/cs?searchtype=author&query=Guiang%2C+J">Jonathan Guiang</a>, 
<a href="/search/cs?searchtype=author&query=Bockelman%2C+B">Brian Bockelman</a>, 
<a href="/search/cs?searchtype=author&query=Thain%2C+G">Greg Thain</a>, 
<a href="/search/cs?searchtype=author&query=Wurthwein%2C+F">Frank Wurthwein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26TH INTERNATIONAL CONFERENCE ON COMPUTING IN HIGH ENERGY &amp; NUCLEAR PHYSICS - 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Creating new materials, discovering new drugs, and simulating systems are
essential processes for research and innovation and require substantial
computational power. While many applications can be split into many smaller
independent tasks, some cannot and may take hours or weeks to run to
completion. To better manage those longer-running jobs, it would be desirable
to stop them at any arbitrary point in time and later continue their
computation on another compute resource; this is usually referred to as
checkpointing. While some applications can manage checkpointing
programmatically, it would be preferable if the batch scheduling system could
do that independently. This paper evaluates the feasibility of using CRIU
(Checkpoint Restore in Userspace), an open-source tool for the GNU/Linux
environments, emphasizing the OSG's OSPool HTCondor setup. CRIU allows
checkpointing the process state into a disk image and can deal with both open
files and established network connections seamlessly. Furthermore, it can
checkpoint traditional Linux processes and containerized workloads. The
functionality seems adequate for many scenarios supported in the OSPool.
However, some limitations prevent it from being usable in all circumstances.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05245" title="Abstract">arXiv:2402.05245</a> [<a href="/pdf/2402.05245" title="Download PDF">pdf</a>, <a href="/ps/2402.05245" title="Download PostScript">ps</a>, <a href="/format/2402.05245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Outcome Equivalence of Extensive-Form and Behavioral Correlated  Equilibria
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B+H">Brian Hu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sandholm%2C+T">Tuomas Sandholm</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We investigate two notions of correlated equilibrium for extensive-form
games: extensive-form correlated equilibrium (EFCE) and behavioral correlated
equilibrium (BCE). We show that the two are outcome-equivalent, in the sense
that every outcome distribution achievable under one notion is achievable under
the other. Our result implies, to our knowledge, the first polynomial-time
algorithm for computing a BCE.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05248" title="Abstract">arXiv:2402.05248</a> [<a href="/pdf/2402.05248" title="Download PDF">pdf</a>, <a href="/ps/2402.05248" title="Download PostScript">ps</a>, <a href="/format/2402.05248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparative Analysis of Kinect-Based and Oculus-Based Gaze Region  Estimation Methods in a Driving Simulator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gonz%C3%A1lez-Ortega%2C+D">David Gonz&#xe1;lez-Ortega</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%ADaz-Perna%2C+F+J">Francisco Javier D&#xed;az-Perna</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez-Zarzuela%2C+M">Mario Mart&#xed;nez-Zarzuela</a>, 
<a href="/search/cs?searchtype=author&query=Ant%C3%B3n-Rodr%C3%ADguez%2C+M">M&#xed;riam Ant&#xf3;n-Rodr&#xed;guez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Sensors 2021, 21, 26
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Driver's gaze information can be crucial in driving research because of its
relation to driver attention. Particularly, the inclusion of gaze data in
driving simulators broadens the scope of research studies as they can relate
drivers' gaze patterns to their features and performance. In this paper, we
present two gaze region estimation modules integrated in a driving simulator.
One uses the 3D Kinect device and another uses the virtual reality Oculus Rift
device. The modules are able to detect the region, out of seven in which the
driving scene was divided, where a driver is gazing at in every route processed
frame. Four methods were implemented and compared for gaze estimation, which
learn the relation between gaze displacement and head movement. Two are simpler
and based on points that try to capture this relation and two are based on
classifiers such as MLP and SVM. Experiments were carried out with 12 users
that drove on the same scenario twice, each one with a different visualization
display, first with a big screen and later with Oculus Rift. On the whole,
Oculus Rift outperformed Kinect as the best hardware for gaze estimation. The
Oculus-based gaze region estimation method with the highest performance
achieved an accuracy of 97.94%. The information provided by the Oculus Rift
module enriches the driving simulator data and makes it possible a multimodal
driving performance analysis apart from the immersion and realism obtained with
the virtual reality experience provided by Oculus.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05249" title="Abstract">arXiv:2402.05249</a> [<a href="/pdf/2402.05249" title="Download PDF">pdf</a>, <a href="/ps/2402.05249" title="Download PostScript">ps</a>, <a href="/format/2402.05249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digital Distractions from the Point of View of Higher Education Students
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez-Ju%C3%A1rez%2C+M+%C3%81">Mar&#xed;a &#xc1;ngeles P&#xe9;rez-Ju&#xe1;rez</a>, 
<a href="/search/cs?searchtype=author&query=Gonz%C3%A1lez-Ortega%2C+D">David Gonz&#xe1;lez-Ortega</a>, 
<a href="/search/cs?searchtype=author&query=Aguiar-P%C3%A9rez%2C+J+M">Javier Manuel Aguiar-P&#xe9;rez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Sustainability 2023, 15, 6044
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Technology enables a more sustainable and universally accessible educational
model. However, technology has brought a paradox into students' lives: it helps
them engage in learning activities, but it is also a source of distraction.
During the academic year 2021-2022, the authors conducted a study focusing on
classroom distractions. One of the objectives was to identify the main digital
distractions from the point of view of students. The study was carried out at
an engineering school, where technology is fully integrated in the classroom
and in the academic routines of teachers and students. Discussions and surveys,
complemented by a statistical study based on bivariate correlations, were used
with participating students (n = 105). Students considered digital distractions
to have a significant impact on their performance in lab sessions. This
performance was mainly self-assessed as improvable. Contrary to other
contemporary research, the results were not influenced by the year of study of
the subject, as the issue is important regardless of the students' backgrounds.
Professors should implement strategies to raise students' awareness of the
significant negative effects of digital distractions on their performance, as
well as to develop students' self-control skills. This is of vital importance
for the use of technology to be sustainable in the long-term.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05252" title="Abstract">arXiv:2402.05252</a> [<a href="/pdf/2402.05252" title="Download PDF">pdf</a>, <a href="/format/2402.05252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Fair Ranking Policies via Differentiable Optimization of  Ordered Weighted Averages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dinh%2C+M+H">My H. Dinh</a>, 
<a href="/search/cs?searchtype=author&query=Kotary%2C+J">James Kotary</a>, 
<a href="/search/cs?searchtype=author&query=Fioretto%2C+F">Ferdinando Fioretto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Learning to Rank (LTR) is one of the most widely used machine learning
applications. It is a key component in platforms with profound societal
impacts, including job search, healthcare information retrieval, and social
media content feeds. Conventional LTR models have been shown to produce biases
results, stimulating a discourse on how to address the disparities introduced
by ranking systems that solely prioritize user relevance. However, while
several models of fair learning to rank have been proposed, they suffer from
deficiencies either in accuracy or efficiency, thus limiting their
applicability to real-world ranking platforms. This paper shows how
efficiently-solvable fair ranking models, based on the optimization of Ordered
Weighted Average (OWA) functions, can be integrated into the training loop of
an LTR model to achieve favorable balances between fairness, user utility, and
runtime efficiency. In particular, this paper is the first to show how to
backpropagate through constrained optimizations of OWA objectives, enabling
their use in integrated prediction and decision models.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05254" title="Abstract">arXiv:2402.05254</a> [<a href="/pdf/2402.05254" title="Download PDF">pdf</a>, <a href="/format/2402.05254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online and Certifiably Correct Visual Odometry and Mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+D+R">Devansh R Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Govindjee%2C+R">Rajiv Govindjee</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jiangbo Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ravikumar%2C+A">Anurekha Ravikumar</a>, 
<a href="/search/cs?searchtype=author&query=Panagou%2C+D">Dimitra Panagou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper proposes two new algorithms for certified perception in
safety-critical robotic applications. The first is a Certified Visual Odometry
algorithm, which uses a RGBD camera with bounded sensor noise to construct a
visual odometry estimate with provable error bounds. The second is a Certified
Mapping algorithm which, using the same RGBD images, constructs a Signed
Distance Field of the obstacle environment, always safely underestimating the
distance to the nearest obstacle. This is required to avoid errors due to VO
drift. The algorithms are demonstrated in hardware experiments, where we
demonstrate both running online at 30FPS. The methods are also compared to
state-of-the-art techniques for odometry and mapping.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05255" title="Abstract">arXiv:2402.05255</a> [<a href="/pdf/2402.05255" title="Download PDF">pdf</a>, <a href="/ps/2402.05255" title="Download PostScript">ps</a>, <a href="/format/2402.05255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Overview of Machine Learning-Enabled Network Softwarization for the  Internet of Things
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zormati%2C+M+A">Mohamed Ali Zormati</a>, 
<a href="/search/cs?searchtype=author&query=Lakhlef%2C+H">Hicham Lakhlef</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 International Conference on Software, Telecommunications and
  Computer Networks (SoftCOM), Split, Croatia, 2023, pp. 1-6
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The Internet of Things (IoT) has evolved from a novel technology to an
integral part of our everyday lives. It encompasses a multitude of
heterogeneous devices that collect valuable data through various sensors. The
sheer volume of these interconnected devices poses significant challenges as
IoT provides complex network services with diverse requirements on a shared
infrastructure. Network softwarization could help address these issues as it
has emerged as a paradigm that enhances traditional networking by decoupling
hardware from software and leveraging enabling technologies such as Software
Defined Networking (SDN) and Network Function Virtualization (NFV). In
networking, Machine Learning (ML) has demonstrated impressive results across
multiple domains. By smoothly integrating with network softwarization, ML plays
a pivotal role in building efficient and intelligent IoT networks. This paper
explores the fundamentals of IoT, network softwarization, and ML, while
reviewing the latest advances in ML-enabled network softwarization for IoT.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05256" title="Abstract">arXiv:2402.05256</a> [<a href="/pdf/2402.05256" title="Download PDF">pdf</a>, <a href="/format/2402.05256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IRFuzzer: Specialized Fuzzing for LLVM Backend Code Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rong%2C+Y">Yuyang Rong</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhanghan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+Z">Zhenkai Weng</a>, 
<a href="/search/cs?searchtype=author&query=Neuendorffer%2C+S">Stephen Neuendorffer</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Modern compilers, such as LLVM, are complex pieces of software. Due to their
complexity, manual testing is unlikely to suffice, yet formal verification is
difficult to scale. End-to-end fuzzing can be used, but it has difficulties in
achieving high coverage of some components of LLVM.
<br />In this paper, we implement IRFuzzer to investigate the effectiveness of
specialized fuzzing of the LLVM compiler backend. We focus on two approaches to
improve the fuzzer: guaranteed input validity using constrained mutations and
improved feedback quality. The mutator in IRFuzzer is capable of generating a
wide range of LLVM IR inputs, including structured control flow, vector types,
and function definitions. The system instruments coding patterns in the
compiler to monitor the execution status of instruction selection. The
instrumentation not only provides a new coverage feedback called matcher table
coverage, but also provides an architecture specific guidance to the mutator.
<br />We show that IRFuzzer is more effective than existing fuzzers by fuzzing on
29 mature LLVM backend targets. In the process, we reported 74 confirmed new
bugs in LLVM upstream, out of which 49 have been fixed, five have been back
ported to LLVM 15, showing that specialized fuzzing provides useful and
actionable insights to LLVM developers.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05259" title="Abstract">arXiv:2402.05259</a> [<a href="/pdf/2402.05259" title="Download PDF">pdf</a>, <a href="/format/2402.05259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling Architecture for Distributed Intelligent Network Softwarization  for the Internet of Things
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zormati%2C+M+A">Mohamed Ali Zormati</a>, 
<a href="/search/cs?searchtype=author&query=Lakhlef%2C+H">Hicham Lakhlef</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE 20th International Conference on Mobile Ad Hoc and Smart
  Systems (MASS), Toronto, ON, Canada, 2023, pp. 608-609
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The Internet of Things (IoT) is becoming a part of everyday life through its
various sensing devices that collect valuable information. The huge number of
interconnected heterogeneous IoT devices poses immense challenges, and network
softwarization techniques are an adequate solution to these concerns. Software
Defined Networking (SDN) and Network Function Virtualization (NFV) are two key
softwarization techniques that enable the realization of efficient, agile IoT
networks, especially when combined with Machine Learning (ML), mainly Federated
Learning (FL). Unfortunately, existing solutions do not take advantage of such
a combination to strengthen IoT networks in terms of efficiency and
scalability. In this paper, we propose a novel architecture to achieve
distributed intelligent network softwarization for IoT, in which SDN, NFV, and
ML combine forces to enhance IoT constrained networks.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05263" title="Abstract">arXiv:2402.05263</a> [<a href="/pdf/2402.05263" title="Download PDF">pdf</a>, <a href="/ps/2402.05263" title="Download PostScript">ps</a>, <a href="/format/2402.05263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Position and Speed Control of Brushless DC Motors Using Sensorless  Techniques and Application Trends
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gamazo-Real%2C+J">Jose-Carlos Gamazo-Real</a>, 
<a href="/search/eess?searchtype=author&query=Vazquez-Sanchez%2C+E">Ernesto Vazquez-Sanchez</a>, 
<a href="/search/eess?searchtype=author&query=Gomez-Gil%2C+J">Jaime Gomez-Gil</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Sensors, 2010, vol. 10, no. 7, pp. 6901-6947, ISSN 1424-8220
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper provides a technical review of position and speed sensorless
methods for controlling Brushless Direct Current (BLDC) motor drives, including
the background analysis using sensors, limitations and advances. The
performance and reliability of BLDC motor drivers have been improved because
the conventional control and sensing techniques have been improved through
sensorless technology. Then, in this paper sensorless advances are reviewed and
recent developments in this area are introduced with their inherent advantages
and drawbacks, including the analysis of practical implementation issues and
applications. The study includes a deep overview of state-of-the-art back-EMF
sensing methods, which includes Terminal Voltage Sensing, Third Harmonic
Voltage Integration, Terminal Current Sensing, Back-EMF Integration and PWM
strategies. Also, the most relevant techniques based on estimation and models
are briefly analysed, such as Sliding-mode Observer, Extended Kalman Filter,
Model Reference Adaptive System, Adaptive observers (Full-order and
Pseudoreduced-order) and Artificial Neural Networks.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05264" title="Abstract">arXiv:2402.05264</a> [<a href="/pdf/2402.05264" title="Download PDF">pdf</a>, <a href="/format/2402.05264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdaBatchGrad: Combining Adaptive Batch Size and Adaptive Step Size
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ostroukhov%2C+P">Petr Ostroukhov</a>, 
<a href="/search/cs?searchtype=author&query=Zhumabayeva%2C+A">Aigerim Zhumabayeva</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+C">Chulu Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Gasnikov%2C+A">Alexander Gasnikov</a>, 
<a href="/search/cs?searchtype=author&query=Tak%C3%A1%C4%8D%2C+M">Martin Tak&#xe1;&#x10d;</a>, 
<a href="/search/cs?searchtype=author&query=Kamzolov%2C+D">Dmitry Kamzolov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper presents a novel adaptation of the Stochastic Gradient Descent
(SGD), termed AdaBatchGrad. This modification seamlessly integrates an adaptive
step size with an adjustable batch size. An increase in batch size and a
decrease in step size are well-known techniques to tighten the area of
convergence of SGD and decrease its variance. A range of studies by R. Byrd and
J. Nocedal introduced various testing techniques to assess the quality of
mini-batch gradient approximations and choose the appropriate batch sizes at
every step. Methods that utilized exact tests were observed to converge within
$O(LR^2/\varepsilon)$ iterations. Conversely, inexact test implementations
sometimes resulted in non-convergence and erratic performance. To address these
challenges, AdaBatchGrad incorporates both adaptive batch and step sizes,
enhancing the method's robustness and stability. For exact tests, our approach
converges in $O(LR^2/\varepsilon)$ iterations, analogous to standard gradient
descent. For inexact tests, it achieves convergence in $O(\max\lbrace
LR^2/\varepsilon, \sigma^2 R^2/\varepsilon^2 \rbrace )$ iterations. This makes
AdaBatchGrad markedly more robust and computationally efficient relative to
prevailing methods. To substantiate the efficacy of our method, we
experimentally show, how the introduction of adaptive step size and adaptive
batch size gradually improves the performance of regular SGD. The results imply
that AdaBatchGrad surpasses alternative methods, especially when applied to
inexact tests.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05266" title="Abstract">arXiv:2402.05266</a> [<a href="/pdf/2402.05266" title="Download PDF">pdf</a>, <a href="/format/2402.05266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A computational approach to visual ecology with deep reinforcement  learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sokoloski%2C+S">Sacha Sokoloski</a>, 
<a href="/search/cs?searchtype=author&query=Majnik%2C+J">Jure Majnik</a>, 
<a href="/search/cs?searchtype=author&query=Berens%2C+P">Philipp Berens</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">Animal vision is thought to optimize various objectives from metabolic
efficiency to discrimination performance, yet its ultimate objective is to
facilitate the survival of the animal within its ecological niche. However,
modeling animal behavior in complex environments has been challenging. To study
how environments shape and constrain visual processing, we developed a deep
reinforcement learning framework in which an agent moves through a 3-d
environment that it perceives through a vision model, where its only goal is to
survive. Within this framework we developed a foraging task where the agent
must gather food that sustains it, and avoid food that harms it. We first
established that the complexity of the vision model required for survival on
this task scaled with the variety and visual complexity of the food in the
environment. Moreover, we showed that a recurrent network architecture was
necessary to fully exploit complex vision models on the most visually demanding
tasks. Finally, we showed how different network architectures learned distinct
representations of the environment and task, and lead the agent to exhibit
distinct behavioural strategies. In summary, this paper lays the foundation for
a computational approach to visual ecology, provides extensive benchmarks for
future work, and demonstrates how representations and behaviour emerge from an
agent's drive for survival.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05270" title="Abstract">arXiv:2402.05270</a> [<a href="/pdf/2402.05270" title="Download PDF">pdf</a>, <a href="/format/2402.05270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Review and Analysis of Recent Advances in Intelligent Network  Softwarization for the Internet of Things
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zormati%2C+M+A">Mohamed Ali Zormati</a>, 
<a href="/search/cs?searchtype=author&query=Lakhlef%2C+H">Hicham Lakhlef</a>, 
<a href="/search/cs?searchtype=author&query=Ouni%2C+S">Sofiane Ouni</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computer Networks, Volume 241, 2024, 110215, ISSN 1389-1286
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The Internet of Things (IoT) is an emerging technology that aims to connect
heterogeneous and constrained objects to each other and to the Internet. It has
grown significantly in a wide variety of applications such as smart homes,
smart cities, smart vehicles, etc. The huge number of connected devices
increases the challenges, as IoT provides diverse and complex network services
with different requirements on a common infrastructure. Network Softwarization
is the latest network paradigm that transforms traditional network processes to
the separation of hardware and software by using some enabling network
technologies such as Software Defined Networking (SDN) and Network Function
Virtualization (NFV). Machine Learning (ML) plays an essential role in creating
smarter IoT networks, as it has shown remarkable results in various domains.
Given that the network softwarization allows it to be easily integrated, ML can
play a crucial role in efficient and self-adaptive IoT networks. In this paper,
we provide a detailed overview of the concepts of IoT, network softwarization,
and ML, and we study and discuss the state of the art of intelligent ML-enabled
network softwarization for IoT. We also identify the most prominent future
research directions to be considered.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05273" title="Abstract">arXiv:2402.05273</a> [<a href="/pdf/2402.05273" title="Download PDF">pdf</a>, <a href="/format/2402.05273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ASCENT: A Context-Aware Spectrum Coexistence Design and Implementation  Toolset for Policymakers in Satellite Bands
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Niloy%2C+T+R">Ta-seen Reaz Niloy</a>, 
<a href="/search/eess?searchtype=author&query=Kumar%2C+S">Saurav Kumar</a>, 
<a href="/search/eess?searchtype=author&query=Hore%2C+A">Aniruddha Hore</a>, 
<a href="/search/eess?searchtype=author&query=Hassan%2C+Z">Zoheb Hassan</a>, 
<a href="/search/eess?searchtype=author&query=Dietrich%2C+C">Carl Dietrich</a>, 
<a href="/search/eess?searchtype=author&query=Burger%2C+E+W">Eric W. Burger</a>, 
<a href="/search/eess?searchtype=author&query=Reed%2C+J+H">Jeffrey H. Reed</a>, 
<a href="/search/eess?searchtype=author&query=Shah%2C+V+K">Vijay K. Shah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper introduces ASCENT (context Aware Spectrum Coexistence Design and
Implementation) toolset, an advanced context-aware terrestrial satellite
spectrum sharing toolset designed for researchers, policymakers, and
regulators. It serves two essential purposes (a) evaluating the potential for
harmful interference to primary users in satellite bands and (b) facilitating
the analysis, design, and implementation of diverse regulatory policies on
spectrum usage and sharing. Notably, ASCENT implements a closed-loop feedback
system that allows dynamic adaptation of policies according to a wide range of
contextual factors (e.g., weather, buildings, summer/winter foliage, etc.) and
feedback on the impact of these policies through realistic simulation.
Specifically, ASCENT comprises the following components (i) interference
evaluation tool for evaluating interference at the incumbents in a
spectrum-sharing environment while taking the underlying contexts, (ii) dynamic
spectrum access (DSA) framework for providing context-aware instructions to
adapt networking parameters and control secondary terrestrial network's access
to the shared spectrum band according to context aware prioritization, (iii)
Context broker to acquire essential and relevant contexts from external context
information providers; and (iv) DSA Database to store dynamic and static
contexts and the regulator's policy information. The closed-loop feedback
system of ASCENT is implemented by integrating these components in a modular
software architecture. A case study of sharing the lower 12 GHz Ku band
(12.2-12.7 GHz) with the 5G terrestrial cellular network is considered, and the
usability of ASCENT is demonstrated by dynamically changing exclusion zone's
radius in different weather conditions.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05274" title="Abstract">arXiv:2402.05274</a> [<a href="/pdf/2402.05274" title="Download PDF">pdf</a>, <a href="/ps/2402.05274" title="Download PostScript">ps</a>, <a href="/format/2402.05274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence for Natural Policy Gradient on Infinite-State Average-Reward  Markov Decision Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grosof%2C+I">Isaac Grosof</a>, 
<a href="/search/cs?searchtype=author&query=Maguluri%2C+S+T">Siva Theja Maguluri</a>, 
<a href="/search/cs?searchtype=author&query=Srikant%2C+R">R. Srikant</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Infinite-state Markov Decision Processes (MDPs) are essential in modeling and
optimizing a wide variety of engineering problems. In the reinforcement
learning (RL) context, a variety of algorithms have been developed to learn and
optimize these MDPs. At the heart of many popular policy-gradient based
learning algorithms, such as natural actor-critic, TRPO, and PPO, lies the
Natural Policy Gradient (NPG) algorithm. Convergence results for these RL
algorithms rest on convergence results for the NPG algorithm. However, all
existing results on the convergence of the NPG algorithm are limited to
finite-state settings.
<br />We prove the first convergence rate bound for the NPG algorithm for
infinite-state average-reward MDPs, proving a $O(1/\sqrt{T})$ convergence rate,
if the NPG algorithm is initialized with a good initial policy. Moreover, we
show that in the context of a large class of queueing MDPs, the MaxWeight
policy suffices to satisfy our initial-policy requirement and achieve a
$O(1/\sqrt{T})$ convergence rate. Key to our result are state-dependent bounds
on the relative value function achieved by the iterate policies of the NPG
algorithm.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05275" title="Abstract">arXiv:2402.05275</a> [<a href="/pdf/2402.05275" title="Download PDF">pdf</a>, <a href="/ps/2402.05275" title="Download PostScript">ps</a>, <a href="/format/2402.05275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Hierarchical Classification Performance for Time Series Data:  Dissimilarity Measures and Classifier Comparisons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alagoz%2C+C">Celal Alagoz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 figures, 5th International Mediterranean Congress 1, 1367-1376
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The comparative performance of hierarchical classification (HC) and flat
classification (FC) methodologies in the realm of time series data analysis is
investigated in this study. Dissimilarity measures, including Jensen-Shannon
Distance (JSD), Task Similarity Distance (TSD), and Classifier Based Distance
(CBD), are leveraged alongside various classifiers such as MINIROCKET, STSF,
and SVM. A subset of datasets from the UCR archive, focusing on multi-class
cases comprising more than two classes, is employed for analysis. A significant
trend is observed wherein HC demonstrates significant superiority over FC when
paired with MINIROCKET utilizing TSD, diverging from conventional
understandings. Conversely, FC exhibits consistent dominance across all
configurations when employing alternative classifiers such as STSF and SVM.
Moreover, TSD is found to consistently outperform both CBD and JSD across
nearly all scenarios, except in instances involving the STSF classifier where
CBD showcases superior performance. This discrepancy underscores the nuanced
nature of dissimilarity measures and emphasizes the importance of their
tailored selection based on the dataset and classifier employed. Valuable
insights into the dynamic interplay between classification methodologies and
dissimilarity measures in the realm of time series data analysis are provided
by these findings. By elucidating the performance variations across different
configurations, a foundation is laid for refining classification methodologies
and dissimilarity measures to optimize performance in diverse analytical
scenarios. Furthermore, the need for continued research aimed at elucidating
the underlying mechanisms driving classification performance in time series
data analysis is underscored, with implications for enhancing predictive
modeling and decision-making in various domains.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05277" title="Abstract">arXiv:2402.05277</a> [<a href="/pdf/2402.05277" title="Download PDF">pdf</a>, <a href="/format/2402.05277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safe Human-UAS Collaboration Abstraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rastgoftar%2C+H">Hossein Rastgoftar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper studies the problem of safe humanuncrewed aerial system (UAS)
collaboration in a shared work environment. By considering human and UAS as
co-workers, we use Petri Nets to abstractly model evolution of shared tasks
assigned to human and UAS co-workers. Particularly, the Petri Nets places
represent work stations; therefore, the Petri Nets transitions can formally
specify displacements between the work stations. The first objective is to
incorporate uncertainty regarding the intentions of human co-workers into
motion planning for UAS, when UAS co-workers closely interact with human
co-workers. To this end, the proposed Petri Nets model uses conflict constructs
to represent situations at which UAS deals with incomplete knowledge about
human co-worker intention. The second objective is then to plan the motion of
the UAS in a resilient and safe manner, in the presence of non-cooperative
human co-workers. In order to achieve this objective, UAS equipped with onboard
perception and decision-making capabilities are able to, through real-time
processing of in-situ observation, predict human intention, quantify human
distraction, and apply a non-stationary Markov Decision model to safely plan
UAS motion in the presence of uncertainty.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05279" title="Abstract">arXiv:2402.05279</a> [<a href="/pdf/2402.05279" title="Download PDF">pdf</a>, <a href="/format/2402.05279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safety Filters for Black-Box Dynamical Systems by Learning  Discriminating Hyperplanes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lavanakul%2C+W">Will Lavanakul</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J+J">Jason J. Choi</a>, 
<a href="/search/cs?searchtype=author&query=Sreenath%2C+K">Koushil Sreenath</a>, 
<a href="/search/cs?searchtype=author&query=Tomlin%2C+C+J">Claire J. Tomlin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> * indicate co-first authors. This is an extended version of the paper submitted to L4DC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Learning-based approaches are emerging as an effective approach for safety
filters for black-box dynamical systems. Existing methods have relied on
certificate functions like Control Barrier Functions (CBFs) and Hamilton-Jacobi
(HJ) reachability value functions. The primary motivation for our work is the
recognition that ultimately, enforcing the safety constraint as a control input
constraint at each state is what matters. By focusing on this constraint, we
can eliminate dependence on any specific certificate function-based design. To
achieve this, we define a discriminating hyperplane that shapes the half-space
constraint on control input at each state, serving as a sufficient condition
for safety. This concept not only generalizes over traditional safety methods
but also simplifies safety filter design by eliminating dependence on specific
certificate functions. We present two strategies to learn the discriminating
hyperplane: (a) a supervised learning approach, using pre-verified control
invariant sets for labeling, and (b) a reinforcement learning (RL) approach,
which does not require such labels. The main advantage of our method, unlike
conventional safe RL approaches, is the separation of performance and safety.
This offers a reusable safety filter for learning new tasks, avoiding the need
to retrain from scratch. As such, we believe that the new notion of the
discriminating hyperplane offers a more generalizable direction towards
designing safety filters, encompassing and extending existing
certificate-function-based or safe RL methodologies.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05280" title="Abstract">arXiv:2402.05280</a> [<a href="/pdf/2402.05280" title="Download PDF">pdf</a>, <a href="/ps/2402.05280" title="Download PostScript">ps</a>, <a href="/format/2402.05280" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> No Dimensional Sampling Coresets for Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alishahi%2C+M">Meysam Alishahi</a>, 
<a href="/search/cs?searchtype=author&query=Phillips%2C+J+M">Jeff M. Phillips</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 47 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Geometry (cs.CG)

</div>
<p class="mathjax">We refine and generalize what is known about coresets for classification
problems via the sensitivity sampling framework. Such coresets seek the
smallest possible subsets of input data, so one can optimize a loss function on
the coreset and ensure approximation guarantees with respect to the original
data. Our analysis provides the first no dimensional coresets, so the size does
not depend on the dimension. Moreover, our results are general, apply for
distributional input and can use iid samples, so provide sample complexity
bounds, and work for a variety of loss functions. A key tool we develop is a
Radamacher complexity version of the main sensitivity sampling approach, which
can be of independent interest.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05281" title="Abstract">arXiv:2402.05281</a> [<a href="/pdf/2402.05281" title="Download PDF">pdf</a>, <a href="/format/2402.05281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics Informed and Data Driven Simulation of Underwater Images via  Residual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mondal%2C+T">Tanmoy Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Mendoza%2C+R">Ricardo Mendoza</a>, 
<a href="/search/cs?searchtype=author&query=Drumetz%2C+L">Lucas Drumetz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">In general, underwater images suffer from color distortion and low contrast,
because light is attenuated and backscattered as it propagates through water
(differently depending on wavelength and on the properties of the water body).
An existing simple degradation model (similar to atmospheric image "hazing"
effects), though helpful, is not sufficient to properly represent the
underwater image degradation because there are unaccounted for and
non-measurable factors e.g. scattering of light due to turbidity of water,
reflective characteristics of turbid medium etc. We propose a deep
learning-based architecture to automatically simulate the underwater effects
where only a dehazing-like image formation equation is known to the network,
and the additional degradation due to the other unknown factors if inferred in
a data-driven way. We only use RGB images (because in real-time scenario depth
image is not available) to estimate the depth image. For testing, we have
proposed (due to the lack of real underwater image datasets) a complex image
formation model/equation to manually generate images that resemble real
underwater images (used as ground truth). However, only the classical image
formation equation (the one used for image dehazing) is informed to the
network. This mimics the fact that in a real scenario, the physics are never
completely known and only simplified models are known. Thanks to the ground
truth, generated by a complex image formation equation, we could successfully
perform a qualitative and quantitative evaluation of proposed technique,
compared to other purely data driven approaches
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05282" title="Abstract">arXiv:2402.05282</a> [<a href="/pdf/2402.05282" title="Download PDF">pdf</a>, <a href="/format/2402.05282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TreeForm: End-to-end Annotation and Evaluation for Form Document Parsing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zmigrod%2C+R">Ran Zmigrod</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhiqiang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Nourbakhsh%2C+A">Armineh Nourbakhsh</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+S">Sameena Shah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Visually Rich Form Understanding (VRFU) poses a complex research problem due
to the documents' highly structured nature and yet highly variable style and
content. Current annotation schemes decompose form understanding and omit key
hierarchical structure, making development and evaluation of end-to-end models
difficult. In this paper, we propose a novel F1 metric to evaluate form parsers
and describe a new content-agnostic, tree-based annotation scheme for VRFU:
TreeForm. We provide methods to convert previous annotation schemes into
TreeForm structures and evaluate TreeForm predictions using a modified version
of the normalized tree-edit distance. We present initial baselines for our
end-to-end performance metric and the TreeForm edit distance, averaged over the
FUNSD and XFUND datasets, of 61.5 and 26.4 respectively. We hope that TreeForm
encourages deeper research in annotating, modeling, and evaluating the
complexities of form-like documents.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05284" title="Abstract">arXiv:2402.05284</a> [<a href="/pdf/2402.05284" title="Download PDF">pdf</a>, <a href="/format/2402.05284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Adversarial Inputs in Deep Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Corsi%2C+D">Davide Corsi</a>, 
<a href="/search/cs?searchtype=author&query=Amir%2C+G">Guy Amir</a>, 
<a href="/search/cs?searchtype=author&query=Katz%2C+G">Guy Katz</a>, 
<a href="/search/cs?searchtype=author&query=Farinelli%2C+A">Alessandro Farinelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In recent years, Deep Reinforcement Learning (DRL) has become a popular
paradigm in machine learning due to its successful applications to real-world
and complex systems. However, even the state-of-the-art DRL models have been
shown to suffer from reliability concerns -- for example, their susceptibility
to adversarial inputs, i.e., small and abundant input perturbations that can
fool the models into making unpredictable and potentially dangerous decisions.
This drawback limits the deployment of DRL systems in safety-critical contexts,
where even a small error cannot be tolerated. In this work, we present a
comprehensive analysis of the characterization of adversarial inputs, through
the lens of formal verification. Specifically, we introduce a novel metric, the
Adversarial Rate, to classify models based on their susceptibility to such
perturbations, and present a set of tools and algorithms for its computation.
Our analysis empirically demonstrates how adversarial inputs can affect the
safety of a given DRL system with respect to such perturbations. Moreover, we
analyze the behavior of these configurations to suggest several useful
practices and guidelines to help mitigate the vulnerability of trained DRL
networks.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05290" title="Abstract">arXiv:2402.05290</a> [<a href="/pdf/2402.05290" title="Download PDF">pdf</a>, <a href="/format/2402.05290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Transformer World Models Give Better Policy Gradients?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+M">Michel Ma</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+T">Tianwei Ni</a>, 
<a href="/search/cs?searchtype=author&query=Gehring%2C+C">Clement Gehring</a>, 
<a href="/search/cs?searchtype=author&query=D%27Oro%2C+P">Pierluca D&#x27;Oro</a>, 
<a href="/search/cs?searchtype=author&query=Bacon%2C+P">Pierre-Luc Bacon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Michel Ma and Pierluca D'Oro contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">A natural approach for reinforcement learning is to predict future rewards by
unrolling a neural network world model, and to backpropagate through the
resulting computational graph to learn a policy. However, this method often
becomes impractical for long horizons since typical world models induce
hard-to-optimize loss landscapes. Transformers are known to efficiently
propagate gradients overlong horizons: could they be the solution to this
problem? Surprisingly, we show that commonly-used transformer world models
produce circuitous gradient paths, which can be detrimental to long-range
policy gradients. To tackle this challenge, we propose a class of world models
called Actions World Models (AWMs), designed to provide more direct routes for
gradient propagation. We integrate such AWMs into a policy gradient framework
that underscores the relationship between network architectures and the policy
gradient updates they inherently represent. We demonstrate that AWMs can
generate optimization landscapes that are easier to navigate even when compared
to those from the simulator itself. This property allows transformer AWMs to
produce better policies than competitive baselines in realistic long-horizon
tasks.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05291" title="Abstract">arXiv:2402.05291</a> [<a href="/pdf/2402.05291" title="Download PDF">pdf</a>, <a href="/format/2402.05291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Networks as Fast and High-fidelity Emulators for  Finite-Element Ice Sheet Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahnemoonfar%2C+M">Maryam Rahnemoonfar</a>, 
<a href="/search/cs?searchtype=author&query=Koo%2C+Y">Younghyun Koo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 7 figures, 3 tables, Submitted to Nature Communications on Feb 7, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Although the finite element approach of the Ice-sheet and Sea-level System
Model (ISSM) solves ice dynamics problems governed by Stokes equations quickly
and accurately, such numerical modeling requires intensive computation on
central processing units (CPU). In this study, we develop graph neural networks
(GNN) as fast surrogate models to preserve the finite element structure of
ISSM. Using the 20-year transient simulations in the Pine Island Glacier (PIG),
we train and test three GNNs: graph convolutional network (GCN), graph
attention network (GAT), and equivariant graph convolutional network (EGCN).
These GNNs reproduce ice thickness and velocity with better accuracy than the
classic convolutional neural network (CNN) and multi-layer perception (MLP). In
particular, GNNs successfully capture the ice mass loss and acceleration
induced by higher basal melting rates in the PIG. When our GNN emulators are
implemented on graphic processing units (GPUs), they show up to 50 times faster
computational time than the CPU-based ISSM simulation.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05293" title="Abstract">arXiv:2402.05293</a> [<a href="/pdf/2402.05293" title="Download PDF">pdf</a>, <a href="/ps/2402.05293" title="Download PostScript">ps</a>, <a href="/format/2402.05293" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A comparative study on feature selection for a risk prediction model for  colorectal cancer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cueto-L%C3%B3pez%2C+N">N. Cueto-L&#xf3;pez</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Ord%C3%A1s%2C+M+T">M. T. Garc&#xed;a-Ord&#xe1;s</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%A1vila-Batista%2C+V">V. D&#xe1;vila-Batista</a>, 
<a href="/search/cs?searchtype=author&query=Moreno%2C+V">V. Moreno</a>, 
<a href="/search/cs?searchtype=author&query=Aragon%C3%A9s%2C+N">N. Aragon&#xe9;s</a>, 
<a href="/search/cs?searchtype=author&query=Alaiz-Rodr%C3%ADguez%2C+R">R. Alaiz-Rodr&#xed;guez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Background and objective
<br />Risk prediction models aim at identifying people at higher risk of developing
a target disease. Feature selection is particularly important to improve the
prediction model performance avoiding overfitting and to identify the leading
cancer risk (and protective) factors. Assessing the stability of feature
selection/ranking algorithms becomes an important issue when the aim is to
analyze the features with more prediction power. Methods
<br />This work is focused on colorectal cancer, assessing several feature ranking
algorithms in terms of performance for a set of risk prediction models (Neural
Networks, Support Vector Machines (SVM), Logistic Regression, k-Nearest
Neighbors and Boosted Trees). Additionally, their robustness is evaluated
following a conventional approach with scalar stability metrics and a visual
approach proposed in this work to study both similarity among feature ranking
techniques as well as their individual stability. A comparative analysis is
carried out between the most relevant features found out in this study and
features provided by the experts according to the state-of-the-art knowledge.
Results
<br />The two best performance results in terms of Area Under the ROC Curve (AUC)
are achieved with a SVM classifier using the top-41 features selected by the
SVM wrapper approach (AUC=0.693) and Logistic Regression with the top-40
features selected by the Pearson (AUC=0.689). Experiments showed that
performing feature selection contributes to classification performance with a
3.9% and 1.9% improvement in AUC for the SVM and Logistic Regression
classifier, respectively, with respect to the results using the full feature
set. The visual approach proposed in this work allows to see that the Neural
Network-based wrapper ranking is the most unstable while the Random Forest is
the most stable.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05294" title="Abstract">arXiv:2402.05294</a> [<a href="/pdf/2402.05294" title="Download PDF">pdf</a>, <a href="/format/2402.05294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Examining Modality Incongruity in Multimodal Federated Learning for  Medical Vision and Language-based Disease Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saha%2C+P">Pramit Saha</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+D">Divyanshu Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Wagner%2C+F">Felix Wagner</a>, 
<a href="/search/cs?searchtype=author&query=Kamnitsas%2C+K">Konstantinos Kamnitsas</a>, 
<a href="/search/cs?searchtype=author&query=Noble%2C+J+A">J. Alison Noble</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 42 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Multimodal Federated Learning (MMFL) utilizes multiple modalities in each
client to build a more powerful Federated Learning (FL) model than its unimodal
counterpart. However, the impact of missing modality in different clients, also
called modality incongruity, has been greatly overlooked. This paper, for the
first time, analyses the impact of modality incongruity and reveals its
connection with data heterogeneity across participating clients. We
particularly inspect whether incongruent MMFL with unimodal and multimodal
clients is more beneficial than unimodal FL. Furthermore, we examine three
potential routes of addressing this issue. Firstly, we study the effectiveness
of various self-attention mechanisms towards incongruity-agnostic information
fusion in MMFL. Secondly, we introduce a modality imputation network (MIN)
pre-trained in a multimodal client for modality translation in unimodal clients
and investigate its potential towards mitigating the missing modality problem.
Thirdly, we assess the capability of client-level and server-level
regularization techniques towards mitigating modality incongruity effects.
Experiments are conducted under several MMFL settings on two publicly available
real-world datasets, MIMIC-CXR and Open-I, with Chest X-Ray and radiology
reports.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05295" title="Abstract">arXiv:2402.05295</a> [<a href="/pdf/2402.05295" title="Download PDF">pdf</a>, <a href="/ps/2402.05295" title="Download PostScript">ps</a>, <a href="/format/2402.05295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An information theoretic approach to quantify the stability of feature  selection and ranking algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alaiz-Rodriguez">Alaiz-Rodriguez</a>, R., 
<a href="/search/cs?searchtype=author&query=Parnell">Parnell</a>, 
<a href="/search/cs?searchtype=author&query=C%2C+A">A. C</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Feature selection is a key step when dealing with high dimensional data. In
particular, these techniques simplify the process of knowledge discovery from
the data by selecting the most relevant features out of the noisy, redundant
and irrelevant features. A problem that arises in many of these practical
applications is that the outcome of the feature selection algorithm is not
stable. Thus, small variations in the data may yield very different feature
rankings. Assessing the stability of these methods becomes an important issue
in the previously mentioned situations. We propose an information theoretic
approach based on the Jensen Shannon divergence to quantify this robustness.
Unlike other stability measures, this metric is suitable for different
algorithm outcomes: full ranked lists, feature subsets as well as the lesser
studied partial ranked lists. This generalized metric quantifies the difference
among a whole set of lists with the same size, following a probabilistic
approach and being able to give more importance to the disagreements that
appear at the top of the list. Moreover, it possesses desirable properties
including correction for change, upper lower bounds and conditions for a
deterministic selection. We illustrate the use of this stability metric with
data generated in a fully controlled way and compare it with popular metrics
including the Spearmans rank correlation and the Kunchevas index on feature
ranking and selection outcomes, respectively. Additionally, experimental
validation of the proposed approach is carried out on a real-world problem of
food quality assessment showing its potential to quantify stability from
different perspectives.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05296" title="Abstract">arXiv:2402.05296</a> [<a href="/pdf/2402.05296" title="Download PDF">pdf</a>, <a href="/ps/2402.05296" title="Download PostScript">ps</a>, <a href="/format/2402.05296" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Classifying spam emails using agglomerative hierarchical clustering and  a topic-based approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Janez-Martino%2C+F">F. Janez-Martino</a>, 
<a href="/search/cs?searchtype=author&query=Alaiz-Rodriguez%2C+R">R. Alaiz-Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez-Castro%2C+V">V. Gonzalez-Castro</a>, 
<a href="/search/cs?searchtype=author&query=Fidalgo%2C+E">E. Fidalgo</a>, 
<a href="/search/cs?searchtype=author&query=Alegre%2C+E">E. Alegre</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Spam emails are unsolicited, annoying and sometimes harmful messages which
may contain malware, phishing or hoaxes. Unlike most studies that address the
design of efficient anti-spam filters, we approach the spam email problem from
a different and novel perspective. Focusing on the needs of cybersecurity
units, we follow a topic-based approach for addressing the classification of
spam email into multiple categories. We propose SPEMC-15K-E and SPEMC-15K-S,
two novel datasets with approximately 15K emails each in English and Spanish,
respectively, and we label them using agglomerative hierarchical clustering
into 11 classes. We evaluate 16 pipelines, combining four text representation
techniques -Term Frequency-Inverse Document Frequency (TF-IDF), Bag of Words,
Word2Vec and BERT- and four classifiers: Support Vector Machine, N\"aive Bayes,
Random Forest and Logistic Regression. Experimental results show that the
highest performance is achieved with TF-IDF and LR for the English dataset,
with a F1 score of 0.953 and an accuracy of 94.6%, and while for the Spanish
dataset, TF-IDF with NB yields a F1 score of 0.945 and 98.5% accuracy.
Regarding the processing time, TF-IDF with LR leads to the fastest
classification, processing an English and Spanish spam email in and on average,
respectively.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05300" title="Abstract">arXiv:2402.05300</a> [<a href="/pdf/2402.05300" title="Download PDF">pdf</a>, <a href="/ps/2402.05300" title="Download PostScript">ps</a>, <a href="/format/2402.05300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Player Resource-Sharing Games with Fair Reward Allocation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wijewardena%2C+M">Mevan Wijewardena</a>, 
<a href="/search/cs?searchtype=author&query=Neely%2C+M+J">Michael. J Neely</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper considers a multi-player resource-sharing game with a fair reward
allocation model. Multiple players choose from a collection of resources. Each
resource brings a random reward equally divided among the players who choose
it. We consider two settings. The first setting is a one-slot game where the
mean rewards of the resources are known to all the players, and the objective
of player 1 is to maximize their worst-case expected utility. Certain special
cases of this setting have explicit solutions. These cases provide interesting
yet non-intuitive insights into the problem. The second setting is an online
setting, where the game is played over a finite time horizon, where the mean
rewards are unknown to the first player. Instead, the first player receives, as
feedback, the rewards of the resources they chose after the action. We develop
a novel Upper Confidence Bound (UCB) algorithm that minimizes the worst-case
regret of the first player using the feedback received.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05301" title="Abstract">arXiv:2402.05301</a> [<a href="/pdf/2402.05301" title="Download PDF">pdf</a>, <a href="/format/2402.05301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and  Parametric CAD Designs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Regenwetter%2C+L">Lyle Regenwetter</a>, 
<a href="/search/cs?searchtype=author&query=Obaideh%2C+Y+A">Yazan Abu Obaideh</a>, 
<a href="/search/cs?searchtype=author&query=Nobari%2C+A+H">Amin Heyrani Nobari</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+F">Faez Ahmed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper introduces a public dataset of 1.4 million procedurally-generated
bicycle designs represented parametrically, as JSON files, and as rasterized
images. The dataset is created through the use of a rendering engine which
harnesses the BikeCAD software to generate vector graphics from parametric
designs. This rendering engine is discussed in the paper and also released
publicly alongside the dataset. Though this dataset has numerous applications,
a principal motivation is the need to train cross-modal predictive models
between parametric and image-based design representations. For example, we
demonstrate that a predictive model can be trained to accurately estimate
Contrastive Language-Image Pretraining (CLIP) embeddings from a parametric
representation directly. This allows similarity relations to be established
between parametric bicycle designs and text strings or reference images.
Trained predictive models are also made public. The dataset joins the BIKED
dataset family which includes thousands of mixed-representation human-designed
bicycle models and several datasets quantifying design performance. The code
and dataset can be found at:
https://github.com/Lyleregenwetter/BIKED_multimodal/tree/main
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05302" title="Abstract">arXiv:2402.05302</a> [<a href="/pdf/2402.05302" title="Download PDF">pdf</a>, <a href="/format/2402.05302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training DNN Models over Heterogeneous Clusters with Optimal Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nie%2C+C">Chengyi Nie</a>, 
<a href="/search/cs?searchtype=author&query=Maghakian%2C+J">Jessica Maghakian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenhua Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Adjusting batch sizes and adaptively tuning other hyperparameters can
significantly speed up deep neural network (DNN) training. Despite the ubiquity
of heterogeneous clusters, existing adaptive DNN training techniques solely
consider homogeneous environments. Optimizing distributed DNN training over
heterogeneous clusters is technically challenging, and directly adapting
existing techniques results in low utilization and poor performance. To solve
this problem, we introduce Cannikin -- a novel data-parallel distributed
training system. Cannikin achieves efficient and near-optimal performance by
accurately modeling the optimal system performance and predicting adaptive
batch size training metrics for DNNs in heterogeneous clusters. We implemented
Cannikin in PyTorch and conducted experiments over 16 GPUs in Chameleon.
Empirical results show that Cannikin reduces DNN training in heterogeneous
clusters by up to $52\%$ compared to the state-of-the-art adaptive training
system and up to $85\%$ compared to native PyTorch DistributedDataParallel.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05303" title="Abstract">arXiv:2402.05303</a> [<a href="/pdf/2402.05303" title="Download PDF">pdf</a>, <a href="/format/2402.05303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Control of AC-AC interlinking converters for multi-grids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Watson%2C+J">Jeremy Watson</a>, 
<a href="/search/eess?searchtype=author&query=Lestas%2C+I">Ioannis Lestas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper considers the control of AC-AC inter-linking converters (ILCs) in
a multi-grid network. We overview the control schemes in the literature and
propose a passivity framework for the stabilization of multi-grid networks,
considering both AC grid-following and AC grid-forming behavior for the ILC
connections. We then analyze a range of AC/AC interlinking converter control
methods derived from the literature and propose suitable controllers for this
purpose including both AC grid-forming and grid-following behavior. The
controller we propose is partially grid-forming; in particular, it is based on
a combination of a grid-following and a grid-forming converter to improve the
stability properties of the network. Simulation results and theoretical
analysis confirm that the proposed ILC control designs are appropriate for the
multi-grid network.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05305" title="Abstract">arXiv:2402.05305</a> [<a href="/pdf/2402.05305" title="Download PDF">pdf</a>, <a href="/format/2402.05305" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Distillation for Road Detection based on cross-model  Semi-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Wanli Ma</a>, 
<a href="/search/cs?searchtype=author&query=Karakus%2C+O">Oktay Karakus</a>, 
<a href="/search/cs?searchtype=author&query=Rosin%2C+P+L">Paul L. Rosin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The advancement of knowledge distillation has played a crucial role in
enabling the transfer of knowledge from larger teacher models to smaller and
more efficient student models, and is particularly beneficial for online and
resource-constrained applications. The effectiveness of the student model
heavily relies on the quality of the distilled knowledge received from the
teacher. Given the accessibility of unlabelled remote sensing data,
semi-supervised learning has become a prevalent strategy for enhancing model
performance. However, relying solely on semi-supervised learning with smaller
models may be insufficient due to their limited capacity for feature
extraction. This limitation restricts their ability to exploit training data.
To address this issue, we propose an integrated approach that combines
knowledge distillation and semi-supervised learning methods. This hybrid
approach leverages the robust capabilities of large models to effectively
utilise large unlabelled data whilst subsequently providing the small student
model with rich and informative features for enhancement. The proposed
semi-supervised learning-based knowledge distillation (SSLKD) approach
demonstrates a notable improvement in the performance of the student model, in
the application of road segmentation, surpassing the effectiveness of
traditional semi-supervised learning methods.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05306" title="Abstract">arXiv:2402.05306</a> [<a href="/pdf/2402.05306" title="Download PDF">pdf</a>, <a href="/format/2402.05306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sym-Q: Adaptive Symbolic Regression via Sequential Decision-Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuan Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wenqi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Hao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Kammer%2C+D+S">David S. Kammer</a>, 
<a href="/search/cs?searchtype=author&query=Fink%2C+O">Olga Fink</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Symbolic regression holds great potential for uncovering underlying
mathematical and physical relationships from empirical data. While existing
transformer-based models have recently achieved significant success in this
domain, they face challenges in terms of generalizability and adaptability.
Typically, in cases where the output expressions do not adequately fit
experimental data, the models lack efficient mechanisms to adapt or modify the
expression. This inflexibility hinders their application in real-world
scenarios, particularly in discovering unknown physical or biological
relationships. Inspired by how human experts refine and adapt expressions, we
introduce Symbolic Q-network (Sym-Q), a novel reinforcement learning-based
model that redefines symbolic regression as a sequential decision-making task.
Sym-Q leverages supervised demonstrations and refines expressions based on
reward signals indicating the quality of fitting precision. Its distinctive
ability to manage the complexity of expression trees and perform precise
step-wise updates significantly enhances flexibility and efficiency. Our
results demonstrate that Sym-Q excels not only in recovering underlying
mathematical structures but also uniquely learns to efficiently refine the
output expression based on reward signals, thereby discovering underlying
expressions. Sym-Q paves the way for more intuitive and impactful discoveries
in physical science, marking a substantial advancement in the field of symbolic
regression.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05307" title="Abstract">arXiv:2402.05307</a> [<a href="/pdf/2402.05307" title="Download PDF">pdf</a>, <a href="/format/2402.05307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Three Pathways to Neurosymbolic Reinforcement Learning with  Interpretable Model and Policy Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Graf%2C+P">Peter Graf</a>, 
<a href="/search/cs?searchtype=author&query=Emami%2C+P">Patrick Emami</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Neurosymbolic AI combines the interpretability, parsimony, and explicit
reasoning of classical symbolic approaches with the statistical learning of
data-driven neural approaches. Models and policies that are simultaneously
differentiable and interpretable may be key enablers of this marriage. This
paper demonstrates three pathways to implementing such models and policies in a
real-world reinforcement learning setting. Specifically, we study a broad class
of neural networks that build interpretable semantics directly into their
architecture. We reveal and highlight both the potential and the essential
difficulties of combining logic, simulation, and learning. One lesson is that
learning benefits from continuity and differentiability, but classical logic is
discrete and non-differentiable. The relaxation to real-valued, differentiable
representations presents a trade-off; the more learnable, the less
interpretable. Another lesson is that using logic in the context of a numerical
simulation involves a non-trivial mapping from raw (e.g., real-valued time
series) simulation data to logical predicates. Some open questions this note
exposes include: What are the limits of rule-based controllers, and how
learnable are they? Do the differentiable interpretable approaches discussed
here scale to large, complex, uncertain systems? Can we truly achieve
interpretability? We highlight these and other themes across the three
approaches.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05308" title="Abstract">arXiv:2402.05308</a> [<a href="/pdf/2402.05308" title="Download PDF">pdf</a>, <a href="/format/2402.05308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Corotational modeling and NURBS-based kinematic constraint  implementation in three-dimensional vehicle-track-structure interaction  analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fedorova%2C+M">Maria Fedorova</a>, 
<a href="/search/math?searchtype=author&query=Sivaselvan%2C+M+V">M. V. Sivaselvan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages, 22 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">An algorithm for three-dimensional dynamic vehicle-track-structure
interaction (VTSI) analysis is described in this paper. The algorithm is
described in terms of bridges and high-speed trains, but more generally applies
to multibody systems coupled to deformable structures by time-varying kinematic
constraints. Coupling is accomplished by a kinematic constraint/Lagrange
multiplier approach, resulting in a system of index-3 Differential Algebraic
Equations (DAE). Three main new concepts are developed. (i) A corotational
approach is used to represent the vehicle (train) dynamics. Reference
coordinate frames are fitted to the undeformed geometry of the bridge. While
the displacements of the train can be large, deformations are taken to be small
within these frames, resulting in linear (time-varying) rather than nonlinear
dynamics. (ii) If conventional finite elements are used to discretize the
track, the curvature is discontinuous across elements (and possibly rotation,
too, for curved tracks). This results in spurious numerical oscillations in
computed contact forces and accelerations, quantities of key interest in VTSI.
A NURBS-based discretization is employed for the track to mitigate such
oscillations. (iii) The higher order continuity due to using NURBS allows for
alternative techniques for solving the VTSI system. First, enforcing
constraints at the acceleration level reduces an index-3 DAE to an index-1
system that can be solved without numerical dissipation. Second, a constraint
projection method is proposed to solve an index-3 DAE system without numerical
dissipation by correcting wheel velocities and accelerations. Moreover, the
modularity of the presented algorithm, resulting from a kinematic
constraint/Lagrange multiplier formulation, enables ready integration of this
VTSI approach in existing structural analysis and finite element software.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05309" title="Abstract">arXiv:2402.05309</a> [<a href="/pdf/2402.05309" title="Download PDF">pdf</a>, <a href="/format/2402.05309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Generalization Behaviours of Generative Flow Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Atanackovic%2C+L">Lazar Atanackovic</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+E">Emmanuel Bengio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Generative Flow Networks (GFlowNets, GFNs) are a generative framework for
learning unnormalized probability mass functions over discrete spaces. Since
their inception, GFlowNets have proven to be useful for learning generative
models in applications where the majority of the discrete space is unvisited
during training. This has inspired some to hypothesize that GFlowNets, when
paired with deep neural networks (DNNs), have favourable generalization
properties. In this work, we empirically verify some of the hypothesized
mechanisms of generalization of GFlowNets. In particular, we find that the
functions that GFlowNets learn to approximate have an implicit underlying
structure which facilitate generalization. We also find that GFlowNets are
sensitive to being trained offline and off-policy; however, the reward
implicitly learned by GFlowNets is robust to changes in the training
distribution.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05310" title="Abstract">arXiv:2402.05310</a> [<a href="/pdf/2402.05310" title="Download PDF">pdf</a>, <a href="/format/2402.05310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual-disentangled Deep Multiple Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiawei Yao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Juhua Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by SDM'24. Project page: <a href="https://github.com/Alexander-Yao/DDMC">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multiple clustering has gathered significant attention in recent years due to
its potential to reveal multiple hidden structures of the data from different
perspectives. Most of multiple clustering methods first derive feature
representations by controlling the dissimilarity among them, subsequently
employing traditional clustering methods (e.g., k-means) to achieve the final
multiple clustering outcomes. However, the learned feature representations can
exhibit a weak relevance to the ultimate goal of distinct clustering. Moreover,
these features are often not explicitly learned for the purpose of clustering.
Therefore, in this paper, we propose a novel Dual-Disentangled deep Multiple
Clustering method named DDMC by learning disentangled representations.
Specifically, DDMC is achieved by a variational Expectation-Maximization (EM)
framework. In the E-step, the disentanglement learning module employs
coarse-grained and fine-grained disentangled representations to obtain a more
diverse set of latent factors from the data. In the M-step, the cluster
assignment module utilizes a cluster objective function to augment the
effectiveness of the cluster output. Our extensive experiments demonstrate that
DDMC consistently outperforms state-of-the-art methods across seven commonly
used tasks. Our code is available at https://github.com/Alexander-Yao/DDMC.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05312" title="Abstract">arXiv:2402.05312</a> [<a href="/pdf/2402.05312" title="Download PDF">pdf</a>, <a href="/format/2402.05312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SplitSim: Large-Scale Simulations for Evaluating Network Systems  Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hejing Li</a>, 
<a href="/search/cs?searchtype=author&query=Balasubramanian%2C+P">Praneeth Balasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Meiers%2C+M">Marvin Meiers</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jialin Li</a>, 
<a href="/search/cs?searchtype=author&query=Kaufmann%2C+A">Antoine Kaufmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, under submission to peer-reviewed conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">When physical testbeds are out of reach for evaluating a networked system, we
frequently turn to simulation. In today's datacenter networks, bottlenecks are
rarely at the network protocol level, but instead in end-host software or
hardware components, thus current protocol-level simulations are inadequate
means of evaluation. End-to-end simulations covering these components on the
other hand, simply cannot achieve the required scale with feasible simulation
performance and computational resources.
<br />In this paper, we address this with SplitSim, a simulation framework for
end-to-end evaluation for large-scale network and distributed systems. To this
end, SplitSim builds on prior work on modular end-to-end simulations and
combines this with key elements to achieve scalability. First, mixed fidelity
simulations judiciously reduce detail in simulation of parts of the system
where this can be tolerated, while retaining the necessary detail elsewhere.
SplitSim then parallelizes bottleneck simulators by decomposing them into
multiple parallel but synchronized processes. Next, SplitSim provides a
profiler to help users understand simulation performance and where the
bottlenecks are, so users can adjust the configuration. Finally SplitSim
provides abstractions to make it easy for users to build complex large-scale
simulations. Our evaluation demonstrates SplitSim in multiple large-scale case
studies.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05314" title="Abstract">arXiv:2402.05314</a> [<a href="/pdf/2402.05314" title="Download PDF">pdf</a>, <a href="/ps/2402.05314" title="Download PostScript">ps</a>, <a href="/format/2402.05314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Barriers to device longevity and reuse: A vintage device empirical study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goodwin%2C+C">Craig Goodwin</a>, 
<a href="/search/cs?searchtype=author&query=Woolley%2C+S">Sandra Woolley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in the Journal of Systems and Software - Elsevier
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">This extended paper contributes a methodology and a detailed analysis of app
installation and functionality on a 'vintage' device. Experimental results are
presented that demonstrate barriers to the reuse of vintage Apple devices. and
solutions are posited. 230 apps across 23 unique app categories were tested to
determine if they could be downloaded, installed, and opened, and whether they
appeared functional on a vintage Apple device. Only 29 (12.6%) of the apps
could be downloaded directly, and in contrast 140 (60.9%) of the apps were
downloadable with the aid of another Apple device. In total, 141 (61.3%) of
applications downloaded either directly or indirectly were considered
functional and capable of running on the device. We discuss measures Apple and
developers could take to support legacy device users, prolong device use,
enable reuse and, potentially, prevent functional devices from becoming
e-waste.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05317" title="Abstract">arXiv:2402.05317</a> [<a href="/pdf/2402.05317" title="Download PDF">pdf</a>, <a href="/ps/2402.05317" title="Download PostScript">ps</a>, <a href="/format/2402.05317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emerging Results on Automated Support for Searching and Selecting  Evidence for Systematic Literature Review Updates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Napole%C3%A3o%2C+B+M">Bianca Minetto Napole&#xe3;o</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+R">Ritika Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Hall%C3%A9%2C+S">Sylvain Hall&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Petrillo%2C+F">Fabio Petrillo</a>, 
<a href="/search/cs?searchtype=author&query=Kalinowski%2C+M">Marcos Kalinowski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Context: The constant growth of primary evidence and Systematic Literature
Reviews (SLRs) publications in the Software Engineering (SE) field leads to the
need for SLR Updates. However, searching and selecting evidence for SLR updates
demands significant effort from SE researchers. Objective: We present emerging
results on an automated approach to support searching and selecting studies for
SLR updates in SE. Method: We developed an automated tool prototype to perform
the snowballing search technique and support selecting relevant studies for SLR
updates using Machine Learning (ML) algorithms. We evaluated our automation
proposition through a small-scale evaluation with a reliable dataset from an
SLR replication and its update. Results: Effectively automating
snowballing-based search strategies showed feasibility with minor losses,
specifically related to papers without Digital Object Identifier (DOI). The ML
algorithm giving the highest performance to select studies for SLR updates was
Linear Support Vector Machine, with approximately 74% recall and 15% precision.
Using such algorithms with conservative thresholds to minimize the risk of
missing papers can significantly reduce evidence selection efforts. Conclusion:
The preliminary results of our evaluation point in promising directions,
indicating the potential of automating snowballing search efforts and of
reducing the number of papers to be manually analyzed by about 2.5 times when
selecting evidence for updating SLRs in SE.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05318" title="Abstract">arXiv:2402.05318</a> [<a href="/pdf/2402.05318" title="Download PDF">pdf</a>, <a href="/format/2402.05318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Navigating the Knowledge Sea: Planet-scale answer retrieval using LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+D">Dipankar Sarkar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Information retrieval is a rapidly evolving field of information retrieval,
which is characterized by a continuous refinement of techniques and
technologies, from basic hyperlink-based navigation to sophisticated
algorithm-driven search engines. This paper aims to provide a comprehensive
overview of the evolution of Information Retrieval Technology, with a
particular focus on the role of Large Language Models (LLMs) in bridging the
gap between traditional search methods and the emerging paradigm of answer
retrieval. The integration of LLMs in the realms of response retrieval and
indexing signifies a paradigm shift in how users interact with information
systems. This paradigm shift is driven by the integration of large language
models (LLMs) like GPT-4, which are capable of understanding and generating
human-like text, thus enabling them to provide more direct and contextually
relevant answers to user queries. Through this exploration, we seek to
illuminate the technological milestones that have shaped this journey and the
potential future directions in this rapidly changing field.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05319" title="Abstract">arXiv:2402.05319</a> [<a href="/pdf/2402.05319" title="Download PDF">pdf</a>, <a href="/format/2402.05319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal energy-aware task scheduling for batteryless IoT devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Delgado%2C+C">Carmen Delgado</a>, 
<a href="/search/cs?searchtype=author&query=Famaey%2C+J">Jeroen Famaey</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Emerging Topics in Computing, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Today's IoT devices rely on batteries, which offer stable energy storage but
contain harmful chemicals. Having billions of IoT devices powered by batteries
is not sustainable for the future. As an alternative, batteryless devices run
on long-lived capacitors charged using energy harvesters. The small energy
storage capacity of capacitors results in intermittent on-off behaviour.
Traditional computing schedulers can not handle this intermittency, and in this
paper we propose a first step towards an energy-aware task scheduler for
constrained batteryless devices. We present a new energy-aware task scheduling
algorithm that is able to optimally schedule application tasks to avoid power
failures, and that will allow us to provide insights on the optimal look-ahead
time for energy prediction. Our insights can be used as a basis for practical
energy-aware scheduling and energy availability prediction algorithms. We
formulate the scheduling problem as a Mixed Integer Linear Program. We evaluate
its performance improvement when comparing it with state-of-the-art schedulers
for batteryless IoT devices. Our results show that making the task scheduler
energy aware avoids power failures and allows more tasks to successfully
execute. Moreover, we conclude that a relatively short look-ahead energy
prediction time of 8 future task executions is enough to achieve optimality.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05322" title="Abstract">arXiv:2402.05322</a> [<a href="/pdf/2402.05322" title="Download PDF">pdf</a>, <a href="/format/2402.05322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning on Multimodal Graphs: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+C">Ciyuan Peng</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jiayuan He</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+F">Feng Xia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Multimodal data pervades various domains, including healthcare, social media,
and transportation, where multimodal graphs play a pivotal role. Machine
learning on multimodal graphs, referred to as multimodal graph learning (MGL),
is essential for successful artificial intelligence (AI) applications. The
burgeoning research in this field encompasses diverse graph data types and
modalities, learning techniques, and application scenarios. This survey paper
conducts a comparative analysis of existing works in multimodal graph learning,
elucidating how multimodal learning is achieved across different graph types
and exploring the characteristics of prevalent learning techniques.
Additionally, we delineate significant applications of multimodal graph
learning and offer insights into future directions in this domain.
Consequently, this paper serves as a foundational resource for researchers
seeking to comprehend existing MGL techniques and their applicability across
diverse scenarios.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05328" title="Abstract">arXiv:2402.05328</a> [<a href="/pdf/2402.05328" title="Download PDF">pdf</a>, <a href="/ps/2402.05328" title="Download PostScript">ps</a>, <a href="/format/2402.05328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two Simple Proofs of M&#xfc;ller&#x27;s Theorem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Epstein%2C+S">Samuel Epstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">Due to M\"{u}ller's theorem, the Kolmogorov complexity of a string was shown
to be equal to its quantum Kolmogorov complexity. Thus there are no benefits to
using quantum mechanics to compress classical information. The quantitative
amount of information in classical sources is invariant to the physical model
used. These consequences make this theorem arguably the most important result
in the intersection of algorithmic information theory and physics. The original
proof is quite extensive. This paper contains two simple proofs of this
theorem.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05332" title="Abstract">arXiv:2402.05332</a> [<a href="/pdf/2402.05332" title="Download PDF">pdf</a>, <a href="/format/2402.05332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain-Agnostic Hardware Fingerprinting-Based Device Identifier for  Zero-Trust IoT Security
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elmaghbub%2C+A">Abdurrahman Elmaghbub</a>, 
<a href="/search/cs?searchtype=author&query=Hamdaoui%2C+B">Bechir Hamdaoui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper to be published in IEEE Wireless Communications Magazine 2024. arXiv admin note: substantial text overlap with <a href="/abs/2308.04467">arXiv:2308.04467</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI); Signal Processing (eess.SP)

</div>
<p class="mathjax">Next-generation networks aim for comprehensive connectivity, interconnecting
humans, machines, devices, and systems seamlessly. This interconnectivity
raises concerns about privacy and security, given the potential network-wide
impact of a single compromise. To address this challenge, the Zero Trust (ZT)
paradigm emerges as a key method for safeguarding network integrity and data
confidentiality. This work introduces EPS-CNN, a novel deep-learning-based
wireless device identification framework designed to serve as the device
authentication layer within the ZT architecture, with a focus on
resource-constrained IoT devices. At the core of EPS-CNN, a Convolutional
Neural Network (CNN) is utilized to generate the device identity from a unique
RF signal representation, known as the Double-Sided Envelope Power Spectrum
(EPS), which effectively captures the device-specific hardware characteristics
while ignoring device-unrelated information. Experimental evaluations show that
the proposed framework achieves over 99%, 93%, and 95% testing accuracy when
tested in same-domain (day, location, and channel), cross-day, and
cross-location scenarios, respectively. Our findings demonstrate the
superiority of the proposed framework in enhancing the accuracy, robustness,
and adaptability of deep learning-based methods, thus offering a pioneering
solution for enabling ZT IoT device identification.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05333" title="Abstract">arXiv:2402.05333</a> [<a href="/pdf/2402.05333" title="Download PDF">pdf</a>, <a href="/ps/2402.05333" title="Download PostScript">ps</a>, <a href="/format/2402.05333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ML-Enabled Systems Model Deployment and Monitoring: Status Quo and  Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zimelewicz%2C+E">Eduardo Zimelewicz</a>, 
<a href="/search/cs?searchtype=author&query=Kalinowski%2C+M">Marcos Kalinowski</a>, 
<a href="/search/cs?searchtype=author&query=Mendez%2C+D">Daniel Mendez</a>, 
<a href="/search/cs?searchtype=author&query=Giray%2C+G">G&#xf6;rkem Giray</a>, 
<a href="/search/cs?searchtype=author&query=Alves%2C+A+P+S">Antonio Pedro Santos Alves</a>, 
<a href="/search/cs?searchtype=author&query=Lavesson%2C+N">Niklas Lavesson</a>, 
<a href="/search/cs?searchtype=author&query=Azevedo%2C+K">Kelly Azevedo</a>, 
<a href="/search/cs?searchtype=author&query=Villamizar%2C+H">Hugo Villamizar</a>, 
<a href="/search/cs?searchtype=author&query=Escovedo%2C+T">Tatiana Escovedo</a>, 
<a href="/search/cs?searchtype=author&query=Lopes%2C+H">Helio Lopes</a>, 
<a href="/search/cs?searchtype=author&query=Biffl%2C+S">Stefan Biffl</a>, 
<a href="/search/cs?searchtype=author&query=Musil%2C+J">Juergen Musil</a>, 
<a href="/search/cs?searchtype=author&query=Felderer%2C+M">Michael Felderer</a>, 
<a href="/search/cs?searchtype=author&query=Wagner%2C+S">Stefan Wagner</a>, 
<a href="/search/cs?searchtype=author&query=Baldassarre%2C+T">Teresa Baldassarre</a>, 
<a href="/search/cs?searchtype=author&query=Gorschek%2C+T">Tony Gorschek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2310.06726">arXiv:2310.06726</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">[Context] Systems incorporating Machine Learning (ML) models, often called
ML-enabled systems, have become commonplace. However, empirical evidence on how
ML-enabled systems are engineered in practice is still limited, especially for
activities surrounding ML model dissemination. [Goal] We investigate
contemporary industrial practices and problems related to ML model
dissemination, focusing on the model deployment and the monitoring of ML life
cycle phases. [Method] We conducted an international survey to gather
practitioner insights on how ML-enabled systems are engineered. We gathered a
total of 188 complete responses from 25 countries. We analyze the status quo
and problems reported for the model deployment and monitoring phases. We
analyzed contemporary practices using bootstrapping with confidence intervals
and conducted qualitative analyses on the reported problems applying open and
axial coding procedures. [Results] Practitioners perceive the model deployment
and monitoring phases as relevant and difficult. With respect to model
deployment, models are typically deployed as separate services, with limited
adoption of MLOps principles. Reported problems include difficulties in
designing the architecture of the infrastructure for production deployment and
legacy application integration. Concerning model monitoring, many models in
production are not monitored. The main monitored aspects are inputs, outputs,
and decisions. Reported problems involve the absence of monitoring practices,
the need to create custom monitoring tools, and the selection of suitable
metrics. [Conclusion] Our results help provide a better understanding of the
adopted practices and problems in practice and support guiding ML deployment
and monitoring research in a problem-driven manner.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05334" title="Abstract">arXiv:2402.05334</a> [<a href="/pdf/2402.05334" title="Download PDF">pdf</a>, <a href="/ps/2402.05334" title="Download PostScript">ps</a>, <a href="/format/2402.05334" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Interaction between Software Engineers and Data Scientists when  building Machine Learning-Enabled Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Busquim%2C+G">Gabriel Busquim</a>, 
<a href="/search/cs?searchtype=author&query=Villamizar%2C+H">Hugo Villamizar</a>, 
<a href="/search/cs?searchtype=author&query=Lima%2C+M+J">Maria Julia Lima</a>, 
<a href="/search/cs?searchtype=author&query=Kalinowski%2C+M">Marcos Kalinowski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">In recent years, Machine Learning (ML) components have been increasingly
integrated into the core systems of organizations. Engineering such systems
presents various challenges from both a theoretical and practical perspective.
One of the key challenges is the effective interaction between actors with
different backgrounds who need to work closely together, such as software
engineers and data scientists. This paper presents an exploratory case study to
understand the current interaction and collaboration dynamics between these
roles in ML projects. We conducted semi-structured interviews with four
practitioners with experience in software engineering and data science of a
large ML-enabled system project and analyzed the data using reflexive thematic
analysis. Our findings reveal several challenges that can hinder collaboration
between software engineers and data scientists, including differences in
technical expertise, unclear definitions of each role's duties, and the lack of
documents that support the specification of the ML-enabled system. We also
indicate potential solutions to address these challenges, such as fostering a
collaborative culture, encouraging team communication, and producing concise
system documentation. This study contributes to understanding the complex
dynamics between software engineers and data scientists in ML projects and
provides insights for improving collaboration and communication in this
context. We encourage future studies investigating this interaction in other
projects.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05337" title="Abstract">arXiv:2402.05337</a> [<a href="/pdf/2402.05337" title="Download PDF">pdf</a>, <a href="/ps/2402.05337" title="Download PostScript">ps</a>, <a href="/format/2402.05337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating the Impact of SOLID Design Principles on Machine Learning  Code Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cabral%2C+R">Raphael Cabral</a>, 
<a href="/search/cs?searchtype=author&query=Kalinowski%2C+M">Marcos Kalinowski</a>, 
<a href="/search/cs?searchtype=author&query=Baldassarre%2C+M+T">Maria Teresa Baldassarre</a>, 
<a href="/search/cs?searchtype=author&query=Villamizar%2C+H">Hugo Villamizar</a>, 
<a href="/search/cs?searchtype=author&query=Escovedo%2C+T">Tatiana Escovedo</a>, 
<a href="/search/cs?searchtype=author&query=Lopes%2C+H">H&#xe9;lio Lopes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">[Context] Applying design principles has long been acknowledged as beneficial
for understanding and maintainability in traditional software projects. These
benefits may similarly hold for Machine Learning (ML) projects, which involve
iterative experimentation with data, models, and algorithms. However, ML
components are often developed by data scientists with diverse educational
backgrounds, potentially resulting in code that doesn't adhere to software
design best practices. [Goal] In order to better understand this phenomenon, we
investigated the impact of the SOLID design principles on ML code
understanding. [Method] We conducted a controlled experiment with three
independent trials involving 100 data scientists. We restructured real
industrial ML code that did not use SOLID principles. Within each trial, one
group was presented with the original ML code, while the other was presented
with ML code incorporating SOLID principles. Participants of both groups were
asked to analyze the code and fill out a questionnaire that included both
open-ended and closed-ended questions on their understanding. [Results] The
study results provide statistically significant evidence that the adoption of
the SOLID design principles can improve code understanding within the realm of
ML projects. [Conclusion] We put forward that software engineering design
principles should be spread within the data science community and considered
for enhancing the maintainability of ML code.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05339" title="Abstract">arXiv:2402.05339</a> [<a href="/pdf/2402.05339" title="Download PDF">pdf</a>, <a href="/ps/2402.05339" title="Download PostScript">ps</a>, <a href="/format/2402.05339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can participation in a hackathon impact the motivation of software  engineering students? A preliminary case study analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ara%C3%BAjo%2C+A+A">Allysson Allex Ara&#xfa;jo</a>, 
<a href="/search/cs?searchtype=author&query=Kalinowski%2C+M">Marcos Kalinowski</a>, 
<a href="/search/cs?searchtype=author&query=Baldassarre%2C+M+T">Maria Teresa Baldassarre</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">[Background] Hackathons are increasingly gaining prominence in Software
Engineering (SE) education, lauded for their ability to elevate students' skill
sets. [Objective] This paper investigates whether hackathons can impact the
motivation of SE students. [Method] We conducted an evaluative case study
assessing students' motivations before and after a hackathon, combining
quantitative analysis using the Academic Motivation Scale (AMS) and qualitative
coding of open-ended responses. [Results] Pre-hackathon findings reveal a
diverse range of motivations with an overall acceptance, while post-hackathon
responses highlight no statistically significant shift in participants'
perceptions. Qualitative findings uncovered themes related to networking, team
dynamics, and skill development. From a practical perspective, our findings
highlight the potential of hackathons to impact participants' motivation.
[Conclusion] While our study enhances the comprehension of hackathons as a
motivational tool, it also underscores the need for further exploration of
psychometric dimensions in SE educational research.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05340" title="Abstract">arXiv:2402.05340</a> [<a href="/pdf/2402.05340" title="Download PDF">pdf</a>, <a href="/ps/2402.05340" title="Download PostScript">ps</a>, <a href="/format/2402.05340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> POLARIS: A framework to guide the development of Trustworthy AI systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baldassarre%2C+M+T">Maria Teresa Baldassarre</a>, 
<a href="/search/cs?searchtype=author&query=Gigante%2C+D">Domenico Gigante</a>, 
<a href="/search/cs?searchtype=author&query=Kalinowski%2C+M">Marcos Kalinowski</a>, 
<a href="/search/cs?searchtype=author&query=Ragone%2C+A">Azzurra Ragone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">In the ever-expanding landscape of Artificial Intelligence (AI), where
innovation thrives and new products and services are continuously being
delivered, ensuring that AI systems are designed and developed responsibly
throughout their entire lifecycle is crucial. To this end, several AI ethics
principles and guidelines have been issued to which AI systems should conform.
Nevertheless, relying solely on high-level AI ethics principles is far from
sufficient to ensure the responsible engineering of AI systems. In this field,
AI professionals often navigate by sight. Indeed, while recommendations
promoting Trustworthy AI (TAI) exist, these are often high-level statements
that are difficult to translate into concrete implementation strategies. There
is a significant gap between high-level AI ethics principles and low-level
concrete practices for AI professionals. To address this challenge, our work
presents an experience report where we develop a novel holistic framework for
Trustworthy AI - designed to bridge the gap between theory and practice - and
report insights from its application in an industrial case study. The framework
is built on the result of a systematic review of the state of the practice, a
survey, and think-aloud interviews with 34 AI practitioners. The framework,
unlike most of those already in the literature, is designed to provide
actionable guidelines and tools to support different types of stakeholders
throughout the entire Software Development Life Cycle (SDLC). Our goal is to
empower AI professionals to confidently navigate the ethical dimensions of TAI
through practical insights, ensuring that the vast potential of AI is exploited
responsibly for the benefit of society as a whole.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05346" title="Abstract">arXiv:2402.05346</a> [<a href="/pdf/2402.05346" title="Download PDF">pdf</a>, <a href="/format/2402.05346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KIX: A Metacognitive Generalization Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Arun Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Schrater%2C+P">Paul Schrater</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Humans and other animals aptly exhibit general intelligence behaviors in
solving a variety of tasks with flexibility and ability to adapt to novel
situations by reusing and applying high level knowledge acquired over time. But
artificial agents are more of a specialist, lacking such generalist behaviors.
Artificial agents will require understanding and exploiting critical structured
knowledge representations. We present a metacognitive generalization framework,
Knowledge-Interaction-eXecution (KIX), and argue that interactions with objects
leveraging type space facilitate the learning of transferable interaction
concepts and generalization. It is a natural way of integrating knowledge into
reinforcement learning and promising to act as an enabler for autonomous and
generalist behaviors in artificial intelligence systems.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05347" title="Abstract">arXiv:2402.05347</a> [<a href="/pdf/2402.05347" title="Download PDF">pdf</a>, <a href="/ps/2402.05347" title="Download PostScript">ps</a>, <a href="/format/2402.05347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Implicit Adaptive Low Rank Time-Stepping Methods for Matrix  Differential Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Appel%C3%B6%2C+D">Daniel Appel&#xf6;</a>, 
<a href="/search/math?searchtype=author&query=Cheng%2C+Y">Yingda Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this work, we develop implicit rank-adaptive schemes for time-dependent
matrix differential equations. The dynamic low rank approximation (DLRA) is a
well-known technique to capture the dynamic low rank structure based on
Dirac-Frenkel time-dependent variational principle. In recent years, it has
attracted a lot of attention due to its wide applicability. Our schemes are
inspired by the three-step procedure used in the rank adaptive version of the
unconventional robust integrator (the so called BUG integrator) for DLRA.
First, a prediction (basis update) step is made computing the approximate
column and row spaces at the next time level. Second, a Galerkin evolution step
is invoked using a base implicit solve for the small core matrix. Finally, a
truncation is made according to a prescribed error threshold. Since the DLRA is
evolving the differential equation projected on to the tangent space of the low
rank manifold, the error estimate of the BUG integrator contains the tangent
projection (modeling) error which cannot be easily controlled by mesh
refinement. This can cause convergence issue for equations with cross terms.
<br />To address this issue, we propose a simple modification, consisting of
merging the row and column spaces from the explicit step truncation method
together with the BUG spaces in the prediction step. In addition, we propose an
adaptive strategy where the BUG spaces are only computed if the residual for
the solution obtained from the prediction space by explicit step truncation
method, is too large. We prove stability and estimate the local truncation
error of the schemes under assumptions. We benchmark the schemes in several
tests, such as anisotropic diffusion, solid body rotation and the combination
of the two, to show robust convergence properties.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05348" title="Abstract">arXiv:2402.05348</a> [<a href="/pdf/2402.05348" title="Download PDF">pdf</a>, <a href="/format/2402.05348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are We Asking the Right Questions?: Designing for Community  Stakeholders&#x27; Interactions with AI in Policing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haque%2C+M+R">MD Romael Haque</a>, 
<a href="/search/cs?searchtype=author&query=Saxena%2C+D">Devansh Saxena</a>, 
<a href="/search/cs?searchtype=author&query=Weathington%2C+K">Katy Weathington</a>, 
<a href="/search/cs?searchtype=author&query=Chudzik%2C+J">Joseph Chudzik</a>, 
<a href="/search/cs?searchtype=author&query=Guha%2C+S">Shion Guha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Research into recidivism risk prediction in the criminal legal system has
garnered significant attention from HCI, critical algorithm studies, and the
emerging field of human-AI decision-making. This study focuses on algorithmic
crime mapping, a prevalent yet underexplored form of algorithmic decision
support (ADS) in this context. We conducted experiments and follow-up
interviews with 60 participants, including community members, technical
experts, and law enforcement agents (LEAs), to explore how lived experiences,
technical knowledge, and domain expertise shape interactions with the ADS,
impacting human-AI decision-making. Surprisingly, we found that domain experts
(LEAs) often exhibited anchoring bias, readily accepting and engaging with the
first crime map presented to them. Conversely, community members and technical
experts were more inclined to engage with the tool, adjust controls, and
generate different maps. Our findings highlight that all three stakeholders
were able to provide critical feedback regarding AI design and use - community
members questioned the core motivation of the tool, technical experts drew
attention to the elastic nature of data science practice, and LEAs suggested
redesign pathways such that the tool could complement their domain expertise.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05349" title="Abstract">arXiv:2402.05349</a> [<a href="/pdf/2402.05349" title="Download PDF">pdf</a>, <a href="/format/2402.05349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scrapping The Web For Early Wildfire Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lostanlen%2C+M">Mateo Lostanlen</a>, 
<a href="/search/cs?searchtype=author&query=Veith%2C+F">Felix Veith</a>, 
<a href="/search/cs?searchtype=author&query=Buc%2C+C">Cristian Buc</a>, 
<a href="/search/cs?searchtype=author&query=Barriere%2C+V">Valentin Barriere</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint of ongoing work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Early wildfire detection is of the utmost importance to enable rapid response
efforts, and thus minimize the negative impacts of wildfire spreads. To this
end, we present \Pyro, a web-scraping-based dataset composed of videos of
wildfires from a network of cameras that were enhanced with manual
bounding-box-level annotations. Our dataset was filtered based on a strategy to
improve the quality and diversity of the data, reducing the final data to a set
of 10,000 images. We ran experiments using a state-of-the-art object detection
model and found out that the proposed dataset is challenging and its use in
concordance with other public dataset helps to reach higher results overall. We
will make our code and data publicly available.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05350" title="Abstract">arXiv:2402.05350</a> [<a href="/pdf/2402.05350" title="Download PDF">pdf</a>, <a href="/format/2402.05350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Descanning: From Scanned to the Original Images with a Color Correction  Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cha%2C+J">Junghun Cha</a>, 
<a href="/search/cs?searchtype=author&query=Haider%2C+A">Ali Haider</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Seoyun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+H">Hoeyeong Jin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Subin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Uddin%2C+A+F+M+S">A. F. M. Shahab Uddin</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jaehyoung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S+Y">Soo Ye Kim</a>, 
<a href="/search/cs?searchtype=author&query=Bae%2C+S">Sung-Ho Bae</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">A significant volume of analog information, i.e., documents and images, have
been digitized in the form of scanned copies for storing, sharing, and/or
analyzing in the digital world. However, the quality of such contents is
severely degraded by various distortions caused by printing, storing, and
scanning processes in the physical world. Although restoring high-quality
content from scanned copies has become an indispensable task for many products,
it has not been systematically explored, and to the best of our knowledge, no
public datasets are available. In this paper, we define this problem as
Descanning and introduce a new high-quality and large-scale dataset named
DESCAN-18K. It contains 18K pairs of original and scanned images collected in
the wild containing multiple complex degradations. In order to eliminate such
complex degradations, we propose a new image restoration model called
DescanDiffusion consisting of a color encoder that corrects the global color
degradation and a conditional denoising diffusion probabilistic model (DDPM)
that removes local degradations. To further improve the generalization ability
of DescanDiffusion, we also design a synthetic data generation scheme by
reproducing prominent degradations in scanned images. We demonstrate that our
DescanDiffusion outperforms other baselines including commercial restoration
products, objectively and subjectively, via comprehensive experiments and
analyses.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05353" title="Abstract">arXiv:2402.05353</a> [<a href="/pdf/2402.05353" title="Download PDF">pdf</a>, <a href="/format/2402.05353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Early-Learning Regularization When Federated Learning Meets  Noisy Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taehyeon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Donggyu Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Se-Young Yun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">In the evolving landscape of federated learning (FL), addressing label noise
presents unique challenges due to the decentralized and diverse nature of data
collection across clients. Traditional centralized learning approaches to
mitigate label noise are constrained in FL by privacy concerns and the
heterogeneity of client data. This paper revisits early-learning
regularization, introducing an innovative strategy, Federated Label-mixture
Regularization (FLR). FLR adeptly adapts to FL's complexities by generating new
pseudo labels, blending local and global model predictions. This method not
only enhances the accuracy of the global model in both i.i.d. and non-i.i.d.
settings but also effectively counters the memorization of noisy labels.
Demonstrating compatibility with existing label noise and FL techniques, FLR
paves the way for improved generalization in FL environments fraught with label
inaccuracies.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05355" title="Abstract">arXiv:2402.05355</a> [<a href="/pdf/2402.05355" title="Download PDF">pdf</a>, <a href="/format/2402.05355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Safe Multi-Modal Learning System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tianyi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liangliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Lu Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the wide deployment of multimodal learning systems (MMLS) in real-world
scenarios, safety concerns have become increasingly prominent. The absence of
systematic research into their safety is a significant barrier to progress in
this field. To bridge the gap, we present the first taxonomy for MMLS safety,
identifying four essential pillars of these concerns. Leveraging this taxonomy,
we conduct in-depth reviews for each pillar, highlighting key limitations based
on the current state of development. Finally, we pinpoint unique challenges in
MMLS safety and provide potential directions for future research.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05356" title="Abstract">arXiv:2402.05356</a> [<a href="/pdf/2402.05356" title="Download PDF">pdf</a>, <a href="/format/2402.05356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Learning Complexity for Downstream Data Pruning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wenyu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenlong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zejian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Songxin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jing%2C+B">Bingyi Jing</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hongxin Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The over-parameterized pre-trained models pose a great challenge to
fine-tuning with limited computation resources. An intuitive solution is to
prune the less informative samples from the fine-tuning dataset. A series of
training-based scoring functions are proposed to quantify the informativeness
of the data subset but the pruning cost becomes non-negligible due to the heavy
parameter updating. For efficient pruning, it is viable to adapt the similarity
scoring function of geometric-based methods from training-based to
training-free. However, we empirically show that such adaption distorts the
original pruning and results in inferior performance on the downstream tasks.
In this paper, we propose to treat the learning complexity (LC) as the scoring
function for classification and regression tasks. Specifically, the learning
complexity is defined as the average predicted confidence of subnets with
different capacities, which encapsulates data processing within a converged
model. Then we preserve the diverse and easy samples for fine-tuning. Extensive
experiments with vision datasets demonstrate the effectiveness and efficiency
of the proposed scoring function for classification tasks. For the instruction
fine-tuning of large language models, our method achieves state-of-the-art
performance with stable convergence, outperforming the full training with only
10\% of the instruction dataset.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05357" title="Abstract">arXiv:2402.05357</a> [<a href="/pdf/2402.05357" title="Download PDF">pdf</a>, <a href="/format/2402.05357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Geometric Connectivity in the Plane with Constant Query Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chan%2C+T+M">Timothy M. Chan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhengcheng Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">We present the first fully dynamic connectivity data structures for geometric
intersection graphs achieving constant query time and sublinear amortized
update time for most types of geometric objects in 2D. Our data structures can
answer connectivity queries between two objects, as well as "global"
connectivity queries (e.g., deciding whether the entire graph is connected).
Previously, the data structure by Afshani and Chan (ESA'06) achieved such
bounds only in the special case of axis-aligned line segments or rectangles but
did not work for arbitrary line segments or disks, whereas the data structures
by Chan, P\u{a}tra\c{s}cu and Roditty (FOCS'08) worked for more general classes
of geometric objects but required $n^{\Omega(1)}$ query time and could not
handle global connectivity queries.
<br />Specifically, we obtain new data structures with $O(1)$ query time and
amortized update time near $n^{4/5}$, $n^{7/8}$, and $n^{20/21}$ for
axis-aligned line segments, disks, and arbitrary line segments respectively.
Besides greatly reducing the query time, our data structures also improve the
previous update times for axis-aligned line segments by Afshani and Chan (from
near $n^{10/11}$ to $n^{4/5}$) and for disks by Chan, P\u{a}tra\c{s}cu, and
Roditty (from near $n^{20/21}$ to $n^{7/8}$).
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05359" title="Abstract">arXiv:2402.05359</a> [<a href="/pdf/2402.05359" title="Download PDF">pdf</a>, <a href="/format/2402.05359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guiding Large Language Models with Divide-and-Conquer Program for  Discerning Problem Solving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yizhou Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+L">Lun Du</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+D">Defu Cao</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Q">Qiang Fu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yan Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Foundation models, such as Large language Models (LLMs), have attracted
significant amount of interest due to their large number of applications.
Existing works show that appropriate prompt design, such as Chain-of-Thoughts,
can unlock LLM's powerful capacity in diverse areas. However, when handling
tasks involving repetitive sub-tasks and/or deceptive contents, such as
arithmetic calculation and article-level fake news detection, existing
prompting strategies either suffers from insufficient expressive power or
intermediate errors triggered by hallucination. To make LLM more discerning to
such intermediate errors, we propose to guide LLM with a Divide-and-Conquer
program that simultaneously ensures superior expressive power and disentangles
task decomposition, sub-task resolution, and resolution assembly process.
Theoretic analysis reveals that our strategy can guide LLM to extend the
expressive power of fixed-depth Transformer. Experiments indicate that our
proposed method can achieve better performance than typical prompting
strategies in tasks bothered by intermediate errors and deceptive contents,
such as large integer multiplication, hallucination detection and
misinformation detection.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05367" title="Abstract">arXiv:2402.05367</a> [<a href="/pdf/2402.05367" title="Download PDF">pdf</a>, <a href="/ps/2402.05367" title="Download PostScript">ps</a>, <a href="/format/2402.05367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Principled Preferential Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wenjie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenbin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuning Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Svetozarevic%2C+B">Bratislav Svetozarevic</a>, 
<a href="/search/cs?searchtype=author&query=Jones%2C+C+N">Colin N. Jones</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We study the problem of preferential Bayesian optimization (BO), where we aim
to optimize a black-box function with only preference feedback over a pair of
candidate solutions. Inspired by the likelihood ratio idea, we construct a
confidence set of the black-box function using only the preference feedback. An
optimistic algorithm with an efficient computational method is then developed
to solve the problem, which enjoys an information-theoretic bound on the
cumulative regret, a first-of-its-kind for preferential BO. This bound further
allows us to design a scheme to report an estimated best solution, with a
guaranteed convergence rate. Experimental results on sampled instances from
Gaussian processes, standard test functions, and a thermal comfort optimization
problem all show that our method stably achieves better or competitive
performance as compared to the existing state-of-the-art heuristics, which,
however, do not have theoretical guarantees on regret bounds or convergence.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05369" title="Abstract">arXiv:2402.05369</a> [<a href="/pdf/2402.05369" title="Download PDF">pdf</a>, <a href="/format/2402.05369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Noise Contrastive Alignment of Language Models with Explicit Rewards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huayu Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+G">Guande He</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hang Su</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jun Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">User intentions are typically formalized as evaluation rewards to be
maximized when fine-tuning language models (LMs). Existing alignment methods,
such as Direct Preference Optimization (DPO), are mainly tailored for pairwise
preference data where rewards are implicitly defined rather than explicitly
given. In this paper, we introduce a general framework for LM alignment,
leveraging Noise Contrastive Estimation (NCE) to bridge the gap in handling
reward datasets explicitly annotated with scalar evaluations. Our framework
comprises two parallel algorithms, NCA and InfoNCA, both enabling the direct
extraction of an LM policy from reward data as well as preference data.
Notably, we show that the DPO loss is a special case of our proposed InfoNCA
objective under pairwise preference settings, thereby integrating and extending
current alignment theories. By contrasting NCA and InfoNCA, we show that
InfoNCA and DPO adjust relative likelihood across different responses to a
single instruction, while NCA optimizes absolute likelihood for each response.
We apply our methods to align a 7B language model with a GPT-4 annotated reward
dataset. Experimental results suggest that InfoNCA surpasses the DPO baseline
in GPT-4 evaluations, while NCA enjoys better training stability with
competitive performance.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05370" title="Abstract">arXiv:2402.05370</a> [<a href="/pdf/2402.05370" title="Download PDF">pdf</a>, <a href="/format/2402.05370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention as Robust Representation for Time Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niu%2C+P">PeiSong Niu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tian Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Liang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+R">Rong Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Time series forecasting is essential for many practical applications, with
the adoption of transformer-based models on the rise due to their impressive
performance in NLP and CV. Transformers' key feature, the attention mechanism,
dynamically fusing embeddings to enhance data representation, often relegating
attention weights to a byproduct role. Yet, time series data, characterized by
noise and non-stationarity, poses significant forecasting challenges. Our
approach elevates attention weights as the primary representation for time
series, capitalizing on the temporal relationships among data points to improve
forecasting accuracy. Our study shows that an attention map, structured using
global landmarks and local windows, acts as a robust kernel representation for
data points, withstanding noise and shifts in distribution. Our method
outperforms state-of-the-art models, reducing mean squared error (MSE) in
multivariate time series forecasting by a notable 3.6% without altering the
core neural network architecture. It serves as a versatile component that can
readily replace recent patching based embedding schemes in transformer-based
models, boosting their performance.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05371" title="Abstract">arXiv:2402.05371</a> [<a href="/pdf/2402.05371" title="Download PDF">pdf</a>, <a href="/format/2402.05371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Control Emulated Muscles in Real Robots: Towards Exploiting  Bio-Inspired Actuator Morphology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schumacher%2C+P">Pierre Schumacher</a>, 
<a href="/search/cs?searchtype=author&query=Krause%2C+L">Lorenz Krause</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+J">Jan Schneider</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%BCchler%2C+D">Dieter B&#xfc;chler</a>, 
<a href="/search/cs?searchtype=author&query=Martius%2C+G">Georg Martius</a>, 
<a href="/search/cs?searchtype=author&query=Haeufle%2C+D">Daniel Haeufle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Recent studies have demonstrated the immense potential of exploiting muscle
actuator morphology for natural and robust movement -- in simulation. A
validation on real robotic hardware is yet missing. In this study, we emulate
muscle actuator properties on hardware in real-time, taking advantage of modern
and affordable electric motors. We demonstrate that our setup can emulate a
simplified muscle model on a real robot while being controlled by a learned
policy. We improve upon an existing muscle model by deriving a damping rule
that ensures that the model is not only performant and stable but also tuneable
for the real hardware. Our policies are trained by reinforcement learning
entirely in simulation, where we show that previously reported benefits of
muscles extend to the case of quadruped locomotion and hopping: the learned
policies are more robust and exhibit more regular gaits. Finally, we confirm
that the learned policies can be executed on real hardware and show that
sim-to-real transfer with real-time emulated muscles on a quadruped robot is
possible. These results show that artificial muscles can be highly beneficial
actuators for future generations of robust legged robots.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05374" title="Abstract">arXiv:2402.05374</a> [<a href="/pdf/2402.05374" title="Download PDF">pdf</a>, <a href="/format/2402.05374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CIC: A framework for Culturally-aware Image Captioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yun%2C+Y">Youngsik Yun</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jihie Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Image Captioning generates descriptive sentences from images using
Vision-Language Pre-trained models (VLPs) such as BLIP, which has improved
greatly. However, current methods lack the generation of detailed descriptive
captions for the cultural elements depicted in the images, such as the
traditional clothing worn by people from Asian cultural groups. In this paper,
we propose a new framework, \textbf{Culturally-aware Image Captioning (CIC)},
that generates captions and describes cultural elements extracted from cultural
visual elements in images representing cultures. Inspired by methods combining
visual modality and Large Language Models (LLMs) through appropriate prompts,
our framework (1) generates questions based on cultural categories from images,
(2) extracts cultural visual elements from Visual Question Answering (VQA)
using generated questions, and (3) generates culturally-aware captions using
LLMs with the prompts. Our human evaluation conducted on 45 participants from 4
different cultural groups with a high understanding of the corresponding
culture shows that our proposed framework generates more culturally descriptive
captions when compared to the image captioning baseline based on VLPs. Our code
and dataset will be made publicly available upon acceptance.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05375" title="Abstract">arXiv:2402.05375</a> [<a href="/pdf/2402.05375" title="Download PDF">pdf</a>, <a href="/format/2402.05375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Get What You Want, Not What You Don&#x27;t: Image Content Suppression for  Text-to-Image Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Senmao Li</a>, 
<a href="/search/cs?searchtype=author&query=van+de+Weijer%2C+J">Joost van de Weijer</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+T">Taihang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+F+S">Fahad Shahbaz Khan</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Q">Qibin Hou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaxing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jian Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024. Our code is available in <a href="https://github.com/sen-mao/SuppressEOT">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The success of recent text-to-image diffusion models is largely due to their
capacity to be guided by a complex text prompt, which enables users to
precisely describe the desired content. However, these models struggle to
effectively suppress the generation of undesired content, which is explicitly
requested to be omitted from the generated image in the prompt. In this paper,
we analyze how to manipulate the text embeddings and remove unwanted content
from them. We introduce two contributions, which we refer to as
$\textit{soft-weighted regularization}$ and $\textit{inference-time text
embedding optimization}$. The first regularizes the text embedding matrix and
effectively suppresses the undesired content. The second method aims to further
suppress the unwanted content generation of the prompt, and encourages the
generation of desired content. We evaluate our method quantitatively and
qualitatively on extensive experiments, validating its effectiveness.
Furthermore, our method is generalizability to both the pixel-space diffusion
models (i.e. DeepFloyd-IF) and the latent-space diffusion models (i.e. Stable
Diffusion).
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05376" title="Abstract">arXiv:2402.05376</a> [<a href="/pdf/2402.05376" title="Download PDF">pdf</a>, <a href="/format/2402.05376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot Chain-of-Thought Reasoning Guided by Evolutionary Algorithms  in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+F">Feihu Jin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yifan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Y">Ying Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 5 figures, 16 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated remarkable performance across
diverse tasks and exhibited impressive reasoning abilities by applying
zero-shot Chain-of-Thought (CoT) prompting. However, due to the evolving nature
of sentence prefixes during the pre-training phase, existing zero-shot CoT
prompting methods that employ identical CoT prompting across all task instances
may not be optimal. In this paper, we introduce a novel zero-shot prompting
method that leverages evolutionary algorithms to generate diverse promptings
for LLMs dynamically. Our approach involves initializing two CoT promptings,
performing evolutionary operations based on LLMs to create a varied set, and
utilizing the LLMs to select a suitable CoT prompting for a given problem.
Additionally, a rewriting operation, guided by the selected CoT prompting,
enhances the understanding of the LLMs about the problem. Extensive experiments
conducted across ten reasoning datasets demonstrate the superior performance of
our proposed method compared to current zero-shot CoT prompting methods on
GPT-3.5-turbo and GPT-4. Moreover, in-depth analytical experiments underscore
the adaptability and effectiveness of our method in various reasoning tasks.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05379" title="Abstract">arXiv:2402.05379</a> [<a href="/pdf/2402.05379" title="Download PDF">pdf</a>, <a href="/format/2402.05379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tradeoffs of Diagonal Fisher Information Matrix Estimators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soen%2C+A">Alexander Soen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+K">Ke Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The Fisher information matrix characterizes the local geometry in the
parameter space of neural networks. It elucidates insightful theories and
useful tools to understand and optimize neural networks. Given its high
computational cost, practitioners often use random estimators and evaluate only
the diagonal entries. We examine two such estimators, whose accuracy and sample
complexity depend on their associated variances. We derive bounds of the
variances and instantiate them in regression and classification networks. We
navigate trade-offs of both estimators based on analytical and numerical
studies. We find that the variance quantities depend on the non-linearity with
respect to different parameter groups and should not be neglected when
estimating the Fisher information.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05382" title="Abstract">arXiv:2402.05382</a> [<a href="/pdf/2402.05382" title="Download PDF">pdf</a>, <a href="/format/2402.05382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Task-customized Masked AutoEncoder via Mixture of Cluster-conditional  Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhili Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jianhua Han</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lanqing Hong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Kwok%2C+J+T">James T. Kwok</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Masked Autoencoder~(MAE) is a prevailing self-supervised learning method that
achieves promising results in model pre-training. However, when the various
downstream tasks have data distributions different from the pre-training data,
the semantically irrelevant pre-training information might result in negative
transfer, impeding MAE's scalability. To address this issue, we propose a novel
MAE-based pre-training paradigm, Mixture of Cluster-conditional Experts (MoCE),
which can be trained once but provides customized pre-training models for
diverse downstream tasks. Different from the mixture of experts (MoE), our MoCE
trains each expert only with semantically relevant images by using
cluster-conditional gates. Thus, each downstream task can be allocated to its
customized model pre-trained with data most similar to the downstream data.
Experiments on a collection of 11 downstream tasks show that MoCE outperforms
the vanilla MAE by 2.45\% on average. It also obtains new state-of-the-art
self-supervised learning results on detection and segmentation.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05387" title="Abstract">arXiv:2402.05387</a> [<a href="/pdf/2402.05387" title="Download PDF">pdf</a>, <a href="/format/2402.05387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Channels be Fully Inferred Between Two Antenna Panels?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Y">Y. Qiu</a>, 
<a href="/search/cs?searchtype=author&query=W%2C+D">D. W</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Y">Y. Zeng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 6figures, IEEE Communications Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This letter considers a two-panel massive multiple-input multiple-output
(MIMO) communication system, where the base station (BS) is equipped with two
antenna panels that may use different frequency bands for communication. By
exploiting the geometric relationships between antenna panels, efficient
channel inference methods across antenna panels are proposed to reduce the
overhead of real-time channel estimation. Four scenarios are considered, namely
far-field free-space, near-field free-space, multi-path sharing far-field
scatterers, and multi-path sharing near-field scatterers. For both far-field
and near-field free-space scenarios, we show that the channel of one panel can
be fully inferred from that of the other panel, as long as the multi-path
components (MPCs) composing the channel can be resolved. On the other hand, for
the multi-path scenarios sharing far-field or near-field scatterers, only the
angles or range of angles of the MPCs can be inferred, respectively. Simulation
results based on commercial 3D ray-tracing software are presented to validate
our developed channel inference techniques.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05388" title="Abstract">arXiv:2402.05388</a> [<a href="/pdf/2402.05388" title="Download PDF">pdf</a>, <a href="/format/2402.05388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Form-From: A Design Space of Social Media Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A+X">Amy X. Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bernstein%2C+M+S">Michael S. Bernstein</a>, 
<a href="/search/cs?searchtype=author&query=Karger%2C+D+R">David R. Karger</a>, 
<a href="/search/cs?searchtype=author&query=Ackerman%2C+M+S">Mark S. Ackerman</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proc. ACM Hum.-Comput. Interact. 8, CSCW1, Article 167 (April
  2024), 47 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Social media systems are as varied as they are pervasive. They have been
almost universally adopted for a broad range of purposes including work,
entertainment, activism, and decision making. As a result, they have also
diversified, with many distinct designs differing in content type,
organization, delivery mechanism, access control, and many other dimensions. In
this work, we aim to characterize and then distill a concise design space of
social media systems that can help us understand similarities and differences,
recognize potential consequences of design choice, and identify spaces for
innovation. Our model, which we call Form-From, characterizes social media
based on (1) the form of the content, either threaded or flat, and (2) from
where or from whom one might receive content, ranging from spaces to networks
to the commons. We derive Form-From inductively from a larger set of 62
dimensions organized into 10 categories. To demonstrate the utility of our
model, we trace the history of social media systems as they traverse the
Form-From space over time, and we identify common design patterns within cells
of the model.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05390" title="Abstract">arXiv:2402.05390</a> [<a href="/pdf/2402.05390" title="Download PDF">pdf</a>, <a href="/format/2402.05390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrated Sensing and Communication Driven Digital Twin for Intelligent  Machine Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhiqing Wei</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yucong Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qixun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wangjun Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yanpeng Cui</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Z">Zeyang Meng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Huici Wu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zhiyong Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures, 1 Table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Intelligent machines (IMs), including industrial machines, unmanned aerial
vehicles (UAVs), and unmanned vehicles, etc., could perform effective
cooperation in complex environment when they form IM network. The efficient
environment sensing and communication are crucial for IM network, enabling the
real-time and stable control of IMs. With the emergence of integrated sensing
and communication (ISAC) technology, IM network is empowered with ubiquitous
sensing capabilities, which is helpful in improving the efficiency of
communication and sensing with the mutual benefit of them. However, the massive
amount of sensing information brings challenges for the processing, storage and
application of sensing information. In this article, ISAC driven digital twin
(DT) is proposed for IM network, and the architecture and enabling technologies
are revealed. ISAC driven DT structurally stores the sensing information, which
is further applied to optimize communication, networking and control schemes of
IMs, promoting the widespread applications of IMs.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05391" title="Abstract">arXiv:2402.05391</a> [<a href="/pdf/2402.05391" title="Download PDF">pdf</a>, <a href="/format/2402.05391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yichi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yin Fang</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+Y">Yuxia Geng</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Lingbing Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qian Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiaoyan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yushan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoze Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J+Z">Jeff Z. Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Ongoing work; 55 pages, 619 citations, 11 Tables, 13 Figures; Paper list is available at <a href="https://github.com/zjukg/KG-MM-Survey">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Knowledge Graphs (KGs) play a pivotal role in advancing various AI
applications, with the semantic web community's exploration into multi-modal
dimensions unlocking new avenues for innovation. In this survey, we carefully
review over 300 articles, focusing on KG-aware research in two principal
aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal
tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into
the MMKG realm. We begin by defining KGs and MMKGs, then explore their
construction progress. Our review includes two primary task categories:
KG-aware multi-modal learning tasks, such as Image Classification and Visual
Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph
Completion and Entity Alignment, highlighting specific research trajectories.
For most of these tasks, we provide definitions, evaluation benchmarks, and
additionally outline essential insights for conducting relevant research.
Finally, we discuss current challenges and identify emerging trends, such as
progress in Large Language Modeling and Multi-modal Pre-training strategies.
This survey aims to serve as a comprehensive reference for researchers already
involved in or considering delving into KG and multi-modal learning research,
offering insights into the evolving landscape of MMKG research and supporting
future work.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05394" title="Abstract">arXiv:2402.05394</a> [<a href="/pdf/2402.05394" title="Download PDF">pdf</a>, <a href="/format/2402.05394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Zero-shot Counting via Language-guided Exemplar Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yong Dai</a>, 
<a href="/search/cs?searchtype=author&query=Buys%2C+E">Eric Buys</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+M">Minglun Gong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recently, Class-Agnostic Counting (CAC) problem has garnered increasing
attention owing to its intriguing generality and superior efficiency compared
to Category-Specific Counting (CSC). This paper proposes a novel ExpressCount
to enhance zero-shot object counting by delving deeply into language-guided
exemplar learning. Specifically, the ExpressCount is comprised of an innovative
Language-oriented Exemplar Perceptron and a downstream visual Zero-shot
Counting pipeline. Thereinto, the perceptron hammers at exploiting accurate
exemplar cues from collaborative language-vision signals by inheriting rich
semantic priors from the prevailing pre-trained Large Language Models (LLMs),
whereas the counting pipeline excels in mining fine-grained features through
dual-branch and cross-attention schemes, contributing to the high-quality
similarity learning. Apart from building a bridge between the LLM in vogue and
the visual counting tasks, expression-guided exemplar estimation significantly
advances zero-shot learning capabilities for counting instances with arbitrary
classes. Moreover, devising a FSC-147-Express with annotations of meticulous
linguistic expressions pioneers a new venue for developing and validating
language-based counting models. Extensive experiments demonstrate the
state-of-the-art performance of our ExpressCount, even showcasing the accuracy
on par with partial CSC models.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05396" title="Abstract">arXiv:2402.05396</a> [<a href="/pdf/2402.05396" title="Download PDF">pdf</a>, <a href="/format/2402.05396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph  Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+G">Gangda Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hongkuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+H">Hanqing Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yinglong Xia</a>, 
<a href="/search/cs?searchtype=author&query=Leung%2C+C">Christopher Leung</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianbo Li</a>, 
<a href="/search/cs?searchtype=author&query=Kannan%2C+R">Rajgopal Kannan</a>, 
<a href="/search/cs?searchtype=author&query=Prasanna%2C+V">Viktor Prasanna</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated
state-of-the-art performance in various high-impact applications, including
fraud detection and content recommendation. Despite the success of TGNNs, they
are prone to the prevalent noise found in real-world dynamic graphs like
time-deprecated links and skewed interaction distribution. The noise causes two
critical issues that significantly compromise the accuracy of TGNNs: (1) models
are supervised by inferior interactions, and (2) noisy input induces high
variance in the aggregated messages. However, current TGNN denoising techniques
do not consider the diverse and dynamic noise pattern of each node. In
addition, they also suffer from the excessive mini-batch generation overheads
caused by traversing more neighbors. We believe the remedy for fast and
accurate TGNNs lies in temporal adaptive sampling. In this work, we propose
TASER, the first adaptive sampling method for TGNNs optimized for accuracy,
efficiency, and scalability. TASER adapts its mini-batch selection based on
training dynamics and temporal neighbor selection based on the contextual,
structural, and temporal properties of past interactions. To alleviate the
bottleneck in mini-batch generation, TASER implements a pure GPU-based temporal
neighbor finder and a dedicated GPU feature cache. We evaluate the performance
of TASER using two state-of-the-art backbone TGNNs. On five popular datasets,
TASER outperforms the corresponding baselines by an average of 2.3% in Mean
Reciprocal Rank (MRR) while achieving an average of 5.1x speedup in training
time.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05398" title="Abstract">arXiv:2402.05398</a> [<a href="/pdf/2402.05398" title="Download PDF">pdf</a>, <a href="/format/2402.05398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Effect of Image Resolution on Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+R">Ritambhara Singh</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+A">Abhishek Jain</a>, 
<a href="/search/cs?searchtype=author&query=Perona%2C+P">Pietro Perona</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+S">Shivani Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Junfeng Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2209.08667">arXiv:2209.08667</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">High-resolution semantic segmentation requires substantial computational
resources. Traditional approaches in the field typically downscale the input
images before processing and then upscale the low-resolution outputs back to
their original dimensions. While this strategy effectively identifies broad
regions, it often misses finer details. In this study, we demonstrate that a
streamlined model capable of directly producing high-resolution segmentations
can match the performance of more complex systems that generate
lower-resolution results. By simplifying the network architecture, we enable
the processing of images at their native resolution. Our approach leverages a
bottom-up information propagation technique across various scales, which we
have empirically shown to enhance segmentation accuracy. We have rigorously
tested our method using leading-edge semantic segmentation datasets.
Specifically, for the Cityscapes dataset, we further boost accuracy by applying
the Noisy Student Training technique.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05399" title="Abstract">arXiv:2402.05399</a> [<a href="/pdf/2402.05399" title="Download PDF">pdf</a>, <a href="/format/2402.05399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CURE: Simulation-Augmented Auto-Tuning in Robotics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hossen%2C+M+A">Md Abir Hossen</a>, 
<a href="/search/cs?searchtype=author&query=Kharade%2C+S">Sonam Kharade</a>, 
<a href="/search/cs?searchtype=author&query=O%27Kane%2C+J+M">Jason M. O&#x27;Kane</a>, 
<a href="/search/cs?searchtype=author&query=Schmerl%2C+B">Bradley Schmerl</a>, 
<a href="/search/cs?searchtype=author&query=Garlan%2C+D">David Garlan</a>, 
<a href="/search/cs?searchtype=author&query=Jamshidi%2C+P">Pooyan Jamshidi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted in IEEE Transactions on Robotics (T-RO), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Robotic systems are typically composed of various subsystems, such as
localization and navigation, each encompassing numerous configurable components
(e.g., selecting different planning algorithms). Once an algorithm has been
selected for a component, its associated configuration options must be set to
the appropriate values. Configuration options across the system stack interact
non-trivially. Finding optimal configurations for highly configurable robots to
achieve desired performance poses a significant challenge due to the
interactions between configuration options across software and hardware that
result in an exponentially large and complex configuration space. These
challenges are further compounded by the need for transferability between
different environments and robotic platforms. Data efficient optimization
algorithms (e.g., Bayesian optimization) have been increasingly employed to
automate the tuning of configurable parameters in cyber-physical systems.
However, such optimization algorithms converge at later stages, often after
exhausting the allocated budget (e.g., optimization steps, allotted time) and
lacking transferability. This paper proposes CURE -- a method that identifies
causally relevant configuration options, enabling the optimization process to
operate in a reduced search space, thereby enabling faster optimization of
robot performance. CURE abstracts the causal relationships between various
configuration options and robot performance objectives by learning a causal
model in the source (a low-cost environment such as the Gazebo simulator) and
applying the learned knowledge to perform optimization in the target (e.g.,
Turtlebot 3 physical robot). We demonstrate the effectiveness and
transferability of CURE by conducting experiments that involve varying degrees
of deployment changes in both physical robots and simulation.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05400" title="Abstract">arXiv:2402.05400</a> [<a href="/pdf/2402.05400" title="Download PDF">pdf</a>, <a href="/format/2402.05400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing for ROC Curves on Class-Imbalanced Data by Training over a  Family of Loss Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lieberman%2C+K">Kelsey Lieberman</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Shuai Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ravindran%2C+S+K">Swarna Kamlam Ravindran</a>, 
<a href="/search/cs?searchtype=author&query=Tomasi%2C+C">Carlo Tomasi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Although binary classification is a well-studied problem in computer vision,
training reliable classifiers under severe class imbalance remains a
challenging problem. Recent work has proposed techniques that mitigate the
effects of training under imbalance by modifying the loss functions or
optimization methods. While this work has led to significant improvements in
the overall accuracy in the multi-class case, we observe that slight changes in
hyperparameter values of these methods can result in highly variable
performance in terms of Receiver Operating Characteristic (ROC) curves on
binary problems with severe imbalance. To reduce the sensitivity to
hyperparameter choices and train more general models, we propose training over
a family of loss functions, instead of a single loss function. We develop a
method for applying Loss Conditional Training (LCT) to an imbalanced
classification problem. Extensive experiment results, on both CIFAR and Kaggle
competition datasets, show that our method improves model performance and is
more robust to hyperparameter choices. Code will be made available at:
https://github.com/klieberman/roc_lct.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05401" title="Abstract">arXiv:2402.05401</a> [<a href="/pdf/2402.05401" title="Download PDF">pdf</a>, <a href="/format/2402.05401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Activation Functions for Predictive Modeling with Sparse  Experimental Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pourkamali-Anaraki%2C+F">Farhad Pourkamali-Anaraki</a>, 
<a href="/search/cs?searchtype=author&query=Nasrin%2C+T">Tahamina Nasrin</a>, 
<a href="/search/cs?searchtype=author&query=Jensen%2C+R+E">Robert E. Jensen</a>, 
<a href="/search/cs?searchtype=author&query=Peterson%2C+A+M">Amy M. Peterson</a>, 
<a href="/search/cs?searchtype=author&query=Hansen%2C+C+J">Christopher J. Hansen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)

</div>
<p class="mathjax">A pivotal aspect in the design of neural networks lies in selecting
activation functions, crucial for introducing nonlinear structures that capture
intricate input-output patterns. While the effectiveness of adaptive or
trainable activation functions has been studied in domains with ample data,
like image classification problems, significant gaps persist in understanding
their influence on classification accuracy and predictive uncertainty in
settings characterized by limited data availability. This research aims to
address these gaps by investigating the use of two types of adaptive activation
functions. These functions incorporate shared and individual trainable
parameters per hidden layer and are examined in three testbeds derived from
additive manufacturing problems containing fewer than one hundred training
instances. Our investigation reveals that adaptive activation functions, such
as Exponential Linear Unit (ELU) and Softplus, with individual trainable
parameters, result in accurate and confident prediction models that outperform
fixed-shape activation functions and the less flexible method of using
identical trainable activation functions in a hidden layer. Therefore, this
work presents an elegant way of facilitating the design of adaptive neural
networks in scientific and engineering problems.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05402" title="Abstract">arXiv:2402.05402</a> [<a href="/pdf/2402.05402" title="Download PDF">pdf</a>, <a href="/format/2402.05402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A State-of-the-art Survey on Full-duplex Network Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yonghwi Kim</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+H">Hyung-Joo Moon</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+H">Hanju Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Byoungnam">Byoungnam</a> (Klaus)Kim, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kai-Kit Wong</a>, 
<a href="/search/cs?searchtype=author&query=Chae%2C+C">Chan-Byoung Chae</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 10 figures, To appear in Proceedings of the IEEE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP); Systems and Control (eess.SY)

</div>
<p class="mathjax">Full-duplex (FD) technology is gaining popularity for integration into a wide
range of wireless networks due to its demonstrated potential in recent studies.
In contrast to half-duplex (HD) technology, the implementation of FD in
networks necessitates considering inter-node interference (INI) from various
network perspectives. When deploying FD technology in networks, several
critical factors must be taken into account. These include self-interference
(SI) and the requisite SI cancellation (SIC) processes, as well as the
selection of multiple user equipment (UE) per time slot. Additionally,
inter-node interference (INI), including cross-link interference (CLI) and
inter-cell interference (ICI), become crucial issues during concurrent uplink
(UL) and downlink (DL) transmission and reception, similar to SI. Since most
INI is challenging to eliminate, a comprehensive investigation that covers
radio resource control (RRC), medium access control (MAC), and the physical
layer (PHY) is essential in the context of FD network design, rather than
focusing on individual network layers and types. This paper covers
state-of-the-art studies, including protocols and documents from 3GPP for FD,
MAC protocol, user scheduling, and CLI handling. The methods are also compared
through a network-level system simulation based on 3D ray-tracing.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05403" title="Abstract">arXiv:2402.05403</a> [<a href="/pdf/2402.05403" title="Download PDF">pdf</a>, <a href="/ps/2402.05403" title="Download PostScript">ps</a>, <a href="/format/2402.05403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Principle Learning from Mistakes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianjun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Madaan%2C+A">Aman Madaan</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+L">Luyu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Steven Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Swaroop Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yiming Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tandon%2C+N">Niket Tandon</a>, 
<a href="/search/cs?searchtype=author&query=Alon%2C+U">Uri Alon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In-context learning (ICL, also known as few-shot prompting) has been the
standard method of adapting LLMs to downstream tasks, by learning from a few
input-output examples. Nonetheless, all ICL-based approaches only learn from
correct input-output pairs. In this paper, we revisit this paradigm, by
learning more from the few given input-output examples. We introduce Learning
Principles (LEAP): First, we intentionally induce the model to make mistakes on
these few examples; then we reflect on these mistakes, and learn explicit
task-specific "principles" from them, which help solve similar problems and
avoid common mistakes; finally, we prompt the model to answer unseen test
questions using the original few-shot examples and these learned general
principles. We evaluate LEAP on a wide range of benchmarks, including multi-hop
question answering (Hotpot QA), textual QA (DROP), Big-Bench Hard reasoning,
and math problems (GSM8K and MATH); in all these benchmarks, LEAP improves the
strongest available LLMs such as GPT-3.5-turbo, GPT-4, GPT-4 turbo and
Claude-2.1. For example, LEAP improves over the standard few-shot prompting
using GPT-4 by 7.5% in DROP, and by 3.3% in HotpotQA. Importantly, LEAP does
not require any more input or examples than the standard few-shot prompting
settings.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05406" title="Abstract">arXiv:2402.05406</a> [<a href="/pdf/2402.05406" title="Download PDF">pdf</a>, <a href="/format/2402.05406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dery%2C+L">Lucio Dery</a>, 
<a href="/search/cs?searchtype=author&query=Kolawole%2C+S">Steven Kolawole</a>, 
<a href="/search/cs?searchtype=author&query=Kagey%2C+J">Jean-Francois Kagey</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+V">Virginia Smith</a>, 
<a href="/search/cs?searchtype=author&query=Neubig%2C+G">Graham Neubig</a>, 
<a href="/search/cs?searchtype=author&query=Talwalkar%2C+A">Ameet Talwalkar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 4 fiigures, 15 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Given the generational gap in available hardware between lay practitioners
and the most endowed institutions, LLMs are becoming increasingly inaccessible
as they grow in size. Whilst many approaches have been proposed to compress
LLMs to make their resource consumption manageable, these methods themselves
tend to be resource intensive, putting them out of the reach of the very user
groups they target. In this work, we explore the problem of structured pruning
of LLMs using only forward passes. We seek to empower practitioners to prune
models so large that their available hardware has just enough memory to run
inference. We develop Bonsai, a gradient-free, perturbative pruning method
capable of delivering small, fast, and accurate pruned models.
<br />We observe that Bonsai outputs pruned models that (i) outperform those
generated by more expensive gradient-based structured pruning methods, and (ii)
are twice as fast (with comparable accuracy) as those generated by
semi-structured pruning methods requiring comparable resources as Bonsai. We
also leverage Bonsai to produce a new sub-2B model using a single A6000 that
yields state-of-the-art performance on 4/6 tasks on the Huggingface Open LLM
leaderboard.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05407" title="Abstract">arXiv:2402.05407</a> [<a href="/pdf/2402.05407" title="Download PDF">pdf</a>, <a href="/format/2402.05407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Version age-based client scheduling policy for federated learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xinyi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Pappas%2C+N">Nikolaos Pappas</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H+H">Howard H. Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures, ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated Learning (FL) has emerged as a privacy-preserving machine learning
paradigm facilitating collaborative training across multiple clients without
sharing local data. Despite advancements in edge device capabilities,
communication bottlenecks present challenges in aggregating a large number of
clients; only a portion of the clients can update their parameters upon each
global aggregation. This phenomenon introduces the critical challenge of
stragglers in FL and the profound impact of client scheduling policies on
global model convergence and stability. Existing scheduling strategies address
staleness but predominantly focus on either timeliness or content. Motivated by
this, we introduce the novel concept of Version Age of Information (VAoI) to
FL. Unlike traditional Age of Information metrics, VAoI considers both
timeliness and content staleness. Each client's version age is updated
discretely, indicating the freshness of information. VAoI is incorporated into
the client scheduling policy to minimize the average VAoI, mitigating the
impact of outdated local updates and enhancing the stability of FL systems.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05408" title="Abstract">arXiv:2402.05408</a> [<a href="/pdf/2402.05408" title="Download PDF">pdf</a>, <a href="/format/2402.05408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MIGC: Multi-Instance Generation Controller for Text-to-Image Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Dewei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">You Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+F">Fan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zongxin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present a Multi-Instance Generation (MIG) task, simultaneously generating
multiple instances with diverse controls in one image. Given a set of
predefined coordinates and their corresponding descriptions, the task is to
ensure that generated instances are accurately at the designated locations and
that all instances' attributes adhere to their corresponding description. This
broadens the scope of current research on Single-instance generation, elevating
it to a more versatile and practical dimension. Inspired by the idea of divide
and conquer, we introduce an innovative approach named Multi-Instance
Generation Controller (MIGC) to address the challenges of the MIG task.
Initially, we break down the MIG task into several subtasks, each involving the
shading of a single instance. To ensure precise shading for each instance, we
introduce an instance enhancement attention mechanism. Lastly, we aggregate all
the shaded instances to provide the necessary information for accurately
generating multiple instances in stable diffusion (SD). To evaluate how well
generation models perform on the MIG task, we provide a COCO-MIG benchmark
along with an evaluation pipeline. Extensive experiments were conducted on the
proposed COCO-MIG benchmark, as well as on various commonly used benchmarks.
The evaluation results illustrate the exceptional control capabilities of our
model in terms of quantity, position, attribute, and interaction.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05410" title="Abstract">arXiv:2402.05410</a> [<a href="/pdf/2402.05410" title="Download PDF">pdf</a>, <a href="/format/2402.05410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpirDet: Towards Efficient, Accurate and Lightweight Infrared Small  Target Detector
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+Q">Qianchen Mao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bingshu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongjun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+T">Tao Dai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C+L+P">C.L. Philip Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In recent years, the detection of infrared small targets using deep learning
methods has garnered substantial attention due to notable advancements. To
improve the detection capability of small targets, these methods commonly
maintain a pathway that preserves high-resolution features of sparse and tiny
targets. However, it can result in redundant and expensive computations. To
tackle this challenge, we propose SpirDet, a novel approach for efficient
detection of infrared small targets. Specifically, to cope with the
computational redundancy issue, we employ a new dual-branch sparse decoder to
restore the feature map. Firstly, the fast branch directly predicts a sparse
map indicating potential small target locations (occupying only 0.5\% area of
the map). Secondly, the slow branch conducts fine-grained adjustments at the
positions indicated by the sparse map. Additionally, we design an lightweight
DO-RepEncoder based on reparameterization with the Downsampling Orthogonality,
which can effectively reduce memory consumption and inference latency.
Extensive experiments show that the proposed SpirDet significantly outperforms
state-of-the-art models while achieving faster inference speed and fewer
parameters. For example, on the IRSTD-1K dataset, SpirDet improves $MIoU$ by
4.7 and has a $7\times$ $FPS$ acceleration compared to the previous
state-of-the-art model. The code will be open to the public.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05412" title="Abstract">arXiv:2402.05412</a> [<a href="/pdf/2402.05412" title="Download PDF">pdf</a>, <a href="/ps/2402.05412" title="Download PostScript">ps</a>, <a href="/format/2402.05412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Network Constrained Operational Optimization in Community  Integrated Energy Systems: A Safe Reinforcement Learning Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hu%2C+Z">Ze Hu</a>, 
<a href="/search/eess?searchtype=author&query=Chan%2C+K+W">Ka Wing Chan</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+Z">Ziqing Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Wei%2C+X">Xiang Wei</a>, 
<a href="/search/eess?searchtype=author&query=Bu%2C+S">Siqi Bu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The integrated community energy system (ICES) has emerged as a promising
solution for enhancing the efficiency of the distribution system by effectively
coordinating multiple energy sources. However, the operational optimization of
ICES is hindered by the physical constraints of heterogeneous networks
including electricity, natural gas, and heat. These challenges are difficult to
address due to the non-linearity of network constraints and the high complexity
of multi-network coordination. This paper, therefore, proposes a novel Safe
Reinforcement Learning (SRL) algorithm to optimize the multi-network
constrained operation problem of ICES. Firstly, a comprehensive ICES model is
established considering integrated demand response (IDR), multiple energy
devices, and network constraints. The multi-network operational optimization
problem of ICES is then presented and reformulated as a constrained Markov
Decision Process (C-MDP) accounting for violating physical network constraints.
The proposed novel SRL algorithm, named Primal-Dual Twin Delayed Deep
Deterministic Policy Gradient (PD-TD3), solves the C-MDP by employing a
Lagrangian multiplier to penalize the multi-network constraint violation,
ensuring that violations are within a tolerated range and avoid
over-conservative strategy with a low reward at the same time. The proposed
algorithm accurately estimates the cumulative reward and cost of the training
process, thus achieving a fair balance between improving profits and reducing
constraint violations in a privacy-protected environment with only partial
information. A case study comparing the proposed algorithm with benchmark RL
algorithms demonstrates the computational performance in increasing total
profits and alleviating the network constraint violations.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05417" title="Abstract">arXiv:2402.05417</a> [<a href="/pdf/2402.05417" title="Download PDF">pdf</a>, <a href="/format/2402.05417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Segmentation-free Connectionist Temporal Classification loss based OCR  Model for Text Captcha Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khatavkar%2C+V">Vaibhav Khatavkar</a>, 
<a href="/search/cs?searchtype=author&query=Velankar%2C+M">Makarand Velankar</a>, 
<a href="/search/cs?searchtype=author&query=Petkar%2C+S">Sneha Petkar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Captcha are widely used to secure systems from automatic responses by
distinguishing computer responses from human responses. Text, audio, video,
picture picture-based Optical Character Recognition (OCR) are used for creating
captcha. Text-based OCR captcha are the most often used captcha which faces
issues namely, complex and distorted contents. There are attempts to build
captcha detection and classification-based systems using machine learning and
neural networks, which need to be tuned for accuracy. The existing systems face
challenges in the recognition of distorted characters, handling variable-length
captcha and finding sequential dependencies in captcha. In this work, we
propose a segmentation-free OCR model for text captcha classification based on
the connectionist temporal classification loss technique. The proposed model is
trained and tested on a publicly available captcha dataset. The proposed model
gives 99.80\% character level accuracy, while 95\% word level accuracy. The
accuracy of the proposed model is compared with the state-of-the-art models and
proves to be effective. The variable length complex captcha can be thus
processed with the segmentation-free connectionist temporal classification loss
technique with dependencies which will be massively used in securing the
software systems.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05420" title="Abstract">arXiv:2402.05420</a> [<a href="/pdf/2402.05420" title="Download PDF">pdf</a>, <a href="/format/2402.05420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Visibility-based Search in Polygonal Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huynh%2C+K+C">Kien C. Huynh</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+J+S+B">Joseph S. B. Mitchell</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+L">Linh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Polishchuk%2C+V">Valentin Polishchuk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">Given a geometric domain $P$, visibility-based search problems seek routes
for one or more mobile agents (``watchmen'') to move within $P$ in order to be
able to see a portion (or all) of $P$, while optimizing objectives, such as the
length(s) of the route(s), the size (e.g., area or volume) of the portion seen,
the probability of detecting a target distributed within $P$ according to a
prior distribution, etc. The classic watchman route problem seeks a shortest
route for an observer, with omnidirectional vision, to see all of $P$. In this
paper we study bicriteria optimization problems for a single mobile agent
within a polygonal domain $P$ in the plane, with the criteria of route length
and area seen. Specifically, we address the problem of computing a minimum
length route that sees at least a specified area of $P$ (minimum length, for a
given area quota). We also study the problem of computing a length-constrained
route that sees as much area as possible. We provide hardness results and
approximation algorithms. In particular, for a simple polygon $P$ we provide
the first fully polynomial-time approximation scheme for the problem of
computing a shortest route seeing an area quota, as well as a (slightly more
efficient) polynomial dual approximation. We also consider polygonal domains
$P$ (with holes) and the special case of a planar domain consisting of a union
of lines. Our results yield the first approximation algorithms for computing a
time-optimal search route in $P$ to guarantee some specified probability of
detection of a static target within $P$, randomly distributed in $P$ according
to a given prior distribution.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05421" title="Abstract">arXiv:2402.05421</a> [<a href="/pdf/2402.05421" title="Download PDF">pdf</a>, <a href="/format/2402.05421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement  and Imitation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+W">Weikang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yufei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Erickson%2C+Z">Zackory Erickson</a>, 
<a href="/search/cs?searchtype=author&query=Held%2C+D">David Held</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">This paper introduces DiffTOP, which utilizes Differentiable Trajectory
OPtimization as the policy representation to generate actions for deep
reinforcement and imitation learning. Trajectory optimization is a powerful and
widely used algorithm in control, parameterized by a cost and a dynamics
function. The key to our approach is to leverage the recent progress in
differentiable trajectory optimization, which enables computing the gradients
of the loss with respect to the parameters of trajectory optimization. As a
result, the cost and dynamics functions of trajectory optimization can be
learned end-to-end. DiffTOP addresses the ``objective mismatch'' issue of prior
model-based RL algorithms, as the dynamics model in DiffTOP is learned to
directly maximize task performance by differentiating the policy gradient loss
through the trajectory optimization process. We further benchmark DiffTOP for
imitation learning on standard robotic manipulation task suites with
high-dimensional sensory observations and compare our method to feed-forward
policy classes as well as Energy-Based Models (EBM) and Diffusion. Across 15
model-based RL tasks and 13 imitation learning tasks with high-dimensional
image and point cloud inputs, DiffTOP outperforms prior state-of-the-art
methods in both domains.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05423" title="Abstract">arXiv:2402.05423</a> [<a href="/pdf/2402.05423" title="Download PDF">pdf</a>, <a href="/format/2402.05423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MTSA-SNN: A Multi-modal Time Series Analysis Model Based on Spiking  Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chengzhi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+C">Chong Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+M">Mingyu Jin</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Z">Zheng Tao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zihong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chenghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shuliang Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures, published to International Conference on Computer Supported Cooperative Work in Design
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Conference on Computer Supported Cooperative Work in
  Design 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Time series analysis and modelling constitute a crucial research area.
Traditional artificial neural networks struggle with complex, non-stationary
time series data due to high computational complexity, limited ability to
capture temporal information, and difficulty in handling event-driven data. To
address these challenges, we propose a Multi-modal Time Series Analysis Model
Based on Spiking Neural Network (MTSA-SNN). The Pulse Encoder unifies the
encoding of temporal images and sequential information in a common pulse-based
representation. The Joint Learning Module employs a joint learning function and
weight allocation mechanism to fuse information from multi-modal pulse signals
complementary. Additionally, we incorporate wavelet transform operations to
enhance the model's ability to analyze and evaluate temporal information.
Experimental results demonstrate that our method achieved superior performance
on three complex time-series tasks. This work provides an effective
event-driven approach to overcome the challenges associated with analyzing
intricate temporal information. Access to the source code is available at
https://github.com/Chenngzz/MTSA-SNN}{https://github.com/Chenngzz/MTSA-SNN
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05424" title="Abstract">arXiv:2402.05424</a> [<a href="/pdf/2402.05424" title="Download PDF">pdf</a>, <a href="/format/2402.05424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Circuit Diagrams: Robust Diagrams for the Communication,  Implementation, and Analysis of Deep Learning Architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abbott%2C+V">Vincent Abbott</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Transactions on Machine Learning Research (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Diagrams matter. Unfortunately, the deep learning community has no standard
method for diagramming architectures. The current combination of linear algebra
notation and ad-hoc diagrams fails to offer the necessary precision to
understand architectures in all their detail. However, this detail is critical
for faithful implementation, mathematical analysis, further innovation, and
ethical assurances. I present neural circuit diagrams, a graphical language
tailored to the needs of communicating deep learning architectures. Neural
circuit diagrams naturally keep track of the changing arrangement of data,
precisely show how operations are broadcast over axes, and display the critical
parallel behavior of linear operations. A lingering issue with existing
diagramming methods is the inability to simultaneously express the detail of
axes and the free arrangement of data, which neural circuit diagrams solve.
Their compositional structure is analogous to code, creating a close
correspondence between diagrams and implementation.
<br />In this work, I introduce neural circuit diagrams for an audience of machine
learning researchers. After introducing neural circuit diagrams, I cover a host
of architectures to show their utility and breed familiarity. This includes the
transformer architecture, convolution (and its difficult-to-explain
extensions), residual networks, the U-Net, and the vision transformer. I
include a Jupyter notebook that provides evidence for the close correspondence
between diagrams and code. Finally, I examine backpropagation using neural
circuit diagrams. I show their utility in providing mathematical insight and
analyzing algorithms' time and space complexities.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05427" title="Abstract">arXiv:2402.05427</a> [<a href="/pdf/2402.05427" title="Download PDF">pdf</a>, <a href="/format/2402.05427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Sampling Theory Perspective on Activations for Implicit Neural  Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saratchandran%2C+H">Hemanth Saratchandran</a>, 
<a href="/search/cs?searchtype=author&query=Ramasinghe%2C+S">Sameera Ramasinghe</a>, 
<a href="/search/cs?searchtype=author&query=Shevchenko%2C+V">Violetta Shevchenko</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+A">Alexander Long</a>, 
<a href="/search/cs?searchtype=author&query=Lucey%2C+S">Simon Lucey</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Implicit Neural Representations (INRs) have gained popularity for encoding
signals as compact, differentiable entities. While commonly using techniques
like Fourier positional encodings or non-traditional activation functions
(e.g., Gaussian, sinusoid, or wavelets) to capture high-frequency content,
their properties lack exploration within a unified theoretical framework.
Addressing this gap, we conduct a comprehensive analysis of these activations
from a sampling theory perspective. Our investigation reveals that sinc
activations, previously unused in conjunction with INRs, are theoretically
optimal for signal encoding. Additionally, we establish a connection between
dynamical systems and INRs, leveraging sampling theory to bridge these two
paradigms.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05428" title="Abstract">arXiv:2402.05428</a> [<a href="/pdf/2402.05428" title="Download PDF">pdf</a>, <a href="/format/2402.05428" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixture Density Networks for Classification with an Application to  Product Bundling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gugulothu%2C+N">Narendhar Gugulothu</a>, 
<a href="/search/cs?searchtype=author&query=Bhat%2C+S+P">Sanjay P. Bhat</a>, 
<a href="/search/cs?searchtype=author&query=Bodas%2C+T">Tejas Bodas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While mixture density networks (MDNs) have been extensively used for
regression tasks, they have not been used much for classification tasks. One
reason for this is that the usability of MDNs for classification is not clear
and straightforward. In this paper, we propose two MDN-based models for
classification tasks. Both models fit mixtures of Gaussians to the the data and
use the fitted distributions to classify a given sample by evaluating the
learnt cumulative distribution function for the given input features. While the
proposed MDN-based models perform slightly better than, or on par with, five
baseline classification models on three publicly available datasets, the real
utility of our models comes out through a real-world product bundling
application. Specifically, we use our MDN-based models to learn the
willingness-to-pay (WTP) distributions for two products from synthetic sales
data of the individual products. The Gaussian mixture representation of the
learnt WTP distributions is then exploited to obtain the WTP distribution of
the bundle consisting of both the products. The proposed MDN-based models are
able to approximate the true WTP distributions of both products and the bundle
well.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05435" title="Abstract">arXiv:2402.05435</a> [<a href="/pdf/2402.05435" title="Download PDF">pdf</a>, <a href="/format/2402.05435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPT-4 Generated Narratives of Life Events using a Structured Narrative  Prompt: A Validation Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lynch%2C+C+J">Christopher J. Lynch</a>, 
<a href="/search/cs?searchtype=author&query=Jensen%2C+E">Erik Jensen</a>, 
<a href="/search/cs?searchtype=author&query=Munro%2C+M+H">Madison H. Munro</a>, 
<a href="/search/cs?searchtype=author&query=Zamponi%2C+V">Virginia Zamponi</a>, 
<a href="/search/cs?searchtype=author&query=Martinez%2C+J">Joseph Martinez</a>, 
<a href="/search/cs?searchtype=author&query=O%27Brien%2C+K">Kevin O&#x27;Brien</a>, 
<a href="/search/cs?searchtype=author&query=Feldhaus%2C+B">Brandon Feldhaus</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+K">Katherine Smith</a>, 
<a href="/search/cs?searchtype=author&query=Reinhold%2C+A+M">Ann Marie Reinhold</a>, 
<a href="/search/cs?searchtype=author&query=Gore%2C+R">Ross Gore</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 24 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) play a pivotal role in generating vast arrays of
narratives, facilitating a systematic exploration of their effectiveness for
communicating life events in narrative form. In this study, we employ a
zero-shot structured narrative prompt to generate 24,000 narratives using
OpenAI's GPT-4. From this dataset, we manually classify 2,880 narratives and
evaluate their validity in conveying birth, death, hiring, and firing events.
Remarkably, 87.43% of the narratives sufficiently convey the intention of the
structured prompt. To automate the identification of valid and invalid
narratives, we train and validate nine Machine Learning models on the
classified datasets. Leveraging these models, we extend our analysis to predict
the classifications of the remaining 21,120 narratives. All the ML models
excelled at classifying valid narratives as valid, but experienced challenges
at simultaneously classifying invalid narratives as invalid. Our findings not
only advance the study of LLM capabilities, limitations, and validity but also
offer practical insights for narrative generation and natural language
processing applications.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05439" title="Abstract">arXiv:2402.05439</a> [<a href="/pdf/2402.05439" title="Download PDF">pdf</a>, <a href="/format/2402.05439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Uncertainty-Aware Temporally-Extended Actions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Joongkyu Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S+J">Seung Joon Park</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yunhao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+M">Min-hwan Oh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in AAAI 2024 (Main Technical Track)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In reinforcement learning, temporal abstraction in the action space,
exemplified by action repetition, is a technique to facilitate policy learning
through extended actions. However, a primary limitation in previous studies of
action repetition is its potential to degrade performance, particularly when
sub-optimal actions are repeated. This issue often negates the advantages of
action repetition. To address this, we propose a novel algorithm named
Uncertainty-aware Temporal Extension (UTE). UTE employs ensemble methods to
accurately measure uncertainty during action extension. This feature allows
policies to strategically choose between emphasizing exploration or adopting an
uncertainty-averse approach, tailored to their specific needs. We demonstrate
the effectiveness of UTE through experiments in Gridworld and Atari 2600
environments. Our findings show that UTE outperforms existing action repetition
algorithms, effectively mitigating their inherent limitations and significantly
enhancing policy learning efficiency.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05440" title="Abstract">arXiv:2402.05440</a> [<a href="/pdf/2402.05440" title="Download PDF">pdf</a>, <a href="/format/2402.05440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Agent Interactions in Virtual Environments with Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jack Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Enhancing AI systems with efficient communication skills for effective human
assistance necessitates proactive initiatives from the system side to discern
specific circumstances and interact aptly. This research focuses on a
collective building assignment in the Minecraft dataset, employing language
modeling to enhance task understanding through state-of-the-art methods. These
models focus on grounding multi-modal understanding and task-oriented dialogue
comprehension tasks, providing insights into their interpretative and
responsive capabilities. Our experimental results showcase a substantial
improvement over existing methods, indicating a promising direction for future
research in this domain.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05441" title="Abstract">arXiv:2402.05441</a> [<a href="/pdf/2402.05441" title="Download PDF">pdf</a>, <a href="/ps/2402.05441" title="Download PostScript">ps</a>, <a href="/format/2402.05441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spiking Neural Network Enhanced Hand Gesture Recognition Using Low-Cost  Single-photon Avalanche Diode Array
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zang%2C+Z">Zhenya Zang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xingda Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D+D+U">David Day Uei Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">We present a compact spiking convolutional neural network (SCNN) and spiking
multilayer perceptron (SMLP) to recognize ten different gestures in dark and
bright light environments, using a $9.6 single-photon avalanche diode (SPAD)
array. In our hand gesture recognition (HGR) system, photon intensity data was
leveraged to train and test the network. A vanilla convolutional neural network
(CNN) was also implemented to compare the performance of SCNN with the same
network topologies and training strategies. Our SCNN was trained from scratch
instead of being converted from the CNN. We tested the three models in dark and
ambient light (AL)-corrupted environments. The results indicate that SCNN
achieves comparable accuracy (90.8%) to CNN (92.9%) and exhibits lower floating
operations with only 8 timesteps. SMLP also presents a trade-off between
computational workload and accuracy. The code and collected datasets of this
work are available at https://github.com/zzy666666zzy/TinyLiDAR_NET_SNN.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05443" title="Abstract">arXiv:2402.05443</a> [<a href="/pdf/2402.05443" title="Download PDF">pdf</a>, <a href="/format/2402.05443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Wasserstein Gradient Flow for Generative Modeling through  Unbalanced Optimal Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jaemoo Choi</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jaewoong Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+M">Myungjoo Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Wasserstein Gradient Flow (WGF) describes the gradient dynamics of
probability density within the Wasserstein space. WGF provides a promising
approach for conducting optimization over the probability distributions.
Numerically approximating the continuous WGF requires the time discretization
method. The most well-known method for this is the JKO scheme. In this regard,
previous WGF models employ the JKO scheme and parametrize transport map for
each JKO step. However, this approach results in quadratic training complexity
$O(K^2)$ with the number of JKO step $K$. This severely limits the scalability
of WGF models. In this paper, we introduce a scalable WGF-based generative
model, called Semi-dual JKO (S-JKO). Our model is based on the semi-dual form
of the JKO step, derived from the equivalence between the JKO step and the
Unbalanced Optimal Transport. Our approach reduces the training complexity to
$O(K)$. We demonstrate that our model significantly outperforms existing
WGF-based generative models, achieving FID scores of 2.62 on CIFAR-10 and 6.19
on CelebA-HQ-256, which are comparable to state-of-the-art image generative
models.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05445" title="Abstract">arXiv:2402.05445</a> [<a href="/pdf/2402.05445" title="Download PDF">pdf</a>, <a href="/format/2402.05445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accurate LoRA-Finetuning Quantization of LLMs via Information Retention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+H">Haotong Qin</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xudong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xingyu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shouda Liu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jie Luo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xianglong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Magno%2C+M">Michele Magno</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">The LoRA-finetuning quantization of LLMs has been extensively studied to
obtain accurate yet compact LLMs for deployment on resource-constrained
hardware. However, existing methods cause the quantized LLM to severely degrade
and even fail to benefit from the finetuning of LoRA. This paper proposes a
novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate
through information retention. The proposed IR-QLoRA mainly relies on two
technologies derived from the perspective of unified information: (1)
statistics-based Information Calibration Quantization allows the quantized
parameters of LLM to retain original information accurately; (2)
finetuning-based Information Elastic Connection makes LoRA utilizes elastic
representation transformation with diverse information. Comprehensive
experiments show that IR-QLoRA can significantly improve accuracy across LLaMA
and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%
improvement on MMLU compared with the state-of-the-art methods. The significant
performance gain requires only a tiny 0.31% additional time consumption,
revealing the satisfactory efficiency of our IRQLoRA. We highlight that
IR-QLoRA enjoys excellent versatility, compatible with various frameworks
(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.
The code is available at https://github.com/htqin/ir-qlora.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05448" title="Abstract">arXiv:2402.05448</a> [<a href="/pdf/2402.05448" title="Download PDF">pdf</a>, <a href="/format/2402.05448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minecraft-ify: Minecraft Style Image Generation with Text-guided Image  Editing for In-Game Application
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+B">Bumsoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Byun%2C+S">Sanghyun Byun</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+Y">Yonghoon Jung</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+W">Wonseop Shin</a>, 
<a href="/search/cs?searchtype=author&query=Amin%2C+S+U">Sareer UI Amin</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+S">Sanghyun Seo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 pages, 2 figures. Accepted to NeurIPS 2023 Workshop on Machine Learning for Creativity and Design
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
<p class="mathjax">In this paper, we first present the character texture generation system
\textit{Minecraft-ify}, specified to Minecraft video game toward in-game
application. Ours can generate face-focused image for texture mapping tailored
to 3D virtual character having cube manifold. While existing projects or works
only generate texture, proposed system can inverse the user-provided real
image, or generate average/random appearance from learned distribution.
Moreover, it can be manipulated with text-guidance using StyleGAN and
StyleCLIP. These features provide a more extended user experience with enlarged
freedom as a user-friendly AI-tool. Project page can be found at
https://gh-bumsookim.github.io/Minecraft-ify/
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05451" title="Abstract">arXiv:2402.05451</a> [<a href="/pdf/2402.05451" title="Download PDF">pdf</a>, <a href="/ps/2402.05451" title="Download PostScript">ps</a>, <a href="/format/2402.05451" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-degree phase transitions for detecting a planted clique in sublinear  time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mardia%2C+J">Jay Mardia</a>, 
<a href="/search/cs?searchtype=author&query=Verchand%2C+K+A">Kabir Aladin Verchand</a>, 
<a href="/search/cs?searchtype=author&query=Wein%2C+A+S">Alexander S. Wein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC); Machine Learning (stat.ML)

</div>
<p class="mathjax">We consider the problem of detecting a planted clique of size $k$ in a random
graph on $n$ vertices. When the size of the clique exceeds $\Theta(\sqrt{n})$,
polynomial-time algorithms for detection proliferate. We study faster --
namely, sublinear time -- algorithms in the high-signal regime when $k =
\Theta(n^{1/2 + \delta})$, for some $\delta &gt; 0$. To this end, we consider
algorithms that non-adaptively query a subset $M$ of entries of the adjacency
matrix and then compute a low-degree polynomial function of the revealed
entries. We prove a computational phase transition for this class of
non-adaptive low-degree algorithms: under the scaling $\lvert M \rvert =
\Theta(n^{\gamma})$, the clique can be detected when $\gamma &gt; 3(1/2 - \delta)$
but not when $\gamma &lt; 3(1/2 - \delta)$. As a result, the best known runtime
for detecting a planted clique, $\widetilde{O}(n^{3(1/2-\delta)})$, cannot be
improved without looking beyond the non-adaptive low-degree class.
<br />Our proof of the lower bound -- based on bounding the conditional low-degree
likelihood ratio -- reveals further structure in non-adaptive detection of a
planted clique. Using (a bound on) the conditional low-degree likelihood ratio
as a potential function, we show that for every non-adaptive query pattern,
there is a highly structured query pattern of the same size that is at least as
effective.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05453" title="Abstract">arXiv:2402.05453</a> [<a href="/pdf/2402.05453" title="Download PDF">pdf</a>, <a href="/format/2402.05453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenlong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+L">Lei Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+H">Huiping Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xiaofeng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Hongxin Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Machine learning models are susceptible to membership inference attacks
(MIAs), which aim to infer whether a sample is in the training set. Existing
work utilizes gradient ascent to enlarge the loss variance of training data,
alleviating the privacy risk. However, optimizing toward a reverse direction
may cause the model parameters to oscillate near local minima, leading to
instability and suboptimal performance. In this work, we propose a novel method
-- Convex-Concave Loss, which enables a high variance of training loss
distribution by gradient descent. Our method is motivated by the theoretical
analysis that convex losses tend to decrease the loss variance during training.
Thus, our key idea behind CCL is to reduce the convexity of loss functions with
a concave term. Trained with CCL, neural networks produce losses with high
variance for training data, reinforcing the defense against MIAs. Extensive
experiments demonstrate the superiority of CCL, achieving state-of-the-art
balance in the privacy-utility trade-off.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05455" title="Abstract">arXiv:2402.05455</a> [<a href="/pdf/2402.05455" title="Download PDF">pdf</a>, <a href="/format/2402.05455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models for Psycholinguistic Plausibility Pretesting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amouyal%2C+S+J">Samuel Joseph Amouyal</a>, 
<a href="/search/cs?searchtype=author&query=Meltzer-Asscher%2C+A">Aya Meltzer-Asscher</a>, 
<a href="/search/cs?searchtype=author&query=Berant%2C+J">Jonathan Berant</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In psycholinguistics, the creation of controlled materials is crucial to
ensure that research outcomes are solely attributed to the intended
manipulations and not influenced by extraneous factors. To achieve this,
psycholinguists typically pretest linguistic materials, where a common pretest
is to solicit plausibility judgments from human evaluators on specific
sentences. In this work, we investigate whether Language Models (LMs) can be
used to generate these plausibility judgements. We investigate a wide range of
LMs across multiple linguistic structures and evaluate whether their
plausibility judgements correlate with human judgements. We find that GPT-4
plausibility judgements highly correlate with human judgements across the
structures we examine, whereas other LMs correlate well with humans on commonly
used syntactic structures. We then test whether this correlation implies that
LMs can be used instead of humans for pretesting. We find that when
coarse-grained plausibility judgements are needed, this works well, but when
fine-grained judgements are necessary, even GPT-4 does not provide satisfactory
discriminative power.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05457" title="Abstract">arXiv:2402.05457</a> [<a href="/pdf/2402.05457" title="Download PDF">pdf</a>, <a href="/format/2402.05457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> It&#x27;s Never Too Late: Fusing Acoustic Information into Large Language  Models for Automatic Speech Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Ruizhe Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yuchen Hu</a>, 
<a href="/search/cs?searchtype=author&query=Siniscalchi%2C+S+M">Sabato Marco Siniscalchi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pin-Yu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chng%2C+E">Ensiong Chng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C+H">Chao-Han Huck Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR 2024, 17 pages. This work will be open sourced under MIT license
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Multimedia (cs.MM); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Recent studies have successfully shown that large language models (LLMs) can
be successfully used for generative error correction (GER) on top of the
automatic speech recognition (ASR) output. Specifically, an LLM is utilized to
carry out a direct mapping from the N-best hypotheses list generated by an ASR
system to the predicted output transcription. However, despite its
effectiveness, GER introduces extra data uncertainty since the LLM is trained
without taking into account acoustic information available in the speech
signal. In this work, we aim to overcome such a limitation by infusing acoustic
information before generating the predicted transcription through a novel late
fusion solution termed Uncertainty-Aware Dynamic Fusion (UADF). UADF is a
multimodal fusion approach implemented into an auto-regressive decoding process
and works in two stages: (i) It first analyzes and calibrates the token-level
LLM decision, and (ii) it then dynamically assimilates the information from the
acoustic modality. Experimental evidence collected from various ASR tasks shows
that UADF surpasses existing fusion mechanisms in several ways. It yields
significant improvements in word error rate (WER) while mitigating data
uncertainty issues in LLM and addressing the poor generalization relied with
sole modality during fusion. We also demonstrate that UADF seamlessly adapts to
audio-visual speech recognition.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05460" title="Abstract">arXiv:2402.05460</a> [<a href="/pdf/2402.05460" title="Download PDF">pdf</a>, <a href="/format/2402.05460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> I-FENN with Temporal Convolutional Networks: expediting the load-history  analysis of non-local gradient damage propagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pantidis%2C+P">Panos Pantidis</a>, 
<a href="/search/cs?searchtype=author&query=Eldababy%2C+H">Habiba Eldababy</a>, 
<a href="/search/cs?searchtype=author&query=Abueidda%2C+D">Diab Abueidda</a>, 
<a href="/search/cs?searchtype=author&query=Mobasher%2C+M+E">Mostafa E. Mobasher</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">In this paper, we demonstrate for the first time how the Integrated Finite
Element Neural Network (I-FENN) framework, previously proposed by the authors,
can efficiently simulate the entire loading history of non-local gradient
damage propagation. To achieve this goal, we first adopt a Temporal
Convolutional Network (TCN) as the neural network of choice to capture the
history-dependent evolution of the non-local strain in a coarsely meshed
domain. The quality of the network predictions governs the computational
performance of I-FENN, and therefore we perform an extended investigation aimed
at enhancing them. We explore a data-driven vs. physics-informed TCN setup to
arrive at an optimum network training, evaluating the network based on a
coherent set of relevant performance metrics. We address the crucial issue of
training a physics-informed network with input data that span vastly different
length scales by proposing a systematic way of input normalization and output
un-normalization. We then integrate the trained TCN within the nonlinear
iterative FEM solver and apply I-FENN to simulate the damage propagation
analysis. I-FENN is always applied in mesh idealizations different from the one
used for the TCN training, showcasing the framework's ability to be used at
progressively refined mesh resolutions. We illustrate several cases that I-FENN
completes the simulation using either a modified or a full Newton-Raphson
scheme, and we showcase its computational savings compared to both the
classical monolithic and staggered FEM solvers. We underline that we satisfy
very strict convergence criteria for every increment across the entire
simulation, providing clear evidence of the robustness and accuracy of I-FENN.
All the code and data used in this work will be made publicly available upon
publication of the article.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05466" title="Abstract">arXiv:2402.05466</a> [<a href="/pdf/2402.05466" title="Download PDF">pdf</a>, <a href="/format/2402.05466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Engineering End-to-End Remote Labs using IoT-based Retrofitting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Viswanadh%2C+K+S">K. S. Viswanadh</a>, 
<a href="/search/cs?searchtype=author&query=Gureja%2C+A">Akshit Gureja</a>, 
<a href="/search/cs?searchtype=author&query=Walchatwar%2C+N">Nagesh Walchatwar</a>, 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+R">Rishabh Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+S">Shiven Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhari%2C+S">Sachin Chaudhari</a>, 
<a href="/search/cs?searchtype=author&query=Vaidhyanathan%2C+K">Karthik Vaidhyanathan</a>, 
<a href="/search/cs?searchtype=author&query=Choppella%2C+V">Venkatesh Choppella</a>, 
<a href="/search/cs?searchtype=author&query=Bhimalapuram%2C+P">Prabhakar Bhimalapuram</a>, 
<a href="/search/cs?searchtype=author&query=Kandath%2C+H">Harikumar Kandath</a>, 
<a href="/search/cs?searchtype=author&query=Hussain%2C+A">Aftab Hussain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 7 tables and 20 figures. Submitted to ACM Transactions on IoT
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Remote labs are a groundbreaking development in the education industry,
providing students with access to laboratory education anytime, anywhere.
However, most remote labs are costly and difficult to scale, especially in
developing countries. With this as a motivation, this paper proposes a new
remote labs (RLabs) solution that includes two use case experiments: Vanishing
Rod and Focal Length. The hardware experiments are built at a low-cost by
retrofitting Internet of Things (IoT) components. They are also made portable
by designing miniaturised and modular setups. The software architecture
designed as part of the solution seamlessly supports the scalability of the
experiments, offering compatibility with a wide range of hardware devices and
IoT platforms. Additionally, it can live-stream remote experiments without
needing dedicated server space for the stream. The software architecture also
includes an automation suite that periodically checks the status of the
experiments using computer vision (CV). RLabs is qualitatively evaluated
against seven non-functional attributes - affordability, portability,
scalability, compatibility, maintainability, usability, and universality.
Finally, user feedback was collected from a group of students, and the scores
indicate a positive response to the students' learning and the platform's
usability.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05467" title="Abstract">arXiv:2402.05467</a> [<a href="/pdf/2402.05467" title="Download PDF">pdf</a>, <a href="/format/2402.05467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation  and Echopraxia
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+G">Guangyu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Siyuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaiyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+G">Guanhong Tao</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+S">Shengwei An</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+L">Lu Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shiqing Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiangyu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Large Language Models (LLMs) have become prevalent across diverse sectors,
transforming human life with their extraordinary reasoning and comprehension
abilities. As they find increased use in sensitive tasks, safety concerns have
gained widespread attention. Extensive efforts have been dedicated to aligning
LLMs with human moral principles to ensure their safe deployment. Despite their
potential, recent research indicates aligned LLMs are prone to specialized
jailbreaking prompts that bypass safety measures to elicit violent and harmful
content. The intrinsic discrete nature and substantial scale of contemporary
LLMs pose significant challenges in automatically generating diverse,
efficient, and potent jailbreaking prompts, representing a continuous obstacle.
In this paper, we introduce RIPPLE (Rapid Optimization via Subconscious
Exploitation and Echopraxia), a novel optimization-based method inspired by two
psychological concepts: subconsciousness and echopraxia, which describe the
processes of the mind that occur without conscious awareness and the
involuntary mimicry of actions, respectively. Evaluations across 6 open-source
LLMs and 4 commercial LLM APIs show RIPPLE achieves an average Attack Success
Rate of 91.5\%, outperforming five current methods by up to 47.0\% with an 8x
reduction in overhead. Furthermore, it displays significant transferability and
stealth, successfully evading established detection mechanisms. The code of our
work is available at
\url{https://github.com/SolidShen/RIPPLE_official/tree/official}
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05468" title="Abstract">arXiv:2402.05468</a> [<a href="/pdf/2402.05468" title="Download PDF">pdf</a>, <a href="/format/2402.05468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit Diffusion: Efficient Optimization through Stochastic Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marion%2C+P">Pierre Marion</a>, 
<a href="/search/cs?searchtype=author&query=Korba%2C+A">Anna Korba</a>, 
<a href="/search/cs?searchtype=author&query=Bartlett%2C+P">Peter Bartlett</a>, 
<a href="/search/cs?searchtype=author&query=Blondel%2C+M">Mathieu Blondel</a>, 
<a href="/search/cs?searchtype=author&query=De+Bortoli%2C+V">Valentin De Bortoli</a>, 
<a href="/search/cs?searchtype=author&query=Doucet%2C+A">Arnaud Doucet</a>, 
<a href="/search/cs?searchtype=author&query=Llinares-L%C3%B3pez%2C+F">Felipe Llinares-L&#xf3;pez</a>, 
<a href="/search/cs?searchtype=author&query=Paquette%2C+C">Courtney Paquette</a>, 
<a href="/search/cs?searchtype=author&query=Berthet%2C+Q">Quentin Berthet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We present a new algorithm to optimize distributions defined implicitly by
parameterized stochastic diffusions. Doing so allows us to modify the outcome
distribution of sampling processes by optimizing over their parameters. We
introduce a general framework for first-order optimization of these processes,
that performs jointly, in a single loop, optimization and sampling steps. This
approach is inspired by recent advances in bilevel optimization and automatic
implicit differentiation, leveraging the point of view of sampling as
optimization over the space of probability distributions. We provide
theoretical guarantees on the performance of our method, as well as
experimental results demonstrating its effectiveness in real-world settings.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05472" title="Abstract">arXiv:2402.05472</a> [<a href="/pdf/2402.05472" title="Download PDF">pdf</a>, <a href="/format/2402.05472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Question Aware Vision Transformer for Multimodal Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ganz%2C+R">Roy Ganz</a>, 
<a href="/search/cs?searchtype=author&query=Kittenplon%2C+Y">Yair Kittenplon</a>, 
<a href="/search/cs?searchtype=author&query=Aberdam%2C+A">Aviad Aberdam</a>, 
<a href="/search/cs?searchtype=author&query=Avraham%2C+E+B">Elad Ben Avraham</a>, 
<a href="/search/cs?searchtype=author&query=Nuriel%2C+O">Oren Nuriel</a>, 
<a href="/search/cs?searchtype=author&query=Mazor%2C+S">Shai Mazor</a>, 
<a href="/search/cs?searchtype=author&query=Litman%2C+R">Ron Litman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Vision-Language (VL) models have gained significant research focus, enabling
remarkable advances in multimodal reasoning. These architectures typically
comprise a vision encoder, a Large Language Model (LLM), and a projection
module that aligns visual features with the LLM's representation space. Despite
their success, a critical limitation persists: the vision encoding process
remains decoupled from user queries, often in the form of image-related
questions. Consequently, the resulting visual features may not be optimally
attuned to the query-specific elements of the image. To address this, we
introduce QA-ViT, a Question Aware Vision Transformer approach for multimodal
reasoning, which embeds question awareness directly within the vision encoder.
This integration results in dynamic visual features focusing on relevant image
aspects to the posed question. QA-ViT is model-agnostic and can be incorporated
efficiently into any VL architecture. Extensive experiments demonstrate the
effectiveness of applying our method to various multimodal architectures,
leading to consistent improvement across diverse tasks and showcasing its
potential for enhancing visual and scene-text understanding.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05476" title="Abstract">arXiv:2402.05476</a> [<a href="/pdf/2402.05476" title="Download PDF">pdf</a>, <a href="/format/2402.05476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Timescale Ensemble Q-learning for Markov Decision Process Policy  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bozkus%2C+T">Talha Bozkus</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+U">Urbashi Mitra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Reinforcement learning (RL) is a classical tool to solve network control or
policy optimization problems in unknown environments. The original Q-learning
suffers from performance and complexity challenges across very large networks.
Herein, a novel model-free ensemble reinforcement learning algorithm which
adapts the classical Q-learning is proposed to handle these challenges for
networks which admit Markov decision process (MDP) models. Multiple Q-learning
algorithms are run on multiple, distinct, synthetically created and
structurally related Markovian environments in parallel; the outputs are fused
using an adaptive weighting mechanism based on the Jensen-Shannon divergence
(JSD) to obtain an approximately optimal policy with low complexity. The
theoretical justification of the algorithm, including the convergence of key
statistics and Q-functions are provided. Numerical results across several
network models show that the proposed algorithm can achieve up to 55% less
average policy error with up to 50% less runtime complexity than the
state-of-the-art Q-learning algorithms. Numerical results validate assumptions
made in the theoretical analysis.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05480" title="Abstract">arXiv:2402.05480</a> [<a href="/pdf/2402.05480" title="Download PDF">pdf</a>, <a href="/ps/2402.05480" title="Download PostScript">ps</a>, <a href="/format/2402.05480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kontextbasierte Aktivit&#xe4;tserkennung -- Synergie von Mensch und Technik  in der Social Networked Industry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niemann%2C+F">Friedrich Niemann</a>, 
<a href="/search/cs?searchtype=author&query=Reining%2C+C">Christopher Reining</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> in German language. 30. Deutscher Materialfluss-Kongress 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">In a social networked industry, the focus is on collaboration between humans
and technology. Communication is the basic prerequisite for synergetic
collaboration between all players. It includes non-verbal as well as verbal
interactions. To enable non-verbal interaction, machines must be able to detect
and understand human movements. This article presents the ongoing fundamental
research on the analysis of human movements using sensor-based activity
recognition and identifies potential for a transfer to industrial applications.
The focus is on the practical feasibility of activity recognition by adding
further data streams such as the position data of logistical objects and tools,
meaning the context in which a certain activity is carried out.
<br />--
<br />In der Social Networked Industry steht die Zusammenarbeit von Mensch und
Technik im Vordergrund. Grundvoraussetzung f\"ur eine synergetische
Zusammenarbeit aller Akteure ist die Kommunikation, welche neben verbalen auch
nonverbale Interaktionen umfasst. Um eine nonverbale Interaktion zu
erm\"oglichen, m\"ussen Maschinen in der Lage sein, menschliche Bewegungen zu
erfassen und zu verstehen. Dieser Beitrag stellt die laufende
Grundlagenforschung zur Analyse menschlicher Bewegungen mittels
sensorgest\"utzter Aktivit\"atserkennung vor und zeigt Ankn\"upfungspunkte
f\"ur einen Transfer in industrielle Anwendungen. Im Fokus steht die
Praxistauglichkeit der Aktivit\"atserkennung durch die Hinzunahme weiterer
Datenstr\"ome wie beispielsweise den Positionsdaten logistischer Objekte und
Hilfsmitteln, d. h. dem Kontext, in dem eine gewisse Aktivit\"at ausgef\"uhrt
wird.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05483" title="Abstract">arXiv:2402.05483</a> [<a href="/pdf/2402.05483" title="Download PDF">pdf</a>, <a href="/format/2402.05483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconsidering the performance of DEVS modeling and simulation  environments using the DEVStone benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Risco-Mart%C3%ADn%2C+J+L">Jos&#xe9; L. Risco-Mart&#xed;n</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+S">Saurabh Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Fabero%2C+J+C">Juan Carlos Fabero</a>, 
<a href="/search/cs?searchtype=author&query=Zapater%2C+M">Marina Zapater</a>, 
<a href="/search/cs?searchtype=author&query=Hermida%2C+R">Rom&#xe1;n Hermida</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> SIMULATION, 93(6), 2017
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Performance (cs.PF)</span>

</div>
<p class="mathjax">The Discrete Event System Specification formalism (DEVS), which supports
hierarchical and modular model composition, has been widely used to understand,
analyze and develop a variety of systems. DEVS has been implemented in various
languages and platforms over the years. The DEVStone benchmark was conceived to
generate a set of models with varied structure and behavior, and to automate
the evaluation of the performance of DEVS-based simulators. However, DEVStone
is still in a preliminar phase and more model analysis is required. In this
paper, we revisit DEVStone introducing new equations to compute the number of
events triggered. We also introduce a new benchmark, called HOmem, designed as
an alternative version of HOmod, with similar CPU and memory requirements, but
with an easier implementation and analytically more manageable. Finally, we
compare both the performance and memory footprint of five different DEVS
simulators in two different hardware platforms.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05484" title="Abstract">arXiv:2402.05484</a> [<a href="/pdf/2402.05484" title="Download PDF">pdf</a>, <a href="/ps/2402.05484" title="Download PostScript">ps</a>, <a href="/format/2402.05484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging AI for Enhanced Software Effort Estimation: A Comprehensive  Study and Framework Proposal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tran%2C+N">Nhi Tran</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+T">Tan Tran</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+N">Nam Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper presents an extensive study on the application of AI techniques
for software effort estimation in the past five years from 2017 to 2023. By
overcoming the limitations of traditional methods, the study aims to improve
accuracy and reliability. Through performance evaluation and comparison with
diverse Machine Learning models, including Artificial Neural Network (ANN),
Support Vector Machine (SVM), Linear Regression, Random Forest and other
techniques, the most effective method is identified. The proposed AI-based
framework holds the potential to enhance project planning and resource
allocation, contributing to the research area of software project effort
estimation.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05489" title="Abstract">arXiv:2402.05489</a> [<a href="/pdf/2402.05489" title="Download PDF">pdf</a>, <a href="/ps/2402.05489" title="Download PostScript">ps</a>, <a href="/format/2402.05489" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multispecies bird sound recognition using a fully convolutional neural  network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Ord%C3%A1s%2C+M+T">Mar&#xed;a Teresa Garc&#xed;a-Ord&#xe1;s</a>, 
<a href="/search/cs?searchtype=author&query=Rubio-Mart%C3%ADn%2C+S">Sergio Rubio-Mart&#xed;n</a>, 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>, 
<a href="/search/cs?searchtype=author&query=Alaiz-Moret%C3%B3n%2C+H">Hector Alaiz-Moret&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Rodr%C3%ADguez%2C+I">Isa&#xed;as Garc&#xed;a-Rodr&#xed;guez</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Applied Intelligence, Volume 53, July 2023, pp. 23287 - 23300
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This study proposes a method based on fully convolutional neural networks
(FCNs) to identify migratory birds from their songs, with the objective of
recognizing which birds pass through certain areas and at what time. To
determine the best FCN architecture, extensive experimentation was conducted
through a grid search, exploring the optimal depth, width, and activation
function of the network. The results showed that the optimal number of filters
is 400 in the widest layer, with 4 convolutional blocks with maxpooling and an
adaptive activation function. The proposed FCN offers a significant advantage
over other techniques, as it can recognize the sound of a bird in audio of any
length with an accuracy greater than 85%. Furthermore, due to its architecture,
the network can detect more than one species from audio and can carry out
near-real-time sound recognition. Additionally, the proposed method is
lightweight, making it ideal for deployment and use in IoT devices. The study
also presents a comparative analysis of the proposed method against other
techniques, demonstrating an improvement of over 67% in the best-case scenario.
These findings contribute to advancing the field of bird sound recognition and
provide valuable insights into the practical application of FCNs in real-world
scenarios.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05491" title="Abstract">arXiv:2402.05491</a> [<a href="/pdf/2402.05491" title="Download PDF">pdf</a>, <a href="/ps/2402.05491" title="Download PostScript">ps</a>, <a href="/format/2402.05491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Determining the severity of Parkinson&#x27;s disease in patients using a  multi task neural network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Ord%C3%A1s%2C+M+T">Mar&#xed;a Teresa Garc&#xed;a-Ord&#xe1;s</a>, 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>, 
<a href="/search/cs?searchtype=author&query=Aveleira-Mata%2C+J">Jose Aveleira-Mata</a>, 
<a href="/search/cs?searchtype=author&query=Alija-P%C3%A9rez%2C+J">Jos&#xe9;-Manuel Alija-P&#xe9;rez</a>, 
<a href="/search/cs?searchtype=author&query=Benavides%2C+C">Carmen Benavides</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Multimedia Tools and Applications, Volume 83, pages 6077-6092,
  2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Parkinson's disease is easy to diagnose when it is advanced, but it is very
difficult to diagnose in its early stages. Early diagnosis is essential to be
able to treat the symptoms. It impacts on daily activities and reduces the
quality of life of both the patients and their families and it is also the
second most prevalent neurodegenerative disorder after Alzheimer in people over
the age of 60. Most current studies on the prediction of Parkinson's severity
are carried out in advanced stages of the disease. In this work, the study
analyzes a set of variables that can be easily extracted from voice analysis,
making it a very non-intrusive technique. In this paper, a method based on
different deep learning techniques is proposed with two purposes. On the one
hand, to find out if a person has severe or non-severe Parkinson's disease, and
on the other hand, to determine by means of regression techniques the degree of
evolution of the disease in a given patient. The UPDRS (Unified Parkinson's
Disease Rating Scale) has been used by taking into account both the motor and
total labels, and the best results have been obtained using a mixed multi-layer
perceptron (MLP) that classifies and regresses at the same time and the most
important features of the data obtained are taken as input, using an
autoencoder. A success rate of 99.15% has been achieved in the problem of
predicting whether a person suffers from severe Parkinson's disease or
non-severe Parkinson's disease. In the degree of disease involvement prediction
problem case, a MSE (Mean Squared Error) of 0.15 has been obtained. Using a
full deep learning pipeline for data preprocessing and classification has
proven to be very promising in the field Parkinson's outperforming the
state-of-the-art proposals.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05493" title="Abstract">arXiv:2402.05493</a> [<a href="/pdf/2402.05493" title="Download PDF">pdf</a>, <a href="/format/2402.05493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating White-Box Attacks for On-Device Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mingyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xiang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hailong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Li Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The International Conference on Software Engineering 2024 (ICSE'24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Numerous mobile apps have leveraged deep learning capabilities. However,
on-device models are vulnerable to attacks as they can be easily extracted from
their corresponding mobile apps. Existing on-device attacking approaches only
generate black-box attacks, which are far less effective and efficient than
white-box strategies. This is because mobile deep learning frameworks like
TFLite do not support gradient computing, which is necessary for white-box
attacking algorithms. Thus, we argue that existing findings may underestimate
the harmfulness of on-device attacks. To this end, we conduct a study to answer
this research question: Can on-device models be directly attacked via white-box
strategies? We first systematically analyze the difficulties of transforming
the on-device model to its debuggable version, and propose a Reverse
Engineering framework for On-device Models (REOM), which automatically reverses
the compiled on-device TFLite model to the debuggable model. Specifically, REOM
first transforms compiled on-device models into Open Neural Network Exchange
format, then removes the non-debuggable parts, and converts them to the
debuggable DL models format that allows attackers to exploit in a white-box
setting. Our experimental results show that our approach is effective in
achieving automated transformation among 244 TFLite models. Compared with
previous attacks using surrogate models, REOM enables attackers to achieve
higher attack success rates with a hundred times smaller attack perturbations.
In addition, because the ONNX platform has plenty of tools for model format
exchanging, the proposed method based on the ONNX platform can be adapted to
other model formats. Our findings emphasize the need for developers to
carefully consider their model deployment strategies, and use white-box methods
to evaluate the vulnerability of on-device models.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05495" title="Abstract">arXiv:2402.05495</a> [<a href="/pdf/2402.05495" title="Download PDF">pdf</a>, <a href="/ps/2402.05495" title="Download PostScript">ps</a>, <a href="/format/2402.05495" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Heart disease risk prediction using deep learning techniques with  feature augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Ord%C3%A1s%2C+M+T">Mar&#xed;a Teresa Garc&#xed;a-Ord&#xe1;s</a>, 
<a href="/search/cs?searchtype=author&query=Bay%C3%B3n-Guti%C3%A9rrez%2C+M">Mart&#xed;n Bay&#xf3;n-Guti&#xe9;rrez</a>, 
<a href="/search/cs?searchtype=author&query=Benavides%2C+C">Carmen Benavides</a>, 
<a href="/search/cs?searchtype=author&query=Aveleira-Mata%2C+J">Jose Aveleira-Mata</a>, 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Multimedia Tools and Applications, Volume 82, pp. 31759 - 31773,
  August 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Cardiovascular diseases state as one of the greatest risks of death for the
general population. Late detection in heart diseases highly conditions the
chances of survival for patients. Age, sex, cholesterol level, sugar level,
heart rate, among other factors, are known to have an influence on
life-threatening heart problems, but, due to the high amount of variables, it
is often difficult for an expert to evaluate each patient taking this
information into account. In this manuscript, the authors propose using deep
learning methods, combined with feature augmentation techniques for evaluating
whether patients are at risk of suffering cardiovascular disease. The results
of the proposed methods outperform other state of the art methods by 4.4%,
leading to a precision of a 90%, which presents a significant improvement, even
more so when it comes to an affliction that affects a large population.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05498" title="Abstract">arXiv:2402.05498</a> [<a href="/pdf/2402.05498" title="Download PDF">pdf</a>, <a href="/ps/2402.05498" title="Download PostScript">ps</a>, <a href="/format/2402.05498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Solution for Commercializing, Decentralizing and Storing Electronic  Medical Records by Integrating Proxy Re-Encryption, IPFS, and Blockchain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tran%2C+P">Phong Tran</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Thong Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+L">Long Chu</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+N">Nhi Tran</a>, 
<a href="/search/cs?searchtype=author&query=Ta%2C+H">Hang Ta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The rapid expansion of user medical records across global systems presents
not only opportunities but also new challenges in maintaining effective
application models that ensure user privacy, controllability, and the ability
to commercialize patient medical records. Moreover, the proliferation of data
analysis models in healthcare institutions necessitates the decentralization
and restorability of medical record data. It is imperative that user medical
data collected from these systems can be easily analyzed and utilized even
years after collection, without the risk of data loss due to numerous factors.
Additionally, medical information must be authorized by the data owner,
granting patients the right to accept or decline data usage requests from
medical research agencies. In response, we propose an innovative solution for
implementing a decentralized system utilizing an EVM-compatible blockchain and
IPFS for decentralized storage. To ensure privacy and control, we employ Proxy
Re-Encryption (PRE), a cryptographic authorized method, within the medical data
marketplace. Our proposed architecture significantly reduces costs associated
with granting read access to healthcare research agencies by minimizing the
encryption and decryption time of stored records. Furthermore, it empowers
users with enhanced control over their health data through tamperproof
blockchain smart contracts and IPFS, safeguarding the integrity and privacy of
their medical records.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05499" title="Abstract">arXiv:2402.05499</a> [<a href="/pdf/2402.05499" title="Download PDF">pdf</a>, <a href="/ps/2402.05499" title="Download PostScript">ps</a>, <a href="/format/2402.05499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sustainable allocation of greenhouse gas emission permits for firms with  Leontief technologies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gutierrez%2C+E">Elisabeth Gutierrez</a>, 
<a href="/search/cs?searchtype=author&query=Llorca%2C+N">Natividad Llorca</a>, 
<a href="/search/cs?searchtype=author&query=Sanchez-Soriano%2C+J">Joaquin Sanchez-Soriano</a>, 
<a href="/search/cs?searchtype=author&query=Mosquera%2C+M+A">Manuel A. Mosquera</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> European Journal of Operational Research 269 (2018) 5-15
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Numerical Analysis (math.NA); Optimization and Control (math.OC)

</div>
<p class="mathjax">In this paper we deal with production situations where a cap or limit to the
amount of greenhouse gas emissions permitted is imposed. Fixing a tax for each
ton of pollutant emitted is also considered. We use bankruptcy rules to define
cooperative games with externalities associated with these situations and
analyze the existence of coalitionally stable allocations of the emission
permits. We prove that the constrained equal awards ( CEA ) rule provides
stable allocations and as a direct mechanism, it is incentive compatible. These
two facts have interesting managerial implications to control pollution
emissions.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05502" title="Abstract">arXiv:2402.05502</a> [<a href="/pdf/2402.05502" title="Download PDF">pdf</a>, <a href="/format/2402.05502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Optimal Control Formulation of Tool Affordance Applied to Impact  Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ti%2C+B">Boyang Ti</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yongsheng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jie Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Calinon%2C+S">Sylvain Calinon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 16 figures, journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Humans use tools to complete impact-aware tasks such as hammering a nail or
playing tennis. The postures adopted to use these tools can significantly
influence the performance of these tasks, where the force or velocity of the
hand holding a tool plays a crucial role. The underlying motion planning
challenge consists of grabbing the tool in preparation for the use of this tool
with an optimal body posture. Directional manipulability describes the
dexterity of force and velocity in a joint configuration along a specific
direction. In order to take directional manipulability and tool affordances
into account, we apply an optimal control method combining iterative linear
quadratic regulator(iLQR) with the alternating direction method of
multipliers(ADMM). Our approach considers the notion of tool affordances to
solve motion planning problems, by introducing a cost based on directional
velocity manipulability. The proposed approach is applied to impact tasks in
simulation and on a real 7-axis robot, specifically in a nail-hammering task
with the assistance of a pilot hole. Our comparison study demonstrates the
importance of maximizing directional manipulability in impact-aware tasks.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05507" title="Abstract">arXiv:2402.05507</a> [<a href="/pdf/2402.05507" title="Download PDF">pdf</a>, <a href="/format/2402.05507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SeAr PC: Sensitivity Enhanced Arbitrary Polynomial Chaos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Pepper%2C+N">Nick Pepper</a>, 
<a href="/search/math?searchtype=author&query=Montomoli%2C+F">Francesco Montomoli</a>, 
<a href="/search/math?searchtype=author&query=Kantarakias%2C+K">Kyriakos Kantarakias</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This paper presents a method for performing Uncertainty Quantification in
high-dimensional uncertain spaces by combining arbitrary polynomial chaos with
a recently proposed scheme for sensitivity enhancement (1). Including available
sensitivity information offers a way to mitigate the curse of dimensionality in
Polynomial Chaos Expansions (PCEs). Coupling the sensitivity enhancement to
arbitrary Polynomial Chaos allows the formulation to be extended to a wide
range of stochastic processes, including multi-modal, fat-tailed, and truncated
probability distributions. In so doing, this work addresses two of the barriers
to widespread industrial application of PCEs. The method is demonstrated for a
number of synthetic test cases, including an uncertainty analysis of a Finite
Element structure, determined using Topology Optimisation, with 306 uncertain
inputs. We demonstrate that by exploiting sensitivity information, PCEs can
feasibly be applied to such problems and through the Sobol sensitivity indices,
can allow a designer to easily visualise the spatial distribution of the
contributions to uncertainty in the structure.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05508" title="Abstract">arXiv:2402.05508</a> [<a href="/pdf/2402.05508" title="Download PDF">pdf</a>, <a href="/ps/2402.05508" title="Download PostScript">ps</a>, <a href="/format/2402.05508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Evaluation of Associative Watermarking Using Statistical  Neurodynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kanegae%2C+R">Ryoto Kanegae</a>, 
<a href="/search/cs?searchtype=author&query=Kawamura%2C+M">Masaki Kawamura</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Statistical Mechanics (cond-mat.stat-mech)

</div>
<p class="mathjax">We theoretically evaluated the performance of our proposed associative
watermarking method in which the watermark is not embedded directly into the
image. We previously proposed a watermarking method that extends the
zero-watermarking model by applying associative memory models. In this model,
the hetero-associative memory model is introduced to the mapping process
between image features and watermarks, and the auto-associative memory model is
applied to correct watermark errors. We herein show that the associative
watermarking model outperforms the zero-watermarking model through computer
simulations using actual images. In this paper, we describe how we derive the
macroscopic state equation for the associative watermarking model using the
Okada theory. The theoretical results obtained by the fourth-order theory were
in good agreement with those obtained by computer simulations. Furthermore, the
performance of the associative watermarking model was evaluated using the bit
error rate of the watermark, both theoretically and using computer simulations.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05512" title="Abstract">arXiv:2402.05512</a> [<a href="/pdf/2402.05512" title="Download PDF">pdf</a>, <a href="/format/2402.05512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GPTs Are Multilingual Annotators for Sequence Generation Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Juhwan Choi</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+E">Eunju Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+K">Kyohoon Jin</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">YoungBin Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024 Findings: Camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Data annotation is an essential step for constructing new datasets. However,
the conventional approach of data annotation through crowdsourcing is both
time-consuming and expensive. In addition, the complexity of this process
increases when dealing with low-resource languages owing to the difference in
the language pool of crowdworkers. To address these issues, this study proposes
an autonomous annotation method by utilizing large language models, which have
been recently demonstrated to exhibit remarkable performance. Through our
experiments, we demonstrate that the proposed method is not just cost-efficient
but also applicable for low-resource language annotation. Additionally, we
constructed an image captioning dataset using our approach and are committed to
open this dataset for future study. We have opened our source code for further
study and reproducibility.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05515" title="Abstract">arXiv:2402.05515</a> [<a href="/pdf/2402.05515" title="Download PDF">pdf</a>, <a href="/format/2402.05515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NoisyICL: A Little Noise in Model Parameters Calibrates In-context  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yufeng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Sakai%2C+Y">Yoshihiro Sakai</a>, 
<a href="/search/cs?searchtype=author&query=Inoue%2C+N">Naoya Inoue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 28 figures, 7 tables (5 pages, 4 figures, 1 table in main body)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In-Context Learning (ICL) is suffering from unsatisfactory performance and
under-calibration due to high prior bias and unfaithful confidence. Some
previous works fine-tuned language models for better ICL performance with
enormous datasets and computing costs. In this paper, we propose NoisyICL,
simply perturbing the model parameters by random noises to strive for better
performance and calibration. Our experiments on 2 models and 12 downstream
datasets show that NoisyICL can help ICL produce more accurate predictions. Our
further analysis indicates that NoisyICL enables the model to provide more fair
predictions, and also with less unfaithful confidence. Therefore, we believe
that NoisyICL is an effective calibration of ICL. Our experimental code is
uploaded to Github.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05519" title="Abstract">arXiv:2402.05519</a> [<a href="/pdf/2402.05519" title="Download PDF">pdf</a>, <a href="/ps/2402.05519" title="Download PostScript">ps</a>, <a href="/format/2402.05519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can ChatGPT evaluate research quality?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thelwall%2C+M">Mike Thelwall</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Purpose: Assess whether ChatGPT 4.0 is accurate enough to perform research
evaluations on journal articles to automate this time-consuming task.
Design/methodology/approach: Test the extent to which ChatGPT-4 can assess the
quality of journal articles using a case study of the published scoring
guidelines of the UK Research Excellence Framework (REF) 2021 to create a
research evaluation ChatGPT. This was applied to 51 of my own articles and
compared against my own quality judgements. Findings: ChatGPT-4 can produce
plausible document summaries and quality evaluation rationales that match the
REF criteria. Its overall scores have weak correlations with my self-evaluation
scores of the same documents (averaging r=0.281 over 15 iterations, with 8
being statistically significantly different from 0). In contrast, the average
scores from the 15 iterations produced a statistically significant positive
correlation of 0.509. Thus, averaging scores from multiple ChatGPT-4 rounds
seems more effective than individual scores. The positive correlation may be
due to ChatGPT being able to extract the author's significance, rigour, and
originality claims from inside each paper. If my weakest articles are removed,
then the correlation with average scores (r=0.200) falls below statistical
significance, suggesting that ChatGPT struggles to make fine-grained
evaluations. Research limitations: The data is self-evaluations of a
convenience sample of articles from one academic in one field. Practical
implications: Overall, ChatGPT does not yet seem to be accurate enough to be
trusted for any formal or informal research quality evaluation tasks. Research
evaluators, including journal editors, should therefore take steps to control
its use. Originality/value: This is the first published attempt at
post-publication expert review accuracy testing for ChatGPT.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05521" title="Abstract">arXiv:2402.05521</a> [<a href="/pdf/2402.05521" title="Download PDF">pdf</a>, <a href="/format/2402.05521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linearizing Models for Efficient yet Robust Private Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+S">Sreetama Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Kundu%2C+S">Souvik Kundu</a>, 
<a href="/search/cs?searchtype=author&query=Beerel%2C+P+A">Peter A. Beerel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The growing concern about data privacy has led to the development of private
inference (PI) frameworks in client-server applications which protects both
data privacy and model IP. However, the cryptographic primitives required yield
significant latency overhead which limits its wide-spread application. At the
same time, changing environments demand the PI service to be robust against
various naturally occurring and gradient-based perturbations. Despite several
works focused on the development of latency-efficient models suitable for PI,
the impact of these models on robustness has remained unexplored. Towards this
goal, this paper presents RLNet, a class of robust linearized networks that can
yield latency improvement via reduction of high-latency ReLU operations while
improving the model performance on both clean and corrupted images. In
particular, RLNet models provide a "triple win ticket" of improved
classification accuracy on clean, naturally perturbed, and gradient-based
perturbed images using a shared-mask shared-weight architecture with over an
order of magnitude fewer ReLUs than baseline models. To demonstrate the
efficacy of RLNet, we perform extensive experiments with ResNet and WRN model
variants on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets. Our experimental
evaluations show that RLNet can yield models with up to 11.14x fewer ReLUs,
with accuracy close to the all-ReLU models, on clean, naturally perturbed, and
gradient-based perturbed images. Compared with the SoTA non-robust linearized
models at similar ReLU budgets, RLNet achieves an improvement in adversarial
accuracy of up to ~47%, naturally perturbed accuracy up to ~16.4%, while
improving clean image accuracy up to ~1.5%.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05525" title="Abstract">arXiv:2402.05525</a> [<a href="/pdf/2402.05525" title="Download PDF">pdf</a>, <a href="/format/2402.05525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Model-Based Offline Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rio%2C+A">Alexandre Rio</a>, 
<a href="/search/cs?searchtype=author&query=Barlier%2C+M">Merwan Barlier</a>, 
<a href="/search/cs?searchtype=author&query=Colin%2C+I">Igor Colin</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+A">Albert Thomas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (stat.ML)

</div>
<p class="mathjax">We address offline reinforcement learning with privacy guarantees, where the
goal is to train a policy that is differentially private with respect to
individual trajectories in the dataset. To achieve this, we introduce DP-MORL,
an MBRL algorithm coming with differential privacy guarantees. A private model
of the environment is first learned from offline data using DP-FedAvg, a
training method for neural networks that provides differential privacy
guarantees at the trajectory level. Then, we use model-based policy
optimization to derive a policy from the (penalized) private model, without any
further interaction with the system or access to the input data. We empirically
show that DP-MORL enables the training of private RL agents from offline data
and we furthermore outline the price of privacy in this setting.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05526" title="Abstract">arXiv:2402.05526</a> [<a href="/pdf/2402.05526" title="Download PDF">pdf</a>, <a href="/format/2402.05526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Buffer Overflow in Mixture of Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hayes%2C+J">Jamie Hayes</a>, 
<a href="/search/cs?searchtype=author&query=Shumailov%2C+I">Ilia Shumailov</a>, 
<a href="/search/cs?searchtype=author&query=Yona%2C+I">Itay Yona</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Mixture of Experts (MoE) has become a key ingredient for scaling large
foundation models while keeping inference costs steady. We show that expert
routing strategies that have cross-batch dependencies are vulnerable to
attacks. Malicious queries can be sent to a model and can affect a model's
output on other benign queries if they are grouped in the same batch. We
demonstrate this via a proof-of-concept attack in a toy experimental setting.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05529" title="Abstract">arXiv:2402.05529</a> [<a href="/pdf/2402.05529" title="Download PDF">pdf</a>, <a href="/format/2402.05529" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asynchronous Diffusion Learning with Agent Subsampling and Local Updates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rizk%2C+E">Elsa Rizk</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+K">Kun Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Sayed%2C+A+H">Ali H. Sayed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">In this work, we examine a network of agents operating asynchronously, aiming
to discover an ideal global model that suits individual local datasets. Our
assumption is that each agent independently chooses when to participate
throughout the algorithm and the specific subset of its neighbourhood with
which it will cooperate at any given moment. When an agent chooses to take
part, it undergoes multiple local updates before conveying its outcomes to the
sub-sampled neighbourhood. Under this setup, we prove that the resulting
asynchronous diffusion strategy is stable in the mean-square error sense and
provide performance guarantees specifically for the federated learning setting.
We illustrate the findings with numerical simulations.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05532" title="Abstract">arXiv:2402.05532</a> [<a href="/pdf/2402.05532" title="Download PDF">pdf</a>, <a href="/format/2402.05532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NCRF: Neural Contact Radiance Fields for Free-Viewpoint Rendering of  Hand-Object Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhongqun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jifei Song</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez-Pellitero%2C+E">Eduardo P&#xe9;rez-Pellitero</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yiren Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+H+J">Hyung Jin Chang</a>, 
<a href="/search/cs?searchtype=author&query=Leonardis%2C+A">Ale&#x161; Leonardis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Modeling hand-object interactions is a fundamentally challenging task in 3D
computer vision. Despite remarkable progress that has been achieved in this
field, existing methods still fail to synthesize the hand-object interaction
photo-realistically, suffering from degraded rendering quality caused by the
heavy mutual occlusions between the hand and the object, and inaccurate
hand-object pose estimation. To tackle these challenges, we present a novel
free-viewpoint rendering framework, Neural Contact Radiance Field (NCRF), to
reconstruct hand-object interactions from a sparse set of videos. In
particular, the proposed NCRF framework consists of two key components: (a) A
contact optimization field that predicts an accurate contact field from 3D
query points for achieving desirable contact between the hand and the object.
(b) A hand-object neural radiance field to learn an implicit hand-object
representation in a static canonical space, in concert with the specifically
designed hand-object motion field to produce observation-to-canonical
correspondences. We jointly learn these key components where they mutually help
and regularize each other with visual and geometric constraints, producing a
high-quality hand-object reconstruction that achieves photo-realistic novel
view synthesis. Extensive experiments on HO3D and DexYCB datasets show that our
approach outperforms the current state-of-the-art in terms of both rendering
quality and pose estimation accuracy.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05534" title="Abstract">arXiv:2402.05534</a> [<a href="/pdf/2402.05534" title="Download PDF">pdf</a>, <a href="/format/2402.05534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Parameter Fitting to Realistic Network Models via Iterative  Stochastic Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bl%C3%A4sius%2C+T">Thomas Bl&#xe4;sius</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+S">Sarel Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Fischbeck%2C+P">Philipp Fischbeck</a>, 
<a href="/search/cs?searchtype=author&query=Friedrich%2C+T">Tobias Friedrich</a>, 
<a href="/search/cs?searchtype=author&query=Krejca%2C+M+S">Martin S. Krejca</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Random graph models are widely used to understand network properties and
graph algorithms. Key to such analyses are the different parameters of each
model, which affect various network features, such as its size, clustering, or
degree distribution. The exact effect of the parameters on these features is
not well understood, mainly because we lack tools to thoroughly investigate
this relation. Moreover, the parameters cannot be considered in isolation, as
changing one affects multiple features. Existing approaches for finding the
best model parameters of desired features, such as a grid search or estimating
the parameter-feature relations, are not well suited, as they are inaccurate or
computationally expensive.
<br />We introduce an efficient iterative fitting method, named ParFit, that finds
parameters using only a few network samples, based on the Robbins-Monro
algorithm. We test ParFit on three well-known graph models, namely
Erd\H{o}s-R\'enyi, Chung-Lu, and geometric inhomogeneous random graphs, as well
as on real-world networks, including web networks. We find that ParFit performs
well in terms of quality and running time across most parameter configurations.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05535" title="Abstract">arXiv:2402.05535</a> [<a href="/pdf/2402.05535" title="Download PDF">pdf</a>, <a href="/format/2402.05535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Optimizing Deterministic Concurrent Scheduling for Smart Contracts  and Blockchains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hay%2C+Y">Yaron Hay</a>, 
<a href="/search/cs?searchtype=author&query=Friedman%2C+R">Roy Friedman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 68 pages, 31 figures, LaTeX with Auxiliary Files, short single line
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Databases (cs.DB)

</div>
<p class="mathjax">Executing smart contracts is a compute and storage-intensive task, which
currently dominates modern blockchain's performance. Given that computers are
becoming increasingly multicore, concurrency is an attractive approach to
improve programs' execution runtime. A unique challenge of blockchains is that
all replicas (minors or validators) must execute all smart contracts in the
same logical order to maintain the semantics of State Machine Replication
(SMR). While non-conflicting transactions can be executed in any actual order,
replicas need to enforce a unique logical order among all pairs of conflicting
transactions.
<br />In this work, we formally study the maximal level of parallelism obtainable
when focusing on the conflict graphs between transactions packaged in the same
block, rather than relying on the total ordering order. To that end, we
describe a generic framework for Active State Machine Replication (ASMR) that
is strictly serializable. The generic framework allows for shifting our focus
to developing efficient execution engines for transactions without introducing
non-deterministic results.
<br />Then, we suggest the concept of graph scheduling, and the minimal latency
scheduling problem, which we prove to be NP-Hard. We show that the restricted
version of the problem for homogeneous transactions is equivalent to the
classic Graph Vertex Coloring Problem, yet the heterogenous case is more
complex. We discuss practical implications of these results.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05536" title="Abstract">arXiv:2402.05536</a> [<a href="/pdf/2402.05536" title="Download PDF">pdf</a>, <a href="/ps/2402.05536" title="Download PostScript">ps</a>, <a href="/format/2402.05536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empowering machine learning models with contextual knowledge for  enhancing the detection of eating disorders in social media posts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Ord%C3%A1s%2C+M+T">Mar&#xed;a Teresa Garc&#xed;a-Ord&#xe1;s</a>, 
<a href="/search/cs?searchtype=author&query=Russo%2C+M">Mayra Russo</a>, 
<a href="/search/cs?searchtype=author&query=Sakor%2C+A">Ahmad Sakor</a>, 
<a href="/search/cs?searchtype=author&query=Rotger%2C+L+D+F">Luis Daniel Fernandes Rotger</a>, 
<a href="/search/cs?searchtype=author&query=Vidal%2C+M">Maria-Esther Vidal</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Semantic Web, Volume 4, Issue 5, pp. 873-892, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Social networks are vital for information sharing, especially in the health
sector for discussing diseases and treatments. These platforms, however, often
feature posts as brief texts, posing challenges for Artificial Intelligence
(AI) in understanding context. We introduce a novel hybrid approach combining
community-maintained knowledge graphs (like Wikidata) with deep learning to
enhance the categorization of social media posts. This method uses advanced
entity recognizers and linkers (like Falcon 2.0) to connect short post entities
to knowledge graphs. Knowledge graph embeddings (KGEs) and contextualized word
embeddings (like BERT) are then employed to create rich, context-based
representations of these posts.
<br />Our focus is on the health domain, particularly in identifying posts related
to eating disorders (e.g., anorexia, bulimia) to aid healthcare providers in
early diagnosis. We tested our approach on a dataset of 2,000 tweets about
eating disorders, finding that merging word embeddings with knowledge graph
information enhances the predictive models' reliability. This methodology aims
to assist health experts in spotting patterns indicative of mental disorders,
thereby improving early detection and accurate diagnosis for personalized
medicine.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05540" title="Abstract">arXiv:2402.05540</a> [<a href="/pdf/2402.05540" title="Download PDF">pdf</a>, <a href="/format/2402.05540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tightly Coupled Range Inertial Localization on a 3D Prior Map Based on  Sliding Window Factor Graph Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koide%2C+K">Kenji Koide</a>, 
<a href="/search/cs?searchtype=author&query=Oishi%2C+S">Shuji Oishi</a>, 
<a href="/search/cs?searchtype=author&query=Yokozuka%2C+M">Masashi Yokozuka</a>, 
<a href="/search/cs?searchtype=author&query=Banno%2C+A">Atsuhiko Banno</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE International Conference on Robotics and Automation (ICRA2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper presents a range inertial localization algorithm for a 3D prior
map. The proposed algorithm tightly couples scan-to-scan and scan-to-map point
cloud registration factors along with IMU factors on a sliding window factor
graph. The tight coupling of the scan-to-scan and scan-to-map registration
factors enables a smooth fusion of sensor ego-motion estimation and map-based
trajectory correction that results in robust tracking of the sensor pose under
severe point cloud degeneration and defective regions in a map. We also propose
an initial sensor state estimation algorithm that robustly estimates the
gravity direction and IMU state and helps perform global localization in 3- or
4-DoF for system initialization without prior position information.
Experimental results show that the proposed method outperforms existing
state-of-the-art methods in extremely severe situations where the point cloud
data becomes degenerate, there are momentary sensor interruptions, or the
sensor moves along the map boundary or into unmapped regions.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05541" title="Abstract">arXiv:2402.05541</a> [<a href="/pdf/2402.05541" title="Download PDF">pdf</a>, <a href="/format/2402.05541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning as a Catalyst for Robust and Fair Federated  Learning: Deciphering the Dynamics of Client Contributions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jialuo He</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaojin Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Recent advancements in federated learning (FL) have produced models that
retain user privacy by training across multiple decentralized devices or
systems holding local data samples. However, these strategies often neglect the
inherent challenges of statistical heterogeneity and vulnerability to
adversarial attacks, which can degrade model robustness and fairness.
Personalized FL strategies offer some respite by adjusting models to fit
individual client profiles, yet they tend to neglect server-side aggregation
vulnerabilities. To address these issues, we propose Reinforcement Federated
Learning (RFL), a novel framework that leverages deep reinforcement learning to
adaptively optimize client contribution during aggregation, thereby enhancing
both model robustness against malicious clients and fairness across
participants under non-identically distributed settings. To achieve this goal,
we propose a meticulous approach involving a Deep Deterministic Policy
Gradient-based algorithm for continuous control of aggregation weights, an
innovative client selection method based on model parameter distances, and a
reward mechanism guided by validation set performance. Empirically, extensive
experiments demonstrate that, in terms of robustness, RFL outperforms the
state-of-the-art methods, while maintaining comparable levels of fairness,
offering a promising solution to build resilient and fair federated systems.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05545" title="Abstract">arXiv:2402.05545</a> [<a href="/pdf/2402.05545" title="Download PDF">pdf</a>, <a href="/ps/2402.05545" title="Download PostScript">ps</a>, <a href="/format/2402.05545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Named Entity Recognition for Address Extraction in Speech-to-Text  Transcriptions Using Synthetic Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Laj%C4%8Dinov%C3%A1%2C+B">Bibi&#xe1;na Laj&#x10d;inov&#xe1;</a>, 
<a href="/search/cs?searchtype=author&query=Val%C3%A1bek%2C+P">Patrik Val&#xe1;bek</a>, 
<a href="/search/cs?searchtype=author&query=Spi%C5%A1iak%2C+M">Michal Spi&#x161;iak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper introduces an approach for building a Named Entity Recognition
(NER) model built upon a Bidirectional Encoder Representations from
Transformers (BERT) architecture, specifically utilizing the SlovakBERT model.
This NER model extracts address parts from data acquired from speech-to-text
transcriptions. Due to scarcity of real data, a synthetic dataset using GPT API
was generated. The importance of mimicking spoken language variability in this
artificial data is emphasized. The performance of our NER model, trained solely
on synthetic data, is evaluated using small real test dataset.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05546" title="Abstract">arXiv:2402.05546</a> [<a href="/pdf/2402.05546" title="Download PDF">pdf</a>, <a href="/format/2402.05546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Offline Actor-Critic Reinforcement Learning Scales to Large Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Springenberg%2C+J+T">Jost Tobias Springenberg</a>, 
<a href="/search/cs?searchtype=author&query=Abdolmaleki%2C+A">Abbas Abdolmaleki</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Groth%2C+O">Oliver Groth</a>, 
<a href="/search/cs?searchtype=author&query=Bloesch%2C+M">Michael Bloesch</a>, 
<a href="/search/cs?searchtype=author&query=Lampe%2C+T">Thomas Lampe</a>, 
<a href="/search/cs?searchtype=author&query=Brakel%2C+P">Philemon Brakel</a>, 
<a href="/search/cs?searchtype=author&query=Bechtle%2C+S">Sarah Bechtle</a>, 
<a href="/search/cs?searchtype=author&query=Kapturowski%2C+S">Steven Kapturowski</a>, 
<a href="/search/cs?searchtype=author&query=Hafner%2C+R">Roland Hafner</a>, 
<a href="/search/cs?searchtype=author&query=Heess%2C+N">Nicolas Heess</a>, 
<a href="/search/cs?searchtype=author&query=Riedmiller%2C+M">Martin Riedmiller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">We show that offline actor-critic reinforcement learning can scale to large
models - such as transformers - and follows similar scaling laws as supervised
learning. We find that offline actor-critic algorithms can outperform strong,
supervised, behavioral cloning baselines for multi-task training on a large
dataset containing both sub-optimal and expert behavior on 132 continuous
control tasks. We introduce a Perceiver-based actor-critic model and elucidate
the key model features needed to make offline RL work with self- and
cross-attention modules. Overall, we find that: i) simple offline actor critic
algorithms are a natural choice for gradually moving away from the currently
predominant paradigm of behavioral cloning, and ii) via offline RL it is
possible to learn multi-task policies that master many domains simultaneously,
including real robotics tasks, from sub-optimal demonstrations or
self-generated data.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05547" title="Abstract">arXiv:2402.05547</a> [<a href="/pdf/2402.05547" title="Download PDF">pdf</a>, <a href="/format/2402.05547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking Large Language Models on Communicative Medical Coaching: a  Novel System and Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hengguan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Songtao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongfu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Ye Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Traditional applications of natural language processing (NLP) in healthcare
have predominantly focused on patient-centered services, enhancing patient
interactions and care delivery, such as through medical dialogue systems.
However, the potential of NLP to benefit inexperienced doctors, particularly in
areas such as communicative medical coaching, remains largely unexplored. We
introduce ``ChatCoach,'' an integrated human-AI cooperative framework. Within
this framework, both a patient agent and a coaching agent collaboratively
support medical learners in practicing their medical communication skills
during consultations. Unlike traditional dialogue systems, ChatCoach provides a
simulated environment where a human doctor can engage in medical dialogue with
a patient agent. Simultaneously, a coaching agent provides real-time feedback
to the doctor. To construct the ChatCoach system, we developed a dataset and
integrated Large Language Models such as ChatGPT and Llama2, aiming to assess
their effectiveness in communicative medical coaching tasks. Our comparative
analysis demonstrates that instruction-tuned Llama2 significantly outperforms
ChatGPT's prompting-based approaches.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05548" title="Abstract">arXiv:2402.05548</a> [<a href="/pdf/2402.05548" title="Download PDF">pdf</a>, <a href="/format/2402.05548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Expression Neutrality Estimation with Application to Face  Recognition Utility Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grimmer%2C+M">Marcel Grimmer</a>, 
<a href="/search/cs?searchtype=author&query=Veldhuis%2C+R+N+J">Raymond N. J. Veldhuis</a>, 
<a href="/search/cs?searchtype=author&query=Busch%2C+C">Christoph Busch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">The recognition performance of biometric systems strongly depends on the
quality of the compared biometric samples. Motivated by the goal of
establishing a common understanding of face image quality and enabling system
interoperability, the committee draft of ISO/IEC 29794-5 introduces expression
neutrality as one of many component quality elements affecting recognition
performance. In this study, we train classifiers to assess facial expression
neutrality using seven datasets. We conduct extensive performance benchmarking
to evaluate their classification and face recognition utility prediction
abilities. Our experiments reveal significant differences in how each
classifier distinguishes "neutral" from "non-neutral" expressions. While Random
Forests and AdaBoost classifiers are most suitable for distinguishing neutral
from non-neutral facial expressions with high accuracy, they underperform
compared to Support Vector Machines in predicting face recognition utility.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05551" title="Abstract">arXiv:2402.05551</a> [<a href="/pdf/2402.05551" title="Download PDF">pdf</a>, <a href="/format/2402.05551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Thermodynamical Deep-Learning-Vision-Based Flexible Robotic  Cell for Circular Healthcare
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zocco%2C+F">Federico Zocco</a>, 
<a href="/search/cs?searchtype=author&query=Sleath%2C+D">Denis Sleath</a>, 
<a href="/search/cs?searchtype=author&query=Rahimifard%2C+S">Shahin Rahimifard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be submitted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The dependence on finite reserves of raw materials and the production of
waste are two unsolved problems of the traditional linear economy. Healthcare,
as a major sector of any nation, is currently facing them. Hence, in this
paper, we report theoretical and practical advances of robotic reprocessing of
small medical devices. Specifically, on the theory, we combine compartmental
dynamical thermodynamics with the mechanics of robots to integrate robotics
into a system-level perspective, and then, propose graph-based circularity
indicators by leveraging our thermodynamic framework. Our thermodynamic
framework is also a step forward in defining the theoretical foundations of
circular material flow designs as it improves material flow analysis (MFA) by
adding dynamical energy balances to the usual mass balances. On the practice,
we report on the on-going design of a flexible robotic cell enabled by
deep-learning vision for resources mapping and quantification, disassembly, and
waste sorting of small medical devices.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05557" title="Abstract">arXiv:2402.05557</a> [<a href="/pdf/2402.05557" title="Download PDF">pdf</a>, <a href="/ps/2402.05557" title="Download PostScript">ps</a>, <a href="/format/2402.05557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Convolutional Vision Transformers for Yield Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Inderka%2C+A">Alvin Inderka</a>, 
<a href="/search/cs?searchtype=author&query=Huber%2C+F">Florian Huber</a>, 
<a href="/search/cs?searchtype=author&query=Steinhage%2C+V">Volker Steinhage</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">While a variety of methods offer good yield prediction on histogrammed remote
sensing data, vision Transformers are only sparsely represented in the
literature. The Convolution vision Transformer (CvT) is being tested to
evaluate vision Transformers that are currently achieving state-of-the-art
results in many other vision tasks. CvT combines some of the advantages of
convolution with the advantages of dynamic attention and global context fusion
of Transformers. It performs worse than widely tested methods such as XGBoost
and CNNs, but shows that Transformers have potential to improve yield
prediction.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05558" title="Abstract">arXiv:2402.05558</a> [<a href="/pdf/2402.05558" title="Download PDF">pdf</a>, <a href="/format/2402.05558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flashback: Understanding and Mitigating Forgetting in Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aljahdali%2C+M">Mohammed Aljahdali</a>, 
<a href="/search/cs?searchtype=author&query=Abdelmoniem%2C+A+M">Ahmed M. Abdelmoniem</a>, 
<a href="/search/cs?searchtype=author&query=Canini%2C+M">Marco Canini</a>, 
<a href="/search/cs?searchtype=author&query=Horv%C3%A1th%2C+S">Samuel Horv&#xe1;th</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">In Federated Learning (FL), forgetting, or the loss of knowledge across
rounds, hampers algorithm convergence, particularly in the presence of severe
data heterogeneity among clients. This study explores the nuances of this
issue, emphasizing the critical role of forgetting in FL's inefficient learning
within heterogeneous data contexts. Knowledge loss occurs in both client-local
updates and server-side aggregation steps; addressing one without the other
fails to mitigate forgetting. We introduce a metric to measure forgetting
granularly, ensuring distinct recognition amid new knowledge acquisition.
Leveraging these insights, we propose Flashback, an FL algorithm with a dynamic
distillation approach that is used to regularize the local models, and
effectively aggregate their knowledge. Across different benchmarks, Flashback
outperforms other methods, mitigates forgetting, and achieves faster
round-to-target-accuracy, by converging in 6 to 16 rounds.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05559" title="Abstract">arXiv:2402.05559</a> [<a href="/pdf/2402.05559" title="Download PDF">pdf</a>, <a href="/format/2402.05559" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatizing Software Cognitive Complexity Reduction through Integer  Linear Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saborido%2C+R">Rub&#xe9;n Saborido</a>, 
<a href="/search/cs?searchtype=author&query=Ferrer%2C+J">Javier Ferrer</a>, 
<a href="/search/cs?searchtype=author&query=Chicano%2C+F">Francisco Chicano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">Reducing the cognitive complexity of a piece of code to a given threshold is
not trivial. Recently, we modeled software cognitive complexity reduction as an
optimization problem and we proposed an approach to assist developers on this
task. This approach enumerates sequences of code extraction refactoring
operations until a stopping criterion is met. As a result, it returns the
minimal sequence of code extraction refactoring operations that is able to
reduce the cognitive complexity of a code to the given threshold. However,
exhaustive enumeration algorithms fail to scale with the code size. The number
of refactoring plans can grow exponentially with the number of lines of code.
In this paper, instead of enumerating sequences of code extraction refactoring
operations, we model the cognitive complexity reduction as an Integer Linear
Programming problem. This opens the door to the use of efficient solvers to
find optimal solutions in large programs.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05560" title="Abstract">arXiv:2402.05560</a> [<a href="/pdf/2402.05560" title="Download PDF">pdf</a>, <a href="/format/2402.05560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tight Approximation Bounds on a Simple Algorithm for Minimum Average  Search Time in Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=H%C3%B8gemo%2C+S">Svein H&#xf8;gemo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 8 figures. Submitted to SWAT 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">The graph invariant EPT-sum has cropped up in several unrelated fields in
later years: As an objective function for hierarchical clustering, as a more
fine-grained version of the classical edge ranking problem, and, specifically
when the input is a vertex-weighted tree, as a measure of average/expected
search length in a partially ordered set. The EPT-sum of a graph $G$ is defined
as the minimum sum of the depth of every leaf in an edge partition tree (EPT),
a rooted tree where leaves correspond to vertices in $G$ and internal nodes
correspond to edges in $G$.
<br />A simple algorithm that approximates EPT-sum on trees is given by recursively
choosing the most balanced edge in the input tree $G$ to build an EPT of $G$.
Due to its fast runtime, this balanced cut algorithm is used in practice. In
this paper, we show that the balanced cut algorithm gives a 1.5-approximation
of EPT-sum on trees, which amounts to a tight analysis and answers a question
posed by Cicalese et al. in 2014.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05562" title="Abstract">arXiv:2402.05562</a> [<a href="/pdf/2402.05562" title="Download PDF">pdf</a>, <a href="/format/2402.05562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty calibration for probabilistic projection methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fanaskov%2C+V">Vladimir Fanaskov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Statistics Theory (math.ST)

</div>
<p class="mathjax">Classical Krylov subspace projection methods for the solution of linear
problem $Ax = b$ output an approximate solution $\widetilde{x}\simeq x$.
Recently, it has been recognized that projection methods can be understood from
a statistical perspective. These probabilistic projection methods return a
distribution $p(\widetilde{x})$ in place of a point estimate $\widetilde{x}$.
The resulting uncertainty, codified as a distribution, can, in theory, be
meaningfully combined with other uncertainties, can be propagated through
computational pipelines, and can be used in the framework of probabilistic
decision theory. The problem we address is that the current probabilistic
projection methods lead to the poorly calibrated posterior distribution. We
improve the covariance matrix from previous works in a way that it does not
contain such undesirable objects as $A^{-1}$ or $A^{-1}A^{-T}$, results in
nontrivial uncertainty, and reproduces an arbitrary projection method as a mean
of the posterior distribution. We also propose a variant that is numerically
inexpensive in the case the uncertainty is calibrated a priori. Since it
usually is not, we put forward a practical way to calibrate uncertainty that
performs reasonably well, albeit at the expense of roughly doubling the
numerical cost of the underlying projection method.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05563" title="Abstract">arXiv:2402.05563</a> [<a href="/pdf/2402.05563" title="Download PDF">pdf</a>, <a href="/ps/2402.05563" title="Download PostScript">ps</a>, <a href="/format/2402.05563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Multigrid Architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fanaskov%2C+V">Vladimir Fanaskov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We propose a convenient matrix-free neural architecture for the multigrid
method. The architecture is simple enough to be implemented in less than fifty
lines of code, yet it encompasses a large number of distinct multigrid solvers.
We argue that a fixed neural network without dense layers can not realize an
efficient iterative method. Because of that, standard training protocols do not
lead to competitive solvers. To overcome this difficulty, we use parameter
sharing and serialization of layers. The resulting network can be trained on
linear problems with thousands of unknowns and retains its efficiency on
problems with millions of unknowns. From the point of view of numerical linear
algebra network's training corresponds to finding optimal smoothers for the
geometric multigrid method. We demonstrate our approach on a few second-order
elliptic equations. For tested linear systems, we obtain from two to five times
smaller spectral radius of the error propagation matrix compare to a basic
linear multigrid with Jacobi smoother.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05564" title="Abstract">arXiv:2402.05564</a> [<a href="/pdf/2402.05564" title="Download PDF">pdf</a>, <a href="/format/2402.05564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Game-Theoretical Approach for Optimal Supervisory Control of Discrete  Event Systems under Energy Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lv%2C+P">Peng Lv</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+S">Shaoyuan Li</a>, 
<a href="/search/eess?searchtype=author&query=Yin%2C+X">Xiang Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, we investigate the problem of optimal supervisory control for
the discrete event systems under energy constraints. We consider that the
execution of events consumes energy and the energy can be replenished at
specific reload states. When the energy level drops below zero, the system will
be crashed. To capture the above scenario, we introduce a new model, called
consumption discrete event system (cDES). Our objective is to find the minimal
initial energy value and synthesize an optimal supervisor ensuring that the
energy will never be exhausted. To solve this problem, we propose a
game-theoretical approach by converting the cDES as a consumption two-player
graph game (cTPG) and reformulate the optimal supervisory control problem in
game theory. In particular, we demonstrate that the converted game can be
decomposed into independent reachability games related to reload vertices,
which can be solved by a fixed point iterative algorithm proposed in this
paper. Through iteratively removing unsafe reload vertices and solving
reachability games for the remaining reload vertices, a solution can be found.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05566" title="Abstract">arXiv:2402.05566</a> [<a href="/pdf/2402.05566" title="Download PDF">pdf</a>, <a href="/format/2402.05566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Succint Interaction-Aware Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Sascha Xu</a>, 
<a href="/search/cs?searchtype=author&query=C%C3%BCppers%2C+J">Joscha C&#xfc;ppers</a>, 
<a href="/search/cs?searchtype=author&query=Vreeken%2C+J">Jilles Vreeken</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">SHAP is a popular approach to explain black-box models by revealing the
importance of individual features. As it ignores feature interactions, SHAP
explanations can be confusing up to misleading. NSHAP, on the other hand,
reports the additive importance for all subsets of features. While this does
include all interacting sets of features, it also leads to an exponentially
sized, difficult to interpret explanation. In this paper, we propose to combine
the best of these two worlds, by partitioning the features into parts that
significantly interact, and use these parts to compose a succinct,
interpretable, additive explanation. We derive a criterion by which to measure
the representativeness of such a partition for a models behavior, traded off
against the complexity of the resulting explanation. To efficiently find the
best partition out of super-exponentially many, we show how to prune
sub-optimal solutions using a statistical test, which not only improves runtime
but also helps to detect spurious interactions. Experiments on synthetic and
real world data show that our explanations are both more accurate resp. more
easily interpretable than those of SHAP and NSHAP.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05567" title="Abstract">arXiv:2402.05567</a> [<a href="/pdf/2402.05567" title="Download PDF">pdf</a>, <a href="/format/2402.05567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Listening Between the Lines: Synthetic Speech Detection Disregarding  Verbal Content
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salvi%2C+D">Davide Salvi</a>, 
<a href="/search/cs?searchtype=author&query=Balcha%2C+T+S">Temesgen Semu Balcha</a>, 
<a href="/search/cs?searchtype=author&query=Bestagini%2C+P">Paolo Bestagini</a>, 
<a href="/search/cs?searchtype=author&query=Tubaro%2C+S">Stefano Tubaro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Recent advancements in synthetic speech generation have led to the creation
of forged audio data that are almost indistinguishable from real speech. This
phenomenon poses a new challenge for the multimedia forensics community, as the
misuse of synthetic media can potentially cause adverse consequences. Several
methods have been proposed in the literature to mitigate potential risks and
detect synthetic speech, mainly focusing on the analysis of the speech itself.
However, recent studies have revealed that the most crucial frequency bands for
detection lie in the highest ranges (above 6000 Hz), which do not include any
speech content. In this work, we extensively explore this aspect and
investigate whether synthetic speech detection can be performed by focusing
only on the background component of the signal while disregarding its verbal
content. Our findings indicate that the speech component is not the predominant
factor in performing synthetic speech detection. These insights provide
valuable guidance for the development of new synthetic speech detectors and
their interpretability, together with some considerations on the existing work
in the audio forensics field.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05569" title="Abstract">arXiv:2402.05569</a> [<a href="/pdf/2402.05569" title="Download PDF">pdf</a>, <a href="/format/2402.05569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hypergraph Node Classification With Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+B">Bohan Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zexi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+K">Keyue Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Siheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xiaowen Dong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Signal Processing (eess.SP); Machine Learning (stat.ML)

</div>
<p class="mathjax">Hypergraphs, with hyperedges connecting more than two nodes, are key for
modelling higher-order interactions in real-world data. The success of graph
neural networks (GNNs) reveals the capability of neural networks to process
data with pairwise interactions. This inspires the usage of neural networks for
data with higher-order interactions, thereby leading to the development of
hypergraph neural networks (HyperGNNs). GNNs and HyperGNNs are typically
considered distinct since they are designed for data on different geometric
topologies. However, in this paper, we theoretically demonstrate that, in the
context of node classification, most HyperGNNs can be approximated using a GNN
with a weighted clique expansion of the hypergraph. This leads to WCE-GNN, a
simple and efficient framework comprising a GNN and a weighted clique expansion
(WCE), for hypergraph node classification. Experiments on nine real-world
hypergraph node classification benchmarks showcase that WCE-GNN demonstrates
not only higher classification accuracy compared to state-of-the-art HyperGNNs,
but also superior memory and runtime efficiency.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05570" title="Abstract">arXiv:2402.05570</a> [<a href="/pdf/2402.05570" title="Download PDF">pdf</a>, <a href="/format/2402.05570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design and Prototyping of Transmissive RIS-Aided Wireless Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+J">Jianan Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Xiong%2C+R">Rujing Xiong</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Junshuo Liu</a>, 
<a href="/search/eess?searchtype=author&query=Mi%2C+T">Tiebin Mi</a>, 
<a href="/search/eess?searchtype=author&query=Qiu%2C+R+C">Robert Caiming Qiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Reconfigurable Intelligent Surfaces (RISs) exhibit promising enhancements in
coverage and data rates for wireless communication systems, particularly in the
context of 5G and beyond. This paper introduces a novel approach by focusing on
the design and prototyping of a transmissive RIS, contrasting with existing
research predominantly centered on reflective RIS. The achievement of 1-bit
transmissive RIS through the antisymmetry configuration of the two PIN diodes,
nearly uniform transmission magnitudes but inversed phase states in a wide band
can be obtained. A transmissive RIS prototype consisting of 16 $\times$ 16
elements is meticulously designed, fabricated, and subjected to measurement to
validate the proposed design. The results demonstrate that the proposed RIS
unit cell achieves effective 1-bit phase tuning with minimal insertion loss and
a transmission bandwidth of 3 dB exceeding $20\%$ at 5.8GHz. By dynamically
modulating the quantized code distributions on the RIS, it becomes possible to
construct scanning beams. The experimental outcomes of the RIS-assisted
communication system validate that, in comparison to scenarios without RIS, the
signal receiving power experiences an increase of approximately 7dB when RIS is
deployed to overcome obstacles. This underscores the potential applicability of
mobile RIS in practical communication.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05571" title="Abstract">arXiv:2402.05571</a> [<a href="/pdf/2402.05571" title="Download PDF">pdf</a>, <a href="/ps/2402.05571" title="Download PostScript">ps</a>, <a href="/format/2402.05571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Traditional Machine Learning Models and Bidirectional Encoder  Representations From Transformer (BERT)-Based Automatic Classification of  Tweets About Eating Disorders: Algorithm Development and Validation Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>, 
<a href="/search/cs?searchtype=author&query=Alija-P%C3%A9rez%2C+J">Jos&#xe9;-Manuel Alija-P&#xe9;rez</a>, 
<a href="/search/cs?searchtype=author&query=Vidal%2C+M">Maria-Esther Vidal</a>, 
<a href="/search/cs?searchtype=author&query=Pastor-Vargas%2C+R">Rafael Pastor-Vargas</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Ord%C3%A1s%2C+M+T">Mar&#xed;a Teresa Garc&#xed;a-Ord&#xe1;s</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> JMIR Medical Informatics, Volume 10, Issue 2, 2022, ID e34492
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Background: Eating disorders are increasingly prevalent, and social networks
offer valuable information.
<br />Objective: Our goal was to identify efficient machine learning models for
categorizing tweets related to eating disorders.
<br />Methods: Over three months, we collected tweets about eating disorders. A
2,000-tweet subset was labeled for: (1) being written by individuals with
eating disorders, (2) promoting eating disorders, (3) informativeness, and (4)
scientific content. Both traditional machine learning and deep learning models
were employed for classification, assessing accuracy, F1 score, and
computational time.
<br />Results: From 1,058,957 collected tweets, transformer-based bidirectional
encoder representations achieved the highest F1 scores (71.1%-86.4%) across all
four categories.
<br />Conclusions: Transformer-based models outperform traditional techniques in
classifying eating disorder-related tweets, though they require more
computational resources.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05575" title="Abstract">arXiv:2402.05575</a> [<a href="/pdf/2402.05575" title="Download PDF">pdf</a>, <a href="/format/2402.05575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simultaneously Achieving Group Exposure Fairness and Within-Group  Meritocracy in Stochastic Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pokhriyal%2C+S">Subham Pokhriyal</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+S">Shweta Jain</a>, 
<a href="/search/cs?searchtype=author&query=Ghalme%2C+G">Ganesh Ghalme</a>, 
<a href="/search/cs?searchtype=author&query=Dhamal%2C+S">Swapnil Dhamal</a>, 
<a href="/search/cs?searchtype=author&query=Gujar%2C+S">Sujit Gujar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in AAMAS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Existing approaches to fairness in stochastic multi-armed bandits (MAB)
primarily focus on exposure guarantee to individual arms. When arms are
naturally grouped by certain attribute(s), we propose Bi-Level Fairness, which
considers two levels of fairness. At the first level, Bi-Level Fairness
guarantees a certain minimum exposure to each group. To address the unbalanced
allocation of pulls to individual arms within a group, we consider meritocratic
fairness at the second level, which ensures that each arm is pulled according
to its merit within the group. Our work shows that we can adapt a UCB-based
algorithm to achieve a Bi-Level Fairness by providing (i) anytime Group
Exposure Fairness guarantees and (ii) ensuring individual-level Meritocratic
Fairness within each group. We first show that one can decompose regret bounds
into two components: (a) regret due to anytime group exposure fairness and (b)
regret due to meritocratic fairness within each group. Our proposed algorithm
BF-UCB balances these two regrets optimally to achieve the upper bound of
$O(\sqrt{T})$ on regret; $T$ being the stopping time. With the help of
simulated experiments, we further show that BF-UCB achieves sub-linear regret;
provides better group and individual exposure guarantees compared to existing
algorithms; and does not result in a significant drop in reward with respect to
UCB algorithm, which does not impose any fairness constraint.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05576" title="Abstract">arXiv:2402.05576</a> [<a href="/pdf/2402.05576" title="Download PDF">pdf</a>, <a href="/format/2402.05576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digital Computers Break the Curse of Dimensionality: Adaptive Bounds via  Finite Geometry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kratsios%2C+A">Anastasis Kratsios</a>, 
<a href="/search/cs?searchtype=author&query=Neuman%2C+A+M">A. Martina Neuman</a>, 
<a href="/search/cs?searchtype=author&query=Pammer%2C+G">Gudmund Pammer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Many of the foundations of machine learning rely on the idealized premise
that all input and output spaces are infinite, e.g.~$\mathbb{R}^d$. This core
assumption is systematically violated in practice due to digital computing
limitations from finite machine precision, rounding, and limited RAM. In short,
digital computers operate on finite grids in $\mathbb{R}^d$. By exploiting
these discrete structures, we show the curse of dimensionality in statistical
learning is systematically broken when models are implemented on real
computers. Consequentially, we obtain new generalization bounds with
dimension-free rates for kernel and deep ReLU MLP regressors, which are
implemented on real-world machines.
<br />Our results are derived using a new non-asymptotic concentration of measure
result between a probability measure over any finite metric space and its
empirical version associated with $N$ i.i.d. samples when measured in the
$1$-Wasserstein distance. Unlike standard concentration of measure results, the
concentration rates in our bounds do not hold uniformly for all sample sizes
$N$; instead, our rates can adapt to any given $N$. This yields significantly
tighter bounds for realistic sample sizes while achieving the optimal
worst-case rate of $\mathcal{O}(1/N^{1/2})$ for massive. Our results are built
on new techniques combining metric embedding theory with optimal transport
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05581" title="Abstract">arXiv:2402.05581</a> [<a href="/pdf/2402.05581" title="Download PDF">pdf</a>, <a href="/format/2402.05581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Establishing degrees of closeness between audio recordings along  different dimensions using large-scale cross-lingual models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fily%2C+M">Maxime Fily</a>, 
<a href="/search/cs?searchtype=author&query=Wisniewski%2C+G">Guillaume Wisniewski</a>, 
<a href="/search/cs?searchtype=author&query=Guillaume%2C+S">Severine Guillaume</a>, 
<a href="/search/cs?searchtype=author&query=Adda%2C+G">Gilles Adda</a>, 
<a href="/search/cs?searchtype=author&query=Michaud%2C+A">Alexis Michaud</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Findings of the EACL2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In the highly constrained context of low-resource language studies, we
explore vector representations of speech from a pretrained model to determine
their level of abstraction with regard to the audio signal. We propose a new
unsupervised method using ABX tests on audio recordings with carefully curated
metadata to shed light on the type of information present in the
representations. ABX tests determine whether the representations computed by a
multilingual speech model encode a given characteristic. Three experiments are
devised: one on room acoustics aspects, one on linguistic genre, and one on
phonetic aspects. The results confirm that the representations extracted from
recordings with different linguistic/extra-linguistic characteristics differ
along the same lines. Embedding more audio signal in one vector better
discriminates extra-linguistic characteristics, whereas shorter snippets are
better to distinguish segmental information. The method is fully unsupervised,
potentially opening new research avenues for comparative work on
under-documented languages.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05583" title="Abstract">arXiv:2402.05583</a> [<a href="/pdf/2402.05583" title="Download PDF">pdf</a>, <a href="/ps/2402.05583" title="Download PostScript">ps</a>, <a href="/format/2402.05583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Spectral Efficiency of Indoor Wireless Networks with a Rotary  Uniform Linear Array
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tominaga%2C+E+N">Eduardo Noboro Tominaga</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%B3pez%2C+O+L+A">Onel Luis Alcaraz L&#xf3;pez</a>, 
<a href="/search/cs?searchtype=author&query=Svensson%2C+T">Tommy Svensson</a>, 
<a href="/search/cs?searchtype=author&query=Souza%2C+R+D">Richard Demo Souza</a>, 
<a href="/search/cs?searchtype=author&query=Alves%2C+H">Hirley Alves</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 7 figures. Manuscript submitted to the 2024 Joint European Conference on Networks and Communications (EuCNC) &amp; 6G Summit, Antwerp, Belgium, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Contemporary wireless communication systems rely on Multi-User Multiple-Input
Multiple-Output (MU-MIMO) techniques. In such systems, each Access Point (AP)
is equipped with multiple antenna elements and serves multiple devices
simultaneously. Notably, traditional systems utilize fixed antennas, i.e.,
antennas without any movement capabilities, while the idea of movable antennas
has recently gained traction among the research community. By moving in a
confined region, movable antennas are able to exploit the wireless channel
variation in the continuous domain. This additional degree of freedom may
enhance the quality of the wireless links, and consequently the communication
performance. However, movable antennas for MU-MIMO proposed in the literature
are complex, bulky, expensive and present a high power consumption. In this
paper, we propose an alternative to such systems that has lower complexity and
lower cost. More specifically, we propose the incorporation of rotation
capabilities to APs equipped with Uniform Linear Arrays (ULAs) of antennas. We
consider the uplink of an indoor scenario where the AP serves multiple devices
simultaneously. The optimal rotation of the ULA is computed based on estimates
of the positions of the active devices and aiming at maximizing the per-user
mean achievable Spectral Efficiency (SE). Adopting a spatially correlated
Rician channel model, our numerical results show that the rotation capabilities
of the AP can bring substantial improvements in the SE in scenarios where the
line-of-sight component of the channel vectors is strong. Moreover, our
proposed system is robust against imperfect positioning estimates.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05584" title="Abstract">arXiv:2402.05584</a> [<a href="/pdf/2402.05584" title="Download PDF">pdf</a>, <a href="/format/2402.05584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoAugment Is What You Need: Enhancing Rule-based Augmentation Methods  in Low-resource Regimes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Juhwan Choi</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+K">Kyohoon Jin</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Junho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Sangmin Song</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Youngbin Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024 Student Research Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Text data augmentation is a complex problem due to the discrete nature of
sentences. Although rule-based augmentation methods are widely adopted in
real-world applications because of their simplicity, they suffer from potential
semantic damage. Previous researchers have suggested easy data augmentation
with soft labels (softEDA), employing label smoothing to mitigate this problem.
However, finding the best factor for each model and dataset is challenging;
therefore, using softEDA in real-world applications is still difficult. In this
paper, we propose adapting AutoAugment to solve this problem. The experimental
results suggest that the proposed method can boost existing augmentation
methods and that rule-based methods can enhance cutting-edge pre-trained
language models. We offer the source code.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05585" title="Abstract">arXiv:2402.05585</a> [<a href="/pdf/2402.05585" title="Download PDF">pdf</a>, <a href="/format/2402.05585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural functional a posteriori error estimates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fanaskov%2C+V">Vladimir Fanaskov</a>, 
<a href="/search/math?searchtype=author&query=Rudikov%2C+A">Alexander Rudikov</a>, 
<a href="/search/math?searchtype=author&query=Oseledets%2C+I">Ivan Oseledets</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review for ICML2024, was reviewed at <a href="https://openreview.net/forum?id=z62Xc88jgF">this https URL</a> for ICLR2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We propose a new loss function for supervised and physics-informed training
of neural networks and operators that incorporates a posteriori error estimate.
More specifically, during the training stage, the neural network learns
additional physical fields that lead to rigorous error majorants after a
computationally cheap postprocessing stage. Theoretical results are based upon
the theory of functional a posteriori error estimates, which allows for the
systematic construction of such loss functions for a diverse class of
practically relevant partial differential equations. From the numerical side,
we demonstrate on a series of elliptic problems that for a variety of
architectures and approaches (physics-informed neural networks,
physics-informed neural operators, neural operators, and classical
architectures in the regression and physics-informed settings), we can reach
better or comparable accuracy and in addition to that cheaply recover
high-quality upper bounds on the error after training.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05587" title="Abstract">arXiv:2402.05587</a> [<a href="/pdf/2402.05587" title="Download PDF">pdf</a>, <a href="/format/2402.05587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to synchronize Digital Twins? A Communication Performance Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cakir%2C+L+V">Lal Verda Cakir</a>, 
<a href="/search/cs?searchtype=author&query=Al-Shareeda%2C+S">Sarah Al-Shareeda</a>, 
<a href="/search/cs?searchtype=author&query=Oktug%2C+S+F">Sema F. Oktug</a>, 
<a href="/search/cs?searchtype=author&query=%C3%96zdem%2C+M">Mehmet &#xd6;zdem</a>, 
<a href="/search/cs?searchtype=author&query=Broadbent%2C+M">Matthew Broadbent</a>, 
<a href="/search/cs?searchtype=author&query=Canberk%2C+B">Berk Canberk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Synchronization is fundamental for mirroring real-world entities in real-time
and supporting effective operations of Digital Twins (DTs). Such
synchronization is enabled by the communication between the physical and
virtual realms, and it is mostly assumed to occur in real-time. However, this
is not the case, as real-life scenarios witness performance degradation that
may lead to synchronization problems. Hence, as such a problem has yet to be
thoroughly analyzed in the literature, this work attempts to uncover potential
challenges by emulating and analyzing the DT traffic flows in networks of
different scales, for different communication protocols, and with various flow
configurations. We propose a Twin Alignment Ratio metric to evaluate the
synchronization performance to achieve this goal. Consequently, the findings
reveal the interplay of network infrastructure, protocol selection, and
twinning rate on synchronization and performance.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05589" title="Abstract">arXiv:2402.05589</a> [<a href="/pdf/2402.05589" title="Download PDF">pdf</a>, <a href="/format/2402.05589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RESMatch: Referring Expression Segmentation in a Semi-Supervised Manner
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zang%2C+Y">Ying Zang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+C">Chenglong Fu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+R">Runlong Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+D">Didi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wenjun Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lanyun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianrun Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Referring expression segmentation (RES), a task that involves localizing
specific instance-level objects based on free-form linguistic descriptions, has
emerged as a crucial frontier in human-AI interaction. It demands an intricate
understanding of both visual and textual contexts and often requires extensive
training data. This paper introduces RESMatch, the first semi-supervised
learning (SSL) approach for RES, aimed at reducing reliance on exhaustive data
annotation. Extensive validation on multiple RES datasets demonstrates that
RESMatch significantly outperforms baseline approaches, establishing a new
state-of-the-art. Although existing SSL techniques are effective in image
segmentation, we find that they fall short in RES. Facing the challenges
including the comprehension of free-form linguistic descriptions and the
variability in object attributes, RESMatch introduces a trifecta of
adaptations: revised strong perturbation, text augmentation, and adjustments
for pseudo-label quality and strong-weak supervision. This pioneering work lays
the groundwork for future research in semi-supervised learning for referring
expression segmentation.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05591" title="Abstract">arXiv:2402.05591</a> [<a href="/pdf/2402.05591" title="Download PDF">pdf</a>, <a href="/ps/2402.05591" title="Download PostScript">ps</a>, <a href="/format/2402.05591" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SoftEDA: Rethinking Rule-Based Data Augmentation with Soft Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Juhwan Choi</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+K">Kyohoon Jin</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Junho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Sangmin Song</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Youngbin Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2023 Tiny Papers
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Rule-based text data augmentation is widely used for NLP tasks due to its
simplicity. However, this method can potentially damage the original meaning of
the text, ultimately hurting the performance of the model. To overcome this
limitation, we propose a straightforward technique for applying soft labels to
augmented data. We conducted experiments across seven different classification
tasks and empirically demonstrated the effectiveness of our proposed approach.
We have publicly opened our source code for reproducibility.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05592" title="Abstract">arXiv:2402.05592</a> [<a href="/pdf/2402.05592" title="Download PDF">pdf</a>, <a href="/format/2402.05592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MERP: Metaverse Extended Realtiy Portal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+A">Anisha Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+A">Aditya Mitra</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+A">Anik Saha</a>, 
<a href="/search/cs?searchtype=author&query=Sethuraman%2C+S+C">Sibi Chakkaravarthy Sethuraman</a>, 
<a href="/search/cs?searchtype=author&query=Subramanian%2C+A">Anitha Subramanian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">A standardized control system called Metaverse Extended Reality Portal (MERP)
is presented as a solution to the issues with conventional VR eyewear. The MERP
system improves user awareness of the physical world while offering an
immersive 3D view of the metaverse by using a shouldermounted projector to
display a Heads-Up Display (HUD) in a designated Metaverse Experience Room. To
provide natural and secure interaction inside the metaverse, a compass module
and gyroscope integration enable accurate mapping of real-world motions to
avatar actions. Through user tests and research, the MERP system shows that it
may reduce mishaps brought on by poor spatial awareness, offering an improved
metaverse experience and laying the groundwork for future developments in
virtual reality technology. MERP, which is compared with existing Virtual
Reality (VR) glasses used to traverse the metaverse, is projected to become a
seamless, novel and better alternative. Existing VR headsets and AR glasses
have well-known drawbacks that making them ineffective for prolonged usage as
it causes harm to the eyes.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05593" title="Abstract">arXiv:2402.05593</a> [<a href="/pdf/2402.05593" title="Download PDF">pdf</a>, <a href="/format/2402.05593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Concept for Reconstructing Stucco Statues from historic Sketches using  synthetic Data only
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=P%C3%B6llabauer%2C+T">Thomas P&#xf6;llabauer</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%BChn%2C+J">Julius K&#xfc;hn</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Eurographics Workshop on Graphics and Cultural Heritage 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In medieval times, stuccoworkers used a red color, called sinopia, to first
create a sketch of the to-be-made statue on the wall. Today, many of these
statues are destroyed, but using the original drawings, deriving from the red
color also called sinopia, we can reconstruct how the final statue might have
looked.We propose a fully-automated approach to reconstruct a point cloud and
show preliminary results by generating a color-image, a depth-map, as well as
surface normals requiring only a single sketch, and without requiring a
collection of other, similar samples. Our proposed solution allows real-time
reconstruction on-site, for instance, within an exhibition, or to generate a
useful starting point for an expert, trying to manually reconstruct the statue,
all while using only synthetic data for training.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05598" title="Abstract">arXiv:2402.05598</a> [<a href="/pdf/2402.05598" title="Download PDF">pdf</a>, <a href="/format/2402.05598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural operators meet conjugate gradients: The FCG-NO method for  efficient PDE solving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Rudikov%2C+A">Alexander Rudikov</a>, 
<a href="/search/math?searchtype=author&query=Fanaskov%2C+V">Vladimir Fanaskov</a>, 
<a href="/search/math?searchtype=author&query=Muravleva%2C+E">Ekaterina Muravleva</a>, 
<a href="/search/math?searchtype=author&query=Laevsky%2C+Y+M">Yuri M. Laevsky</a>, 
<a href="/search/math?searchtype=author&query=Oseledets%2C+I">Ivan Oseledets</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Deep learning solvers for partial differential equations typically have
limited accuracy. We propose to overcome this problem by using them as
preconditioners. More specifically, we apply discretization-invariant neural
operators to learn preconditioners for the flexible conjugate gradient method
(FCG). Architecture paired with novel loss function and training scheme allows
for learning efficient preconditioners that can be used across different
resolutions. On the theoretical side, FCG theory allows us to safely use
nonlinear preconditioners that can be applied in $O(N)$ operations without
constraining the form of the preconditioners matrix. To justify learning scheme
components (the loss function and the way training data is collected) we
perform several ablation studies. Numerical results indicate that our approach
favorably compares with classical preconditioners and allows to reuse of
preconditioners learned for lower resolution to the higher resolution data.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05602" title="Abstract">arXiv:2402.05602</a> [<a href="/pdf/2402.05602" title="Download PDF">pdf</a>, <a href="/format/2402.05602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AttnLRP: Attention-Aware Layer-wise Relevance Propagation for  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Achtibat%2C+R">Reduan Achtibat</a>, 
<a href="/search/cs?searchtype=author&query=Hatefi%2C+S+M+V">Sayed Mohammad Vakilzadeh Hatefi</a>, 
<a href="/search/cs?searchtype=author&query=Dreyer%2C+M">Maximilian Dreyer</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+A">Aakriti Jain</a>, 
<a href="/search/cs?searchtype=author&query=Wiegand%2C+T">Thomas Wiegand</a>, 
<a href="/search/cs?searchtype=author&query=Lapuschkin%2C+S">Sebastian Lapuschkin</a>, 
<a href="/search/cs?searchtype=author&query=Samek%2C+W">Wojciech Samek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models are prone to biased predictions and hallucinations,
underlining the paramount importance of understanding their model-internal
reasoning process. However, achieving faithful attributions for the entirety of
a black-box transformer model and maintaining computational efficiency is an
unsolved challenge. By extending the Layer-wise Relevance Propagation
attribution method to handle attention layers, we address these challenges
effectively. While partial solutions exist, our method is the first to
faithfully and holistically attribute not only input but also latent
representations of transformer models with the computational efficiency similar
to a singular backward pass. Through extensive evaluations against existing
methods on Llama 2, Flan-T5 and the Vision Transformer architecture, we
demonstrate that our proposed approach surpasses alternative methods in terms
of faithfulness and enables the understanding of latent representations,
opening up the door for concept-based explanations. We provide an open-source
implementation on GitHub https://github.com/rachtibat/LRP-for-Transformers.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05605" title="Abstract">arXiv:2402.05605</a> [<a href="/pdf/2402.05605" title="Download PDF">pdf</a>, <a href="/format/2402.05605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Delegation in Collaborative Human-AI Hybrid Teams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fuchs%2C+A">Andrew Fuchs</a>, 
<a href="/search/cs?searchtype=author&query=Passarella%2C+A">Andrea Passarella</a>, 
<a href="/search/cs?searchtype=author&query=Conti%2C+M">Marco Conti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">When humans and autonomous systems operate together as what we refer to as a
hybrid team, we of course wish to ensure the team operates successfully and
effectively. We refer to team members as agents. In our proposed framework, we
address the case of hybrid teams in which, at any time, only one team member
(the control agent) is authorized to act as control for the team. To determine
the best selection of a control agent, we propose the addition of an AI manager
(via Reinforcement Learning) which learns as an outside observer of the team.
The manager learns a model of behavior linking observations of agent
performance and the environment/world the team is operating in, and from these
observations makes the most desirable selection of a control agent. We restrict
the manager task by introducing a set of constraints. The manager constraints
indicate acceptable team operation, so a violation occurs if the team enters a
condition which is unacceptable and requires manager intervention. To ensure
minimal added complexity or potential inefficiency for the team, the manager
should attempt to minimize the number of times the team reaches a constraint
violation and requires subsequent manager intervention. Therefore our manager
is optimizing its selection of authorized agents to boost overall team
performance while minimizing the frequency of manager intervention. We
demonstrate our manager performance in a simulated driving scenario
representing the case of a hybrid team of agents composed of a human driver and
autonomous driving system. We perform experiments for our driving scenario with
interfering vehicles, indicating the need for collision avoidance and proper
speed control. Our results indicate a positive impact of our manager, with some
cases resulting in increased team performance up to ~187% that of the best solo
agent performance.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05606" title="Abstract">arXiv:2402.05606</a> [<a href="/pdf/2402.05606" title="Download PDF">pdf</a>, <a href="/format/2402.05606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Learning-based Model Predictive Control Scheme with Application to  Temperature Control Units
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xie%2C+J">Jing Xie</a>, 
<a href="/search/eess?searchtype=author&query=Simpson%2C+L">L&#xe9;o Simpson</a>, 
<a href="/search/eess?searchtype=author&query=Asprion%2C+J">Jonas Asprion</a>, 
<a href="/search/eess?searchtype=author&query=Scattolini%2C+R">Riccardo Scattolini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Temperature control is a complex task due to its often unknown dynamics and
disturbances. This paper explores the use of Neural Nonlinear AutoRegressive
eXogenous (NNARX) models for nonlinear system identification and model
predictive control of a temperature control unit. First, the NNARX model is
identified from input-output data collected from the real plant, and a
state-space representation with known measurable states consisting of past
input and output variables is formulated. Second, a tailored model predictive
controller is designed based on the trained NNARX network. The proposed control
architecture is experimentally tested on the temperature control units
manufactured by Tool-Temp AG. The results achieved are compared with those
obtained using a PI controller and a linear MPC. The findings illustrate that
the proposed scheme achieves satisfactory tracking performance while incurring
the lowest energy cost among the compared controllers.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05607" title="Abstract">arXiv:2402.05607</a> [<a href="/pdf/2402.05607" title="Download PDF">pdf</a>, <a href="/format/2402.05607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Internal Model Control design for systems learned by Control Affine  Neural Nonlinear Autoregressive Exogenous Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xie%2C+J">Jing Xie</a>, 
<a href="/search/eess?searchtype=author&query=Bonassi%2C+F">Fabio Bonassi</a>, 
<a href="/search/eess?searchtype=author&query=Scattolini%2C+R">Riccardo Scattolini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper explores the use of Control Affine Neural Nonlinear AutoRegressive
eXogenous (CA-NNARX) models for nonlinear system identification and model-based
control design. The idea behind this architecture is to match the known
control-affine structure of the system to achieve improved performance.
Coherently with recent literature of neural networks for data-driven control,
we first analyze the stability properties of CA-NNARX models, devising
sufficient conditions for their incremental Input-to-State Stability
($\delta$ISS) that can be enforced at the model training stage. The model's
stability property is then leveraged to design a stable Internal Model Control
(IMC) architecture. The proposed control scheme is tested on a simulated
Quadruple Tank benchmark system to address the output reference tracking
problem. The results achieved show that (i) the modeling accuracy of CA-NNARX
is superior to the one of a standard NNARX model for given weight size and
training epochs, and (ii) the proposed IMC law provides performance comparable
to the ones of a standard Model Predictive Controller (MPC) at a significantly
lower computational burden.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05608" title="Abstract">arXiv:2402.05608</a> [<a href="/pdf/2402.05608" title="Download PDF">pdf</a>, <a href="/format/2402.05608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Diffusion Models with State Space Backbone
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fei%2C+Z">Zhengcong Fei</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+M">Mingyuan Fan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Changqian Yu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Junshi Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">This paper presents a new exploration into a category of diffusion models
built upon state space architecture. We endeavor to train diffusion models for
image data, wherein the traditional U-Net backbone is supplanted by a state
space backbone, functioning on raw patches or latent space. Given its notable
efficacy in accommodating long-range dependencies, Diffusion State Space Models
(DiS) are distinguished by treating all inputs including time, condition, and
noisy image patches as tokens. Our assessment of DiS encompasses both
unconditional and class-conditional image generation scenarios, revealing that
DiS exhibits comparable, if not superior, performance to CNN-based or
Transformer-based U-Net architectures of commensurate size. Furthermore, we
analyze the scalability of DiS, gauged by the forward pass complexity
quantified in Gflops. DiS models with higher Gflops, achieved through
augmentation of depth/width or augmentation of input tokens, consistently
demonstrate lower FID. In addition to demonstrating commendable scalability
characteristics, DiS-H/2 models in latent space achieve performance levels akin
to prior diffusion models on class-conditional ImageNet benchmarks at the
resolution of 256$\times$256 and 512$\times$512, while significantly reducing
the computational burden. The code and models are available at:
https://github.com/feizc/DiS.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05610" title="Abstract">arXiv:2402.05610</a> [<a href="/pdf/2402.05610" title="Download PDF">pdf</a>, <a href="/format/2402.05610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extending 6D Object Pose Estimators for Stereo Vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=P%C3%B6llabauer%2C+T">Thomas P&#xf6;llabauer</a>, 
<a href="/search/cs?searchtype=author&query=Emrich%2C+J">Jan Emrich</a>, 
<a href="/search/cs?searchtype=author&query=Knauthe%2C+V">Volker Knauthe</a>, 
<a href="/search/cs?searchtype=author&query=Kuijper%2C+A">Arjan Kuijper</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Estimating the 6D pose of objects accurately, quickly, and robustly remains a
difficult task. However, recent methods for directly regressing poses from RGB
images using dense features have achieved state-of-the-art results. Stereo
vision, which provides an additional perspective on the object, can help reduce
pose ambiguity and occlusion. Moreover, stereo can directly infer the distance
of an object, while mono-vision requires internalized knowledge of the object's
size. To extend the state-of-the-art in 6D object pose estimation to stereo, we
created a BOP compatible stereo version of the YCB-V dataset. Our method
outperforms state-of-the-art 6D pose estimation algorithms by utilizing stereo
vision and can easily be adopted for other dense feature-based algorithms.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05611" title="Abstract">arXiv:2402.05611</a> [<a href="/pdf/2402.05611" title="Download PDF">pdf</a>, <a href="/format/2402.05611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Implementation for Dynamic Application Allocation in Shared Sensor  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Delgado%2C+C">Carmen Delgado</a>, 
<a href="/search/cs?searchtype=author&query=Batista%2C+S">Sergio Batista</a>, 
<a href="/search/cs?searchtype=author&query=Canales%2C+M">Mar&#xed;a Canales</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%A1llego%2C+J+R">Jos&#xe9; Ram&#xf3;n G&#xe1;llego</a>, 
<a href="/search/cs?searchtype=author&query=Ort%C3%ADn%2C+J">Jorge Ort&#xed;n</a>, 
<a href="/search/cs?searchtype=author&query=Cesana%2C+M">Matteo Cesana</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2018 11th IFIP Wireless and Mobile Networking Conference (WMNC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">We present a system architecture implementation to perform dynamic
application allocation in shared sensor networks, where highly integrated
wireless sensor systems are used to support multiple applications. The
architecture is based on a central controller that collects the received data
from the sensor nodes, dynamically decides which applications must be
simultaneously deployed in each node and, accordingly, over-the-air reprograms
the sensor nodes. Waspmote devices are used as sensor nodes that communicate
with the controller using ZigBee protocol. Experimental results show the
viability of the proposal.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05615" title="Abstract">arXiv:2402.05615</a> [<a href="/pdf/2402.05615" title="Download PDF">pdf</a>, <a href="/format/2402.05615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DAPlankton: Benchmark Dataset for Multi-instrument Plankton Recognition  via Fine-grained Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Batrakhanov%2C+D">Daniel Batrakhanov</a>, 
<a href="/search/cs?searchtype=author&query=Eerola%2C+T">Tuomas Eerola</a>, 
<a href="/search/cs?searchtype=author&query=Kraft%2C+K">Kaisa Kraft</a>, 
<a href="/search/cs?searchtype=author&query=Haraguchi%2C+L">Lumi Haraguchi</a>, 
<a href="/search/cs?searchtype=author&query=Lensu%2C+L">Lasse Lensu</a>, 
<a href="/search/cs?searchtype=author&query=Suikkanen%2C+S">Sanna Suikkanen</a>, 
<a href="/search/cs?searchtype=author&query=Camarena-G%C3%B3mez%2C+M+T">Mar&#xed;a Teresa Camarena-G&#xf3;mez</a>, 
<a href="/search/cs?searchtype=author&query=Sepp%C3%A4l%C3%A4%2C+J">Jukka Sepp&#xe4;l&#xe4;</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%A4lvi%C3%A4inen%2C+H">Heikki K&#xe4;lvi&#xe4;inen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Plankton recognition provides novel possibilities to study various
environmental aspects and an interesting real-world context to develop domain
adaptation (DA) methods. Different imaging instruments cause domain shift
between datasets hampering the development of general plankton recognition
methods. A promising remedy for this is DA allowing to adapt a model trained on
one instrument to other instruments. In this paper, we present a new DA dataset
called DAPlankton which consists of phytoplankton images obtained with
different instruments. Phytoplankton provides a challenging DA problem due to
the fine-grained nature of the task and high class imbalance in real-world
datasets. DAPlankton consists of two subsets. DAPlankton_LAB contains images of
cultured phytoplankton providing a balanced dataset with minimal label
uncertainty. DAPlankton_SEA consists of images collected from the Baltic Sea
providing challenging real-world data with large intra-class variance and class
imbalance. We further present a benchmark comparison of three widely used DA
methods.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05616" title="Abstract">arXiv:2402.05616</a> [<a href="/pdf/2402.05616" title="Download PDF">pdf</a>, <a href="/format/2402.05616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pretrained Generative Language Models as General Learning Frameworks for  Sequence-Based Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fauber%2C+B">Ben Fauber</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose that small pretrained foundational generative language models with
millions of parameters can be utilized as a general learning framework for
sequence-based tasks. Our proposal overcomes the computational resource, skill
set, and timeline challenges associated with training neural networks and
language models from scratch. Further, our approach focuses on creating small
and highly specialized models that can accurately execute a challenging task of
which the base model is incapable of performing. We demonstrate that 125M,
350M, and 1.3B parameter pretrained foundational language models can be
instruction fine-tuned with 10,000-to-1,000,000 instruction examples to achieve
near state-of-the-art results on challenging cheminformatics tasks. We also
demonstrate the role of successive language model fine-tuning epochs on
improved outcomes, as well as the importance of both data formatting and
pretrained foundational language model selection for instruction fine-tuning
success.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05617" title="Abstract">arXiv:2402.05617</a> [<a href="/pdf/2402.05617" title="Download PDF">pdf</a>, <a href="/format/2402.05617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning-based Computational Job Market Analysis: A Survey on Skill  Extraction and Classification from Job Postings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Senger%2C+E">Elena Senger</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mike Zhang</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Goot%2C+R">Rob van der Goot</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at NLP4HR 2024 (EACL Workshop)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent years have brought significant advances to Natural Language Processing
(NLP), which enabled fast progress in the field of computational job market
analysis. Core tasks in this application domain are skill extraction and
classification from job postings. Because of its quick growth and its
interdisciplinary nature, there is no exhaustive assessment of this emerging
field. This survey aims to fill this gap by providing a comprehensive overview
of deep learning methodologies, datasets, and terminologies specific to
NLP-driven skill extraction and classification. Our comprehensive cataloging of
publicly available datasets addresses the lack of consolidated information on
dataset creation and characteristics. Finally, the focus on terminology
addresses the current lack of consistent definitions for important concepts,
such as hard and soft skills, and terms relating to skill extraction and
classification.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05619" title="Abstract">arXiv:2402.05619</a> [<a href="/pdf/2402.05619" title="Download PDF">pdf</a>, <a href="/format/2402.05619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linking Vision and Multi-Agent Communication through Visible Light  Communication using Event Cameras
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nakagawa%2C+H">Haruyuki Nakagawa</a>, 
<a href="/search/cs?searchtype=author&query=Miyatani%2C+Y">Yoshitaka Miyatani</a>, 
<a href="/search/cs?searchtype=author&query=Kanezaki%2C+A">Asako Kanezaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 7 figures, submitted to AAMAS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Various robots, rovers, drones, and other agents of mass-produced products
are expected to encounter scenes where they intersect and collaborate in the
near future. In such multi-agent systems, individual identification and
communication play crucial roles. In this paper, we explore camera-based
visible light communication using event cameras to tackle this problem. An
event camera captures the events occurring in regions with changes in
brightness and can be utilized as a receiver for visible light communication,
leveraging its high temporal resolution. Generally, agents with identical
appearances in mass-produced products are visually indistinguishable when using
conventional CMOS cameras. Therefore, linking visual information with
information acquired through conventional radio communication is challenging.
We empirically demonstrate the advantages of a visible light communication
system employing event cameras and LEDs for visual individual identification
over conventional CMOS cameras with ArUco marker recognition. In the
simulation, we also verified scenarios where our event camera-based visible
light communication outperforms conventional radio communication in situations
with visually indistinguishable multi-agents. Finally, our newly implemented
multi-agent system verifies its functionality through physical robot
experiments.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05620" title="Abstract">arXiv:2402.05620</a> [<a href="/pdf/2402.05620" title="Download PDF">pdf</a>, <a href="/ps/2402.05620" title="Download PostScript">ps</a>, <a href="/format/2402.05620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimized Denial-of-Service Threats on the Scalability of LT Coded  Blockchains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=K.%2C+H">Harikrishnan K.</a>, 
<a href="/search/cs?searchtype=author&query=Harshan%2C+J">J. Harshan</a>, 
<a href="/search/cs?searchtype=author&query=Datta%2C+A">Anwitaman Datta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be presented in IEEE International Conference on Communications 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Coded blockchains have acquired prominence in the recent past as a promising
approach to slash the storage costs as well as to facilitate scalability.
Within this class, Luby Transform (LT) coded blockchains are an appealing
choice for scalability in heterogeneous networks owing to the availability of a
wide range of low-complexity LT decoders. While these architectures have been
studied from the aspects of storage savings and scalability, not much is known
in terms of their security vulnerabilities. Pointing at this research gap, in
this work, we present novel denial-of-service (DoS) threats on LT coded
blockchains that target nodes with specific decoding capabilities, thereby
preventing them from joining the network. Our proposed threats are
non-oblivious in nature, wherein adversaries gain access to the archived
blocks, and choose to execute their threat on a subset of them based on
underlying coding scheme. We show that our optimized threats can achieve the
same level of damage as that of blind attacks, however, with limited amount of
resources. This is the first work of its kind that opens up new questions on
designing coded blockchains to jointly provide storage savings, scalability and
resilience to optimized threats.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05624" title="Abstract">arXiv:2402.05624</a> [<a href="/pdf/2402.05624" title="Download PDF">pdf</a>, <a href="/format/2402.05624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Models for the Detection of Hate, Abuse and Profanity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tillmann%2C+C">Christoph Tillmann</a>, 
<a href="/search/cs?searchtype=author&query=Trivedi%2C+A">Aashka Trivedi</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharjee%2C+B">Bishwaranjan Bhattacharjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Large Language Models (LLMs) are the cornerstone for many Natural Language
Processing (NLP) tasks like sentiment analysis, document classification, named
entity recognition, question answering, summarization, etc. LLMs are often
trained on data which originates from the web. This data is prone to having
content with Hate, Abuse and Profanity (HAP). For a detailed definition of HAP,
please refer to the Appendix. Due to the LLMs being exposed to HAP content
during training, the models learn it and may then generate hateful or profane
content. For example, when the open-source RoBERTa model (specifically, the
RoBERTA base model) from the HuggingFace (HF) Transformers library is prompted
to replace the mask token in `I do not know that Persian people are that MASK`
it returns the word `stupid` with the highest score. This is unacceptable in
civil discourse.The detection of Hate, Abuse and Profanity in text is a vital
component of creating civil and unbiased LLMs, which is needed not only for
English, but for all languages. In this article, we briefly describe the
creation of HAP detectors and various ways of using them to make models civil
and acceptable in the output they generate.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05625" title="Abstract">arXiv:2402.05625</a> [<a href="/pdf/2402.05625" title="Download PDF">pdf</a>, <a href="/format/2402.05625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coded Many-User Multiple Access via Approximate Message Passing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+K">Kuan Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Venkataramanan%2C+R">Ramji Venkataramanan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">We consider communication over the Gaussian multiple-access channel in the
regime where the number of users grows linearly with the codelength. We
investigate coded CDMA schemes where each user's information is encoded via a
linear code before being modulated with a signature sequence. We propose an
efficient approximate message passing (AMP) decoder that can be tailored to the
structure of the linear code, and provide an exact asymptotic characterization
of its performance. Based on this result, we consider a decoder that integrates
AMP and belief propagation and characterize the tradeoff between spectral
efficiency and signal-to-noise ratio, for a given target error rate. Simulation
results are provided to demonstrate the benefits of the concatenated scheme at
finite lengths.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05626" title="Abstract">arXiv:2402.05626</a> [<a href="/pdf/2402.05626" title="Download PDF">pdf</a>, <a href="/format/2402.05626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Loss Landscape of Shallow ReLU-like Neural Networks: Stationary  Points, Saddle Escaping, and Network Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhengqing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Simsek%2C+B">Berfin Simsek</a>, 
<a href="/search/cs?searchtype=author&query=Ged%2C+F">Francois Ged</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this paper, we investigate the loss landscape of one-hidden-layer neural
networks with ReLU-like activation functions trained with the empirical squared
loss. As the activation function is non-differentiable, it is so far unclear
how to completely characterize the stationary points. We propose the conditions
for stationarity that apply to both non-differentiable and differentiable
cases. Additionally, we show that, if a stationary point does not contain
"escape neurons", which are defined with first-order conditions, then it must
be a local minimum. Moreover, for the scalar-output case, the presence of an
escape neuron guarantees that the stationary point is not a local minimum. Our
results refine the description of the saddle-to-saddle training process
starting from infinitesimally small (vanishing) initialization for shallow
ReLU-like networks, linking saddle escaping directly with the parameter changes
of escape neurons. Moreover, we are also able to fully discuss how network
embedding, which is to instantiate a narrower network within a wider network,
reshapes the stationary points.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05627" title="Abstract">arXiv:2402.05627</a> [<a href="/pdf/2402.05627" title="Download PDF">pdf</a>, <a href="/format/2402.05627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Binding Dynamics in Rotating Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=L%C3%B6we%2C+S">Sindy L&#xf6;we</a>, 
<a href="/search/cs?searchtype=author&query=Locatello%2C+F">Francesco Locatello</a>, 
<a href="/search/cs?searchtype=author&query=Welling%2C+M">Max Welling</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">In human cognition, the binding problem describes the open question of how
the brain flexibly integrates diverse information into cohesive object
representations. Analogously, in machine learning, there is a pursuit for
models capable of strong generalization and reasoning by learning
object-centric representations in an unsupervised manner. Drawing from
neuroscientific theories, Rotating Features learn such representations by
introducing vector-valued features that encapsulate object characteristics in
their magnitudes and object affiliation in their orientations. The
"$\chi$-binding" mechanism, embedded in every layer of the architecture, has
been shown to be crucial, but remains poorly understood. In this paper, we
propose an alternative "cosine binding" mechanism, which explicitly computes
the alignment between features and adjusts weights accordingly, and we show
that it achieves equivalent performance. This allows us to draw direct
connections to self-attention and biological neural processes, and to shed
light on the fundamental dynamics for object-centric representations to emerge
in Rotating Features.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05628" title="Abstract">arXiv:2402.05628</a> [<a href="/pdf/2402.05628" title="Download PDF">pdf</a>, <a href="/format/2402.05628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RepQuant: Towards Accurate Post-Training Quantization of Large  Transformer Models via Scale Reparameterization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhikai Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuewen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Q">Qingyi Gu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Large transformer models have demonstrated remarkable success. Post-training
quantization (PTQ), which requires only a small dataset for calibration and
avoids end-to-end retraining, is a promising solution for compressing these
large models. Regrettably, existing PTQ methods typically exhibit non-trivial
performance loss. We find that the performance bottleneck stems from
over-consideration of hardware compatibility in the quantization process,
compelling them to reluctantly employ simple quantizers, albeit at the expense
of accuracy. With the above insights, we propose RepQuant, a novel PTQ
framework with quantization-inference decoupling paradigm to address the above
issues. RepQuant employs complex quantizers in the quantization process and
simplified quantizers in the inference process, and performs mathematically
equivalent transformations between the two through quantization scale
reparameterization, thus ensuring both accurate quantization and efficient
inference. More specifically, we focus on two components with extreme
distributions: LayerNorm activations and Softmax activations. Initially, we
apply channel-wise quantization and log$\sqrt{2}$ quantization, respectively,
which are tailored to their distributions. In particular, for the former, we
introduce a learnable per-channel dual clipping scheme, which is designed to
efficiently identify outliers in the unbalanced activations with fine
granularity. Then, we reparameterize the scales to hardware-friendly layer-wise
quantization and log2 quantization for inference. Moreover, quantized weight
reconstruction is seamlessly integrated into the above procedure to further
push the performance limits. Extensive experiments are performed on different
large-scale transformer variants on multiple tasks, including vision, language,
and multi-modal transformers, and RepQuant encouragingly demonstrates
significant performance advantages.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05629" title="Abstract">arXiv:2402.05629</a> [<a href="/pdf/2402.05629" title="Download PDF">pdf</a>, <a href="/format/2402.05629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature  of Aggregated Factual Claims in Long-Form Generations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chiang%2C+C">Cheng-Han Chiang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Long-form generations from large language models (LLMs) contains a mix of
factual and non-factual claims, making evaluating factuality difficult. To
evaluate factual precision of long-form generations in a more fine-grained way,
prior works propose to decompose long-form generations into multiple verifiable
facts and verify those facts independently. The factuality of the generation is
the proportion of verifiable facts among all the facts. Such methods assume
that combining factual claims forms a factual paragraph. This paper shows that
the assumption can be violated due to entity ambiguity. We show that LLMs can
generate paragraphs that contain verifiable facts, but the facts are combined
to form a non-factual paragraph due to entity ambiguity. We further reveal that
existing factual precision metrics, including FActScore and citation recall,
cannot properly evaluate the factuality of these non-factual paragraphs. To
address this, we introduce an enhanced metric, D-FActScore, specifically
designed for content with ambiguous entities. We evaluate the D-FActScores of
people biographies generated with retrieval-augmented generation (RAG). We show
that D-FActScore can better assess the factuality of paragraphs with entity
ambiguity than FActScore. We also find that four widely used open-source LLMs
tend to mix information of distinct entities to form non-factual paragraphs.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05630" title="Abstract">arXiv:2402.05630</a> [<a href="/pdf/2402.05630" title="Download PDF">pdf</a>, <a href="/format/2402.05630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strassen&#x27;s algorithm is not optimally accurate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dumas%2C+J">Jean-Guillaume Dumas</a> (UGA, LJK, CASC), 
<a href="/search/math?searchtype=author&query=Pernet%2C+C">Cl&#xe9;ment Pernet</a> (UGA, LJK, CASC), 
<a href="/search/math?searchtype=author&query=Sedoglavic%2C+A">Alexandre Sedoglavic</a> (CRIStAL)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Symbolic Computation (cs.SC)

</div>
<p class="mathjax">We propose a non-commutative algorithm for multiplying 2x2 matrices using 7
coefficient products. This algorithm reaches simultaneously a better accuracy
in practice compared to previously known such fast algorithms, and a time
complexity bound with the best currently known leading term (obtained via
alternate basis sparsification). To build this algorithm, we consider matrix
and tensor norms bounds governing the stability and accuracy of numerical
matrix multiplication. First, we reduce those bounds by minimizing a growth
factor along the unique orbit of Strassen's 2x2-matrix multiplication tensor
decomposition. Second, we develop heuristics for minimizing the number of
operations required to realize a given bilinear formula, while further
improving its accuracy. Third, we perform an alternate basis sparsification
that improves on the time complexity constant and mostly preserves the overall
accuracy.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05631" title="Abstract">arXiv:2402.05631</a> [<a href="/pdf/2402.05631" title="Download PDF">pdf</a>, <a href="/format/2402.05631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ShiftDTW: adapting the DTW metric for cyclic time series clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Foulon%2C+L">Lucas Foulon</a> (DM), 
<a href="/search/cs?searchtype=author&query=Korichi%2C+I">Ilyes Korichi</a> (DM), 
<a href="/search/cs?searchtype=author&query=Millot%2C+X">Xavier Millot</a> (Oxtaam)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> in French language. GAST@EGC 2024 : Atelier Gestion et Analyse des donn{\'e}es Spatiales et Temporelles, Aur{\'e}lie Leborgne; Nida Meddouri; Lo{\"i}c Salmon; Cl{\'e}ment Iphar, Jan 2024, Dijon, France
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">The elasticity of the DTW metric provides a more flexible comparison between
time series and is used in numerous machine learning domains such as
classification or clustering. However, it does not align the measurements at
the beginning and end of time series if they have a shift occurring right at
the start of one series, with the omitted part appearing at the end of that
series. Due to the cyclicity of such series - which lack a definite beginning
or end - we rely on the Cyclic DTW approach to propose a less computationally
expensive approximation of this calculation method. This approximation will
then be employed in conjunction with the K-Means clustering method.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05636" title="Abstract">arXiv:2402.05636</a> [<a href="/pdf/2402.05636" title="Download PDF">pdf</a>, <a href="/ps/2402.05636" title="Download PostScript">ps</a>, <a href="/format/2402.05636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Impact of AI Tool on Engineering at ANZ Bank An Emperical Study on  GitHub Copilot within Coporate Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+S">Sayan Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C+L">Ching Louis Liu</a>, 
<a href="/search/cs?searchtype=author&query=Rowland%2C+G">Gareth Rowland</a>, 
<a href="/search/cs?searchtype=author&query=Hogarth%2C+T">Tim Hogarth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 4 figures. in proceeding for 10th International Conference on Software Engineering (SEC 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The increasing popularity of AI, particularly Large Language Models (LLMs),
has significantly impacted various domains, including Software Engineering.
This study explores the integration of AI tools in software engineering
practices within a large organization. We focus on ANZ Bank, which employs over
5000 engineers covering all aspects of the software development life cycle.
This paper details an experiment conducted using GitHub Copilot, a notable AI
tool, within a controlled environment to evaluate its effectiveness in
real-world engineering tasks. Additionally, this paper shares initial findings
on the productivity improvements observed after GitHub Copilot was adopted on a
large scale, with about 1000 engineers using it. ANZ Bank's six-week experiment
with GitHub Copilot included two weeks of preparation and four weeks of active
testing. The study evaluated participant sentiment and the tool's impact on
productivity, code quality, and security. Initially, participants used GitHub
Copilot for proposed use-cases, with their feedback gathered through regular
surveys. In the second phase, they were divided into Control and Copilot
groups, each tackling the same Python challenges, and their experiences were
again surveyed. Results showed a notable boost in productivity and code quality
with GitHub Copilot, though its impact on code security remained inconclusive.
Participant responses were overall positive, confirming GitHub Copilot's
effectiveness in large-scale software engineering environments. Early data from
1000 engineers also indicated a significant increase in productivity and job
satisfaction.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05637" title="Abstract">arXiv:2402.05637</a> [<a href="/pdf/2402.05637" title="Download PDF">pdf</a>, <a href="/format/2402.05637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning pseudo-contractive denoisers for inverse problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+D">Deliang Wei</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Peng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+F">Fang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep denoisers have shown excellent performance in solving inverse problems
in signal and image processing. In order to guarantee the convergence, the
denoiser needs to satisfy some Lipschitz conditions like non-expansiveness.
However, enforcing such constraints inevitably compromises recovery
performance. This paper introduces a novel training strategy that enforces a
weaker constraint on the deep denoiser called pseudo-contractiveness. By
studying the spectrum of the Jacobian matrix, relationships between different
denoiser assumptions are revealed. Effective algorithms based on gradient
descent and Ishikawa process are derived, and further assumptions of strict
pseudo-contractiveness yield efficient algorithms using half-quadratic
splitting and forward-backward splitting. The proposed algorithms theoretically
converge strongly to a fixed point. A training strategy based on holomorphic
transformation and functional calculi is proposed to enforce the
pseudo-contractive denoiser assumption. Extensive experiments demonstrate
superior performance of the pseudo-contractive denoiser compared to related
denoisers. The proposed methods are competitive in terms of visual effects and
quantitative values.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05641" title="Abstract">arXiv:2402.05641</a> [<a href="/pdf/2402.05641" title="Download PDF">pdf</a>, <a href="/ps/2402.05641" title="Download PostScript">ps</a>, <a href="/format/2402.05641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Dynamic TDD in Small Cell Networks by the Multiplicative Weight  Update Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiaqi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Pappas%2C+N">Nikolaos Pappas</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H+H">Howard H. Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">We leverage the Multiplicative Weight Update (MWU) method to develop a
decentralized algorithm that significantly improves the performance of dynamic
time division duplexing (D-TDD) in small cell networks. The proposed algorithm
adaptively adjusts the time portion allocated to uplink (UL) and downlink (DL)
transmissions at every node during each scheduled time slot, aligning the
packet transmissions toward the most appropriate link directions according to
the feedback of signal-to-interference ratio information. Our simulation
results reveal that compared to the (conventional) fixed configuration of UL/DL
transmission probabilities in D-TDD, incorporating MWU into D-TDD brings about
a two-fold improvement of mean packet throughput in the DL and a three-fold
improvement of the same performance metric in the UL, resulting in the D-TDD
even outperforming Static-TDD in the UL. It also shows that the proposed scheme
maintains a consistent performance gain in the presence of an ascending traffic
load, validating its effectiveness in boosting the network performance. This
work also demonstrates an approach that accounts for algorithmic considerations
at the forefront when solving stochastic problems.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05643" title="Abstract">arXiv:2402.05643</a> [<a href="/pdf/2402.05643" title="Download PDF">pdf</a>, <a href="/format/2402.05643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Token-Based World Models with Parallel Observation Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cohen%2C+L">Lior Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaixin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+B">Bingyi Kang</a>, 
<a href="/search/cs?searchtype=author&query=Mannor%2C+S">Shie Mannor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Motivated by the success of Transformers when applied to sequences of
discrete symbols, token-based world models (TBWMs) were recently proposed as
sample-efficient methods. In TBWMs, the world model consumes agent experience
as a language-like sequence of tokens, where each observation constitutes a
sub-sequence. However, during imagination, the sequential token-by-token
generation of next observations results in a severe bottleneck, leading to long
training times, poor GPU utilization, and limited representations. To resolve
this bottleneck, we devise a novel Parallel Observation Prediction (POP)
mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode
tailored to our reinforcement learning setting. We incorporate POP in a novel
TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster
imagination compared to prior TBWMs. REM attains superhuman performance on 12
out of 26 games of the Atari 100K benchmark, while training in less than 12
hours. Our code is available at \url{https://github.com/leor-c/REM}.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05644" title="Abstract">arXiv:2402.05644</a> [<a href="/pdf/2402.05644" title="Download PDF">pdf</a>, <a href="/format/2402.05644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FuncGrasp: Learning Object-Centric Neural Grasp Functions from Single  Annotated Example Object
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hanzhi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+B">Binbin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Leutenegger%2C+S">Stefan Leutenegger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We present FuncGrasp, a framework that can infer dense yet reliable grasp
configurations for unseen objects using one annotated object and single-view
RGB-D observation via categorical priors. Unlike previous works that only
transfer a set of grasp poses, FuncGrasp aims to transfer infinite
configurations parameterized by an object-centric continuous grasp function
across varying instances. To ease the transfer process, we propose Neural
Surface Grasping Fields (NSGF), an effective neural representation defined on
the surface to densely encode grasp configurations. Further, we exploit
function-to-function transfer using sphere primitives to establish semantically
meaningful categorical correspondences, which are learned in an unsupervised
fashion without any expert knowledge. We showcase the effectiveness through
extensive experiments in both simulators and the real world. Remarkably, our
framework significantly outperforms several strong baseline methods in terms of
density and reliability for generated grasps.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05645" title="Abstract">arXiv:2402.05645</a> [<a href="/pdf/2402.05645" title="Download PDF">pdf</a>, <a href="/format/2402.05645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Reproducibility in Deep Learning-Based Software Fault  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mukhtar%2C+A">Adil Mukhtar</a>, 
<a href="/search/cs?searchtype=author&query=Jannach%2C+D">Dietmar Jannach</a>, 
<a href="/search/cs?searchtype=author&query=Wotawa%2C+F">Franz Wotawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Over the past few years, deep learning methods have been applied for a wide
range of Software Engineering (SE) tasks, including in particular for the
important task of automatically predicting and localizing faults in software.
With the rapid adoption of increasingly complex machine learning models, it
however becomes more and more difficult for scholars to reproduce the results
that are reported in the literature. This is in particular the case when the
applied deep learning models and the evaluation methodology are not properly
documented and when code and data are not shared. Given some recent -- and very
worrying -- findings regarding reproducibility and progress in other areas of
applied machine learning, the goal of this work is to analyze to what extent
the field of software engineering, in particular in the area of software fault
prediction, is plagued by similar problems. We have therefore conducted a
systematic review of the current literature and examined the level of
reproducibility of 56 research articles that were published between 2019 and
2022 in top-tier software engineering conferences. Our analysis revealed that
scholars are apparently largely aware of the reproducibility problem, and about
two thirds of the papers provide code for their proposed deep learning models.
However, it turned out that in the vast majority of cases, crucial elements for
reproducibility are missing, such as the code of the compared baselines, code
for data pre-processing or code for hyperparameter tuning. In these cases, it
therefore remains challenging to exactly reproduce the results in the current
research literature. Overall, our meta-analysis therefore calls for improved
research practices to ensure the reproducibility of machine-learning based
research.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05650" title="Abstract">arXiv:2402.05650</a> [<a href="/pdf/2402.05650" title="Download PDF">pdf</a>, <a href="/format/2402.05650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rocks Coding, Not Development--A Human-Centric, Experimental Evaluation  of LLM-Supported SE Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+H">Huilong Ning</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Gaowei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Libo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yi Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper has been accepted by ACM International Conference on the Foundations of Software Engineering (FSE 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recently, large language models (LLM) based generative AI has been gaining
momentum for their impressive high-quality performances in multiple domains,
particularly after the release of the ChatGPT. Many believe that they have the
potential to perform general-purpose problem-solving in software development
and replace human software developers. Nevertheless, there are in a lack of
serious investigation into the capability of these LLM techniques in fulfilling
software development tasks. In a controlled 2 $\times$ 2 between-subject
experiment with 109 participants, we examined whether and to what degree
working with ChatGPT was helpful in the coding task and typical software
development task and how people work with ChatGPT. We found that while ChatGPT
performed well in solving simple coding problems, its performance in supporting
typical software development tasks was not that good. We also observed the
interactions between participants and ChatGPT and found the relations between
the interactions and the outcomes. Our study thus provides first-hand insights
into using ChatGPT to fulfill software engineering tasks with real-world
developers and motivates the need for novel interaction mechanisms that help
developers effectively work with large language models to achieve desired
outcomes.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05655" title="Abstract">arXiv:2402.05655</a> [<a href="/pdf/2402.05655" title="Download PDF">pdf</a>, <a href="/format/2402.05655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time Holistic Robot Pose Estimation with Unknown States
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ban%2C+S">Shikun Ban</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+J">Juling Fan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wentao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xiaoxuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yizhou Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Estimating robot pose from RGB images is a crucial problem in computer vision
and robotics. While previous methods have achieved promising performance, most
of them presume full knowledge of robot internal states, e.g. ground-truth
robot joint angles, which are not always available in real-world scenarios. On
the other hand, existing approaches that estimate robot pose without joint
state priors suffer from heavy computation burdens and thus cannot support
real-time applications. This work addresses the urgent need for efficient robot
pose estimation with unknown states. We propose an end-to-end pipeline for
real-time, holistic robot pose estimation from a single RGB image, even in the
absence of known robot states. Our method decomposes the problem into
estimating camera-to-robot rotation, robot state parameters, keypoint
locations, and root depth. We further design a corresponding neural network
module for each task. This approach allows for learning multi-facet
representations and facilitates sim-to-real transfer through self-supervised
learning. Notably, our method achieves inference with a single feedforward,
eliminating the need for costly test-time iterative optimization. As a result,
it delivers a 12-time speed boost with state-of-the-art accuracy, enabling
real-time holistic robot pose estimation for the first time. Code is available
at https://oliverbansk.github.io/Holistic-Robot-Pose/.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05657" title="Abstract">arXiv:2402.05657</a> [<a href="/pdf/2402.05657" title="Download PDF">pdf</a>, <a href="/ps/2402.05657" title="Download PostScript">ps</a>, <a href="/format/2402.05657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> q-Parikh Matrices and q-deformed binomial coefficients of words
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Renard%2C+A">Antoine Renard</a>, 
<a href="/search/cs?searchtype=author&query=Rigo%2C+M">Michel Rigo</a>, 
<a href="/search/cs?searchtype=author&query=Whiteland%2C+M+A">Markus A. Whiteland</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, submitted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
<p class="mathjax">We have introduced a q-deformation, i.e., a polynomial in q with natural
coefficients, of the binomial coefficient of two finite words u and v counting
the number of occurrences of v as a subword of u. In this paper, we examine the
q-deformation of Parikh matrices as introduced by E\u{g}ecio\u{g}lu in 2004.
<br />Many classical results concerning Parikh matrices generalize to this new
framework: Our first important observation is that the elements of such a
matrix are in fact q-deformations of binomial coefficients of words. We also
study their inverses and as an application, we obtain new identities about
q-binomials.
<br />For a finite word z and for the sequence $(p_n)_{n\ge 0}$ of prefixes of an
infinite word, we show that the polynomial sequence $\binom{p_n}{z}_q$
converges to a formal series. We present links with additive number theory and
k-regular sequences. In the case of a periodic word $u^\omega$, we generalize a
result of Salomaa: the sequence $\binom{u^n}{z}_q$ satisfies a linear
recurrence relation with polynomial coefficients. Related to the theory of
integer partition, we describe the growth and the zero set of the coefficients
of the series associated with $u^\omega$.
<br />Finally, we show that the minors of a q-Parikh matrix are polynomials with
natural coefficients and consider a generalization of Cauchy's inequality. We
also compare q-Parikh matrices associated with an arbitrary word with those
associated with a canonical word $12\cdots k$ made of pairwise distinct
symbols.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05660" title="Abstract">arXiv:2402.05660</a> [<a href="/pdf/2402.05660" title="Download PDF">pdf</a>, <a href="/format/2402.05660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Propagation for Unsupervised Graph Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Meihan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zeyu Fang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+M">Ming Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Sheng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bu%2C+J">Jiajun Bu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI-24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Unsupervised Graph Domain Adaptation (UGDA) aims to transfer knowledge from a
labelled source graph to an unlabelled target graph in order to address the
distribution shifts between graph domains. Previous works have primarily
focused on aligning data from the source and target graph in the representation
space learned by graph neural networks (GNNs). However, the inherent
generalization capability of GNNs has been largely overlooked. Motivated by our
empirical analysis, we reevaluate the role of GNNs in graph domain adaptation
and uncover the pivotal role of the propagation process in GNNs for adapting to
different graph domains. We provide a comprehensive theoretical analysis of
UGDA and derive a generalization bound for multi-layer GNNs. By formulating GNN
Lipschitz for k-layer GNNs, we show that the target risk bound can be tighter
by removing propagation layers in source graph and stacking multiple
propagation layers in target graph. Based on the empirical and theoretical
analysis mentioned above, we propose a simple yet effective approach called
A2GNN for graph domain adaptation. Through extensive experiments on real-world
datasets, we demonstrate the effectiveness of our proposed A2GNN framework.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05662" title="Abstract">arXiv:2402.05662</a> [<a href="/pdf/2402.05662" title="Download PDF">pdf</a>, <a href="/format/2402.05662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic COLREGs Evaluation for Safe Navigation under Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hansen%2C+P+N">Peter Nicholas Hansen</a>, 
<a href="/search/eess?searchtype=author&query=Papageorgiou%2C+D">Dimitrios Papageorgiou</a>, 
<a href="/search/eess?searchtype=author&query=Galeazzi%2C+R">Roberto Galeazzi</a>, 
<a href="/search/eess?searchtype=author&query=Blanke%2C+M">Mogens Blanke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE Transactions on Intelligent Transportation Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">The encounter situation between marine vessels determines how they should
navigate to obey COLREGs, but time-varying and stochastic uncertainty in
estimation of angles of encounter, and of closest point of approach, easily
give rise to different assessment of situation at two approaching vessels. This
may lead to high-risk conditions and could cause collision. This article
considers decision making under uncertainty and suggests a novel method for
probabilistic interpretation of vessel encounters that is explainable and
provides a measure of uncertainty in the evaluation. The method is equally
useful for decision support on a manned bridge as on Marine Autonomous Surface
Ships (MASS) where it provides input for automated navigation. The method makes
formal safety assessment and validation feasible. We obtain a resilient
algorithm for machine interpretation of COLREGs under uncertainty and show its
efficacy by simulations.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05663" title="Abstract">arXiv:2402.05663</a> [<a href="/pdf/2402.05663" title="Download PDF">pdf</a>, <a href="/format/2402.05663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mesoscale Traffic Forecasting for Real-Time Bottleneck and Shockwave  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chekroun%2C+R">Raphael Chekroun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Han Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jonathan Lee</a>, 
<a href="/search/cs?searchtype=author&query=Toromanoff%2C+M">Marin Toromanoff</a>, 
<a href="/search/cs?searchtype=author&query=Hornauer%2C+S">Sascha Hornauer</a>, 
<a href="/search/cs?searchtype=author&query=Moutarde%2C+F">Fabien Moutarde</a>, 
<a href="/search/cs?searchtype=author&query=Monache%2C+M+L+D">Maria Laura Delle Monache</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">Accurate real-time traffic state forecasting plays a pivotal role in traffic
control research. In particular, the CIRCLES consortium project necessitates
predictive techniques to mitigate the impact of data source delays. After the
success of the MegaVanderTest experiment, this paper aims at overcoming the
current system limitations and develop a more suited approach to improve the
real-time traffic state estimation for the next iterations of the experiment.
In this paper, we introduce the SA-LSTM, a deep forecasting method integrating
Self-Attention (SA) on the spatial dimension with Long Short-Term Memory (LSTM)
yielding state-of-the-art results in real-time mesoscale traffic forecasting.
We extend this approach to multi-step forecasting with the n-step SA-LSTM,
which outperforms traditional multi-step forecasting methods in the trade-off
between short-term and long-term predictions, all while operating in real-time.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05667" title="Abstract">arXiv:2402.05667</a> [<a href="/pdf/2402.05667" title="Download PDF">pdf</a>, <a href="/format/2402.05667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> S$&#x3a9;$I: Score-based O-INFORMATION Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bounoua%2C+M">Mustapha Bounoua</a>, 
<a href="/search/cs?searchtype=author&query=Franzese%2C+G">Giulio Franzese</a>, 
<a href="/search/cs?searchtype=author&query=Michiardi%2C+P">Pietro Michiardi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">The analysis of scientific data and complex multivariate systems requires
information quantities that capture relationships among multiple random
variables. Recently, new information-theoretic measures have been developed to
overcome the shortcomings of classical ones, such as mutual information, that
are restricted to considering pairwise interactions. Among them, the concept of
information synergy and redundancy is crucial for understanding the high-order
dependencies between variables. One of the most prominent and versatile
measures based on this concept is O-information, which provides a clear and
scalable way to quantify the synergy-redundancy balance in multivariate
systems. However, its practical application is limited to simplified cases. In
this work, we introduce S$\Omega$I, which allows for the first time to compute
O-information without restrictive assumptions about the system. Our experiments
validate our approach on synthetic data, and demonstrate the effectiveness of
S$\Omega$I in the context of a real-world use case.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05668" title="Abstract">arXiv:2402.05668</a> [<a href="/pdf/2402.05668" title="Download PDF">pdf</a>, <a href="/format/2402.05668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comprehensive Assessment of Jailbreak Attacks Against LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+J">Junjie Chu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yugeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Ziqing Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xinyue Shen</a>, 
<a href="/search/cs?searchtype=author&query=Backes%2C+M">Michael Backes</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Misuse of the Large Language Models (LLMs) has raised widespread concern. To
address this issue, safeguards have been taken to ensure that LLMs align with
social ethics. However, recent findings have revealed an unsettling
vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By
applying techniques, such as employing role-playing scenarios, adversarial
examples, or subtle subversion of safety objectives as a prompt, LLMs can
produce an inappropriate or even harmful response. While researchers have
studied several categories of jailbreak attacks, they have done so in
isolation. To fill this gap, we present the first large-scale measurement of
various jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak
methods from four categories, 160 questions from 16 violation categories, and
six popular LLMs. Our extensive experimental results demonstrate that the
optimized jailbreak prompts consistently achieve the highest attack success
rates, as well as exhibit robustness across different LLMs. Some jailbreak
prompt datasets, available from the Internet, can also achieve high attack
success rates on many LLMs, such as ChatGLM3, GPT-3.5, and PaLM2. Despite the
claims from many organizations regarding the coverage of violation categories
in their policies, the attack success rates from these categories remain high,
indicating the challenges of effectively aligning LLM policies and the ability
to counter jailbreak attacks. We also discuss the trade-off between the attack
performance and efficiency, as well as show that the transferability of the
jailbreak prompts is still viable, becoming an option for black-box models.
Overall, our research highlights the necessity of evaluating different
jailbreak methods. We hope our study can provide insights for future research
on jailbreak attacks and serve as a benchmark tool for evaluating them for
practitioners.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05672" title="Abstract">arXiv:2402.05672</a> [<a href="/pdf/2402.05672" title="Download PDF">pdf</a>, <a href="/format/2402.05672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilingual E5 Text Embeddings: A Technical Report
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+N">Nan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaolong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Linjun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Majumder%2C+R">Rangan Majumder</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+F">Furu Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">This technical report presents the training methodology and evaluation
results of the open-source multilingual E5 text embedding models, released in
mid-2023. Three embedding models of different sizes (small / base / large) are
provided, offering a balance between the inference efficiency and embedding
quality. The training procedure adheres to the English E5 model recipe,
involving contrastive pre-training on 1 billion multilingual text pairs,
followed by fine-tuning on a combination of labeled datasets. Additionally, we
introduce a new instruction-tuned embedding model, whose performance is on par
with state-of-the-art, English-only models of similar sizes. Information
regarding the model release can be found at
https://github.com/microsoft/unilm/tree/master/e5 .
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05675" title="Abstract">arXiv:2402.05675</a> [<a href="/pdf/2402.05675" title="Download PDF">pdf</a>, <a href="/format/2402.05675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Adversarial Training with Compressed Datasets Effective?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Selvan%2C+R">Raghavendra Selvan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 14 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Dataset Condensation (DC) refers to the recent class of dataset compression
methods that generate a smaller, synthetic, dataset from a larger dataset. This
synthetic dataset retains the essential information of the original dataset,
enabling models trained on it to achieve performance levels comparable to those
trained on the full dataset. Most current DC methods have mainly concerned with
achieving high test performance with limited data budget, and have not directly
addressed the question of adversarial robustness. In this work, we investigate
the impact of adversarial robustness on models trained with compressed
datasets. We show that the compressed datasets obtained from DC methods are not
effective in transferring adversarial robustness to models. As a solution to
improve dataset compression efficiency and adversarial robustness
simultaneously, we propose a novel robustness-aware dataset compression method
based on finding the Minimal Finite Covering (MFC) of the dataset. The proposed
method is (1) obtained by one-time computation and is applicable for any model,
(2) more effective than DC methods when applying adversarial training over MFC,
(3) provably robust by minimizing the generalized adversarial loss.
Additionally, empirical evaluation on three datasets shows that the proposed
method is able to achieve better robustness and performance trade-off compared
to DC methods such as distribution matching.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05680" title="Abstract">arXiv:2402.05680</a> [<a href="/pdf/2402.05680" title="Download PDF">pdf</a>, <a href="/ps/2402.05680" title="Download PostScript">ps</a>, <a href="/format/2402.05680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable classifiers for tabular data via discretization and  feature selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jaakkola%2C+R">Reijo Jaakkola</a>, 
<a href="/search/cs?searchtype=author&query=Janhunen%2C+T">Tomi Janhunen</a>, 
<a href="/search/cs?searchtype=author&query=Kuusisto%2C+A">Antti Kuusisto</a>, 
<a href="/search/cs?searchtype=author&query=Rankooh%2C+M+F">Masood Feyzbakhsh Rankooh</a>, 
<a href="/search/cs?searchtype=author&query=Vilander%2C+M">Miikka Vilander</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">We introduce a method for computing immediately human interpretable yet
accurate classifiers from tabular data. The classifiers obtained are short
DNF-formulas, computed via first discretizing the original data to Boolean form
and then using feature selection coupled with a very fast algorithm for
producing the best possible Boolean classifier for the setting. We demonstrate
the approach via 14 experiments, obtaining results with accuracies mainly
similar to ones obtained via random forests, XGBoost, and existing results for
the same datasets in the literature. In several cases, our approach in fact
outperforms the reference results in relation to accuracy, even though the main
objective of our study is the immediate interpretability of our classifiers. We
also prove a new result on the probability that the classifier we obtain from
real-life data corresponds to the ideally best classifier with respect to the
background distribution the data comes from.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05681" title="Abstract">arXiv:2402.05681</a> [<a href="/pdf/2402.05681" title="Download PDF">pdf</a>, <a href="/format/2402.05681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Gr&#xfc;nbaum&#x27;s Conjecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ortlieb%2C+C">Christian Ortlieb</a>, 
<a href="/search/cs?searchtype=author&query=Schmidt%2C+J+M">Jens M. Schmidt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">Given a spanning tree $T$ of a planar graph $G$, the co-tree of $T$ is the
spanning tree of the dual graph $G^*$ with edge set $(E(G)-E(T))^*$. Gr\"unbaum
conjectured in 1970 that every planar 3-connected graph $G$ contains a spanning
tree $T$ such that both $T$ and its co-tree have maximum degree at most 3.
<br />While Gr\"unbaum's conjecture remains open, Biedl proved that there is a
spanning tree $T$ such that $T$ and its co-tree have maximum degree at most 5.
By using new structural insights into Schnyder woods, we prove that there is a
spanning tree $T$ such that $T$ and its co-tree have maximum degree at most 4.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05684" title="Abstract">arXiv:2402.05684</a> [<a href="/pdf/2402.05684" title="Download PDF">pdf</a>, <a href="/ps/2402.05684" title="Download PostScript">ps</a>, <a href="/format/2402.05684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimum dimensional synthesis of planar mechanisms with geometric  constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Garcia-Marina%2C+V">V. Garcia-Marina</a>, 
<a href="/search/math?searchtype=author&query=de+Bustos%2C+I+F">I. Fernandez de Bustos</a>, 
<a href="/search/math?searchtype=author&query=Urkullu%2C+G">G. Urkullu</a>, 
<a href="/search/math?searchtype=author&query=Ansola%2C+R">R. Ansola</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Meccanica (2020) 55:2135--2158
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Dynamical Systems (math.DS)

</div>
<p class="mathjax">The deformed energy method has shown to be a good option for dimensional
synthesis of mechanisms. In this paper the introduction of some new features to
such approach is proposed. First, constraints fixing dimensions of certain
links are introduced in the error function of the synthesis problem. Second,
requirements on distances between determinate nodes are included in the error
function for the analysis of the deformed position problem. Both the overall
synthesis error function and the inner analysis error function are optimized
using a Sequential Quadratic Problem (SQP) approach. This also reduces the
probability of branch or circuit defects. In the case of the inner function
analytical derivatives are used, while in the synthesis optimization
approximate derivatives have been introduced. Furthermore, constraints are
analyzed under two formulations, the Euclidean distance and an alternative
approach that uses the previous raised to the power of two. The latter approach
is often used in kinematics, and simplifies the computation of derivatives.
Some examples are provided to show the convergence order of the error function
and the fulfilment of the constraints in both formulations studied under
different topological situations or achieved energy levels.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05685" title="Abstract">arXiv:2402.05685</a> [<a href="/pdf/2402.05685" title="Download PDF">pdf</a>, <a href="/ps/2402.05685" title="Download PostScript">ps</a>, <a href="/format/2402.05685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Ordinal Regression Framework for a Deep Learning Based Severity  Assessment for Chest Radiographs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wienholt%2C+P">Patrick Wienholt</a>, 
<a href="/search/cs?searchtype=author&query=Hermans%2C+A">Alexander Hermans</a>, 
<a href="/search/cs?searchtype=author&query=Khader%2C+F">Firas Khader</a>, 
<a href="/search/cs?searchtype=author&query=Puladi%2C+B">Behrus Puladi</a>, 
<a href="/search/cs?searchtype=author&query=Leibe%2C+B">Bastian Leibe</a>, 
<a href="/search/cs?searchtype=author&query=Kuhl%2C+C">Christiane Kuhl</a>, 
<a href="/search/cs?searchtype=author&query=Nebelung%2C+S">Sven Nebelung</a>, 
<a href="/search/cs?searchtype=author&query=Truhn%2C+D">Daniel Truhn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 3 figures, the code is available at: <a href="https://github.com/paddyOnGithub/ordinal_regression">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This study investigates the application of ordinal regression methods for
categorizing disease severity in chest radiographs. We propose a framework that
divides the ordinal regression problem into three parts: a model, a target
function, and a classification function. Different encoding methods, including
one-hot, Gaussian, progress-bar, and our soft-progress-bar, are applied using
ResNet50 and ViT-B-16 deep learning models. We show that the choice of encoding
has a strong impact on performance and that the best encoding depends on the
chosen weighting of Cohen's kappa and also on the model architecture used. We
make our code publicly available on GitHub.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05687" title="Abstract">arXiv:2402.05687</a> [<a href="/pdf/2402.05687" title="Download PDF">pdf</a>, <a href="/format/2402.05687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessment of the Sparsity-Diversity Trade-offs in Active Users  Detection for mMTC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Jesus%2C+G+M">Gabriel Martins de Jesus</a>, 
<a href="/search/cs?searchtype=author&query=Lopez%2C+O+L+A">Onel Luis Alcaraz Lopez</a>, 
<a href="/search/cs?searchtype=author&query=Souza%2C+R+D">Richard Demo Souza</a>, 
<a href="/search/cs?searchtype=author&query=Mahmood%2C+N+H">Nurul Huda Mahmood</a>, 
<a href="/search/cs?searchtype=author&query=Juntti%2C+M">Markku Juntti</a>, 
<a href="/search/cs?searchtype=author&query=Latva-Aho%2C+M">Matti Latva-Aho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 5 figures. Manuscript submitted to IEEE Wireless Communications Letters for review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Wireless communication systems must increasingly support a multitude of
machine-type communications (MTC) devices, thus calling for advanced strategies
for active user detection (AUD). Recent literature has delved into AUD
techniques based on compressed sensing, highlighting the critical role of
signal sparsity. This study investigates the relationship between frequency
diversity and signal sparsity in the AUD problem. Single-antenna users transmit
multiple copies of non-orthogonal pilots across multiple frequency channels and
the base station independently performs AUD in each channel using the
orthogonal matching pursuit algorithm. We note that, although frequency
diversity may improve the likelihood of successful reception of the signals, it
may also damage the channel sparsity level, leading to important trade-offs. We
show that a sparser signal significantly benefits AUD, surpassing the
advantages brought by frequency diversity in scenarios with limited temporal
resources and/or high numbers of receive antennas. Conversely, with longer
pilots and fewer receive antennas, investing in frequency diversity becomes
more impactful, resulting in a tenfold AUD performance improvement.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05689" title="Abstract">arXiv:2402.05689</a> [<a href="/pdf/2402.05689" title="Download PDF">pdf</a>, <a href="/format/2402.05689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of  Average-Reward Restless Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+Y">Yige Hong</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qiaomin Xie</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yudong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weina Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Probability (math.PR)

</div>
<p class="mathjax">We consider the infinite-horizon, average-reward restless bandit problem in
discrete time. We propose a new class of policies that are designed to drive a
progressively larger subset of arms toward the optimal distribution. We show
that our policies are asymptotically optimal with an $O(1/\sqrt{N})$ optimality
gap for an $N$-armed problem, provided that the single-armed relaxed problem is
unichain and aperiodic. Our approach departs from most existing work that
focuses on index or priority policies, which rely on the Uniform Global
Attractor Property (UGAP) to guarantee convergence to the optimum, or a
recently developed simulation-based policy, which requires a Synchronization
Assumption (SA).
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05698" title="Abstract">arXiv:2402.05698</a> [<a href="/pdf/2402.05698" title="Download PDF">pdf</a>, <a href="/format/2402.05698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolving AI for Wellness: Dynamic and Personalized Real-time Loneliness  Detection Using Passive Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qirtas%2C+M+M">Malik Muhammad Qirtas</a>, 
<a href="/search/cs?searchtype=author&query=Zafeiridi%2C+E">Evi Zafeiridi</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+E+B">Eleanor Bantry White</a>, 
<a href="/search/cs?searchtype=author&query=Pesch%2C+D">Dirk Pesch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Loneliness is a growing health concern as it can lead to depression and other
associated mental health problems for people who experience feelings of
loneliness over prolonged periods of time. Utilizing passive sensing methods
that use smartphone and wearable sensor data to capture daily behavioural
patterns offers a promising approach for the early detection of loneliness.
Given the subjective nature of loneliness and people's varying daily routines,
past detection approaches using machine learning models often face challenges
with effectively detecting loneliness. This paper proposes a methodologically
novel approach, particularly developing a loneliness detection system that
evolves over time, adapts to new data, and provides real-time detection. Our
study utilized the Globem dataset, a comprehensive collection of passive
sensing data acquired over 10 weeks from university students. The base of our
approach is the continuous identification and refinement of similar behavioural
groups among students using an incremental clustering method. As we add new
data, the model improves based on changing behavioural patterns. Parallel to
this, we create and update classification models to detect loneliness among the
evolving behavioural groups of students. When unique behavioural patterns are
observed among student data, specialized classification models have been
created. For predictions of loneliness, a collaborative effort between the
generalized and specialized models is employed, treating each prediction as a
vote. This study's findings reveal that group-based loneliness detection models
exhibit superior performance compared to generic models, underscoring the
necessity for more personalized approaches tailored to specific behavioural
patterns. These results pave the way for future research, emphasizing the
development of finely-tuned, individualized mental health interventions.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05699" title="Abstract">arXiv:2402.05699</a> [<a href="/pdf/2402.05699" title="Download PDF">pdf</a>, <a href="/format/2402.05699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Alignment of Large Language Models via Monopolylogue-based Social  Scene Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+X">Xianghe Pang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Shuo Tang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+R">Rui Ye</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Y">Yuxin Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bolun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Siheng Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Aligning large language models (LLMs) with human values is imperative to
mitigate potential adverse effects resulting from their misuse. Drawing from
the sociological insight that acknowledging all parties' concerns is a key
factor in shaping human values, this paper proposes a novel direction to align
LLMs by themselves: social scene simulation. To achieve this, we present
MATRIX, a novel social scene simulator that emulates realistic scenes around a
user's input query, enabling the LLM to take social consequences into account
before responding. MATRIX serves as a virtual rehearsal space, akin to a
Monopolylogue, where the LLM performs diverse roles related to the query and
practice by itself. To inject this alignment, we fine-tune the LLM with
MATRIX-simulated data, ensuring adherence to human values without compromising
inference speed. We theoretically show that the LLM with MATRIX outperforms
Constitutional AI under mild assumptions. Finally, extensive experiments
validate that our method outperforms over 10 baselines across 4 benchmarks. As
evidenced by 875 user ratings, our tuned 13B-size LLM exceeds GPT-4 in aligning
with human values. Code is available at https://github.com/pangxianghe/MATRIX.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05703" title="Abstract">arXiv:2402.05703</a> [<a href="/pdf/2402.05703" title="Download PDF">pdf</a>, <a href="/format/2402.05703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Offline Risk-sensitive RL with Partial Observability to Enhance  Performance in Human-Robot Teaming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Angelotti%2C+G">Giorgio Angelotti</a>, 
<a href="/search/cs?searchtype=author&query=Chanel%2C+C+P+C">Caroline P. C. Chanel</a>, 
<a href="/search/cs?searchtype=author&query=Pinto%2C+A+H+M">Adam H. M. Pinto</a>, 
<a href="/search/cs?searchtype=author&query=Lounis%2C+C">Christophe Lounis</a>, 
<a href="/search/cs?searchtype=author&query=Chauffaut%2C+C">Corentin Chauffaut</a>, 
<a href="/search/cs?searchtype=author&query=Drougard%2C+N">Nicolas Drougard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a full paper at AAMAS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">The integration of physiological computing into mixed-initiative human-robot
interaction systems offers valuable advantages in autonomous task allocation by
incorporating real-time features as human state observations into the
decision-making system. This approach may alleviate the cognitive load on human
operators by intelligently allocating mission tasks between agents.
Nevertheless, accommodating a diverse pool of human participants with varying
physiological and behavioral measurements presents a substantial challenge. To
address this, resorting to a probabilistic framework becomes necessary, given
the inherent uncertainty and partial observability on the human's state. Recent
research suggests to learn a Partially Observable Markov Decision Process
(POMDP) model from a data set of previously collected experiences that can be
solved using Offline Reinforcement Learning (ORL) methods. In the present work,
we not only highlight the potential of partially observable representations and
physiological measurements to improve human operator state estimation and
performance, but also enhance the overall mission effectiveness of a
human-robot team. Importantly, as the fixed data set may not contain enough
information to fully represent complex stochastic processes, we propose a
method to incorporate model uncertainty, thus enabling risk-sensitive
sequential decision-making. Experiments were conducted with a group of
twenty-six human participants within a simulated robot teleoperation
environment, yielding empirical evidence of the method's efficacy. The obtained
adaptive task allocation policy led to statistically significant higher scores
than the one that was used to collect the data set, allowing for generalization
across diverse participants also taking into account risk-sensitive metrics.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05706" title="Abstract">arXiv:2402.05706</a> [<a href="/pdf/2402.05706" title="Download PDF">pdf</a>, <a href="/format/2402.05706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified Speech-Text Pretraining for Spoken Dialog Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Heeseung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+S">Soonshin Seo</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+K">Kyeongseok Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+O">Ohsung Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jungwhan Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jaehong Lee</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+E">Eunwoo Song</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+M">Myungwoo Oh</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+S">Sungroh Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+K+M">Kang Min Yoo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">While recent work shows promising results in expanding the capabilities of
large language models (LLM) to directly understand and synthesize speech, an
LLM-based strategy for modeling spoken dialogs remains elusive and calls for
further investigation. This work proposes an extensive speech-text LLM
framework, named the Unified Spoken Dialog Model (USDM), to generate coherent
spoken responses with organic prosodic features relevant to the given input
speech without relying on automatic speech recognition (ASR) or text-to-speech
(TTS) solutions. Our approach employs a multi-step speech-text inference scheme
that leverages chain-of-reasoning capabilities exhibited by the underlying LLM.
We also propose a generalized speech-text pretraining scheme that helps with
capturing cross-modal semantics. Automatic and human evaluations show that the
proposed approach is effective in generating natural-sounding spoken responses,
outperforming both prior and cascaded baselines. Detailed comparative studies
reveal that, despite the cascaded approach being stronger in individual
components, the joint speech-text modeling improves robustness against
recognition errors and speech quality. Demo is available at
https://unifiedsdm.github.io.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05707" title="Abstract">arXiv:2402.05707</a> [<a href="/pdf/2402.05707" title="Download PDF">pdf</a>, <a href="/ps/2402.05707" title="Download PostScript">ps</a>, <a href="/format/2402.05707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neumann-Neumann type domain decomposition of elliptic problems on metric  graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kov%C3%A1cs%2C+M">Mih&#xe1;ly Kov&#xe1;cs</a>, 
<a href="/search/math?searchtype=author&query=V%C3%A1ghy%2C+M+A">Mih&#xe1;ly Andr&#xe1;s V&#xe1;ghy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper we develop a Neumann-Neumann type domain decomposition method
for elliptic problems on metric graphs. We describe the iteration in the
continuous and discrete setting, reformulate the latter in the abstract
additive Schwarz framework and prove its convergence to the finite element
solution. We provide an implementation and test it on various examples of
interest.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05709" title="Abstract">arXiv:2402.05709</a> [<a href="/pdf/2402.05709" title="Download PDF">pdf</a>, <a href="/format/2402.05709" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Nostr Ecosystem: A Study of Decentralization and  Resilience
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yiluo Wei</a>, 
<a href="/search/cs?searchtype=author&query=Tyson%2C+G">Gareth Tyson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Nostr is an open decentralized social network launched in 2022. From a user's
perspective, it is similar to a micro-blogging service like Twitter. However,
the underlying infrastructure is very different, and Nostr boasts a range of
unique features that set it apart. Nostr introduces the concept of relays,
which act as open storage servers that receive, store, and distribute user
posts. Each user is uniquely identified by a public key, ensuring authenticity
of posts through digital signatures. Consequently, users are able to securely
send and receive posts through various relays, which frees them from
single-server reliance and enhances post availability (e.g., making it more
censorship resistant). The Nostr ecosystem has garnered significant attention,
boasting 4 million users and 60 million posts in just 2 years. To understand
its characteristics and challenges, we conduct the first large-scale
measurement of the Nostr ecosystem, spanning from July 1, 2023, to December 31,
2023. Our study focuses on two key aspects: Nostr relays and post replication
strategies. We find that Nostr achieves superior decentralization compared to
traditional Fediverse applications. However, relay availability remains a
challenge, where financial sustainability (particularly for free-to-use relays)
emerges as a contributing factor. We also find that the replication of posts
across relays enhances post availability but introduces significant overhead.
To address this, we propose two design innovations. One to control the number
of post replications, and another to reduce the overhead during post retrieval.
Via data-driven evaluations, we demonstrate their effectiveness without
negatively impacting the system.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05711" title="Abstract">arXiv:2402.05711</a> [<a href="/pdf/2402.05711" title="Download PDF">pdf</a>, <a href="/format/2402.05711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Changeset Descriptions as a Data Source to Assist Feature Location
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chochlov%2C+M">Muslim Chochlov</a>, 
<a href="/search/cs?searchtype=author&query=English%2C+M">Michael English</a>, 
<a href="/search/cs?searchtype=author&query=Buckley%2C+J">Jim Buckley</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE 15th International Working Conference on Source Code Analysis
  and Manipulation (SCAM), 2015
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Feature location attempts to assist developers in discovering functionality
in source code. Many textual feature location techniques utilize information
retrieval and rely on comments and identifiers of source code to describe
software entities. An interesting alternative would be to employ the changeset
descriptions of the code altered in that changeset as a data source to describe
such software entities. To investigate this we implement a technique utilizing
changeset descriptions and conduct an empirical study to observe this
technique's overall performance. Moreover, we study how the granularity (i.e.
file or method level of software entities) and changeset range inclusion (i.e.
most recent or all historical changesets) affect such an approach. The results
of a preliminary study with Rhino and Mylyn.Tasks systems suggest that the
approach could lead to a potentially efficient feature location technique. They
also suggest that it is advantageous in terms of the effort to configure the
technique at method level granularity and that older changesets from older
systems may reduce the effectiveness of the technique.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05712" title="Abstract">arXiv:2402.05712</a> [<a href="/pdf/2402.05712" title="Download PDF">pdf</a>, <a href="/format/2402.05712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion  Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhiyuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiangyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+G">Guojun Qi</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+C">Chen Qian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaoxiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Z">Zhen Lei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures. Code is avalable at <a href="https://github.com/theEricMa/DiffSpeaker">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Speech-driven 3D facial animation is important for many multimedia
applications. Recent work has shown promise in using either Diffusion models or
Transformer architectures for this task. However, their mere aggregation does
not lead to improved performance. We suspect this is due to a shortage of
paired audio-4D data, which is crucial for the Transformer to effectively
perform as a denoiser within the Diffusion framework. To tackle this issue, we
present DiffSpeaker, a Transformer-based network equipped with novel biased
conditional attention modules. These modules serve as substitutes for the
traditional self/cross-attention in standard Transformers, incorporating
thoughtfully designed biases that steer the attention mechanisms to concentrate
on both the relevant task-specific and diffusion-related conditions. We also
explore the trade-off between accurate lip synchronization and non-verbal
facial expressions within the Diffusion paradigm. Experiments show our model
not only achieves state-of-the-art performance on existing benchmarks, but also
fast inference speed owing to its ability to generate facial motions in
parallel.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05713" title="Abstract">arXiv:2402.05713</a> [<a href="/pdf/2402.05713" title="Download PDF">pdf</a>, <a href="/format/2402.05713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on  Vulnerable Patient Populations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+P">Pranav Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+A">Andrew Chan</a>, 
<a href="/search/cs?searchtype=author&query=Navarathna%2C+N">Nithya Navarathna</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+S">Skylar Chan</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+P+H">Paul H. Yi</a>, 
<a href="/search/cs?searchtype=author&query=Parekh%2C+V+S">Vishwa S. Parekh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 20 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The proliferation of artificial intelligence (AI) in radiology has shed light
on the risk of deep learning (DL) models exacerbating clinical biases towards
vulnerable patient populations. While prior literature has focused on
quantifying biases exhibited by trained DL models, demographically targeted
adversarial bias attacks on DL models and its implication in the clinical
environment remains an underexplored field of research in medical imaging. In
this work, we demonstrate that demographically targeted label poisoning attacks
can introduce adversarial underdiagnosis bias in DL models and degrade
performance on underrepresented groups without impacting overall model
performance. Moreover, our results across multiple performance metrics and
demographic groups like sex, age, and their intersectional subgroups indicate
that a group's vulnerability to undetectable adversarial bias attacks is
directly correlated with its representation in the model's training data.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05722" title="Abstract">arXiv:2402.05722</a> [<a href="/pdf/2402.05722" title="Download PDF">pdf</a>, <a href="/format/2402.05722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physical Layer Security over Fluid Antenna Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghadi%2C+F+R">Farshad Rostami Ghadi</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kai-Kit Wong</a>, 
<a href="/search/cs?searchtype=author&query=Lopez-Martinez%2C+F+J">F. Javier Lopez-Martinez</a>, 
<a href="/search/cs?searchtype=author&query=New%2C+W+K">Wee Kiat New</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chae%2C+C">Chan-Byoung Chae</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">This paper investigates the performance of physical layer security (PLS) in
fluid antenna-aided communication systems under arbitrary correlated fading
channels. In particular, it is considered that a single fixed-antenna
transmitter aims to send confidential information to a legitimate receiver
equipped with a planar fluid antenna system (FAS), while an eavesdropper, also
taking advantage of a planar FAS, attempts to decode the desired message. For
this scenario, we first present analytical expressions of the equivalent
channel distributions at the legitimate user and eavesdropper by using copula,
so that the obtained analytical results are valid for any arbitrarily
correlated fading distributions. Then, with the help of Gauss-Laguerre
quadrature, we derive compact analytical expressions for the average secrecy
capacity (ASC), the secrecy outage probability (SOP), and the secrecy energy
efficiency (SEE) for the FAS wiretap channel. Moreover, for exemplary purposes,
we also obtain the compact expression of ASC, SOP, and SEE by utilizing the
Gaussian copula under correlated Rayleigh fading channels as a special case.
Eventually, numerical results indicate that applying the fluid antenna with
only one active port to PLS can guarantee more secure and reliable
transmission, when compared to traditional antenna systems (TAS) exploiting
maximal ratio combining (MRC).
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05723" title="Abstract">arXiv:2402.05723</a> [<a href="/pdf/2402.05723" title="Download PDF">pdf</a>, <a href="/format/2402.05723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Learning Can Re-learn Forbidden Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xhonneux%2C+S">Sophie Xhonneux</a>, 
<a href="/search/cs?searchtype=author&query=Dobre%2C+D">David Dobre</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jian Tang</a>, 
<a href="/search/cs?searchtype=author&query=Gidel%2C+G">Gauthier Gidel</a>, 
<a href="/search/cs?searchtype=author&query=Sridhar%2C+D">Dhanya Sridhar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Despite significant investment into safety training, large language models
(LLMs) deployed in the real world still suffer from numerous vulnerabilities.
One perspective on LLM safety training is that it algorithmically forbids the
model from answering toxic or harmful queries. To assess the effectiveness of
safety training, in this work, we study forbidden tasks, i.e., tasks the model
is designed to refuse to answer. Specifically, we investigate whether
in-context learning (ICL) can be used to re-learn forbidden tasks despite the
explicit fine-tuning of the model to refuse them. We first examine a toy
example of refusing sentiment classification to demonstrate the problem. Then,
we use ICL on a model fine-tuned to refuse to summarise made-up news articles.
Finally, we investigate whether ICL can undo safety training, which could
represent a major security risk. For the safety task, we look at Vicuna-7B,
Starling-7B, and Llama2-7B. We show that the attack works out-of-the-box on
Starling-7B and Vicuna-7B but fails on Llama2-7B. Finally, we propose an ICL
attack that uses the chat template tokens like a prompt injection attack to
achieve a better attack success rate on Vicuna-7B and Starling-7B.
<br />Trigger Warning: the appendix contains LLM-generated text with violence,
suicide, and misinformation.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05724" title="Abstract">arXiv:2402.05724</a> [<a href="/pdf/2402.05724" title="Download PDF">pdf</a>, <a href="/format/2402.05724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-Based RL for Mean-Field Games is not Statistically Harder than  Single-Agent RL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiawei Huang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+N">Niao He</a>, 
<a href="/search/cs?searchtype=author&query=Krause%2C+A">Andreas Krause</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 49 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the sample complexity of reinforcement learning (RL) in Mean-Field
Games (MFGs) with model-based function approximation that requires strategic
exploration to find a Nash Equilibrium policy. We introduce the Partial
Model-Based Eluder Dimension (P-MBED), a more effective notion to characterize
the model class complexity. Notably, P-MBED measures the complexity of the
single-agent model class converted from the given mean-field model class, and
potentially, can be exponentially lower than the MBED proposed by
\citet{huang2023statistical}. We contribute a model elimination algorithm
featuring a novel exploration strategy and establish sample complexity results
polynomial w.r.t.~P-MBED. Crucially, our results reveal that, under the basic
realizability and Lipschitz continuity assumptions, \emph{learning Nash
Equilibrium in MFGs is no more statistically challenging than solving a
logarithmic number of single-agent RL problems}. We further extend our results
to Multi-Type MFGs, generalizing from conventional MFGs and involving multiple
types of agents. This extension implies statistical tractability of a broader
class of Markov Games through the efficacy of mean-field approximation.
Finally, inspired by our theoretical algorithm, we present a heuristic approach
with improved computational efficiency and empirically demonstrate its
effectiveness.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05725" title="Abstract">arXiv:2402.05725</a> [<a href="/pdf/2402.05725" title="Download PDF">pdf</a>, <a href="/format/2402.05725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual-modal Tactile E-skin: Enabling Bidirectional Human-Robot  Interaction via Integrated Tactile Perception and Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mu%2C+S">Shilong Mu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Runze Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zenan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shoujie Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenchang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiao-Ping Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+W">Wenbo Ding</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 8 figures. Submitted to 2024 IEEE International Conference on Robotics and Automation (ICRA), Japan, Yokohama
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">To foster an immersive and natural human-robot interaction, the
implementation of tactile perception and feedback becomes imperative,
effectively bridging the conventional sensory gap. In this paper, we propose a
dual-modal electronic skin (e-skin) that integrates magnetic tactile sensing
and vibration feedback for enhanced human-robot interaction. The dual-modal
tactile e-skin offers multi-functional tactile sensing and programmable haptic
feedback, underpinned by a layered structure comprised of flexible magnetic
films, soft silicone, a Hall sensor and actuator array, and a microcontroller
unit. The e-skin captures the magnetic field changes caused by subtle
deformations through Hall sensors, employing deep learning for accurate tactile
perception. Simultaneously, the actuator array generates mechanical vibrations
to facilitate haptic feedback, delivering diverse mechanical stimuli. Notably,
the dual-modal e-skin is capable of transmitting tactile information
bidirectionally, enabling object recognition and fine-weighing operations. This
bidirectional tactile interaction framework will enhance the immersion and
efficiency of interactions between humans and robots.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05728" title="Abstract">arXiv:2402.05728</a> [<a href="/pdf/2402.05728" title="Download PDF">pdf</a>, <a href="/format/2402.05728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CTGAN: Semantic-guided Conditional Texture Generator for 3D Shapes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yi-Ting Pan</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Chai-Rong Lee</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+S">Shu-Ho Fan</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jheng-Wei Su</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jia-Bin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chuang%2C+Y">Yung-Yu Chuang</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+H">Hung-Kuo Chu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The entertainment industry relies on 3D visual content to create immersive
experiences, but traditional methods for creating textured 3D models can be
time-consuming and subjective. Generative networks such as StyleGAN have
advanced image synthesis, but generating 3D objects with high-fidelity textures
is still not well explored, and existing methods have limitations. We propose
the Semantic-guided Conditional Texture Generator (CTGAN), producing
high-quality textures for 3D shapes that are consistent with the viewing angle
while respecting shape semantics. CTGAN utilizes the disentangled nature of
StyleGAN to finely manipulate the input latent codes, enabling explicit control
over both the style and structure of the generated textures. A coarse-to-fine
encoder architecture is introduced to enhance control over the structure of the
resulting textures via input segmentation. Experimental results show that CTGAN
outperforms existing methods on multiple quality metrics and achieves
state-of-the-art performance on texture generation in both conditional and
unconditional settings.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05731" title="Abstract">arXiv:2402.05731</a> [<a href="/pdf/2402.05731" title="Download PDF">pdf</a>, <a href="/format/2402.05731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Framework for Assessing Proportionate Intervention with Face  Recognition Systems in Real-Life Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Negri%2C+P">Pablo Negri</a>, 
<a href="/search/cs?searchtype=author&query=Hupont%2C+I">Isabelle Hupont</a>, 
<a href="/search/cs?searchtype=author&query=Gomez%2C+E">Emilia Gomez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the second revision round of IEEE Face and Gesture Recognition 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Face recognition (FR) has reached a high technical maturity. However, its use
needs to be carefully assessed from an ethical perspective, especially in
sensitive scenarios. This is precisely the focus of this paper: the use of FR
for the identification of specific subjects in moderately to densely crowded
spaces (e.g. public spaces, sports stadiums, train stations) and law
enforcement scenarios. In particular, there is a need to consider the trade-off
between the need to protect privacy and fundamental rights of citizens as well
as their safety. Recent Artificial Intelligence (AI) policies, notably the
European AI Act, propose that such FR interventions should be proportionate and
deployed only when strictly necessary. Nevertheless, concrete guidelines on how
to address the concept of proportional FR intervention are lacking to date.
This paper proposes a framework to contribute to assessing whether an FR
intervention is proportionate or not for a given context of use in the above
mentioned scenarios. It also identifies the main quantitative and qualitative
variables relevant to the FR intervention decision (e.g. number of people in
the scene, level of harm that the person(s) in search could perpetrate,
consequences to individual rights and freedoms) and propose a 2D graphical
model making it possible to balance these variables in terms of ethical cost vs
security gain. Finally, different FR scenarios inspired by real-world
deployments validate the proposed model. The framework is conceived as a simple
support tool for decision makers when confronted with the deployment of an FR
system.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05733" title="Abstract">arXiv:2402.05733</a> [<a href="/pdf/2402.05733" title="Download PDF">pdf</a>, <a href="/format/2402.05733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TimeArena: Shaping Efficient Multitasking Language Agents in a  Time-Aware Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yikai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Siyu Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+C">Caiyu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Richardson%2C+K">Kyle Richardson</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yanghua Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiangjie Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Despite remarkable advancements in emulating human-like behavior through
Large Language Models (LLMs), current textual simulations do not adequately
address the notion of time. To this end, we introduce TimeArena, a novel
textual simulated environment that incorporates complex temporal dynamics and
constraints that better reflect real-life planning scenarios. In TimeArena,
agents are asked to complete multiple tasks as soon as possible, allowing for
parallel processing to save time. We implement the dependency between actions,
the time duration for each action, and the occupancy of the agent and the
objects in the environment. TimeArena grounds to 30 real-world tasks in
cooking, household activities, and laboratory work. We conduct extensive
experiments with various state-of-the-art LLMs using TimeArena. Our findings
reveal that even the most powerful models, e.g., GPT-4, still lag behind humans
in effective multitasking, underscoring the need for enhanced temporal
awareness in the development of language agents.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05737" title="Abstract">arXiv:2402.05737</a> [<a href="/pdf/2402.05737" title="Download PDF">pdf</a>, <a href="/format/2402.05737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blockchain Based Residential Smart Rent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Proen%C3%A7a%2C+A+S">Andr&#xe9; S. Proen&#xe7;a</a>, 
<a href="/search/cs?searchtype=author&query=Dias%2C+T+R">Tiago R. Dias</a>, 
<a href="/search/cs?searchtype=author&query=Correia%2C+M+P">Miguel P. Correia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The real estate market includes complex and inefficient mediation processes.
Renting a property envolves multiple entities with different responsibilities
and interests. Therefore it is imperative to establish a trustful relationship
between parties through intermediaries such as notaries, banks or real estate
agencies to avoid eventual disputes. Although an intermediary ensures trust,
the current process still has some drawbacks concerning efficiency, costs,
transparency, bureaucracy and data security. The blockchain technology aims to
reduce this issues by providing transparent and secure real estate
transactions. We propose a GDPR compliant blockchain-based residential smart
rental platform, designed to allow both landlords and tenants to establish
rental contracts and make rental payments securely.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05738" title="Abstract">arXiv:2402.05738</a> [<a href="/pdf/2402.05738" title="Download PDF">pdf</a>, <a href="/format/2402.05738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit Bias and Fast Convergence Rates for Self-attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vasudeva%2C+B">Bhavya Vasudeva</a>, 
<a href="/search/cs?searchtype=author&query=Deora%2C+P">Puneesh Deora</a>, 
<a href="/search/cs?searchtype=author&query=Thrampoulidis%2C+C">Christos Thrampoulidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">Self-attention, the core mechanism of transformers, distinguishes them from
traditional neural networks and drives their outstanding performance. Towards
developing the fundamental optimization principles of self-attention, we
investigate the implicit bias of gradient descent (GD) in training a
self-attention layer with fixed linear decoder in binary classification.
Drawing inspiration from the study of GD in linear logistic regression over
separable data, recent work demonstrates that as the number of iterations $t$
approaches infinity, the key-query matrix $W_t$ converges locally (with respect
to the initialization direction) to a hard-margin SVM solution $W_{mm}$. Our
work enhances this result in four aspects. Firstly, we identify non-trivial
data settings for which convergence is provably global, thus shedding light on
the optimization landscape. Secondly, we provide the first finite-time
convergence rate for $W_t$ to $W_{mm}$, along with quantifying the rate of
sparsification in the attention map. Thirdly, through an analysis of normalized
GD and Polyak step-size, we demonstrate analytically that adaptive step-size
rules can accelerate the convergence of self-attention. Additionally, we remove
the restriction of prior work on a fixed linear decoder. Our results reinforce
the implicit-bias perspective of self-attention and strengthen its connections
to implicit-bias in linear logistic regression, despite the intricate
non-convex nature of the former.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05740" title="Abstract">arXiv:2402.05740</a> [<a href="/pdf/2402.05740" title="Download PDF">pdf</a>, <a href="/format/2402.05740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CounterCLR: Counterfactual Contrastive Learning with Non-random Missing  Data in Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+D">Dongxu Liang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+E">Enyun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ou%2C+W">Wenwu Ou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenjia Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2023 IEEE International Conference on Data Mining (ICDM)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Recommender systems are designed to learn user preferences from observed
feedback and comprise many fundamental tasks, such as rating prediction and
post-click conversion rate (pCVR) prediction. However, the observed feedback
usually suffer from two issues: selection bias and data sparsity, where biased
and insufficient feedback seriously degrade the performance of recommender
systems in terms of accuracy and ranking. Existing solutions for handling the
issues, such as data imputation and inverse propensity score, are highly
susceptible to additional trained imputation or propensity models. In this
work, we propose a novel counterfactual contrastive learning framework for
recommendation, named CounterCLR, to tackle the problem of non-random missing
data by exploiting the advances in contrast learning. Specifically, the
proposed CounterCLR employs a deep representation network, called CauNet, to
infer non-random missing data in recommendations and perform user preference
modeling by further introducing a self-supervised contrastive learning task.
Our CounterCLR mitigates the selection bias problem without the need for
additional models or estimators, while also enhancing the generalization
ability in cases of sparse data. Experiments on real-world datasets demonstrate
the effectiveness and superiority of our method.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05741" title="Abstract">arXiv:2402.05741</a> [<a href="/pdf/2402.05741" title="Download PDF">pdf</a>, <a href="/format/2402.05741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-World Robot Applications of Foundation Models: A Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kawaharazuka%2C+K">Kento Kawaharazuka</a>, 
<a href="/search/cs?searchtype=author&query=Matsushima%2C+T">Tatsuya Matsushima</a>, 
<a href="/search/cs?searchtype=author&query=Gambardella%2C+A">Andrew Gambardella</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jiaxian Guo</a>, 
<a href="/search/cs?searchtype=author&query=Paxton%2C+C">Chris Paxton</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+A">Andy Zeng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent developments in foundation models, like Large Language Models (LLMs)
and Vision-Language Models (VLMs), trained on extensive data, facilitate
flexible application across different tasks and modalities. Their impact spans
various fields, including healthcare, education, and robotics. This paper
provides an overview of the practical application of foundation models in
real-world robotics, with a primary emphasis on the replacement of specific
components within existing robot systems. The summary encompasses the
perspective of input-output relationships in foundation models, as well as
their role in perception, motion planning, and control within the field of
robotics. This paper concludes with a discussion of future challenges and
implications for practical robot applications.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05746" title="Abstract">arXiv:2402.05746</a> [<a href="/pdf/2402.05746" title="Download PDF">pdf</a>, <a href="/format/2402.05746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Editable Scene Simulation for Autonomous Driving via Collaborative  LLM-Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yuxi Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yifan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chenxin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Changxing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Siheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanfeng Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Scene simulation in autonomous driving has gained significant attention
because of its huge potential for generating customized data. However, existing
editable scene simulation approaches face limitations in terms of user
interaction efficiency, multi-camera photo-realistic rendering and external
digital assets integration. To address these challenges, this paper introduces
ChatSim, the first system that enables editable photo-realistic 3D driving
scene simulations via natural language commands with external digital assets.
To enable editing with high command flexibility,~ChatSim leverages a large
language model (LLM) agent collaboration framework. To generate photo-realistic
outcomes, ChatSim employs a novel multi-camera neural radiance field method.
Furthermore, to unleash the potential of extensive high-quality digital assets,
ChatSim employs a novel multi-camera lighting estimation method to achieve
scene-consistent assets' rendering. Our experiments on Waymo Open Dataset
demonstrate that ChatSim can handle complex language commands and generate
corresponding photo-realistic scene videos.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05747" title="Abstract">arXiv:2402.05747</a> [<a href="/pdf/2402.05747" title="Download PDF">pdf</a>, <a href="/format/2402.05747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jacquard V2: Refining Datasets using the Human In the Loop Data  Correction Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qiuhao Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Shenghai Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the context of rapid advancements in industrial automation, vision-based
robotic grasping plays an increasingly crucial role. In order to enhance visual
recognition accuracy, the utilization of large-scale datasets is imperative for
training models to acquire implicit knowledge related to the handling of
various objects. Creating datasets from scratch is a time and labor-intensive
process. Moreover, existing datasets often contain errors due to automated
annotations aimed at expediency, making the improvement of these datasets a
substantial research challenge. Consequently, several issues have been
identified in the annotation of grasp bounding boxes within the popular
Jacquard Grasp. We propose utilizing a Human-In-The-Loop(HIL) method to enhance
dataset quality. This approach relies on backbone deep learning networks to
predict object positions and orientations for robotic grasping. Predictions
with Intersection over Union (IOU) values below 0.2 undergo an assessment by
human operators. After their evaluation, the data is categorized into False
Negatives(FN) and True Negatives(TN). FN are then subcategorized into either
missing annotations or catastrophic labeling errors. Images lacking labels are
augmented with valid grasp bounding box information, whereas images afflicted
by catastrophic labeling errors are completely removed. The open-source tool
Labelbee was employed for 53,026 iterations of HIL dataset enhancement, leading
to the removal of 2,884 images and the incorporation of ground truth
information for 30,292 images. The enhanced dataset, named the Jacquard V2
Grasping Dataset, served as the training data for a range of neural networks.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05749" title="Abstract">arXiv:2402.05749</a> [<a href="/pdf/2402.05749" title="Download PDF">pdf</a>, <a href="/format/2402.05749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Preference Optimization: A Unified Approach to Offline  Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yunhao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z+D">Zhaohan Daniel Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zeyu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Calandriello%2C+D">Daniele Calandriello</a>, 
<a href="/search/cs?searchtype=author&query=Munos%2C+R">R&#xe9;mi Munos</a>, 
<a href="/search/cs?searchtype=author&query=Rowland%2C+M">Mark Rowland</a>, 
<a href="/search/cs?searchtype=author&query=Richemond%2C+P+H">Pierre Harvey Richemond</a>, 
<a href="/search/cs?searchtype=author&query=Valko%2C+M">Michal Valko</a>, 
<a href="/search/cs?searchtype=author&query=Pires%2C+B+%C3%81">Bernardo &#xc1;vila Pires</a>, 
<a href="/search/cs?searchtype=author&query=Piot%2C+B">Bilal Piot</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Offline preference optimization allows fine-tuning large models directly from
offline data, and has proved effective in recent alignment practices. We
propose generalized preference optimization (GPO), a family of offline losses
parameterized by a general class of convex functions. GPO enables a unified
view over preference optimization, encompassing existing algorithms such as
DPO, IPO and SLiC as special cases, while naturally introducing new variants.
The GPO framework also sheds light on how offline algorithms enforce
regularization, through the design of the convex function that defines the
loss. Our analysis and experiments reveal the connections and subtle
differences between the offline regularization and the KL divergence
regularization intended by the canonical RLHF formulation. In all, our results
present new algorithmic toolkits and empirical insights to alignment
practitioners.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05755" title="Abstract">arXiv:2402.05755</a> [<a href="/pdf/2402.05755" title="Download PDF">pdf</a>, <a href="/format/2402.05755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpiRit-LM: Interleaved Spoken and Written Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+A">Tu Anh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Muller%2C+B">Benjamin Muller</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bokai Yu</a>, 
<a href="/search/cs?searchtype=author&query=Costa-jussa%2C+M+R">Marta R. Costa-jussa</a>, 
<a href="/search/cs?searchtype=author&query=Elbayad%2C+M">Maha Elbayad</a>, 
<a href="/search/cs?searchtype=author&query=Popuri%2C+S">Sravya Popuri</a>, 
<a href="/search/cs?searchtype=author&query=Duquenne%2C+P">Paul-Ambroise Duquenne</a>, 
<a href="/search/cs?searchtype=author&query=Algayres%2C+R">Robin Algayres</a>, 
<a href="/search/cs?searchtype=author&query=Mavlyutov%2C+R">Ruslan Mavlyutov</a>, 
<a href="/search/cs?searchtype=author&query=Gat%2C+I">Itai Gat</a>, 
<a href="/search/cs?searchtype=author&query=Synnaeve%2C+G">Gabriel Synnaeve</a>, 
<a href="/search/cs?searchtype=author&query=Pino%2C+J">Juan Pino</a>, 
<a href="/search/cs?searchtype=author&query=Sagot%2C+B">Benoit Sagot</a>, 
<a href="/search/cs?searchtype=author&query=Dupoux%2C+E">Emmanuel Dupoux</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We introduce SPIRIT-LM, a foundation multimodal language model that freely
mixes text and speech. Our model is based on a pretrained text language model
that we extend to the speech modality by continuously training it on text and
speech units. Speech and text sequences are concatenated as a single set of
tokens, and trained with a word-level interleaving method using a small
automatically-curated speech-text parallel corpus. SPIRIT-LM comes in two
versions: a BASE version that uses speech semantic units and an EXPRESSIVE
version that models expressivity using pitch and style units in addition to the
semantic units. For both versions, the text is encoded with subword BPE tokens.
The resulting model displays both the semantic abilities of text models and the
expressive abilities of speech models. Additionally, we demonstrate that
SPIRIT-LM is able to learn new tasks in a few-shot fashion across modalities
(i.e. ASR, TTS, Speech Classification).
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05757" title="Abstract">arXiv:2402.05757</a> [<a href="/pdf/2402.05757" title="Download PDF">pdf</a>, <a href="/format/2402.05757" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When is Mean-Field Reinforcement Learning Tractable and Relevant?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yardim%2C+B">Batuhan Yardim</a>, 
<a href="/search/cs?searchtype=author&query=Goldman%2C+A">Artur Goldman</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+N">Niao He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Multiagent Systems (cs.MA); Optimization and Control (math.OC)

</div>
<p class="mathjax">Mean-field reinforcement learning has become a popular theoretical framework
for efficiently approximating large-scale multi-agent reinforcement learning
(MARL) problems exhibiting symmetry. However, questions remain regarding the
applicability of mean-field approximations: in particular, their approximation
accuracy of real-world systems and conditions under which they become
computationally tractable. We establish explicit finite-agent bounds for how
well the MFG solution approximates the true $N$-player game for two popular
mean-field solution concepts. Furthermore, for the first time, we establish
explicit lower bounds indicating that MFGs are poor or uninformative at
approximating $N$-player games assuming only Lipschitz dynamics and rewards.
Finally, we analyze the computational complexity of solving MFGs with only
Lipschitz properties and prove that they are in the class of
\textsc{PPAD}-complete problems conjectured to be intractable, similar to
general sum $N$ player games. Our theoretical results underscore the
limitations of MFGs and complement and justify existing work by proving
difficulty in the absence of common theoretical assumptions.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05758" title="Abstract">arXiv:2402.05758</a> [<a href="/pdf/2402.05758" title="Download PDF">pdf</a>, <a href="/format/2402.05758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent variable model for high-dimensional point process with structured  missingness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sinelnikov%2C+M">Maksim Sinelnikov</a>, 
<a href="/search/cs?searchtype=author&query=Haussmann%2C+M">Manuel Haussmann</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%A4hdesm%C3%A4ki%2C+H">Harri L&#xe4;hdesm&#xe4;ki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Longitudinal data are important in numerous fields, such as healthcare,
sociology and seismology, but real-world datasets present notable challenges
for practitioners because they can be high-dimensional, contain structured
missingness patterns, and measurement time points can be governed by an unknown
stochastic process. While various solutions have been suggested, the majority
of them have been designed to account for only one of these challenges. In this
work, we propose a flexible and efficient latent-variable model that is capable
of addressing all these limitations. Our approach utilizes Gaussian processes
to capture temporal correlations between samples and their associated
missingness masks as well as to model the underlying point process. We
construct our model as a variational autoencoder together with deep neural
network parameterised encoder and decoder models, and develop a scalable
amortised variational inference approach for efficient model training. We
demonstrate competitive performance using both simulated and real datasets.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05764" title="Abstract">arXiv:2402.05764</a> [<a href="/pdf/2402.05764" title="Download PDF">pdf</a>, <a href="/ps/2402.05764" title="Download PostScript">ps</a>, <a href="/format/2402.05764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Datastringer: easy dataset monitoring for journalists
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shearer%2C+M">Matt Shearer</a>, 
<a href="/search/cs?searchtype=author&query=Simon%2C+B">Basile Simon</a>, 
<a href="/search/cs?searchtype=author&query=Geiger%2C+C">Cl&#xe9;ment Geiger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">We created a software enabling journalists to define a set of criteria they
would like to see applied regularly to a constantly-updated dataset, sending
them an alert when these criteria are met, thus signaling them that there may
be a story to write. The main challenges were to keep the product scalable and
powerful, while making sure that it could be used by journalists who would not
possess all the technical knowledge to exploit it fully. In order to do so, we
had to choose Javascript as our main language, as well as designing the code in
such a way that it would allow re-usability and further improvements. This
project is a proof of concept being tested in a real-life environment, and will
be developed towards more and more accessibility.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05766" title="Abstract">arXiv:2402.05766</a> [<a href="/pdf/2402.05766" title="Download PDF">pdf</a>, <a href="/format/2402.05766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Off-policy Distributional Q($&#x3bb;$): Distributional RL without  Importance Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yunhao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Rowland%2C+M">Mark Rowland</a>, 
<a href="/search/cs?searchtype=author&query=Munos%2C+R">R&#xe9;mi Munos</a>, 
<a href="/search/cs?searchtype=author&query=Pires%2C+B+%C3%81">Bernardo &#xc1;vila Pires</a>, 
<a href="/search/cs?searchtype=author&query=Dabney%2C+W">Will Dabney</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We introduce off-policy distributional Q($\lambda$), a new addition to the
family of off-policy distributional evaluation algorithms. Off-policy
distributional Q($\lambda$) does not apply importance sampling for off-policy
learning, which introduces intriguing interactions with signed measures. Such
unique properties distributional Q($\lambda$) from other existing alternatives
such as distributional Retrace. We characterize the algorithmic properties of
distributional Q($\lambda$) and validate theoretical insights with tabular
experiments. We show how distributional Q($\lambda$)-C51, a combination of
Q($\lambda$) with the C51 agent, exhibits promising results on deep RL
benchmarks.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05768" title="Abstract">arXiv:2402.05768</a> [<a href="/pdf/2402.05768" title="Download PDF">pdf</a>, <a href="/ps/2402.05768" title="Download PostScript">ps</a>, <a href="/format/2402.05768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A non-damped stabilization algorithm for multibody dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=de+Bustos%2C+I+F">Igor Fernandez de Bustos</a>, 
<a href="/search/math?searchtype=author&query=Uriarte%2C+H">Haritz Uriarte</a>, 
<a href="/search/math?searchtype=author&query=Urkullu%2C+G">Gorka Urkullu</a>, 
<a href="/search/math?searchtype=author&query=Garcia-Marina%2C+V">Vanessa Garcia-Marina</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Meccanica (2022) 57:371-399
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The stability of integrators dealing with high order Differential Algebraic
Equations (DAEs) is a major issue. The usual procedures give rise to
instabilities that are not predicted by the usual linear analysis, rendering
the common checks (developed for ODEs) unusable. The appearance of these
difficult-toexplain and unexpected problems leads to methods that arise heavy
numerical damping for avoiding them. This has the undesired consequences of
lack of convergence of the methods, along with a need of smaller stepsizes. In
this paper a new approach is presented. The algorithm presented here allows us
to avoid the interference of the constraints in the integration, thus allowing
the linear criteria to be applied. In order to do so, the integrator is applied
to a set of instantaneous minimal coordinates that are obtained through the
application of the null space. The new approach can be utilized along with any
integration method. Some experiments using the Newmark method have been carried
out, which validate the methodology and also show that the method behaves in a
predictable way if one considers linear stability criteria.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05773" title="Abstract">arXiv:2402.05773</a> [<a href="/pdf/2402.05773" title="Download PDF">pdf</a>, <a href="/format/2402.05773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UAV-Rain1k: A Benchmark for Raindrop Removal from UAV Aerial Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+W">Wenhui Chang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongming Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xin He</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Liangduo Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Raindrops adhering to the lens of UAVs can obstruct visibility of the
background scene and degrade image quality. Despite recent progress in image
deraining methods and datasets, there is a lack of focus on raindrop removal
from UAV aerial imagery due to the unique challenges posed by varying angles
and rapid movement during drone flight. To fill the gap in this research, we
first construct a new benchmark dataset for removing raindrops from UAV images,
called UAV-Rain1k. In this letter, we provide a dataset generation pipeline,
which includes modeling raindrop shapes using Blender, collecting background
images from various UAV angles, random sampling of rain masks and etc. Based on
the proposed benchmark, we further present a comprehensive evaluation of
existing representative image deraining algorithms, and reveal future research
opportunities worth exploring. The proposed dataset will be publicly available
at https://github.com/cschenxiang/UAV-Rain1k.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05774" title="Abstract">arXiv:2402.05774</a> [<a href="/pdf/2402.05774" title="Download PDF">pdf</a>, <a href="/format/2402.05774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stable Autonomous Flow Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sprague%2C+C+I">Christopher Iliffe Sprague</a>, 
<a href="/search/cs?searchtype=author&query=Elofsson%2C+A">Arne Elofsson</a>, 
<a href="/search/cs?searchtype=author&query=Azizpour%2C+H">Hossein Azizpour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
<p class="mathjax">In contexts where data samples represent a physically stable state, it is
often assumed that the data points represent the local minima of an energy
landscape. In control theory, it is well-known that energy can serve as an
effective Lyapunov function. Despite this, connections between control theory
and generative models in the literature are sparse, even though there are
several machine learning applications with physically stable data points. In
this paper, we focus on such data and a recent class of deep generative models
called flow matching. We apply tools of stochastic stability for
time-independent systems to flow matching models. In doing so, we characterize
the space of flow matching models that are amenable to this treatment, as well
as draw connections to other control theory principles. We demonstrate our
theoretical results on two examples.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05779" title="Abstract">arXiv:2402.05779</a> [<a href="/pdf/2402.05779" title="Download PDF">pdf</a>, <a href="/format/2402.05779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Examining Gender and Racial Bias in Large Vision-Language Models Using a  Novel Dataset of Parallel Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fraser%2C+K+C">Kathleen C. Fraser</a>, 
<a href="/search/cs?searchtype=author&query=Kiritchenko%2C+S">Svetlana Kiritchenko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Following on recent advances in large language models (LLMs) and subsequent
chat models, a new wave of large vision-language models (LVLMs) has emerged.
Such models can incorporate images as input in addition to text, and perform
tasks such as visual question answering, image captioning, story generation,
etc. Here, we examine potential gender and racial biases in such systems, based
on the perceived characteristics of the people in the input images. To
accomplish this, we present a new dataset PAIRS (PArallel Images for eveRyday
Scenarios). The PAIRS dataset contains sets of AI-generated images of people,
such that the images are highly similar in terms of background and visual
content, but differ along the dimensions of gender (man, woman) and race
(Black, white). By querying the LVLMs with such images, we observe significant
differences in the responses according to the perceived gender or race of the
person depicted.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05782" title="Abstract">arXiv:2402.05782</a> [<a href="/pdf/2402.05782" title="Download PDF">pdf</a>, <a href="/format/2402.05782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysing the Sample Complexity of Opponent Shaping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fung%2C+K">Kitty Fung</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qizhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chris Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+J">Jia Wan</a>, 
<a href="/search/cs?searchtype=author&query=Willi%2C+T">Timon Willi</a>, 
<a href="/search/cs?searchtype=author&query=Foerster%2C+J">Jakob Foerster</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> AAMAS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Learning in general-sum games often yields collectively sub-optimal results.
Addressing this, opponent shaping (OS) methods actively guide the learning
processes of other agents, empirically leading to improved individual and group
performances in many settings. Early OS methods use higher-order derivatives to
shape the learning of co-players, making them unsuitable for shaping multiple
learning steps. Follow-up work, Model-free Opponent Shaping (M-FOS), addresses
these by reframing the OS problem as a meta-game. In contrast to early OS
methods, there is little theoretical understanding of the M-FOS framework.
Providing theoretical guarantees for M-FOS is hard because A) there is little
literature on theoretical sample complexity bounds for meta-reinforcement
learning B) M-FOS operates in continuous state and action spaces, so
theoretical analysis is challenging. In this work, we present R-FOS, a tabular
version of M-FOS that is more suitable for theoretical analysis. R-FOS
discretises the continuous meta-game MDP into a tabular MDP. Within this
discretised MDP, we adapt the $R_{max}$ algorithm, most prominently used to
derive PAC-bounds for MDPs, as the meta-learner in the R-FOS algorithm. We
derive a sample complexity bound that is exponential in the cardinality of the
inner state and action space and the number of agents. Our bound guarantees
that, with high probability, the final policy learned by an R-FOS agent is
close to the optimal policy, apart from a constant factor. Finally, we
investigate how R-FOS's sample complexity scales in the size of state-action
space. Our theoretical results on scaling are supported empirically in the
Matching Pennies environment.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05783" title="Abstract">arXiv:2402.05783</a> [<a href="/pdf/2402.05783" title="Download PDF">pdf</a>, <a href="/format/2402.05783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text-to-Code Generation with Modality-relative Pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Christopoulou%2C+F">Fenia Christopoulou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guchun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lampouras%2C+G">Gerasimos Lampouras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 5 figures, 6 tables. Accepted at EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large pre-trained language models have recently been expanded and applied to
programming language tasks with great success, often through further
pre-training of a strictly-natural language model--where training sequences
typically contain both natural and (linearised) programming language. Such
approaches effectively map both modalities of the sequence into the same
embedding space. However, programming language keywords (e.g. ``while'') often
have very strictly defined semantics. As such, transfer learning from their
natural language usage may not necessarily be beneficial to their code
application and vise versa. Assuming an already pre-trained language model, in
this work we investigate how sequence tokens can be adapted and represented
differently, depending on which modality they belong to, and to the ultimate
benefit of the downstream task. We experiment with separating embedding spaces
between modalities during further model pre-training with modality-relative
training objectives. We focus on text-to-code generation and observe consistent
improvements across two backbone models and two test sets, measuring pass@$k$
and a novel incremental variation.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05785" title="Abstract">arXiv:2402.05785</a> [<a href="/pdf/2402.05785" title="Download PDF">pdf</a>, <a href="/format/2402.05785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Limits of Transformer Language Models on Algorithmic Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thomm%2C+J">Jonathan Thomm</a>, 
<a href="/search/cs?searchtype=author&query=Terzic%2C+A">Aleksandar Terzic</a>, 
<a href="/search/cs?searchtype=author&query=Karunaratne%2C+G">Geethan Karunaratne</a>, 
<a href="/search/cs?searchtype=author&query=Camposampiero%2C+G">Giacomo Camposampiero</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>, 
<a href="/search/cs?searchtype=author&query=Rahimi%2C+A">Abbas Rahimi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">We analyze the capabilities of Transformer language models on learning
discrete algorithms. To this end, we introduce two new tasks demanding the
composition of several discrete sub-tasks. On both training LLaMA models from
scratch and prompting on GPT-4 and Gemini we measure learning compositions of
learned primitives. We observe that the compositional capabilities of
state-of-the-art Transformer language models are very limited and sample-wise
scale worse than relearning all sub-tasks for a new algorithmic composition. We
also present a theorem in complexity theory, showing that gradient descent on
memorizing feedforward models can be exponentially data inefficient.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05786" title="Abstract">arXiv:2402.05786</a> [<a href="/pdf/2402.05786" title="Download PDF">pdf</a>, <a href="/ps/2402.05786" title="Download PostScript">ps</a>, <a href="/format/2402.05786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting Fairness: Artificial Intelligence as Game Players
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Henry%2C+J">Jazmia Henry</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Utilitarian games such as dictator games to measure fairness have been
studied in the social sciences for decades. These games have given us insight
into not only how humans view fairness but also in what conditions the
frequency of fairness, altruism and greed increase or decrease. While these
games have traditionally been focused on humans, the rise of AI gives us the
ability to study how these models play these games. AI is becoming a constant
in human interaction and examining how these models portray fairness in game
play can give us some insight into how AI makes decisions. Over 101 rounds of
the dictator game, I conclude that AI has a strong sense of fairness that is
dependant of it it deems the person it is playing with as trustworthy, framing
has a strong effect on how much AI gives a recipient when designated the
trustee, and there may be evidence that AI experiences inequality aversion just
as humans.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05790" title="Abstract">arXiv:2402.05790</a> [<a href="/pdf/2402.05790" title="Download PDF">pdf</a>, <a href="/format/2402.05790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Underwater MEMS Gyrocompassing: A Virtual Testing Ground
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Engelsman%2C+D">Daniel Engelsman</a>, 
<a href="/search/eess?searchtype=author&query=Klein%2C+I">Itzik Klein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 Pages, 7 figures, OCEANS 2024 Singapore
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In underwater navigation, accurate heading information is crucial for
accurately and continuously tracking trajectories, especially during extended
missions beneath the waves. In order to determine the initial heading, a
gyrocompassing procedure must be employed. As unmanned underwater vehicles
(UUV) are susceptible to ocean currents and other disturbances, the model-based
gyrocompassing procedure may experience degraded performance. To cope with such
situations, this paper introduces a dedicated learning framework aimed at
mitigating environmental effects and offering precise underwater
gyrocompassing. Through the analysis of the dynamic UUV signature obtained from
inertial measurements, our proposed framework learns to refine disturbed
signals, enabling a focused examination of the earth's rotation rate vector.
Leveraging recent machine learning advancements, empirical simulations assess
the framework's adaptability to challenging underwater conditions. Ultimately,
its contribution lies in providing a resilient gyrocompassing solution for
UUVs.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05791" title="Abstract">arXiv:2402.05791</a> [<a href="/pdf/2402.05791" title="Download PDF">pdf</a>, <a href="/ps/2402.05791" title="Download PostScript">ps</a>, <a href="/format/2402.05791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Determining the significance and relative importance of parameters of a  simulated quenching algorithm using statistical tools
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Castillo%2C+P+A">Pedro A. Castillo</a>, 
<a href="/search/cs?searchtype=author&query=Arenas%2C+M+G">Maribel Garc&#xed;a Arenas</a>, 
<a href="/search/cs?searchtype=author&query=Rico%2C+N">Nuria Rico</a>, 
<a href="/search/cs?searchtype=author&query=Mora%2C+A+M">Antonio Miguel Mora</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-S%C3%A1nchez%2C+P">Pablo Garc&#xed;a-S&#xe1;nchez</a>, 
<a href="/search/cs?searchtype=author&query=Laredo%2C+J+L+J">Juan Luis Jim&#xe9;nez Laredo</a>, 
<a href="/search/cs?searchtype=author&query=Guerv%C3%B3s%2C+J+J+M">Juan Juli&#xe1;n Merelo Guerv&#xf3;s</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">When search methods are being designed it is very important to know which
parameters have the greatest influence on the behaviour and performance of the
algorithm. To this end, algorithm parameters are commonly calibrated by means
of either theoretic analysis or intensive experimentation. When undertaking a
detailed statistical analysis of the influence of each parameter, the designer
should pay attention mostly to the parameters that are statistically
significant. In this paper the ANOVA (ANalysis Of the VAriance) method is used
to carry out an exhaustive analysis of a simulated annealing based method and
the different parameters it requires. Following this idea, the significance and
relative importance of the parameters regarding the obtained results, as well
as suitable values for each of these, were obtained using ANOVA and post-hoc
Tukey HSD test, on four well known function optimization problems and the
likelihood function that is used to estimate the parameters involved in the
lognormal diffusion process. Through this statistical study we have verified
the adequacy of parameter values available in the bibliography using parametric
hypothesis tests.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05794" title="Abstract">arXiv:2402.05794</a> [<a href="/pdf/2402.05794" title="Download PDF">pdf</a>, <a href="/format/2402.05794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Phonetically rich corpus construction for a low-resourced language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amadeus%2C+M">Marcellus Amadeus</a>, 
<a href="/search/cs?searchtype=author&query=Casta%C3%B1eda%2C+W+A+C">William Alberto Cruz Casta&#xf1;eda</a>, 
<a href="/search/cs?searchtype=author&query=Lobato%2C+W">Wilmer Lobato</a>, 
<a href="/search/cs?searchtype=author&query=Aquino%2C+N">Niasche Aquino</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Speech technologies rely on capturing a speaker's voice variability while
obtaining comprehensive language information. Textual prompts and sentence
selection methods have been proposed in the literature to comprise such
adequate phonetic data, referred to as a phonetically rich \textit{corpus}.
However, they are still insufficient for acoustic modeling, especially critical
for languages with limited resources. Hence, this paper proposes a novel
approach and outlines the methodological aspects required to create a
\textit{corpus} with broad phonetic coverage for a low-resourced language,
Brazilian Portuguese. Our methodology includes text dataset collection up to a
sentence selection algorithm based on triphone distribution. Furthermore, we
propose a new phonemic classification according to acoustic-articulatory speech
features since the absolute number of distinct triphones, or low-probability
triphones, does not guarantee an adequate representation of every possible
combination. Using our algorithm, we achieve a 55.8\% higher percentage of
distinct triphones -- for samples of similar size -- while the currently
available phonetic-rich corpus, CETUC and TTS-Portuguese, 12.6\% and 12.3\% in
comparison to a non-phonetically rich dataset.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05797" title="Abstract">arXiv:2402.05797</a> [<a href="/pdf/2402.05797" title="Download PDF">pdf</a>, <a href="/format/2402.05797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TaE: Task-aware Expandable Representation for Long Tail Class  Incremental Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Linjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">S. Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhenyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=yang%2C+J">JI yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Class-incremental learning (CIL) aims to train classifiers that learn new
classes without forgetting old ones. Most CIL methods focus on balanced data
distribution for each task, overlooking real-world long-tailed distributions.
Therefore, Long-Tailed Class-Incremental Learning (LT-CIL) has been introduced,
which trains on data where head classes have more samples than tail classes.
Existing methods mainly focus on preserving representative samples from
previous classes to combat catastrophic forgetting. Recently, dynamic network
algorithms frozen old network structures and expanded new ones, achieving
significant performance. However, with the introduction of the long-tail
problem, merely extending task-specific parameters can lead to miscalibrated
predictions, while expanding the entire model results in an explosion of memory
size. To address these issues, we introduce a novel Task-aware Expandable (TaE)
framework, dynamically allocating and updating task-specific trainable
parameters to learn diverse representations from each incremental task, while
resisting forgetting through the majority of frozen model parameters. To
further encourage the class-specific feature representation, we develop a
Centroid-Enhanced (CEd) method to guide the update of these task-aware
parameters. This approach is designed to adaptively minimize the distances
between intra-class features while simultaneously maximizing the distances
between inter-class features across all seen classes. The utility of this
centroid-enhanced method extends to all "training from scratch" CIL algorithms.
Extensive experiments were conducted on CIFAR-100 and ImageNet100 under
different settings, which demonstrates that TaE achieves state-of-the-art
performance.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05798" title="Abstract">arXiv:2402.05798</a> [<a href="/pdf/2402.05798" title="Download PDF">pdf</a>, <a href="/format/2402.05798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Harmony: Text-Visual Interplay in Circular Infographics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shuqi He</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuqing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yuxin Xia</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yichun Li</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+H">Hai-Ning Liang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lingyun Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Infographics are visual representations designed for efficient and effective
communication of data and knowledge. One crucial aspect of infographic design
is the interplay between text and visual elements, particularly in circular
visualizations where the textual descriptions can either be embedded within the
graphics or placed adjacent to the visual representation. While several studies
have examined text layout design in visualizations in general, the text-visual
interplay in infographics and its subsequent perceptual effects remain
underexplored. To address this, our study investigates how varying text
placement and descriptiveness impact pleasantness, comprehension and overall
memorability in the infographics viewing experience. We recruited 30
participants and presented them with a collection of 15 infographics across a
diverse set of topics, including media and public events, health and nutrition,
science and research, and sustainability. The text placement (embed,
side-to-side) and descriptiveness (simplistic, normal, descriptive) were
systematically manipulated, resulting in a total of six experimental
conditions. Our key findings indicate that text placement can significantly
influence the memorability of infographics, whereas descriptiveness can
significantly impact the pleasantness of the viewing experience. Embedding text
placement and simplistic text can potentially contribute to more effective
infographic designs. These results offer valuable insights for infographic
designers, contributing to the creation of more effective and memorable visual
representations.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05802" title="Abstract">arXiv:2402.05802</a> [<a href="/pdf/2402.05802" title="Download PDF">pdf</a>, <a href="/format/2402.05802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Discovery of Clinical Disease Signatures Using  Probabilistic Independence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lasko%2C+T+A">Thomas A. Lasko</a>, 
<a href="/search/cs?searchtype=author&query=Still%2C+J+M">John M. Still</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T+Z">Thomas Z. Li</a>, 
<a href="/search/cs?searchtype=author&query=Mota%2C+M+B">Marco Barbero Mota</a>, 
<a href="/search/cs?searchtype=author&query=Stead%2C+W+W">William W. Stead</a>, 
<a href="/search/cs?searchtype=author&query=Strobl%2C+E+V">Eric V. Strobl</a>, 
<a href="/search/cs?searchtype=author&query=Landman%2C+B+A">Bennett A. Landman</a>, 
<a href="/search/cs?searchtype=author&query=Maldonado%2C+F">Fabien Maldonado</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 Pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Applications (stat.AP); Machine Learning (stat.ML)

</div>
<p class="mathjax">Insufficiently precise diagnosis of clinical disease is likely responsible
for many treatment failures, even for common conditions and treatments. With a
large enough dataset, it may be possible to use unsupervised machine learning
to define clinical disease patterns more precisely. We present an approach to
learning these patterns by using probabilistic independence to disentangle the
imprint on the medical record of causal latent sources of disease. We inferred
a broad set of 2000 clinical signatures of latent sources from 9195 variables
in 269,099 Electronic Health Records. The learned signatures produced better
discrimination than the original variables in a lung cancer prediction task
unknown to the inference algorithm, predicting 3-year malignancy in patients
with no history of cancer before a solitary lung nodule was discovered. More
importantly, the signatures' greater explanatory power identified pre-nodule
signatures of apparently undiagnosed cancer in many of those patients.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05803" title="Abstract">arXiv:2402.05803</a> [<a href="/pdf/2402.05803" title="Download PDF">pdf</a>, <a href="/format/2402.05803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AvatarMMC: 3D Head Avatar Generation and Editing with Multi-Modal  Conditioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Para%2C+W+R">Wamiq Reyaz Para</a>, 
<a href="/search/cs?searchtype=author&query=Eldesokey%2C+A">Abdelrahman Eldesokey</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+P">Pradyumna Reddy</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jiankang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wonka%2C+P">Peter Wonka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">We introduce an approach for 3D head avatar generation and editing with
multi-modal conditioning based on a 3D Generative Adversarial Network (GAN) and
a Latent Diffusion Model (LDM). 3D GANs can generate high-quality head avatars
given a single or no condition. However, it is challenging to generate samples
that adhere to multiple conditions of different modalities. On the other hand,
LDMs excel at learning complex conditional distributions. To this end, we
propose to exploit the conditioning capabilities of LDMs to enable multi-modal
control over the latent space of a pre-trained 3D GAN. Our method can generate
and edit 3D head avatars given a mixture of control signals such as RGB input,
segmentation masks, and global attributes. This provides better control over
the generation and editing of synthetic avatars both globally and locally.
Experiments show that our proposed approach outperforms a solely GAN-based
approach both qualitatively and quantitatively on generation and editing tasks.
To the best of our knowledge, our approach is the first to introduce
multi-modal conditioning to 3D avatar generation and editing.
\\href{avatarmmc-sig24.github.io}{Project Page}
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05804" title="Abstract">arXiv:2402.05804</a> [<a href="/pdf/2402.05804" title="Download PDF">pdf</a>, <a href="/format/2402.05804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InkSight: Offline-to-Online Handwriting Conversion by Learning to Read  and Write
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mitrevski%2C+B">Blagoj Mitrevski</a>, 
<a href="/search/cs?searchtype=author&query=Rak%2C+A">Arina Rak</a>, 
<a href="/search/cs?searchtype=author&query=Schnitzler%2C+J">Julian Schnitzler</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengkun Li</a>, 
<a href="/search/cs?searchtype=author&query=Maksai%2C+A">Andrii Maksai</a>, 
<a href="/search/cs?searchtype=author&query=Berent%2C+J">Jesse Berent</a>, 
<a href="/search/cs?searchtype=author&query=Musat%2C+C">Claudiu Musat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Digital note-taking is gaining popularity, offering a durable, editable, and
easily indexable way of storing notes in the vectorized form, known as digital
ink. However, a substantial gap remains between this way of note-taking and
traditional pen-and-paper note-taking, a practice still favored by a vast
majority. Our work, InkSight, aims to bridge the gap by empowering physical
note-takers to effortlessly convert their work (offline handwriting) to digital
ink (online handwriting), a process we refer to as Derendering. Prior research
on the topic has focused on the geometric properties of images, resulting in
limited generalization beyond their training domains. Our approach combines
reading and writing priors, allowing training a model in the absence of large
amounts of paired samples, which are difficult to obtain. To our knowledge,
this is the first work that effectively derenders handwritten text in arbitrary
photos with diverse visual characteristics and backgrounds. Furthermore, it
generalizes beyond its training domain into simple sketches. Our human
evaluation reveals that 87% of the samples produced by our model on the
challenging HierText dataset are considered as a valid tracing of the input
image and 67% look like a pen trajectory traced by a human.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05806" title="Abstract">arXiv:2402.05806</a> [<a href="/pdf/2402.05806" title="Download PDF">pdf</a>, <a href="/format/2402.05806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Calibration and Conformal Prediction of Deep Classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dabah%2C+L">Lahav Dabah</a>, 
<a href="/search/cs?searchtype=author&query=Tirer%2C+T">Tom Tirer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In many classification applications, the prediction of a deep neural network
(DNN) based classifier needs to be accompanied with some confidence indication.
Two popular post-processing approaches for that aim are: 1) calibration:
modifying the classifier's softmax values such that their maximum (associated
with the prediction) better estimates the correctness probability; and 2)
conformal prediction (CP): devising a score (based on the softmax values) from
which a set of predictions with theoretically guaranteed marginal coverage of
the correct class is produced. While in practice both types of indications can
be desired, so far the interplay between them has not been investigated. Toward
filling this gap, in this paper we study the effect of temperature scaling,
arguably the most common calibration technique, on prominent CP methods. We
start with an extensive empirical study that among other insights shows that,
surprisingly, calibration has a detrimental effect on popular adaptive CP
methods: it frequently leads to larger prediction sets. Then, we turn to
theoretically analyze this behavior. We reveal several mathematical properties
of the procedure, according to which we provide a reasoning for the phenomenon.
Our study suggests that it may be worthwhile to utilize adaptive CP methods,
chosen for their enhanced conditional coverage, based on softmax values prior
to (or after canceling) temperature scaling calibration.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05808" title="Abstract">arXiv:2402.05808</a> [<a href="/pdf/2402.05808" title="Download PDF">pdf</a>, <a href="/format/2402.05808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Large Language Models for Reasoning through Reverse Curriculum  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xi%2C+Z">Zhiheng Xi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenxiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+B">Boyang Hong</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Senjie Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Rui Zheng</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+W">Wei He</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yiwen Ding</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shichun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junzhe Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Honglin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+W">Wei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+X">Xiaoran Fan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuhao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+S">Shihan Dou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinbo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+P">Peng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we propose R$^3$: Learning Reasoning through Reverse
Curriculum Reinforcement Learning (RL), a novel method that employs only
outcome supervision to achieve the benefits of process supervision for large
language models. The core challenge in applying RL to complex reasoning is to
identify a sequence of actions that result in positive rewards and provide
appropriate supervision for optimization. Outcome supervision provides sparse
rewards for final results without identifying error locations, whereas process
supervision offers step-wise rewards but requires extensive manual annotation.
R$^3$ overcomes these limitations by learning from correct demonstrations.
Specifically, R$^3$ progressively slides the start state of reasoning from a
demonstration's end to its beginning, facilitating easier model exploration at
all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome
supervision to offer step-level signals and precisely pinpoint errors. Using
Llama2-7B, our method surpasses RL baseline on eight reasoning tasks by $4.1$
points on average. Notebaly, in program-based reasoning on GSM8K, it exceeds
the baseline by $4.2$ points across three backbone models, and without any
extra data, Codellama-7B + R$^3$ performs comparable to larger models or
closed-source models.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05809" title="Abstract">arXiv:2402.05809</a> [<a href="/pdf/2402.05809" title="Download PDF">pdf</a>, <a href="/format/2402.05809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> You Only Need One Color Space: An Efficient Network for Low-light Image  Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yixu Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Peng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Q">Qingsen Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanning Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Low-Light Image Enhancement (LLIE) task tends to restore the details and
visual information from corrupted low-light images. Most existing methods learn
the mapping function between low/normal-light images by Deep Neural Networks
(DNNs) on sRGB and HSV color space. Nevertheless, enhancement involves
amplifying image signals, and applying these color spaces to low-light images
with a low signal-to-noise ratio can introduce sensitivity and instability into
the enhancement process. Consequently, this results in the presence of color
artifacts and brightness artifacts in the enhanced images. To alleviate this
problem, we propose a novel trainable color space, named
Horizontal/Vertical-Intensity (HVI). It not only decouples brightness and color
from RGB channels to mitigate the instability during enhancement but also
adapts to low-light images in different illumination ranges due to the
trainable parameters. Further, we design a novel Color and Intensity Decoupling
Network (CIDNet) with two branches dedicated to processing the decoupled image
brightness and color in the HVI space. Within CIDNet, we introduce the
Lightweight Cross-Attention (LCA) module to facilitate interaction between
image structure and content information in both branches, while also
suppressing noise in low-light images. Finally, we conducted 22 quantitative
and qualitative experiments to show that the proposed CIDNet outperforms the
state-of-the-art methods on 11 datasets. The code will be available at
https://github.com/Fediory/HVI-CIDNet.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05810" title="Abstract">arXiv:2402.05810</a> [<a href="/pdf/2402.05810" title="Download PDF">pdf</a>, <a href="/format/2402.05810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Natural Language User Profiles for Transparent and Scrutable  Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramos%2C+J">Jerome Ramos</a>, 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+H+A">Hossen A. Rahmani</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+X">Xiao Fu</a>, 
<a href="/search/cs?searchtype=author&query=Lipani%2C+A">Aldo Lipani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Current state-of-the-art recommender systems predominantly rely on either
implicit or explicit feedback from users to suggest new items. While effective
in recommending novel options, these conventional systems often use
uninterpretable embeddings. This lack of transparency not only limits user
understanding of why certain items are suggested but also reduces the user's
ability to easily scrutinize and edit their preferences. For example, if a user
has a change in interests, they would need to make significant changes to their
interaction history to adjust the model's recommendations. To address these
limitations, we introduce a novel method that utilizes user reviews to craft
personalized, natural language profiles describing users' preferences. Through
these descriptive profiles, our system provides transparent recommendations in
natural language. Our evaluations show that this novel approach maintains a
performance level on par with established recommender systems, but with the
added benefits of transparency and user control. By enabling users to
scrutinize why certain items are recommended, they can more easily verify,
adjust, and have greater autonomy over their recommendations.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05812" title="Abstract">arXiv:2402.05812</a> [<a href="/pdf/2402.05812" title="Download PDF">pdf</a>, <a href="/format/2402.05812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FAQ-Gen: An automated system to generate domain-specific FAQs to aid  content comprehension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kale%2C+S">Sahil Kale</a>, 
<a href="/search/cs?searchtype=author&query=Khaire%2C+G">Gautam Khaire</a>, 
<a href="/search/cs?searchtype=author&query=Patankar%2C+J">Jay Patankar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Frequently Asked Questions (FAQs) refer to the most common inquiries about
specific content. They serve as content comprehension aids by simplifying
topics and enhancing understanding through succinct presentation of
information. In this paper, we address FAQ generation as a well-defined Natural
Language Processing (NLP) task through the development of an end-to-end system
leveraging text-to-text transformation models. We present a literature review
covering traditional question-answering systems, highlighting their limitations
when applied directly to the FAQ generation task. We propose our system capable
of building FAQs from textual content tailored to specific domains, enhancing
their accuracy and relevance. We utilise self-curated algorithms for obtaining
optimal representation of information to be provided as input and also for
ranking the question-answer pairs to maximise human comprehension. Qualitative
human evaluation showcases the generated FAQs to be well-constructed and
readable, while also utilising domain-specific constructs to highlight
domain-based nuances and jargon in the original content.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05813" title="Abstract">arXiv:2402.05813</a> [<a href="/pdf/2402.05813" title="Download PDF">pdf</a>, <a href="/format/2402.05813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Selective Forgetting: Advancing Machine Unlearning Techniques and  Evaluation in Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lingzhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xingshan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jinsong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kam-Fai Wong</a>, 
<a href="/search/cs?searchtype=author&query=Gottlob%2C+G">Georg Gottlob</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The aim of this study is to investigate Machine Unlearning (MU), a burgeoning
field focused on addressing concerns related to neural models inadvertently
retaining personal or sensitive data. Here, a novel approach is introduced to
achieve precise and selective forgetting within language models. Unlike
previous methodologies that adopt completely opposing training objectives, this
approach aims to mitigate adverse effects on language model performance,
particularly in generation tasks. Furthermore, two innovative evaluation
metrics are proposed: Sensitive Information Extraction Likelihood (S-EL) and
Sensitive Information Memory Accuracy (S-MA), designed to gauge the
effectiveness of sensitive information elimination. To reinforce the forgetting
framework, an effective method for annotating sensitive scopes is presented,
involving both online and offline strategies. The online selection mechanism
leverages language probability scores to ensure computational efficiency, while
the offline annotation entails a robust two-stage process based on Large
Language Models (LLMs).
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05821" title="Abstract">arXiv:2402.05821</a> [<a href="/pdf/2402.05821" title="Download PDF">pdf</a>, <a href="/format/2402.05821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guided Evolution with Binary Discriminators for ML Program Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Co-Reyes%2C+J+D">John D. Co-Reyes</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+Y">Yingjie Miao</a>, 
<a href="/search/cs?searchtype=author&query=Tucker%2C+G">George Tucker</a>, 
<a href="/search/cs?searchtype=author&query=Faust%2C+A">Aleksandra Faust</a>, 
<a href="/search/cs?searchtype=author&query=Real%2C+E">Esteban Real</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">How to automatically design better machine learning programs is an open
problem within AutoML. While evolution has been a popular tool to search for
better ML programs, using learning itself to guide the search has been less
successful and less understood on harder problems but has the promise to
dramatically increase the speed and final performance of the optimization
process. We propose guiding evolution with a binary discriminator, trained
online to distinguish which program is better given a pair of programs. The
discriminator selects better programs without having to perform a costly
evaluation and thus speed up the convergence of evolution. Our method can
encode a wide variety of ML components including symbolic optimizers, neural
architectures, RL loss functions, and symbolic regression equations with the
same directed acyclic graph representation. By combining this representation
with modern GNNs and an adaptive mutation strategy, we demonstrate our method
can speed up evolution across a set of diverse problems including a 3.7x
speedup on the symbolic search for ML optimizers and a 4x speedup for RL loss
functions.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05823" title="Abstract">arXiv:2402.05823</a> [<a href="/pdf/2402.05823" title="Download PDF">pdf</a>, <a href="/format/2402.05823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FusionSF: Fuse Heterogeneous Modalities in a Vector Quantized Framework  for Robust Solar Power Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Ziqing Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tian Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+B">Bingqing Peng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Liang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+R">Rong Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Accurate solar power forecasting is crucial to integrate photovoltaic plants
into the electric grid, schedule and secure the power grid safety. This problem
becomes more demanding for those newly installed solar plants which lack
sufficient data. Current research predominantly relies on historical solar
power data or numerical weather prediction in a single-modality format,
ignoring the complementary information provided in different modalities. In
this paper, we propose a multi-modality fusion framework to integrate
historical power data, numerical weather prediction, and satellite images,
significantly improving forecast performance. We introduce a vector quantized
framework that aligns modalities with varying information densities, striking a
balance between integrating sufficient information and averting model
overfitting. Our framework demonstrates strong zero-shot forecasting
capability, which is especially useful for those newly installed plants.
Moreover, we collect and release a multi-modal solar power (MMSP) dataset from
real-world plants to further promote the research of multi-modal solar
forecasting algorithms. Our extensive experiments show that our model not only
operates with robustness but also boosts accuracy in both zero-shot forecasting
and scenarios rich with training data, surpassing leading models. We have
incorporated it into our eForecaster platform and deployed it for more than 300
solar plants with a capacity of over 15GW.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05827" title="Abstract">arXiv:2402.05827</a> [<a href="/pdf/2402.05827" title="Download PDF">pdf</a>, <a href="/format/2402.05827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is it Possible to Edit Large Language Models Robustly?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xinbei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+T">Tianjie Ju</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+J">Jiyang Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuosheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lifeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yulong Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have played a pivotal role in building
communicative AI to imitate human behaviors but face the challenge of efficient
customization. To tackle this challenge, recent studies have delved into the
realm of model editing, which manipulates specific memories of language models
and changes the related language generation. However, the robustness of model
editing remains an open question. This work seeks to understand the strengths
and limitations of editing methods, thus facilitating robust, realistic
applications of communicative AI. Concretely, we conduct extensive analysis to
address the three key research questions. Q1: Can edited LLMs behave
consistently resembling communicative AI in realistic situations? Q2: To what
extent does the rephrasing of prompts lead LLMs to deviate from the edited
knowledge memory? Q3: Which knowledge features are correlated with the
performance and robustness of editing? Our experimental results uncover a
substantial disparity between existing editing methods and the practical
application of LLMs. On rephrased prompts that are complex and flexible but
common in realistic applications, the performance of editing experiences a
significant decline. Further analysis shows that more popular knowledge is
memorized better, easier to recall, and more challenging to edit effectively.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05828" title="Abstract">arXiv:2402.05828</a> [<a href="/pdf/2402.05828" title="Download PDF">pdf</a>, <a href="/format/2402.05828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering Temporally-Aware Reinforcement Learning Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jackson%2C+M+T">Matthew Thomas Jackson</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chris Lu</a>, 
<a href="/search/cs?searchtype=author&query=Kirsch%2C+L">Louis Kirsch</a>, 
<a href="/search/cs?searchtype=author&query=Lange%2C+R+T">Robert Tjarko Lange</a>, 
<a href="/search/cs?searchtype=author&query=Whiteson%2C+S">Shimon Whiteson</a>, 
<a href="/search/cs?searchtype=author&query=Foerster%2C+J+N">Jakob Nicolaus Foerster</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advancements in meta-learning have enabled the automatic discovery of
novel reinforcement learning algorithms parameterized by surrogate objective
functions. To improve upon manually designed algorithms, the parameterization
of this learned objective function must be expressive enough to represent novel
principles of learning (instead of merely recovering already established ones)
while still generalizing to a wide range of settings outside of its
meta-training distribution. However, existing methods focus on discovering
objective functions that, like many widely used objective functions in
reinforcement learning, do not take into account the total number of steps
allowed for training, or "training horizon". In contrast, humans use a plethora
of different learning objectives across the course of acquiring a new ability.
For instance, students may alter their studying techniques based on the
proximity to exam deadlines and their self-assessed capabilities. This paper
contends that ignoring the optimization time horizon significantly restricts
the expressive potential of discovered learning algorithms. We propose a simple
augmentation to two existing objective discovery approaches that allows the
discovered algorithm to dynamically update its objective function throughout
the agent's training procedure, resulting in expressive schedules and increased
generalization across different training horizons. In the process, we find that
commonly used meta-gradient approaches fail to discover such adaptive objective
functions while evolution strategies discover highly dynamic learning rules. We
demonstrate the effectiveness of our approach on a wide range of tasks and
analyze the resulting learned algorithms, which we find effectively balance
exploration and exploitation by modifying the structure of their learning rules
throughout the agent's lifetime.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05829" title="Abstract">arXiv:2402.05829</a> [<a href="/pdf/2402.05829" title="Download PDF">pdf</a>, <a href="/format/2402.05829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Limitations of Agents Simulated by Predictive Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Douglas%2C+R">Raymond Douglas</a>, 
<a href="/search/cs?searchtype=author&query=Karwowski%2C+J">Jacek Karwowski</a>, 
<a href="/search/cs?searchtype=author&query=Bae%2C+C">Chan Bae</a>, 
<a href="/search/cs?searchtype=author&query=Draguns%2C+A">Andis Draguns</a>, 
<a href="/search/cs?searchtype=author&query=Krakovna%2C+V">Victoria Krakovna</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">There is increasing focus on adapting predictive models into agent-like
systems, most notably AI assistants based on language models. We outline two
structural reasons for why these models can fail when turned into agents.
First, we discuss auto-suggestive delusions. Prior work has shown theoretically
that models fail to imitate agents that generated the training data if the
agents relied on hidden observations: the hidden observations act as
confounding variables, and the models treat actions they generate as evidence
for nonexistent observations. Second, we introduce and formally study a
related, novel limitation: predictor-policy incoherence. When a model generates
a sequence of actions, the model's implicit prediction of the policy that
generated those actions can serve as a confounding variable. The result is that
models choose actions as if they expect future actions to be suboptimal,
causing them to be overly conservative. We show that both of those failures are
fixed by including a feedback loop from the environment, that is, re-training
the models on their own actions. We give simple demonstrations of both
limitations using Decision Transformers and confirm that empirical results
agree with our conceptual and formal analysis. Our treatment provides a
unifying view of those failure modes, and informs the question of why
fine-tuning offline learned policies with online learning makes them more
effective.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05830" title="Abstract">arXiv:2402.05830</a> [<a href="/pdf/2402.05830" title="Download PDF">pdf</a>, <a href="/format/2402.05830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse-VQ Transformer: An FFN-Free Framework with Vector Quantization  for Enhanced Time Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yanjun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tian Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Liang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yi Qian</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+R">Rong Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Time series analysis is vital for numerous applications, and transformers
have become increasingly prominent in this domain. Leading methods customize
the transformer architecture from NLP and CV, utilizing a patching technique to
convert continuous signals into segments. Yet, time series data are uniquely
challenging due to significant distribution shifts and intrinsic noise levels.
To address these two challenges,we introduce the Sparse Vector Quantized
FFN-Free Transformer (Sparse-VQ). Our methodology capitalizes on a sparse
vector quantization technique coupled with Reverse Instance Normalization
(RevIN) to reduce noise impact and capture sufficient statistics for
forecasting, serving as an alternative to the Feed-Forward layer (FFN) in the
transformer architecture. Our FFN-free approach trims the parameter count,
enhancing computational efficiency and reducing overfitting. Through
evaluations across ten benchmark datasets, including the newly introduced CAISO
dataset, Sparse-VQ surpasses leading models with a 7.84% and 4.17% decrease in
MAE for univariate and multivariate time series forecasting, respectively.
Moreover, it can be seamlessly integrated with existing transformer-based
models to elevate their performance.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05835" title="Abstract">arXiv:2402.05835</a> [<a href="/pdf/2402.05835" title="Download PDF">pdf</a>, <a href="/format/2402.05835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Much is Unseen Depends Chiefly on Information About the Seen
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seongmin Lee</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%B6hme%2C+M">Marcel B&#xf6;hme</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages with 5 pages of appendix, 5 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)

</div>
<p class="mathjax">It might seem counter-intuitive at first: We find that, in expectation, the
proportion of data points in an unknown population-that belong to classes that
do not appear in the training data-is almost entirely determined by the number
$f_k$ of classes that do appear in the training data the same number of times.
While in theory we show that the difference of the induced estimator decays
exponentially in the size of the sample, in practice the high variance prevents
us from using it directly for an estimator of the sample coverage. However, our
precise characterization of the dependency between $f_k$'s induces a large
search space of different representations of the expected value, which can be
deterministically instantiated as estimators. Hence, we turn to optimization
and develop a genetic algorithm that, given only the sample, searches for an
estimator with minimal mean-squared error (MSE). In our experiments, our
genetic algorithm discovers estimators that have a substantially smaller MSE
than the state-of-the-art Good-Turing estimator. This holds for over 96% of
runs when there are at least as many samples as classes. Our estimators' MSE is
roughly 80% of the Good-Turing estimator's.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05837" title="Abstract">arXiv:2402.05837</a> [<a href="/pdf/2402.05837" title="Download PDF">pdf</a>, <a href="/format/2402.05837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shape Optimization of Eigenfrequencies in MEMS Gyroscopes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schiwietz%2C+D">Daniel Schiwietz</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%B6rsting%2C+M">Marian H&#xf6;rsting</a>, 
<a href="/search/cs?searchtype=author&query=Weig%2C+E+M">Eva Maria Weig</a>, 
<a href="/search/cs?searchtype=author&query=Degenfeld-Schonburg%2C+P">Peter Degenfeld-Schonburg</a>, 
<a href="/search/cs?searchtype=author&query=Wenzel%2C+M">Matthias Wenzel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Microelectromechanical systems (MEMS) gyroscopes are widely used in consumer
and automotive applications. They have to fulfill a vast number of product
requirements which lead to complex mechanical designs of the resonating
structure. Arriving at a final design is a cumbersome process that relies
heavily on human experience in conjunction with design optimization methods. In
this work, we apply node-based shape optimization to the design of a MEMS
gyroscope. For that purpose, we parametrize the coordinates of the nodes of the
finite element method (FEM) mesh that discretize the shapes of the springs. We
then implement the gradients of the mechanical eigenfrequencies and typical
MEMS manufacturability constraints, with respect to the design parameters, in a
FEM code. Using gradient-based optimization we tune the gyroscope's frequency
split and shift spurious modes away from the first three multiples of the
gyroscope's drive frequency while manufacturability constraints are fulfilled.
The resulting optimized design exhibits novel geometrical shapes which defy any
human intuition. Overall, we demonstrate that shape optimization can not only
solve optimization problems in MEMS design without required human intervention,
but also explores geometry solutions which can otherwise not be addressed. In
this way, node-based shape optimization opens up a much larger space of
possible design solutions, which is crucial for facing the ever increasing
product requirements. Our approach is generic and applicable to many other
types of MEMS resonators.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05840" title="Abstract">arXiv:2402.05840</a> [<a href="/pdf/2402.05840" title="Download PDF">pdf</a>, <a href="/format/2402.05840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception  Uncertainties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sirohi%2C+K">Kshitij Sirohi</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%BCscher%2C+D">Daniel B&#xfc;scher</a>, 
<a href="/search/cs?searchtype=author&query=Burgard%2C+W">Wolfram Burgard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The availability of a reliable map and a robust localization system is
critical for the operation of an autonomous vehicle. In a modern system, both
mapping and localization solutions generally employ convolutional neural
network (CNN) --based perception. Hence, any algorithm should consider
potential errors in perception for safe and robust functioning. In this work,
we present uncertainty-aware panoptic Localization and Mapping (uPLAM), which
employs perception uncertainty as a bridge to fuse the perception information
with classical localization and mapping approaches. We introduce an
uncertainty-based map aggregation technique to create a long-term panoptic
bird's eye view map and provide an associated mapping uncertainty. Our map
consists of surface semantics and landmarks with unique IDs. Moreover, we
present panoptic uncertainty-aware particle filter-based localization. To this
end, we propose an uncertainty-based particle importance weight calculation for
the adaptive incorporation of perception information into localization. We also
present a new dataset for evaluating long-term panoptic mapping and map-based
localization. Extensive evaluations showcase that our proposed uncertainty
incorporation leads to better mapping with reliable uncertainty estimates and
accurate localization. We make our dataset and code available at:
\url{<a href="http://uplam.cs.uni-freiburg.de">this http URL</a>}
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05853" title="Abstract">arXiv:2402.05853</a> [<a href="/pdf/2402.05853" title="Download PDF">pdf</a>, <a href="/format/2402.05853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Experimental Emulation of Printability and Fleet Aware Generic Mesh  Decomposition for Enabling Aerial 3D Printing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stamatopoulos%2C+M">Marios-Nektarios Stamatopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+A">Avijit Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Nikolakopoulos%2C+G">George Nikolakopoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for publication at IEEE International Conference on Robotics and Automation (ICRA) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This article introduces an experimental emulation of a novel chunk-based
flexible multi-DoF aerial 3D printing framework. The experimental demonstration
of the overall autonomy focuses on precise motion planning and task allocation
for a UAV, traversing through a series of planned space-filling paths involved
in the aerial 3D printing process without physically depositing the overlaying
material. The flexible multi-DoF aerial 3D printing is a newly developed
framework and has the potential to strategically distribute the envisioned 3D
model to be printed into small, manageable chunks suitable for distributed 3D
printing. Moreover, by harnessing the dexterous flexibility due to the 6 DoF
motion of UAV, the framework enables the provision of integrating the overall
autonomy stack, potentially opening up an entirely new frontier in additive
manufacturing. However, it's essential to note that the feasibility of this
pioneering concept is still in its very early stage of development, which yet
needs to be experimentally verified. Towards this direction, experimental
emulation serves as the crucial stepping stone, providing a pseudo mockup
scenario by virtual material deposition, helping to identify technological gaps
from simulation to reality. Experimental emulation results, supported by
critical analysis and discussion, lay the foundation for addressing the
technological and research challenges to significantly push the boundaries of
the state-of-the-art 3D printing mechanism.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05854" title="Abstract">arXiv:2402.05854</a> [<a href="/pdf/2402.05854" title="Download PDF">pdf</a>, <a href="/format/2402.05854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> (Almost) Affine Higher-Order Tree Transducers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguy%C3%AAn%2C+L+T+D+T">L&#xea; Th&#xe0;nh D&#x169;ng Tito Nguy&#xea;n</a>, 
<a href="/search/cs?searchtype=author&query=Vanoni%2C+G">Gabriele Vanoni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">We investigate the tree-to-tree functions computed by
\enquote{affine$\lambda$-transducers}: tree automata whose memory consists of
an affine $\lambda$-term instead of a finite state. They can be seen as
variations on Gallot, Lemay and Salvati's Linear High-Order Deterministic Tree
Transducers. When the memory is almost purely affine (\textit{\`a la}
Kanazawa), we show that these machines can be translated to tree-walking
transducers (and with a purely affine memory, we get a reversible tree-walking
transducer). This leads to a proof of an inexpressivity conjecture of
\titocecilia on \enquote{implicit automata} in an affine $\lambda$-calculus.
The key technical tool in our proofs is the Interaction Abstract Machine (IAM),
an operational avatar of the \enquote{geometry of interaction} semantics of
linear logic. We work with ad-hoc specializations to (almost) affine
$\lambda$-terms of a tree-generating version of the IAM.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05859" title="Abstract">arXiv:2402.05859</a> [<a href="/pdf/2402.05859" title="Download PDF">pdf</a>, <a href="/format/2402.05859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Route Among Specialized Experts for Zero-Shot Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Muqeeth%2C+M">Mohammed Muqeeth</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haokun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yufan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Raffel%2C+C">Colin Raffel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recently, there has been a widespread proliferation of "expert" language
models that are specialized to a specific task or domain through
parameter-efficient fine-tuning. How can we recycle large collections of expert
language models to improve zero-shot generalization to unseen tasks? In this
work, we propose Post-Hoc Adaptive Tokenwise Gating Over an Ocean of
Specialized Experts (PHATGOOSE), which learns to route among specialized
modules that were produced through parameter-efficient fine-tuning. Unlike past
methods that learn to route among specialized models, PHATGOOSE explores the
possibility that zero-shot generalization will be improved if different experts
can be adaptively chosen for each token and at each layer in the model.
Crucially, our method is post-hoc - it does not require simultaneous access to
the datasets used to create the specialized models and only requires a modest
amount of additional compute after each expert model is trained. In experiments
covering a range of specialized model collections and zero-shot generalization
benchmarks, we find that PHATGOOSE outperforms past methods for post-hoc
routing and, in some cases, outperforms explicit multitask training (which
requires simultaneous data access). To better understand the routing strategy
learned by PHATGOOSE, we perform qualitative experiments to validate that
PHATGOOSE's performance stems from its ability to make adaptive per-token and
per-module expert choices. We release all of our code to support future work on
improving zero-shot generalization by recycling specialized experts.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05860" title="Abstract">arXiv:2402.05860</a> [<a href="/pdf/2402.05860" title="Download PDF">pdf</a>, <a href="/format/2402.05860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-Preserving Synthetic Continual Semantic Segmentation for Robotic  Surgery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mengya Xu</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M">Mobarakol Islam</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+L">Long Bai</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Hongliang Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 8 figures, IEEE Transactions on Medical Image (accepted)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Deep Neural Networks (DNNs) based semantic segmentation of the robotic
instruments and tissues can enhance the precision of surgical activities in
robot-assisted surgery. However, in biological learning, DNNs cannot learn
incremental tasks over time and exhibit catastrophic forgetting, which refers
to the sharp decline in performance on previously learned tasks after learning
a new one. Specifically, when data scarcity is the issue, the model shows a
rapid drop in performance on previously learned instruments after learning new
data with new instruments. The problem becomes worse when it limits releasing
the dataset of the old instruments for the old model due to privacy concerns
and the unavailability of the data for the new or updated version of the
instruments for the continual learning model. For this purpose, we develop a
privacy-preserving synthetic continual semantic segmentation framework by
blending and harmonizing (i) open-source old instruments foreground to the
synthesized background without revealing real patient data in public and (ii)
new instruments foreground to extensively augmented real background. To boost
the balanced logit distillation from the old model to the continual learning
model, we design overlapping class-aware temperature normalization (CAT) by
controlling model learning utility. We also introduce multi-scale
shifted-feature distillation (SD) to maintain long and short-range spatial
relationships among the semantic objects where conventional short-range spatial
features with limited information reduce the power of feature distillation. We
demonstrate the effectiveness of our framework on the EndoVis 2017 and 2018
instrument segmentation dataset with a generalized continual learning setting.
Code is available at~\url{https://github.com/XuMengyaAmy/Synthetic_CAT_SD}.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05861" title="Abstract">arXiv:2402.05861</a> [<a href="/pdf/2402.05861" title="Download PDF">pdf</a>, <a href="/format/2402.05861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory Consolidation Enables Long-Context Video Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bala%C5%BEevi%C4%87%2C+I">Ivana Bala&#x17e;evi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuge Shi</a>, 
<a href="/search/cs?searchtype=author&query=Papalampidi%2C+P">Pinelopi Papalampidi</a>, 
<a href="/search/cs?searchtype=author&query=Chaabouni%2C+R">Rahma Chaabouni</a>, 
<a href="/search/cs?searchtype=author&query=Koppula%2C+S">Skanda Koppula</a>, 
<a href="/search/cs?searchtype=author&query=H%C3%A9naff%2C+O+J">Olivier J. H&#xe9;naff</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Most transformer-based video encoders are limited to short temporal contexts
due to their quadratic complexity. While various attempts have been made to
extend this context, this has often come at the cost of both conceptual and
computational complexity. We propose to instead re-purpose existing pre-trained
video transformers by simply fine-tuning them to attend to memories derived
non-parametrically from past activations. By leveraging redundancy reduction,
our memory-consolidated vision transformer (MC-ViT) effortlessly extends its
context far into the past and exhibits excellent scaling behavior when learning
from longer videos. In doing so, MC-ViT sets a new state-of-the-art in
long-context video understanding on EgoSchema, Perception Test, and Diving48,
outperforming methods that benefit from orders of magnitude more parameters.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05862" title="Abstract">arXiv:2402.05862</a> [<a href="/pdf/2402.05862" title="Download PDF">pdf</a>, <a href="/format/2402.05862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Let Your Graph Do the Talking: Encoding Structured Data for LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Perozzi%2C+B">Bryan Perozzi</a>, 
<a href="/search/cs?searchtype=author&query=Fatemi%2C+B">Bahare Fatemi</a>, 
<a href="/search/cs?searchtype=author&query=Zelle%2C+D">Dustin Zelle</a>, 
<a href="/search/cs?searchtype=author&query=Tsitsulin%2C+A">Anton Tsitsulin</a>, 
<a href="/search/cs?searchtype=author&query=Kazemi%2C+M">Mehran Kazemi</a>, 
<a href="/search/cs?searchtype=author&query=Al-Rfou%2C+R">Rami Al-Rfou</a>, 
<a href="/search/cs?searchtype=author&query=Halcrow%2C+J">Jonathan Halcrow</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI); Machine Learning (stat.ML)

</div>
<p class="mathjax">How can we best encode structured data into sequential form for use in large
language models (LLMs)? In this work, we introduce a parameter-efficient method
to explicitly represent structured data for LLMs. Our method, GraphToken,
learns an encoding function to extend prompts with explicit structured
information. Unlike other work which focuses on limited domains (e.g. knowledge
graph representation), our work is the first effort focused on the general
encoding of structured data to be used for various reasoning tasks. We show
that explicitly representing the graph structure allows significant
improvements to graph reasoning tasks. Specifically, we see across the board
improvements - up to 73% points - on node, edge and, graph-level tasks from the
GraphQA benchmark.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05863" title="Abstract">arXiv:2402.05863</a> [<a href="/pdf/2402.05863" title="Download PDF">pdf</a>, <a href="/format/2402.05863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bianchi%2C+F">Federico Bianchi</a>, 
<a href="/search/cs?searchtype=author&query=Chia%2C+P+J">Patrick John Chia</a>, 
<a href="/search/cs?searchtype=author&query=Yuksekgonul%2C+M">Mert Yuksekgonul</a>, 
<a href="/search/cs?searchtype=author&query=Tagliabue%2C+J">Jacopo Tagliabue</a>, 
<a href="/search/cs?searchtype=author&query=Jurafsky%2C+D">Dan Jurafsky</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">James Zou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Negotiation is the basis of social interactions; humans negotiate everything
from the price of cars to how to share common resources. With rapidly growing
interest in using large language models (LLMs) to act as agents on behalf of
human users, such LLM agents would also need to be able to negotiate. In this
paper, we study how well LLMs can negotiate with each other. We develop
NegotiationArena: a flexible framework for evaluating and probing the
negotiation abilities of LLM agents. We implemented three types of scenarios in
NegotiationArena to assess LLM's behaviors in allocating shared resources
(ultimatum games), aggregate resources (trading games) and buy/sell goods
(price negotiations). Each scenario allows for multiple turns of flexible
dialogues between LLM agents to allow for more complex negotiations.
Interestingly, LLM agents can significantly boost their negotiation outcomes by
employing certain behavioral tactics. For example, by pretending to be desolate
and desperate, LLMs can improve their payoffs by 20\% when negotiating against
the standard GPT-4. We also quantify irrational negotiation behaviors exhibited
by the LLM agents, many of which also appear in humans. Together,
\NegotiationArena offers a new environment to investigate LLM interactions,
enabling new insights into LLM's theory of mind, irrationality, and reasoning
abilities.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05864" title="Abstract">arXiv:2402.05864</a> [<a href="/pdf/2402.05864" title="Download PDF">pdf</a>, <a href="/format/2402.05864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xuandong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu-Xiang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we propose a new decoding method called Permute-and-Flip (PF)
decoder. It enjoys robustness properties similar to the standard sampling
decoder, but is provably up to 2x better in its quality-robustness tradeoff
than sampling and never worse than any other decoder. We also design a
cryptographic watermarking scheme analogous to Aaronson's Gumbel watermark, but
naturally tailored for PF decoder. The watermarking scheme does not change the
distribution to sample, while allowing arbitrarily low false positive rate and
high recall whenever the generated text has high entropy. Our experiments show
that the PF decoder (and its watermarked counterpart) significantly
outperform(s) naive sampling (and it's Gumbel watermarked counterpart) in terms
of perplexity, while retaining the same robustness (and detectability), hence
making it a promising new approach for LLM decoding. The code is available at
https://github.com/XuandongZhao/pf-decoding
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05865" title="Abstract">arXiv:2402.05865</a> [<a href="/pdf/2402.05865" title="Download PDF">pdf</a>, <a href="/format/2402.05865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;Can You Play Anything Else?&quot; Understanding Play Style Flexibility in  League of Legends
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Emily Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bisberg%2C+A">Alexander Bisberg</a>, 
<a href="/search/cs?searchtype=author&query=Ferrara%2C+E">Emilio Ferrara</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">This study investigates the concept of flexibility within League of Legends,
a popular online multiplayer game, focusing on the relationship between user
adaptability and team success. Utilizing a dataset encompassing players of
varying skill levels and play styles, we calculate two measures of flexibility
for each player: overall flexibility and temporal flexibility. Our findings
suggest that the flexibility of a user is dependent upon a user's preferred
play style, and flexibility does impact match outcome. This work also shows
that skill level not only indicates how willing a player is to adapt their play
style but also how their adaptability changes over time. This paper highlights
the the duality and balance of mastery versus flexibility, providing insights
that can inform strategic planning, collaboration and resource allocation in
competitive environments.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05867" title="Abstract">arXiv:2402.05867</a> [<a href="/pdf/2402.05867" title="Download PDF">pdf</a>, <a href="/ps/2402.05867" title="Download PostScript">ps</a>, <a href="/format/2402.05867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring pseudorandom value addition operations in datasets: A layered  approach to escape from normal-Gaussian patterns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=de+Moraes+Silva%2C+E+C">Ergon Cugler de Moraes Silva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computation (stat.CO)

</div>
<p class="mathjax">In the realm of statistical exploration, the manipulation of pseudo-random
values to discern their impact on data distribution presents a compelling
avenue of inquiry. This article investigates the question: Is it possible to
add pseudo-random values without compelling a shift towards a normal
distribution?. Employing Python techniques, the study explores the nuances of
pseudo-random value addition within the context of additions, aiming to unravel
the interplay between randomness and resulting statistical characteristics. The
Materials and Methods chapter details the construction of datasets comprising
up to 300 billion pseudo-random values, employing three distinct layers of
manipulation. The Results chapter visually and quantitatively explores the
generated datasets, emphasizing distribution and standard deviation metrics.
The study concludes with reflections on the implications of pseudo-random value
manipulation and suggests avenues for future research. In the layered
exploration, the first layer introduces subtle normalization with increasing
summations, while the second layer enhances normality. The third layer disrupts
typical distribution patterns, leaning towards randomness despite pseudo-random
value summation. Standard deviation patterns across layers further illuminate
the dynamic interplay of pseudo-random operations on statistical
characteristics. While not aiming to disrupt academic norms, this work modestly
contributes insights into data distribution complexities. Future studies are
encouraged to delve deeper into the implications of data manipulation on
statistical outcomes, extending the understanding of pseudo-random operations
in diverse contexts.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05868" title="Abstract">arXiv:2402.05868</a> [<a href="/pdf/2402.05868" title="Download PDF">pdf</a>, <a href="/format/2402.05868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PromptCrypt: Prompt Encryption for Secure Communication with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+G">Guo Lin</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+W">Wenyue Hua</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongfeng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 4 figures, 2 tables, comments and suggestions are welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Cloud-based large language models (LLMs) such as ChatGPT have increasingly
become integral to daily operations, serving as vital tools across various
applications. While these models offer substantial benefits in terms of
accessibility and functionality, they also introduce significant privacy
concerns: the transmission and storage of user data in cloud infrastructures
pose substantial risks of data breaches and unauthorized access to sensitive
information; even if the transmission and storage of data is encrypted, the LLM
service provider itself still knows the real contents of the data, preventing
individuals or entities from confidently using such LLM services. To address
these concerns, this paper proposes a simple yet effective mechanism
PromptCrypt to protect user privacy. It uses Emoji to encrypt the user inputs
before sending them to LLM, effectively rendering them indecipherable to human
or LLM's examination while retaining the original intent of the prompt, thus
ensuring the model's performance remains unaffected. We conduct experiments on
three tasks, personalized recommendation, sentiment analysis, and tabular data
analysis. Experiment results reveal that PromptCrypt can encrypt personal
information within prompts in such a manner that not only prevents the
discernment of sensitive data by humans or LLM itself, but also maintains or
even improves the precision without further tuning, achieving comparable or
even better task accuracy than directly prompting the LLM without prompt
encryption. These results highlight the practicality of adopting encryption
measures that safeguard user privacy without compromising the functional
integrity and performance of LLMs. Code and dataset are available at
https://github.com/agiresearch/PromptCrypt.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05869" title="Abstract">arXiv:2402.05869</a> [<a href="/pdf/2402.05869" title="Download PDF">pdf</a>, <a href="/format/2402.05869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Surface Normal Constraint for Geometric Estimation from  Monocular Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+X">Xiaoxiao Long</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yuhang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yupeng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+B">Beiwen Tian</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Cheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lingjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Guyue Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenping Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2103.15483">arXiv:2103.15483</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We introduce a novel approach to learn geometries such as depth and surface
normal from images while incorporating geometric context. The difficulty of
reliably capturing geometric context in existing methods impedes their ability
to accurately enforce the consistency between the different geometric
properties, thereby leading to a bottleneck of geometric estimation quality. We
therefore propose the Adaptive Surface Normal (ASN) constraint, a simple yet
efficient method. Our approach extracts geometric context that encodes the
geometric variations present in the input image and correlates depth estimation
with geometric constraints. By dynamically determining reliable local geometry
from randomly sampled candidates, we establish a surface normal constraint,
where the validity of these candidates is evaluated using the geometric
context. Furthermore, our normal estimation leverages the geometric context to
prioritize regions that exhibit significant geometric variations, which makes
the predicted normals accurately capture intricate and detailed geometric
information. Through the integration of geometric context, our method unifies
depth and surface normal estimations within a cohesive framework, which enables
the generation of high-quality 3D geometry from images. We validate the
superiority of our approach over state-of-the-art methods through extensive
evaluations and comparisons on diverse indoor and outdoor datasets, showcasing
its efficiency and robustness.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05872" title="Abstract">arXiv:2402.05872</a> [<a href="/pdf/2402.05872" title="Download PDF">pdf</a>, <a href="/format/2402.05872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> You&#x27;ve Got to Feel It To Believe It: Multi-Modal Bayesian Inference for  Semantic and Property Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ewen%2C+P">Parker Ewen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuzhen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Anran Li</a>, 
<a href="/search/cs?searchtype=author&query=Bagali%2C+A">Anup Bagali</a>, 
<a href="/search/cs?searchtype=author&query=Gunjal%2C+G">Gitesh Gunjal</a>, 
<a href="/search/cs?searchtype=author&query=Vasudevan%2C+R">Ram Vasudevan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Robots must be able to understand their surroundings to perform complex tasks
in challenging environments and many of these complex tasks require estimates
of physical properties such as friction or weight. Estimating such properties
using learning is challenging due to the large amounts of labelled data
required for training and the difficulty of updating these learned models
online at run time. To overcome these challenges, this paper introduces a
novel, multi-modal approach for representing semantic predictions and physical
property estimates jointly in a probabilistic manner. By using conjugate pairs,
the proposed method enables closed-form Bayesian updates given visual and
tactile measurements without requiring additional training data. The efficacy
of the proposed algorithm is demonstrated through several hardware experiments.
In particular, this paper illustrates that by conditioning semantic
classifications on physical properties, the proposed method quantitatively
outperforms state-of-the-art semantic classification methods that rely on
vision alone. To further illustrate its utility, the proposed method is used in
several applications including to represent affordance-based properties
probabilistically and a challenging terrain traversal task using a legged
robot. In the latter task, the proposed method represents the coefficient of
friction of the terrain probabilistically, which enables the use of an on-line
risk-aware planner that switches the legged robot from a dynamic gait to a
static, stable gait when the expected value of the coefficient of friction
falls below a given threshold. Videos of these case studies are presented in
the multimedia attachment. The proposed framework includes an open-source C++
and ROS interface.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05873" title="Abstract">arXiv:2402.05873</a> [<a href="/pdf/2402.05873" title="Download PDF">pdf</a>, <a href="/format/2402.05873" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coordinated Activity Modulates the Behavior and Emotions of Organic  Users: A Case Study on Tweets about the Gaza Conflict
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dey%2C+P">Priyanka Dey</a>, 
<a href="/search/cs?searchtype=author&query=Luceri%2C+L">Luca Luceri</a>, 
<a href="/search/cs?searchtype=author&query=Ferrara%2C+E">Emilio Ferrara</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Social media has become a crucial conduit for the swift dissemination of
information during global crises. However, this also paves the way for the
manipulation of narratives by malicious actors. This research delves into the
interaction dynamics between coordinated (malicious) entities and organic
(regular) users on Twitter amidst the Gaza conflict. Through the analysis of
approximately 3.5 million tweets from over 1.3 million users, our study
uncovers that coordinated users significantly impact the information landscape,
successfully disseminating their content across the network: a substantial
fraction of their messages is adopted and shared by organic users. Furthermore,
the study documents a progressive increase in organic users' engagement with
coordinated content, which is paralleled by a discernible shift towards more
emotionally polarized expressions in their subsequent communications. These
results highlight the critical need for vigilance and a nuanced understanding
of information manipulation on social media platforms.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05876" title="Abstract">arXiv:2402.05876</a> [<a href="/pdf/2402.05876" title="Download PDF">pdf</a>, <a href="/format/2402.05876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Offline Reinforcement Learning: Collaborative Single-Policy  Coverage Suffices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Woo%2C+J">Jiin Woo</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+L">Laixi Shi</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+G">Gauri Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+Y">Yuejie Chi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Multiagent Systems (cs.MA); Machine Learning (stat.ML)

</div>
<p class="mathjax">Offline reinforcement learning (RL), which seeks to learn an optimal policy
using offline data, has garnered significant interest due to its potential in
critical applications where online data collection is infeasible or expensive.
This work explores the benefit of federated learning for offline RL, aiming at
collaboratively leveraging offline datasets at multiple agents. Focusing on
finite-horizon episodic tabular Markov decision processes (MDPs), we design
FedLCB-Q, a variant of the popular model-free Q-learning algorithm tailored for
federated offline RL. FedLCB-Q updates local Q-functions at agents with novel
learning rate schedules and aggregates them at a central server using
importance averaging and a carefully designed pessimistic penalty term. Our
sample complexity analysis reveals that, with appropriately chosen parameters
and synchronization schedules, FedLCB-Q achieves linear speedup in terms of the
number of agents without requiring high-quality datasets at individual agents,
as long as the local datasets collectively cover the state-action space visited
by the optimal policy, highlighting the power of collaboration in the federated
setting. In fact, the sample complexity almost matches that of the single-agent
counterpart, as if all the data are stored at a central location, up to
polynomial factors of the horizon length. Furthermore, FedLCB-Q is
communication-efficient, where the number of communication rounds is only
linear with respect to the horizon length up to logarithmic factors.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05880" title="Abstract">arXiv:2402.05880</a> [<a href="/pdf/2402.05880" title="Download PDF">pdf</a>, <a href="/format/2402.05880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Echo Chamber? Effects of LLM-Powered Search Systems on  Diverse Information Seeking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+N">Nikhil Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Q+V">Q. Vera Liao</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Ziang Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in CHI'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Large language models (LLMs) powered conversational search systems have
already been used by hundreds of millions of people, and are believed to bring
many benefits over conventional search. However, while decades of research and
public discourse interrogated the risk of search systems in increasing
selective exposure and creating echo chambers -- limiting exposure to diverse
opinions and leading to opinion polarization, little is known about such a risk
of LLM-powered conversational search. We conduct two experiments to
investigate: 1) whether and how LLM-powered conversational search increases
selective exposure compared to conventional search; 2) whether and how LLMs
with opinion biases that either reinforce or challenge the user's view change
the effect. Overall, we found that participants engaged in more biased
information querying with LLM-powered conversational search, and an opinionated
LLM reinforcing their views exacerbated this bias. These results present
critical implications for the development of LLMs and conversational search
systems, and the policy governing these technologies.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05881" title="Abstract">arXiv:2402.05881</a> [<a href="/pdf/2402.05881" title="Download PDF">pdf</a>, <a href="/format/2402.05881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Localized and Distributed Beyond Diagonal Reconfigurable Intelligent  Surfaces with Lossy Interconnections: Modeling and Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nerini%2C+M">Matteo Nerini</a>, 
<a href="/search/cs?searchtype=author&query=Ghiaasi%2C+G">Golsa Ghiaasi</a>, 
<a href="/search/cs?searchtype=author&query=Clerckx%2C+B">Bruno Clerckx</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE for publication
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Reconfigurable intelligent surface (RIS) is a key technology to control the
communication environment in future wireless networks. Recently, beyond
diagonal RIS (BD-RIS) emerged as a generalization of RIS achieving larger
coverage through additional tunable impedance components interconnecting the
RIS elements. However, conventional RIS and BD-RIS can effectively serve only
users in their proximity, resulting in limited coverage. To overcome this
limitation, in this paper, we investigate distributed RIS, whose elements are
distributed over a wide region, in opposition to localized RIS commonly
considered in the literature. The scaling laws of distributed BD-RIS reveal
that it offers significant gains over distributed conventional RIS and
localized BD-RIS, enabled by its interconnections allowing signal propagation
within the BD-RIS. To assess the practical performance of distributed BD-RIS,
we model and optimize BD-RIS with lossy interconnections through transmission
line theory. Our model accounts for phase changes and losses over the BD-RIS
interconnections arising when the interconnection lengths are not much smaller
than the wavelength. Numerical results show that the performance of localized
BD-RIS is only slightly impacted by losses, given the short interconnection
lengths. Besides, distributed BD-RIS can achieve orders of magnitude of gains
over conventional RIS, even in the presence of low losses.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05882" title="Abstract">arXiv:2402.05882</a> [<a href="/pdf/2402.05882" title="Download PDF">pdf</a>, <a href="/format/2402.05882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GET-Tok: A GenAI-Enriched Multimodal TikTok Dataset Documenting the 2022  Attempted Coup in Peru
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pinto%2C+G">Gabriela Pinto</a>, 
<a href="/search/cs?searchtype=author&query=Burghardt%2C+K">Keith Burghardt</a>, 
<a href="/search/cs?searchtype=author&query=Lerman%2C+K">Kristina Lerman</a>, 
<a href="/search/cs?searchtype=author&query=Ferrara%2C+E">Emilio Ferrara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Github repository: <a href="https://github.com/gabbypinto/GET-Tok-Peru">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">TikTok is one of the largest and fastest-growing social media sites in the
world. TikTok features, however, such as voice transcripts, are often missing
and other important features, such as OCR or video descriptions, do not exist.
We introduce the Generative AI Enriched TikTok (GET-Tok) data, a pipeline for
collecting TikTok videos and enriched data by augmenting the TikTok Research
API with generative AI models. As a case study, we collect videos about the
attempted coup in Peru initiated by its former President, Pedro Castillo, and
its accompanying protests. The data includes information on 43,697 videos
published from November 20, 2022 to March 1, 2023 (102 days). Generative AI
augments the collected data via transcripts of TikTok videos, text descriptions
of what is shown in the videos, what text is displayed within the video, and
the stances expressed in the video. Overall, this pipeline will contribute to a
better understanding of online discussion in a multimodal setting with
applications of Generative AI, especially outlining the utility of this
pipeline in non-English-language social media. Our code used to produce the
pipeline is in a public Github repository:
https://github.com/gabbypinto/GET-Tok-Peru.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05884" title="Abstract">arXiv:2402.05884</a> [<a href="/pdf/2402.05884" title="Download PDF">pdf</a>, <a href="/ps/2402.05884" title="Download PostScript">ps</a>, <a href="/format/2402.05884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cutsets and EF1 Fair Division of Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiehua Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zwicker%2C+W+S">William S. Zwicker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended abstract accepted at AAMAS'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">In fair division of a connected graph $G = (V, E)$, each of $n$ agents
receives a share of $G$'s vertex set $V$. These shares partition $V$, with each
share required to induce a connected subgraph. Agents use their own valuation
functions to determine the non-negative numerical values of the shares, which
determine whether the allocation is fair in some specified sense. We introduce
forbidden substructures called graph cutsets, which block divisions that are
fair in the EF1 (envy-free up to one item) sense by cutting the graph into "too
many pieces". Two parameters - gap and valence - determine blocked values of
$n$. If $G$ guarantees connected EF1 allocations for $n$ agents with valuations
that are CA (common and additive), then $G$ contains no elementary cutset of
gap $k \ge 2$ and valence in the interval $\[n - k + 1, n - 1\]$. If $G$
guarantees connected EF1 allocations for $n$ agents with valuations in the
broader CM (common and monotone) class, then $G$ contains no cutset of gap $k
\ge 2$ and valence in the interval $\[n - k + 1, n - 1\]$. These results rule
out the existence of connected EF1 allocations in a variety of situations. For
some graphs $G$ we can, with help from some new positive results, pin down
$G$'s spectrum - the list of exactly which values of $n$ do/do not guarantee
connected EF1 allocations. Examples suggest a conjectured common spectral
pattern for all graphs. Further, we show that it is NP-hard to determine
whether a graph admits a cutset. We also provide an example of a
(non-traceable) graph on eight vertices that has no cutsets of gap $\ge 2$ at
all, yet fails to guarantee connected EF1 allocations for three agents with CA
preferences.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05885" title="Abstract">arXiv:2402.05885</a> [<a href="/pdf/2402.05885" title="Download PDF">pdf</a>, <a href="/format/2402.05885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EUGENE: Explainable Unsupervised Approximation of Graph Edit Distance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bommakanti%2C+A">Aditya Bommakanti</a>, 
<a href="/search/cs?searchtype=author&query=Vonteri%2C+H+R">Harshith Reddy Vonteri</a>, 
<a href="/search/cs?searchtype=author&query=Ranu%2C+S">Sayan Ranu</a>, 
<a href="/search/cs?searchtype=author&query=Karras%2C+P">Panagiotis Karras</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The need to identify graphs having small structural distance from a query
arises in biology, chemistry, recommender systems, and social network analysis.
Among several methods to measure inter graph distance, Graph Edit Distance
(GED) is preferred for its comprehensibility, yet hindered by the NP-hardness
of its computation. State-of-the-art GED approximations predominantly employ
neural methods, which, however, (i) lack an explanatory edit path corresponding
to the approximated GED; (ii) require the NP-hard generation of ground-truth
GEDs for training; and (iii) necessitate separate training on each dataset. In
this paper, we propose an efficient algebraic unsuper vised method, EUGENE,
that approximates GED and yields edit paths corresponding to the approx imated
cost, while eliminating the need for ground truth generation and data-specific
training. Extensive experimental evaluation demonstrates that the
aforementioned benefits of EUGENE do not come at the cost of efficacy.
Specifically, EUGENE consistently ranks among the most accurate methods across
all of the benchmark datasets and outperforms majority of the neural
approaches.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05889" title="Abstract">arXiv:2402.05889</a> [<a href="/pdf/2402.05889" title="Download PDF">pdf</a>, <a href="/format/2402.05889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CREMA: Multimodal Compositional Video Reasoning via Efficient Modular  Adaptation and Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Shoubin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+J">Jaehong Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> project page: <a href="https://CREMA-VideoLLM.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Despite impressive advancements in multimodal compositional reasoning
approaches, they are still limited in their flexibility and efficiency by
processing fixed modality inputs while updating a lot of model parameters. This
paper tackles these critical challenges and proposes CREMA, an efficient and
modular modality-fusion framework for injecting any new modality into video
reasoning. We first augment multiple informative modalities (such as optical
flow, 3D point cloud, audio) from given videos without extra human annotation
by leveraging existing pre-trained models. Next, we introduce a query
transformer with multiple parameter-efficient modules associated with each
accessible modality. It projects diverse modality features to the LLM token
embedding space, allowing the model to integrate different data types for
response generation. Furthermore, we propose a fusion module designed to
compress multimodal queries, maintaining computational efficiency in the LLM
while combining additional modalities. We validate our method on video-3D,
video-audio, and video-language reasoning tasks and achieve better/equivalent
performance against strong multimodal LLMs, including BLIP-2, 3D-LLM, and
SeViLA while using 96% fewer trainable parameters. We provide extensive
analyses of CREMA, including the impact of each modality on reasoning domains,
the design of the fusion module, and example visualizations.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05891" title="Abstract">arXiv:2402.05891</a> [<a href="/pdf/2402.05891" title="Download PDF">pdf</a>, <a href="/format/2402.05891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On benefits of cooperation under strategic power
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fiestras-Janeiro%2C+M+G">M. Gloria Fiestras-Janeiro</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Jurado%2C+I">Ignacio Garc&#xed;a-Jurado</a>, 
<a href="/search/cs?searchtype=author&query=Meca%2C+A">Ana Meca</a>, 
<a href="/search/cs?searchtype=author&query=Mosquera%2C+M+A">Manuel A. Mosquera</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Annals of Operations Research 288 (2019) 285-306
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">We introduce a new model involving TU-games and exogenous structures.
Specifically, we consider that each player in a population can choose an
element in a strategy set and that, for every possible strategy profile, a
TU-game is associated with the population. This is what we call a TU-game with
strategies. We propose and characterize the maxmin procedure to map every game
with strategies to a TU-game. We also study whether or not the relevant
properties of TU-games are transmitted by applying the maxmin procedure.
Finally, we examine two relevant classes of TU-games with strategies: airport
and simple games with strategies.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05892" title="Abstract">arXiv:2402.05892</a> [<a href="/pdf/2402.05892" title="Download PDF">pdf</a>, <a href="/format/2402.05892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shufan Li</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+H">Harkanwar Singh</a>, 
<a href="/search/cs?searchtype=author&query=Grover%2C+A">Aditya Grover</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In recent years, Transformers have become the de-facto architecture for
sequence modeling on text and a variety of multi-dimensional data, such as
images and video. However, the use of self-attention layers in a Transformer
incurs prohibitive compute and memory complexity that scales quadratically
w.r.t. the sequence length. A recent architecture, Mamba, based on state space
models has been shown to achieve comparable performance for modeling text
sequences, while scaling linearly with the sequence length. In this work, we
present Mamba-ND, a generalized design extending the Mamba architecture to
arbitrary multi-dimensional data. Our design alternatively unravels the input
data across different dimensions following row-major orderings. We provide a
systematic comparison of Mamba-ND with several other alternatives, based on
prior multi-dimensional extensions such as Bi-directional LSTMs and S4ND.
Empirically, we show that Mamba-ND demonstrates performance competitive with
the state-of-the-art on a variety of multi-dimensional benchmarks, including
ImageNet-1K classification, HMDB-51 action recognition, and ERA5 weather
forecasting.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05893" title="Abstract">arXiv:2402.05893</a> [<a href="/pdf/2402.05893" title="Download PDF">pdf</a>, <a href="/format/2402.05893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalizing Driver Safety Interfaces via Driver Cognitive Factors  Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sumner%2C+E+S">Emily S Sumner</a>, 
<a href="/search/cs?searchtype=author&query=DeCastro%2C+J">Jonathan DeCastro</a>, 
<a href="/search/cs?searchtype=author&query=Costa%2C+J">Jean Costa</a>, 
<a href="/search/cs?searchtype=author&query=Gopinath%2C+D+E">Deepak E Gopinath</a>, 
<a href="/search/cs?searchtype=author&query=Kimani%2C+E">Everlyne Kimani</a>, 
<a href="/search/cs?searchtype=author&query=Hakimi%2C+S">Shabnam Hakimi</a>, 
<a href="/search/cs?searchtype=author&query=Morgan%2C+A">Allison Morgan</a>, 
<a href="/search/cs?searchtype=author&query=Best%2C+A">Andrew Best</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+H">Hieu Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Brooks%2C+D+J">Daniel J Brooks</a>, 
<a href="/search/cs?searchtype=author&query=Haq%2C+B+u">Bassam ul Haq</a>, 
<a href="/search/cs?searchtype=author&query=Patrikalakis%2C+A">Andrew Patrikalakis</a>, 
<a href="/search/cs?searchtype=author&query=Yasuda%2C+H">Hiroshi Yasuda</a>, 
<a href="/search/cs?searchtype=author&query=Sieck%2C+K">Kate Sieck</a>, 
<a href="/search/cs?searchtype=author&query=Balachandran%2C+A">Avinash Balachandran</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tiffany Chen</a>, 
<a href="/search/cs?searchtype=author&query=Rosman%2C+G">Guy Rosman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Recent advances in AI and intelligent vehicle technology hold promise to
revolutionize mobility and transportation, in the form of advanced driving
assistance (ADAS) interfaces. Although it is widely recognized that certain
cognitive factors, such as impulsivity and inhibitory control, are related to
risky driving behavior, play a significant role in on-road risk-taking,
existing systems fail to leverage such factors. Varying levels of these
cognitive factors could influence the effectiveness and acceptance of driver
safety interfaces.
<br />We demonstrate an approach for personalizing driver interaction via driver
safety interfaces that are triggered based on a learned recurrent neural
network. The network is trained from a population of human drivers to infer
impulsivity and inhibitory control from recent driving behavior. Using a
high-fidelity vehicle motion simulator, we demonstrate the ability to deduce
these factors from driver behavior. We then use these inferred factors to make
instantaneous determinations on whether or not to engage a driver safety
interface. This interface aims to decrease a driver's speed during yellow
lights and reduce their inclination to run through them.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05894" title="Abstract">arXiv:2402.05894</a> [<a href="/pdf/2402.05894" title="Download PDF">pdf</a>, <a href="/format/2402.05894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model Meets Graph Neural Network in Knowledge  Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Shengxiang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+G">Guobing Zou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Song Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bofeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yixin Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 6 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Despite recent community revelations about the advancements and potential of
Large Language Models (LLMs) in understanding Text-Attributed Graphs (TAG), the
deployment of LLMs for production is hindered by their high computational and
storage requirements, as well as long latencies during inference.
Simultaneously, although traditional Graph Neural Networks (GNNs) are light
weight and adept at learning structural features of graphs, their ability to
grasp the complex semantics in TAGs is somewhat constrained for real
applications. To address these limitations, we concentrate on the downstream
task of node classification in TAG and propose a novel graph knowledge
distillation framework, termed Linguistic Graph Knowledge Distillation
(LinguGKD), using LLMs as teacher models and GNNs as student models for
knowledge distillation. It involves TAG-oriented instruction tuning of LLM on
designed node classification prompts, followed by aligning the hierarchically
learned node features of the teacher LLM and the student GNN in latent space,
employing a layer-adaptive contrastive learning strategy. Through extensive
experiments on a variety of LLM and GNN models and multiple benchmark datasets,
the proposed LinguGKD significantly boosts the student GNN's predictive
accuracy and convergence rate, without the need of extra data or model
parameters. Compared to teacher LLM, distilled GNN achieves superior inference
speed equipped with much fewer computing and storage demands, when surpassing
the teacher LLM's classification performance on some of benchmark datasets.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05895" title="Abstract">arXiv:2402.05895</a> [<a href="/pdf/2402.05895" title="Download PDF">pdf</a>, <a href="/format/2402.05895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Voting and Abstract Argumentation to Understand Online  Discussions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bernreiter%2C+M">Michael Bernreiter</a>, 
<a href="/search/cs?searchtype=author&query=Maly%2C+J">Jan Maly</a>, 
<a href="/search/cs?searchtype=author&query=Nardi%2C+O">Oliviero Nardi</a>, 
<a href="/search/cs?searchtype=author&query=Woltran%2C+S">Stefan Woltran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages. Extended version of an accepted AAMAS-24 paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Online discussion platforms are a vital part of the public discourse in a
deliberative democracy. However, how to interpret the outcomes of the
discussions on these platforms is often unclear. In this paper, we propose a
novel and explainable method for selecting a set of most representative,
consistent points of view by combining methods from computational social choice
and abstract argumentation. Specifically, we model online discussions as
abstract argumentation frameworks combined with information regarding which
arguments voters approve of. Based on ideas from approval-based multiwinner
voting, we introduce several voting rules for selecting a set of preferred
extensions that represents voters' points of view. We compare the proposed
methods across several dimensions, theoretically and in numerical simulations,
and give clear suggestions on which methods to use depending on the specific
situation.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05902" title="Abstract">arXiv:2402.05902</a> [<a href="/pdf/2402.05902" title="Download PDF">pdf</a>, <a href="/ps/2402.05902" title="Download PostScript">ps</a>, <a href="/format/2402.05902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ClickSAM: Fine-tuning Segment Anything Model using click prompts for  ultrasound image segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+A">Aimee Guo</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+G">Gace Fei</a>, 
<a href="/search/cs?searchtype=author&query=Pasupuletic%2C+H">Hemanth Pasupuletic</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jing Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures, SPIE Medical Imaging Conference 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Medical Physics (physics.med-ph)

</div>
<p class="mathjax">The newly released Segment Anything Model (SAM) is a popular tool used in
image processing due to its superior segmentation accuracy, variety of input
prompts, training capabilities, and efficient model design. However, its
current model is trained on a diverse dataset not tailored to medical images,
particularly ultrasound images. Ultrasound images tend to have a lot of noise,
making it difficult to segment out important structures. In this project, we
developed ClickSAM, which fine-tunes the Segment Anything Model using click
prompts for ultrasound images. ClickSAM has two stages of training: the first
stage is trained on single-click prompts centered in the ground-truth contours,
and the second stage focuses on improving the model performance through
additional positive and negative click prompts. By comparing the first stage
predictions to the ground-truth masks, true positive, false positive, and false
negative segments are calculated. Positive clicks are generated using the true
positive and false negative segments, and negative clicks are generated using
the false positive segments. The Centroidal Voronoi Tessellation algorithm is
then employed to collect positive and negative click prompts in each segment
that are used to enhance the model performance during the second stage of
training. With click-train methods, ClickSAM exhibits superior performance
compared to other existing models for ultrasound image segmentation.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05904" title="Abstract">arXiv:2402.05904</a> [<a href="/pdf/2402.05904" title="Download PDF">pdf</a>, <a href="/format/2402.05904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+E+C">Eun Cheol Choi</a>, 
<a href="/search/cs?searchtype=author&query=Ferrara%2C+E">Emilio Ferrara</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Our society is facing rampant misinformation harming public health and trust.
To address the societal challenge, we introduce FACT-GPT, a system leveraging
Large Language Models (LLMs) to automate the claim matching stage of
fact-checking. FACT-GPT, trained on a synthetic dataset, identifies social
media content that aligns with, contradicts, or is irrelevant to previously
debunked claims. Our evaluation shows that our specialized LLMs can match the
accuracy of larger models in identifying related claims, closely mirroring
human judgment. This research provides an automated solution for efficient
claim matching, demonstrates the potential of LLMs in supporting fact-checkers,
and offers valuable resources for further research in the field.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05906" title="Abstract">arXiv:2402.05906</a> [<a href="/pdf/2402.05906" title="Download PDF">pdf</a>, <a href="/format/2402.05906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative  Markov Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghaemi%2C+H">Hafez Ghaemi</a>, 
<a href="/search/cs?searchtype=author&query=Kebriaei%2C+H">Hamed Kebriaei</a>, 
<a href="/search/cs?searchtype=author&query=Moghaddam%2C+A+R">Alireza Ramezani Moghaddam</a>, 
<a href="/search/cs?searchtype=author&query=Ahamdabadi%2C+M+N">Majid Nili Ahamdabadi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Classical multi-agent reinforcement learning (MARL) assumes risk neutrality
and complete objectivity for agents. However, in settings where agents need to
consider or model human economic or social preferences, a notion of risk must
be incorporated into the RL optimization problem. This will be of greater
importance in MARL where other human or non-human agents are involved, possibly
with their own risk-sensitive policies. In this work, we consider
risk-sensitive and non-cooperative MARL with cumulative prospect theory (CPT),
a non-convex risk measure and a generalization of coherent measures of risk.
CPT is capable of explaining loss aversion in humans and their tendency to
overestimate/underestimate small/large probabilities. We propose a distributed
sampling-based actor-critic (AC) algorithm with CPT risk for network
aggregative Markov games (NAMGs), which we call Distributed Nested CPT-AC.
Under a set of assumptions, we prove the convergence of the algorithm to a
subjective notion of Markov perfect Nash equilibrium in NAMGs. The experimental
results show that subjective CPT policies obtained by our algorithm can be
different from the risk-neutral ones, and agents with a higher loss aversion
are more inclined to socially isolate themselves in an NAMG.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05913" title="Abstract">arXiv:2402.05913</a> [<a href="/pdf/2402.05913" title="Download PDF">pdf</a>, <a href="/format/2402.05913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Stagewise Pretraining via Progressive Subnetworks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Panigrahi%2C+A">Abhishek Panigrahi</a>, 
<a href="/search/cs?searchtype=author&query=Saunshi%2C+N">Nikunj Saunshi</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+K">Kaifeng Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Miryoosefi%2C+S">Sobhan Miryoosefi</a>, 
<a href="/search/cs?searchtype=author&query=Reddi%2C+S">Sashank Reddi</a>, 
<a href="/search/cs?searchtype=author&query=Kale%2C+S">Satyen Kale</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sanjiv Kumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent developments in large language models have sparked interest in
efficient pretraining methods. A recent effective paradigm is to perform
stage-wise training, where the size of the model is gradually increased over
the course of training (e.g. gradual stacking (Reddi et al., 2023)). While the
resource and wall-time savings are appealing, it has limitations, particularly
the inability to evaluate the full model during earlier stages, and degradation
in model quality due to smaller model capacity in the initial stages. In this
work, we propose an alternative framework, progressive subnetwork training,
that maintains the full model throughout training, but only trains subnetworks
within the model in each step. We focus on a simple instantiation of this
framework, Random Path Training (RaPTr) that only trains a sub-path of layers
in each step, progressively increasing the path lengths in stages. RaPTr
achieves better pre-training loss for BERT and UL2 language models while
requiring 20-33% fewer FLOPs compared to standard training, and is competitive
or better than other efficient training methods. Furthermore, RaPTr shows
better downstream performance on UL2, improving QA tasks and SuperGLUE by 1-5%
compared to standard training and stacking. Finally, we provide a theoretical
basis for RaPTr to justify (a) the increasing complexity of subnetworks in
stages, and (b) the stability in loss across stage transitions due to residual
connections and layer norm.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05914" title="Abstract">arXiv:2402.05914</a> [<a href="/pdf/2402.05914" title="Download PDF">pdf</a>, <a href="/format/2402.05914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Triangular phase-shift detector for drone precise vertical landing RF  systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ara%C3%B1a-Pulido%2C+V">V&#xed;ctor Ara&#xf1;a-Pulido</a>, 
<a href="/search/eess?searchtype=author&query=Jim%C3%A9nez-Ygu%C3%A1cel%2C+E">Eugenio Jim&#xe9;nez-Ygu&#xe1;cel</a>, 
<a href="/search/eess?searchtype=author&query=Cabrera-Almeida%2C+F">Francisco Cabrera-Almeida</a>, 
<a href="/search/eess?searchtype=author&query=Quintana-Morales%2C+P">Pedro Quintana-Morales</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper has been accepted by IEEE Transactions on Instrumentation and Measurement, vol. 70, pp. 1-8, 2021
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Instrumentation and Measurement, vol. 70, pp.
  1-8, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper presents a circuit for precise vertical landing of drones based on
a three phase-shifts detection of a single frequency transmitted from the
landing point. The circuit can be considered as a new navigation sensor that
assists in guidance corrections for landing at a specific point. The circuit
has three inputs to which the signal transmitted from an oscillator located at
the landing point arrives with different delays. The input signals are combined
in pairs in each of the three analog phase detectors, after having passed
through 3 dB@90 o hybrid couplers that guarantee a theoretical non-ambiguous
phase-shift range of +-90 degree. Each output has a voltage that is
proportional to the phase-shift between each of the input signals, which in
turn depend on the position relative to the landing point. A simple landing
algorithm based on phase-shift values is proposed, which could be integrated
into the same flight control platform, thus avoiding the need to add additional
processing components. To demonstrate the feasibility of the proposed design, a
triangular phase-shift detector prototype has been implemented using commercial
devices. Calibration and measurements at 2.46 GHz show a dynamic range of 30 dB
and a non-ambiguous detection range of +-80 degree in the worst cases. Those
specs let us to track the drone during the landing maneuver in an inverted cone
formed by a surface with a +-4.19 m radius at 10m high and the landing point.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05916" title="Abstract">arXiv:2402.05916</a> [<a href="/pdf/2402.05916" title="Download PDF">pdf</a>, <a href="/format/2402.05916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenEFT: Understanding Statics and Dynamics of Model Generalization via  Effective Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baek%2C+D+D">David D. Baek</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tegmark%2C+M">Max Tegmark</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We present GenEFT: an effective theory framework for shedding light on the
statics and dynamics of neural network generalization, and illustrate it with
graph learning examples. We first investigate the generalization phase
transition as data size increases, comparing experimental results with
information-theory-based approximations. We find generalization in a Goldilocks
zone where the decoder is neither too weak nor too powerful. We then introduce
an effective theory for the dynamics of representation learning, where
latent-space representations are modeled as interacting particles (repons), and
find that it explains our experimentally observed phase transition between
generalization and overfitting as encoder and decoder learning rates are
scanned. This highlights the power of physics-inspired effective theories for
bridging the gap between theoretical predictions and practice in machine
learning.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05917" title="Abstract">arXiv:2402.05917</a> [<a href="/pdf/2402.05917" title="Download PDF">pdf</a>, <a href="/format/2402.05917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Point-VOS: Pointing Up Video Object Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zulfikar%2C+I+E">Idil Esen Zulfikar</a>, 
<a href="/search/cs?searchtype=author&query=Mahadevan%2C+S">Sabarinath Mahadevan</a>, 
<a href="/search/cs?searchtype=author&query=Voigtlaender%2C+P">Paul Voigtlaender</a>, 
<a href="/search/cs?searchtype=author&query=Leibe%2C+B">Bastian Leibe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 20 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Current state-of-the-art Video Object Segmentation (VOS) methods rely on
dense per-object mask annotations both during training and testing. This
requires time-consuming and costly video annotation mechanisms. We propose a
novel Point-VOS task with a spatio-temporally sparse point-wise annotation
scheme that substantially reduces the annotation effort. We apply our
annotation scheme to two large-scale video datasets with text descriptions and
annotate over 19M points across 133K objects in 32K videos. Based on our
annotations, we propose a new Point-VOS benchmark, and a corresponding
point-based training mechanism, which we use to establish strong baseline
results. We show that existing VOS methods can easily be adapted to leverage
our point annotations during training, and can achieve results close to the
fully-supervised performance when trained on pseudo-masks generated from these
points. In addition, we show that our data can be used to improve models that
connect vision and language, by evaluating it on the Video Narrative Grounding
(VNG) task. We will make our code and annotations available at
https://pointvos.github.io.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05918" title="Abstract">arXiv:2402.05918</a> [<a href="/pdf/2402.05918" title="Download PDF">pdf</a>, <a href="/format/2402.05918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consensus-driven Deviated Pursuit for Guaranteed Simultaneous  Interception of Moving Targets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sinha%2C+A">Abhinav Sinha</a>, 
<a href="/search/eess?searchtype=author&query=Mukherjee%2C+D">Dwaipayan Mukherjee</a>, 
<a href="/search/eess?searchtype=author&query=Kumar%2C+S+R">Shashi Ranjan Kumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Multiagent Systems (cs.MA); Dynamical Systems (math.DS); Optimization and Control (math.OC); Adaptation and Self-Organizing Systems (nlin.AO)

</div>
<p class="mathjax">This work proposes a cooperative strategy that employs deviated pursuit
guidance to simultaneously intercept a moving (but not manoeuvring) target. As
opposed to many existing cooperative guidance strategies which use estimates of
time-to-go, based on proportional-navigation guidance, the proposed strategy
uses an exact expression for time-to-go to ensure simultaneous interception.
The guidance design considers nonlinear engagement kinematics, allowing the
proposed strategy to remain effective over a large operating regime. Unlike
existing strategies on simultaneous interception that achieve interception at
the average value of their initial time-to-go estimates, this work provides
flexibility in the choice of impact time. By judiciously choosing the edge
weights of the communication network, a weighted consensus in time-to-go can be
achieved. It has been shown that by allowing an edge weight to be negative,
consensus in time-to-go can even be achieved for an impact time that lies
outside the convex hull of the set of initial time-to-go values of the
individual interceptors. The bounds on such negative weights have been analysed
for some special graphs, using Nyquist criterion. Simulations are provided to
vindicate the efficacy of the proposed strategy.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05919" title="Abstract">arXiv:2402.05919</a> [<a href="/pdf/2402.05919" title="Download PDF">pdf</a>, <a href="/format/2402.05919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaborative Control for Geometry-Conditioned PBR Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vainer%2C+S">Shimon Vainer</a>, 
<a href="/search/cs?searchtype=author&query=Boss%2C+M">Mark Boss</a>, 
<a href="/search/cs?searchtype=author&query=Parger%2C+M">Mathias Parger</a>, 
<a href="/search/cs?searchtype=author&query=Kutsy%2C+K">Konstantin Kutsy</a>, 
<a href="/search/cs?searchtype=author&query=De+Nigris%2C+D">Dante De Nigris</a>, 
<a href="/search/cs?searchtype=author&query=Rowles%2C+C">Ciara Rowles</a>, 
<a href="/search/cs?searchtype=author&query=Perony%2C+N">Nicolas Perony</a>, 
<a href="/search/cs?searchtype=author&query=Donn%C3%A9%2C+S">Simon Donn&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 10 figures; Project page: <a href="https://unity-research.github.io/holo-gen/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Current 3D content generation builds on generative models that output RGB
images. Modern graphics pipelines, however, require physically-based rendering
(PBR) material properties. We propose to model the PBR image distribution
directly to avoid photometric inaccuracies in RGB generation and the inherent
ambiguity in extracting PBR from RGB. Existing paradigms for cross-modal
finetuning are not suited for PBR generation due to a lack of data and the high
dimensionality of the output modalities: we overcome both challenges by
retaining a frozen RGB model and tightly linking a newly trained PBR model
using a novel cross-network communication paradigm. As the base RGB model is
fully frozen, the proposed method does not risk catastrophic forgetting during
finetuning and remains compatible with techniques such as IPAdapter pretrained
for the base RGB model. We validate our design choices, robustness to data
sparsity, and compare against existing paradigms with an extensive experimental
section.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05926" title="Abstract">arXiv:2402.05926</a> [<a href="/pdf/2402.05926" title="Download PDF">pdf</a>, <a href="/format/2402.05926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Convergence of Zeroth-Order Federated Tuning in Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ling%2C+Z">Zhenqing Ling</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Daoyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+L">Liuyi Yao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Ying Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">The confluence of Federated Learning (FL) and Large Language Models (LLMs) is
ushering in a new era in privacy-preserving natural language processing.
However, the intensive memory requirements for fine-tuning LLMs pose
significant challenges, especially when deploying on edge devices with limited
computational resources. To circumvent this, we explore the novel integration
of Memory-efficient Zeroth-Order Optimization within a federated setting, a
synergy we denote as FedMeZO. Our study is the first to examine the theoretical
underpinnings of FedMeZO in the context of LLMs, tackling key questions
regarding the influence of large parameter spaces on optimization behavior, the
establishment of convergence properties, and the identification of critical
parameters for convergence to inform personalized federated strategies. Our
extensive empirical evidence supports the theory, showing that FedMeZO not only
converges faster than traditional first-order methods such as SGD but also
significantly reduces GPU memory usage during training to levels comparable to
those during inference. Moreover, the proposed personalized FL strategy that is
built upon the theoretical insights to customize the client-wise learning rate
can effectively accelerate loss reduction. We hope our work can help to bridge
theoretical and practical aspects of federated fine-tuning for LLMs and
facilitate further development and research.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05928" title="Abstract">arXiv:2402.05928</a> [<a href="/pdf/2402.05928" title="Download PDF">pdf</a>, <a href="/ps/2402.05928" title="Download PostScript">ps</a>, <a href="/format/2402.05928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation  for the Square Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ziemann%2C+I">Ingvar Ziemann</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+S">Stephen Tu</a>, 
<a href="/search/cs?searchtype=author&query=Pappas%2C+G+J">George J. Pappas</a>, 
<a href="/search/cs?searchtype=author&query=Matni%2C+N">Nikolai Matni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In this work, we study statistical learning with dependent ($\beta$-mixing)
data and square loss in a hypothesis class $\mathscr{F}\subset L_{\Psi_p}$
where $\Psi_p$ is the norm $\|f\|_{\Psi_p} \triangleq \sup_{m\geq 1} m^{-1/p}
\|f\|_{L^m} $ for some $p\in [2,\infty]$. Our inquiry is motivated by the
search for a sharp noise interaction term, or variance proxy, in learning with
dependent data. Absent any realizability assumption, typical non-asymptotic
results exhibit variance proxies that are deflated \emph{multiplicatively} by
the mixing time of the underlying covariates process. We show that whenever the
topologies of $L^2$ and $\Psi_p$ are comparable on our hypothesis class
$\mathscr{F}$ -- that is, $\mathscr{F}$ is a weakly sub-Gaussian class:
$\|f\|_{\Psi_p} \lesssim \|f\|_{L^2}^\eta$ for some $\eta\in (0,1]$ -- the
empirical risk minimizer achieves a rate that only depends on the complexity of
the class and second order statistics in its leading term. Our result holds
whether the problem is realizable or not and we refer to this as a \emph{near
mixing-free rate}, since direct dependence on mixing is relegated to an
additive higher order term. We arrive at our result by combining the above
notion of a weakly sub-Gaussian class with mixed tail generic chaining. This
combination allows us to compute sharp, instance-optimal rates for a wide range
of problems. %Our approach, reliant on mixed tail generic chaining, allows us
to obtain sharp, instance-optimal rates. Examples that satisfy our framework
include sub-Gaussian linear regression, more general smoothly parameterized
function classes, finite hypothesis classes, and bounded smoothness classes.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05929" title="Abstract">arXiv:2402.05929</a> [<a href="/pdf/2402.05929" title="Download PDF">pdf</a>, <a href="/format/2402.05929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Interactive Agent Foundation Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Durante%2C+Z">Zane Durante</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+B">Bidipta Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+R">Ran Gong</a>, 
<a href="/search/cs?searchtype=author&query=Taori%2C+R">Rohan Taori</a>, 
<a href="/search/cs?searchtype=author&query=Noda%2C+Y">Yusuke Noda</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+P">Paul Tang</a>, 
<a href="/search/cs?searchtype=author&query=Adeli%2C+E">Ehsan Adeli</a>, 
<a href="/search/cs?searchtype=author&query=Lakshmikanth%2C+S+K">Shrinidhi Kowshika Lakshmikanth</a>, 
<a href="/search/cs?searchtype=author&query=Schulman%2C+K">Kevin Schulman</a>, 
<a href="/search/cs?searchtype=author&query=Milstein%2C+A">Arnold Milstein</a>, 
<a href="/search/cs?searchtype=author&query=Terzopoulos%2C+D">Demetri Terzopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Famoti%2C+A">Ade Famoti</a>, 
<a href="/search/cs?searchtype=author&query=Kuno%2C+N">Noboru Kuno</a>, 
<a href="/search/cs?searchtype=author&query=Llorens%2C+A">Ashley Llorens</a>, 
<a href="/search/cs?searchtype=author&query=Vo%2C+H">Hoi Vo</a>, 
<a href="/search/cs?searchtype=author&query=Ikeuchi%2C+K">Katsu Ikeuchi</a>, 
<a href="/search/cs?searchtype=author&query=Fei-Fei%2C+L">Li Fei-Fei</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wake%2C+N">Naoki Wake</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qiuyuan Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">The development of artificial intelligence systems is transitioning from
creating static, task-specific models to dynamic, agent-based systems capable
of performing well in a wide range of applications. We propose an Interactive
Agent Foundation Model that uses a novel multi-task agent training paradigm for
training AI agents across a wide range of domains, datasets, and tasks. Our
training paradigm unifies diverse pre-training strategies, including visual
masked auto-encoders, language modeling, and next-action prediction, enabling a
versatile and adaptable AI framework. We demonstrate the performance of our
framework across three separate domains -- Robotics, Gaming AI, and Healthcare.
Our model demonstrates its ability to generate meaningful and contextually
relevant outputs in each area. The strength of our approach lies in its
generality, leveraging a variety of data sources such as robotics sequences,
gameplay data, large-scale video datasets, and textual information for
effective multimodal and multi-task learning. Our approach provides a promising
avenue for developing generalist, action-taking, multimodal systems.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05930" title="Abstract">arXiv:2402.05930</a> [<a href="/pdf/2402.05930" title="Download PDF">pdf</a>, <a href="/format/2402.05930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WebLINX: Real-World Website Navigation with Multi-Turn Dialogue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=L%C3%B9%2C+X+H">Xing Han L&#xf9;</a>, 
<a href="/search/cs?searchtype=author&query=Kasner%2C+Z">Zden&#x11b;k Kasner</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+S">Siva Reddy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose the problem of conversational web navigation, where a digital
agent controls a web browser and follows user instructions to solve real-world
tasks in a multi-turn dialogue fashion. To support this problem, we introduce
WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert
demonstrations of conversational web navigation. Our benchmark covers a broad
range of patterns on over 150 real-world websites and can be used to train and
evaluate agents in diverse scenarios. Due to the magnitude of information
present, Large Language Models (LLMs) cannot process entire web pages in
real-time. To solve this bottleneck, we design a retrieval-inspired model that
efficiently prunes HTML pages by ranking relevant elements. We use the selected
elements, along with screenshots and action history, to assess a variety of
models for their ability to replicate human behavior when navigating the web.
Our experiments span from small text-only to proprietary multimodal LLMs. We
find that smaller finetuned decoders surpass the best zero-shot LLMs (including
GPT-4V), but also larger finetuned multimodal models which were explicitly
pretrained on screenshots. However, all finetuned models struggle to generalize
to unseen websites. Our findings highlight the need for large multimodal models
that can generalize to novel settings. Our code, data and models are available
for research: https://mcgill-nlp.github.io/weblinx
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05932" title="Abstract">arXiv:2402.05932</a> [<a href="/pdf/2402.05932" title="Download PDF">pdf</a>, <a href="/format/2402.05932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Driving Everywhere with Large Language Model Policy Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Boyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+J">Jiageng Mao</a>, 
<a href="/search/cs?searchtype=author&query=Ivanovic%2C+B">Boris Ivanovic</a>, 
<a href="/search/cs?searchtype=author&query=Veer%2C+S">Sushant Veer</a>, 
<a href="/search/cs?searchtype=author&query=Leung%2C+K">Karen Leung</a>, 
<a href="/search/cs?searchtype=author&query=Pavone%2C+M">Marco Pavone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Adapting driving behavior to new environments, customs, and laws is a
long-standing problem in autonomous driving, precluding the widespread
deployment of autonomous vehicles (AVs). In this paper, we present LLaDA, a
simple yet powerful tool that enables human drivers and autonomous vehicles
alike to drive everywhere by adapting their tasks and motion plans to traffic
rules in new locations. LLaDA achieves this by leveraging the impressive
zero-shot generalizability of large language models (LLMs) in interpreting the
traffic rules in the local driver handbook. Through an extensive user study, we
show that LLaDA's instructions are useful in disambiguating in-the-wild
unexpected situations. We also demonstrate LLaDA's ability to adapt AV motion
planning policies in real-world datasets; LLaDA outperforms baseline planning
approaches on all our metrics. Please check our website for more details:
https://boyiliee.github.io/llada.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05933" title="Abstract">arXiv:2402.05933</a> [<a href="/pdf/2402.05933" title="Download PDF">pdf</a>, <a href="/format/2402.05933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time Series Diffusion in the Frequency Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Crabb%C3%A9%2C+J">Jonathan Crabb&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Huynh%2C+N">Nicolas Huynh</a>, 
<a href="/search/cs?searchtype=author&query=Stanczuk%2C+J">Jan Stanczuk</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Fourier analysis has been an instrumental tool in the development of signal
processing. This leads us to wonder whether this framework could similarly
benefit generative modelling. In this paper, we explore this question through
the scope of time series diffusion models. More specifically, we analyze
whether representing time series in the frequency domain is a useful inductive
bias for score-based diffusion models. By starting from the canonical SDE
formulation of diffusion in the time domain, we show that a dual diffusion
process occurs in the frequency domain with an important nuance: Brownian
motions are replaced by what we call mirrored Brownian motions, characterized
by mirror symmetries among their components. Building on this insight, we show
how to adapt the denoising score matching approach to implement diffusion
models in the frequency domain. This results in frequency diffusion models,
which we compare to canonical time diffusion models. Our empirical evaluation
on real-world datasets, covering various domains like healthcare and finance,
shows that frequency diffusion models better capture the training distribution
than time diffusion models. We explain this observation by showing that time
series from these datasets tend to be more localized in the frequency domain
than in the time domain, which makes them easier to model in the former case.
All our observations point towards impactful synergies between Fourier analysis
and diffusion models.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05934" title="Abstract">arXiv:2402.05934</a> [<a href="/pdf/2402.05934" title="Download PDF">pdf</a>, <a href="/format/2402.05934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Classifying Nodes in Graphs without GNNs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Winter%2C+D">Daniel Winter</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+N">Niv Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Hoshen%2C+Y">Yedid Hoshen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Graph neural networks (GNNs) are the dominant paradigm for classifying nodes
in a graph, but they have several undesirable attributes stemming from their
message passing architecture. Recently, distillation methods succeeded in
eliminating the use of GNNs at test time but they still require them during
training. We perform a careful analysis of the role that GNNs play in
distillation methods. This analysis leads us to propose a fully GNN-free
approach for node classification, not requiring them at train or test time. Our
method consists of three key components: smoothness constraints,
pseudo-labeling iterations and neighborhood-label histograms. Our final
approach can match the state-of-the-art accuracy on standard popular benchmarks
such as citation and co-purchase networks, without training a GNN.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05935" title="Abstract">arXiv:2402.05935</a> [<a href="/pdf/2402.05935" title="Download PDF">pdf</a>, <a href="/format/2402.05935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+P">Peng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Renrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chris Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+L">Longtian Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Siyuan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Weifeng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shitian Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+S">Shijie Geng</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Ziyi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+P">Peng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+W">Wenqi Shao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chao Xu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+C">Conghui He</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Junjun He</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+H">Hao Shao</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+P">Pan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongsheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and models are released at <a href="https://github.com/Alpha-VLLM/LLaMA2-Accessory">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)
series developed upon SPHINX. To improve the architecture and training
efficiency, we modify the SPHINX framework by removing redundant visual
encoders, bypassing fully-padded sub-images with skip tokens, and simplifying
multi-stage training into a one-stage all-in-one paradigm. To fully unleash the
potential of MLLMs, we assemble a comprehensive multi-domain and multimodal
dataset covering publicly available resources in language, vision, and
vision-language tasks. We further enrich this collection with our curated OCR
intensive and Set-of-Mark datasets, extending the diversity and generality. By
training over different base LLMs including TinyLlama1.1B, InternLM2-7B,
LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in
parameter size and multilingual capabilities. Comprehensive benchmarking
reveals a strong correlation between the multi-modal performance with the data
and parameter scales. Code and models are released at
https://github.com/Alpha-VLLM/LLaMA2-Accessory
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05937" title="Abstract">arXiv:2402.05937</a> [<a href="/pdf/2402.05937" title="Download PDF">pdf</a>, <a href="/format/2402.05937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstaGen: Enhancing Object Detection by Training on Synthetic Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Chengjian Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yujie Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Jie%2C+Z">Zequn Jie</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Weidi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lin Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Tech report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we introduce a novel paradigm to enhance the ability of object
detector, e.g., expanding categories or improving detection performance, by
training on synthetic dataset generated from diffusion models. Specifically, we
integrate an instance-level grounding head into a pre-trained, generative
diffusion model, to augment it with the ability of localising arbitrary
instances in the generated images. The grounding head is trained to align the
text embedding of category names with the regional visual feature of the
diffusion model, using supervision from an off-the-shelf object detector, and a
novel self-training scheme on (novel) categories not covered by the detector.
This enhanced version of diffusion model, termed as InstaGen, can serve as a
data synthesizer for object detection. We conduct thorough experiments to show
that, object detector can be enhanced while training on the synthetic dataset
from InstaGen, demonstrating superior performance over existing
state-of-the-art methods in open-vocabulary (+4.5 AP) and data-sparse (+1.2 to
5.2 AP) scenarios.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Fri,  9 Feb 24</h3>
<dl>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05137" title="Abstract">arXiv:2402.05137</a> (cross-list from astro-ph.IM) [<a href="/pdf/2402.05137" title="Download PDF">pdf</a>, <a href="/format/2402.05137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LtU-ILI: An All-in-One Framework for Implicit Inference in Astrophysics  and Cosmology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Ho%2C+M">Matthew Ho</a>, 
<a href="/search/astro-ph?searchtype=author&query=Bartlett%2C+D+J">Deaglan J. Bartlett</a>, 
<a href="/search/astro-ph?searchtype=author&query=Chartier%2C+N">Nicolas Chartier</a>, 
<a href="/search/astro-ph?searchtype=author&query=Cuesta-Lazaro%2C+C">Carolina Cuesta-Lazaro</a>, 
<a href="/search/astro-ph?searchtype=author&query=Ding%2C+S">Simon Ding</a>, 
<a href="/search/astro-ph?searchtype=author&query=Lapel%2C+A">Axel Lapel</a>, 
<a href="/search/astro-ph?searchtype=author&query=Lemos%2C+P">Pablo Lemos</a>, 
<a href="/search/astro-ph?searchtype=author&query=Lovell%2C+C+C">Christopher C. Lovell</a>, 
<a href="/search/astro-ph?searchtype=author&query=Makinen%2C+T+L">T. Lucas Makinen</a>, 
<a href="/search/astro-ph?searchtype=author&query=Modi%2C+C">Chirag Modi</a>, 
<a href="/search/astro-ph?searchtype=author&query=Pandya%2C+V">Viraj Pandya</a>, 
<a href="/search/astro-ph?searchtype=author&query=Pandey%2C+S">Shivam Pandey</a>, 
<a href="/search/astro-ph?searchtype=author&query=Perez%2C+L+A">Lucia A. Perez</a>, 
<a href="/search/astro-ph?searchtype=author&query=Wandelt%2C+B">Benjamin Wandelt</a>, 
<a href="/search/astro-ph?searchtype=author&query=Bryan%2C+G+L">Greg L. Bryan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 10 figures, submitted to the Open Journal of Astrophysics. Code available at <a href="https://github.com/maho3/ltu-ili">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Cosmology and Nongalactic Astrophysics (astro-ph.CO); Astrophysics of Galaxies (astro-ph.GA); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper presents the Learning the Universe Implicit Likelihood Inference
(LtU-ILI) pipeline, a codebase for rapid, user-friendly, and cutting-edge
machine learning (ML) inference in astrophysics and cosmology. The pipeline
includes software for implementing various neural architectures, training
schema, priors, and density estimators in a manner easily adaptable to any
research workflow. It includes comprehensive validation metrics to assess
posterior estimate coverage, enhancing the reliability of inferred results.
Additionally, the pipeline is easily parallelizable, designed for efficient
exploration of modeling hyperparameters. To demonstrate its capabilities, we
present real applications across a range of astrophysics and cosmology
problems, such as: estimating galaxy cluster masses from X-ray photometry;
inferring cosmology from matter power spectra and halo point clouds;
characterising progenitors in gravitational wave signals; capturing physical
dust parameters from galaxy colors and luminosities; and establishing
properties of semi-analytic models of galaxy formation. We also include
exhaustive benchmarking and comparisons of all implemented methods as well as
discussions about the challenges and pitfalls of ML inference in astronomical
sciences. All code and examples are made publicly available at
https://github.com/maho3/ltu-ili.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05141" title="Abstract">arXiv:2402.05141</a> (cross-list from math.OC) [<a href="/pdf/2402.05141" title="Download PDF">pdf</a>, <a href="/format/2402.05141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tensor Completion via Integer Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+X">Xin Chen</a>, 
<a href="/search/math?searchtype=author&query=Kudva%2C+S">Sukanya Kudva</a>, 
<a href="/search/math?searchtype=author&query=Dai%2C+Y">Yongzheng Dai</a>, 
<a href="/search/math?searchtype=author&query=Aswani%2C+A">Anil Aswani</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+C">Chen Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The main challenge with the tensor completion problem is a fundamental
tension between computation power and the information-theoretic sample
complexity rate. Past approaches either achieve the information-theoretic rate
but lack practical algorithms to compute the corresponding solution, or have
polynomial-time algorithms that require an exponentially-larger number of
samples for low estimation error. This paper develops a novel tensor completion
algorithm that resolves this tension by achieving both provable convergence (in
numerical tolerance) in a linear number of oracle steps and the
information-theoretic rate. Our approach formulates tensor completion as a
convex optimization problem constrained using a gauge-based tensor norm, which
is defined in a way that allows the use of integer linear optimization to solve
linear separation problems over the unit-ball in this new norm. Adaptations
based on this insight are incorporated into a Frank-Wolfe variant to build our
algorithm. We show our algorithm scales-well using numerical experiments on
tensors with up to ten million entries.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05155" title="Abstract">arXiv:2402.05155</a> (cross-list from math.OC) [<a href="/pdf/2402.05155" title="Download PDF">pdf</a>, <a href="/format/2402.05155" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-convergence to global minimizers for Adam and stochastic gradient  descent optimization and constructions of local minimizers in the training of  artificial neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jentzen%2C+A">Arnulf Jentzen</a>, 
<a href="/search/math?searchtype=author&query=Riekert%2C+A">Adrian Riekert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Stochastic gradient descent (SGD) optimization methods such as the plain
vanilla SGD method and the popular Adam optimizer are nowadays the method of
choice in the training of artificial neural networks (ANNs). Despite the
remarkable success of SGD methods in the ANN training in numerical simulations,
it remains in essentially all practical relevant scenarios an open problem to
rigorously explain why SGD methods seem to succeed to train ANNs. In
particular, in most practically relevant supervised learning problems, it seems
that SGD methods do with high probability not converge to global minimizers in
the optimization landscape of the ANN training problem. Nevertheless, it
remains an open problem of research to disprove the convergence of SGD methods
to global minimizers. In this work we solve this research problem in the
situation of shallow ANNs with the rectified linear unit (ReLU) and related
activations with the standard mean square error loss by disproving in the
training of such ANNs that SGD methods (such as the plain vanilla SGD, the
momentum SGD, the AdaGrad, the RMSprop, and the Adam optimizers) can find a
global minimizer with high probability. Even stronger, we reveal in the
training of such ANNs that SGD methods do with high probability fail to
converge to global minimizers in the optimization landscape. The findings of
this work do, however, not disprove that SGD methods succeed to train ANNs
since they do not exclude the possibility that SGD methods find good local
minimizers whose risk values are close to the risk values of the global
minimizers. In this context, another key contribution of this work is to
establish the existence of a hierarchical structure of local minimizers with
distinct risk values in the optimization landscape of ANN training problems
with ReLU and related activations.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05176" title="Abstract">arXiv:2402.05176</a> (cross-list from astro-ph.IM) [<a href="/pdf/2402.05176" title="Download PDF">pdf</a>, <a href="/format/2402.05176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> cecilia: A Machine Learning-Based Pipeline for Measuring Metal  Abundances of Helium-rich Polluted White Dwarfs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Badenas-Agusti%2C+M">M. Badenas-Agusti</a>, 
<a href="/search/astro-ph?searchtype=author&query=Via%C3%B1a%2C+J">J. Via&#xf1;a</a>, 
<a href="/search/astro-ph?searchtype=author&query=Vanderburg%2C+A">A. Vanderburg</a>, 
<a href="/search/astro-ph?searchtype=author&query=Blouin%2C+S">S. Blouin</a>, 
<a href="/search/astro-ph?searchtype=author&query=Dufour%2C+P">P. Dufour</a>, 
<a href="/search/astro-ph?searchtype=author&query=Xu%2C+S">S. Xu</a>, 
<a href="/search/astro-ph?searchtype=author&query=Sha%2C+L">L. Sha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 16 figures, 5 tables. Accepted for publication in MNRAS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Earth and Planetary Astrophysics (astro-ph.EP); Solar and Stellar Astrophysics (astro-ph.SR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Over the past several decades, conventional spectral analysis techniques of
polluted white dwarfs have become powerful tools to learn about the geology and
chemistry of extrasolar bodies. Despite their proven capabilities and extensive
legacy of scientific discoveries, these techniques are however still limited by
their manual, time-intensive, and iterative nature. As a result, they are
susceptible to human errors and are difficult to scale up to population-wide
studies of metal pollution. This paper seeks to address this problem by
presenting cecilia, the first Machine Learning (ML)-powered spectral modeling
code designed to measure the metal abundances of intermediate-temperature
(10,000$\leq T_{\rm eff} \leq$20,000 K), Helium-rich polluted white dwarfs.
Trained with more than 22,000 randomly drawn atmosphere models and stellar
parameters, our pipeline aims to overcome the limitations of classical methods
by replacing the generation of synthetic spectra from computationally expensive
codes and uniformly spaced model grids, with a fast, automated, and efficient
neural-network-based interpolator. More specifically, cecilia combines
state-of-the-art atmosphere models, powerful artificial intelligence tools, and
robust statistical techniques to rapidly generate synthetic spectra of polluted
white dwarfs in high-dimensional space, and enable accurate ($\lesssim$0.1 dex)
and simultaneous measurements of 14 stellar parameters -- including 11
elemental abundances -- from real spectroscopic observations. As massively
multiplexed astronomical surveys begin scientific operations, cecilia's
performance has the potential to unlock large-scale studies of extrasolar
geochemistry and propel the field of white dwarf science into the era of Big
Data. In doing so, we aspire to uncover new statistical insights that were
previously impractical with traditional white dwarf characterisation
techniques.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05187" title="Abstract">arXiv:2402.05187</a> (cross-list from stat.ML) [<a href="/pdf/2402.05187" title="Download PDF">pdf</a>, <a href="/format/2402.05187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta-learning the mirror map in policy mirror descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Alfano%2C+C">Carlo Alfano</a>, 
<a href="/search/stat?searchtype=author&query=Towers%2C+S">Sebastian Towers</a>, 
<a href="/search/stat?searchtype=author&query=Sapora%2C+S">Silvia Sapora</a>, 
<a href="/search/stat?searchtype=author&query=Lu%2C+C">Chris Lu</a>, 
<a href="/search/stat?searchtype=author&query=Rebeschini%2C+P">Patrick Rebeschini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">Policy Mirror Descent (PMD) is a popular framework in reinforcement learning,
serving as a unifying perspective that encompasses numerous algorithms. These
algorithms are derived through the selection of a mirror map and enjoy
finite-time convergence guarantees. Despite its popularity, the exploration of
PMD's full potential is limited, with the majority of research focusing on a
particular mirror map -- namely, the negative entropy -- which gives rise to
the renowned Natural Policy Gradient (NPG) method. It remains uncertain from
existing theoretical studies whether the choice of mirror map significantly
influences PMD's efficacy. In our work, we conduct empirical investigations to
show that the conventional mirror map choice (NPG) often yields
less-than-optimal outcomes across several standard benchmark environments. By
applying a meta-learning approach, we identify more efficient mirror maps that
enhance performance, both on average and in terms of best performance achieved
along the training trajectory. We analyze the characteristics of these learned
mirror maps and reveal shared traits among certain settings. Our results
suggest that mirror maps have the potential to be adaptable across various
environments, raising questions about how to best match a mirror map to an
environment's structure and characteristics.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05193" title="Abstract">arXiv:2402.05193</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2402.05193" title="Download PDF">pdf</a>, <a href="/format/2402.05193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JAX-Fluids 2.0: Towards HPC for Differentiable CFD of Compressible  Two-phase Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Bezgin%2C+D+A">Deniz A. Bezgin</a>, 
<a href="/search/physics?searchtype=author&query=Buhendwa%2C+A+B">Aaron B. Buhendwa</a>, 
<a href="/search/physics?searchtype=author&query=Adams%2C+N+A">Nikolaus A. Adams</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
<p class="mathjax">In our effort to facilitate machine learning-assisted computational fluid
dynamics (CFD), we introduce the second iteration of JAX-Fluids. JAX-Fluids is
a Python-based fully-differentiable CFD solver designed for compressible
single- and two-phase flows. In this work, the first version is extended to
incorporate high-performance computing (HPC) capabilities. We introduce a
parallelization strategy utilizing JAX primitive operations that scales
efficiently on GPU (up to 512 NVIDIA A100 graphics cards) and TPU (up to 1024
TPU v3 cores) HPC systems. We further demonstrate the stable parallel
computation of automatic differentiation gradients across extended integration
trajectories. The new code version offers enhanced two-phase flow modeling
capabilities. In particular, a five-equation diffuse-interface model is
incorporated which complements the level-set sharp-interface model. Additional
algorithmic improvements include positivity-preserving limiters for increased
robustness, support for stretched Cartesian meshes, refactored I/O handling,
comprehensive post-processing routines, and an updated list of state-of-the-art
high-order numerical discretization schemes. We verify newly added numerical
models by showcasing simulation results for single- and two-phase flows,
including turbulent boundary layer and channel flows, air-helium shock bubble
interactions, and air-water shock drop interactions.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05200" title="Abstract">arXiv:2402.05200</a> (cross-list from cond-mat.mtrl-sci) [<a href="/pdf/2402.05200" title="Download PDF">pdf</a>, <a href="/format/2402.05200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are LLMs Ready for Real-World Materials Discovery?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Miret%2C+S">Santiago Miret</a>, 
<a href="/search/cond-mat?searchtype=author&query=Krishnan%2C+N+M+A">N M Anoop Krishnan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) create exciting possibilities for powerful
language processing tools to accelerate research in materials science. While
LLMs have great potential to accelerate materials understanding and discovery,
they currently fall short in being practical materials science tools. In this
position paper, we show relevant failure cases of LLMs in materials science
that reveal current limitations of LLMs related to comprehending and reasoning
over complex, interconnected materials science knowledge. Given those
shortcomings, we outline a framework for developing Materials Science LLMs
(MatSci-LLMs) that are grounded in materials science knowledge and hypothesis
generation followed by hypothesis testing. The path to attaining performant
MatSci-LLMs rests in large part on building high-quality, multi-modal datasets
sourced from scientific literature where various information extraction
challenges persist. As such, we describe key materials science information
extraction challenges which need to be overcome in order to build large-scale,
multi-modal datasets that capture valuable materials science knowledge.
Finally, we outline a roadmap for applying future MatSci-LLMs for real-world
materials discovery via: 1. Automated Knowledge Base Generation; 2. Automated
In-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials
Laboratories.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05209" title="Abstract">arXiv:2402.05209</a> (cross-list from stat.ME) [<a href="/pdf/2402.05209" title="Download PDF">pdf</a>, <a href="/ps/2402.05209" title="Download PostScript">ps</a>, <a href="/format/2402.05209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic modeling of Random Access Memories reset transitions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Aguilera-Morillo%2C+M+C">M Carmen Aguilera-Morillo</a>, 
<a href="/search/stat?searchtype=author&query=Aguilera%2C+A+M">Ana M Aguilera</a>, 
<a href="/search/stat?searchtype=author&query=Jim%C3%A9nez-Molinos%2C+F">Francisco Jim&#xe9;nez-Molinos</a>, 
<a href="/search/stat?searchtype=author&query=Rold%C3%A1n%2C+J+B">Juan B Rold&#xe1;n</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Mathematics and Computers in Simulation, 159, 197-209, 2019
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Emerging Technologies (cs.ET); Statistics Theory (math.ST)

</div>
<p class="mathjax">Resistive Random Access Memories (RRAMs) are being studied by the industry
and academia because it is widely accepted that they are promising candidates
for the next generation of high density nonvolatile memories. Taking into
account the stochastic nature of mechanisms behind resistive switching, a new
technique based on the use of functional data analysis has been developed to
accurately model resistive memory device characteristics. Functional principal
component analysis (FPCA) based on Karhunen-Loeve expansion is applied to
obtain an orthogonal decomposition of the reset process in terms of
uncorrelated scalar random variables. Then, the device current has been
accurately described making use of just one variable presenting a modeling
approach that can be very attractive from the circuit simulation viewpoint. The
new method allows a comprehensive description of the stochastic variability of
these devices by introducing a probability distribution that allows the
simulation of the main parameter that is employed for the model implementation.
A rigorous description of the mathematical theory behind the technique is given
and its application for a broad set of experimental measurements is explained.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05210" title="Abstract">arXiv:2402.05210</a> (cross-list from eess.IV) [<a href="/pdf/2402.05210" title="Download PDF">pdf</a>, <a href="/format/2402.05210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anatomically-Controllable Medical Image Generation with  Segmentation-Guided Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Konz%2C+N">Nicholas Konz</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Y">Yuwen Chen</a>, 
<a href="/search/eess?searchtype=author&query=Dong%2C+H">Haoyu Dong</a>, 
<a href="/search/eess?searchtype=author&query=Mazurowski%2C+M+A">Maciej A. Mazurowski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and synthetic dataset: <a href="https://github.com/mazurowski-lab/segmentation-guided-diffusion">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Diffusion models have enabled remarkably high-quality medical image
generation, which can help mitigate the expenses of acquiring and annotating
new images by supplementing small or imbalanced datasets, along with other
applications. However, these are hampered by the challenge of enforcing global
anatomical realism in generated images. To this end, we propose a diffusion
model for anatomically-controlled medical image generation. Our model follows a
multi-class anatomical segmentation mask at each sampling step and incorporates
a \textit{random mask ablation} training algorithm, to enable conditioning on a
selected combination of anatomical constraints while allowing flexibility in
other anatomical areas. This also improves the network's learning of anatomical
realism for the completely unconditional (unconstrained generation) case.
Comparative evaluation on breast MRI and abdominal/neck-to-pelvis CT datasets
demonstrates superior anatomical realism and input mask faithfulness over
state-of-the-art models. We also offer an accessible codebase and release a
dataset of generated paired breast MRIs. Our approach facilitates diverse
applications, including pre-registered image generation, counterfactual
scenarios, and others.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05218" title="Abstract">arXiv:2402.05218</a> (cross-list from eess.IV) [<a href="/pdf/2402.05218" title="Download PDF">pdf</a>, <a href="/format/2402.05218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-calibrated convolution towards glioma segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Salvagnini%2C+F+C+R">Felipe C. R. Salvagnini</a>, 
<a href="/search/eess?searchtype=author&query=Barbosa%2C+G+O">Gerson O. Barbosa</a>, 
<a href="/search/eess?searchtype=author&query=Falcao%2C+A+X">Alexandre X. Falcao</a>, 
<a href="/search/eess?searchtype=author&query=Santos%2C+C+A+N">Cid A. N. Santos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Accurate brain tumor segmentation in the early stages of the disease is
crucial for the treatment's effectiveness, avoiding exhaustive visual
inspection of a qualified specialist on 3D MR brain images of multiple
protocols (e.g., T1, T2, T2-FLAIR, T1-Gd). Several networks exist for Glioma
segmentation, being nnU-Net one of the best. In this work, we evaluate
self-calibrated convolutions in different parts of the nnU-Net network to
demonstrate that self-calibrated modules in skip connections can significantly
improve the enhanced-tumor and tumor-core segmentation accuracy while
preserving the wholetumor segmentation accuracy.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05220" title="Abstract">arXiv:2402.05220</a> (cross-list from stat.ML) [<a href="/pdf/2402.05220" title="Download PDF">pdf</a>, <a href="/format/2402.05220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Parameter Estimation in Deviated Gaussian Mixture of Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Nguyen%2C+H">Huy Nguyen</a>, 
<a href="/search/stat?searchtype=author&query=Nguyen%2C+K">Khai Nguyen</a>, 
<a href="/search/stat?searchtype=author&query=Ho%2C+N">Nhat Ho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We consider the parameter estimation problem in the deviated Gaussian mixture
of experts in which the data are generated from $(1 - \lambda^{\ast}) g_0(Y|
X)+ \lambda^{\ast} \sum_{i = 1}^{k_{\ast}} p_{i}^{\ast}
f(Y|(a_{i}^{\ast})^{\top}X+b_i^{\ast},\sigma_{i}^{\ast})$, where $X, Y$ are
respectively a covariate vector and a response variable, $g_{0}(Y|X)$ is a
known function, $\lambda^{\ast} \in [0, 1]$ is true but unknown mixing
proportion, and $(p_{i}^{\ast}, a_{i}^{\ast}, b_{i}^{\ast}, \sigma_{i}^{\ast})$
for $1 \leq i \leq k^{\ast}$ are unknown parameters of the Gaussian mixture of
experts. This problem arises from the goodness-of-fit test when we would like
to test whether the data are generated from $g_{0}(Y|X)$ (null hypothesis) or
they are generated from the whole mixture (alternative hypothesis). Based on
the algebraic structure of the expert functions and the distinguishability
between $g_0$ and the mixture part, we construct novel Voronoi-based loss
functions to capture the convergence rates of maximum likelihood estimation
(MLE) for our models. We further demonstrate that our proposed loss functions
characterize the local convergence rates of parameter estimation more
accurately than the generalized Wasserstein, a loss function being commonly
used for estimating parameters in the Gaussian mixture of experts.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05239" title="Abstract">arXiv:2402.05239</a> (cross-list from quant-ph) [<a href="/pdf/2402.05239" title="Download PDF">pdf</a>, <a href="/format/2402.05239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient approximate unitary designs from random Pauli rotations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Haah%2C+J">Jeongwan Haah</a>, 
<a href="/search/quant-ph?searchtype=author&query=Liu%2C+Y">Yunchao Liu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Tan%2C+X">Xinyu Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">We construct random walks on simple Lie groups that quickly converge to the
Haar measure for all moments up to order $t$. Specifically, a step of the walk
on the unitary or orthognoal group of dimension $2^{\mathsf n}$ is a random
Pauli rotation $e^{\mathrm i \theta P /2}$. The spectral gap of this random
walk is shown to be $\Omega(1/t)$, which coincides with the best previously
known bound for a random walk on the permutation group on $\{0,1\}^{\mathsf
n}$. This implies that the walk gives an $\varepsilon$-approximate unitary
$t$-design in depth $O(\mathsf n t^2 + t \log 1/\varepsilon)d$ where $d=O(\log
\mathsf n)$ is the circuit depth to implement $e^{\mathrm i \theta P /2}$. Our
simple proof uses quadratic Casimir operators of Lie algebras.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05271" title="Abstract">arXiv:2402.05271</a> (cross-list from stat.ML) [<a href="/pdf/2402.05271" title="Download PDF">pdf</a>, <a href="/format/2402.05271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradient descent induces alignment between weights and the empirical NTK  for deep non-linear networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Beaglehole%2C+D">Daniel Beaglehole</a>, 
<a href="/search/stat?searchtype=author&query=Mitliagkas%2C+I">Ioannis Mitliagkas</a>, 
<a href="/search/stat?searchtype=author&query=Agarwala%2C+A">Atish Agarwala</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Understanding the mechanisms through which neural networks extract statistics
from input-label pairs is one of the most important unsolved problems in
supervised learning. Prior works have identified that the gram matrices of the
weights in trained neural networks of general architectures are proportional to
the average gradient outer product of the model, in a statement known as the
Neural Feature Ansatz (NFA). However, the reason these quantities become
correlated during training is poorly understood. In this work, we explain the
emergence of this correlation. We identify that the NFA is equivalent to
alignment between the left singular structure of the weight matrices and a
significant component of the empirical neural tangent kernels associated with
those weights. We establish that the NFA introduced in prior works is driven by
a centered NFA that isolates this alignment. We show that the speed of NFA
development can be predicted analytically at early training times in terms of
simple statistics of the inputs and labels. Finally, we introduce a simple
intervention to increase NFA correlation at any given layer, which dramatically
improves the quality of features learned.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05276" title="Abstract">arXiv:2402.05276</a> (cross-list from econ.TH) [<a href="/pdf/2402.05276" title="Download PDF">pdf</a>, <a href="/ps/2402.05276" title="Download PostScript">ps</a>, <a href="/format/2402.05276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spreading Information via Social Networks: An Irrelevance Result
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Awaya%2C+Y">Yu Awaya</a>, 
<a href="/search/econ?searchtype=author&query=Krishna%2C+V">Vijay Krishna</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">An informed planner wishes to spread information among a group of agents in
order to induce efficient coordination -- say the adoption of a new technology
with positive externalities. The agents are connected via a social network. The
planner informs a seed and then the information spreads via the network. While
the structure of the network affects the rate of diffusion, we show that the
rate of adoption is the same for all acyclic networks.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05330" title="Abstract">arXiv:2402.05330</a> (cross-list from stat.ML) [<a href="/pdf/2402.05330" title="Download PDF">pdf</a>, <a href="/format/2402.05330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Classification under Nuisance Parameters and Generalized Label Shift in  Likelihood-Free Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Masserano%2C+L">Luca Masserano</a>, 
<a href="/search/stat?searchtype=author&query=Shen%2C+A">Alex Shen</a>, 
<a href="/search/stat?searchtype=author&query=Doro%2C+M">Michele Doro</a>, 
<a href="/search/stat?searchtype=author&query=Dorigo%2C+T">Tommaso Dorigo</a>, 
<a href="/search/stat?searchtype=author&query=Izbicki%2C+R">Rafael Izbicki</a>, 
<a href="/search/stat?searchtype=author&query=Lee%2C+A+B">Ann B. Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">An open scientific challenge is how to classify events with reliable measures
of uncertainty, when we have a mechanistic model of the data-generating process
but the distribution over both labels and latent nuisance parameters is
different between train and target data. We refer to this type of
distributional shift as generalized label shift (GLS). Direct classification
using observed data $\mathbf{X}$ as covariates leads to biased predictions and
invalid uncertainty estimates of labels $Y$. We overcome these biases by
proposing a new method for robust uncertainty quantification that casts
classification as a hypothesis testing problem under nuisance parameters. The
key idea is to estimate the classifier's receiver operating characteristic
(ROC) across the entire nuisance parameter space, which allows us to devise
cutoffs that are invariant under GLS. Our method effectively endows a
pre-trained classifier with domain adaptation capabilities and returns valid
prediction sets while maintaining high power. We demonstrate its performance on
two challenging scientific problems in biology and astroparticle physics with
data from realistic mechanistic models.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05368" title="Abstract">arXiv:2402.05368</a> (cross-list from physics.soc-ph) [<a href="/pdf/2402.05368" title="Download PDF">pdf</a>, <a href="/format/2402.05368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bounded-Confidence Models of Opinion Dynamics with Neighborhood Effects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Krishnagopal%2C+S">Sanjukta Krishnagopal</a>, 
<a href="/search/physics?searchtype=author&query=Porter%2C+M+A">Mason A. Porter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 6 figures; the abstract on the arXiv page is abbreviated in parts because of character-count limitations
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">As people's opinions change, their social networks typically coevolve with
them. People are often more susceptible to influence by people with similar
opinions than by people with dissimilar opinions. In a bounded-confidence model
(BCM) of opinion dynamics, interacting individuals influence each other through
dyadic influence if and only if their opinions are sufficiently similar to each
other. We introduce `neighborhood BCMs' (NBCMs) that include both the usual
dyadic influence and a transitive influence, which models the effect of friends
of a friend when determining whether or not an interaction with a friend
influences an individual. In this transitive influence, an individual's opinion
is influenced by a neighbor when, on average, the opinions of the neighbor's
neighbors are sufficiently similar to their own opinion. We formulate
neighborhood Deffuant--Weisbuch (NDW) and neighborhood Hegselmann--Krause (NHK)
BCMs. We simulate our NDW model on time-independent networks and observe
interesting opinion states that cannot occur in an associated baseline DW
model. We also simulate our NDW model on adaptive networks that coevolve with
opinions by changing its structure through `transitive homophily'. An
individual that breaks a tie to one of its neighbors and then rewires that tie
to a new individual, with a preference for individuals with a mean neighbor
opinion that is closer to that individual's opinion. We explore how the
qualitative opinion dynamics and network properties of our time-independent and
adaptive NDWM models change as we adjust the relative proportions of dyadic and
transitive influence. Finally, we study a two-layer opinion--disease model in
which we couple our NDW model with disease spread through a shared adaptive
network that can change both on the opinion layer and on the disease layer and
we examine how the opinion dynamics affect disease spread.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05372" title="Abstract">arXiv:2402.05372</a> (cross-list from physics.flu-dyn) [<a href="/pdf/2402.05372" title="Download PDF">pdf</a>, <a href="/format/2402.05372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reduced-order modeling of unsteady fluid flow using neural network  ensembles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Halder%2C+R">Rakesh Halder</a>, 
<a href="/search/physics?searchtype=author&query=Ataei%2C+M">Mohammadmehdi Ataei</a>, 
<a href="/search/physics?searchtype=author&query=Salehipour%2C+H">Hesam Salehipour</a>, 
<a href="/search/physics?searchtype=author&query=Fidkowski%2C+K">Krzysztof Fidkowski</a>, 
<a href="/search/physics?searchtype=author&query=Maki%2C+K">Kevin Maki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The use of deep learning has become increasingly popular in reduced-order
models (ROMs) to obtain low-dimensional representations of full-order models.
Convolutional autoencoders (CAEs) are often used to this end as they are adept
at handling data that are spatially distributed, including solutions to partial
differential equations. When applied to unsteady physics problems, ROMs also
require a model for time-series prediction of the low-dimensional latent
variables. Long short-term memory (LSTM) networks, a type of recurrent neural
network useful for modeling sequential data, are frequently employed in
data-driven ROMs for autoregressive time-series prediction. When making
predictions at unseen design points over long time horizons, error propagation
is a frequently encountered issue, where errors made early on can compound over
time and lead to large inaccuracies. In this work, we propose using bagging, a
commonly used ensemble learning technique, to develop a fully data-driven ROM
framework referred to as the CAE-eLSTM ROM that uses CAEs for spatial
reconstruction of the full-order model and LSTM ensembles for time-series
prediction. When applied to two unsteady fluid dynamics problems, our results
show that the presented framework effectively reduces error propagation and
leads to more accurate time-series prediction of latent variables at unseen
points.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05373" title="Abstract">arXiv:2402.05373</a> (cross-list from eess.IV) [<a href="/pdf/2402.05373" title="Download PDF">pdf</a>, <a href="/format/2402.05373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unleashing the Infinity Power of Geometry: A Novel Geometry-Aware  Transformer (GOAT) for Whole Slide Histopathology Image Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liu%2C+M">Mingxin Liu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yunzan Liu</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+P">Pengbo Xu</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+J">Jiquan Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures. Accepted by 21st IEEE International Symposium on Biomedical Imaging (ISBI 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The histopathology analysis is of great significance for the diagnosis and
prognosis of cancers, however, it has great challenges due to the enormous
heterogeneity of gigapixel whole slide images (WSIs) and the intricate
representation of pathological features. However, recent methods have not
adequately exploited geometrical representation in WSIs which is significant in
disease diagnosis. Therefore, we proposed a novel weakly-supervised framework,
Geometry-Aware Transformer (GOAT), in which we urge the model to pay attention
to the geometric characteristics within the tumor microenvironment which often
serve as potent indicators. In addition, a context-aware attention mechanism is
designed to extract and enhance the morphological features within WSIs.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05378" title="Abstract">arXiv:2402.05378</a> (cross-list from eess.SP) [<a href="/pdf/2402.05378" title="Download PDF">pdf</a>, <a href="/format/2402.05378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Networks for Physical-Layer Security in Multi-User  Flexible-Duplex Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Perera%2C+T">Tharaka Perera</a>, 
<a href="/search/eess?searchtype=author&query=Atapattu%2C+S">Saman Atapattu</a>, 
<a href="/search/eess?searchtype=author&query=Fang%2C+Y">Yuting Fang</a>, 
<a href="/search/eess?searchtype=author&query=Evans%2C+J">Jamie Evans</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper explores Physical-Layer Security (PLS) in Flexible Duplex (FlexD)
networks, considering scenarios involving eavesdroppers. Our investigation
revolves around the intricacies of the sum secrecy rate maximization problem,
particularly when faced with coordinated and distributed eavesdroppers
employing a Minimum Mean Square Error (MMSE) receiver. Our contributions
include an iterative classical optimization solution and an unsupervised
learning strategy based on Graph Neural Networks (GNNs). To the best of our
knowledge, this work marks the initial exploration of GNNs for PLS
applications. Additionally, we extend the GNN approach to address the absence
of eavesdroppers' channel knowledge. Extensive numerical simulations highlight
FlexD's superiority over Half-Duplex (HD) communications and the GNN approach's
superiority over the classical method in both performance and time complexity.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05404" title="Abstract">arXiv:2402.05404</a> (cross-list from nlin.CG) [<a href="/pdf/2402.05404" title="Download PDF">pdf</a>, <a href="/format/2402.05404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Computation for Reversibility of Arbitrary One-dimensional Finite  Cellular Automata Becomes Efficient
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/nlin?searchtype=author&query=Wang%2C+C">Chen Wang</a>, 
<a href="/search/nlin?searchtype=author&query=Ma%2C+J">Junchi Ma</a>, 
<a href="/search/nlin?searchtype=author&query=Wang%2C+C">Chao Wang</a>, 
<a href="/search/nlin?searchtype=author&query=Lin%2C+D">Defu Lin</a>, 
<a href="/search/nlin?searchtype=author&query=Chen%2C+W">Weilin Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cellular Automata and Lattice Gases (nlin.CG)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">In this paper, we completely solve the reversibility of one-dimensional
finite cellular automata (FCA). This means that we will have an efficient
method to determine the reversibility of any FCA with all numbers (n) of cells.
The complexity of this algorithm is independent of n. We perform calculations
on two new kinds of graphs and discover that the reversibility of any FCA
exhibits periodicity as n increases. We successfully provide a method to
compute the reversibility sequence that encompasses the reversibility of FCA
with any number of cells. Additionally, the calculations in this paper are
applicable to FCA with various types of boundaries.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05422" title="Abstract">arXiv:2402.05422</a> (cross-list from eess.IV) [<a href="/pdf/2402.05422" title="Download PDF">pdf</a>, <a href="/format/2402.05422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory-efficient deep end-to-end posterior network (DEEPEN) for inverse  problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chand%2C+J+R">Jyothi Rikhab Chand</a>, 
<a href="/search/eess?searchtype=author&query=Jacob%2C+M">Mathews Jacob</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">End-to-End (E2E) unrolled optimization frameworks show promise for Magnetic
Resonance (MR) image recovery, but suffer from high memory usage during
training. In addition, these deterministic approaches do not offer
opportunities for sampling from the posterior distribution. In this paper, we
introduce a memory-efficient approach for E2E learning of the posterior
distribution. We represent this distribution as the combination of a
data-consistency-induced likelihood term and an energy model for the prior,
parameterized by a Convolutional Neural Network (CNN). The CNN weights are
learned from training data in an E2E fashion using maximum likelihood
optimization. The learned model enables the recovery of images from
undersampled measurements using the Maximum A Posteriori (MAP) optimization. In
addition, the posterior model can be sampled to derive uncertainty maps about
the reconstruction. Experiments on parallel MR image reconstruction show that
our approach performs comparable to the memory-intensive E2E unrolled
algorithm, performs better than its memory-efficient counterpart, and can
provide uncertainty maps. Our framework paves the way towards MR image
reconstruction in 3D and higher dimensions
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05474" title="Abstract">arXiv:2402.05474</a> (cross-list from quant-ph) [<a href="/pdf/2402.05474" title="Download PDF">pdf</a>, <a href="/format/2402.05474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resources of the Quantum World
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Gour%2C+G">Gilad Gour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 956 Pages (including appendices), Preliminary Version, Feedback and comments are most welcome especially typos, errors, and missing references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT); Mathematical Physics (math-ph)

</div>
<p class="mathjax">This book delves into the burgeoning field of quantum resource theories, a
novel and vibrant area of research within quantum information science that
seeks to unify diverse quantum phenomena under a single framework. By
recognizing various attributes of physical systems as "resources," this
approach offers a fresh perspective on quantum phenomena, transforming our
understanding and application of concepts such as quantum entanglement,
coherence, and more. With a focus on the pedagogical, the book aims to equip
readers with the advanced mathematical tools and physical principles needed to
navigate and contribute to this rapidly evolving field. It covers a wide range
of topics, from the foundational aspects of quantum mechanics and quantum
information to detailed explorations of specific resource theories, including
entanglement, asymmetry, and thermodynamics. Through rigorous mathematical
exposition and a unique axiomatic approach, the book provides deep insights
into the operational and conceptual frameworks that underpin quantum resource
theories, making it an invaluable resource for graduate students, early-career
researchers, and anyone interested in the cutting-edge developments in quantum
information science.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05482" title="Abstract">arXiv:2402.05482</a> (cross-list from eess.SP) [<a href="/pdf/2402.05482" title="Download PDF">pdf</a>, <a href="/format/2402.05482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Non-Intrusive Neural Quality Assessment Model for Surface  Electromyography Signals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lee%2C+C">Cho-Yuan Lee</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+K">Kuan-Chen Wang</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+K">Kai-Chun Liu</a>, 
<a href="/search/eess?searchtype=author&query=Lu%2C+X">Xugang Lu</a>, 
<a href="/search/eess?searchtype=author&query=Yeh%2C+P">Ping-Chen Yeh</a>, 
<a href="/search/eess?searchtype=author&query=Tsao%2C+Y">Yu Tsao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In practical scenarios involving the measurement of surface electromyography
(sEMG) in muscles, particularly those areas near the heart, one of the primary
sources of contamination is the presence of electrocardiogram (ECG) signals. To
assess the quality of real-world sEMG data more effectively, this study
proposes QASE-net, a new non-intrusive model that predicts the SNR of sEMG
signals. QASE-net combines CNN-BLSTM with attention mechanisms and follows an
end-to-end training strategy. Our experimental framework utilizes real-world
sEMG and ECG data from two open-access databases, the Non-Invasive Adaptive
Prosthetics Database and the MIT-BIH Normal Sinus Rhythm Database,
respectively. The experimental results demonstrate the superiority of QASE-net
over the previous assessment model, exhibiting significantly reduced prediction
errors and notably higher linear correlations with the ground truth. These
findings show the potential of QASE-net to substantially enhance the
reliability and precision of sEMG quality assessment in practical applications.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05501" title="Abstract">arXiv:2402.05501</a> (cross-list from math.OC) [<a href="/pdf/2402.05501" title="Download PDF">pdf</a>, <a href="/format/2402.05501" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning Augmented Branch and Bound for Mixed Integer Linear  Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Scavuzzo%2C+L">Lara Scavuzzo</a>, 
<a href="/search/math?searchtype=author&query=Aardal%2C+K">Karen Aardal</a>, 
<a href="/search/math?searchtype=author&query=Lodi%2C+A">Andrea Lodi</a>, 
<a href="/search/math?searchtype=author&query=Yorke-Smith%2C+N">Neil Yorke-Smith</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Mixed Integer Linear Programming (MILP) is a pillar of mathematical
optimization that offers a powerful modeling language for a wide range of
applications. During the past decades, enormous algorithmic progress has been
made in solving MILPs, and many commercial and academic software packages
exist. Nevertheless, the availability of data, both from problem instances and
from solvers, and the desire to solve new problems and larger (real-life)
instances, trigger the need for continuing algorithmic development. MILP
solvers use branch and bound as their main component. In recent years, there
has been an explosive development in the use of machine learning algorithms for
enhancing all main tasks involved in the branch-and-bound algorithm, such as
primal heuristics, branching, cutting planes, node selection and solver
configuration decisions. This paper presents a survey of such approaches,
addressing the vision of integration of machine learning and mathematical
optimization as complementary technologies, and how this integration can
benefit MILP solving. In particular, we give detailed attention to machine
learning algorithms that automatically optimize some metric of branch-and-bound
efficiency. We also address how to represent MILPs in the context of applying
learning algorithms, MILP benchmarks and software.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05543" title="Abstract">arXiv:2402.05543</a> (cross-list from q-bio.GN) [<a href="/pdf/2402.05543" title="Download PDF">pdf</a>, <a href="/format/2402.05543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine learning applied to omics data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Calvi%C3%B1o%2C+A">Aida Calvi&#xf1;o</a>, 
<a href="/search/q-bio?searchtype=author&query=Moreno-Ribera%2C+A">Almudena Moreno-Ribera</a>, 
<a href="/search/q-bio?searchtype=author&query=Pineda%2C+S">Silvia Pineda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Part of the book "Statistical Methods at the Forefront of Biomedical Advances" published by Springer Cham
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Machine Learning (cs.LG); Applications (stat.AP)

</div>
<p class="mathjax">In this chapter we illustrate the use of some Machine Learning techniques in
the context of omics data. More precisely, we review and evaluate the use of
Random Forest and Penalized Multinomial Logistic Regression for integrative
analysis of genomics and immunomics in pancreatic cancer. Furthermore, we
propose the use of association rules with predictive purposes to overcome the
low predictive power of the previously mentioned models. Finally, we apply the
reviewed methods to a real data set from TCGA made of 107 tumoral pancreatic
samples and 117,486 germline SNPs, showing the good performance of the proposed
methods to predict the immunological infiltration in pancreatic cancer.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05552" title="Abstract">arXiv:2402.05552</a> (cross-list from quant-ph) [<a href="/pdf/2402.05552" title="Download PDF">pdf</a>, <a href="/ps/2402.05552" title="Download PostScript">ps</a>, <a href="/format/2402.05552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning quantum Hamiltonians at any temperature in polynomial time with  Chebyshev and bit complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Wodecki%2C+A">Ales Wodecki</a>, 
<a href="/search/quant-ph?searchtype=author&query=Marecek%2C+J">Jakub Marecek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">We consider the problem of learning local quantum Hamiltonians given copies
of their Gibbs state at a known inverse temperature, following Haah et al.
[<a href="/abs/2108.04842">2108.04842</a>] and Bakshi et al. [<a href="/abs/2310.02243">arXiv:2310.02243</a>]. Our main technical
contribution is a new flat polynomial approximation of the exponential function
based on the Chebyshev expansion, which enables the formulation of learning
quantum Hamiltonians as a polynomial optimization problem. This, in turn, can
benefit from the use of moment/SOS relaxations, whose polynomial bit complexity
requires careful analysis [O'Donnell, ITCS 2017]. Finally, we show that
learning a $k$-local Hamiltonian, whose dual interaction graph is of bounded
degree, runs in polynomial time under mild assumptions.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05554" title="Abstract">arXiv:2402.05554</a> (cross-list from eess.IV) [<a href="/pdf/2402.05554" title="Download PDF">pdf</a>, <a href="/ps/2402.05554" title="Download PostScript">ps</a>, <a href="/format/2402.05554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in  Ultrasound Images Using Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Peng%2C+J">Jiayu Peng</a>, 
<a href="/search/eess?searchtype=author&query=Zeng%2C+J">Jiajun Zeng</a>, 
<a href="/search/eess?searchtype=author&query=Lai%2C+M">Manlin Lai</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+R">Ruobing Huang</a>, 
<a href="/search/eess?searchtype=author&query=Ni%2C+D">Dong Ni</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Z">Zhenzhou Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Ultrasound in Medicine &amp; Biology
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Ultrasound in Medicine &amp; Biology, Volume 50, Issue 2, February
  2024, Pages 304-314
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Objective: Ultrasound (US) examination has unique advantages in diagnosing
carpal tunnel syndrome (CTS) while identifying the median nerve (MN) and
diagnosing CTS depends heavily on the expertise of examiners. To alleviate this
problem, we aimed to develop a one-stop automated CTS diagnosis system
(OSA-CTSD) and evaluate its effectiveness as a computer-aided diagnostic tool.
Methods: We combined real-time MN delineation, accurate biometric measurements,
and explainable CTS diagnosis into a unified framework, called OSA-CTSD. We
collected a total of 32,301 static images from US videos of 90 normal wrists
and 40 CTS wrists for evaluation using a simplified scanning protocol. Results:
The proposed model showed better segmentation and measurement performance than
competing methods, reporting that HD95 score of 7.21px, ASSD score of 2.64px,
Dice score of 85.78%, and IoU score of 76.00%, respectively. In the reader
study, it demonstrated comparable performance with the average performance of
the experienced in classifying the CTS, while outperformed that of the
inexperienced radiologists in terms of classification metrics (e.g., accuracy
score of 3.59% higher and F1 score of 5.85% higher). Conclusion: The OSA-CTSD
demonstrated promising diagnostic performance with the advantages of real-time,
automation, and clinical interpretability. The application of such a tool can
not only reduce reliance on the expertise of examiners, but also can help to
promote the future standardization of the CTS diagnosis process, benefiting
both patients and radiologists.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05568" title="Abstract">arXiv:2402.05568</a> (cross-list from physics.med-ph) [<a href="/pdf/2402.05568" title="Download PDF">pdf</a>, <a href="/format/2402.05568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Graphics Primitives-based Deformable Image Registration for  On-the-fly Motion Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Li%2C+X">Xia Li</a>, 
<a href="/search/physics?searchtype=author&query=Zhang%2C+F">Fabian Zhang</a>, 
<a href="/search/physics?searchtype=author&query=Li%2C+M">Muheng Li</a>, 
<a href="/search/physics?searchtype=author&query=Weber%2C+D">Damien Weber</a>, 
<a href="/search/physics?searchtype=author&query=Lomax%2C+A">Antony Lomax</a>, 
<a href="/search/physics?searchtype=author&query=Buhmann%2C+J">Joachim Buhmann</a>, 
<a href="/search/physics?searchtype=author&query=Zhang%2C+Y">Ye Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Medical Physics (physics.med-ph)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Intra-fraction motion in radiotherapy is commonly modeled using deformable
image registration (DIR). However, existing methods often struggle to balance
speed and accuracy, limiting their applicability in clinical scenarios. This
study introduces a novel approach that harnesses Neural Graphics Primitives
(NGP) to optimize the displacement vector field (DVF). Our method leverages
learned primitives, processed as splats, and interpolates within space using a
shallow neural network. Uniquely, it enables self-supervised optimization at an
ultra-fast speed, negating the need for pre-training on extensive datasets and
allowing seamless adaptation to new cases. We validated this approach on the
4D-CT lung dataset DIR-lab, achieving a target registration error (TRE) of
1.15\pm1.15 mm within a remarkable time of 1.77 seconds. Notably, our method
also addresses the sliding boundary problem, a common challenge in conventional
DIR methods.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05579" title="Abstract">arXiv:2402.05579</a> (cross-list from math.OC) [<a href="/pdf/2402.05579" title="Download PDF">pdf</a>, <a href="/format/2402.05579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifier Elimination for Normal Cone Computations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Mandlmayr%2C+M">Michael Mandlmayr</a>, 
<a href="/search/math?searchtype=author&query=Uncu%2C+A+K">Ali Kemal Uncu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Symbolic Computation (cs.SC); Numerical Analysis (math.NA)

</div>
<p class="mathjax">We present effective procedures to calculate regular normal cones and other
related objects using quantifier elimination. This method of normal cone
calculations is complementary to computing Lagrangians and it works best at
points where the constraint qualifications fail and extra work for other
methods becomes inevitable. This method also serves as a tool to calculate the
regular co-derivative for semismooth* Newton methods. We list algorithms and
their demonstrations of different use cases for this approach.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05582" title="Abstract">arXiv:2402.05582</a> (cross-list from eess.IV) [<a href="/pdf/2402.05582" title="Download PDF">pdf</a>, <a href="/ps/2402.05582" title="Download PostScript">ps</a>, <a href="/format/2402.05582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint End-to-End Image Compression and Denoising: Leveraging Contrastive  Learning and Multi-Scale Self-ONNs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xie%2C+Y">Yuxin Xie</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+L">Li Yu</a>, 
<a href="/search/eess?searchtype=author&query=Pakdaman%2C+F">Farhad Pakdaman</a>, 
<a href="/search/eess?searchtype=author&query=Gabbouj%2C+M">Moncef Gabbouj</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Copyright 2024 IEEE - Submitted to IEEE ICIP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)

</div>
<p class="mathjax">Noisy images are a challenge to image compression algorithms due to the
inherent difficulty of compressing noise. As noise cannot easily be discerned
from image details, such as high-frequency signals, its presence leads to extra
bits needed for compression. Since the emerging learned image compression
paradigm enables end-to-end optimization of codecs, recent efforts were made to
integrate denoising into the compression model, relying on clean image features
to guide denoising. However, these methods exhibit suboptimal performance under
high noise levels, lacking the capability to generalize across diverse noise
types. In this paper, we propose a novel method integrating a multi-scale
denoiser comprising of Self Organizing Operational Neural Networks, for joint
image compression and denoising. We employ contrastive learning to boost the
network ability to differentiate noise from high frequency signal components,
by emphasizing the correlation between noisy and clean counterparts.
Experimental results demonstrate the effectiveness of the proposed method both
in rate-distortion performance, and codec speed, outperforming the current
state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05596" title="Abstract">arXiv:2402.05596</a> (cross-list from math.CO) [<a href="/pdf/2402.05596" title="Download PDF">pdf</a>, <a href="/ps/2402.05596" title="Download PostScript">ps</a>, <a href="/format/2402.05596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved upper bounds for wide-sense frameproof codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhao%2C+Y">Yuhao Zhao</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+X">Xiande Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Frameproof codes have been extensively studied for many years due to their
application in copyright protection and their connection to extremal set
theory. In this paper, we investigate upper bounds on the cardinality of
wide-sense $t$-frameproof codes. For $t=2$, we apply results from Sperner
theory to give a better upper bound, which significantly improves a recent
bound by Zhou and Zhou. For $t\geq 3$, we provide a general upper bound by
establishing a relation between wide-sense frameproof codes and cover-free
families. Finally, when the code length $n$ is at most
$\frac{15+\sqrt{33}}{24}(t-1)^2$, we show that a wide-sense $t$-frameproof code
has at most $n$ codewords, and the unique optimal code consists of all
weight-one codewords. As byproducts, our results improve several best known
results on binary $t$-frameproof codes.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05639" title="Abstract">arXiv:2402.05639</a> (cross-list from stat.ML) [<a href="/pdf/2402.05639" title="Download PDF">pdf</a>, <a href="/format/2402.05639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonparametric Instrumental Variable Regression through Stochastic  Approximate Gradients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Peixoto%2C+C">Caio Peixoto</a>, 
<a href="/search/stat?searchtype=author&query=Saporito%2C+Y">Yuri Saporito</a>, 
<a href="/search/stat?searchtype=author&query=Fonseca%2C+Y">Yuri Fonseca</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper proposes SAGD-IV, a novel framework for conducting nonparametric
instrumental variable (NPIV) regression by employing stochastic approximate
gradients to minimize the projected populational risk. Instrumental Variables
(IVs) are widely used in econometrics to address estimation problems in the
presence of unobservable confounders, and the Machine Learning community has
devoted significant effort to improving existing methods and devising new ones
in the NPIV setting, which is known to be an ill-posed linear inverse problem.
We provide theoretical support for our algorithm and further exemplify its
competitive performance through empirical experiments. Furthermore, we address,
with promising results, the case of binary outcomes, which has not received as
much attention from the community as its continuous counterpart.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05642" title="Abstract">arXiv:2402.05642</a> (cross-list from eess.IV) [<a href="/pdf/2402.05642" title="Download PDF">pdf</a>, <a href="/ps/2402.05642" title="Download PostScript">ps</a>, <a href="/format/2402.05642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Optimization-based Baseline for Rigid 2D/3D Registration Applied to  Spine Surgical Navigation Using CMA-ES
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+M">Minheng Chen</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+T">Tonglong Li</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Z">Zhirun Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Kong%2C+Y">Youyong Kong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">A robust and efficient optimization-based 2D/3D registration framework is
crucial for the navigation system of orthopedic surgical robots. It can provide
precise position information of surgical instruments and implants during
surgery. While artificial intelligence technology has advanced rapidly in
recent years, traditional optimization-based registration methods remain
indispensable in the field of 2D/3D registration.he exceptional precision of
this method enables it to be considered as a post-processing step of the
learning-based methods, thereby offering a reliable assurance for registration.
In this paper, we present a coarse-to-fine registration framework based on the
CMA-ES algorithm. We conducted intensive testing of our method using data from
different parts of the spine. The results shows the effectiveness of the
proposed framework on real orthopedic spine surgery clinical data. This work
can be viewed as an additional extension that complements the
optimization-based methods employed in our previous studies.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05674" title="Abstract">arXiv:2402.05674</a> (cross-list from stat.ML) [<a href="/pdf/2402.05674" title="Download PDF">pdf</a>, <a href="/format/2402.05674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A High Dimensional Model for Adversarial Training: Geometry and  Trade-Offs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Tanner%2C+K">Kasimir Tanner</a>, 
<a href="/search/stat?searchtype=author&query=Vilucchio%2C+M">Matteo Vilucchio</a>, 
<a href="/search/stat?searchtype=author&query=Loureiro%2C+B">Bruno Loureiro</a>, 
<a href="/search/stat?searchtype=author&query=Krzakala%2C+F">Florent Krzakala</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG)

</div>
<p class="mathjax">This work investigates adversarial training in the context of margin-based
linear classifiers in the high-dimensional regime where the dimension $d$ and
the number of data points $n$ diverge with a fixed ratio $\alpha = n / d$. We
introduce a tractable mathematical model where the interplay between the data
and adversarial attacker geometries can be studied, while capturing the core
phenomenology observed in the adversarial robustness literature. Our main
theoretical contribution is an exact asymptotic description of the sufficient
statistics for the adversarial empirical risk minimiser, under generic convex
and non-increasing losses. Our result allow us to precisely characterise which
directions in the data are associated with a higher generalisation/robustness
trade-off, as defined by a robustness and a usefulness metric. In particular,
we unveil the existence of directions which can be defended without penalising
accuracy. Finally, we show the advantage of defending non-robust features
during training, identifying a uniform protection as an inherently effective
defence mechanism.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05676" title="Abstract">arXiv:2402.05676</a> (cross-list from math.DS) [<a href="/pdf/2402.05676" title="Download PDF">pdf</a>, <a href="/ps/2402.05676" title="Download PostScript">ps</a>, <a href="/format/2402.05676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using nodal coordinates as variables for the dimensional synthesis of  mechanisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Garcia-Marina%2C+V">V. Garcia-Marina</a>, 
<a href="/search/math?searchtype=author&query=de+Bustos%2C+I+F">I. Fernandez de Bustos</a>, 
<a href="/search/math?searchtype=author&query=Urkullu%2C+G">G. Urkullu</a>, 
<a href="/search/math?searchtype=author&query=Abasolo%2C+M">M. Abasolo</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Meccanica (2018) 53:1981--1996
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Dynamical Systems (math.DS)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">The method of the lower deformation energy has been successfully used for the
synthesis of mechanisms for quite a while. It has shown to be a versatile, yet
powerful method for assisting in the design of mechanisms. Until now, most of
the implementations of this method used the dimensions of the mechanism as the
synthesis variables, which has some advantages and some drawbacks. For example,
the assembly configuration is not taken into account in the optimization
process, and this means that the same initial configuration is used when
computing the deformed positions in each synthesis point. This translates into
a reduction of the total search space. A possible solution to this problem is
the use of a set of initial coordinates as variables for the synthesis, which
has been successfully applied to other methods. This also has some additional
advantages, such as the fact that any generated mechanism can be assembled.
Another advantage is that the fixed joint locations are also included in the
optimization at no additional cost. But the change from dimensions to initial
coordinates means a reformulation of the optimization problem when using
derivatives if one wants them to be analytically derived. This paper tackles
this reformulation, along with a proper comparison of the use of both
alternatives using sequential quadratic programming methods. In order to do so,
some examples are developed and studied.
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05677" title="Abstract">arXiv:2402.05677</a> (cross-list from math.CO) [<a href="/pdf/2402.05677" title="Download PDF">pdf</a>, <a href="/ps/2402.05677" title="Download PostScript">ps</a>, <a href="/format/2402.05677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vectorial Negabent Concepts: Similarities, Differences, and  Generalizations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Anbar%2C+N">Nurdag&#xfc;l Anbar</a>, 
<a href="/search/math?searchtype=author&query=Kudin%2C+S">Sadmir Kudin</a>, 
<a href="/search/math?searchtype=author&query=Meidl%2C+W">Wilfried Meidl</a>, 
<a href="/search/math?searchtype=author&query=Pasalic%2C+E">Enes Pasalic</a>, 
<a href="/search/math?searchtype=author&query=Polujan%2C+A">Alexandr Polujan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Information Theory (cs.IT)

</div>
<p class="mathjax">In Pasalic et al., IEEE Trans. Inform. Theory 69 (2023), 2702--2712, and in
Anbar, Meidl, Cryptogr. Commun. 10 (2018), 235--249, two different vectorial
negabent and vectorial bent-negabent concepts are introduced, which leads to
seemingly contradictory results. One of the main motivations for this article
is to clarify the differences and similarities between these two concepts.
Moreover, the negabent concept is extended to generalized Boolean functions
from \(\mathbb{F}_2^n\) to the cyclic group \(\mathbb{Z}_{2^k}\). It is shown
how to obtain nega-\(\mathbb{Z}_{2^k}\)-bent functions from
\(\mathbb{Z}_{2^k}\)-bent functions, or equivalently, corresponding
non-splitting relative difference sets from the splitting relative difference
sets. This generalizes the shifting results for Boolean bent and negabent
functions. We finally point to constructions of \(\mathbb{Z}_8\)-bent functions
employing permutations with the \((\mathcal{A}_m)\) property, and more
generally we show that the inverse permutation gives rise to
\(\mathbb{Z}_{2^k}\)-bent functions.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05696" title="Abstract">arXiv:2402.05696</a> (cross-list from stat.ML) [<a href="/pdf/2402.05696" title="Download PDF">pdf</a>, <a href="/ps/2402.05696" title="Download PostScript">ps</a>, <a href="/format/2402.05696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fixed width treelike neural networks capacity analysis -- generic  activations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Stojnic%2C+M">Mihailo Stojnic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Information Theory (cs.IT); Machine Learning (cs.LG); Probability (math.PR)

</div>
<p class="mathjax">We consider the capacity of \emph{treelike committee machines} (TCM) neural
networks. Relying on Random Duality Theory (RDT), \cite{Stojnictcmspnncaprdt23}
recently introduced a generic framework for their capacity analysis. An upgrade
based on the so-called \emph{partially lifted} RDT (pl RDT) was then presented
in \cite{Stojnictcmspnncapliftedrdt23}. Both lines of work focused on the
networks with the most typical, \emph{sign}, activations. Here, on the other
hand, we focus on networks with other, more general, types of activations and
show that the frameworks of
\cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23} are sufficiently
powerful to enable handling of such scenarios as well. In addition to the
standard \emph{linear} activations, we uncover that particularly convenient
results can be obtained for two very commonly used activations, namely, the
\emph{quadratic} and \emph{rectified linear unit (ReLU)} ones. In more concrete
terms, for each of these activations, we obtain both the RDT and pl RDT based
memory capacities upper bound characterization for \emph{any} given (even)
number of the hidden layer neurons, $d$. In the process, we also uncover the
following two, rather remarkable, facts: 1) contrary to the common wisdom, both
sets of results show that the bounding capacity decreases for large $d$ (the
width of the hidden layer) while converging to a constant value; and 2) the
maximum bounding capacity is achieved for the networks with precisely
\textbf{\emph{two}} hidden layer neurons! Moreover, the large $d$ converging
values are observed to be in excellent agrement with the statistical physics
replica theory based predictions.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05705" title="Abstract">arXiv:2402.05705</a> (cross-list from math.OC) [<a href="/pdf/2402.05705" title="Download PDF">pdf</a>, <a href="/format/2402.05705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Optimal Communication Weights in Distributed Optimization  Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Colla%2C+S">Sebastien Colla</a>, 
<a href="/search/math?searchtype=author&query=Hendrickx%2C+J+M">Julien M. Hendrickx</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages. Submitted to MTNS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">We establish that in distributed optimization, the prevalent strategy of
minimizing the second-largest eigenvalue modulus (SLEM) of the averaging matrix
for selecting communication weights, while optimal for existing theoretical
performance bounds, is generally not optimal regarding the exact worst-case
performance of the algorithms. This exact performance can be computed using the
Performance Estimation Problem (PEP) approach. We thus rely on PEP to formulate
an optimization problem that determines the optimal communication weights for a
distributed optimization algorithm deployed on a specified undirected graph.
Our results show that the optimal weights can outperform the weights minimizing
the second-largest eigenvalue modulus (SLEM) of the averaging matrix. This
suggests that the SLEM is not the best characterization of weighted network
performance for decentralized optimization. Additionally, we explore and
compare alternative heuristics for weight selection in distributed
optimization.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05715" title="Abstract">arXiv:2402.05715</a> (cross-list from stat.ML) [<a href="/pdf/2402.05715" title="Download PDF">pdf</a>, <a href="/format/2402.05715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaborative non-parametric two-sample testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=de+la+Concha%2C+A">Alejandro de la Concha</a>, 
<a href="/search/stat?searchtype=author&query=Vayatis%2C+N">Nicolas Vayatis</a>, 
<a href="/search/stat?searchtype=author&query=Kalogeratos%2C+A">Argyris Kalogeratos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper addresses the multiple two-sample test problem in a
graph-structured setting, which is a common scenario in fields such as Spatial
Statistics and Neuroscience. Each node $v$ in fixed graph deals with a
two-sample testing problem between two node-specific probability density
functions (pdfs), $p_v$ and $q_v$. The goal is to identify nodes where the null
hypothesis $p_v = q_v$ should be rejected, under the assumption that connected
nodes would yield similar test outcomes. We propose the non-parametric
collaborative two-sample testing (CTST) framework that efficiently leverages
the graph structure and minimizes the assumptions over $p_v$ and $q_v$. Our
methodology integrates elements from f-divergence estimation, Kernel Methods,
and Multitask Learning. We use synthetic experiments and a real sensor network
detecting seismic activity to demonstrate that CTST outperforms
state-of-the-art non-parametric statistical tests that apply at each node
independently, hence disregard the geometry of the problem.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05718" title="Abstract">arXiv:2402.05718</a> (cross-list from stat.ML) [<a href="/pdf/2402.05718" title="Download PDF">pdf</a>, <a href="/format/2402.05718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> REMEDI: Corrective Transformations for Improved Neural Entropy  Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Nilsson%2C+V">Viktor Nilsson</a>, 
<a href="/search/stat?searchtype=author&query=Samaddar%2C+A">Anirban Samaddar</a>, 
<a href="/search/stat?searchtype=author&query=Madireddy%2C+S">Sandeep Madireddy</a>, 
<a href="/search/stat?searchtype=author&query=Nyquist%2C+P">Pierre Nyquist</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Information theoretic quantities play a central role in machine learning. The
recent surge in the complexity of data and models has increased the demand for
accurate estimation of these quantities. However, as the dimension grows the
estimation presents significant challenges, with existing methods struggling
already in relatively low dimensions. To address this issue, in this work, we
introduce $\texttt{REMEDI}$ for efficient and accurate estimation of
differential entropy, a fundamental information theoretic quantity. The
approach combines the minimization of the cross-entropy for simple, adaptive
base models and the estimation of their deviation, in terms of the relative
entropy, from the data density. Our approach demonstrates improvement across a
broad spectrum of estimation tasks, encompassing entropy estimation on both
synthetic and natural data. Further, we extend important theoretical
consistency results to a more generalized setting required by our approach. We
illustrate how the framework can be naturally extended to information theoretic
supervised learning models, with a specific focus on the Information Bottleneck
approach. It is demonstrated that the method delivers better accuracy compared
to the existing methods in Information Bottleneck. In addition, we explore a
natural connection between $\texttt{REMEDI}$ and generative modeling using
rejection sampling and Langevin dynamics.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05719" title="Abstract">arXiv:2402.05719</a> (cross-list from stat.ML) [<a href="/pdf/2402.05719" title="Download PDF">pdf</a>, <a href="/ps/2402.05719" title="Download PostScript">ps</a>, <a href="/format/2402.05719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact capacity of the \emph{wide} hidden layer treelike neural networks  with generic activations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Stojnic%2C+M">Mihailo Stojnic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Information Theory (cs.IT); Machine Learning (cs.LG); Probability (math.PR)

</div>
<p class="mathjax">Recent progress in studying \emph{treelike committee machines} (TCM) neural
networks (NN) in
\cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23,Stojnictcmspnncapdiffactrdt23}
showed that the Random Duality Theory (RDT) and its a \emph{partially
lifted}(pl RDT) variant are powerful tools that can be used for very precise
networks capacity analysis. Here, we consider \emph{wide} hidden layer networks
and uncover that certain aspects of numerical difficulties faced in
\cite{Stojnictcmspnncapdiffactrdt23} miraculously disappear. In particular, we
employ recently developed \emph{fully lifted} (fl) RDT to characterize the
\emph{wide} ($d\rightarrow \infty$) TCM nets capacity. We obtain explicit,
closed form, capacity characterizations for a very generic class of the hidden
layer activations. While the utilized approach significantly lowers the amount
of the needed numerical evaluations, the ultimate fl RDT usefulness and success
still require a solid portion of the residual numerical work. To get the
concrete capacity values, we take four very famous activations examples:
\emph{\textbf{ReLU}}, \textbf{\emph{quadratic}}, \textbf{\emph{erf}}, and
\textbf{\emph{tanh}}. After successfully conducting all the residual numerical
work for all of them, we uncover that the whole lifting mechanism exhibits a
remarkably rapid convergence with the relative improvements no better than
$\sim 0.1\%$ happening already on the 3-rd level of lifting. As a convenient
bonus, we also uncover that the capacity characterizations obtained on the
first and second level of lifting precisely match those obtained through the
statistical physics replica theory methods in \cite{ZavPeh21} for the generic
and in \cite{BalMalZech19} for the ReLU activations.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05739" title="Abstract">arXiv:2402.05739</a> (cross-list from physics.soc-ph) [<a href="/pdf/2402.05739" title="Download PDF">pdf</a>, <a href="/format/2402.05739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Critical mobility in policy making for epidemic containment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=L%C3%B3pez%2C+J+A+M">Jes&#xfa;s A. Moreno L&#xf3;pez</a>, 
<a href="/search/physics?searchtype=author&query=Mateo%2C+D">David Mateo</a>, 
<a href="/search/physics?searchtype=author&query=Hernando%2C+A">Alberto Hernando</a>, 
<a href="/search/physics?searchtype=author&query=Meloni%2C+S">Sandro Meloni</a>, 
<a href="/search/physics?searchtype=author&query=Ramasco%2C+J+J">Jose J. Ramasco</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Multiagent Systems (cs.MA); Populations and Evolution (q-bio.PE)

</div>
<p class="mathjax">When considering airborne epidemic spreading in social systems, a natural
connection arises between mobility and epidemic contacts. As individuals
travel, possibilities to encounter new people either at the final destination
or during the transportation process appear. Such contacts can lead to new
contagion events. In fact, mobility has been a crucial target for early
non-pharmaceutical containment measures against the recent COVID-19 pandemic,
with a degree of intensity ranging from public transportation line closures to
regional, city or even home confinements. Nonetheless, quantitative knowledge
on the relationship between mobility-contagions and, consequently, on the
efficiency of containment measures remains elusive. Here we introduce an
agent-based model with a simple interaction between mobility and contacts.
Despite its simplicity our model shows the emergence of a critical mobility
level, inducing major outbreaks when surpassed. We explore the interplay
between mobility restrictions and the infection in recent intervention policies
seen across many countries, and how interventions in the form of closures
triggered by incidence rates can guide the epidemic into an oscillatory regime
with recurrent waves. We consider how the different interventions impact
societal well-being, the economy and the population. Finally, we propose a
mitigation framework based on the critical nature of mobility in an epidemic,
able to suppress incidence and oscillations at will, preventing extreme
incidence peaks with potential to saturate health care resources.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05750" title="Abstract">arXiv:2402.05750</a> (cross-list from q-bio.QM) [<a href="/pdf/2402.05750" title="Download PDF">pdf</a>, <a href="/ps/2402.05750" title="Download PostScript">ps</a>, <a href="/format/2402.05750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Metamodeling and Control of Medical Digital Twins
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Fonseca%2C+L+L">Luis L. Fonseca</a>, 
<a href="/search/q-bio?searchtype=author&query=B%C3%B6ttcher%2C+L">Lucas B&#xf6;ttcher</a>, 
<a href="/search/q-bio?searchtype=author&query=Mehrad%2C+B">Borna Mehrad</a>, 
<a href="/search/q-bio?searchtype=author&query=Laubenbacher%2C+R+C">Reinhard C. Laubenbacher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 54 pages, 4 figures, 22 supplementary figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Systems and Control (eess.SY); Dynamical Systems (math.DS); Optimization and Control (math.OC)

</div>
<p class="mathjax">The vision of personalized medicine is to identify interventions that
maintain or restore a person's health based on their individual biology.
Medical digital twins, computational models that integrate a wide range of
health-related data about a person and can be dynamically updated, are a key
technology that can help guide medical decisions. Such medical digital twin
models can be high-dimensional, multi-scale, and stochastic. To be practical
for healthcare applications, they need to be simplified into low-dimensional
metamodels that can be used for forecasting and optimal design of
interventions. This paper introduces metamodeling algorithms for the purpose of
optimal control applications. It uses agent-based models as a use case, a
common model type in biomedicine for which there are no readily available
optimal control algorithms. With systems of ordinary differential equations as
metamodels, optimal control methods can be applied to the metamodels, and
results can be lifted to the agent-based model.
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05753" title="Abstract">arXiv:2402.05753</a> (cross-list from math.CO) [<a href="/pdf/2402.05753" title="Download PDF">pdf</a>, <a href="/format/2402.05753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cops and Robber on Hyperbolic Manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ir%C5%A1i%C4%8D%2C+V">Vesna Ir&#x161;i&#x10d;</a>, 
<a href="/search/math?searchtype=author&query=Mohar%2C+B">Bojan Mohar</a>, 
<a href="/search/math?searchtype=author&query=Wesolek%2C+A">Alexandra Wesolek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Geometry (cs.CG); Metric Geometry (math.MG)

</div>
<p class="mathjax">The Cops and Robber game on geodesic spaces is a pursuit-evasion game with
discrete steps which captures the behavior of the game played on graphs, as
well as that of continuous pursuit-evasion games. One of the outstanding open
problems about the game on graphs is to determine which graphs embeddable in a
surface of genus $g$ have largest cop number. It is known that the cop number
of genus $g$ graphs is $O(g)$ and that there are examples whose cop number is
$\tilde\Omega(\sqrt{g}\,)$. The same phenomenon occurs when the game is played
on geodesic surfaces. In this paper we obtain a surprising result about the
game on a surface with constant curvature. It is shown that two cops have a
strategy to come arbitrarily close to the robber, independently of the genus.
We also discuss upper bounds on the number of cops needed to catch the robber.
Our results generalize to higher-dimensional hyperbolic manifolds.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05787" title="Abstract">arXiv:2402.05787</a> (cross-list from stat.ML) [<a href="/pdf/2402.05787" title="Download PDF">pdf</a>, <a href="/format/2402.05787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How do Transformers perform In-Context Autoregressive Learning?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Sander%2C+M+E">Michael E. Sander</a>, 
<a href="/search/stat?searchtype=author&query=Giryes%2C+R">Raja Giryes</a>, 
<a href="/search/stat?searchtype=author&query=Suzuki%2C+T">Taiji Suzuki</a>, 
<a href="/search/stat?searchtype=author&query=Blondel%2C+M">Mathieu Blondel</a>, 
<a href="/search/stat?searchtype=author&query=Peyr%C3%A9%2C+G">Gabriel Peyr&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Transformers have achieved state-of-the-art performance in language modeling
tasks. However, the reasons behind their tremendous success are still unclear.
In this paper, towards a better understanding, we train a Transformer model on
a simple next token prediction task, where sequences are generated as a
first-order autoregressive process $s_{t+1} = W s_t$. We show how a trained
Transformer predicts the next token by first learning $W$ in-context, then
applying a prediction mapping. We call the resulting procedure in-context
autoregressive learning. More precisely, focusing on commuting orthogonal
matrices $W$, we first show that a trained one-layer linear Transformer
implements one step of gradient descent for the minimization of an inner
objective function, when considering augmented tokens. When the tokens are not
augmented, we characterize the global minima of a one-layer diagonal linear
multi-head Transformer. Importantly, we exhibit orthogonality between heads and
show that positional encoding captures trigonometric relations in the data. On
the experimental side, we consider the general case of non-commuting orthogonal
matrices and generalize our theoretical findings.
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05793" title="Abstract">arXiv:2402.05793</a> (cross-list from quant-ph) [<a href="/pdf/2402.05793" title="Download PDF">pdf</a>, <a href="/format/2402.05793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact quantum sensing limits for bosonic dephasing channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Huang%2C+Z">Zixin Huang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Lami%2C+L">Ludovico Lami</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wilde%2C+M+M">Mark M. Wilde</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v1: 21 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Other Condensed Matter (cond-mat.other); Information Theory (cs.IT)

</div>
<p class="mathjax">Dephasing is a prominent noise mechanism that afflicts quantum information
carriers, and it is one of the main challenges towards realizing useful quantum
computation, communication, and sensing. Here we consider discrimination and
estimation of bosonic dephasing channels, when using the most general adaptive
strategies allowed by quantum mechanics. We reduce these difficult quantum
problems to simple classical ones based on the probability densities defining
the bosonic dephasing channels. By doing so, we rigorously establish the
optimal performance of various distinguishability and estimation tasks and
construct explicit strategies to achieve this performance. To the best of our
knowledge, this is the first example of a non-Gaussian bosonic channel for
which there are exact solutions for these tasks.
</p>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05817" title="Abstract">arXiv:2402.05817</a> (cross-list from eess.IV) [<a href="/pdf/2402.05817" title="Download PDF">pdf</a>, <a href="/ps/2402.05817" title="Download PostScript">ps</a>, <a href="/format/2402.05817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using YOLO v7 to Detect Kidney in Magnetic Resonance Imaging: A  Supervised Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Anari%2C+P+Y">Pouria Yazdian Anari</a>, 
<a href="/search/eess?searchtype=author&query=Obiezu%2C+F">Fiona Obiezu</a>, 
<a href="/search/eess?searchtype=author&query=Lay%2C+N">Nathan Lay</a>, 
<a href="/search/eess?searchtype=author&query=Firouzabadi%2C+F+D">Fatemeh Dehghani Firouzabadi</a>, 
<a href="/search/eess?searchtype=author&query=Chaurasia%2C+A">Aditi Chaurasia</a>, 
<a href="/search/eess?searchtype=author&query=Golagha%2C+M">Mahshid Golagha</a>, 
<a href="/search/eess?searchtype=author&query=Singh%2C+S">Shiva Singh</a>, 
<a href="/search/eess?searchtype=author&query=Homayounieh%2C+F">Fatemeh Homayounieh</a>, 
<a href="/search/eess?searchtype=author&query=Zahergivar%2C+A">Aryan Zahergivar</a>, 
<a href="/search/eess?searchtype=author&query=Harmon%2C+S">Stephanie Harmon</a>, 
<a href="/search/eess?searchtype=author&query=Turkbey%2C+E">Evrim Turkbey</a>, 
<a href="/search/eess?searchtype=author&query=Gautam%2C+R">Rabindra Gautam</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+K">Kevin Ma</a>, 
<a href="/search/eess?searchtype=author&query=Merino%2C+M">Maria Merino</a>, 
<a href="/search/eess?searchtype=author&query=Jones%2C+E+C">Elizabeth C. Jones</a>, 
<a href="/search/eess?searchtype=author&query=Ball%2C+M+W">Mark W. Ball</a>, 
<a href="/search/eess?searchtype=author&query=Linehan%2C+W+M">W. Marston Linehan</a>, 
<a href="/search/eess?searchtype=author&query=Turkbey%2C+B">Baris Turkbey</a>, 
<a href="/search/eess?searchtype=author&query=Malayeri%2C+A+A">Ashkan A. Malayeri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Introduction This study explores the use of the latest You Only Look Once
(YOLO V7) object detection method to enhance kidney detection in medical
imaging by training and testing a modified YOLO V7 on medical image formats.
Methods Study includes 878 patients with various subtypes of renal cell
carcinoma (RCC) and 206 patients with normal kidneys. A total of 5657 MRI scans
for 1084 patients were retrieved. 326 patients with 1034 tumors recruited from
a retrospective maintained database, and bounding boxes were drawn around their
tumors. A primary model was trained on 80% of annotated cases, with 20% saved
for testing (primary test set). The best primary model was then used to
identify tumors in the remaining 861 patients and bounding box coordinates were
generated on their scans using the model. Ten benchmark training sets were
created with generated coordinates on not-segmented patients. The final model
used to predict the kidney in the primary test set. We reported the positive
predictive value (PPV), sensitivity, and mean average precision (mAP). Results
The primary training set showed an average PPV of 0.94 +/- 0.01, sensitivity of
0.87 +/- 0.04, and mAP of 0.91 +/- 0.02. The best primary model yielded a PPV
of 0.97, sensitivity of 0.92, and mAP of 0.95. The final model demonstrated an
average PPV of 0.95 +/- 0.03, sensitivity of 0.98 +/- 0.004, and mAP of 0.95
+/- 0.01. Conclusion Using a semi-supervised approach with a medical image
library, we developed a high-performing model for kidney detection. Further
external validation is required to assess the model's generalizability.
</p>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05818" title="Abstract">arXiv:2402.05818</a> (cross-list from math.CO) [<a href="/pdf/2402.05818" title="Download PDF">pdf</a>, <a href="/ps/2402.05818" title="Download PostScript">ps</a>, <a href="/format/2402.05818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $L$-systems and the Lov&#xe1;sz number
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Linz%2C+W">William Linz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Given integers $n &gt; k &gt; 0$, and a set of integers $L \subset [0, k-1]$, an
$L$-system is a family of sets $\mathcal{F} \subset \binom{[n]}{k}$ such that
$|F \cap F'| \in L$ for distinct $F, F'\in \mathcal{F}$. $L$-systems correspond
to independent sets in a certain generalized Johnson graph $G(n, k, L)$, so
that the maximum size of an $L$-system is equivalent to finding the
independence number of the graph $G(n, k, L)$. The Lov\'asz number
$\vartheta(G)$ is a semidefinite programming approximation of the independence
number of a graph $G$.
<br />In this paper, we determine the order of magnitude of $\vartheta(G(n, k, L))$
of any generalized Johnson graph with $k$ and $L$ fixed and $n\rightarrow
\infty$. As an application of this theorem, we give an explicit construction of
a graph $G$ on $n$ vertices with large gap between the Lov\'asz number and the
Shannon capacity $c(G)$. Specifically, we prove that for any $\epsilon &gt; 0$,
for infinitely many $n$ there is a generalized Johnson graph $G$ on $n$
vertices which has ratio $\vartheta(G)/c(G) = \Omega(n^{1-\epsilon})$, which
greatly improves on the best known explicit construction.
</p>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05819" title="Abstract">arXiv:2402.05819</a> (cross-list from eess.AS) [<a href="/pdf/2402.05819" title="Download PDF">pdf</a>, <a href="/format/2402.05819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Self-supervised Speech Model with Pseudo Word-level Targets  from Visually-grounded Speech Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fang%2C+H">Hung-Chieh Fang</a>, 
<a href="/search/eess?searchtype=author&query=Ye%2C+N">Nai-Xuan Ye</a>, 
<a href="/search/eess?searchtype=author&query=Shih%2C+Y">Yi-Jen Shih</a>, 
<a href="/search/eess?searchtype=author&query=Peng%2C+P">Puyuan Peng</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+H">Hsuan-Fu Wang</a>, 
<a href="/search/eess?searchtype=author&query=Berry%2C+L">Layne Berry</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>, 
<a href="/search/eess?searchtype=author&query=Harwath%2C+D">David Harwath</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICASSP 2024 workshop on Self-supervision in Audio, Speech, and Beyond (SASB)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advances in self-supervised speech models have shown significant
improvement in many downstream tasks. However, these models predominantly
centered on frame-level training objectives, which can fall short in spoken
language understanding tasks that require semantic comprehension. Existing
works often rely on additional speech-text data as intermediate targets, which
is costly in the real-world setting. To address this challenge, we propose
Pseudo-Word HuBERT (PW-HuBERT), a framework that integrates pseudo word-level
targets into the training process, where the targets are derived from a
visually-ground speech model, notably eliminating the need for speech-text
paired data. Our experimental results on four spoken language understanding
(SLU) benchmarks suggest the superiority of our model in capturing semantic
information.
</p>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05838" title="Abstract">arXiv:2402.05838</a> (cross-list from math.CO) [<a href="/pdf/2402.05838" title="Download PDF">pdf</a>, <a href="/format/2402.05838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Introducing q-deformed binomial coefficients of words
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Renard%2C+A">Antoine Renard</a>, 
<a href="/search/math?searchtype=author&query=Rigo%2C+M">Michel Rigo</a>, 
<a href="/search/math?searchtype=author&query=Whiteland%2C+M+A">Markus A. Whiteland</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, submitted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">Gaussian binomial coefficients are q-analogues of the binomial coefficients
of integers. On the other hand, binomial coefficients have been extended to
finite words, i.e., elements of the finitely generated free monoids. In this
paper we bring together these two notions by introducing q-analogues of
binomial coefficients of words. We study their basic properties, e.g., by
extending classical formulas such as the q-Vandermonde and Manvel's et al.
identities to our setting. As a consequence, we get information about the
structure of the considered words: these q-deformations of binomial
coefficients of words contain much richer information than the original
coefficients. From an algebraic perspective, we introduce a q-shuffle and a
family q-infiltration products for non-commutative formal power series.
Finally, we apply our results to generalize a theorem of Eilenberg
characterizing so-called p-group languages. We show that a language is of this
type if and only if it is a Boolean combination of specific languages defined
through q-binomial coefficients seen as polynomials over $\mathbb{F}_p$.
</p>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05841" title="Abstract">arXiv:2402.05841</a> (cross-list from q-bio.BM) [<a href="/pdf/2402.05841" title="Download PDF">pdf</a>, <a href="/format/2402.05841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dirichlet Flow Matching with Applications to DNA Sequence Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Stark%2C+H">Hannes Stark</a>, 
<a href="/search/q-bio?searchtype=author&query=Jing%2C+B">Bowen Jing</a>, 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+C">Chenyu Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Corso%2C+G">Gabriele Corso</a>, 
<a href="/search/q-bio?searchtype=author&query=Berger%2C+B">Bonnie Berger</a>, 
<a href="/search/q-bio?searchtype=author&query=Barzilay%2C+R">Regina Barzilay</a>, 
<a href="/search/q-bio?searchtype=author&query=Jaakkola%2C+T">Tommi Jaakkola</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Discrete diffusion or flow models could enable faster and more controllable
sequence generation than autoregressive models. We show that na\"ive linear
flow matching on the simplex is insufficient toward this goal since it suffers
from discontinuities in the training target and further pathologies. To
overcome this, we develop Dirichlet flow matching on the simplex based on
mixtures of Dirichlet distributions as probability paths. In this framework, we
derive a connection between the mixtures' scores and the flow's vector field
that allows for classifier and classifier-free guidance. Further, we provide
distilled Dirichlet flow matching, which enables one-step sequence generation
with minimal performance hits, resulting in $O(L)$ speedups compared to
autoregressive models. On complex DNA sequence generation tasks, we demonstrate
superior performance compared to all baselines in distributional metrics and in
achieving desired design targets for generated sequences. Finally, we show that
our classifier-free guidance approach improves unconditional generation and is
effective for generating DNA that satisfies design targets. Code is available
at https://github.com/HannesStark/dirichlet-flow-matching.
</p>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05856" title="Abstract">arXiv:2402.05856</a> (cross-list from q-bio.BM) [<a href="/pdf/2402.05856" title="Download PDF">pdf</a>, <a href="/format/2402.05856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure-Informed Protein Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+Z">Zuobai Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Lu%2C+J">Jiarui Lu</a>, 
<a href="/search/q-bio?searchtype=author&query=Chenthamarakshan%2C+V">Vijil Chenthamarakshan</a>, 
<a href="/search/q-bio?searchtype=author&query=Lozano%2C+A">Aur&#xe9;lie Lozano</a>, 
<a href="/search/q-bio?searchtype=author&query=Das%2C+P">Payel Das</a>, 
<a href="/search/q-bio?searchtype=author&query=Tang%2C+J">Jian Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Protein language models are a powerful tool for learning protein
representations through pre-training on vast protein sequence datasets.
However, traditional protein language models lack explicit structural
supervision, despite its relevance to protein function. To address this issue,
we introduce the integration of remote homology detection to distill structural
information into protein language models without requiring explicit protein
structures as input. We evaluate the impact of this structure-informed training
on downstream protein function prediction tasks. Experimental results reveal
consistent improvements in function annotation accuracy for EC number and GO
term prediction. Performance on mutant datasets, however, varies based on the
relationship between targeted properties and protein structures. This
underscores the importance of considering this relationship when applying
structure-aware training to protein function prediction tasks. Code and model
weights are available at https://github.com/DeepGraphLearning/esm-s.
</p>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05878" title="Abstract">arXiv:2402.05878</a> (cross-list from stat.ML) [<a href="/pdf/2402.05878" title="Download PDF">pdf</a>, <a href="/format/2402.05878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prior-Dependent Allocations for Bayesian Fixed-Budget Best-Arm  Identification in Structured Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Nguyen%2C+N">Nicolas Nguyen</a>, 
<a href="/search/stat?searchtype=author&query=Aouali%2C+I">Imad Aouali</a>, 
<a href="/search/stat?searchtype=author&query=Gy%C3%B6rgy%2C+A">Andr&#xe1;s Gy&#xf6;rgy</a>, 
<a href="/search/stat?searchtype=author&query=Vernade%2C+C">Claire Vernade</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We study the problem of Bayesian fixed-budget best-arm identification (BAI)
in structured bandits. We propose an algorithm that uses fixed allocations
based on the prior information and the structure of the environment. We provide
theoretical bounds on its performance across diverse models, including the
first prior-dependent upper bounds for linear and hierarchical BAI. Our key
contribution is introducing new proof methods that result in tighter bounds for
multi-armed BAI compared to existing methods. We extensively compare our
approach to other fixed-budget BAI methods, demonstrating its consistent and
robust performance in various settings. Our work improves our understanding of
Bayesian fixed-budget BAI in structured bandits and highlights the
effectiveness of our approach in practical scenarios.
</p>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05887" title="Abstract">arXiv:2402.05887</a> (cross-list from eess.IV) [<a href="/pdf/2402.05887" title="Download PDF">pdf</a>, <a href="/format/2402.05887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sandwiched Compression: Repurposing Standard Codecs with Neural Network  Wrappers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Guleryuz%2C+O+G">Onur G. Guleryuz</a>, 
<a href="/search/eess?searchtype=author&query=Chou%2C+P+A">Philip A. Chou</a>, 
<a href="/search/eess?searchtype=author&query=Isik%2C+B">Berivan Isik</a>, 
<a href="/search/eess?searchtype=author&query=Hoppe%2C+H">Hugues Hoppe</a>, 
<a href="/search/eess?searchtype=author&query=Tang%2C+D">Danhang Tang</a>, 
<a href="/search/eess?searchtype=author&query=Du%2C+R">Ruofei Du</a>, 
<a href="/search/eess?searchtype=author&query=Taylor%2C+J">Jonathan Taylor</a>, 
<a href="/search/eess?searchtype=author&query=Davidson%2C+P">Philip Davidson</a>, 
<a href="/search/eess?searchtype=author&query=Fanello%2C+S">Sean Fanello</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">We propose sandwiching standard image and video codecs between pre- and
post-processing neural networks. The networks are jointly trained through a
differentiable codec proxy to minimize a given rate-distortion loss. This
sandwich architecture not only improves the standard codec's performance on its
intended content, it can effectively adapt the codec to other types of
image/video content and to other distortion measures. Essentially, the sandwich
learns to transmit ``neural code images'' that optimize overall rate-distortion
performance even when the overall problem is well outside the scope of the
codec's design. Through a variety of examples, we apply the sandwich
architecture to sources with different numbers of channels, higher resolution,
higher dynamic range, and perceptual distortion measures. The results
demonstrate substantial improvements (up to 9 dB gains or up to 30\% bitrate
reductions) compared to alternative adaptations. We derive VQ equivalents for
the sandwich, establish optimality properties, and design differentiable codec
proxies approximating current standard codecs. We further analyze model
complexity, visual quality under perceptual metrics, as well as sandwich
configurations that offer interesting potentials in image/video compression and
streaming.
</p>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05923" title="Abstract">arXiv:2402.05923</a> (cross-list from math.OC) [<a href="/pdf/2402.05923" title="Download PDF">pdf</a>, <a href="/format/2402.05923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mean field control of droplet dynamics with high order finite element  computations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fu%2C+G">Guosheng Fu</a>, 
<a href="/search/math?searchtype=author&query=Ji%2C+H">Hangjie Ji</a>, 
<a href="/search/math?searchtype=author&query=Pazner%2C+W">Will Pazner</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+W">Wuchen Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA); Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">Liquid droplet dynamics are widely used in biological and engineering
applications, which contain complex interfacial instabilities and pattern
formulation such as droplet merging, splitting, and transport. This paper
studies a class of mean field control formulation towards these droplet
dynamics. They are used to control and maintain the manipulation of droplets in
applications. We first formulate the droplet dynamics as gradient flows of free
energies in modified optimal transport metrics with nonlinear mobilities. We
then design an optimal control problem for these gradient flows. We lastly
apply the primal-dual hybrid gradient algorithm with high-order finite element
methods to simulate the proposed mean field control problems. Numerical
examples, including droplet formation, bead-up/spreading, transport, and
merging/splitting on a two-dimensional spatial domain, demonstrate the
effectiveness of the proposed mean field control mechanism.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Fri,  9 Feb 24</h3>
<dl>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1812.09413" title="Abstract">arXiv:1812.09413</a> (replaced) [<a href="/pdf/1812.09413" title="Download PDF">pdf</a>, <a href="/ps/1812.09413" title="Download PostScript">ps</a>, <a href="/format/1812.09413" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algorithmic aspects of immersibility and embeddability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Manin%2C+F">Fedor Manin</a>, 
<a href="/search/math?searchtype=author&query=Weinberger%2C+S">Shmuel Weinberger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 1 figure. New version includes additional minor clarifications in section 2
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Geometric Topology (math.GT)</span>; Computational Geometry (cs.CG)

</div>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1903.05896" title="Abstract">arXiv:1903.05896</a> (replaced) [<a href="/pdf/1903.05896" title="Download PDF">pdf</a>, <a href="/format/1903.05896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regular Expressions with Backreferences: Polynomial-Time Matching  Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmid%2C+M+L">Markus L. Schmid</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2102.10019" title="Abstract">arXiv:2102.10019</a> (replaced) [<a href="/pdf/2102.10019" title="Download PDF">pdf</a>, <a href="/format/2102.10019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Disparate Impact of Uncertainty: Affirmative Action vs. Affirmative  Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Reich%2C+C+L">Claire Lazar Reich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Theoretical Economics (econ.TH)

</div>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2103.05674" title="Abstract">arXiv:2103.05674</a> (replaced) [<a href="/pdf/2103.05674" title="Download PDF">pdf</a>, <a href="/format/2103.05674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthesizing Computable Functions from Rational Specifications over  Infinite Words
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Filiot%2C+E">Emmanuel Filiot</a>, 
<a href="/search/cs?searchtype=author&query=Winter%2C+S">Sarah Winter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint of <a href="https://doi.org/10.1142/S012905412348009X">this https URL</a> which is the journal version of <a href="https://doi.org/10.4230/LIPIcs.FSTTCS.2021.43">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2106.04096" title="Abstract">arXiv:2106.04096</a> (replaced) [<a href="/pdf/2106.04096" title="Download PDF">pdf</a>, <a href="/format/2106.04096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear Convergence of Entropy-Regularized Natural Policy Gradient with  Linear Function Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cayci%2C+S">Semih Cayci</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+N">Niao He</a>, 
<a href="/search/cs?searchtype=author&query=Srikant%2C+R">R. Srikant</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2108.04567" title="Abstract">arXiv:2108.04567</a> (replaced) [<a href="/pdf/2108.04567" title="Download PDF">pdf</a>, <a href="/format/2108.04567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust and Dexterous Dual-arm Tele-Cooperation using Adaptable Impedance  Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Babarahmati%2C+K+K">Keyhan Kouhkiloui Babarahmati</a>, 
<a href="/search/cs?searchtype=author&query=Kasaei%2C+M">Mohammadreza Kasaei</a>, 
<a href="/search/cs?searchtype=author&query=Tiseo%2C+C">Carlo Tiseo</a>, 
<a href="/search/cs?searchtype=author&query=Mistry%2C+M">Michael Mistry</a>, 
<a href="/search/cs?searchtype=author&query=Vijayakumar%2C+S">Sethu Vijayakumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.05286" title="Abstract">arXiv:2110.05286</a> (replaced) [<a href="/pdf/2110.05286" title="Download PDF">pdf</a>, <a href="/format/2110.05286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning from Ambiguous Demonstrations with Self-Explanation Guided  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zha%2C+Y">Yantian Zha</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+L">Lin Guan</a>, 
<a href="/search/cs?searchtype=author&query=Kambhampati%2C+S">Subbarao Kambhampati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.03637" title="Abstract">arXiv:2202.03637</a> (replaced) [<a href="/pdf/2202.03637" title="Download PDF">pdf</a>, <a href="/ps/2202.03637" title="Download PostScript">ps</a>, <a href="/format/2202.03637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boolean Observation Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Ditmarsch%2C+H">Hans van Ditmarsch</a>, 
<a href="/search/cs?searchtype=author&query=Simon%2C+S">Sunil Simon</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Artificial Intelligence Research, volume 79, 2024,
  pages 307-357
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.01326" title="Abstract">arXiv:2204.01326</a> (replaced) [<a href="/pdf/2204.01326" title="Download PDF">pdf</a>, <a href="/format/2204.01326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Price Optimal Routing in Public Transportation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Euler%2C+R">Ricardo Euler</a>, 
<a href="/search/math?searchtype=author&query=Lindner%2C+N">Niels Lindner</a>, 
<a href="/search/math?searchtype=author&query=Bornd%C3%B6rfer%2C+R">Ralf Bornd&#xf6;rfer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.07052" title="Abstract">arXiv:2205.07052</a> (replaced) [<a href="/pdf/2205.07052" title="Download PDF">pdf</a>, <a href="/format/2205.07052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> General Framework for Linear Secure Distributed Matrix Multiplication  with Byzantine Servers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Makkonen%2C+O">Okko Makkonen</a>, 
<a href="/search/cs?searchtype=author&query=Hollanti%2C+C">Camilla Hollanti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in the IEEE Transactions on Information Theory
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.10200" title="Abstract">arXiv:2205.10200</a> (replaced) [<a href="/pdf/2205.10200" title="Download PDF">pdf</a>, <a href="/format/2205.10200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Fairness of Credit Scoring Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hurlin%2C+C">Christophe Hurlin</a>, 
<a href="/search/stat?searchtype=author&query=P%C3%A9rignon%2C+C">Christophe P&#xe9;rignon</a>, 
<a href="/search/stat?searchtype=author&query=Saurin%2C+S">S&#xe9;bastien Saurin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Risk Management (q-fin.RM)

</div>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.13147" title="Abstract">arXiv:2205.13147</a> (replaced) [<a href="/pdf/2205.13147" title="Download PDF">pdf</a>, <a href="/format/2205.13147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Matryoshka Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kusupati%2C+A">Aditya Kusupati</a>, 
<a href="/search/cs?searchtype=author&query=Bhatt%2C+G">Gantavya Bhatt</a>, 
<a href="/search/cs?searchtype=author&query=Rege%2C+A">Aniket Rege</a>, 
<a href="/search/cs?searchtype=author&query=Wallingford%2C+M">Matthew Wallingford</a>, 
<a href="/search/cs?searchtype=author&query=Sinha%2C+A">Aditya Sinha</a>, 
<a href="/search/cs?searchtype=author&query=Ramanujan%2C+V">Vivek Ramanujan</a>, 
<a href="/search/cs?searchtype=author&query=Howard-Snyder%2C+W">William Howard-Snyder</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kaifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Kakade%2C+S">Sham Kakade</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+P">Prateek Jain</a>, 
<a href="/search/cs?searchtype=author&query=Farhadi%2C+A">Ali Farhadi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Edited related work to include intrinsic dimensionality works
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.04950" title="Abstract">arXiv:2207.04950</a> (replaced) [<a href="/pdf/2207.04950" title="Download PDF">pdf</a>, <a href="/ps/2207.04950" title="Download PostScript">ps</a>, <a href="/format/2207.04950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural and spectral operator surrogates: unified construction and  expression rate bounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Herrmann%2C+L">Lukas Herrmann</a>, 
<a href="/search/math?searchtype=author&query=Schwab%2C+C">Christoph Schwab</a>, 
<a href="/search/math?searchtype=author&query=Zech%2C+J">Jakob Zech</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.08041" title="Abstract">arXiv:2207.08041</a> (replaced) [<a href="/pdf/2207.08041" title="Download PDF">pdf</a>, <a href="/format/2207.08041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalized PCA: Decoupling Shared and Unique Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+N">Naichen Shi</a>, 
<a href="/search/cs?searchtype=author&query=Kontar%2C+R+A">Raed Al Kontar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.04508" title="Abstract">arXiv:2208.04508</a> (replaced) [<a href="/pdf/2208.04508" title="Download PDF">pdf</a>, <a href="/ps/2208.04508" title="Download PostScript">ps</a>, <a href="/format/2208.04508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Overparametrized Neural Networks in Sublinear Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yichuan Deng</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhao Song</a>, 
<a href="/search/cs?searchtype=author&query=Weinstein%2C+O">Omri Weinstein</a>, 
<a href="/search/cs?searchtype=author&query=Zhuo%2C+D">Danyang Zhuo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.04850" title="Abstract">arXiv:2208.04850</a> (replaced) [<a href="/pdf/2208.04850" title="Download PDF">pdf</a>, <a href="/format/2208.04850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evolving finite elements for advection diffusion with an evolving  interface
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Elliott%2C+C+M">C. M. Elliott</a>, 
<a href="/search/math?searchtype=author&query=Ranner%2C+T">T. Ranner</a>, 
<a href="/search/math?searchtype=author&query=Stepanov%2C+P">P. Stepanov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 5 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.07996" title="Abstract">arXiv:2210.07996</a> (replaced) [<a href="/pdf/2210.07996" title="Download PDF">pdf</a>, <a href="/ps/2210.07996" title="Download PostScript">ps</a>, <a href="/format/2210.07996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Degeneracy is OK: Logarithmic Regret for Network Revenue Management with  Indiscrete Distributions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jiashuo Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Will Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiawei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.08659" title="Abstract">arXiv:2210.08659</a> (replaced) [<a href="/pdf/2210.08659" title="Download PDF">pdf</a>, <a href="/ps/2210.08659" title="Download PostScript">ps</a>, <a href="/format/2210.08659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards More Efficient Shared Autonomous Mobility: A Learning-Based  Fleet Repositioning Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Filipovska%2C+M">Monika Filipovska</a>, 
<a href="/search/eess?searchtype=author&query=Hyland%2C+M">Michael Hyland</a>, 
<a href="/search/eess?searchtype=author&query=Bala%2C+H">Haimanti Bala</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.00538" title="Abstract">arXiv:2211.00538</a> (replaced) [<a href="/pdf/2211.00538" title="Download PDF">pdf</a>, <a href="/format/2211.00538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reducing Two-Way Ranging Variance by Signal-Timing Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Shalaby%2C+M+A">Mohammed Ayman Shalaby</a>, 
<a href="/search/eess?searchtype=author&query=Cossette%2C+C+C">Charles Champagne Cossette</a>, 
<a href="/search/eess?searchtype=author&query=Forbes%2C+J+R">James Richard Forbes</a>, 
<a href="/search/eess?searchtype=author&query=Ny%2C+J+L">Jerome Le Ny</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, accepted for publication at IEEE Transactions on Aerospace and Electronic Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.12829" title="Abstract">arXiv:2211.12829</a> (replaced) [<a href="/pdf/2211.12829" title="Download PDF">pdf</a>, <a href="/format/2211.12829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised 3D Keypoint Discovery with Multi-View Geometry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Honari%2C+S">Sina Honari</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Salzmann%2C+M">Mathieu Salzmann</a>, 
<a href="/search/cs?searchtype=author&query=Fua%2C+P">Pascal Fua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in "3DV 2024"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.00773" title="Abstract">arXiv:2212.00773</a> (replaced) [<a href="/pdf/2212.00773" title="Download PDF">pdf</a>, <a href="/format/2212.00773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FakeOut: Leveraging Out-of-domain Self-supervision for Multi-modal Video  Deepfake Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Knafo%2C+G">Gil Knafo</a>, 
<a href="/search/cs?searchtype=author&query=Fried%2C+O">Ohad Fried</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.06905" title="Abstract">arXiv:2212.06905</a> (replaced) [<a href="/pdf/2212.06905" title="Download PDF">pdf</a>, <a href="/format/2212.06905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Query Time Optimized Deep Learning Based Video Inference System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+M">Mingren Shen</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+S">Shuoxuan Dong</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiuyuan He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.07180" title="Abstract">arXiv:2212.07180</a> (replaced) [<a href="/pdf/2212.07180" title="Download PDF">pdf</a>, <a href="/format/2212.07180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rainbow variations on a theme by Mantel: extremal problems for Gallai  colouring templates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Falgas-Ravry%2C+V">Victor Falgas-Ravry</a>, 
<a href="/search/math?searchtype=author&query=Markstr%C3%B6m%2C+K">Klas Markstr&#xf6;m</a>, 
<a href="/search/math?searchtype=author&query=R%C3%A4ty%2C+E">Eero R&#xe4;ty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The original version of this paper has been split into two papers
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.00712" title="Abstract">arXiv:2301.00712</a> (replaced) [<a href="/pdf/2301.00712" title="Download PDF">pdf</a>, <a href="/ps/2301.00712" title="Download PostScript">ps</a>, <a href="/format/2301.00712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Finding Small Hyper-Gradients in Bilevel Optimization: Hardness  Results and Improved Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+L">Lesi Chen</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+J">Jing Xu</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+J">Jingzhao Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Add new upper bounds of nonconvex-PL bilevel problems compared to version 1 in 2023.1
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.09109" title="Abstract">arXiv:2301.09109</a> (replaced) [<a href="/pdf/2301.09109" title="Download PDF">pdf</a>, <a href="/format/2301.09109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Recommendation with Additive Personalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+G">Guodong Long</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.09734" title="Abstract">arXiv:2301.09734</a> (replaced) [<a href="/pdf/2301.09734" title="Download PDF">pdf</a>, <a href="/format/2301.09734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topological Learning in Multi-Class Data Sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Griffin%2C+C">Christopher Griffin</a>, 
<a href="/search/cs?searchtype=author&query=Karn%2C+T">Trevor Karn</a>, 
<a href="/search/cs?searchtype=author&query=Apple%2C+B">Benjamin Apple</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 18 figures. This is a revision of v3 that fixes a typo on Page 2 that resulted in an incorrect equation that did not match the description around it
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Analysis, Statistics and Probability (physics.data-an)

</div>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11048" title="Abstract">arXiv:2301.11048</a> (replaced) [<a href="/pdf/2301.11048" title="Download PDF">pdf</a>, <a href="/ps/2301.11048" title="Download PostScript">ps</a>, <a href="/format/2301.11048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decidability of well quasi-order and atomicity for equivalence relations  under embedding orderings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ironmonger%2C+V">V. Ironmonger</a>, 
<a href="/search/math?searchtype=author&query=Ruskuc%2C+N">N. Ruskuc</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11270" title="Abstract">arXiv:2301.11270</a> (replaced) [<a href="/pdf/2301.11270" title="Download PDF">pdf</a>, <a href="/format/2301.11270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Principled Reinforcement Learning with Human Feedback from Pairwise or  $K$-wise Comparisons
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Banghua Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+J">Jiantao Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Jordan%2C+M+I">Michael I. Jordan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02104" title="Abstract">arXiv:2302.02104</a> (replaced) [<a href="/pdf/2302.02104" title="Download PDF">pdf</a>, <a href="/format/2302.02104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HardSATGEN: Understanding the Difficulty of Hard SAT Formula Generation  and A Strong Structure-Hardness-Aware Baseline
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+W">Wenxuan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xijun Li</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+W">Wanqian Luo</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Junhua Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhen%2C+H">Hui-Ling Zhen</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+M">Mingxuan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at SIGKDD 2023, see <a href="http://dl.acm.org/doi/10.1145/3580305.3599837">this http URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02809" title="Abstract">arXiv:2302.02809</a> (replaced) [<a href="/pdf/2302.02809" title="Download PDF">pdf</a>, <a href="/format/2302.02809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Listen2Scene: Interactive material-aware binaural sound propagation for  reconstructed 3D scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ratnarajah%2C+A">Anton Ratnarajah</a>, 
<a href="/search/eess?searchtype=author&query=Manocha%2C+D">Dinesh Manocha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE VR 2024. Project page: <a href="https://anton-jeran.github.io/Listen2Scene/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Multimedia (cs.MM); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03497" title="Abstract">arXiv:2302.03497</a> (replaced) [<a href="/pdf/2302.03497" title="Download PDF">pdf</a>, <a href="/format/2302.03497" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MMRec: Simplifying Multimodal Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xin Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at MMAsia'23 Workshops
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04086" title="Abstract">arXiv:2302.04086</a> (replaced) [<a href="/pdf/2302.04086" title="Download PDF">pdf</a>, <a href="/format/2302.04086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Block Diagonalization of Quaternion Circulant Matrices with Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Pan%2C+J">Junjun Pan</a>, 
<a href="/search/math?searchtype=author&query=Ng%2C+M+K">Michael K. Ng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.05652" title="Abstract">arXiv:2302.05652</a> (replaced) [<a href="/pdf/2302.05652" title="Download PDF">pdf</a>, <a href="/format/2302.05652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Structural and Spectral Properties of Distance Magic Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Mukherjee%2C+H">Himadri Mukherjee</a>, 
<a href="/search/math?searchtype=author&query=Pawar%2C+R">Ravindra Pawar</a>, 
<a href="/search/math?searchtype=author&query=Singh%2C+T">Tarkeshwar Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.10786" title="Abstract">arXiv:2302.10786</a> (replaced) [<a href="/pdf/2302.10786" title="Download PDF">pdf</a>, <a href="/format/2302.10786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-World Deployment and Evaluation of Kwame for Science, An AI  Teaching Assistant for Science Education in West Africa
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boateng%2C+G">George Boateng</a>, 
<a href="/search/cs?searchtype=author&query=John%2C+S">Samuel John</a>, 
<a href="/search/cs?searchtype=author&query=Boateng%2C+S">Samuel Boateng</a>, 
<a href="/search/cs?searchtype=author&query=Badu%2C+P">Philemon Badu</a>, 
<a href="/search/cs?searchtype=author&query=Agyeman-Budu%2C+P">Patrick Agyeman-Budu</a>, 
<a href="/search/cs?searchtype=author&query=Kumbol%2C+V">Victor Kumbol</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, under review at International Conference on Artificial Intelligence in Education (AIED 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01213" title="Abstract">arXiv:2303.01213</a> (replaced) [<a href="/pdf/2303.01213" title="Download PDF">pdf</a>, <a href="/format/2303.01213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DSD$^2$: Can We Dodge Sparse Double Descent and Compress the Neural  Network Worry-Free?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qu%C3%A9tu%2C+V">Victor Qu&#xe9;tu</a>, 
<a href="/search/cs?searchtype=author&query=Tartaglione%2C+E">Enzo Tartaglione</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.04442" title="Abstract">arXiv:2303.04442</a> (replaced) [<a href="/pdf/2303.04442" title="Download PDF">pdf</a>, <a href="/format/2303.04442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aczel-Mendler Bisimulations in a Regular Category
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dubut%2C+J">Jeremy Dubut</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submission to the CALCO 2023 special LMCS issue
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10555" title="Abstract">arXiv:2303.10555</a> (replaced) [<a href="/pdf/2303.10555" title="Download PDF">pdf</a>, <a href="/format/2303.10555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LiDAR Spoofing Meets the New-Gen: Capability Improvements, Broken  Assumptions, and New Attack Strategies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sato%2C+T">Takami Sato</a>, 
<a href="/search/cs?searchtype=author&query=Hayakawa%2C+Y">Yuki Hayakawa</a>, 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+R">Ryo Suzuki</a>, 
<a href="/search/cs?searchtype=author&query=Shiiki%2C+Y">Yohsuke Shiiki</a>, 
<a href="/search/cs?searchtype=author&query=Yoshioka%2C+K">Kentaro Yoshioka</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q+A">Qi Alfred Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first 3 authors are co-first
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Network and Distributed System Security (NDSS) Symposium 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.14535" title="Abstract">arXiv:2303.14535</a> (replaced) [<a href="/pdf/2303.14535" title="Download PDF">pdf</a>, <a href="/format/2303.14535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EfficientAD: Accurate Visual Anomaly Detection at Millisecond-Level  Latencies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Batzner%2C+K">Kilian Batzner</a>, 
<a href="/search/cs?searchtype=author&query=Heckler%2C+L">Lars Heckler</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6nig%2C+R">Rebecca K&#xf6;nig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as Oral to WACV 2024
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV), 2024, pp. 128-138
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01596" title="Abstract">arXiv:2304.01596</a> (replaced) [<a href="/pdf/2304.01596" title="Download PDF">pdf</a>, <a href="/ps/2304.01596" title="Download PostScript">ps</a>, <a href="/format/2304.01596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prevalence of prejudice-denoting terms in news media worldwide
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rozado%2C+D">David Rozado</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06035" title="Abstract">arXiv:2304.06035</a> (replaced) [<a href="/pdf/2304.06035" title="Download PDF">pdf</a>, <a href="/format/2304.06035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Choose Your Weapon: Survival Strategies for Depressed AI Academics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Togelius%2C+J">Julian Togelius</a>, 
<a href="/search/cs?searchtype=author&query=Yannakakis%2C+G+N">Georgios N. Yannakakis</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the IEEE, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Other Computer Science (cs.OH)</span>; Computers and Society (cs.CY); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07896" title="Abstract">arXiv:2304.07896</a> (replaced) [<a href="/pdf/2304.07896" title="Download PDF">pdf</a>, <a href="/format/2304.07896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Out-of-Variable Generalization for Discriminative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Siyuan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wildberger%2C+J">Jonas Wildberger</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10557" title="Abstract">arXiv:2304.10557</a> (replaced) [<a href="/pdf/2304.10557" title="Download PDF">pdf</a>, <a href="/format/2304.10557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Introduction to Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Turner%2C+R+E">Richard E. Turner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.13105" title="Abstract">arXiv:2304.13105</a> (replaced) [<a href="/pdf/2304.13105" title="Download PDF">pdf</a>, <a href="/ps/2304.13105" title="Download PostScript">ps</a>, <a href="/format/2304.13105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention-Enhanced Deep Learning for Device-Free Through-the-Wall  Presence Detection Using Indoor WiFi Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li-Hsiang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Hsiao%2C+A">An-Hung Hsiao</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+K">Kuan-I Lu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+K">Kai-Ten Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Sensors Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01301" title="Abstract">arXiv:2305.01301</a> (replaced) [<a href="/pdf/2305.01301" title="Download PDF">pdf</a>, <a href="/format/2305.01301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Analysis of Quantum CSS Error-Correcting Codes via  MacWilliams Identities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Forlivesi%2C+D">Diego Forlivesi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Valentini%2C+L">Lorenzo Valentini</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chiani%2C+M">Marco Chiani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 7 figures, submitted to a journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03459" title="Abstract">arXiv:2305.03459</a> (replaced) [<a href="/pdf/2305.03459" title="Download PDF">pdf</a>, <a href="/format/2305.03459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Phase Transitions of the Price-of-Anarchy Function in Multi-Commodity  Routing Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cominetti%2C+R">Roberto Cominetti</a>, 
<a href="/search/cs?searchtype=author&query=Dose%2C+V">Valerio Dose</a>, 
<a href="/search/cs?searchtype=author&query=Scarsini%2C+M">Marco Scarsini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04359" title="Abstract">arXiv:2305.04359</a> (replaced) [<a href="/pdf/2305.04359" title="Download PDF">pdf</a>, <a href="/format/2305.04359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ParlayANN: Scalable and Deterministic Parallel Graph-Based Approximate  Nearest Neighbor Search Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manohar%2C+M+D">Magdalen Dobson Manohar</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zheqi Shen</a>, 
<a href="/search/cs?searchtype=author&query=Blelloch%2C+G+E">Guy E. Blelloch</a>, 
<a href="/search/cs?searchtype=author&query=Dhulipala%2C+L">Laxman Dhulipala</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yan Gu</a>, 
<a href="/search/cs?searchtype=author&query=Simhadri%2C+H+V">Harsha Vardhan Simhadri</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yihan Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05257" title="Abstract">arXiv:2305.05257</a> (replaced) [<a href="/pdf/2305.05257" title="Download PDF">pdf</a>, <a href="/format/2305.05257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Survey of Federated Learning Models for Spatial-Temporal Mobility  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belal%2C+Y">Yacine Belal</a>, 
<a href="/search/cs?searchtype=author&query=Mokhtar%2C+S+B">Sonia Ben Mokhtar</a>, 
<a href="/search/cs?searchtype=author&query=Haddadi%2C+H">Hamed Haddadi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jaron Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mashhadi%2C+A">Afra Mashhadi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Information Retrieval (cs.IR); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05772" title="Abstract">arXiv:2305.05772</a> (replaced) [<a href="/pdf/2305.05772" title="Download PDF">pdf</a>, <a href="/format/2305.05772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spiking Neural Networks in the Alexiewicz Topology: A New Perspective on  Analysis and Error Bounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moser%2C+B+A">Bernhard A. Moser</a>, 
<a href="/search/cs?searchtype=author&query=Lunglmayr%2C+M">Michael Lunglmayr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Discrete Mathematics (cs.DM); Signal Processing (eess.SP); Metric Geometry (math.MG)

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05970" title="Abstract">arXiv:2305.05970</a> (replaced) [<a href="/pdf/2305.05970" title="Download PDF">pdf</a>, <a href="/format/2305.05970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FusionBooster: A Unified Image Fusion Boosting Paradigm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+C">Chunyang Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tianyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiao-Jun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hui Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xi Li</a>, 
<a href="/search/cs?searchtype=author&query=Kittler%2C+J">Josef Kittler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages; v2, including the code repository
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06344" title="Abstract">arXiv:2305.06344</a> (replaced) [<a href="/pdf/2305.06344" title="Download PDF">pdf</a>, <a href="/format/2305.06344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Orthogonal Transforms in Neural Networks Amount to Effective  Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zaj%C4%85c%2C+K">Krzysztof Zaj&#x105;c</a>, 
<a href="/search/cs?searchtype=author&query=Sopot%2C+W">Wojciech Sopot</a>, 
<a href="/search/cs?searchtype=author&query=Wachel%2C+P">Pawe&#x142; Wachel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06927" title="Abstract">arXiv:2305.06927</a> (replaced) [<a href="/pdf/2305.06927" title="Download PDF">pdf</a>, <a href="/format/2305.06927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence of Alternating Gradient Descent for Matrix Factorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ward%2C+R">Rachel Ward</a>, 
<a href="/search/cs?searchtype=author&query=Kolda%2C+T+G">Tamara G. Kolda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07303" title="Abstract">arXiv:2305.07303</a> (replaced) [<a href="/pdf/2305.07303" title="Download PDF">pdf</a>, <a href="/format/2305.07303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Relational Hyperbolic Word Embeddings from Natural Language  Definitions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Valentino%2C+M">Marco Valentino</a>, 
<a href="/search/cs?searchtype=author&query=Carvalho%2C+D+S">Danilo S. Carvalho</a>, 
<a href="/search/cs?searchtype=author&query=Freitas%2C+A">Andr&#xe9; Freitas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 18th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2024), camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08012" title="Abstract">arXiv:2305.08012</a> (replaced) [<a href="/pdf/2305.08012" title="Download PDF">pdf</a>, <a href="/format/2305.08012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantization in Spiking Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moser%2C+B+A">Bernhard A. Moser</a>, 
<a href="/search/cs?searchtype=author&query=Lunglmayr%2C+M">Michael Lunglmayr</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2305.05772">arXiv:2305.05772</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Discrete Mathematics (cs.DM); Emerging Technologies (cs.ET)

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12392" title="Abstract">arXiv:2305.12392</a> (replaced) [<a href="/pdf/2305.12392" title="Download PDF">pdf</a>, <a href="/format/2305.12392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PiVe: Prompting with Iterative Verification Improving Graph-based  Generative Capability of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiuzhou Han</a>, 
<a href="/search/cs?searchtype=author&query=Collier%2C+N">Nigel Collier</a>, 
<a href="/search/cs?searchtype=author&query=Buntine%2C+W">Wray Buntine</a>, 
<a href="/search/cs?searchtype=author&query=Shareghi%2C+E">Ehsan Shareghi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Our code and data is at <a href="https://github.com/Jiuzhouh/PiVe">this https URL</a> (Added results for GPT-4, with new experiments on larger set of few-shot numbers and diverse prompting approaches.)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12815" title="Abstract">arXiv:2305.12815</a> (replaced) [<a href="/pdf/2305.12815" title="Download PDF">pdf</a>, <a href="/format/2305.12815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Agency of LLMs in Human-AI Collaboration Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Ashish Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+S">Sudha Rao</a>, 
<a href="/search/cs?searchtype=author&query=Brockett%2C+C">Chris Brockett</a>, 
<a href="/search/cs?searchtype=author&query=Malhotra%2C+A">Akanksha Malhotra</a>, 
<a href="/search/cs?searchtype=author&query=Jojic%2C+N">Nebojsa Jojic</a>, 
<a href="/search/cs?searchtype=author&query=Dolan%2C+B">Bill Dolan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14405" title="Abstract">arXiv:2305.14405</a> (replaced) [<a href="/pdf/2305.14405" title="Download PDF">pdf</a>, <a href="/format/2305.14405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeuralMatrix: Compute the Entire Neural Networks with Linear Matrix  Operations for Efficient Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+R">Ruiqi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+S">Siwei Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jie Zhao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xin He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yiran Li</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+A">An Zou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 6figures, Submitted to 41st International Conference on Machine Learning
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR)

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19510" title="Abstract">arXiv:2305.19510</a> (replaced) [<a href="/pdf/2305.19510" title="Download PDF">pdf</a>, <a href="/format/2305.19510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mildly Overparameterized ReLU Networks Have a Favorable Loss Landscape
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karhadkar%2C+K">Kedar Karhadkar</a>, 
<a href="/search/cs?searchtype=author&query=Murray%2C+M">Michael Murray</a>, 
<a href="/search/cs?searchtype=author&query=Tseran%2C+H">Hanna Tseran</a>, 
<a href="/search/cs?searchtype=author&query=Mont%C3%BAfar%2C+G">Guido Mont&#xfa;far</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Combinatorics (math.CO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19838" title="Abstract">arXiv:2305.19838</a> (replaced) [<a href="/pdf/2305.19838" title="Download PDF">pdf</a>, <a href="/format/2305.19838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relaxing the Additivity Constraints in Decentralized No-Regret  High-Dimensional Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bardou%2C+A">Anthony Bardou</a>, 
<a href="/search/cs?searchtype=author&query=Thiran%2C+P">Patrick Thiran</a>, 
<a href="/search/cs?searchtype=author&query=Begin%2C+T">Thomas Begin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01710" title="Abstract">arXiv:2306.01710</a> (replaced) [<a href="/pdf/2306.01710" title="Download PDF">pdf</a>, <a href="/format/2306.01710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Data-Driven Measure of Relative Uncertainty for Misclassification  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Dadalto%2C+E">Eduardo Dadalto</a>, 
<a href="/search/stat?searchtype=author&query=Romanelli%2C+M">Marco Romanelli</a>, 
<a href="/search/stat?searchtype=author&query=Pichler%2C+G">Georg Pichler</a>, 
<a href="/search/stat?searchtype=author&query=Piantanida%2C+P">Pablo Piantanida</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ICLR2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02574" title="Abstract">arXiv:2306.02574</a> (replaced) [<a href="/pdf/2306.02574" title="Download PDF">pdf</a>, <a href="/format/2306.02574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Learning of Optimal Policies in Markov Decision Processes with  Countably Infinite State-Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Adler%2C+S">Saghar Adler</a>, 
<a href="/search/eess?searchtype=author&query=Subramanian%2C+V">Vijay Subramanian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03622" title="Abstract">arXiv:2306.03622</a> (replaced) [<a href="/pdf/2306.03622" title="Download PDF">pdf</a>, <a href="/format/2306.03622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FaaSwap: SLO-Aware, GPU-Efficient Serverless Inference via Model  Swapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+M">Minchen Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">Ao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Haoxuan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xiaonan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhuohao Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ruichuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+D">Dapeng Nie</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haoran Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05957" title="Abstract">arXiv:2306.05957</a> (replaced) [<a href="/pdf/2306.05957" title="Download PDF">pdf</a>, <a href="/format/2306.05957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic  Latent Particles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Daniel%2C+T">Tal Daniel</a>, 
<a href="/search/cs?searchtype=author&query=Tamar%2C+A">Aviv Tamar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> TMLR 2024. Project site: <a href="https://taldatech.github.io/ddlp-web">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07701" title="Abstract">arXiv:2306.07701</a> (replaced) [<a href="/pdf/2306.07701" title="Download PDF">pdf</a>, <a href="/format/2306.07701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Particle simulation methods for the Landau-Fokker-Planck equation with  uncertain data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Medaglia%2C+A">Andrea Medaglia</a>, 
<a href="/search/math?searchtype=author&query=Pareschi%2C+L">Lorenzo Pareschi</a>, 
<a href="/search/math?searchtype=author&query=Zanella%2C+M">Mattia Zanella</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Physics (physics.comp-ph); Plasma Physics (physics.plasm-ph)

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07778" title="Abstract">arXiv:2306.07778</a> (replaced) [<a href="/pdf/2306.07778" title="Download PDF">pdf</a>, <a href="/format/2306.07778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NetGAP: A Graph-Grammar approach for concept design of networked  platforms with extra-functional requirements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Moraes%2C+R+S">Rodrigo Saar de Moraes</a>, 
<a href="/search/cs?searchtype=author&query=Nadjm-Tehrani%2C+S">Simin Nadjm-Tehrani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10309" title="Abstract">arXiv:2306.10309</a> (replaced) [<a href="/pdf/2306.10309" title="Download PDF">pdf</a>, <a href="/format/2306.10309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Edge Learning for 6G-enabled Internet of Things: A Comprehensive Survey  of Vulnerabilities, Datasets, and Defenses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferrag%2C+M+A">Mohamed Amine Ferrag</a>, 
<a href="/search/cs?searchtype=author&query=Friha%2C+O">Othmane Friha</a>, 
<a href="/search/cs?searchtype=author&query=Kantarci%2C+B">Burak Kantarci</a>, 
<a href="/search/cs?searchtype=author&query=Tihanyi%2C+N">Norbert Tihanyi</a>, 
<a href="/search/cs?searchtype=author&query=Cordeiro%2C+L">Lucas Cordeiro</a>, 
<a href="/search/cs?searchtype=author&query=Debbah%2C+M">Merouane Debbah</a>, 
<a href="/search/cs?searchtype=author&query=Hamouda%2C+D">Djallel Hamouda</a>, 
<a href="/search/cs?searchtype=author&query=Al-Hawawreh%2C+M">Muna Al-Hawawreh</a>, 
<a href="/search/cs?searchtype=author&query=Choo%2C+K+R">Kim-Kwang Raymond Choo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for publication in IEEE Communications Surveys \&amp; Tutorials
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11029" title="Abstract">arXiv:2306.11029</a> (replaced) [<a href="/pdf/2306.11029" title="Download PDF">pdf</a>, <a href="/format/2306.11029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RemoteCLIP: A Vision Language Foundation Model for Remote Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Delong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+Z">Zhangqingyun Guan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiaocong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiale Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qiaolin Ye</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+L">Liyong Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jun Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11226" title="Abstract">arXiv:2306.11226</a> (replaced) [<a href="/pdf/2306.11226" title="Download PDF">pdf</a>, <a href="/format/2306.11226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Fault-Tolerant Spanners in Euclidean and Doubling Metrics:  Breaking the $&#x3a9;(\log n)$ Lightness Barrier
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le%2C+H">Hung Le</a>, 
<a href="/search/cs?searchtype=author&query=Solomon%2C+S">Shay Solomon</a>, 
<a href="/search/cs?searchtype=author&query=Than%2C+C">Cuong Than</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Abstract is shortened to meet arxiv's requirement on the number of characters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13149" title="Abstract">arXiv:2306.13149</a> (replaced) [<a href="/pdf/2306.13149" title="Download PDF">pdf</a>, <a href="/format/2306.13149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;Filling the Blanks&#x27;&#x27;: Identifying Micro-activities that Compose Complex  Human Activities of Daily Living
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+S">Soumyajit Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+B">Bivas Mitra</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Sandip Chakraborty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 4 tables, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14263" title="Abstract">arXiv:2306.14263</a> (replaced) [<a href="/pdf/2306.14263" title="Download PDF">pdf</a>, <a href="/format/2306.14263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revolutionizing Cyber Threat Detection with Large Language Models: A  privacy-preserving BERT-based Lightweight Model for IoT/IIoT Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferrag%2C+M+A">Mohamed Amine Ferrag</a>, 
<a href="/search/cs?searchtype=author&query=Ndhlovu%2C+M">Mthandazo Ndhlovu</a>, 
<a href="/search/cs?searchtype=author&query=Tihanyi%2C+N">Norbert Tihanyi</a>, 
<a href="/search/cs?searchtype=author&query=Cordeiro%2C+L+C">Lucas C. Cordeiro</a>, 
<a href="/search/cs?searchtype=author&query=Debbah%2C+M">Merouane Debbah</a>, 
<a href="/search/cs?searchtype=author&query=Lestable%2C+T">Thierry Lestable</a>, 
<a href="/search/cs?searchtype=author&query=Thandi%2C+N+S">Narinderjit Singh Thandi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for publication in IEEE Access: <a href="http://dx.doi.org/10.1109/ACCESS.2024.3363469">this http URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01189" title="Abstract">arXiv:2307.01189</a> (replaced) [<a href="/pdf/2307.01189" title="Download PDF">pdf</a>, <a href="/format/2307.01189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trainable Transformer in Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Panigrahi%2C+A">Abhishek Panigrahi</a>, 
<a href="/search/cs?searchtype=author&query=Malladi%2C+S">Sadhika Malladi</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+M">Mengzhou Xia</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+S">Sanjeev Arora</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code base: <a href="https://github.com/abhishekpanigrahi1996/transformer_in_transformer">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01482" title="Abstract">arXiv:2307.01482</a> (replaced) [<a href="/pdf/2307.01482" title="Download PDF">pdf</a>, <a href="/format/2307.01482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contextualizing MLP-Mixers Spatiotemporally for Urban Data Forecast at  Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nie%2C+T">Tong Nie</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+G">Guoyang Qin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lijun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Wei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+Y">Yu Mei</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jian Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03177" title="Abstract">arXiv:2307.03177</a> (replaced) [<a href="/pdf/2307.03177" title="Download PDF">pdf</a>, <a href="/format/2307.03177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PanoDiffusion: 360-degree Panorama Outpainting via Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tianhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chuanxia Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Cham%2C+T">Tat-Jen Cham</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://sm0kywu.github.io/panodiffusion/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03748" title="Abstract">arXiv:2307.03748</a> (replaced) [<a href="/pdf/2307.03748" title="Download PDF">pdf</a>, <a href="/format/2307.03748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incentive-Theoretic Bayesian Inference for Collaborative Science
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Bates%2C+S">Stephen Bates</a>, 
<a href="/search/stat?searchtype=author&query=Jordan%2C+M+I">Michael I. Jordan</a>, 
<a href="/search/stat?searchtype=author&query=Sklar%2C+M">Michael Sklar</a>, 
<a href="/search/stat?searchtype=author&query=Soloff%2C+J+A">Jake A. Soloff</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04191" title="Abstract">arXiv:2307.04191</a> (replaced) [<a href="/pdf/2307.04191" title="Download PDF">pdf</a>, <a href="/ps/2307.04191" title="Download PostScript">ps</a>, <a href="/format/2307.04191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the sample complexity of parameter estimation in logistic regression  with normal design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hsu%2C+D">Daniel Hsu</a>, 
<a href="/search/math?searchtype=author&query=Mazumdar%2C+A">Arya Mazumdar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Information Theory (cs.IT); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04725" title="Abstract">arXiv:2307.04725</a> (replaced) [<a href="/pdf/2307.04725" title="Download PDF">pdf</a>, <a href="/format/2307.04725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models  without Specific Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yuwei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Ceyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+A">Anyi Rao</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Z">Zhengyang Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaohui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Agrawala%2C+M">Maneesh Agrawala</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+B">Bo Dai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Codes and Supplementary Material: <a href="https://github.com/guoyww/AnimateDiff">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04804" title="Abstract">arXiv:2307.04804</a> (replaced) [<a href="/pdf/2307.04804" title="Download PDF">pdf</a>, <a href="/format/2307.04804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> S2vNTM: Semi-supervised vMF Neural Topic Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weijie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Desai%2C+J">Jay Desai</a>, 
<a href="/search/cs?searchtype=author&query=Sengamedu%2C+S">Srinivasan Sengamedu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xiaoyu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Iannacci%2C+F">Francis Iannacci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 9 figures, ICLR Workshop 2023. arXiv admin note: text overlap with <a href="/abs/2307.01226">arXiv:2307.01226</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICLR Workshop 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07084" title="Abstract">arXiv:2307.07084</a> (replaced) [<a href="/pdf/2307.07084" title="Download PDF">pdf</a>, <a href="/format/2307.07084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safe Reinforcement Learning as Wasserstein Variational Inference: Formal  Methods for Interpretability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Boyle%2C+D">David Boyle</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 6 figures, containing Appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11422" title="Abstract">arXiv:2307.11422</a> (replaced) [<a href="/pdf/2307.11422" title="Download PDF">pdf</a>, <a href="/ps/2307.11422" title="Download PostScript">ps</a>, <a href="/format/2307.11422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A pressure-free long-time stable reduced-order model for two-dimensional  Rayleigh-B&#xe9;nard convection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Chand%2C+K">Krishan Chand</a>, 
<a href="/search/physics?searchtype=author&query=Rosenberger%2C+H">Henrik Rosenberger</a>, 
<a href="/search/physics?searchtype=author&query=Sanderse%2C+B">Benjamin Sanderse</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Chaos 34, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12971" title="Abstract">arXiv:2307.12971</a> (replaced) [<a href="/pdf/2307.12971" title="Download PDF">pdf</a>, <a href="/format/2307.12971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Big Data - Supply Chain Management Framework for Forecasting: Data  Preprocessing and Machine Learning Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jahin%2C+M+A">Md Abrar Jahin</a>, 
<a href="/search/cs?searchtype=author&query=Shovon%2C+M+S+H">Md Sakib Hossain Shovon</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jungpil Shin</a>, 
<a href="/search/cs?searchtype=author&query=Ridoy%2C+I+A">Istiyaque Ahmed Ridoy</a>, 
<a href="/search/cs?searchtype=author&query=Mridha%2C+M+F">M. F. Mridha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16096" title="Abstract">arXiv:2307.16096</a> (replaced) [<a href="/pdf/2307.16096" title="Download PDF">pdf</a>, <a href="/ps/2307.16096" title="Download PostScript">ps</a>, <a href="/format/2307.16096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> D-STAR: Dual Simultaneously Transmitting and Reflecting Reconfigurable  Intelligent Surfaces for Joint Uplink/Downlink Transmission
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li-Hsiang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Po-Chen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ku%2C+C">Chia-Jou Ku</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yu-Ting Li</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+K">Kai-Ten Feng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuanwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hanzo%2C+L">Lajos Hanzo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE TCOM
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16704" title="Abstract">arXiv:2307.16704</a> (replaced) [<a href="/pdf/2307.16704" title="Download PDF">pdf</a>, <a href="/format/2307.16704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lookbehind-SAM: k steps back, 1 step forward
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mordido%2C+G">Gon&#xe7;alo Mordido</a>, 
<a href="/search/cs?searchtype=author&query=Malviya%2C+P">Pranshu Malviya</a>, 
<a href="/search/cs?searchtype=author&query=Baratin%2C+A">Aristide Baratin</a>, 
<a href="/search/cs?searchtype=author&query=Chandar%2C+S">Sarath Chandar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00031" title="Abstract">arXiv:2308.00031</a> (replaced) [<a href="/pdf/2308.00031" title="Download PDF">pdf</a>, <a href="/format/2308.00031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning for Generative AI: State of the Art,  Opportunities and Open Research Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Franceschelli%2C+G">Giorgio Franceschelli</a>, 
<a href="/search/cs?searchtype=author&query=Musolesi%2C+M">Mirco Musolesi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in JAIR at <a href="https://www.jair.org/index.php/jair/article/view/15278">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> JAIR 79 (2024) 417-446
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00198" title="Abstract">arXiv:2308.00198</a> (replaced) [<a href="/pdf/2308.00198" title="Download PDF">pdf</a>, <a href="/format/2308.00198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Syntactically and semantically regular languages of lambda-terms  coincide through logical relations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moreau%2C+V">Vincent Moreau</a>, 
<a href="/search/cs?searchtype=author&query=Nguy%C3%AAn%2C+L+T+D">L&#xea; Th&#xe0;nh D&#x169;ng Nguy&#xea;n</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The proofs on "finitely pointable" CCCs in versions 1 and 2 were wrong; we now make slightly weaker claims on well-pointed locally finite CCCs. New in this version: added reference [3] and official DOI (proceedings of CSL 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Formal Languages and Automata Theory (cs.FL); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01084" title="Abstract">arXiv:2308.01084</a> (replaced) [<a href="/pdf/2308.01084" title="Download PDF">pdf</a>, <a href="/format/2308.01084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Driven Identification of Quadratic Representations for Nonlinear  Hamiltonian Systems using Weakly Symplectic Liftings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yildiz%2C+S">S&#xfc;leyman Yildiz</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+P">Pawan Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Bendokat%2C+T">Thomas Bendokat</a>, 
<a href="/search/cs?searchtype=author&query=Benner%2C+P">Peter Benner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02312" title="Abstract">arXiv:2308.02312</a> (replaced) [<a href="/pdf/2308.02312" title="Download PDF">pdf</a>, <a href="/ps/2308.02312" title="Download PostScript">ps</a>, <a href="/format/2308.02312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Stack Overflow Obsolete? An Empirical Study of the Characteristics of  ChatGPT Answers to Stack Overflow Questions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kabir%2C+S">Samia Kabir</a>, 
<a href="/search/cs?searchtype=author&query=Udo-Imeh%2C+D+N">David N. Udo-Imeh</a>, 
<a href="/search/cs?searchtype=author&query=Kou%2C+B">Bonan Kou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianyi Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been conditionally accepted for the CHI Conference on Human Factors in Computing Systems (CHI'24). The new version of this paper has been modified with updated discussions to address more stakeholders
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03970" title="Abstract">arXiv:2308.03970</a> (replaced) [<a href="/pdf/2308.03970" title="Download PDF">pdf</a>, <a href="/format/2308.03970" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dependent Cluster Mapping (DCMAP): Optimal clustering of directed  acyclic graphs for statistical inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+P+P">Paul Pao-Yen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ruggeri%2C+F">Fabrizio Ruggeri</a>, 
<a href="/search/cs?searchtype=author&query=Mengersen%2C+K">Kerrie Mengersen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05893" title="Abstract">arXiv:2308.05893</a> (replaced) [<a href="/pdf/2308.05893" title="Download PDF">pdf</a>, <a href="/ps/2308.05893" title="Download PostScript">ps</a>, <a href="/format/2308.05893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Team-Based Navigation: A Review of Deep Reinforcement Learning  Techniques for Multi-Agent Pathfinding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chung%2C+J">Jaehoon Chung</a>, 
<a href="/search/cs?searchtype=author&query=Fayyad%2C+J">Jamil Fayyad</a>, 
<a href="/search/cs?searchtype=author&query=Younes%2C+Y+A">Younes Al Younes</a>, 
<a href="/search/cs?searchtype=author&query=Najjaran%2C+H">Homayoun Najjaran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 10 figures, published in Artif Intell Rev 57, 41 (2024)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Artif Intell Rev 57, 41 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Multiagent Systems (cs.MA); Robotics (cs.RO); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07688" title="Abstract">arXiv:2308.07688</a> (replaced) [<a href="/pdf/2308.07688" title="Download PDF">pdf</a>, <a href="/ps/2308.07688" title="Download PostScript">ps</a>, <a href="/format/2308.07688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Network Initialization for Medical AI Models Using  Large-Scale, Unlabeled Natural Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Arasteh%2C+S+T">Soroosh Tayebi Arasteh</a>, 
<a href="/search/eess?searchtype=author&query=Misera%2C+L">Leo Misera</a>, 
<a href="/search/eess?searchtype=author&query=Kather%2C+J+N">Jakob Nikolas Kather</a>, 
<a href="/search/eess?searchtype=author&query=Truhn%2C+D">Daniel Truhn</a>, 
<a href="/search/eess?searchtype=author&query=Nebelung%2C+S">Sven Nebelung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in European Radiology Experimental
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Eur Radiol Exp 8, 10 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08261" title="Abstract">arXiv:2308.08261</a> (replaced) [<a href="/pdf/2308.08261" title="Download PDF">pdf</a>, <a href="/format/2308.08261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> B-stability of numerical integrators on Riemannian manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Arnold%2C+M">Martin Arnold</a>, 
<a href="/search/math?searchtype=author&query=Celledoni%2C+E">Elena Celledoni</a>, 
<a href="/search/math?searchtype=author&query=%C3%87okaj%2C+E">Ergys &#xc7;okaj</a>, 
<a href="/search/math?searchtype=author&query=Owren%2C+B">Brynjulf Owren</a>, 
<a href="/search/math?searchtype=author&query=Tumiotto%2C+D">Denise Tumiotto</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Computational Dynamics Vol. 11, No. 1, January 2024,
  pp. 92-107
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10966" title="Abstract">arXiv:2308.10966</a> (replaced) [<a href="/pdf/2308.10966" title="Download PDF">pdf</a>, <a href="/format/2308.10966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deadlock-free, Safe, and Decentralized Multi-Robot Navigation in Social  Mini-Games via Discrete-Time Control Barrier Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chandra%2C+R">Rohan Chandra</a>, 
<a href="/search/cs?searchtype=author&query=Zinage%2C+V">Vrushabh Zinage</a>, 
<a href="/search/cs?searchtype=author&query=Bakolas%2C+E">Efstathios Bakolas</a>, 
<a href="/search/cs?searchtype=author&query=Stone%2C+P">Peter Stone</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+J">Joydeep Biswas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> major update since last revision
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Science and Game Theory (cs.GT); Multiagent Systems (cs.MA); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12535" title="Abstract">arXiv:2308.12535</a> (replaced) [<a href="/pdf/2308.12535" title="Download PDF">pdf</a>, <a href="/format/2308.12535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SCP: Spherical-Coordinate-based Learned Point Cloud Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+A">Ao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linxin Song</a>, 
<a href="/search/cs?searchtype=author&query=Nonaka%2C+K">Keisuke Nonaka</a>, 
<a href="/search/cs?searchtype=author&query=Unno%2C+K">Kyohei Unno</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Heming Sun</a>, 
<a href="/search/cs?searchtype=author&query=Goto%2C+M">Masayuki Goto</a>, 
<a href="/search/cs?searchtype=author&query=Katto%2C+J">Jiro Katto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13401" title="Abstract">arXiv:2308.13401</a> (replaced) [<a href="/pdf/2308.13401" title="Download PDF">pdf</a>, <a href="/format/2308.13401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Min-$k$-planar Drawings of Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Binucci%2C+C">Carla Binucci</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%BCngener%2C+A">Aaron B&#xfc;ngener</a>, 
<a href="/search/cs?searchtype=author&query=Di+Battista%2C+G">Giuseppe Di Battista</a>, 
<a href="/search/cs?searchtype=author&query=Didimo%2C+W">Walter Didimo</a>, 
<a href="/search/cs?searchtype=author&query=Dujmovi%C4%87%2C+V">Vida Dujmovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+S">Seok-Hee Hong</a>, 
<a href="/search/cs?searchtype=author&query=Kaufmann%2C+M">Michael Kaufmann</a>, 
<a href="/search/cs?searchtype=author&query=Liotta%2C+G">Giuseppe Liotta</a>, 
<a href="/search/cs?searchtype=author&query=Morin%2C+P">Pat Morin</a>, 
<a href="/search/cs?searchtype=author&query=Tappini%2C+A">Alessandra Tappini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appears in the Proceedings of the 31st International Symposium on Graph Drawing and Network Visualization (GD 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14120" title="Abstract">arXiv:2308.14120</a> (replaced) [<a href="/pdf/2308.14120" title="Download PDF">pdf</a>, <a href="/ps/2308.14120" title="Download PostScript">ps</a>, <a href="/format/2308.14120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models Streamline Automated Machine Learning for Clinical  Studies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arasteh%2C+S+T">Soroosh Tayebi Arasteh</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+T">Tianyu Han</a>, 
<a href="/search/cs?searchtype=author&query=Lotfinia%2C+M">Mahshad Lotfinia</a>, 
<a href="/search/cs?searchtype=author&query=Kuhl%2C+C">Christiane Kuhl</a>, 
<a href="/search/cs?searchtype=author&query=Kather%2C+J+N">Jakob Nikolas Kather</a>, 
<a href="/search/cs?searchtype=author&query=Truhn%2C+D">Daniel Truhn</a>, 
<a href="/search/cs?searchtype=author&query=Nebelung%2C+S">Sven Nebelung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in Nature Communications. 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16264" title="Abstract">arXiv:2308.16264</a> (replaced) [<a href="/pdf/2308.16264" title="Download PDF">pdf</a>, <a href="/format/2308.16264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resource Allocation for Rate and Fidelity Maximization in Quantum  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Pouryousef%2C+S">Shahrooz Pouryousef</a>, 
<a href="/search/quant-ph?searchtype=author&query=Shapourian%2C+H">Hassan Shapourian</a>, 
<a href="/search/quant-ph?searchtype=author&query=Shabani%2C+A">Alireza Shabani</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kompella%2C+R">Ramana Kompella</a>, 
<a href="/search/quant-ph?searchtype=author&query=Towsley%2C+D">Don Towsley</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 8 figures, 3 appendices
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01945" title="Abstract">arXiv:2309.01945</a> (replaced) [<a href="/pdf/2309.01945" title="Download PDF">pdf</a>, <a href="/format/2309.01945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OHQ: On-chip Hardware-aware Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+H">Haotong Qin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yangdong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jingzhuo Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yulun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Ying Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xianglong Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Hardware Architecture (cs.AR)

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05102" title="Abstract">arXiv:2309.05102</a> (replaced) [<a href="/pdf/2309.05102" title="Download PDF">pdf</a>, <a href="/ps/2309.05102" title="Download PostScript">ps</a>, <a href="/format/2309.05102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Learning in Biological Neural Networks based on Stochastic Gradient  Descent? An analysis using stochastic processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Christensen%2C+S">S&#xf6;ren Christensen</a>, 
<a href="/search/q-bio?searchtype=author&query=Kallsen%2C+J">Jan Kallsen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07034" title="Abstract">arXiv:2309.07034</a> (replaced) [<a href="/pdf/2309.07034" title="Download PDF">pdf</a>, <a href="/format/2309.07034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sensitivity, Performance, Robustness: Deconstructing the Effect of  Sociodemographic Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beck%2C+T">Tilman Beck</a>, 
<a href="/search/cs?searchtype=author&query=Schuff%2C+H">Hendrik Schuff</a>, 
<a href="/search/cs?searchtype=author&query=Lauscher%2C+A">Anne Lauscher</a>, 
<a href="/search/cs?searchtype=author&query=Gurevych%2C+I">Iryna Gurevych</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024 camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07311" title="Abstract">arXiv:2309.07311</a> (replaced) [<a href="/pdf/2309.07311" title="Download PDF">pdf</a>, <a href="/format/2309.07311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and  Simplicity Bias in MLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Angelica Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shwartz-Ziv%2C+R">Ravid Shwartz-Ziv</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+K">Kyunghyun Cho</a>, 
<a href="/search/cs?searchtype=author&query=Leavitt%2C+M+L">Matthew L. Leavitt</a>, 
<a href="/search/cs?searchtype=author&query=Saphra%2C+N">Naomi Saphra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09323" title="Abstract">arXiv:2309.09323</a> (replaced) [<a href="/pdf/2309.09323" title="Download PDF">pdf</a>, <a href="/format/2309.09323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Answering Causal Queries at Layer 3 with DiscoSCMs-Embracing  Heterogeneity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+H">Heyang Gong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10356" title="Abstract">arXiv:2309.10356</a> (replaced) [<a href="/pdf/2309.10356" title="Download PDF">pdf</a>, <a href="/format/2309.10356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoadFormer: Duplex Transformer for RGB-Normal Semantic Road Scene  Parsing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiahang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yikang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+P">Peng Yun</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+G">Guangliang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qijun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+R">Rui Fan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13160" title="Abstract">arXiv:2309.13160</a> (replaced) [<a href="/pdf/2309.13160" title="Download PDF">pdf</a>, <a href="/format/2309.13160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to train your VAE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rivera%2C+M">Mariano Rivera</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14063" title="Abstract">arXiv:2309.14063</a> (replaced) [<a href="/e-print/2309.14063" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preferential Multi-Target Search in Indoor Environments using Semantic  SLAM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chikhalikar%2C+A">Akash Chikhalikar</a>, 
<a href="/search/cs?searchtype=author&query=Ravankar%2C+A+A">Ankit A. Ravankar</a>, 
<a href="/search/cs?searchtype=author&query=Luces%2C+J+V+S">Jose Victorio Salazar Luces</a>, 
<a href="/search/cs?searchtype=author&query=Hirata%2C+Y">Yasuhisa Hirata</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> There are some errors in Fig. 7 that were previously missed. Specifically, some of the chart values were interchanged
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14865" title="Abstract">arXiv:2309.14865</a> (replaced) [<a href="/pdf/2309.14865" title="Download PDF">pdf</a>, <a href="/format/2309.14865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Reconstruction of 3D Human Pose Interactions From 2D Poses  Alone
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hardy%2C+P">Peter Hardy</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hansung Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14891" title="Abstract">arXiv:2309.14891</a> (replaced) [<a href="/pdf/2309.14891" title="Download PDF">pdf</a>, <a href="/format/2309.14891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RE-SORT: Removing Spurious Correlation in Multilevel Interaction for CTR  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Songli Wu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+L">Liang Du</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jia-Qi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+D">De-Chuan Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shuang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zixun Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15516" title="Abstract">arXiv:2309.15516</a> (replaced) [<a href="/pdf/2309.15516" title="Download PDF">pdf</a>, <a href="/format/2309.15516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Teaching Text-to-Image Models to Communicate in Dialog
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xiaowen Sun</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jiazhan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Y">Yuxuan Lai</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xingyu Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+D">Dongyan Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00152" title="Abstract">arXiv:2310.00152</a> (replaced) [<a href="/pdf/2310.00152" title="Download PDF">pdf</a>, <a href="/format/2310.00152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Rewrite Prompts for Personalized Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Cheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+Q">Qiaozhu Mei</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+W">Weize Kong</a>, 
<a href="/search/cs?searchtype=author&query=Bendersky%2C+M">Michael Bendersky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of the ACM Web Conference 2024 (WWW '24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00313" title="Abstract">arXiv:2310.00313</a> (replaced) [<a href="/pdf/2310.00313" title="Download PDF">pdf</a>, <a href="/format/2310.00313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding In-Context Learning: Neuroscience-inspired Analysis of  Representations in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yousefi%2C+S">Safoora Yousefi</a>, 
<a href="/search/cs?searchtype=author&query=Betthauser%2C+L">Leo Betthauser</a>, 
<a href="/search/cs?searchtype=author&query=Hasanbeig%2C+H">Hosein Hasanbeig</a>, 
<a href="/search/cs?searchtype=author&query=Milli%C3%A8re%2C+R">Rapha&#xeb;l Milli&#xe8;re</a>, 
<a href="/search/cs?searchtype=author&query=Momennejad%2C+I">Ida Momennejad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICML 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00889" title="Abstract">arXiv:2310.00889</a> (replaced) [<a href="/pdf/2310.00889" title="Download PDF">pdf</a>, <a href="/format/2310.00889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Convergent Boundary Integral Methods for Slender Bodies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Malhotra%2C+D">Dhairya Malhotra</a>, 
<a href="/search/math?searchtype=author&query=Barnett%2C+A">Alex Barnett</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Physics (physics.comp-ph); Fluid Dynamics (physics.flu-dyn)

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03249" title="Abstract">arXiv:2310.03249</a> (replaced) [<a href="/pdf/2310.03249" title="Download PDF">pdf</a>, <a href="/format/2310.03249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Large Language Models be Good Path Planners? A Benchmark and  Investigation on Spatial-temporal Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aghzal%2C+M">Mohamed Aghzal</a>, 
<a href="/search/cs?searchtype=author&query=Plaku%2C+E">Erion Plaku</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Z">Ziyu Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03821" title="Abstract">arXiv:2310.03821</a> (replaced) [<a href="/pdf/2310.03821" title="Download PDF">pdf</a>, <a href="/format/2310.03821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WLST: Weak Labels Guided Self-training for Weakly-supervised Domain  Adaptation on 3D Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsou%2C+T">Tsung-Lin Tsou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tsung-Han Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+W+H">Winston H. Hsu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICRA 2024. Code is available at <a href="https://github.com/jacky121298/WLST">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03945" title="Abstract">arXiv:2310.03945</a> (replaced) [<a href="/pdf/2310.03945" title="Download PDF">pdf</a>, <a href="/format/2310.03945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Wasserstein distances for affine transformations of random vectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Hamm%2C+K">Keaton Hamm</a>, 
<a href="/search/stat?searchtype=author&query=Korzeniowski%2C+A">Andrzej Korzeniowski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04353" title="Abstract">arXiv:2310.04353</a> (replaced) [<a href="/pdf/2310.04353" title="Download PDF">pdf</a>, <a href="/format/2310.04353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An In-Context Learning Agent for Formal Theorem-Proving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thakur%2C+A">Amitayush Thakur</a>, 
<a href="/search/cs?searchtype=author&query=Tsoukalas%2C+G">George Tsoukalas</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Y">Yeming Wen</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+J">Jimmy Xin</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhuri%2C+S">Swarat Chaudhuri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04521" title="Abstract">arXiv:2310.04521</a> (replaced) [<a href="/pdf/2310.04521" title="Download PDF">pdf</a>, <a href="/format/2310.04521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lie Neurons: Adjoint-Equivariant Neural Networks for Semisimple Lie  Algebras
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tzu-Yuan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Minghan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ghaffari%2C+M">Maani Ghaffari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06312" title="Abstract">arXiv:2310.06312</a> (replaced) [<a href="/pdf/2310.06312" title="Download PDF">pdf</a>, <a href="/format/2310.06312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering Mixtures of Structural Causal Models from Time Series Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Varambally%2C+S">Sumanth Varambally</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yi-An Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+R">Rose Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08278" title="Abstract">arXiv:2310.08278</a> (replaced) [<a href="/pdf/2310.08278" title="Download PDF">pdf</a>, <a href="/format/2310.08278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lag-Llama: Towards Foundation Models for Probabilistic Time Series  Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rasul%2C+K">Kashif Rasul</a>, 
<a href="/search/cs?searchtype=author&query=Ashok%2C+A">Arjun Ashok</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+A+R">Andrew Robert Williams</a>, 
<a href="/search/cs?searchtype=author&query=Ghonia%2C+H">Hena Ghonia</a>, 
<a href="/search/cs?searchtype=author&query=Bhagwatkar%2C+R">Rishika Bhagwatkar</a>, 
<a href="/search/cs?searchtype=author&query=Khorasani%2C+A">Arian Khorasani</a>, 
<a href="/search/cs?searchtype=author&query=Bayazi%2C+M+J+D">Mohammad Javad Darvishi Bayazi</a>, 
<a href="/search/cs?searchtype=author&query=Adamopoulos%2C+G">George Adamopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Riachi%2C+R">Roland Riachi</a>, 
<a href="/search/cs?searchtype=author&query=Hassen%2C+N">Nadhir Hassen</a>, 
<a href="/search/cs?searchtype=author&query=Bilo%C5%A1%2C+M">Marin Bilo&#x161;</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Sahil Garg</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+A">Anderson Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Chapados%2C+N">Nicolas Chapados</a>, 
<a href="/search/cs?searchtype=author&query=Drouin%2C+A">Alexandre Drouin</a>, 
<a href="/search/cs?searchtype=author&query=Zantedeschi%2C+V">Valentina Zantedeschi</a>, 
<a href="/search/cs?searchtype=author&query=Nevmyvaka%2C+Y">Yuriy Nevmyvaka</a>, 
<a href="/search/cs?searchtype=author&query=Rish%2C+I">Irina Rish</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First two authors contributed equally. All data, models and code used are open-source. GitHub: <a href="https://github.com/time-series-foundation-models/lag-llama">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13367" title="Abstract">arXiv:2310.13367</a> (replaced) [<a href="/pdf/2310.13367" title="Download PDF">pdf</a>, <a href="/format/2310.13367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VFedMH: Vertical Federated Learning for Training Multiple Heterogeneous  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gai%2C+K">Keke Gai</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Liehuang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Choo%2C+K+R">Kim-Kwang Raymond Choo</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+B">Bin Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14450" title="Abstract">arXiv:2310.14450</a> (replaced) [<a href="/pdf/2310.14450" title="Download PDF">pdf</a>, <a href="/format/2310.14450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TATA: Stance Detection via Topic-Agnostic and Topic-Aware Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hanley%2C+H+W+A">Hans W. A. Hanley</a>, 
<a href="/search/cs?searchtype=author&query=Durumeric%2C+Z">Zakir Durumeric</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023; Updated citations
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17923" title="Abstract">arXiv:2310.17923</a> (replaced) [<a href="/pdf/2310.17923" title="Download PDF">pdf</a>, <a href="/format/2310.17923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Grasping of Unknown Objects with a Multi-Fingered Hand
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Burkhardt%2C+Y">Yannick Burkhardt</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Q">Qian Feng</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+K">Karan Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhaopeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Knoll%2C+A">Alois Knoll</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19789" title="Abstract">arXiv:2310.19789</a> (replaced) [<a href="/pdf/2310.19789" title="Download PDF">pdf</a>, <a href="/format/2310.19789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffEnc: Variational Diffusion with a Learned Encoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nielsen%2C+B+M+G">Beatrix M. G. Nielsen</a>, 
<a href="/search/cs?searchtype=author&query=Christensen%2C+A">Anders Christensen</a>, 
<a href="/search/cs?searchtype=author&query=Dittadi%2C+A">Andrea Dittadi</a>, 
<a href="/search/cs?searchtype=author&query=Winther%2C+O">Ole Winther</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00217" title="Abstract">arXiv:2311.00217</a> (replaced) [<a href="/pdf/2311.00217" title="Download PDF">pdf</a>, <a href="/ps/2311.00217" title="Download PostScript">ps</a>, <a href="/format/2311.00217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Large Language Models Capture Public Opinion about Global Warming?  An Empirical Assessment of Algorithmic Fidelity and Bias
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">S. Lee</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+T+Q">T. Q. Peng</a>, 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+M+H">M. H. Goldberg</a>, 
<a href="/search/cs?searchtype=author&query=Rosenthal%2C+S+A">S. A. Rosenthal</a>, 
<a href="/search/cs?searchtype=author&query=Kotcher%2C+J+E">J. E. Kotcher</a>, 
<a href="/search/cs?searchtype=author&query=Maibach%2C+E+W">E. W. Maibach</a>, 
<a href="/search/cs?searchtype=author&query=Leiserowitz%2C+A">A. Leiserowitz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 6 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00875" title="Abstract">arXiv:2311.00875</a> (replaced) [<a href="/pdf/2311.00875" title="Download PDF">pdf</a>, <a href="/format/2311.00875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Collective Behaviors from Observation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jinchao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+M">Ming Zhong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Multiagent Systems (cs.MA); Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02772" title="Abstract">arXiv:2311.02772</a> (replaced) [<a href="/pdf/2311.02772" title="Download PDF">pdf</a>, <a href="/ps/2311.02772" title="Download PostScript">ps</a>, <a href="/format/2311.02772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attention or Convolution: Transformer Encoders in Audio Language Models  for Inference Efficiency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeon%2C+S">Sungho Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Yeh%2C+C">Ching-Feng Yeh</a>, 
<a href="/search/cs?searchtype=author&query=Inan%2C+H">Hakan Inan</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+W">Wei-Ning Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Rungta%2C+R">Rashi Rungta</a>, 
<a href="/search/cs?searchtype=author&query=Mehdad%2C+Y">Yashar Mehdad</a>, 
<a href="/search/cs?searchtype=author&query=Bikel%2C+D">Daniel Bikel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages; accepted to Self-supervision in Audio, Speech and Beyond (SASB) workshop in ICASSP24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.03952" title="Abstract">arXiv:2311.03952</a> (replaced) [<a href="/pdf/2311.03952" title="Download PDF">pdf</a>, <a href="/format/2311.03952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Analysis of Dialogue Repair in Voice Assistants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Galbraith%2C+M">Matthew Galbraith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In WTF Workshop Proceedings (<a href="/abs/2401.04108">arXiv:2401.04108</a>) held in conjunction with the ACM conference on Conversational User Interfaces (CUI), 19 - 21/07 2023, in Eindhoven, The Netherlands
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07607" title="Abstract">arXiv:2311.07607</a> (replaced) [<a href="/pdf/2311.07607" title="Download PDF">pdf</a>, <a href="/format/2311.07607" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling Choice via Self-Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ko%2C+J">Joohwan Ko</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A+A">Andrew A. Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.11546" title="Abstract">arXiv:2311.11546</a> (replaced) [<a href="/pdf/2311.11546" title="Download PDF">pdf</a>, <a href="/format/2311.11546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Correlation-based Dual-band THz Channel Measurements and  Characterization in a Laboratory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuanbo Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiqin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Ziming Yu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Chong Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12398" title="Abstract">arXiv:2311.12398</a> (replaced) [<a href="/pdf/2311.12398" title="Download PDF">pdf</a>, <a href="/format/2311.12398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RFTrans: Leveraging Refractive Flow of Transparent Objects for Surface  Normal Estimation and Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+T">Tutian Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jieyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Haoyuan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wenqiang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Cewu Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12944" title="Abstract">arXiv:2311.12944</a> (replaced) [<a href="/pdf/2311.12944" title="Download PDF">pdf</a>, <a href="/format/2311.12944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DroneOptiNet: A Framework for Optimal Drone-based Load Redistribution  Mechanism for 5G and Beyond Solar Small Cell Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dave%2C+D">Daksh Dave</a>, 
<a href="/search/cs?searchtype=author&query=Chamola%2C+V">Vinay Chamola</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+S">Sandeep Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Zeadally%2C+S">Sherali Zeadally</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.17954" title="Abstract">arXiv:2311.17954</a> (replaced) [<a href="/pdf/2311.17954" title="Download PDF">pdf</a>, <a href="/format/2311.17954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformer-empowered Multi-modal Item Embedding for Enhanced Image  Search in E-Commerce
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+P">Peng Hou</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+A">Anxiang Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Han Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.18208" title="Abstract">arXiv:2311.18208</a> (replaced) [<a href="/pdf/2311.18208" title="Download PDF">pdf</a>, <a href="/format/2311.18208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SMaRt: Improving GANs with Score Matching Regularity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+M">Mengfei Xia</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yujun Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Ceyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+R">Ran Yi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yong-jin Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.18780" title="Abstract">arXiv:2311.18780</a> (replaced) [<a href="/pdf/2311.18780" title="Download PDF">pdf</a>, <a href="/format/2311.18780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MultiResFormer: Transformer with Adaptive Multi-Resolution Modeling for  General Time Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+L">Linfeng Du</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+J">Ji Xin</a>, 
<a href="/search/cs?searchtype=author&query=Labach%2C+A">Alex Labach</a>, 
<a href="/search/cs?searchtype=author&query=Zuberi%2C+S">Saba Zuberi</a>, 
<a href="/search/cs?searchtype=author&query=Volkovs%2C+M">Maksims Volkovs</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+R+G">Rahul G. Krishnan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.01210" title="Abstract">arXiv:2312.01210</a> (replaced) [<a href="/pdf/2312.01210" title="Download PDF">pdf</a>, <a href="/format/2312.01210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When accurate prediction models yield harmful self-fulfilling prophecies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=van+Amsterdam%2C+W+A+C">Wouter A.C. van Amsterdam</a>, 
<a href="/search/stat?searchtype=author&query=van+Geloven%2C+N">Nan van Geloven</a>, 
<a href="/search/stat?searchtype=author&query=Krijthe%2C+J+H">Jesse H. Krijthe</a>, 
<a href="/search/stat?searchtype=author&query=Ranganath%2C+R">Rajesh Ranganath</a>, 
<a href="/search/stat?searchtype=author&query=Cin%C3%A1%2C+G">Giovanni Cin&#xe1;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.01709" title="Abstract">arXiv:2312.01709</a> (replaced) [<a href="/pdf/2312.01709" title="Download PDF">pdf</a>, <a href="/ps/2312.01709" title="Download PostScript">ps</a>, <a href="/format/2312.01709" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A New Challenging Curve Fitting Benchmark Test Set for Global  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cheng%2C+P">Peicong Cheng</a>, 
<a href="/search/math?searchtype=author&query=Cheng%2C+P">Peicheng Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Mathematical Software (cs.MS); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.03406" title="Abstract">arXiv:2312.03406</a> (replaced) [<a href="/pdf/2312.03406" title="Download PDF">pdf</a>, <a href="/format/2312.03406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SVQ: Sparse Vector Quantization for Spatiotemporal Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tian Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yanjun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Liang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+R">Rong Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.03450" title="Abstract">arXiv:2312.03450</a> (replaced) [<a href="/pdf/2312.03450" title="Download PDF">pdf</a>, <a href="/format/2312.03450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Autoencoder for Channel Estimation: Real-World Measurement  Insights
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Baur%2C+M">Michael Baur</a>, 
<a href="/search/eess?searchtype=author&query=B%C3%B6ck%2C+B">Benedikt B&#xf6;ck</a>, 
<a href="/search/eess?searchtype=author&query=Turan%2C+N">Nurettin Turan</a>, 
<a href="/search/eess?searchtype=author&query=Utschick%2C+W">Wolfgang Utschick</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures, accepted at WSA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.05538" title="Abstract">arXiv:2312.05538</a> (replaced) [<a href="/pdf/2312.05538" title="Download PDF">pdf</a>, <a href="/format/2312.05538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CSL: Class-Agnostic Structure-Constrained Learning for Segmentation  Including the Unseen
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+F">Fang Li</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+L">Lu Qi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming-Hsuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ahuja%2C+N">Narendra Ahuja</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.06837" title="Abstract">arXiv:2312.06837</a> (replaced) [<a href="/pdf/2312.06837" title="Download PDF">pdf</a>, <a href="/format/2312.06837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral State Space Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+N">Naman Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Suo%2C+D">Daniel Suo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hazan%2C+E">Elad Hazan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.06999" title="Abstract">arXiv:2312.06999</a> (replaced) [<a href="/pdf/2312.06999" title="Download PDF">pdf</a>, <a href="/format/2312.06999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DGNet: Dynamic Gradient-Guided Network for Water-Related Optics Image  Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jingchun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zongxin He</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Q">Qiuping Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+K">Kui Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+X">Xianping Fu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuelong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07364" title="Abstract">arXiv:2312.07364</a> (replaced) [<a href="/pdf/2312.07364" title="Download PDF">pdf</a>, <a href="/format/2312.07364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collapse-Aware Triplet Decoupling for Adversarially Robust Image  Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+Q">Qiwei Tian</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenhao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhengyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qian Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chao Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08157" title="Abstract">arXiv:2312.08157</a> (replaced) [<a href="/pdf/2312.08157" title="Download PDF">pdf</a>, <a href="/format/2312.08157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CIDR: A Cooperative Integrated Dynamic Refining Method for Minimal  Feature Removal Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Taolin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongyang Li</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiaofeng He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08213" title="Abstract">arXiv:2312.08213</a> (replaced) [<a href="/pdf/2312.08213" title="Download PDF">pdf</a>, <a href="/format/2312.08213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerated Event-Based Feature Detection and Compression for  Surveillance Video Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Freeman%2C+A+C">Andrew C. Freeman</a>, 
<a href="/search/cs?searchtype=author&query=Mayer-Patel%2C+K">Ketan Mayer-Patel</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Montek Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in the proceedings of ACM Multimedia Systems '24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.10308" title="Abstract">arXiv:2312.10308</a> (replaced) [<a href="/pdf/2312.10308" title="Download PDF">pdf</a>, <a href="/format/2312.10308" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Event-Based Contrastive Learning for Medical Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeong%2C+H">Hyewon Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Oufattole%2C+N">Nassim Oufattole</a>, 
<a href="/search/cs?searchtype=author&query=Mcdermott%2C+M">Matthew Mcdermott</a>, 
<a href="/search/cs?searchtype=author&query=Balagopalan%2C+A">Aparna Balagopalan</a>, 
<a href="/search/cs?searchtype=author&query=Jangeesingh%2C+B">Bryan Jangeesingh</a>, 
<a href="/search/cs?searchtype=author&query=Ghassemi%2C+M">Marzyeh Ghassemi</a>, 
<a href="/search/cs?searchtype=author&query=Stultz%2C+C">Collin Stultz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Unifying Representations in Neural Models Workshop in NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.10396" title="Abstract">arXiv:2312.10396</a> (replaced) [<a href="/pdf/2312.10396" title="Download PDF">pdf</a>, <a href="/ps/2312.10396" title="Download PostScript">ps</a>, <a href="/format/2312.10396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Far Can Fairness Constraints Help Recover From Biased Data?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+M">Mohit Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Deshpande%2C+A">Amit Deshpande</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11441" title="Abstract">arXiv:2312.11441</a> (replaced) [<a href="/pdf/2312.11441" title="Download PDF">pdf</a>, <a href="/format/2312.11441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social Learning: Towards Collaborative Learning with Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohtashami%2C+A">Amirkeivan Mohtashami</a>, 
<a href="/search/cs?searchtype=author&query=Hartmann%2C+F">Florian Hartmann</a>, 
<a href="/search/cs?searchtype=author&query=Gooding%2C+S">Sian Gooding</a>, 
<a href="/search/cs?searchtype=author&query=Zilka%2C+L">Lukas Zilka</a>, 
<a href="/search/cs?searchtype=author&query=Sharifi%2C+M">Matt Sharifi</a>, 
<a href="/search/cs?searchtype=author&query=Arcas%2C+B+A+y">Blaise Aguera y Arcas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12112" title="Abstract">arXiv:2312.12112</a> (replaced) [<a href="/pdf/2312.12112" title="Download PDF">pdf</a>, <a href="/format/2312.12112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation  in ultra low-data regimes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seedat%2C+N">Nabeel Seedat</a>, 
<a href="/search/cs?searchtype=author&query=Huynh%2C+N">Nicolas Huynh</a>, 
<a href="/search/cs?searchtype=author&query=van+Breugel%2C+B">Boris van Breugel</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> *Seedat &amp; Huynh contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12379" title="Abstract">arXiv:2312.12379</a> (replaced) [<a href="/pdf/2312.12379" title="Download PDF">pdf</a>, <a href="/format/2312.12379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixture of Cluster-conditional LoRA Experts for Vision-language  Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gou%2C+Y">Yunhao Gou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhili Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lanqing Hong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Aoxue Li</a>, 
<a href="/search/cs?searchtype=author&query=Yeung%2C+D">Dit-Yan Yeung</a>, 
<a href="/search/cs?searchtype=author&query=Kwok%2C+J+T">James T. Kwok</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project website: <a href="https://kaichen1998.github.io/projects/mocle/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.13327" title="Abstract">arXiv:2312.13327</a> (replaced) [<a href="/pdf/2312.13327" title="Download PDF">pdf</a>, <a href="/format/2312.13327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Reinforcement Learning for Variable Action Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sinii%2C+V">Viacheslav Sinii</a>, 
<a href="/search/cs?searchtype=author&query=Nikulin%2C+A">Alexander Nikulin</a>, 
<a href="/search/cs?searchtype=author&query=Kurenkov%2C+V">Vladislav Kurenkov</a>, 
<a href="/search/cs?searchtype=author&query=Zisman%2C+I">Ilya Zisman</a>, 
<a href="/search/cs?searchtype=author&query=Kolesnikov%2C+S">Sergey Kolesnikov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, Under Review; code: <a href="https://github.com/corl-team/headless-ad">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.14040" title="Abstract">arXiv:2312.14040</a> (replaced) [<a href="/pdf/2312.14040" title="Download PDF">pdf</a>, <a href="/format/2312.14040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balancing Specialization and Adaptation in a Transforming Scientific  Landscape
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gautheron%2C+L">Lucas Gautheron</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.14209" title="Abstract">arXiv:2312.14209</a> (replaced) [<a href="/pdf/2312.14209" title="Download PDF">pdf</a>, <a href="/format/2312.14209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TextFusion: Unveiling the Power of Textual Semantics for Controllable  Image Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+C">Chunyang Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tianyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiao-Jun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hui Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xi Li</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zhangyong Tang</a>, 
<a href="/search/cs?searchtype=author&query=Kittler%2C+J">Josef Kittler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2 version, 13 pages, 16 figures, with the code repository link
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.14301" title="Abstract">arXiv:2312.14301</a> (replaced) [<a href="/pdf/2312.14301" title="Download PDF">pdf</a>, <a href="/format/2312.14301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autoencoder Based Face Verification System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Solomon%2C+E">Enoch Solomon</a>, 
<a href="/search/cs?searchtype=author&query=Woubie%2C+A">Abraham Woubie</a>, 
<a href="/search/cs?searchtype=author&query=Emiru%2C+E+S">Eyael Solomon Emiru</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.14690" title="Abstract">arXiv:2312.14690</a> (replaced) [<a href="/pdf/2312.14690" title="Download PDF">pdf</a>, <a href="/format/2312.14690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Stochastic Bilevel Optimization: Improved Complexity and  Heterogeneity Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Niu%2C+Y">Youcheng Niu</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+J">Jinming Xu</a>, 
<a href="/search/math?searchtype=author&query=Sun%2C+Y">Ying Sun</a>, 
<a href="/search/math?searchtype=author&query=Huang%2C+Y">Yan Huang</a>, 
<a href="/search/math?searchtype=author&query=Chai%2C+L">Li Chai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 50 pages, 22 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.14877" title="Abstract">arXiv:2312.14877</a> (replaced) [<a href="/pdf/2312.14877" title="Download PDF">pdf</a>, <a href="/format/2312.14877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Knowledge Extraction from Large Language Models using Social  Choice Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Potyka%2C+N">Nico Potyka</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuqicheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yunjie He</a>, 
<a href="/search/cs?searchtype=author&query=Kharlamov%2C+E">Evgeny Kharlamov</a>, 
<a href="/search/cs?searchtype=author&query=Staab%2C+S">Steffen Staab</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAMAS 2024 as a full paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.16809" title="Abstract">arXiv:2312.16809</a> (replaced) [<a href="/pdf/2312.16809" title="Download PDF">pdf</a>, <a href="/format/2312.16809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blind System Identification in Linear Parameter-Varying Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Moghaddam%2C+J+Z">Javad Zahedi Moghaddam</a>, 
<a href="/search/eess?searchtype=author&query=Momeni%2C+H">Hamidreza Momeni</a>, 
<a href="/search/eess?searchtype=author&query=Danesh%2C+M">Mojtaba Danesh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.17293" title="Abstract">arXiv:2312.17293</a> (replaced) [<a href="/pdf/2312.17293" title="Download PDF">pdf</a>, <a href="/format/2312.17293" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $&#x3bc;$GUIDE: a framework for microstructure imaging via generalized  uncertainty-driven inference using deep learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jallais%2C+M">Ma&#xeb;liss Jallais</a>, 
<a href="/search/eess?searchtype=author&query=Palombo%2C+M">Marco Palombo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG); Medical Physics (physics.med-ph)

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.01172" title="Abstract">arXiv:2401.01172</a> (replaced) [<a href="/pdf/2401.01172" title="Download PDF">pdf</a>, <a href="/format/2401.01172" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quadratic Time-Frequency Analysis of Vibration Signals for Diagnosing  Bearing Faults
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Al-Sa%27d%2C+M">Mohammad Al-Sa&#x27;d</a>, 
<a href="/search/cs?searchtype=author&query=Jalonen%2C+T">Tuomas Jalonen</a>, 
<a href="/search/cs?searchtype=author&query=Kiranyaz%2C+S">Serkan Kiranyaz</a>, 
<a href="/search/cs?searchtype=author&query=Gabbouj%2C+M">Moncef Gabbouj</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.01984" title="Abstract">arXiv:2401.01984</a> (replaced) [<a href="/pdf/2401.01984" title="Download PDF">pdf</a>, <a href="/format/2401.01984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AUPIMO: Redefining Visual Anomaly Detection Benchmarks with High Speed  and Low Tolerance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bertoldo%2C+J+P+C">Joao P. C. Bertoldo</a>, 
<a href="/search/cs?searchtype=author&query=Ameln%2C+D">Dick Ameln</a>, 
<a href="/search/cs?searchtype=author&query=Vaidya%2C+A">Ashwin Vaidya</a>, 
<a href="/search/cs?searchtype=author&query=Ak%C3%A7ay%2C+S">Samet Ak&#xe7;ay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This research has been conducted during Google Summer of Code 2023 (GSoC 2023) at OpenVINO (Intel). GSoC 2023 page: <a href="https://summerofcode.withgoogle.com/archive/2023/projects/SPMopugd">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.02740" title="Abstract">arXiv:2401.02740</a> (replaced) [<a href="/pdf/2401.02740" title="Download PDF">pdf</a>, <a href="/ps/2401.02740" title="Download PostScript">ps</a>, <a href="/format/2401.02740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fairness-Aware Job Scheduling for Multi-Job Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuxin Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Han Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.02889" title="Abstract">arXiv:2401.02889</a> (replaced) [<a href="/pdf/2401.02889" title="Download PDF">pdf</a>, <a href="/format/2401.02889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-Preserving Reduced Operator Inference for Efficient Design and  Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Koike%2C+T">Tomoki Koike</a>, 
<a href="/search/math?searchtype=author&query=Qian%2C+E">Elizabeth Qian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, AIAA SciTech Forum 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG); Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.03756" title="Abstract">arXiv:2401.03756</a> (replaced) [<a href="/pdf/2401.03756" title="Download PDF">pdf</a>, <a href="/format/2401.03756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Experimental Design for Policy Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kato%2C+M">Masahiro Kato</a>, 
<a href="/search/cs?searchtype=author&query=Okumura%2C+K">Kyohei Okumura</a>, 
<a href="/search/cs?searchtype=author&query=Ishihara%2C+T">Takuya Ishihara</a>, 
<a href="/search/cs?searchtype=author&query=Kitagawa%2C+T">Toru Kitagawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2302.02988">arXiv:2302.02988</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Econometrics (econ.EM); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04679" title="Abstract">arXiv:2401.04679</a> (replaced) [<a href="/pdf/2401.04679" title="Download PDF">pdf</a>, <a href="/format/2401.04679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nikdan%2C+M">Mahdi Nikdan</a>, 
<a href="/search/cs?searchtype=author&query=Tabesh%2C+S">Soroush Tabesh</a>, 
<a href="/search/cs?searchtype=author&query=Crn%C4%8Devi%C4%87%2C+E">Elvir Crn&#x10d;evi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Alistarh%2C+D">Dan Alistarh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04855" title="Abstract">arXiv:2401.04855</a> (replaced) [<a href="/pdf/2401.04855" title="Download PDF">pdf</a>, <a href="/format/2401.04855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LPAC: Learnable Perception-Action-Communication Loops with Applications  to Coverage Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+S">Saurav Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Muthukrishnan%2C+R">Ramya Muthukrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Gosrich%2C+W">Walker Gosrich</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+V">Vijay Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+A">Alejandro Ribeiro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06451" title="Abstract">arXiv:2401.06451</a> (replaced) [<a href="/pdf/2401.06451" title="Download PDF">pdf</a>, <a href="/ps/2401.06451" title="Download PostScript">ps</a>, <a href="/format/2401.06451" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Logic for Repair and State Recovery in Byzantine Fault-tolerant  Multi-agent Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Ditmarsch%2C+H">Hans van Ditmarsch</a>, 
<a href="/search/cs?searchtype=author&query=Fruzsa%2C+K">Krisztina Fruzsa</a>, 
<a href="/search/cs?searchtype=author&query=Kuznets%2C+R">Roman Kuznets</a>, 
<a href="/search/cs?searchtype=author&query=Schmid%2C+U">Ulrich Schmid</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06550" title="Abstract">arXiv:2401.06550</a> (replaced) [<a href="/pdf/2401.06550" title="Download PDF">pdf</a>, <a href="/ps/2401.06550" title="Download PostScript">ps</a>, <a href="/format/2401.06550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Urban Areas of Interest Generation via Remote Sensing Imagery  and Geographical Prior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chuanji Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yingying Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaotuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qiqi Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08103" title="Abstract">arXiv:2401.08103</a> (replaced) [<a href="/pdf/2401.08103" title="Download PDF">pdf</a>, <a href="/format/2401.08103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resolving Ethics Trade-offs in Implementing Responsible AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sanderson%2C+C">Conrad Sanderson</a>, 
<a href="/search/cs?searchtype=author&query=Schleiger%2C+E">Emma Schleiger</a>, 
<a href="/search/cs?searchtype=author&query=Douglas%2C+D">David Douglas</a>, 
<a href="/search/cs?searchtype=author&query=Kuhnert%2C+P">Petra Kuhnert</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Q">Qinghua Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08518" title="Abstract">arXiv:2401.08518</a> (replaced) [<a href="/pdf/2401.08518" title="Download PDF">pdf</a>, <a href="/format/2401.08518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PPSURF: Combining Patches and Point Convolutions for Detailed Surface  Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Erler%2C+P">Philipp Erler</a>, 
<a href="/search/cs?searchtype=author&query=Fuentes%2C+L">Lizeth Fuentes</a>, 
<a href="/search/cs?searchtype=author&query=Hermosilla%2C+P">Pedro Hermosilla</a>, 
<a href="/search/cs?searchtype=author&query=Guerrero%2C+P">Paul Guerrero</a>, 
<a href="/search/cs?searchtype=author&query=Pajarola%2C+R">Renato Pajarola</a>, 
<a href="/search/cs?searchtype=author&query=Wimmer%2C+M">Michael Wimmer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Computer Graphics Forum (Jan 2024): <a href="https://onlinelibrary.wiley.com/doi/10.1111/cgf.15000">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computer Graphics Forum e15000, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.09071" title="Abstract">arXiv:2401.09071</a> (replaced) [<a href="/pdf/2401.09071" title="Download PDF">pdf</a>, <a href="/format/2401.09071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Spectral Graph Neural Networks with Spatially Adaptive  Filtering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jingwei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaizhu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+X">Xinping Yi</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Z">Zixian Su</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10020" title="Abstract">arXiv:2401.10020</a> (replaced) [<a href="/pdf/2401.10020" title="Download PDF">pdf</a>, <a href="/format/2401.10020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Rewarding Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+W">Weizhe Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+R+Y">Richard Yuanzhe Pang</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+K">Kyunghyun Cho</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xian Li</a>, 
<a href="/search/cs?searchtype=author&query=Sukhbaatar%2C+S">Sainbayar Sukhbaatar</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Weston%2C+J">Jason Weston</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10371" title="Abstract">arXiv:2401.10371</a> (replaced) [<a href="/pdf/2401.10371" title="Download PDF">pdf</a>, <a href="/format/2401.10371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Langevin Unlearning: A New Perspective of Noisy Gradient Descent for  Machine Unlearning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chien%2C+E">Eli Chien</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ziang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pan Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12755" title="Abstract">arXiv:2401.12755</a> (replaced) [<a href="/pdf/2401.12755" title="Download PDF">pdf</a>, <a href="/ps/2401.12755" title="Download PostScript">ps</a>, <a href="/format/2401.12755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Risk Analysis of the Impact of AI on the Deliberate Biological  Threat Landscape
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Walsh%2C+M+E">Matthew E. Walsh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12799" title="Abstract">arXiv:2401.12799</a> (replaced) [<a href="/pdf/2401.12799" title="Download PDF">pdf</a>, <a href="/ps/2401.12799" title="Download PostScript">ps</a>, <a href="/format/2401.12799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Some convergence analysis for multicontinuum homogenization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Leung%2C+W+T">Wing Tat Leung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13169" title="Abstract">arXiv:2401.13169</a> (replaced) [<a href="/pdf/2401.13169" title="Download PDF">pdf</a>, <a href="/format/2401.13169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReposVul: A Repository-Level High-Quality Vulnerability Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinchen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+R">Ruida Hu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Cuiyun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+X">Xin-Cheng Wen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yujia Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Q">Qing Liao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICSE 2024 Industry Challenge Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13213" title="Abstract">arXiv:2401.13213</a> (replaced) [<a href="/pdf/2401.13213" title="Download PDF">pdf</a>, <a href="/format/2401.13213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Common-Sense Bias Discovery and Mitigation for Classification Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Miao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=fryer%2C+Z">Zee fryer</a>, 
<a href="/search/cs?searchtype=author&query=Colman%2C+B">Ben Colman</a>, 
<a href="/search/cs?searchtype=author&query=Shahriyari%2C+A">Ali Shahriyari</a>, 
<a href="/search/cs?searchtype=author&query=Bharaj%2C+G">Gaurav Bharaj</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14016" title="Abstract">arXiv:2401.14016</a> (replaced) [<a href="/pdf/2401.14016" title="Download PDF">pdf</a>, <a href="/format/2401.14016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Uncertainty-Aware Language Agent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiuzhou Han</a>, 
<a href="/search/cs?searchtype=author&query=Buntine%2C+W">Wray Buntine</a>, 
<a href="/search/cs?searchtype=author&query=Shareghi%2C+E">Ehsan Shareghi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The code and data are at <a href="https://uala-agent.github.io.">this https URL</a> (Updated the design for multi-inference setup to be comparable with single-inference experiments.). arXiv admin note: text overlap with <a href="/abs/2310.05915">arXiv:2310.05915</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14394" title="Abstract">arXiv:2401.14394</a> (replaced) [<a href="/pdf/2401.14394" title="Download PDF">pdf</a>, <a href="/ps/2401.14394" title="Download PostScript">ps</a>, <a href="/format/2401.14394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> O(1) Insertion for Random Walk d-ary Cuckoo Hashing up to the Load  Threshold
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bell%2C+T">Tolson Bell</a>, 
<a href="/search/cs?searchtype=author&query=Frieze%2C+A">Alan Frieze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14871" title="Abstract">arXiv:2401.14871</a> (replaced) [<a href="/pdf/2401.14871" title="Download PDF">pdf</a>, <a href="/format/2401.14871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Enabled Policy Optimization for Direct Adaptive Learning of the LQR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhao%2C+F">Feiran Zhao</a>, 
<a href="/search/math?searchtype=author&query=D%C3%B6rfler%2C+F">Florian D&#xf6;rfler</a>, 
<a href="/search/math?searchtype=author&query=Chiuso%2C+A">Alessandro Chiuso</a>, 
<a href="/search/math?searchtype=author&query=You%2C+K">Keyou You</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16191" title="Abstract">arXiv:2401.16191</a> (replaced) [<a href="/e-print/2401.16191" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Tripods to Bipods: Reducing the Queue Number of Planar Graphs Costs  Just One Leg
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=F%C3%B6rster%2C+H">Henry F&#xf6;rster</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The presented decomposition technique (Theorems 1.2/1.3) has been already independently shown by T. Ueckerdt, D.R. Wood, W. Yi (<a href="https://doi.org/10.37236/10614">this https URL</a>); a circumstance that I missed due to the result not being advertised in the corresponding abstract. Moreover, Lemma 4.2 is wrong, thus new technical details are necessary. I would like to thank Sergey Pupyrev for pointing this out
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16661" title="Abstract">arXiv:2401.16661</a> (replaced) [<a href="/pdf/2401.16661" title="Download PDF">pdf</a>, <a href="/ps/2401.16661" title="Download PostScript">ps</a>, <a href="/format/2401.16661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalization of LiNGAM that allows confounding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+J">Joe Suzuki</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tian-Le Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2007.11131">arXiv:2007.11131</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17604" title="Abstract">arXiv:2401.17604</a> (replaced) [<a href="/pdf/2401.17604" title="Download PDF">pdf</a>, <a href="/format/2401.17604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computation and Parameter Efficient Multi-Modal Fusion Transformer for  Cued Speech Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Li Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haizhou Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by TASLP
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00418" title="Abstract">arXiv:2402.00418</a> (replaced) [<a href="/pdf/2402.00418" title="Download PDF">pdf</a>, <a href="/ps/2402.00418" title="Download PostScript">ps</a>, <a href="/format/2402.00418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking Transferable Adversarial Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhibo Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiayu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhiyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huaming Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NDSS 2024 Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00746" title="Abstract">arXiv:2402.00746</a> (replaced) [<a href="/pdf/2402.00746" title="Download PDF">pdf</a>, <a href="/format/2402.00746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+M">Mingyu Jin</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qinkai Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+D">Dong Shu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Suiyuan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+M">Mengnan Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongfeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Y">Yanda Meng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00794" title="Abstract">arXiv:2402.00794</a> (replaced) [<a href="/pdf/2402.00794" title="Download PDF">pdf</a>, <a href="/format/2402.00794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReAGent: A Model-agnostic Feature Attribution Method for Generative  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhixue Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+B">Boxuan Shan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAAI24 workshop ReLM
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01180" title="Abstract">arXiv:2402.01180</a> (replaced) [<a href="/pdf/2402.01180" title="Download PDF">pdf</a>, <a href="/format/2402.01180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time Extended Reality Video Transmission Optimization Based on  Frame-priority Scheduling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+G">Guangjin Pan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shugong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shunqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaojing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yanzan Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Multimedia (cs.MM); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01350" title="Abstract">arXiv:2402.01350</a> (replaced) [<a href="/pdf/2402.01350" title="Download PDF">pdf</a>, <a href="/format/2402.01350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> pFedMoE: Data-Level Personalization with Mixture of Experts for  Model-Heterogeneous Personalized Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+L">Liping Yi</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Han Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+C">Chao Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Heng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Gang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoguang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoxiao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01393" title="Abstract">arXiv:2402.01393</a> (replaced) [<a href="/pdf/2402.01393" title="Download PDF">pdf</a>, <a href="/format/2402.01393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ALERT-Transformer: Bridging Asynchronous and Synchronous Machine  Learning for Real-Time Event-based Spatio-Temporal Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Martin-Turrero%2C+C">Carmen Martin-Turrero</a>, 
<a href="/search/cs?searchtype=author&query=Bouvier%2C+M">Maxence Bouvier</a>, 
<a href="/search/cs?searchtype=author&query=Breitenstein%2C+M">Manuel Breitenstein</a>, 
<a href="/search/cs?searchtype=author&query=Zanuttigh%2C+P">Pietro Zanuttigh</a>, 
<a href="/search/cs?searchtype=author&query=Parret%2C+V">Vincent Parret</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint version. 8 pages, 7 figures, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01397" title="Abstract">arXiv:2402.01397</a> (replaced) [<a href="/pdf/2402.01397" title="Download PDF">pdf</a>, <a href="/format/2402.01397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A survey on robustness in trajectory prediction for autonomous vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hagenus%2C+J">Jeroen Hagenus</a>, 
<a href="/search/cs?searchtype=author&query=Mathiesen%2C+F+B">Frederik Baymler Mathiesen</a>, 
<a href="/search/cs?searchtype=author&query=Schumann%2C+J+F">Julian F. Schumann</a>, 
<a href="/search/cs?searchtype=author&query=Zgonnikov%2C+A">Arkady Zgonnikov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 1 figure, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01501" title="Abstract">arXiv:2402.01501</a> (replaced) [<a href="/pdf/2402.01501" title="Download PDF">pdf</a>, <a href="/ps/2402.01501" title="Download PostScript">ps</a>, <a href="/format/2402.01501" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Satisfiability Modulo Exponential Integer Arithmetic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Frohn%2C+F">Florian Frohn</a>, 
<a href="/search/cs?searchtype=author&query=Giesl%2C+J">J&#xfc;rgen Giesl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01697" title="Abstract">arXiv:2402.01697</a> (replaced) [<a href="/pdf/2402.01697" title="Download PDF">pdf</a>, <a href="/format/2402.01697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> APT-Pipe: An Automatic Prompt-Tuning Tool for Social Computing Data  Annotation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yiming Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zhizhuo Yin</a>, 
<a href="/search/cs?searchtype=author&query=Tyson%2C+G">Gareth Tyson</a>, 
<a href="/search/cs?searchtype=author&query=Haq%2C+E">Ehsan-Ul Haq</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+L">Lik-Hang Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+P">Pan Hui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Just accepted by WWW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01703" title="Abstract">arXiv:2402.01703</a> (replaced) [<a href="/pdf/2402.01703" title="Download PDF">pdf</a>, <a href="/ps/2402.01703" title="Download PostScript">ps</a>, <a href="/format/2402.01703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multi-Perspective Machine Learning Approach to Evaluate Police-Driver  Interaction in Los Angeles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grahama%2C+B+A+T">Benjamin A.T. Grahama</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+L">Lauren Brown</a>, 
<a href="/search/cs?searchtype=author&query=Chochlakis%2C+G">Georgios Chochlakis</a>, 
<a href="/search/cs?searchtype=author&query=Dehghani%2C+M">Morteza Dehghani</a>, 
<a href="/search/cs?searchtype=author&query=Delerme%2C+R">Raquel Delerme</a>, 
<a href="/search/cs?searchtype=author&query=Friedman%2C+B">Brittany Friedman</a>, 
<a href="/search/cs?searchtype=author&query=Graeden%2C+E">Ellie Graeden</a>, 
<a href="/search/cs?searchtype=author&query=Golazizian%2C+P">Preni Golazizian</a>, 
<a href="/search/cs?searchtype=author&query=Hebbar%2C+R">Rajat Hebbar</a>, 
<a href="/search/cs?searchtype=author&query=Hejabi%2C+P">Parsa Hejabi</a>, 
<a href="/search/cs?searchtype=author&query=Kommineni%2C+A">Aditya Kommineni</a>, 
<a href="/search/cs?searchtype=author&query=Salinas%2C+M">Mayag&#xfc;ez Salinas</a>, 
<a href="/search/cs?searchtype=author&query=Sierra-Ar%C3%A9valo%2C+M">Michael Sierra-Ar&#xe9;valo</a>, 
<a href="/search/cs?searchtype=author&query=Trager%2C+J">Jackson Trager</a>, 
<a href="/search/cs?searchtype=author&query=Weller%2C+N">Nicholas Weller</a>, 
<a href="/search/cs?searchtype=author&query=Narayan%2C+S">Shrikanth Narayan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01804" title="Abstract">arXiv:2402.01804</a> (replaced) [<a href="/pdf/2402.01804" title="Download PDF">pdf</a>, <a href="/format/2402.01804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Internet of Things implementation barriers in the cold  supply chain: an integrated ISM-MICMAC and DEMATEL approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+K">Kazrin Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+S">Md. Saiful Islam</a>, 
<a href="/search/cs?searchtype=author&query=Jahin%2C+M+A">Md Abrar Jahin</a>, 
<a href="/search/cs?searchtype=author&query=Mridha%2C+M+F">M. F. Mridha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02152" title="Abstract">arXiv:2402.02152</a> (replaced) [<a href="/pdf/2402.02152" title="Download PDF">pdf</a>, <a href="/ps/2402.02152" title="Download PostScript">ps</a>, <a href="/format/2402.02152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Position Paper: Why the Shooting in the Dark Method Dominates  Recommender Systems Practice; A Call to Abandon Anti-Utopian Thinking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rohde%2C+D">David Rohde</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02242" title="Abstract">arXiv:2402.02242</a> (replaced) [<a href="/pdf/2402.02242" title="Download PDF">pdf</a>, <a href="/format/2402.02242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xin%2C+Y">Yi Xin</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Siqi Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Haodi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+J">Junlong Du</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaohong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yue Fan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qing Li</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yuntao Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02275" title="Abstract">arXiv:2402.02275</a> (replaced) [<a href="/pdf/2402.02275" title="Download PDF">pdf</a>, <a href="/format/2402.02275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SudokuSens: Enhancing Deep Learning Robustness for IoT Sensing  Applications using a Generative Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianshi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruijie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kara%2C+D">Denizhan Kara</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shengzhong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wertheimer%2C+D">Davis Wertheimer</a>, 
<a href="/search/cs?searchtype=author&query=Viros-i-Martin%2C+A">Antoni Viros-i-Martin</a>, 
<a href="/search/cs?searchtype=author&query=Ganti%2C+R">Raghu Ganti</a>, 
<a href="/search/cs?searchtype=author&query=Srivatsa%2C+M">Mudhakar Srivatsa</a>, 
<a href="/search/cs?searchtype=author&query=Abdelzaher%2C+T">Tarek Abdelzaher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in ACM Conference on Embedded Networked Sensor Systems (SenSys 23), November, 2023, Istanbul, Turkiye. This is the author's version of the work. It is posted here for your personal use. Not for redistribution. Publication rights licensed to the Association for Computing Machinery
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02616" title="Abstract">arXiv:2402.02616</a> (replaced) [<a href="/e-print/2402.02616" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Virtues of Pessimism in Inverse Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">David Wu</a>, 
<a href="/search/cs?searchtype=author&query=Swamy%2C+G">Gokul Swamy</a>, 
<a href="/search/cs?searchtype=author&query=Bagnell%2C+J+A">J. Andrew Bagnell</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z+S">Zhiwei Steven Wu</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+S">Sanjiban Choudhury</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been withdrawn by the authors pending edits from other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02632" title="Abstract">arXiv:2402.02632</a> (replaced) [<a href="/pdf/2402.02632" title="Download PDF">pdf</a>, <a href="/format/2402.02632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GIRT-Model: Automated Generation of Issue Report Templates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nikeghbal%2C+N">Nafiseh Nikeghbal</a>, 
<a href="/search/cs?searchtype=author&query=Kargaran%2C+A+H">Amir Hossein Kargaran</a>, 
<a href="/search/cs?searchtype=author&query=Heydarnoori%2C+A">Abbas Heydarnoori</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to be published at the 21st IEEE/ACM International Conference on Mining Software Repositories (MSR 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03019" title="Abstract">arXiv:2402.03019</a> (replaced) [<a href="/pdf/2402.03019" title="Download PDF">pdf</a>, <a href="/format/2402.03019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Taylor Videos for Action Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xiuyuan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Gedeon%2C+T">Tom Gedeon</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Liang Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Research report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03149" title="Abstract">arXiv:2402.03149</a> (replaced) [<a href="/pdf/2402.03149" title="Download PDF">pdf</a>, <a href="/format/2402.03149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparative Analysis of Microrings Based Incoherent Photonic GEMM  Accelerators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vatsavai%2C+S+S">Sairam Sri Vatsavai</a>, 
<a href="/search/cs?searchtype=author&query=Karempudi%2C+V+S+P">Venkata Sai Praneeth Karempudi</a>, 
<a href="/search/cs?searchtype=author&query=Alo%2C+O+A">Oluwaseun Adewunmi Alo</a>, 
<a href="/search/cs?searchtype=author&query=Thakkar%2C+I">Ishan Thakkar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To Appear at ISQED 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Emerging Technologies (cs.ET); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03216" title="Abstract">arXiv:2402.03216</a> (replaced) [<a href="/pdf/2402.03216" title="Download PDF">pdf</a>, <a href="/format/2402.03216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity  Text Embeddings Through Self-Knowledge Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianlv Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+S">Shitao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peitian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+K">Kun Luo</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+D">Defu Lian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zheng Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03227" title="Abstract">arXiv:2402.03227</a> (replaced) [<a href="/pdf/2402.03227" title="Download PDF">pdf</a>, <a href="/format/2402.03227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IGUANe: a 3D generalizable CycleGAN for multicenter harmonization of  brain MR images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roca%2C+V">Vincent Roca</a>, 
<a href="/search/cs?searchtype=author&query=Kuchcinski%2C+G">Gr&#xe9;gory Kuchcinski</a>, 
<a href="/search/cs?searchtype=author&query=Pruvo%2C+J">Jean-Pierre Pruvo</a>, 
<a href="/search/cs?searchtype=author&query=Manouvriez%2C+D">Dorian Manouvriez</a>, 
<a href="/search/cs?searchtype=author&query=Lopes%2C+R">Renaud Lopes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 8 figures; typos corrected
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03473" title="Abstract">arXiv:2402.03473</a> (replaced) [<a href="/pdf/2402.03473" title="Download PDF">pdf</a>, <a href="/format/2402.03473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing the Efficacy of Invisible Watermarks in AI-Generated Medical  Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xing%2C+X">Xiaodan Xing</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+H">Huiyu Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Fang%2C+Y">Yingying Fang</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+G">Guang Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03678" title="Abstract">arXiv:2402.03678</a> (replaced) [<a href="/pdf/2402.03678" title="Download PDF">pdf</a>, <a href="/format/2402.03678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Logical Specifications-guided Dynamic Task Sampling for Reinforcement  Learning Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shukla%2C+Y">Yash Shukla</a>, 
<a href="/search/cs?searchtype=author&query=Burman%2C+T">Tanushree Burman</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+A">Abhishek Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Wright%2C+R">Robert Wright</a>, 
<a href="/search/cs?searchtype=author&query=Velasquez%2C+A">Alvaro Velasquez</a>, 
<a href="/search/cs?searchtype=author&query=Sinapov%2C+J">Jivko Sinapov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03916" title="Abstract">arXiv:2402.03916</a> (replaced) [<a href="/pdf/2402.03916" title="Download PDF">pdf</a>, <a href="/format/2402.03916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Large Language Models Detect Rumors on Social Media?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+X">Xiang Tao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Junfei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04022" title="Abstract">arXiv:2402.04022</a> (replaced) [<a href="/pdf/2402.04022" title="Download PDF">pdf</a>, <a href="/format/2402.04022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A General Theory for Kernel Packets: from state space model to compactly  supported basis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/stat?searchtype=author&query=Tuo%2C+R">Rui Tuo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04097" title="Abstract">arXiv:2402.04097</a> (replaced) [<a href="/pdf/2402.04097" title="Download PDF">pdf</a>, <a href="/format/2402.04097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Deep Image Prior and Exploiting Self-Guidance for Image  Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+S">Shijun Liang</a>, 
<a href="/search/cs?searchtype=author&query=Bell%2C+E">Evan Bell</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+Q">Qing Qu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rongrong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ravishankar%2C+S">Saiprasad Ravishankar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04139" title="Abstract">arXiv:2402.04139</a> (replaced) [<a href="/pdf/2402.04139" title="Download PDF">pdf</a>, <a href="/format/2402.04139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> U-shaped Vision Mamba for Single Image Dehazing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhuoran Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chen Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04154" title="Abstract">arXiv:2402.04154</a> (replaced) [<a href="/pdf/2402.04154" title="Download PDF">pdf</a>, <a href="/format/2402.04154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Read to Play (R2-Play): Decision Transformer with Multimodal Game  Instruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yonggang Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Ge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+T">Tianyu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jiawei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+L">Liuyu Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+S">Shawn Yue</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S+W">Stephen W. Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhaofeng He</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04163" title="Abstract">arXiv:2402.04163</a> (replaced) [<a href="/pdf/2402.04163" title="Download PDF">pdf</a>, <a href="/ps/2402.04163" title="Download PostScript">ps</a>, <a href="/format/2402.04163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tempered Calculus for ML: Application to Hyperbolic Model Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nock%2C+R">Richard Nock</a>, 
<a href="/search/cs?searchtype=author&query=Amid%2C+E">Ehsan Amid</a>, 
<a href="/search/cs?searchtype=author&query=Nielsen%2C+F">Frank Nielsen</a>, 
<a href="/search/cs?searchtype=author&query=Soen%2C+A">Alexander Soen</a>, 
<a href="/search/cs?searchtype=author&query=Warmuth%2C+M+K">Manfred K. Warmuth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04436" title="Abstract">arXiv:2402.04436</a> (replaced) [<a href="/pdf/2402.04436" title="Download PDF">pdf</a>, <a href="/format/2402.04436" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continuous Multidimensional Scaling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Trosset%2C+M+W">Michael W. Trosset</a>, 
<a href="/search/stat?searchtype=author&query=Priebe%2C+C+E">Carey E. Priebe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages. Modified a sentence in the Abstract for greater clarity
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04578" title="Abstract">arXiv:2402.04578</a> (replaced) [<a href="/pdf/2402.04578" title="Download PDF">pdf</a>, <a href="/format/2402.04578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> S-Agents: self-organizing agents in open-ended environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiaqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuxian Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiachen Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Li Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preview, 23 pages, 12 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04614" title="Abstract">arXiv:2402.04614</a> (replaced) [<a href="/pdf/2402.04614" title="Download PDF">pdf</a>, <a href="/format/2402.04614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations  from Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+C">Chirag Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Tanneru%2C+S+H">Sree Harsha Tanneru</a>, 
<a href="/search/cs?searchtype=author&query=Lakkaraju%2C+H">Himabindu Lakkaraju</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04663" title="Abstract">arXiv:2402.04663</a> (replaced) [<a href="/pdf/2402.04663" title="Download PDF">pdf</a>, <a href="/format/2402.04663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLIF: Complementary Leaky Integrate-and-Fire Neuron for Spiking Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yulong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xiaopeng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Hongwei Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yue Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zunchang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+H">Haotian Fu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+B">Biao Pan</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+B">Bojun Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04825" title="Abstract">arXiv:2402.04825</a> (replaced) [<a href="/pdf/2402.04825" title="Download PDF">pdf</a>, <a href="/format/2402.04825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast Timing-Conditioned Latent Audio Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Evans%2C+Z">Zach Evans</a>, 
<a href="/search/cs?searchtype=author&query=Carr%2C+C">CJ Carr</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+J">Josiah Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Hawley%2C+S+H">Scott H. Hawley</a>, 
<a href="/search/cs?searchtype=author&query=Pons%2C+J">Jordi Pons</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code: <a href="https://github.com/Stability-AI/stable-audio-tools.">this https URL</a> Metrics: <a href="https://github.com/Stability-AI/stable-audio-metrics.">this https URL</a> Demo: <a href="https://stability-ai.github.io/stable-audio-demo">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04863" title="Abstract">arXiv:2402.04863</a> (replaced) [<a href="/pdf/2402.04863" title="Download PDF">pdf</a>, <a href="/format/2402.04863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Smart Contract Summarization via LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yingjie Mao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenkai Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04924" title="Abstract">arXiv:2402.04924</a> (replaced) [<a href="/pdf/2402.04924" title="Download PDF">pdf</a>, <a href="/format/2402.04924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two Trades is not Baffled: Condensing Graph via Crafting Rational  Gradient Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianle Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuchen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Beining Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+W">Wenqi Shao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Ping Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J+T">Joey Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yang You</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> An effective method for graph condensation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04971" title="Abstract">arXiv:2402.04971</a> (replaced) [<a href="/pdf/2402.04971" title="Download PDF">pdf</a>, <a href="/format/2402.04971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Sender Persuasion -- A Computational Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hossain%2C+S">Safwan Hossain</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tonghan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiling Chen</a>, 
<a href="/search/cs?searchtype=author&query=Parkes%2C+D+C">David C. Parkes</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haifeng Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05044" title="Abstract">arXiv:2402.05044</a> (replaced) [<a href="/pdf/2402.05044" title="Download PDF">pdf</a>, <a href="/format/2402.05044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lijun Li</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+B">Bowen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruohui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xuhao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+W">Wangmeng Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+J">Jing Shao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05050" title="Abstract">arXiv:2402.05050</a> (replaced) [<a href="/pdf/2402.05050" title="Download PDF">pdf</a>, <a href="/format/2402.05050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning Can Find Friends That Are Beneficial
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tupitsa%2C+N">Nazarii Tupitsa</a>, 
<a href="/search/cs?searchtype=author&query=Horv%C3%A1th%2C+S">Samuel Horv&#xe1;th</a>, 
<a href="/search/cs?searchtype=author&query=Tak%C3%A1%C4%8D%2C+M">Martin Tak&#xe1;&#x10d;</a>, 
<a href="/search/cs?searchtype=author&query=Gorbunov%2C+E">Eduard Gorbunov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05067" title="Abstract">arXiv:2402.05067</a> (replaced) [<a href="/pdf/2402.05067" title="Download PDF">pdf</a>, <a href="/format/2402.05067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiscale Modelling with Physics-informed Neural Network: from  Large-scale Dynamics to Small-scale Predictions in Complex Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Wang%2C+J">Jing Wang</a>, 
<a href="/search/physics?searchtype=author&query=Li%2C+Z">Zheng Li</a>, 
<a href="/search/physics?searchtype=author&query=Lai%2C+P">Pengyu Lai</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/physics?searchtype=author&query=Yang%2C+D">Di Yang</a>, 
<a href="/search/physics?searchtype=author&query=Yang%2C+D">Dewu Yang</a>, 
<a href="/search/physics?searchtype=author&query=Xu%2C+H">Hui Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Machine Learning (cs.LG); Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05080" title="Abstract">arXiv:2402.05080</a> (replaced) [<a href="/pdf/2402.05080" title="Download PDF">pdf</a>, <a href="/format/2402.05080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Designing three-way entangled and nonlocal two-way entangled single  particle states via alternate quantum walks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Panda%2C+D+K">Dinesh Kumar Panda</a>, 
<a href="/search/quant-ph?searchtype=author&query=Benjamin%2C+C">Colin Benjamin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Systems and Control (eess.SY); Optics (physics.optics)

</div>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05088" title="Abstract">arXiv:2402.05088</a> (replaced) [<a href="/pdf/2402.05088" title="Download PDF">pdf</a>, <a href="/format/2402.05088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domination and packing in graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=G%C3%B3mez%2C+R">Renzo G&#xf3;mez</a>, 
<a href="/search/math?searchtype=author&query=Guti%C3%A9rrez%2C+J">Juan Guti&#xe9;rrez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item370">Cross-lists</a></li>
<li><a href="#item425">Replacements</a></li>
</ul>
<small>[ total of 642 entries:  <b>1-642</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2402">2402</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
