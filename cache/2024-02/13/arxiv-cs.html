<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Fri  9 Feb 24  to  Mon 12 Feb 24, announced Tue, 13 Feb 24</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item559">Cross-lists</a></li>
<li><a href="#item640">Replacements</a></li>
</ul>
<small>[ total of 1010 entries:  <b>1-1010</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Tue, 13 Feb 24</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06629" title="Abstract">arXiv:2402.06629</a> [<a href="/pdf/2402.06629" title="Download PDF">pdf</a>, <a href="/format/2402.06629" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards the mathematical foundation of the minimum enclosing ball and  related problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vrahatis%2C+M+N">Michael N. Vrahatis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2401.03232">arXiv:2401.03232</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Artificial Intelligence (cs.AI); Geometric Topology (math.GT)

</div>
<p class="mathjax">Theoretical background is provided towards the mathematical foundation of the
minimum enclosing ball problem. This problem concerns the determination of the
unique spherical surface of smallest radius enclosing a given bounded set in
the d-dimensional Euclidean space. The study of several problems that are
similar or related to the minimum enclosing ball problem has received a
considerable impetus from the large amount of applications of these problems in
various fields of science and technology. The proposed theoretical framework is
based on several enclosing (covering) and partitioning (clustering) theorems
and provides among others bounds and relations between the circumradius,
inradius, diameter and width of a set. These enclosing and partitioning
theorems are considered as cornerstones in the field that strongly influencing
developments and generalizations to other spaces and non-Euclidean geometries.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06634" title="Abstract">arXiv:2402.06634</a> [<a href="/pdf/2402.06634" title="Download PDF">pdf</a>, <a href="/format/2402.06634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SocraSynth: Multi-LLM Reasoning with Conditional Statistics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+E+Y">Edward Y. Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 1 figure, 6 tables, 6 appendices
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs), while promising, face criticisms for biases,
hallucinations, and a lack of reasoning capability. This paper introduces
SocraSynth, a multi-LLM agent reasoning platform developed to mitigate these
issues. SocraSynth utilizes conditional statistics and systematic context
enhancement through continuous arguments, alongside adjustable debate
contentiousness levels. The platform typically involves a human moderator and
two LLM agents representing opposing viewpoints on a given subject. SocraSynth
operates in two main phases: knowledge generation and reasoning evaluation. In
the knowledge generation phase, the moderator defines the debate topic and
contentiousness level, prompting the agents to formulate supporting arguments
for their respective stances. The reasoning evaluation phase then employs
Socratic reasoning and formal logic principles to appraise the quality of the
arguments presented. The dialogue concludes with the moderator adjusting the
contentiousness from confrontational to collaborative, gathering final,
conciliatory remarks to aid in human reasoning and decision-making. Through
case studies in three distinct application domains, this paper showcases
SocraSynth's effectiveness in fostering rigorous research, dynamic reasoning,
comprehensive assessment, and enhanced collaboration. This underscores the
value of multi-agent interactions in leveraging LLMs for advanced knowledge
extraction and decision-making support.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06636" title="Abstract">arXiv:2402.06636</a> [<a href="/pdf/2402.06636" title="Download PDF">pdf</a>, <a href="/ps/2402.06636" title="Download PostScript">ps</a>, <a href="/format/2402.06636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multichain based marketplace Architecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Farooq%2C+M+S">Muhammad Shoaib Farooq</a>, 
<a href="/search/cs?searchtype=author&query=Jamil%2C+H">Hamza Jamil</a>, 
<a href="/search/cs?searchtype=author&query=Riaz%2C+H+S">Hafiz Sohail Riaz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">]A multichain non-fungible tokens (NFTs) marketplace is a decentralized
platform where users can buy, sell, and trade NFTs across multiple blockchain
networks by using cross communication bridge. In past most of NFT marketplace
was based on singlechain in which NFTs have been bought, sold, and traded on a
same blockchain network without the need for any external platform. The
singlechain based marketplace have faced number of issues such as performance,
scalability, flexibility and limited transaction throughput consequently long
confirmation times and high transaction fees during high network usage.
Firstly, this paper provides the comprehensive overview about NFT Multichain
architecture and explore the challenges and opportunities of designing and
implementation phase of multichain NFT marketplace to overcome the issue of
single chain-based architecture. NFT multichain marketplace architecture
includes different blockchain networks that communicate with each other.
Secondly, this paper discusses the concept of mainchain interacting with
sidechains which refers to multi blockchain architecture where multiple
blockchain networks are connected to each other in a hierarchical structure and
identifies key challenges related to interoperability, security, scalability,
and user adoption. Finally, we proposed a novel architecture for a multichain
NFT marketplace, which leverages the benefits of multiple blockchain networks
and marketplaces to overcome these key challenges. Moreover, proposed
architecture is evaluated through a case study, demonstrating its ability to
support efficient and secure transactions across multiple blockchain networks
and highlighting the future trends NFTs and marketplaces and comprehensive
discussion about the technology.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06640" title="Abstract">arXiv:2402.06640</a> [<a href="/pdf/2402.06640" title="Download PDF">pdf</a>, <a href="/ps/2402.06640" title="Download PostScript">ps</a>, <a href="/format/2402.06640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling and Optimization of Epidemiological Control Policies Through  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rao%2C+I">Ishir Rao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 8 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> J. Emerging Investigators Article (2023) Vol. 6
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Populations and Evolution (q-bio.PE)

</div>
<p class="mathjax">Pandemics involve the high transmission of a disease that impacts global and
local health and economic patterns. The impact of a pandemic can be minimized
by enforcing certain restrictions on a community. However, while minimizing
infection and death rates, these restrictions can also lead to economic crises.
Epidemiological models help propose pandemic control strategies based on
non-pharmaceutical interventions such as social distancing, curfews, and
lockdowns, reducing the economic impact of these restrictions. However,
designing manual control strategies while considering disease spread and
economic status is non-trivial. Optimal strategies can be designed through
multi-objective reinforcement learning (MORL) models, which demonstrate how
restrictions can be used to optimize the outcome of a pandemic. In this
research, we utilized an epidemiological Susceptible, Exposed, Infected,
Recovered, Deceased (SEIRD) model: a compartmental model for virtually
simulating a pandemic day by day. We combined the SEIRD model with a deep
double recurrent Q-network to train a reinforcement learning agent to enforce
the optimal restriction on the SEIRD simulation based on a reward function. We
tested two agents with unique reward functions and pandemic goals to obtain two
strategies. The first agent placed long lockdowns to reduce the initial spread
of the disease, followed by cyclical and shorter lockdowns to mitigate the
resurgence of the disease. The second agent provided similar infection rates
but an improved economy by implementing a 10-day lockdown and 20-day
no-restriction cycle. This use of reinforcement learning and epidemiological
modeling allowed for both economic and infection mitigation in multiple
pandemic scenarios.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06647" title="Abstract">arXiv:2402.06647</a> [<a href="/pdf/2402.06647" title="Download PDF">pdf</a>, <a href="/format/2402.06647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Large Language Model Hallucination via a Creativity  Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xuhui Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuxing Tian</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+F">Fengrui Hua</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chengjin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuanzhuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jian Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Hallucinations in large language models (LLMs) are always seen as
limitations. However, could they also be a source of creativity? This survey
explores this possibility, suggesting that hallucinations may contribute to LLM
application by fostering creativity. This survey begins with a review of the
taxonomy of hallucinations and their negative impact on LLM reliability in
critical applications. Then, through historical examples and recent relevant
theories, the survey explores the potential creative benefits of hallucinations
in LLMs. To elucidate the value and evaluation criteria of this connection, we
delve into the definitions and assessment methods of creativity. Following the
framework of divergent and convergent thinking phases, the survey
systematically reviews the literature on transforming and harnessing
hallucinations for creativity in LLMs. Finally, the survey discusses future
research directions, emphasizing the need to further explore and refine the
application of hallucinations in creative processes within LLMs.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06649" title="Abstract">arXiv:2402.06649</a> [<a href="/pdf/2402.06649" title="Download PDF">pdf</a>, <a href="/ps/2402.06649" title="Download PostScript">ps</a>, <a href="/format/2402.06649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Replacing CAPTCHA with XNO micropayments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tiruvayipati%2C+S">Sujanavan Tiruvayipati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">As technology and gadgets continue to evolve, the need for bot-friendly and
user-friendly internet becomes increasingly critical. This work discusses a
methodology for implementation and feasibility of replacing traditional CAPTCHA
mechanisms with Nano(XNO) cryptocurrency micropayments as a win-win solution
and leverages the decentralized and secure nature of cryptocurrencies to
introduce a micropayment-based authentication system. This approach not only
enhances security by adding a financial barrier for automated bots but also
provides a more seamless and efficient user experience. The benefits of this
approach include reducing the burden on users while creating a socio-economic
model that incentivizes internet service providers and content creators, even
when accessed by bots. Furthermore, the integration of XNO micropayments could
potentially contribute to the broader adoption and acceptance of digital
currencies in everyday online transactions.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06650" title="Abstract">arXiv:2402.06650</a> [<a href="/pdf/2402.06650" title="Download PDF">pdf</a>, <a href="/ps/2402.06650" title="Download PostScript">ps</a>, <a href="/format/2402.06650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Shifting Landscape of Cybersecurity: The Impact of Remote Work and  COVID-19 on Data Breach Trends
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ozer%2C+M">Murat Ozer</a>, 
<a href="/search/cs?searchtype=author&query=Kose%2C+Y">Yasin Kose</a>, 
<a href="/search/cs?searchtype=author&query=Bastug%2C+M">Mehmet Bastug</a>, 
<a href="/search/cs?searchtype=author&query=Kucukkaya%2C+G">Goksel Kucukkaya</a>, 
<a href="/search/cs?searchtype=author&query=Varlioglu%2C+E+R">Eva Ruhsar Varlioglu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">This study examines the impact of the COVID-19 pandemic on cybersecurity and
data breaches, with a specific focus on the shift toward remote work. The study
identifies trends and offers insights into cybersecurity incidents by analyzing
data breaches two years before and two years after the start of remote work.
Data was collected from the Montana Department of Justice Data Breach database
and consisted of data breaches that occurred between April 2018 and April 2022.
The findings inform best practices for cybersecurity preparedness in remote
work environments, aiding organizations to enhance their defenses. Although the
study's data is limited to Montana, it offers valuable insights for
cybersecurity professionals worldwide. As remote work continues to evolve,
organizations must remain adaptable and vigilant in their cybersecurity
strategies.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06653" title="Abstract">arXiv:2402.06653</a> [<a href="/pdf/2402.06653" title="Download PDF">pdf</a>, <a href="/format/2402.06653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using remotely sensed data for air pollution assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bernardino%2C+T">Teresa Bernardino</a>, 
<a href="/search/cs?searchtype=author&query=Oliveira%2C+M+A">Maria Alexandra Oliveira</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+J+N">Jo&#xe3;o Nuno Silva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
<p class="mathjax">Air pollution constitutes a global problem of paramount importance that
affects not only human health, but also the environment. The existence of
spatial and temporal data regarding the concentrations of pollutants is crucial
for performing air pollution studies and monitor emissions. However, although
observation data presents great temporal coverage, the number of stations is
very limited and they are usually built in more populated areas.
<br />The main objective of this work is to create models capable of inferring
pollutant concentrations in locations where no observation data exists. A
machine learning model, more specifically the random forest model, was
developed for predicting concentrations in the Iberian Peninsula in 2019 for
five selected pollutants: $NO_2$, $O_3$ $SO_2$, $PM10$, and $PM2.5$. Model
features include satellite measurements, meteorological variables, land use
classification, temporal variables (month, day of year), and spatial variables
(latitude, longitude, altitude).
<br />The models were evaluated using various methods, including station 10-fold
cross-validation, in which in each fold observations from 10\% of the stations
are used as testing data and the rest as training data. The $R^2$, RMSE and
mean bias were determined for each model. The $NO_2$ and $O_3$ models presented
good values of $R^2$, 0.5524 and 0.7462, respectively. However, the $SO_2$,
$PM10$, and $PM2.5$ models performed very poorly in this regard, with $R^2$
values of -0.0231, 0.3722, and 0.3303, respectively. All models slightly
overestimated the ground concentrations, except the $O_3$ model. All models
presented acceptable cross-validation RMSE, except the $O_3$ and $PM10$ models
where the mean value was a little higher (12.5934 $\mu g/m^3$ and 10.4737 $\mu
g/m^3$, respectively).
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06654" title="Abstract">arXiv:2402.06654</a> [<a href="/pdf/2402.06654" title="Download PDF">pdf</a>, <a href="/ps/2402.06654" title="Download PostScript">ps</a>, <a href="/format/2402.06654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conversational Crowdsensing: A Parallel Intelligence Powered Novel  Sensing Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhengqiu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+S">Sihang Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Q">Quanjun Yin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jincai Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei-Yue Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">The transition from CPS-based Industry 4.0 to CPSS-based Industry 5.0 brings
new requirements and opportunities to current sensing approaches, especially in
light of recent progress in Chatbots and Large Language Models (LLMs).
Therefore, the advancement of parallel intelligence-powered Crowdsensing
Intelligence (CSI) is witnessed, which is currently advancing towards
linguistic intelligence. In this paper, we propose a novel sensing paradigm,
namely conversational crowdsensing, for Industry 5.0. It can alleviate workload
and professional requirements of individuals and promote the organization and
operation of diverse workforce, thereby facilitating faster response and wider
popularization of crowdsensing systems. Specifically, we design the
architecture of conversational crowdsensing to effectively organize three types
of participants (biological, robotic, and digital) from diverse communities.
Through three levels of effective conversation (i.e., inter-human, human-AI,
and inter-AI), complex interactions and service functionalities of different
workers can be achieved to accomplish various tasks across three sensing phases
(i.e., requesting, scheduling, and executing). Moreover, we explore the
foundational technologies for realizing conversational crowdsensing,
encompassing LLM-based multi-agent systems, scenarios engineering and
conversational human-AI cooperation. Finally, we present potential industrial
applications of conversational crowdsensing and discuss its implications. We
envision that conversations in natural language will become the primary
communication channel during crowdsensing process, enabling richer information
exchange and cooperative problem-solving among humans, robots, and AI.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06655" title="Abstract">arXiv:2402.06655</a> [<a href="/pdf/2402.06655" title="Download PDF">pdf</a>, <a href="/format/2402.06655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Text Purification: A Large Language Model Approach for  Defense
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moraffah%2C+R">Raha Moraffah</a>, 
<a href="/search/cs?searchtype=author&query=Khandelwal%2C+S">Shubh Khandelwal</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharjee%2C+A">Amrita Bhattacharjee</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huan Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PAKDD 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Adversarial purification is a defense mechanism for safeguarding classifiers
against adversarial attacks without knowing the type of attacks or training of
the classifier. These techniques characterize and eliminate adversarial
perturbations from the attacked inputs, aiming to restore purified samples that
retain similarity to the initially attacked ones and are correctly classified
by the classifier. Due to the inherent challenges associated with
characterizing noise perturbations for discrete inputs, adversarial text
purification has been relatively unexplored. In this paper, we investigate the
effectiveness of adversarial purification methods in defending text
classifiers. We propose a novel adversarial text purification that harnesses
the generative capabilities of Large Language Models (LLMs) to purify
adversarial text without the need to explicitly characterize the discrete noise
perturbations. We utilize prompt engineering to exploit LLMs for recovering the
purified examples for given adversarial examples such that they are
semantically similar and correctly classified. Our proposed method demonstrates
remarkable performance over various classifiers, improving their accuracy under
the attack by over 65% on average.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06659" title="Abstract">arXiv:2402.06659</a> [<a href="/pdf/2402.06659" title="Download PDF">pdf</a>, <a href="/format/2402.06659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuancheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiarui Yao</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+M">Manli Shu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yanchao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zichu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+N">Ning Yu</a>, 
<a href="/search/cs?searchtype=author&query=Goldstein%2C+T">Tom Goldstein</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Vision-Language Models (VLMs) excel in generating textual responses from
visual inputs, yet their versatility raises significant security concerns. This
study takes the first step in exposing VLMs' susceptibility to data poisoning
attacks that can manipulate responses to innocuous, everyday prompts. We
introduce Shadowcast, a stealthy data poisoning attack method where poison
samples are visually indistinguishable from benign images with matching texts.
Shadowcast demonstrates effectiveness in two attack types. The first is Label
Attack, tricking VLMs into misidentifying class labels, such as confusing
Donald Trump for Joe Biden. The second is Persuasion Attack, which leverages
VLMs' text generation capabilities to craft narratives, such as portraying junk
food as health food, through persuasive and seemingly rational descriptions. We
show that Shadowcast are highly effective in achieving attacker's intentions
using as few as 50 poison samples. Moreover, these poison samples remain
effective across various prompts and are transferable across different VLM
architectures in the black-box setting. This work reveals how poisoned VLMs can
generate convincing yet deceptive misinformation and underscores the importance
of data quality for responsible deployments of VLMs. Our code is available at:
https://github.com/umd-huang-lab/VLM-Poisoning.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06660" title="Abstract">arXiv:2402.06660</a> [<a href="/pdf/2402.06660" title="Download PDF">pdf</a>, <a href="/ps/2402.06660" title="Download PostScript">ps</a>, <a href="/format/2402.06660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The role of the metaverse in calibrating an embodied artificial general  intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmalzried%2C+M">Martin Schmalzried</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at the conference second international conference on human-centred AI ethics: seeing the human in the artificial (HCAIE 2023): <a href="https://ethics-ai.eu/hcaie2023/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">This paper examines the concept of embodied artificial general intelligence
(AGI), its relationship to human consciousness, and the key role of the
metaverse in facilitating this relationship. The paper leverages theoretical
frameworks such as embodied cognition, Michael Levin's computational boundary
of a "Self," Donald D. Hoffman's Interface Theory of Perception, and Bernardo
Kastrup's analytical idealism to build the argument for achieving embodied AGI.
It contends that our perceived outer reality is a symbolic representation of
alternate inner states of being, and that AGI could embody a higher
consciousness with a larger computational boundary. The paper further discusses
the developmental stages of AGI, the requirements for the emergence of an
embodied AGI, the importance of a calibrated symbolic interface for AGI, and
the key role played by the metaverse, decentralized systems, open-source
blockchain technology, as well as open-source AI research. It also explores the
idea of a feedback loop between AGI and human users in metaverse spaces as a
tool for AGI calibration, as well as the role of local homeostasis and
decentralized governance as preconditions for achieving a stable embodied AGI.
The paper concludes by emphasizing the importance of achieving a certain degree
of harmony in human relations and recognizing the interconnectedness of
humanity at a global level, as key prerequisites for the emergence of a stable
embodied AGI.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06661" title="Abstract">arXiv:2402.06661</a> [<a href="/pdf/2402.06661" title="Download PDF">pdf</a>, <a href="/ps/2402.06661" title="Download PostScript">ps</a>, <a href="/format/2402.06661" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Authentication and integrity of smartphone videos through multimedia  container structure analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huam%C3%A1n%2C+C+Q">Carlos Quinto Huam&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=Orozco%2C+A+L+S">Ana Lucila Sandoval Orozco</a>, 
<a href="/search/cs?searchtype=author&query=Villalba%2C+L+J+G">Luis Javier Garc&#xed;a Villalba</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Quinto Huam\'an, A. L. Sandoval Orozco, L. J. Garc\'ia Villalba:
  Authentication and Integrity of Smartphone Videos Through Multimedia
  Container Structure Analysis. Future Generation Computer Systems. Vol. 108,
  pp. 15-33, July 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Nowadays, mobile devices have become the natural substitute for the digital
camera, as they capture everyday situations easily and quickly, encouraging
users to express themselves through images and videos. These videos can be
shared across different platforms exposing them to any kind of intentional
manipulation by criminals who are aware of the weaknesses of forensic
techniques to accuse an innocent person or exonerate a guilty person in a
judicial process. Commonly, manufacturers do not comply 100% with the
specifications of the standards for the creation of videos. Also, videos shared
on social networks, and instant messaging applications go through filtering and
compression processes to reduce their size, facilitate their transfer, and
optimize storage on their platforms. The omission of specifications and results
of transformations carried out by the platforms embed a features pattern in the
multimedia container of the videos. These patterns make it possible to
distinguish the brand of the device that generated the video, social network,
and instant messaging application that was used for the transfer. Research in
recent years has focused on the analysis of AVI containers and tiny video
datasets. This work presents a novel technique to detect possible attacks
against MP4, MOV, and 3GP format videos that affect their integrity and
authenticity. The method is based on the analysis of the structure of video
containers generated by mobile devices and their behavior when shared through
social networks, instant messaging applications, or manipulated by editing
programs. The objectives of the proposal are to verify the integrity of videos,
identify the source of acquisition and distinguish between original and
manipulated videos.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06662" title="Abstract">arXiv:2402.06662</a> [<a href="/pdf/2402.06662" title="Download PDF">pdf</a>, <a href="/format/2402.06662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sign Rank Limitations for Attention-Based Graph Decoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S+H">Su Hyeong Lee</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qingqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kondor%2C+R">Risi Kondor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Inner product-based decoders are among the most influential frameworks used
to extract meaningful data from latent embeddings. However, such decoders have
shown limitations in representation capacity in numerous works within the
literature, which have been particularly notable in graph reconstruction
problems. In this paper, we provide the first theoretical elucidation of this
pervasive phenomenon in graph data, and suggest straightforward modifications
to circumvent this issue without deviating from the inner product framework.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06663" title="Abstract">arXiv:2402.06663</a> [<a href="/pdf/2402.06663" title="Download PDF">pdf</a>, <a href="/format/2402.06663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable Adversarial Learning Framework on Physical Layer Secret Keys  Combating Malicious Reconfigurable Intelligent Surface
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhuangkun Wei</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wenxiu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+W">Weisi Guo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The development of reconfigurable intelligent surfaces (RIS) is a
double-edged sword to physical layer security (PLS). Whilst a legitimate RIS
can yield beneficial impacts including increased channel randomness to enhance
physical layer secret key generation (PL-SKG), malicious RIS can poison
legitimate channels and crack most of existing PL-SKGs. In this work, we
propose an adversarial learning framework between legitimate parties (namely
Alice and Bob) to address this Man-in-the-middle malicious RIS (MITM-RIS)
eavesdropping. First, the theoretical mutual information gap between legitimate
pairs and MITM-RIS is deduced. Then, Alice and Bob leverage generative
adversarial networks (GANs) to learn to achieve a common feature surface that
does not have mutual information overlap with MITM-RIS. Next, we aid signal
processing interpretation of black-box neural networks by using a symbolic
explainable AI (xAI) representation. These symbolic terms of dominant neurons
aid feature engineering-based validation and future design of PLS common
feature space. Simulation results show that our proposed GAN-based and
symbolic-based PL-SKGs can achieve high key agreement rates between legitimate
users, and is even resistant to MITM-RIS Eve with the knowledge of legitimate
feature generation (NNs or formulas). This therefore paves the way to secure
wireless communications with untrusted reflective devices in future 6G.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06664" title="Abstract">arXiv:2402.06664</a> [<a href="/pdf/2402.06664" title="Download PDF">pdf</a>, <a href="/format/2402.06664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM Agents can Autonomously Hack Websites
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+R">Richard Fang</a>, 
<a href="/search/cs?searchtype=author&query=Bindu%2C+R">Rohan Bindu</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Akul Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+Q">Qiusi Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Daniel Kang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In recent years, large language models (LLMs) have become increasingly
capable and can now interact with tools (i.e., call functions), read documents,
and recursively call themselves. As a result, these LLMs can now function
autonomously as agents. With the rise in capabilities of these agents, recent
work has speculated on how LLM agents would affect cybersecurity. However, not
much is known about the offensive capabilities of LLM agents.
<br />In this work, we show that LLM agents can autonomously hack websites,
performing tasks as complex as blind database schema extraction and SQL
injections without human feedback. Importantly, the agent does not need to know
the vulnerability beforehand. This capability is uniquely enabled by frontier
models that are highly capable of tool use and leveraging extended context.
Namely, we show that GPT-4 is capable of such hacks, but existing open-source
models are not. Finally, we show that GPT-4 is capable of autonomously finding
vulnerabilities in websites in the wild. Our findings raise questions about the
widespread deployment of LLMs.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06665" title="Abstract">arXiv:2402.06665</a> [<a href="/pdf/2402.06665" title="Download PDF">pdf</a>, <a href="/format/2402.06665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Essential Role of Causality in Foundation World Models for Embodied  AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gupta%2C+T">Tarun Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+W">Wenbo Gong</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Pawlowski%2C+N">Nick Pawlowski</a>, 
<a href="/search/cs?searchtype=author&query=Hilmkil%2C+A">Agrin Hilmkil</a>, 
<a href="/search/cs?searchtype=author&query=Scetbon%2C+M">Meyer Scetbon</a>, 
<a href="/search/cs?searchtype=author&query=Famoti%2C+A">Ade Famoti</a>, 
<a href="/search/cs?searchtype=author&query=Llorens%2C+A+J">Ashley Juan Llorens</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jianfeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Bauer%2C+S">Stefan Bauer</a>, 
<a href="/search/cs?searchtype=author&query=Kragic%2C+D">Danica Kragic</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Cheng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Recent advances in foundation models, especially in large multi-modal models
and conversational agents, have ignited interest in the potential of generally
capable embodied agents. Such agents would require the ability to perform new
tasks in many different real-world environments. However, current foundation
models fail to accurately model physical interactions with the real world thus
not sufficient for Embodied AI. The study of causality lends itself to the
construction of veridical world models, which are crucial for accurately
predicting the outcomes of possible interactions. This paper focuses on the
prospects of building foundation world models for the upcoming generation of
embodied agents and presents a novel viewpoint on the significance of causality
within these. We posit that integrating causal considerations is vital to
facilitate meaningful physical interactions with the world. Finally, we
demystify misconceptions about causality in this context and present our
outlook for future research.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06669" title="Abstract">arXiv:2402.06669</a> [<a href="/pdf/2402.06669" title="Download PDF">pdf</a>, <a href="/ps/2402.06669" title="Download PostScript">ps</a>, <a href="/format/2402.06669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compression effects and scene details on the source camera  identification of digital videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=L%C3%B3pez%2C+R+R">Raquel Ramos L&#xf3;pez</a>, 
<a href="/search/cs?searchtype=author&query=Orozco%2C+A+L+S">Ana Lucila Sandoval Orozco</a>, 
<a href="/search/cs?searchtype=author&query=Villalba%2C+L+J+G">Luis Javier Garc&#xed;a Villalba</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> R. Ramos L\'opez, A. L. Sandoval Orozco, L. J. Garc\'ia Villalba:
  Compression Effects and Scene Details on the Source Camera Identification of
  Digital Videos. Expert Systems with Applications, Vol. 170, pp. 114515, May
  2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The continuous growth of technologies like 4G or 5G has led to a massive use
of mobile devices such as smartphones and tablets. This phenomenon, combined
with the fact that people use mobile phones for a longer period of time,
results in mobile phones becoming the main source of creation of visual
information. However, its reliability as a true representation of reality
cannot be taken for granted due to the constant increase in editing software.
This makes it easier to alter original content without leaving a noticeable
trace in the modification. Therefore, it is essential to introduce forensic
analysis mechanisms to guarantee the authenticity or integrity of a certain
digital video, particularly if it may be considered as evidence in legal
proceedings. This paper explains the branch of multimedia forensic analysis
that allows to determine the identification of the source of acquisition of a
certain video by exploiting the unique traces left by the camera sensor of the
mobile device in visual content. To do this, a technique that performs the
identification of the source of acquisition of digital videos from mobile
devices is presented. It involves 3 stages: (1) Extraction of the sensor
fingerprint by applying the block-based technique. (2) Filtering the strong
component of the PRNU signal to improve the quality of the sensor fingerprint.
(3) Classification of digital videos in an open scenario, that is, where the
forensic analyst does not need to have access to the device that recorded the
video to find out the origin of the video. The main contribution of the
proposed technique eliminates the details of the scene to improve the PRNU
fingerprint. It should be noted that these techniques are applied to digital
images and not to digital videos.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06673" title="Abstract">arXiv:2402.06673</a> [<a href="/pdf/2402.06673" title="Download PDF">pdf</a>, <a href="/format/2402.06673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Explainable AI Toward Human-Like Intelligence: Forging the  Path to Artificial Brain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yongchen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+R">Richard Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The intersection of Artificial Intelligence (AI) and neuroscience in
Explainable AI (XAI) is pivotal for enhancing transparency and interpretability
in complex decision-making processes. This paper explores the evolution of XAI
methodologies, ranging from feature-based to human-centric approaches, and
delves into their applications in diverse domains, including healthcare and
finance. The challenges in achieving explainability in generative models,
ensuring responsible AI practices, and addressing ethical implications are
discussed. The paper further investigates the potential convergence of XAI with
cognitive sciences, the development of emotionally intelligent AI, and the
quest for Human-Like Intelligence (HLI) in AI systems. As AI progresses towards
Artificial General Intelligence (AGI), considerations of consciousness, ethics,
and societal impact become paramount. The ongoing pursuit of deciphering the
mysteries of the brain with AI and the quest for HLI represent transformative
endeavors, bridging technical advancements with multidisciplinary explorations
of human cognition.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06674" title="Abstract">arXiv:2402.06674</a> [<a href="/pdf/2402.06674" title="Download PDF">pdf</a>, <a href="/format/2402.06674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Practical Membership Privacy of Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tobaben%2C+M">Marlon Tobaben</a>, 
<a href="/search/cs?searchtype=author&query=Pradhan%2C+G">Gauri Pradhan</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yuan He</a>, 
<a href="/search/cs?searchtype=author&query=J%C3%A4lk%C3%B6%2C+J">Joonas J&#xe4;lk&#xf6;</a>, 
<a href="/search/cs?searchtype=author&query=Honkela%2C+A">Antti Honkela</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We apply a state-of-the-art membership inference attack (MIA) to
systematically test the practical privacy vulnerability of fine-tuning large
image classification models.We focus on understanding the properties of data
sets and samples that make them vulnerable to membership inference. In terms of
data set properties, we find a strong power law dependence between the number
of examples per class in the data and the MIA vulnerability, as measured by
true positive rate of the attack at a low false positive rate. For an
individual sample, large gradients at the end of training are strongly
correlated with MIA vulnerability.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06675" title="Abstract">arXiv:2402.06675</a> [<a href="/pdf/2402.06675" title="Download PDF">pdf</a>, <a href="/ps/2402.06675" title="Download PostScript">ps</a>, <a href="/format/2402.06675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Masked language model for multi-source EHR trajectories contextual  representation learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amirahmadi%2C+A">Ali Amirahmadi</a> (1), 
<a href="/search/cs?searchtype=author&query=Ohlsson%2C+M">Mattias Ohlsson</a> (1,2), 
<a href="/search/cs?searchtype=author&query=Etminani%2C+K">Kobra Etminani</a> (1), 
<a href="/search/cs?searchtype=author&query=Melander%2C+O">Olle Melander</a> (3), 
<a href="/search/cs?searchtype=author&query=Bj%C3%B6rk%2C+J">Jonas Bj&#xf6;rk</a> (4) ((1) Center for Applied Intelligent Systems Research, Halmstad University, (2) Centre for Environmental and Climate Science, Lund University, (3) Department of Clinical Sciences, Lund University, (4) Division of Occupational and Environmental Medicine, Lund University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at Proceedings of MIE 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Using electronic health records data and machine learning to guide future
decisions needs to address challenges, including 1) long/short-term
dependencies and 2) interactions between diseases and interventions.
Bidirectional transformers have effectively addressed the first challenge. Here
we tackled the latter challenge by masking one source (e.g., ICD10 codes) and
training the transformer to predict it using other sources (e.g., ATC codes).
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06682" title="Abstract">arXiv:2402.06682</a> [<a href="/pdf/2402.06682" title="Download PDF">pdf</a>, <a href="/format/2402.06682" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Private Knowledge Sharing in Distributed Learning: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Supeksala%2C+Y">Yasas Supeksala</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D+C">Dinh C. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Ming Ding</a>, 
<a href="/search/cs?searchtype=author&query=Ranbaduge%2C+T">Thilina Ranbaduge</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+C">Calson Chua</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>, 
<a href="/search/cs?searchtype=author&query=Poor%2C+H+V">H. Vincent Poor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Manuscript submitted to ACM
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The rise of Artificial Intelligence (AI) has revolutionized numerous
industries and transformed the way society operates. Its widespread use has led
to the distribution of AI and its underlying data across many intelligent
systems. In this light, it is crucial to utilize information in learning
processes that are either distributed or owned by different entities. As a
result, modern data-driven services have been developed to integrate
distributed knowledge entities into their outcomes. In line with this goal, the
latest AI models are frequently trained in a decentralized manner. Distributed
learning involves multiple entities working together to make collective
predictions and decisions. However, this collaboration can also bring about
security vulnerabilities and challenges. This paper provides an in-depth survey
on private knowledge sharing in distributed learning, examining various
knowledge components utilized in leading distributed learning architectures.
Our analysis sheds light on the most critical vulnerabilities that may arise
when using these components in a distributed setting. We further identify and
examine defensive strategies for preserving the privacy of these knowledge
components and preventing malicious parties from manipulating or accessing the
knowledge information. Finally, we highlight several key limitations of
knowledge sharing in distributed learning and explore potential avenues for
future research.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06688" title="Abstract">arXiv:2402.06688</a> [<a href="/pdf/2402.06688" title="Download PDF">pdf</a>, <a href="/ps/2402.06688" title="Download PostScript">ps</a>, <a href="/format/2402.06688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparison of machine learning and statistical approaches for digital  elevation model (DEM) correction: interim results
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Okolie%2C+C">Chukwuma Okolie</a>, 
<a href="/search/cs?searchtype=author&query=Adeleke%2C+A">Adedayo Adeleke</a>, 
<a href="/search/cs?searchtype=author&query=Smit%2C+J">Julian Smit</a>, 
<a href="/search/cs?searchtype=author&query=Mills%2C+J">Jon Mills</a>, 
<a href="/search/cs?searchtype=author&query=Maduako%2C+I">Iyke Maduako</a>, 
<a href="/search/cs?searchtype=author&query=Ogbeta%2C+C">Caleb Ogbeta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 pages, 2 figures, 1 table, ISPRS conference extended abstract
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Several methods have been proposed for correcting the elevation bias in
digital elevation models (DEMs) for example, linear regression. Nowadays,
supervised machine learning enables the modelling of complex relationships
between variables, and has been deployed by researchers in a variety of fields.
In the existing literature, several studies have adopted either machine
learning or statistical approaches in the task of DEM correction. However, to
our knowledge, none of these studies have compared the performance of both
approaches, especially with regard to open-access global DEMs. Our previous
work has already shown the potential of machine learning approaches,
specifically gradient boosted decision trees (GBDTs) for DEM correction. In
this study, we share some results from the comparison of three recent
implementations of gradient boosted decision trees (XGBoost, LightGBM and
CatBoost), versus multiple linear regression (MLR) for enhancing the vertical
accuracy of 30 m Copernicus and AW3D global DEMs in Cape Town, South Africa.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06690" title="Abstract">arXiv:2402.06690</a> [<a href="/pdf/2402.06690" title="Download PDF">pdf</a>, <a href="/format/2402.06690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Models for Source Code Synthesis and Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niyogi%2C+M">Mitodru Niyogi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Master thesis submitted to University of Heidelberg, Germany on 30th July, 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Programming Languages (cs.PL)

</div>
<p class="mathjax">Natural language (NL) to code suggestion systems assist developers in
Integrated Development Environments (IDEs) by translating NL utterances into
compilable code snippet. The current approaches mainly involve hard-coded,
rule-based systems based on semantic parsing. These systems make heavy use of
hand-crafted rules that map patterns in NL or elements in its syntax parse tree
to various query constructs and can only work on a limited subset of NL with a
restricted NL syntax. These systems are unable to extract semantic information
from the coding intents of the developer, and often fail to infer types, names,
and the context of the source code to get accurate system-level code
suggestions. In this master thesis, we present sequence-to-sequence deep
learning models and training paradigms to map NL to general-purpose programming
languages that can assist users with suggestions of source code snippets, given
a NL intent, and also extend auto-completion functionality of the source code
to users while they are writing source code. The developed architecture
incorporates contextual awareness into neural models which generate source code
tokens directly instead of generating parse trees/abstract meaning
representations from the source code and converting them back to source code.
The proposed pretraining strategy and the data augmentation techniques improve
the performance of the proposed architecture. The proposed architecture has
been found to exceed the performance of a neural semantic parser, TranX, based
on the BLEU-4 metric by 10.82%. Thereafter, a finer analysis for the parsable
code translations from the NL intent for CoNaLA challenge was introduced. The
proposed system is bidirectional as it can be also used to generate NL code
documentation given source code. Lastly, a RoBERTa masked language model for
Python was proposed to extend the developed system for code completion.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06693" title="Abstract">arXiv:2402.06693</a> [<a href="/pdf/2402.06693" title="Download PDF">pdf</a>, <a href="/format/2402.06693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fostering the integration of European Open Data into Data Spaces through  High-Quality Metadata
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Conde%2C+J">Javier Conde</a>, 
<a href="/search/cs?searchtype=author&query=Pozo%2C+A">Alejandro Pozo</a>, 
<a href="/search/cs?searchtype=author&query=Munoz-Arcentales%2C+A">Andr&#xe9;s Munoz-Arcentales</a>, 
<a href="/search/cs?searchtype=author&query=Choque%2C+J">Johnny Choque</a>, 
<a href="/search/cs?searchtype=author&query=Alonso%2C+%C3%81">&#xc1;lvaro Alonso</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">The term Data Space, understood as the secure exchange of data in distributed
systems, ensuring openness, transparency, decentralization, sovereignty, and
interoperability of information, has gained importance during the last years.
However, Data Spaces are in an initial phase of definition, and new research is
necessary to address their requirements. The Open Data ecosystem can be
understood as one of the precursors of Data Spaces as it provides mechanisms to
ensure the interoperability of information through resource discovery,
information exchange, and aggregation via metadata. However, Data Spaces
require more advanced capabilities including the automatic and scalable
generation and publication of high-quality metadata. In this work, we present a
set of software tools that facilitate the automatic generation and publication
of metadata, the modeling of datasets through standards, and the assessment of
the quality of the generated metadata. We validate all these tools through the
YODA Open Data Portal showing how they can be connected to integrate Open Data
into Data Spaces.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06694" title="Abstract">arXiv:2402.06694</a> [<a href="/pdf/2402.06694" title="Download PDF">pdf</a>, <a href="/ps/2402.06694" title="Download PostScript">ps</a>, <a href="/format/2402.06694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Intelligent Agents in Combat Simulations for Wargaming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Black%2C+S">Scotty Black</a>, 
<a href="/search/cs?searchtype=author&query=Darken%2C+C">Christian Darken</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2402.06075">arXiv:2402.06075</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> I/ITSEC Conference Proceedings 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Remaining competitive in future conflicts with technologically-advanced
competitors requires us to accelerate our research and development in
artificial intelligence (AI) for wargaming. More importantly, leveraging
machine learning for intelligent combat behavior development will be key to one
day achieving superhuman performance in this domain--elevating the quality and
accelerating the speed of our decisions in future wars. Although deep
reinforcement learning (RL) continues to show promising results in intelligent
agent behavior development in games, it has yet to perform at or above the
human level in the long-horizon, complex tasks typically found in combat
modeling and simulation. Capitalizing on the proven potential of RL and recent
successes of hierarchical reinforcement learning (HRL), our research is
investigating and extending the use of HRL to create intelligent agents capable
of performing effectively in these large and complex simulation environments.
Our ultimate goal is to develop an agent capable of superhuman performance that
could then serve as an AI advisor to military planners and decision-makers.
This papers covers our ongoing approach and the first three of our five
research areas aimed at managing the exponential growth of computations that
have thus far limited the use of AI in combat simulations: (1) developing an
HRL training framework and agent architecture for combat units; (2) developing
a multi-model framework for agent decision-making; (3) developing
dimension-invariant observation abstractions of the state space to manage the
exponential growth of computations; (4) developing an intrinsic rewards engine
to enable long-term planning; and (5) implementing this framework into a
higher-fidelity combat simulation.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06695" title="Abstract">arXiv:2402.06695</a> [<a href="/pdf/2402.06695" title="Download PDF">pdf</a>, <a href="/format/2402.06695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating LLMs for Explainable Fault Diagnosis in Complex Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dave%2C+A+J">Akshay J. Dave</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+N">Tat Nghia Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Vilim%2C+R+B">Richard B. Vilim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Systems and Control (eess.SY)

</div>
<p class="mathjax">This paper introduces an integrated system designed to enhance the
explainability of fault diagnostics in complex systems, such as nuclear power
plants, where operator understanding is critical for informed decision-making.
By combining a physics-based diagnostic tool with a Large Language Model, we
offer a novel solution that not only identifies faults but also provides clear,
understandable explanations of their causes and implications. The system's
efficacy is demonstrated through application to a molten salt facility,
showcasing its ability to elucidate the connections between diagnosed faults
and sensor data, answer operator queries, and evaluate historical sensor
anomalies. Our approach underscores the importance of merging model-based
diagnostics with advanced AI to improve the reliability and transparency of
autonomous systems.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06696" title="Abstract">arXiv:2402.06696</a> [<a href="/pdf/2402.06696" title="Download PDF">pdf</a>, <a href="/format/2402.06696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FL-NAS: Towards Fairness of NAS for Resource Constrained Devices via  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+R">Ruiyang Qin</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yuting Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zheyu Yan</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+J">Jinjun Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Abbasi%2C+A">Ahmed Abbasi</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yiyu Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ASP-DAC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Neural Architecture Search (NAS) has become the de fecto tools in the
industry in automating the design of deep neural networks for various
applications, especially those driven by mobile and edge devices with limited
computing resources. The emerging large language models (LLMs), due to their
prowess, have also been incorporated into NAS recently and show some promising
results. This paper conducts further exploration in this direction by
considering three important design metrics simultaneously, i.e., model
accuracy, fairness, and hardware deployment efficiency. We propose a novel
LLM-based NAS framework, FL-NAS, in this paper, and show experimentally that
FL-NAS can indeed find high-performing DNNs, beating state-of-the-art DNN
models by orders-of-magnitude across almost all design considerations.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06697" title="Abstract">arXiv:2402.06697</a> [<a href="/pdf/2402.06697" title="Download PDF">pdf</a>, <a href="/format/2402.06697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feed-Forward Neural Networks as a Mixed-Integer Program
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aftabi%2C+N">Navid Aftabi</a>, 
<a href="/search/cs?searchtype=author&query=Moradi%2C+N">Nima Moradi</a>, 
<a href="/search/cs?searchtype=author&query=Mahroo%2C+F">Fatemeh Mahroo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
<p class="mathjax">Deep neural networks (DNNs) are widely studied in various applications. A DNN
consists of layers of neurons that compute affine combinations, apply nonlinear
operations, and produce corresponding activations. The rectified linear unit
(ReLU) is a typical nonlinear operator, outputting the max of its input and
zero. In scenarios like max pooling, where multiple input values are involved,
a fixed-parameter DNN can be modeled as a mixed-integer program (MIP). This
formulation, with continuous variables representing unit outputs and binary
variables for ReLU activation, finds applications across diverse domains. This
study explores the formulation of trained ReLU neurons as MIP and applies MIP
models for training neural networks (NNs). Specifically, it investigates
interactions between MIP techniques and various NN architectures, including
binary DNNs (employing step activation functions) and binarized DNNs (with
weights and activations limited to $-1,0,+1$). The research focuses on training
and evaluating proposed approaches through experiments on handwritten digit
classification models. The comparative study assesses the performance of
trained ReLU NNs, shedding light on the effectiveness of MIP formulations in
enhancing training processes for NNs.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06699" title="Abstract">arXiv:2402.06699</a> [<a href="/pdf/2402.06699" title="Download PDF">pdf</a>, <a href="/format/2402.06699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High Epsilon Synthetic Data Vulnerabilities in MST and PrivBayes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Golob%2C+S">Steven Golob</a>, 
<a href="/search/cs?searchtype=author&query=Pentyala%2C+S">Sikha Pentyala</a>, 
<a href="/search/cs?searchtype=author&query=Maratkhan%2C+A">Anuar Maratkhan</a>, 
<a href="/search/cs?searchtype=author&query=De+Cock%2C+M">Martine De Cock</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Synthetic data generation (SDG) has become increasingly popular as a
privacy-enhancing technology. It aims to maintain important statistical
properties of its underlying training data, while excluding any personally
identifiable information. There have been a whole host of SDG algorithms
developed in recent years to improve and balance both of these aims. Many of
these algorithms provide robust differential privacy guarantees.
<br />However, we show here that if the differential privacy parameter
$\varepsilon$ is set too high, then unambiguous privacy leakage can result. We
show this by conducting a novel membership inference attack (MIA) on two
state-of-the-art differentially private SDG algorithms: MST and PrivBayes. Our
work suggests that there are vulnerabilities in these generators not previously
seen, and that future work to strengthen their privacy is advisable.
<br />We present the heuristic for our MIA here. It assumes knowledge of auxiliary
"population" data, and also assumes knowledge of which SDG algorithm was used.
We use this information to adapt the recent DOMIAS MIA uniquely to MST and
PrivBayes. Our approach went on to win the SNAKE challenge in November 2023.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06700" title="Abstract">arXiv:2402.06700</a> [<a href="/pdf/2402.06700" title="Download PDF">pdf</a>, <a href="/format/2402.06700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entropy-Regularized Token-Level Policy Optimization for Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+M">Muning Wen</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+C">Cheng Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weinan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Y">Ying Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) have shown promise as intelligent agents in
interactive decision-making tasks. Traditional approaches often depend on
meticulously designed prompts, high-quality examples, or additional reward
models for in-context learning, supervised fine-tuning, or RLHF. Reinforcement
learning (RL) presents a dynamic alternative for LLMs to overcome these
dependencies by engaging directly with task-specific environments. Nonetheless,
it faces significant hurdles: 1) instability stemming from the exponentially
vast action space requiring exploration; 2) challenges in assigning token-level
credit based on action-level reward signals, resulting in discord between
maximizing rewards and accurately modeling corpus data. In response to these
challenges, we introduce Entropy-Regularized Token-level Policy Optimization
(ETPO), an entropy-augmented RL method tailored for optimizing LLMs at the
token level. At the heart of ETPO is our novel per-token soft Bellman update,
designed to harmonize the RL process with the principles of language modeling.
This methodology decomposes the Q-function update from a coarse action-level
view to a more granular token-level perspective, backed by theoretical proof of
optimization consistency. Crucially, this decomposition renders linear time
complexity in action exploration. We assess the effectiveness of ETPO within a
simulated environment that models data science code generation as a series of
multi-step interactive tasks; results show that ETPO achieves effective
performance improvement on the CodeLlama-7B model and surpasses a variant PPO
baseline inherited from RLHF. This underlines ETPO's potential as a robust
method for refining the interactive decision-making capabilities of LLMs.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06701" title="Abstract">arXiv:2402.06701</a> [<a href="/pdf/2402.06701" title="Download PDF">pdf</a>, <a href="/format/2402.06701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy Profiles for Private Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koskela%2C+A">Antti Koskela</a>, 
<a href="/search/cs?searchtype=author&query=Redberg%2C+R">Rachel Redberg</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu-Xiang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Private selection mechanisms (e.g., Report Noisy Max, Sparse Vector) are
fundamental primitives of differentially private (DP) data analysis with wide
applications to private query release, voting, and hyperparameter tuning.
Recent work (Liu and Talwar, 2019; Papernot and Steinke, 2022) has made
significant progress in both generalizing private selection mechanisms and
tightening their privacy analysis using modern numerical privacy accounting
tools, e.g., R\'enyi DP. But R\'enyi DP is known to be lossy when
$(\epsilon,\delta)$-DP is ultimately needed, and there is a trend to close the
gap by directly handling privacy profiles, i.e., $\delta$ as a function of
$\epsilon$ or its equivalent dual form known as $f$-DPs. In this paper, we work
out an easy-to-use recipe that bounds the privacy profiles of ReportNoisyMax
and PrivateTuning using the privacy profiles of the base algorithms they
corral. Numerically, our approach improves over the RDP-based accounting in all
regimes of interest and leads to substantial benefits in end-to-end private
learning experiments. Our analysis also suggests new distributions, e.g.,
binomial distribution for randomizing the number of rounds that leads to more
substantial improvements in certain regimes.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06702" title="Abstract">arXiv:2402.06702</a> [<a href="/pdf/2402.06702" title="Download PDF">pdf</a>, <a href="/ps/2402.06702" title="Download PostScript">ps</a>, <a href="/format/2402.06702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A harmonized and interoperable format for storing and processing  polysomnography data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huttunen%2C+R">Riku Huttunen</a>, 
<a href="/search/cs?searchtype=author&query=Rusanen%2C+M">Matias Rusanen</a>, 
<a href="/search/cs?searchtype=author&query=Nikkonen%2C+S">Sami Nikkonen</a>, 
<a href="/search/cs?searchtype=author&query=Korkalainen%2C+H">Henri Korkalainen</a>, 
<a href="/search/cs?searchtype=author&query=Kainulainen%2C+S">Samu Kainulainen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">Polysomnography (PSG) data is recorded and stored in various formats
depending on the recording software. Although the PSG data can usually be
exported to open formats, such as the European Data Format (EDF), they are
limited in data types, validation, and readability. Moreover, the exported data
is not harmonized, which means different datasets need customized preprocessing
to conduct research on multiple datasets. In this work, we designed and
implemented an open format for storage and processing of PSG data, called the
Sleeplab format (SLF), which is both human and machine-readable, and has
built-in validation of both data types and structures. SLF provides tools for
reading, writing, and compression of the PSG datasets. In addition, SLF
promotes harmonization of data from different sources, which reduces the amount
of work needed to apply the same analytics pipelines to different datasets. SLF
is interoperable as it utilizes the file system and commonly used file formats
to store the data. The goal of developing SLF was to enable fast exploration
and experimentation on PSG data, and to streamline the workflow of building
analytics and machine learning applications that combine PSG data from multiple
sources. The performance of SLF was tested with two open datasets of different
formats (EDF and HDF5). SLF is fully open source and available at
https://github.com/UEF-SmartSleepLab/sleeplab-format.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06704" title="Abstract">arXiv:2402.06704</a> [<a href="/pdf/2402.06704" title="Download PDF">pdf</a>, <a href="/format/2402.06704" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blockchain-based Rental Documentation Management with Audit Support
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Santos%2C+J+F">Jo&#xe3;o F. Santos</a>, 
<a href="/search/cs?searchtype=author&query=Correia%2C+M">Miguel Correia</a>, 
<a href="/search/cs?searchtype=author&query=Dias%2C+T+R">Tiago R. Dias</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Document management in the rental market is a critical process to ensure the
accuracy of financial transactions and regulatory compliance in the sector. In
Portugal, the challenges include the complexity of legislation, particularly
GDPR non-compliance, lack of transparency, and bureaucratic process
inefficiency. With this in mind, a solution based on Hyperledger Fabric, a
blockchain platform, is presented for the implementation of a document
management system for the rental process. This system oversees the rental
process, which consists of three phases: the application for a property by the
prospective tenant through the upload of necessary documents,
acceptance/rejection by the landlord of various received applications, and the
creation of a report by the system, which only the auditor can request and
view. The system smart contract records metadata associated with the documents
(hash, owner) and coordinates requests for file access by landlords to
prospective tenants. Thus, the system is responsible for creating immutable and
traceable records of the entire process. The underlying platform serves as the
foundation for conducting future audits. After the landlord verifies the files
and accepts the rental proposal, any authorised auditor can request a report
for a property by accessing the records through the final report, which
includes all events that occurred during the process.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06706" title="Abstract">arXiv:2402.06706</a> [<a href="/pdf/2402.06706" title="Download PDF">pdf</a>, <a href="/format/2402.06706" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoRe-GD: A Hierarchical Framework for Scalable Graph Visualization with  GNNs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gr%C3%B6tschla%2C+F">Florian Gr&#xf6;tschla</a>, 
<a href="/search/cs?searchtype=author&query=Mathys%2C+J">Jo&#xeb;l Mathys</a>, 
<a href="/search/cs?searchtype=author&query=Veres%2C+R">Robert Veres</a>, 
<a href="/search/cs?searchtype=author&query=Wattenhofer%2C+R">Roger Wattenhofer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Graph Visualization, also known as Graph Drawing, aims to find geometric
embeddings of graphs that optimize certain criteria. Stress is a widely used
metric; stress is minimized when every pair of nodes is positioned at their
shortest path distance. However, stress optimization presents computational
challenges due to its inherent complexity and is usually solved using
heuristics in practice. We introduce a scalable Graph Neural Network (GNN)
based Graph Drawing framework with sub-quadratic runtime that can learn to
optimize stress. Inspired by classical stress optimization techniques and
force-directed layout algorithms, we create a coarsening hierarchy for the
input graph. Beginning at the coarsest level, we iteratively refine and
un-coarsen the layout, until we generate an embedding for the original graph.
To enhance information propagation within the network, we propose a novel
positional rewiring technique based on intermediate node positions. Our
empirical evaluation demonstrates that the framework achieves state-of-the-art
performance while remaining scalable.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06707" title="Abstract">arXiv:2402.06707</a> [<a href="/pdf/2402.06707" title="Download PDF">pdf</a>, <a href="/ps/2402.06707" title="Download PostScript">ps</a>, <a href="/format/2402.06707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-class real-time crash risk forecasting using convolutional neural  network: Istanbul case study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alafi%2C+B">Behnaz Alafi</a>, 
<a href="/search/cs?searchtype=author&query=Moradi%2C+S">Saeid Moradi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The performance of an artificial neural network (ANN) in forecasting crash
risk is shown in this paper. To begin, some traffic and weather data are
acquired as raw data. This data is then analyzed, and relevant characteristics
are chosen to utilize as input data based on additional tree and Pearson
correlation. Furthermore, crash and non-crash time data are separated; then,
feature values for crash and non-crash events are written in three four-minute
intervals prior to the crash and non-crash events using the average of all
available values for that period. The number of non-crash samples was lowered
after calculating crash likelihood for each period based on accident labeling.
The proposed CNN model is capable of learning from recorded, processed, and
categorized input characteristics such as traffic characteristics and
meteorological conditions. The goal of this work is to forecast the chance of a
real-time crash based on three periods before events. The area under the curve
(AUC) for the receiver operating characteristic curve (ROC curve), as well as
sensitivity as the true positive rate and specificity as the false positive
rate, are shown and compared with three typical machine learning and neural
network models. Finally, when it comes to the error value, AUC, sensitivity,
and specificity parameters as performance variables, the executed model
outperforms other models. The findings of this research suggest applying the
CNN model as a multi-class prediction model for real-time crash risk
prediction. Our emphasis is on multi-class prediction, while prior research
used this for binary (two-class) categorization like crash and non-crash.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06714" title="Abstract">arXiv:2402.06714</a> [<a href="/pdf/2402.06714" title="Download PDF">pdf</a>, <a href="/format/2402.06714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Electricity Price Forecasting in the Irish Balancing Market
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=O%27Connor%2C+C">Ciaran O&#x27;Connor</a>, 
<a href="/search/cs?searchtype=author&query=Collins%2C+J">Joseph Collins</a>, 
<a href="/search/cs?searchtype=author&query=Prestwich%2C+S">Steven Prestwich</a>, 
<a href="/search/cs?searchtype=author&query=Visentin%2C+A">Andrea Visentin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Short-term electricity markets are becoming more relevant due to
less-predictable renewable energy sources, attracting considerable attention
from the industry. The balancing market is the closest to real-time and the
most volatile among them. Its price forecasting literature is limited,
inconsistent and outdated, with few deep learning attempts and no public
dataset. This work applies to the Irish balancing market a variety of price
prediction techniques proven successful in the widely studied day-ahead market.
We compare statistical, machine learning, and deep learning models using a
framework that investigates the impact of different training sizes. The
framework defines hyperparameters and calibration settings; the dataset and
models are made public to ensure reproducibility and to be used as benchmarks
for future works. An extensive numerical study shows that well-performing
models in the day-ahead market do not perform well in the balancing one,
highlighting that these markets are fundamentally different constructs. The
best model is LEAR, a statistical approach based on LASSO, which outperforms
more complex and computationally demanding approaches.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06715" title="Abstract">arXiv:2402.06715</a> [<a href="/pdf/2402.06715" title="Download PDF">pdf</a>, <a href="/format/2402.06715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning-augmented Online Algorithm for Two-level Ski-rental Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Keyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhongdong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+N">Nakjung Choi</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+B">Bo Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we study the two-level ski-rental problem,where a user needs
to fulfill a sequence of demands for multiple items by choosing one of the
three payment options: paying for the on-demand usage (i.e., rent), buying
individual items (i.e., single purchase), and buying all the items (i.e., combo
purchase). Without knowing future demands, the user aims to minimize the total
cost (i.e., the sum of the rental, single purchase, and combo purchase costs)
by balancing the trade-off between the expensive upfront costs (for purchase)
and the potential future expenses (for rent). We first design a robust online
algorithm (RDTSR) that offers a worst-case performance guarantee. While online
algorithms are robust against the worst-case scenarios, they are often overly
cautious and thus suffer a poor average performance in typical scenarios. On
the other hand, Machine Learning (ML) algorithms typically show promising
average performance in various applications but lack worst-case performance
guarantees. To harness the benefits of both methods, we develop a
learning-augmented algorithm (LADTSR) by integrating ML predictions into the
robust online algorithm, which outperforms the robust online algorithm under
accurate predictions while ensuring worst-case performance guarantees even when
predictions are inaccurate. Finally, we conduct numerical experiments on both
synthetic and real-world trace data to corroborate the effectiveness of our
approach.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06716" title="Abstract">arXiv:2402.06716</a> [<a href="/pdf/2402.06716" title="Download PDF">pdf</a>, <a href="/format/2402.06716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Graph Information Bottleneck
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Haonan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Q">Qingyun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+X">Xingcheng Fu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+C">Cheng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianxin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the research tracks of The Web Conference 2024 (WWW 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Dynamic Graphs widely exist in the real world, which carry complicated
spatial and temporal feature patterns, challenging their representation
learning. Dynamic Graph Neural Networks (DGNNs) have shown impressive
predictive abilities by exploiting the intrinsic dynamics. However, DGNNs
exhibit limited robustness, prone to adversarial attacks. This paper presents
the novel Dynamic Graph Information Bottleneck (DGIB) framework to learn robust
and discriminative representations. Leveraged by the Information Bottleneck
(IB) principle, we first propose the expected optimal representations should
satisfy the Minimal-Sufficient-Consensual (MSC) Condition. To compress
redundant as well as conserve meritorious information into latent
representation, DGIB iteratively directs and refines the structural and feature
information flow passing through graph snapshots. To meet the MSC Condition, we
decompose the overall IB objectives into DGIB$_{MS}$ and DGIB$_C$, in which the
DGIB$_{MS}$ channel aims to learn the minimal and sufficient representations,
with the DGIB$_{MS}$ channel guarantees the predictive consensus. Extensive
experiments on real-world and synthetic dynamic graph datasets demonstrate the
superior robustness of DGIB against adversarial attacks compared with
state-of-the-art baselines in the link prediction task. To the best of our
knowledge, DGIB is the first work to learn robust representations of dynamic
graphs grounded in the information-theoretic IB principle.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06729" title="Abstract">arXiv:2402.06729</a> [<a href="/pdf/2402.06729" title="Download PDF">pdf</a>, <a href="/ps/2402.06729" title="Download PostScript">ps</a>, <a href="/format/2402.06729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Greedy Matchings in Bipartite Graphs with Ordered Vertex Sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Simon%2C+H+U">Hans U. Simon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, no figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>

</div>
<p class="mathjax">We define and study greedy matchings in vertex-ordered bipartite graphs. It
is shown that each vertex-ordered bipartite graph has a unique greedy matching.
The proof uses (a weak form of) Newman's lemma. The vertex ordering is called a
preference relation. Given a vertex-ordered bipartite graph, the goal is to
match every vertex of one vertex class but to leave unmatched as many as
possible vertices of low preference in the other concept class. We investigate
how well greedy algorithms perform in this setting. It is shown that they have
optimal performance provided that the vertex-ordering is cleverly chosen. The
study of greedy matchings is motivated by problems in learning theory like
illustrating or teaching concepts by means of labeled examples.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06730" title="Abstract">arXiv:2402.06730</a> [<a href="/pdf/2402.06730" title="Download PDF">pdf</a>, <a href="/format/2402.06730" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Scalable Algorithm for Individually Fair K-means Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bateni%2C+M">MohammadHossein Bateni</a>, 
<a href="/search/cs?searchtype=author&query=Cohen-Addad%2C+V">Vincent Cohen-Addad</a>, 
<a href="/search/cs?searchtype=author&query=Epasto%2C+A">Alessandro Epasto</a>, 
<a href="/search/cs?searchtype=author&query=Lattanzi%2C+S">Silvio Lattanzi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 2 figures, to appear at the 27th International Conference on Artificial Intelligence and Statistics (AISTATS) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present a scalable algorithm for the individually fair ($p$,
$k$)-clustering problem introduced by Jung et al. and Mahabadi et al. Given $n$
points $P$ in a metric space, let $\delta(x)$ for $x\in P$ be the radius of the
smallest ball around $x$ containing at least $n / k$ points. A clustering is
then called individually fair if it has centers within distance $\delta(x)$ of
$x$ for each $x\in P$. While good approximation algorithms are known for this
problem no efficient practical algorithms with good theoretical guarantees have
been presented. We design the first fast local-search algorithm that runs in
~$O(nk^2)$ time and obtains a bicriteria $(O(1), 6)$ approximation. Then we
show empirically that not only is our algorithm much faster than prior work,
but it also produces lower-cost solutions.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06733" title="Abstract">arXiv:2402.06733</a> [<a href="/pdf/2402.06733" title="Download PDF">pdf</a>, <a href="/format/2402.06733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NICE: To Optimize In-Context Examples or Not?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+P">Pragya Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Golechha%2C+S">Satvik Golechha</a>, 
<a href="/search/cs?searchtype=author&query=Deshpande%2C+A">Amit Deshpande</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Amit Sharma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent works have shown that large language models (LLMs) work remarkably
well on a wide range of tasks through in-context learning and optimization of
in-context examples (ICE). However, most of these studies assume either a fixed
or no instruction provided in the prompt, leading to the apparent consensus
that the optimization of in-context examples is critical for better
performance. We challenge this consensus for instruction-tuned LLMs by
investigating the necessity of optimizing in-context examples when
task-specific instructions are provided, and find that there are tasks for
which various ways of optimizing in-context examples yield diminishing returns.
We introduce a task-specific metric called \metriclong{} (\metric) that
quantifies the learnability of tasks from a given instruction, and provides a
heuristic that helps decide whether to optimize for instructions or ICE for any
new task. On a wide range of tasks and a systematically created instruction set
with gradually added details, we validate our hypothesis empirically by
computing \metric with query-dependent bins of examples, comparing different
instructions with ICE selection methods, and performing label perturbation
experiments. We conclude that tasks can be divided into two broad classes based
on the \metric metric, where the returns on ICE optimization follow predictable
trends when instructions are provided in the prompt.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06734" title="Abstract">arXiv:2402.06734</a> [<a href="/pdf/2402.06734" title="Download PDF">pdf</a>, <a href="/ps/2402.06734" title="Download PostScript">ps</a>, <a href="/format/2402.06734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Corruption Robust Offline Reinforcement Learning with Human Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mandal%2C+D">Debmalya Mandal</a>, 
<a href="/search/cs?searchtype=author&query=Nika%2C+A">Andi Nika</a>, 
<a href="/search/cs?searchtype=author&query=Kamalaruban%2C+P">Parameswaran Kamalaruban</a>, 
<a href="/search/cs?searchtype=author&query=Singla%2C+A">Adish Singla</a>, 
<a href="/search/cs?searchtype=author&query=Radanovi%C4%87%2C+G">Goran Radanovi&#x107;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We study data corruption robustness for reinforcement learning with human
feedback (RLHF) in an offline setting. Given an offline dataset of pairs of
trajectories along with feedback about human preferences, an
$\varepsilon$-fraction of the pairs is corrupted (e.g., feedback flipped or
trajectory features manipulated), capturing an adversarial attack or noisy
human preferences. We aim to design algorithms that identify a near-optimal
policy from the corrupted data, with provable guarantees. Existing theoretical
works have separately studied the settings of corruption robust RL (learning
from scalar rewards directly under corruption) and offline RLHF (learning from
human feedback without corruption); however, they are inapplicable to our
problem of dealing with corrupted data in offline RLHF setting. To this end, we
design novel corruption robust offline RLHF methods under various assumptions
on the coverage of the data-generating distributions. At a high level, our
methodology robustifies an offline RLHF framework by first learning a reward
model along with confidence sets and then learning a pessimistic optimal policy
over the confidence set. Our key insight is that learning optimal policy can be
done by leveraging an offline corruption-robust RL oracle in different ways
(e.g., zero-order oracle or first-order oracle), depending on the data coverage
assumptions. To our knowledge, ours is the first work that provides provable
corruption robust offline RLHF methods.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06737" title="Abstract">arXiv:2402.06737</a> [<a href="/pdf/2402.06737" title="Download PDF">pdf</a>, <a href="/format/2402.06737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ExGRG: Explicitly-Generated Relation Graph for Self-Supervised  Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naseri%2C+M">Mahdi Naseri</a>, 
<a href="/search/cs?searchtype=author&query=Biparva%2C+M">Mahdi Biparva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Self-supervised Learning (SSL) has emerged as a powerful technique in
pre-training deep learning models without relying on expensive annotated
labels, instead leveraging embedded signals in unlabeled data. While SSL has
shown remarkable success in computer vision tasks through intuitive data
augmentation, its application to graph-structured data poses challenges due to
the semantic-altering and counter-intuitive nature of graph augmentations.
Addressing this limitation, this paper introduces a novel non-contrastive SSL
approach to Explicitly Generate a compositional Relation Graph (ExGRG) instead
of relying solely on the conventional augmentation-based implicit relation
graph. ExGRG offers a framework for incorporating prior domain knowledge and
online extracted information into the SSL invariance objective, drawing
inspiration from the Laplacian Eigenmap and Expectation-Maximization (EM).
Employing an EM perspective on SSL, our E-step involves relation graph
generation to identify candidates to guide the SSL invariance objective, and
M-step updates the model parameters by integrating the derived relational
information. Extensive experimentation on diverse node classification datasets
demonstrates the superiority of our method over state-of-the-art techniques,
affirming ExGRG as an effective adoption of SSL for graph representation
learning.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06738" title="Abstract">arXiv:2402.06738</a> [<a href="/pdf/2402.06738" title="Download PDF">pdf</a>, <a href="/format/2402.06738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EntGPT: Linking Generative Large Language Models with Knowledge Bases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yifan Ding</a>, 
<a href="/search/cs?searchtype=author&query=Poudel%2C+A">Amrit Poudel</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Q">Qingkai Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Weninger%2C+T">Tim Weninger</a>, 
<a href="/search/cs?searchtype=author&query=Veeramani%2C+B">Balaji Veeramani</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+S">Sanmitra Bhattacharya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The ability of Large Language Models (LLMs) to generate factually correct
output remains relatively unexplored due to the lack of fact-checking and
knowledge grounding during training and inference. In this work, we aim to
address this challenge through the Entity Disambiguation (ED) task. We first
consider prompt engineering, and design a three-step hard-prompting method to
probe LLMs' ED performance without supervised fine-tuning (SFT). Overall, the
prompting method improves the micro-F_1 score of the original vanilla models by
a large margin, on some cases up to 36% and higher, and obtains comparable
performance across 10 datasets when compared to existing methods with SFT. We
further improve the knowledge grounding ability through instruction tuning (IT)
with similar prompts and responses. The instruction-tuned model not only
achieves higher micro-F1 score performance as compared to several baseline
methods on supervised entity disambiguation tasks with an average micro-F_1
improvement of 2.1% over the existing baseline models, but also obtains higher
accuracy on six Question Answering (QA) tasks in the zero-shot setting. Our
methodologies apply to both open- and closed-source LLMs.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06740" title="Abstract">arXiv:2402.06740</a> [<a href="/pdf/2402.06740" title="Download PDF">pdf</a>, <a href="/ps/2402.06740" title="Download PostScript">ps</a>, <a href="/format/2402.06740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nearest Neighbor Complexity and Boolean Circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=DiCicco%2C+M">Mason DiCicco</a>, 
<a href="/search/cs?searchtype=author&query=Podolskii%2C+V">Vladimir Podolskii</a>, 
<a href="/search/cs?searchtype=author&query=Reichman%2C+D">Daniel Reichman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
<p class="mathjax">A nearest neighbor representation of a Boolean function $f$ is a set of
vectors (anchors) labeled by $0$ or $1$ such that $f(\vec{x}) = 1$ if and only
if the closest anchor to $x$ is labeled by $1$. This model was introduced by
Hajnal, Liu, and Tur\'an (2022), who studied bounds on the number of anchors
required to represent Boolean functions under different choices of anchors
(real vs. Boolean vectors) as well as the more expressive model of $k$-nearest
neighbors.
<br />We initiate the study of the representational power of nearest and
$k$-nearest neighbors through Boolean circuit complexity. To this end, we
establish a connection between Boolean functions with polynomial nearest
neighbor complexity and those that can be efficiently represented by classes
based on linear inequalities -- min-plus polynomial threshold functions --
previously studied in relation to threshold circuits. This extends an
observation of Hajnal et al. (2022). We obtain exponential lower bounds on the
$k$-nearest neighbors complexity of explicit $n$-variate functions, assuming $k
\leq n^{1-\epsilon}$. Previously, no superlinear lower bound was known for any
$k&gt;1$.
<br />Next, we further extend the connection between nearest neighbor
representations and circuits to the $k$-nearest neighbors case. As a result, we
show that proving superpolynomial lower bounds for the $k$-nearest neighbors
complexity of an explicit function for arbitrary $k$ would require a
breakthrough in circuit complexity. In addition, we prove an exponential
separation between the nearest neighbor and $k$-nearest neighbors complexity
(for unrestricted $k$) of an explicit function. These results address questions
raised by Hajnal et al. (2022) of proving strong lower bounds for $k$-nearest
neighbors and understanding the role of the parameter $k$. Finally, we devise
new bounds on the nearest neighbor complexity for several explicit functions.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06741" title="Abstract">arXiv:2402.06741</a> [<a href="/pdf/2402.06741" title="Download PDF">pdf</a>, <a href="/format/2402.06741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retzzles: Do Jigsaw Puzzle Actions on Interactive Display Maps Increase  the Retention of Map Information?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kova%C4%8Devi%C4%87%2C+N">Nikola Kova&#x10d;evi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Deja%2C+J+A">Jordan Aiko Deja</a>, 
<a href="/search/cs?searchtype=author&query=Weerasinghe%2C+M">Maheshya Weerasinghe</a>, 
<a href="/search/cs?searchtype=author&query=Pucihar%2C+K+%C4%8C">Klen &#x10c;opi&#x10d; Pucihar</a>, 
<a href="/search/cs?searchtype=author&query=Kljun%2C+M">Matja&#x17e; Kljun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> HCI SI 2023: Human-Computer Interaction Slovenia 2023, January 26, 2024, Maribor, Slovenia, 2 tables, 4 figures, 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">While maps provide upfront content, this might not always be the most
effective way for users to remember information. With the proliferation of
interactive displays for tourists and visitors in public spaces, we can create
a more playful user experience with maps than just exploring them. Adding
interactions with the map could also help users retain more information as they
use them. In this paper, we investigated whether completing a jigsaw puzzle of
a map supports users in retaining more information about a specific map. The
results of a between-subject study with a sample of n=28 indicate that
additional interaction helped improve mean scores of textual and spatial recall
but not visual recall. However, the results are not statistically significant,
and the topic is subject to further investigation. Our findings contribute to
discussions on using interactive touchscreen displays in similar learning
scenarios involving memory retention.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06751" title="Abstract">arXiv:2402.06751</a> [<a href="/pdf/2402.06751" title="Download PDF">pdf</a>, <a href="/format/2402.06751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Rank Learning by Design: the Role of Network Architecture and  Activation Linearity in Gradient Rank Collapse
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baker%2C+B+T">Bradley T. Baker</a>, 
<a href="/search/cs?searchtype=author&query=Pearlmutter%2C+B+A">Barak A. Pearlmutter</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+R">Robyn Miller</a>, 
<a href="/search/cs?searchtype=author&query=Calhoun%2C+V+D">Vince D. Calhoun</a>, 
<a href="/search/cs?searchtype=author&query=Plis%2C+S+M">Sergey M. Plis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Our understanding of learning dynamics of deep neural networks (DNNs) remains
incomplete. Recent research has begun to uncover the mathematical principles
underlying these networks, including the phenomenon of "Neural Collapse", where
linear classifiers within DNNs converge to specific geometrical structures
during late-stage training. However, the role of geometric constraints in
learning extends beyond this terminal phase. For instance, gradients in
fully-connected layers naturally develop a low-rank structure due to the
accumulation of rank-one outer products over a training batch. Despite the
attention given to methods that exploit this structure for memory saving or
regularization, the emergence of low-rank learning as an inherent aspect of
certain DNN architectures has been under-explored. In this paper, we conduct a
comprehensive study of gradient rank in DNNs, examining how architectural
choices and structure of the data effect gradient rank bounds. Our theoretical
analysis provides these bounds for training fully-connected, recurrent, and
convolutional neural networks. We also demonstrate, both theoretically and
empirically, how design choices like activation function linearity, bottleneck
layer introduction, convolutional stride, and sequence truncation influence
these bounds. Our findings not only contribute to the understanding of learning
dynamics in DNNs, but also provide practical guidance for deep learning
engineers to make informed design decisions.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06752" title="Abstract">arXiv:2402.06752</a> [<a href="/pdf/2402.06752" title="Download PDF">pdf</a>, <a href="/format/2402.06752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Oriented-grid Encoder for 3D Implicit Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gaur%2C+A">Arihant Gaur</a>, 
<a href="/search/cs?searchtype=author&query=Pais%2C+G+D">G. Dias Pais</a>, 
<a href="/search/cs?searchtype=author&query=Miraldo%2C+P">Pedro Miraldo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3DV 2024 paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Encoding 3D points is one of the primary steps in learning-based implicit
scene representation. Using features that gather information from neighbors
with multi-resolution grids has proven to be the best geometric encoder for
this task. However, prior techniques do not exploit some characteristics of
most objects or scenes, such as surface normals and local smoothness. This
paper is the first to exploit those 3D characteristics in 3D geometric encoders
explicitly. In contrast to prior work on using multiple levels of details,
regular cube grids, and trilinear interpolation, we propose 3D-oriented grids
with a novel cylindrical volumetric interpolation for modeling local planar
invariance. In addition, we explicitly include a local feature aggregation for
feature regularization and smoothing of the cylindrical interpolation features.
We evaluate our approach on ABC, Thingi10k, ShapeNet, and Matterport3D, for
object and scene representation. Compared to the use of regular grids, our
geometric encoder is shown to converge in fewer steps and obtain sharper 3D
surfaces. When compared to the prior techniques, our method gets
state-of-the-art results.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06756" title="Abstract">arXiv:2402.06756</a> [<a href="/pdf/2402.06756" title="Download PDF">pdf</a>, <a href="/format/2402.06756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence of Gradient Descent with Small Initialization for  Unregularized Matrix Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jianhao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Fattahi%2C+S">Salar Fattahi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the problem of symmetric matrix completion, where the goal is to
reconstruct a positive semidefinite matrix $\rm{X}^\star \in
\mathbb{R}^{d\times d}$ of rank-$r$, parameterized by $\rm{U}\rm{U}^{\top}$,
from only a subset of its observed entries. We show that the vanilla gradient
descent (GD) with small initialization provably converges to the ground truth
$\rm{X}^\star$ without requiring any explicit regularization. This convergence
result holds true even in the over-parameterized scenario, where the true rank
$r$ is unknown and conservatively over-estimated by a search rank $r'\gg r$.
The existing results for this problem either require explicit regularization, a
sufficiently accurate initial point, or exact knowledge of the true rank $r$.
<br />In the over-parameterized regime where $r'\geq r$, we show that, with
$\widetilde\Omega(dr^9)$ observations, GD with an initial point $\|\rm{U}_0\|
\leq \epsilon$ converges near-linearly to an $\epsilon$-neighborhood of
$\rm{X}^\star$. Consequently, smaller initial points result in increasingly
accurate solutions. Surprisingly, neither the convergence rate nor the final
accuracy depends on the over-parameterized search rank $r'$, and they are only
governed by the true rank $r$. In the exactly-parameterized regime where
$r'=r$, we further enhance this result by proving that GD converges at a faster
rate to achieve an arbitrarily small accuracy $\epsilon&gt;0$, provided the
initial point satisfies $\|\rm{U}_0\| = O(1/d)$. At the crux of our method lies
a novel weakly-coupled leave-one-out analysis, which allows us to establish the
global convergence of GD, extending beyond what was previously possible using
the classical leave-one-out analysis.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06759" title="Abstract">arXiv:2402.06759</a> [<a href="/pdf/2402.06759" title="Download PDF">pdf</a>, <a href="/ps/2402.06759" title="Download PostScript">ps</a>, <a href="/format/2402.06759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Methodology for Questionnaire Analysis: Insights through Cluster  Analysis of an Investor Competition Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Forster%2C+C+H+Q">Carlos Henrique Q. Forster</a>, 
<a href="/search/cs?searchtype=author&query=de+Castro%2C+P+A+L">Paulo Andr&#xe9; Lima de Castro</a>, 
<a href="/search/cs?searchtype=author&query=Ramalho%2C+A">Andrei Ramalho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper, we propose a methodology for the analysis of questionnaire
data along with its application on discovering insights from investor data
motivated by a day trading competition. The questionnaire includes categorical
questions, which are reduced to binary questions, 'yes' or 'no'. The
methodology reduces dimensionality by grouping questions and participants with
similar responses using clustering analysis. Rule discovery was performed by
using a conversion rate metric. Innovative visual representations were proposed
to validate the cluster analysis and the relation discovery between questions.
When crossing with financial data, additional insights were revealed related to
the recognized clusters.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06761" title="Abstract">arXiv:2402.06761</a> [<a href="/pdf/2402.06761" title="Download PDF">pdf</a>, <a href="/format/2402.06761" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Embedding Compression for Teacher-to-Student Knowledge Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yiwei Ding</a>, 
<a href="/search/cs?searchtype=author&query=Lerch%2C+A">Alexander Lerch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5+1 pages. In ICASSP 2024 Satellite Workshop Deep Neural Network Model Compression
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Common knowledge distillation methods require the teacher model and the
student model to be trained on the same task. However, the usage of embeddings
as teachers has also been proposed for different source tasks and target tasks.
Prior work that uses embeddings as teachers ignores the fact that the teacher
embeddings are likely to contain irrelevant knowledge for the target task. To
address this problem, we propose to use an embedding compression module with a
trainable teacher transformation to obtain a compact teacher embedding. Results
show that adding the embedding compression module improves the classification
performance, especially for unsupervised teacher embeddings. Moreover, student
models trained with the guidance of embeddings show stronger generalizability.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06763" title="Abstract">arXiv:2402.06763</a> [<a href="/pdf/2402.06763" title="Download PDF">pdf</a>, <a href="/format/2402.06763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Kernel Logistic Regression with Nystr&#xf6;m Approximation:  Theoretical Analysis and Application to Discrete Choice Modelling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADn-Baos%2C+J+%C3%81">Jos&#xe9; &#xc1;ngel Mart&#xed;n-Baos</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-R%C3%B3denas%2C+R">Ricardo Garc&#xed;a-R&#xf3;denas</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez-Benitez%2C+L">Luis Rodriguez-Benitez</a>, 
<a href="/search/cs?searchtype=author&query=Bierlaire%2C+M">Michel Bierlaire</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The application of kernel-based Machine Learning (ML) techniques to discrete
choice modelling using large datasets often faces challenges due to memory
requirements and the considerable number of parameters involved in these
models. This complexity hampers the efficient training of large-scale models.
This paper addresses these problems of scalability by introducing the Nystr\"om
approximation for Kernel Logistic Regression (KLR) on large datasets. The study
begins by presenting a theoretical analysis in which: i) the set of KLR
solutions is characterised, ii) an upper bound to the solution of KLR with
Nystr\"om approximation is provided, and finally iii) a specialisation of the
optimisation algorithms to Nystr\"om KLR is described. After this, the
Nystr\"om KLR is computationally validated. Four landmark selection methods are
tested, including basic uniform sampling, a k-means sampling strategy, and two
non-uniform methods grounded in leverage scores. The performance of these
strategies is evaluated using large-scale transport mode choice datasets and is
compared with traditional methods such as Multinomial Logit (MNL) and
contemporary ML techniques. The study also assesses the efficiency of various
optimisation techniques for the proposed Nystr\"om KLR model. The performance
of gradient descent, Momentum, Adam, and L-BFGS-B optimisation methods is
examined on these datasets. Among these strategies, the k-means Nystr\"om KLR
approach emerges as a successful solution for applying KLR to large datasets,
particularly when combined with the L-BFGS-B and Adam optimisation methods. The
results highlight the ability of this strategy to handle datasets exceeding
200,000 observations while maintaining robust performance.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06764" title="Abstract">arXiv:2402.06764</a> [<a href="/pdf/2402.06764" title="Download PDF">pdf</a>, <a href="/format/2402.06764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph  Alignment via Neighborhood Partitioning and Generative Subgraph Encoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dernbach%2C+S">Stefan Dernbach</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+K">Khushbu Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Zuniga%2C+A">Alejandro Zuniga</a>, 
<a href="/search/cs?searchtype=author&query=Henry%2C+M">Michael Henry</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+S">Sutanay Choudhury</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in AAAI Spring Symposium: AAAI-MAKE 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Integrating large language models (LLMs) with knowledge graphs derived from
domain-specific data represents an important advancement towards more powerful
and factual reasoning. As these models grow more capable, it is crucial to
enable them to perform multi-step inferences over real-world knowledge graphs
while minimizing hallucination. While large language models excel at
conversation and text generation, their ability to reason over
domain-specialized graphs of interconnected entities remains limited. For
example, can we query a LLM to identify the optimal contact in a professional
network for a specific goal, based on relationships and attributes in a private
database? The answer is no--such capabilities lie beyond current methods.
However, this question underscores a critical technical gap that must be
addressed. Many high-value applications in areas such as science, security, and
e-commerce rely on proprietary knowledge graphs encoding unique structures,
relationships, and logical constraints. We introduce a fine-tuning framework
for developing Graph-aligned LAnguage Models (GLaM) that transforms a knowledge
graph into an alternate text representation with labeled question-answer pairs.
We demonstrate that grounding the models in specific graph-based knowledge
expands the models' capacity for structure-based reasoning. Our methodology
leverages the large-language model's generative capabilities to create the
dataset and proposes an efficient alternate to retrieval-augmented generation
styled methods.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06766" title="Abstract">arXiv:2402.06766</a> [<a href="/pdf/2402.06766" title="Download PDF">pdf</a>, <a href="/format/2402.06766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluation Metrics for Text Data Augmentation in NLP
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amadeus%2C+M">Marcellus Amadeus</a>, 
<a href="/search/cs?searchtype=author&query=Casta%C3%B1eda%2C+W+A+C">William Alberto Cruz Casta&#xf1;eda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent surveys on data augmentation for natural language processing have
reported different techniques and advancements in the field. Several
frameworks, tools, and repositories promote the implementation of text data
augmentation pipelines. However, a lack of evaluation criteria and standards
for method comparison due to different tasks, metrics, datasets, architectures,
and experimental settings makes comparisons meaningless. Also, a lack of
methods unification exists and text data augmentation research would benefit
from unified metrics to compare different augmentation methods. Thus, academics
and the industry endeavor relevant evaluation metrics for text data
augmentation techniques. The contribution of this work is to provide a taxonomy
of evaluation metrics for text augmentation methods and serve as a direction
for a unified benchmark. The proposed taxonomy organizes categories that
include tools for implementation and metrics calculation. Finally, with this
study, we intend to present opportunities to explore the unification and
standardization of text data augmentation metrics.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06767" title="Abstract">arXiv:2402.06767</a> [<a href="/pdf/2402.06767" title="Download PDF">pdf</a>, <a href="/ps/2402.06767" title="Download PostScript">ps</a>, <a href="/format/2402.06767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Precoded Polar Product Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Co%C5%9Fkun%2C+M+C">Mustafa Cemil Co&#x15f;kun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 1 figure, submitted to the IEEE ISIT 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Precoded polar product codes are proposed, where selected component codes
enable successive cancellation list decoding to generate bit-wise soft messages
efficiently for iterative decoding while targeting optimized distance spectrum
as opposed to eBCH or polar component codes. Rate compatibility is a byproduct
of $1$-bit granularity in the component code design.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06775" title="Abstract">arXiv:2402.06775</a> [<a href="/pdf/2402.06775" title="Download PDF">pdf</a>, <a href="/ps/2402.06775" title="Download PostScript">ps</a>, <a href="/format/2402.06775" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Twenty Constructionist Things to Do with Artificial Intelligence and  Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kafai%2C+Y">Yasmin Kafai</a>, 
<a href="/search/cs?searchtype=author&query=Morales-Navarro%2C+L">Luis Morales-Navarro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">In this paper, we build on the 1971 memo "Twenty Things to Do With a
Computer" by Seymour Papert and Cynthia Solomon and propose twenty
constructionist things to do with artificial intelligence and machine learning.
Several proposals build on ideas developed in the original memo while others
are new and address topics in science, mathematics, and the arts. In reviewing
the big themes, we notice a renewed interest in children's engagement not just
for technical proficiency but also to cultivate a deeper understanding of their
own cognitive processes. Furthermore, the ideas stress the importance of
designing personally relevant AI/ML applications, moving beyond isolated models
and off-the-shelf datasets disconnected from their interests. We also
acknowledge the social aspects of data production involved in making AI/ML
applications. Finally, we highlight the critical dimensions necessary to
address potential harmful algorithmic biases and consequences of AI/ML
applications.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06777" title="Abstract">arXiv:2402.06777</a> [<a href="/pdf/2402.06777" title="Download PDF">pdf</a>, <a href="/format/2402.06777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Capturing Cancer as Music: Cancer Mechanisms Expressed through  Musification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hnatyshyn%2C+R">Rostyslav Hnatyshyn</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Jiayi Hong</a>, 
<a href="/search/cs?searchtype=author&query=Maciejewski%2C+R">Ross Maciejewski</a>, 
<a href="/search/cs?searchtype=author&query=Norby%2C+C">Christopher Norby</a>, 
<a href="/search/cs?searchtype=author&query=Maley%2C+C+C">Carlo C. Maley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Multimedia (cs.MM); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The development of cancer is difficult to express on a simple and intuitive
level due to its complexity. Since cancer is so widespread, raising public
awareness about its mechanisms can help those affected cope with its realities,
as well as inspire others to make lifestyle adjustments and screen for the
disease. Unfortunately, studies have shown that cancer literature is too
technical for the general public to understand. We found that musification, the
process of turning data into music, remains an unexplored avenue for conveying
this information. We explore the pedagogical effectiveness of musification
through the use of an algorithm that manipulates a piece of music in a manner
analogous to the development of cancer. We conducted two lab studies and found
that our approach is marginally more effective at promoting cancer literacy
when accompanied by a text-based article than text-based articles alone.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06782" title="Abstract">arXiv:2402.06782</a> [<a href="/pdf/2402.06782" title="Download PDF">pdf</a>, <a href="/format/2402.06782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Debating with More Persuasive LLMs Leads to More Truthful Answers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+A">Akbir Khan</a>, 
<a href="/search/cs?searchtype=author&query=Hughes%2C+J">John Hughes</a>, 
<a href="/search/cs?searchtype=author&query=Valentine%2C+D">Dan Valentine</a>, 
<a href="/search/cs?searchtype=author&query=Ruis%2C+L">Laura Ruis</a>, 
<a href="/search/cs?searchtype=author&query=Sachan%2C+K">Kshitij Sachan</a>, 
<a href="/search/cs?searchtype=author&query=Radhakrishnan%2C+A">Ansh Radhakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Grefenstette%2C+E">Edward Grefenstette</a>, 
<a href="/search/cs?searchtype=author&query=Bowman%2C+S+R">Samuel R. Bowman</a>, 
<a href="/search/cs?searchtype=author&query=Rockt%C3%A4schel%2C+T">Tim Rockt&#xe4;schel</a>, 
<a href="/search/cs?searchtype=author&query=Perez%2C+E">Ethan Perez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> For code please check: <a href="https://github.com/ucl-dark/llm_debate">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Common methods for aligning large language models (LLMs) with desired
behaviour heavily rely on human-labelled data. However, as models grow
increasingly sophisticated, they will surpass human expertise, and the role of
human evaluation will evolve into non-experts overseeing experts. In
anticipation of this, we ask: can weaker models assess the correctness of
stronger models? We investigate this question in an analogous setting, where
stronger models (experts) possess the necessary information to answer questions
and weaker models (non-experts) lack this information. The method we evaluate
is \textit{debate}, where two LLM experts each argue for a different answer,
and a non-expert selects the answer. We find that debate consistently helps
both non-expert models and humans answer questions, achieving 76\% and 88\%
accuracy respectively (naive baselines obtain 48\% and 60\%). Furthermore,
optimising expert debaters for persuasiveness in an unsupervised manner
improves non-expert ability to identify the truth in debates. Our results
provide encouraging empirical evidence for the viability of aligning models
with debate in the absence of ground truth.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06783" title="Abstract">arXiv:2402.06783</a> [<a href="/pdf/2402.06783" title="Download PDF">pdf</a>, <a href="/format/2402.06783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learn to Teach: Improve Sample Efficiency in Teacher-student Learning  for Sim-to-Real Transfer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Feiyang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Z">Zhaoyuan Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Ye Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+A">Anqi Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Simulation-to-reality (sim-to-real) transfer is a fundamental problem for
robot learning. Domain Randomization, which adds randomization during training,
is a powerful technique that effectively addresses the sim-to-real gap.
However, the noise in observations makes learning significantly harder.
Recently, studies have shown that employing a teacher-student learning paradigm
can accelerate training in randomized environments. Learned with privileged
information, a teacher agent can instruct the student agent to operate in noisy
environments. However, this approach is often not sample efficient as the
experience collected by the teacher is discarded completely when training the
student, wasting information revealed by the environment. In this work, we
extend the teacher-student learning paradigm by proposing a sample efficient
learning framework termed Learn to Teach (L2T) that recycles experience
collected by the teacher agent. We observe that the dynamics of the
environments for both agents remain unchanged, and the state space of the
teacher is coupled with the observation space of the student. We show that a
single-loop algorithm can train both the teacher and student agents under both
Reinforcement Learning and Inverse Reinforcement Learning contexts. We
implement variants of our methods, conduct experiments on the MuJoCo benchmark,
and apply our methods to the Cassie robot locomotion problem. Extensive
experiments show that our method achieves competitive performance while only
requiring environmental interaction with the teacher.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06784" title="Abstract">arXiv:2402.06784</a> [<a href="/pdf/2402.06784" title="Download PDF">pdf</a>, <a href="/format/2402.06784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transfer learning with generative models for object detection on limited  datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Paiano%2C+M">Matteo Paiano</a>, 
<a href="/search/cs?searchtype=author&query=Martina%2C+S">Stefano Martina</a>, 
<a href="/search/cs?searchtype=author&query=Giannelli%2C+C">Carlotta Giannelli</a>, 
<a href="/search/cs?searchtype=author&query=Caruso%2C+F">Filippo Caruso</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Numerical Analysis (math.NA)

</div>
<p class="mathjax">The availability of data is limited in some fields, especially for object
detection tasks, where it is necessary to have correctly labeled bounding boxes
around each object. A notable example of such data scarcity is found in the
domain of marine biology, where it is useful to develop methods to
automatically detect submarine species for environmental monitoring. To address
this data limitation, the state-of-the-art machine learning strategies employ
two main approaches. The first involves pretraining models on existing datasets
before generalizing to the specific domain of interest. The second strategy is
to create synthetic datasets specifically tailored to the target domain using
methods like copy-paste techniques or ad-hoc simulators. The first strategy
often faces a significant domain shift, while the second demands custom
solutions crafted for the specific task. In response to these challenges, here
we propose a transfer learning framework that is valid for a generic scenario.
In this framework, generated images help to improve the performances of an
object detector in a few-real data regime. This is achieved through a
diffusion-based generative model that was pretrained on large generic datasets,
and is not trained on the task-specific domain. We validate our approach on
object detection tasks, specifically focusing on fishes in an underwater
environment, and on the more common domain of cars in an urban setting. Our
method achieves detection performance comparable to models trained on thousands
of images, using only a few hundreds of input data. Our results pave the way
for new generative AI-based protocols for machine learning applications in
various domains, for instance ranging from geophysics to biology and medicine.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06787" title="Abstract">arXiv:2402.06787</a> [<a href="/pdf/2402.06787" title="Download PDF">pdf</a>, <a href="/format/2402.06787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ForestColl: Efficient Collective Communications on Heterogeneous Network  Fabrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liangyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Maleki%2C+S">Saeed Maleki</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Ziyue Yang</a>, 
<a href="/search/cs?searchtype=author&query=Pourreza%2C+H">Hossein Pourreza</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+A">Aashaka Shah</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+C">Changho Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+A">Arvind Krishnamurthy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2305.18461">arXiv:2305.18461</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)

</div>
<p class="mathjax">As modern DNN models grow ever larger, collective communications between the
accelerators (allreduce, etc.) emerge as a significant performance bottleneck.
Designing efficient communication schedules is challenging given today's highly
diverse and heterogeneous network fabrics. In this paper, we present
ForestColl, a tool that generates efficient schedules for any network topology.
ForestColl constructs broadcast/aggregation spanning trees as the communication
schedule, achieving theoretically minimum network congestion. Its schedule
generation runs in strongly polynomial time and is highly scalable. ForestColl
supports any network fabrics, including both switching fabrics and direct
connections, as well as any network graph structure. We evaluated ForestColl on
multi-cluster AMD MI250 and NVIDIA A100 platforms. ForestColl's schedules
achieved up to 52\% higher performance compared to the vendors' own optimized
communication libraries, RCCL and NCCL. ForestColl also outperforms other
state-of-the-art schedule generation techniques with both up to 61\% more
efficient generated schedules and orders of magnitude faster schedule
generation speed.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06790" title="Abstract">arXiv:2402.06790</a> [<a href="/pdf/2402.06790" title="Download PDF">pdf</a>, <a href="/format/2402.06790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Robotic Companions: Understanding Handler-Guide Dog Interactions  for Informed Guide Dog Robot Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hwang%2C+H">Hochul Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+H">Hee-Tae Jung</a>, 
<a href="/search/cs?searchtype=author&query=Giudice%2C+N+A">Nicholas A Giudice</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+J">Joydeep Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S+I">Sunghoon Ivan Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Donghyun Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Dog guides are favored by blind and low-vision (BLV) individuals for their
ability to enhance independence and confidence by reducing safety concerns and
increasing navigation efficiency compared to traditional mobility aids.
However, only a relatively small proportion of BLV individuals work with dog
guides due to their limited availability and associated maintenance
responsibilities. There is considerable recent interest in addressing this
challenge by developing legged guide dog robots. This study was designed to
determine critical aspects of the handler-guide dog interaction and better
understand handler needs to inform guide dog robot development. We conducted
semi-structured interviews and observation sessions with 23 dog guide handlers
and 5 trainers. Thematic analysis revealed critical limitations in guide dog
work, desired personalization in handler-guide dog interaction, and important
perspectives on future guide dog robots. Grounded on these findings, we discuss
pivotal design insights for guide dog robots aimed for adoption within the BLV
community.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06794" title="Abstract">arXiv:2402.06794</a> [<a href="/pdf/2402.06794" title="Download PDF">pdf</a>, <a href="/format/2402.06794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is it safe to cross? Interpretable Risk Assessment with GPT-4V for  Safety-Aware Street Crossing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hwang%2C+H">Hochul Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+S">Sunjae Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yekyung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Donghyun Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Safely navigating street intersections is a complex challenge for blind and
low-vision individuals, as it requires a nuanced understanding of the
surrounding context - a task heavily reliant on visual cues. Traditional
methods for assisting in this decision-making process often fall short, lacking
the ability to provide a comprehensive scene analysis and safety level. This
paper introduces an innovative approach that leverages large multimodal models
(LMMs) to interpret complex street crossing scenes, offering a potential
advancement over conventional traffic signal recognition techniques. By
generating a safety score and scene description in natural language, our method
supports safe decision-making for the blind and low-vision individuals. We
collected crosswalk intersection data that contains multiview egocentric images
captured by a quadruped robot and annotated the images with corresponding
safety scores based on our predefined safety score categorization. Grounded on
the visual knowledge, extracted from images, and text prompt, we evaluate a
large multimodal model for safety score prediction and scene description. Our
findings highlight the reasoning and safety score prediction capabilities of a
LMM, activated by various prompts, as a pathway to developing a trustworthy
system, crucial for applications requiring reliable decision-making support.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06795" title="Abstract">arXiv:2402.06795</a> [<a href="/pdf/2402.06795" title="Download PDF">pdf</a>, <a href="/format/2402.06795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Squidgets: Sketch-based Widget Design and Direct Manipulation of 3D  Scene
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Joonho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+K">Karan Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Squidgets or 'sketch-widgets' is a novel stroke-based UI framework for direct
scene manipulation. Squidgets is motivated by the observation that sketch
strokes comprising visual abstractions of scene elements implicitly provide
natural handles for the direct manipulation of scene parameters. Configurations
of such strokes can further be explicitly drawn by users to author custom
widgets associated with scene attributes. Users manipulate a scene by simply
drawing strokes: a squidget is selected by partially matching the drawn stroke
against both implicit scene contours and explicitly authored curves, and used
in-situ to interactively control scene parameters associated with the squidget.
We present an implementation of squidgets within the 3D modeling animation
system Maya, and report on an evaluation of squidget creation and manipulation,
by both casual users and professional artists.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06798" title="Abstract">arXiv:2402.06798</a> [<a href="/pdf/2402.06798" title="Download PDF">pdf</a>, <a href="/format/2402.06798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reasoning Grasping via Multimodal Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Shiyu Jin</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jinxuan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yutian Lei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liangjun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Despite significant progress in robotic systems for operation within
human-centric environments, existing models still heavily rely on explicit
human commands to identify and manipulate specific objects. This limits their
effectiveness in environments where understanding and acting on implicit human
intentions are crucial. In this study, we introduce a novel task: reasoning
grasping, where robots need to generate grasp poses based on indirect verbal
instructions or intentions. To accomplish this, we propose an end-to-end
reasoning grasping model that integrates a multi-modal Large Language Model
(LLM) with a vision-based robotic grasping framework. In addition, we present
the first reasoning grasping benchmark dataset generated from the GraspNet-1
billion, incorporating implicit instructions for object-level and part-level
grasping, and this dataset will soon be available for public access. Our
results show that directly integrating CLIP or LLaVA with the grasp detection
model performs poorly on the challenging reasoning grasping tasks, while our
proposed model demonstrates significantly enhanced performance both in the
reasoning grasping benchmark and real-world experiments.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06800" title="Abstract">arXiv:2402.06800</a> [<a href="/pdf/2402.06800" title="Download PDF">pdf</a>, <a href="/ps/2402.06800" title="Download PostScript">ps</a>, <a href="/format/2402.06800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Nowcasting of Marine Fog Visibility in the Grand Banks area  and Sable Island in Canada
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gultepe%2C+E">Eren Gultepe</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Blomquist%2C+B">Byron Blomquist</a>, 
<a href="/search/cs?searchtype=author&query=Fernando%2C+H+J+S">Harindra J.S. Fernando</a>, 
<a href="/search/cs?searchtype=author&query=Kreidl%2C+O+P">O. Patrick Kreidl</a>, 
<a href="/search/cs?searchtype=author&query=Delene%2C+D+J">David J. Delene</a>, 
<a href="/search/cs?searchtype=author&query=Gultepe%2C+I">Ismail Gultepe</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Tackling Climate Change with Machine Learning: workshop at NeurIPS
  2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
<p class="mathjax">This study presents the application of generative deep learning techniques to
evaluate marine fog visibility nowcasting using the FATIMA (Fog and turbulence
interactions in the marine atmosphere) campaign observations collected during
July 2022 in the North Atlantic in the Grand Banks area and vicinity of Sable
Island (SI), northeast of Canada. The measurements were collected using the
Vaisala Forward Scatter Sensor model FD70 and Weather Transmitter model WXT50,
and Gill R3A ultrasonic anemometer mounted on the Research Vessel Atlantic
Condor. To perform nowcasting, the time series of fog visibility (Vis), wind
speed, dew point depression, and relative humidity with respect to water were
preprocessed to have lagged time step features. Generative nowcasting of Vis
time series for lead times of 30 and 60 minutes were performed using
conditional generative adversarial networks (cGAN) regression at visibility
thresholds of Vis &lt; 1 km and &lt; 10 km. Extreme gradient boosting (XGBoost) was
used as a baseline method for comparison against cGAN. At the 30 min lead time,
Vis was best predicted with cGAN at Vis &lt; 1 km (RMSE = 0.151 km) and with
XGBoost at Vis &lt; 10 km (RMSE = 2.821 km). At the 60 min lead time, Vis was best
predicted with XGBoost at Vis &lt; 1 km (RMSE = 0.167 km) and Vis &lt; 10 km (RMSE =
3.508 km), but the cGAN RMSE was similar to XGBoost. Despite nowcasting Vis at
30 min being quite difficult, the ability of the cGAN model to track the
variation in Vis at 1 km suggests that there is potential for generative
analysis of marine fog visibility using observational meteorological
parameters.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06801" title="Abstract">arXiv:2402.06801</a> [<a href="/pdf/2402.06801" title="Download PDF">pdf</a>, <a href="/format/2402.06801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fingerprinting New York City&#x27;s Scaffolding Problem with Longitudinal  Dashcam Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shapira%2C+D">Dorin Shapira</a>, 
<a href="/search/cs?searchtype=author&query=Franchi%2C+M">Matt Franchi</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+W">Wendy Ju</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Scaffolds, also called sidewalk sheds, are intended to be temporary
structures to protect pedestrians from construction and repair hazards.
However, some sidewalk sheds are left up for years. Long-term scaffolding
becomes eyesores, creates accessibility issues on sidewalks, and gives cover to
illicit activity. Today, there are over 8,000 active permits for scaffolds in
NYC; the more problematic scaffolds are likely expired or unpermitted. This
research uses computer vision on street-level imagery to develop a longitudinal
map of scaffolding throughout the city. Using a dataset of 29,156,833 dashcam
images taken between August 2023 and January 2024, we develop an algorithm to
track the presence of scaffolding over time. We also design and implement
methods to match detected scaffolds to reported locations of active scaffolding
permits, enabling the identification of sidewalk sheds without corresponding
permits. We identify 850,766 images of scaffolding, tagging 5,156 active
sidewalk sheds and estimating 529 unpermitted sheds. We discuss the
implications of an in-the-wild scaffolding classifier for urban tech,
innovations to governmental inspection processes, and out-of-distribution
evaluations outside of New York City.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06803" title="Abstract">arXiv:2402.06803</a> [<a href="/pdf/2402.06803" title="Download PDF">pdf</a>, <a href="/ps/2402.06803" title="Download PostScript">ps</a>, <a href="/format/2402.06803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On graphs with well distributed edge density
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hassan%2C+S+M">Syed Mujtaba Hassan</a>, 
<a href="/search/cs?searchtype=author&query=Hussain%2C+S">Shahid Hussain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Computational Complexity (cs.CC); Combinatorics (math.CO)

</div>
<p class="mathjax">In this paper we introduce a class of graphs which we call average hereditary
graphs. Most graphs that occur in the usual graph theory applications belongs
to this class of graphs. Many popular types of graphs fall under this class,
such as regular graphs, trees and other popular classes of graphs. We prove an
upper bound for the chromatic number of average hereditary graphs, and show
that this bound is an improvement on previous bounds. This class of graphs is
explored further by proving some interesting properties regarding the class of
average hereditary graphs. We analyze the computational complexity of deciding
if an arbitrary graph is average hereditary. Then an equivalent condition and a
polynomial time sufficient condition is provided for a graph to be average
hereditary. We then provide a constructions for average hereditary graphs,
using which an average hereditary graph can be recursively constructed. We also
show that this class of graphs is closed under a binary operation, from this
another construction is obtained for average hereditary graphs, and we see some
interesting algebraic properties this class of graphs has. We then explore the
effect on the complexity of graph 3-coloring problem when the input is
restricted to average hereditary graphs.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06805" title="Abstract">arXiv:2402.06805</a> [<a href="/pdf/2402.06805" title="Download PDF">pdf</a>, <a href="/format/2402.06805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Event-to-Video Conversion for Overhead Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hannan%2C+D">Darryl Hannan</a>, 
<a href="/search/cs?searchtype=author&query=Arnab%2C+R">Ragib Arnab</a>, 
<a href="/search/cs?searchtype=author&query=Parpart%2C+G">Gavin Parpart</a>, 
<a href="/search/cs?searchtype=author&query=Kenyon%2C+G+T">Garrett T. Kenyon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+E">Edward Kim</a>, 
<a href="/search/cs?searchtype=author&query=Watkins%2C+Y">Yijing Watkins</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 1 figure, SSIAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Collecting overhead imagery using an event camera is desirable due to the
energy efficiency of the image sensor compared to standard cameras. However,
event cameras complicate downstream image processing, especially for complex
tasks such as object detection. In this paper, we investigate the viability of
event streams for overhead object detection. We demonstrate that across a
number of standard modeling approaches, there is a significant gap in
performance between dense event representations and corresponding RGB frames.
We establish that this gap is, in part, due to a lack of overlap between the
event representations and the pre-training data used to initialize the weights
of the object detectors. Then, we apply event-to-video conversion models that
convert event streams into gray-scale video to close this gap. We demonstrate
that this approach results in a large performance increase, outperforming even
event-specific object detection techniques on our overhead target task. These
results suggest that better alignment between event representations and
existing large pre-trained models may result in greater short-term performance
gains compared to end-to-end event-specific architectural improvements.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06806" title="Abstract">arXiv:2402.06806</a> [<a href="/pdf/2402.06806" title="Download PDF">pdf</a>, <a href="/format/2402.06806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Principled Assessment of Tabular Data Synthesis Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yuntao Du</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+N">Ninghui Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The code is available at: <a href="https://github.com/zealscott/SynMeter">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Databases (cs.DB); Machine Learning (cs.LG)

</div>
<p class="mathjax">Data synthesis has been advocated as an important approach for utilizing data
while protecting data privacy. A large number of tabular data synthesis
algorithms (which we call synthesizers) have been proposed. Some synthesizers
satisfy Differential Privacy, while others aim to provide privacy in a
heuristic fashion. A comprehensive understanding of the strengths and
weaknesses of these synthesizers remains elusive due to lacking principled
evaluation metrics and missing head-to-head comparisons of newly developed
synthesizers that take advantage of diffusion models and large language models
with state-of-the-art marginal-based synthesizers.
<br />In this paper, we present a principled and systematic evaluation framework
for assessing tabular data synthesis algorithms. Specifically, we examine and
critique existing evaluation metrics, and introduce a set of new metrics in
terms of fidelity, privacy, and utility to address their limitations. Based on
the proposed metrics, we also devise a unified objective for tuning, which can
consistently improve the quality of synthetic data for all methods. We
conducted extensive evaluations of 8 different types of synthesizers on 12
datasets and identified some interesting findings, which offer new directions
for privacy-preserving data synthesis.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06808" title="Abstract">arXiv:2402.06808</a> [<a href="/pdf/2402.06808" title="Download PDF">pdf</a>, <a href="/format/2402.06808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explain Variance of Prediction in Variational Time Series Models for  Clinical Deterioration Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiacheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+J">Jaideep Srivastava</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In healthcare, thanks to many model agnostic methods, explainability of the
prediction scores made by deep learning applications has improved. However, we
note that for daily or hourly risk of deterioration prediction of in-hospital
patients, not only the predicted risk probability score matters, but also the
variance of the risk scores play key roles in aiding clinical decision making.
In this paper, we propose to use delta's method to approximate variance of
prediction deterministically, such that the SHAP method can be adopted to
attribute contribution of variance. The prediction variance is estimated by
sampling the conditional hidden space in variational models and is propagated
to input clinical variables based on Shapley values of the variance game. This
approach works with variational time series models such as variational
recurrent neural networks and variational transformers. We further argue that
variational time series models are perfect fits for achieving a balance between
predictive power and explainability through a series of experiments on a public
clinical ICU datasets. Since SHAP values are additive, we also postulate that
the SHAP importance of clinical variables with respect to prediction variations
can guide their frequency of measurements.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06809" title="Abstract">arXiv:2402.06809</a> [<a href="/pdf/2402.06809" title="Download PDF">pdf</a>, <a href="/format/2402.06809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain Adaptation Using Pseudo Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chhabra%2C+S">Sachin Chhabra</a>, 
<a href="/search/cs?searchtype=author&query=Venkateswara%2C+H">Hemanth Venkateswara</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Baoxin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages + 3 pages of references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In the absence of labeled target data, unsupervised domain adaptation
approaches seek to align the marginal distributions of the source and target
domains in order to train a classifier for the target. Unsupervised domain
alignment procedures are category-agnostic and end up misaligning the
categories. We address this problem by deploying a pretrained network to
determine accurate labels for the target domain using a multi-stage
pseudo-label refinement procedure. The filters are based on the confidence,
distance (conformity), and consistency of the pseudo labels. Our results on
multiple datasets demonstrate the effectiveness of our simple procedure in
comparison with complex state-of-the-art techniques.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06810" title="Abstract">arXiv:2402.06810</a> [<a href="/pdf/2402.06810" title="Download PDF">pdf</a>, <a href="/format/2402.06810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Co-Creativity using Total Information Flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gokul%2C+V">Vignesh Gokul</a>, 
<a href="/search/cs?searchtype=author&query=Francis%2C+C">Chris Francis</a>, 
<a href="/search/cs?searchtype=author&query=Dubnov%2C+S">Shlomo Dubnov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Information Theory (cs.IT); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Co-creativity in music refers to two or more musicians or musical agents
interacting with one another by composing or improvising music. However, this
is a very subjective process and each musician has their own preference as to
which improvisation is better for some context. In this paper, we aim to create
a measure based on total information flow to quantitatively evaluate the
co-creativity process in music. In other words, our measure is an indication of
how "good" a creative musical process is. Our main hypothesis is that a good
musical creation would maximize information flow between the participants
captured by music voices recorded in separate tracks. We propose a method to
compute the information flow using pre-trained generative models as entropy
estimators. We demonstrate how our method matches with human perception using a
qualitative study.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06811" title="Abstract">arXiv:2402.06811</a> [<a href="/pdf/2402.06811" title="Download PDF">pdf</a>, <a href="/ps/2402.06811" title="Download PostScript">ps</a>, <a href="/format/2402.06811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discipline and Label: A WEIRD Genealogy and Social Theory of Data  Annotation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Smart%2C+A">Andrew Smart</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Ding Wang</a>, 
<a href="/search/cs?searchtype=author&query=Monk%2C+E">Ellis Monk</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%ADaz%2C+M">Mark D&#xed;az</a>, 
<a href="/search/cs?searchtype=author&query=Kasirzadeh%2C+A">Atoosa Kasirzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Van+Liemt%2C+E">Erin Van Liemt</a>, 
<a href="/search/cs?searchtype=author&query=Schmer-Galunder%2C+S">Sonja Schmer-Galunder</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Data annotation remains the sine qua non of machine learning and AI. Recent
empirical work on data annotation has begun to highlight the importance of
rater diversity for fairness, model performance, and new lines of research have
begun to examine the working conditions for data annotation workers, the
impacts and role of annotator subjectivity on labels, and the potential
psychological harms from aspects of annotation work. This paper outlines a
critical genealogy of data annotation; starting with its psychological and
perceptual aspects. We draw on similarities with critiques of the rise of
computerized lab-based psychological experiments in the 1970's which question
whether these experiments permit the generalization of results beyond the
laboratory settings within which these results are typically obtained. Do data
annotations permit the generalization of results beyond the settings, or
locations, in which they were obtained? Psychology is overly reliant on
participants from Western, Educated, Industrialized, Rich, and Democratic
societies (WEIRD). Many of the people who work as data annotation platform
workers, however, are not from WEIRD countries; most data annotation workers
are based in Global South countries. Social categorizations and classifications
from WEIRD countries are imposed on non-WEIRD annotators through instructions
and tasks, and through them, on data, which is then used to train or evaluate
AI models in WEIRD countries. We synthesize evidence from several recent lines
of research and argue that data annotation is a form of automated social
categorization that risks entrenching outdated and static social categories
that are in reality dynamic and changing. We propose a framework for
understanding the interplay of the global social conditions of data annotation
with the subjective phenomenological experience of data annotation work.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06812" title="Abstract">arXiv:2402.06812</a> [<a href="/pdf/2402.06812" title="Download PDF">pdf</a>, <a href="/format/2402.06812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Kalman Filter Based Framework for Monitoring the Performance of  In-Hospital Mortality Prediction Models Over Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiacheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kirkland%2C+L">Lisa Kirkland</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+J">Jaideep Srivastava</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Unlike in a clinical trial, where researchers get to determine the least
number of positive and negative samples required, or in a machine learning
study where the size and the class distribution of the validation set is static
and known, in a real-world scenario, there is little control over the size and
distribution of incoming patients. As a result, when measured during different
time periods, evaluation metrics like Area under the Receiver Operating Curve
(AUCROC) and Area Under the Precision-Recall Curve(AUCPR) may not be directly
comparable. Therefore, in this study, for binary classifiers running in a long
time period, we proposed to adjust these performance metrics for sample size
and class distribution, so that a fair comparison can be made between two time
periods. Note that the number of samples and the class distribution, namely the
ratio of positive samples, are two robustness factors which affect the variance
of AUCROC. To better estimate the mean of performance metrics and understand
the change of performance over time, we propose a Kalman filter based framework
with extrapolated variance adjusted for the total number of samples and the
number of positive samples during different time periods. The efficacy of this
method is demonstrated first on a synthetic dataset and then retrospectively
applied to a 2-days ahead in-hospital mortality prediction model for COVID-19
patients during 2021 and 2022. Further, we conclude that our prediction model
is not significantly affected by the evolution of the disease, improved
treatments and changes in hospital operational plans.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06814" title="Abstract">arXiv:2402.06814</a> [<a href="/pdf/2402.06814" title="Download PDF">pdf</a>, <a href="/format/2402.06814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-Rate Fair-Density Parity-Check Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahdavifar%2C+H">Hessam Mahdavifar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">We introduce fair-density parity-check (FDPC) codes targeting high-rate
applications. In particular, we start with a base parity-check matrix $H_b$ of
dimension $2 \sqrt{n} \times n$, where $n$ is the code block length, and the
number of ones in each row and column of $H_b$ is equal to $\sqrt{n}$ and $2$,
respectively. We propose a deterministic combinatorial method for picking the
base matrix $H_b$, assuming $n=4t^2$ for some integer $t \geq 2$. We then
extend this by obtaining permuted versions of $H_b$ (e.g., via random
permutations of its columns) and stacking them on top of each other leading to
codes of dimension $k \geq n-2s\sqrt{n}+s$, for some $s \geq 2$, referred to as
order-$s$ FDPC codes. We propose methods to explicitly characterize and bound
the weight distribution of the new codes and utilize them to derive union-type
approximate upper bounds on their error probability under Maximum Likelihood
(ML) decoding. For the binary erasure channel (BEC), we demonstrate that the
approximate ML bound of FDPC codes closely follows the random coding upper
bound (RCU) for a wide range of channel parameters. Also, remarkably, FDPC
codes, under the low-complexity min-sum decoder, improve upon 5G-LDPC codes for
transmission over the binary-input additive white Gaussian noise (B-AWGN)
channel by almost 0.5dB (for $n=1024$, and rate $=0.878$). Furthermore, we
propose a new decoder as a combination of weighted min-sum message-passing (MP)
decoding algorithm together with a new progressive list (PL) decoding
component, referred to as the MP-PL decoder, to further boost the performance
of FDPC codes.
<br />This paper opens new avenues for a fresh investigation of new code
constructions and decoding algorithms in high-rate regimes suitable for
ultra-high throughput (high-frequency/optical) applications.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06815" title="Abstract">arXiv:2402.06815</a> [<a href="/pdf/2402.06815" title="Download PDF">pdf</a>, <a href="/format/2402.06815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating Player Performance in Different Contexts Using Fine-tuned  Large Events Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mendes-Neves%2C+T">Tiago Mendes-Neves</a>, 
<a href="/search/cs?searchtype=author&query=Meireles%2C+L">Lu&#xed;s Meireles</a>, 
<a href="/search/cs?searchtype=author&query=Mendes-Moreira%2C+J">Jo&#xe3;o Mendes-Moreira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper introduces an innovative application of Large Event Models (LEMs),
akin to Large Language Models, to the domain of soccer analytics. By learning
the "language" of soccer - predicting variables for subsequent events rather
than words LEMs facilitate the simulation of matches and offer various
applications, including player performance prediction across different team
contexts. We focus on fine-tuning LEMs with the WyScout dataset for the
2017-2018 Premier League season to derive specific insights into player
contributions and team strategies. Our methodology involves adapting these
models to reflect the nuanced dynamics of soccer, enabling the evaluation of
hypothetical transfers. Our findings confirm the effectiveness and limitations
of LEMs in soccer analytics, highlighting the model's capability to forecast
teams' expected standings and explore high-profile scenarios, such as the
potential effects of transferring Cristiano Ronaldo or Lionel Messi to
different teams in the Premier League. This analysis underscores the importance
of context in evaluating player quality. While general metrics may suggest
significant differences between players, contextual analyses reveal narrower
gaps in performance within specific team frameworks.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06818" title="Abstract">arXiv:2402.06818</a> [<a href="/pdf/2402.06818" title="Download PDF">pdf</a>, <a href="/format/2402.06818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Systematic Approach to Design New Ensemble Learning Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mendes-Moreira%2C+J">Jo&#xe3;o Mendes-Moreira</a>, 
<a href="/search/cs?searchtype=author&query=Mendes-Neves%2C+T">Tiago Mendes-Neves</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Ensemble learning has been a focal point of machine learning research due to
its potential to improve predictive performance. This study revisits the
foundational work on ensemble error decomposition, historically confined to
bias-variance-covariance analysis for regression problems since the 1990s.
Recent advancements introduced a "unified theory of diversity," which proposes
an innovative bias-variance-diversity decomposition framework. Leveraging this
contemporary understanding, our research systematically explores the
application of this decomposition to guide the creation of new ensemble
learning algorithms. Focusing on regression tasks, we employ neural networks as
base learners to investigate the practical implications of this theoretical
framework. This approach used 7 simple ensemble methods, we name them
strategies, for neural networks that were used to generate 21 new ensemble
algorithms. Among these, most of the methods aggregated with the snapshot
strategy, one of the 7 strategies used, showcase superior predictive
performance across diverse datasets w.r.t. the Friedman rank test with the
Conover post-hoc test. Our systematic design approach contributes a suite of
effective new algorithms and establishes a structured pathway for future
ensemble learning algorithm development.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06819" title="Abstract">arXiv:2402.06819</a> [<a href="/pdf/2402.06819" title="Download PDF">pdf</a>, <a href="/format/2402.06819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Monitored Markov Decision Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parisi%2C+S">Simone Parisi</a>, 
<a href="/search/cs?searchtype=author&query=Mohammedalamen%2C+M">Montaser Mohammedalamen</a>, 
<a href="/search/cs?searchtype=author&query=Kazemipour%2C+A">Alireza Kazemipour</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+M+E">Matthew E. Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Bowling%2C+M">Michael Bowling</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAMAS 2024, Main Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In reinforcement learning (RL), an agent learns to perform a task by
interacting with an environment and receiving feedback (a numerical reward) for
its actions. However, the assumption that rewards are always observable is
often not applicable in real-world problems. For example, the agent may need to
ask a human to supervise its actions or activate a monitoring system to receive
feedback. There may even be a period of time before rewards become observable,
or a period of time after which rewards are no longer given. In other words,
there are cases where the environment generates rewards in response to the
agent's actions but the agent cannot observe them. In this paper, we formalize
a novel but general RL framework - Monitored MDPs - where the agent cannot
always observe rewards. We discuss the theoretical and practical consequences
of this setting, show challenges raised even in toy environments, and propose
algorithms to begin to tackle this novel setting. This paper introduces a
powerful new formalism that encompasses both new and existing problems and lays
the foundation for future research.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06820" title="Abstract">arXiv:2402.06820</a> [<a href="/pdf/2402.06820" title="Download PDF">pdf</a>, <a href="/format/2402.06820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forecasting Events in Soccer Matches Through Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mendes-Neves%2C+T">Tiago Mendes-Neves</a>, 
<a href="/search/cs?searchtype=author&query=Meireles%2C+L">Lu&#xed;s Meireles</a>, 
<a href="/search/cs?searchtype=author&query=Mendes-Moreira%2C+J">Jo&#xe3;o Mendes-Moreira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper introduces an approach to predicting the next event in a soccer
match, a challenge bearing remarkable similarities to the problem faced by
Large Language Models (LLMs). Unlike other methods that severely limit event
dynamics in soccer, often abstracting from many variables or relying on a mix
of sequential models, our research proposes a novel technique inspired by the
methodologies used in LLMs. These models predict a complete chain of variables
that compose an event, significantly simplifying the construction of Large
Event Models (LEMs) for soccer. Utilizing deep learning on the publicly
available WyScout dataset, the proposed approach notably surpasses the
performance of previous LEM proposals in critical areas, such as the prediction
accuracy of the next event type. This paper highlights the utility of LEMs in
various applications, including betting and match analytics. Moreover, we show
that LEMs provide a simulation backbone on which many analytics pipelines can
be built, an approach opposite to the current specialized single-purpose
models. LEMs represent a pivotal advancement in soccer analytics, establishing
a foundational framework for multifaceted analytics pipelines through a
singular machine-learning model.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06821" title="Abstract">arXiv:2402.06821</a> [<a href="/pdf/2402.06821" title="Download PDF">pdf</a>, <a href="/format/2402.06821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Complexity of Promise Constraint Satisfaction Problem Seen from the  Other Side
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asimi%2C+K">Kristina Asimi</a>, 
<a href="/search/cs?searchtype=author&query=Barto%2C+L">Libor Barto</a>, 
<a href="/search/cs?searchtype=author&query=Dalmau%2C+V">Victor Dalmau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
<p class="mathjax">We introduce the framework of the left-hand side restricted promise
constraint satisfaction problem, which includes problems like approximating
clique number of a graph. We study the parameterized complexity of problems in
this class and provide some initial results. The main technical contribution is
a sufficient condition for W[1]-hardness which, in particular, covers left-hand
side restricted bounded arity CSPs.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06823" title="Abstract">arXiv:2402.06823</a> [<a href="/pdf/2402.06823" title="Download PDF">pdf</a>, <a href="/format/2402.06823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding the Best Route During the Pandemic Disease
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mirgalooyebayat%2C+A">Amirsadegh Mirgalooyebayat</a>, 
<a href="/search/cs?searchtype=author&query=Didehvar%2C+F">Farzad Didehvar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">In this article, we try to find the best routes during the pandemic so that
the probability of contracting the disease is the lowest. According to the
results of this article, we can design software to find the best route.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06826" title="Abstract">arXiv:2402.06826</a> [<a href="/pdf/2402.06826" title="Download PDF">pdf</a>, <a href="/format/2402.06826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Rendering based Urban Scene Reconstruction for Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Shihao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Kerofsky%2C+L">Louis Kerofsky</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+V+R">Varun Ravi Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Yogamani%2C+S">Senthil Yogamani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in Electronic Imaging, Autonomous Vehicles and Machines 2024. Qualitative results are shared in <a href="https://youtu.be/EK47fYJiY3M">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Dense 3D reconstruction has many applications in automated driving including
automated annotation validation, multimodal data augmentation, providing ground
truth annotations for systems lacking LiDAR, as well as enhancing auto-labeling
accuracy. LiDAR provides highly accurate but sparse depth, whereas camera
images enable estimation of dense depth but noisy particularly at long ranges.
In this paper, we harness the strengths of both sensors and propose a
multimodal 3D scene reconstruction using a framework combining neural implicit
surfaces and radiance fields. In particular, our method estimates dense and
accurate 3D structures and creates an implicit map representation based on
signed distance fields, which can be further rendered into RGB images, and
depth maps. A mesh can be extracted from the learned signed distance field and
culled based on occlusion. Dynamic objects are efficiently filtered on the fly
during sampling using 3D object detection models. We demonstrate qualitative
and quantitative results on challenging automotive scenes.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06827" title="Abstract">arXiv:2402.06827</a> [<a href="/pdf/2402.06827" title="Download PDF">pdf</a>, <a href="/format/2402.06827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RAMP: Boosting Adversarial Robustness Against Multiple $l_p$  Perturbations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+E">Enyi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+G">Gagandeep Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">There is considerable work on improving robustness against adversarial
attacks bounded by a single $l_p$ norm using adversarial training (AT).
However, the multiple-norm robustness (union accuracy) of AT models is still
low. We observe that simultaneously obtaining good union and clean accuracy is
hard since there are tradeoffs between robustness against multiple $l_p$
perturbations, and accuracy/robustness/efficiency. By analyzing the tradeoffs
from the lens of distribution shifts, we identify the key tradeoff pair among
$l_p$ attacks to boost efficiency and design a logit pairing loss to improve
the union accuracy. Next, we connect natural training with AT via gradient
projection, to find and incorporate useful information from natural training
into AT, which moderates the accuracy/robustness tradeoff. Combining our
contributions, we propose a framework called \textbf{RAMP}, to boost the
robustness against multiple $l_p$ perturbations. We show \textbf{RAMP} can be
easily adapted for both robust fine-tuning and full AT. For robust fine-tuning,
\textbf{RAMP} obtains a union accuracy up to $53.5\%$ on CIFAR-10, and $29.7\%$
on ImageNet. For training from scratch, \textbf{RAMP} achieves SOTA union
accuracy of $44.6\%$ and relatively good clean accuracy of $81.2\%$ on
ResNet-18 against AutoAttack on CIFAR-10.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06829" title="Abstract">arXiv:2402.06829</a> [<a href="/pdf/2402.06829" title="Download PDF">pdf</a>, <a href="/format/2402.06829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reduced-order Modeling of Modular, Position-dependent Systems with  Translating Interfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Egelmeers%2C+R+A">Robert A. Egelmeers</a>, 
<a href="/search/eess?searchtype=author&query=Janssen%2C+L+A+L">Lars A. L. Janssen</a>, 
<a href="/search/eess?searchtype=author&query=Fey%2C+R+H+B">Rob H. B. Fey</a>, 
<a href="/search/eess?searchtype=author&query=Gerritsen%2C+J">Jasper Gerritsen</a>, 
<a href="/search/eess?searchtype=author&query=van+de+Wouw%2C+N">Nathan van de Wouw</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Many complex mechatronic systems consist of multiple interconnected dynamical
subsystems, which are designed, developed, analyzed, and manufactured by
multiple independent teams. To support such a design approach, a modular model
framework is needed to reduce computational complexity and, at the same time,
enable multiple teams to develop and analyze the subsystems in parallel. In
such a modular framework, the subsystem models are typically interconnected by
means of a static interconnection structure. However, many complex dynamical
systems exhibit position-dependent behavior (e.g., induced by translating
interfaces) which cannot be not captured by such static interconnection models.
In this paper, a modular model framework is proposed, which allows to construct
an interconnected system model, which captures the position-dependent behavior
of systems with translating interfaces, such as linear guide rails, through a
position-dependent interconnection structure. Additionally, this framework
allows to apply model reduction on subsystem level, enabling a more effective
reduction approach, tailored to the specific requirements of each subsystem.
Furthermore, we show the effectiveness of this framework on an industrial wire
bonder. Here, we show that including a position-dependent model of the
interconnection structure 1) enables to accurately model the dynamics of a
system over the operating range of the system and, 2) modular model reduction
methods can be used to obtain a computationally efficient interconnected system
model with guaranteed accuracy specifications.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06831" title="Abstract">arXiv:2402.06831</a> [<a href="/pdf/2402.06831" title="Download PDF">pdf</a>, <a href="/ps/2402.06831" title="Download PostScript">ps</a>, <a href="/format/2402.06831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What We Know About Using Non-Engagement Signals in Content Ranking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cunningham%2C+T">Tom Cunningham</a>, 
<a href="/search/cs?searchtype=author&query=Pandey%2C+S">Sana Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Sigerson%2C+L">Leif Sigerson</a>, 
<a href="/search/cs?searchtype=author&query=Stray%2C+J">Jonathan Stray</a>, 
<a href="/search/cs?searchtype=author&query=Allen%2C+J">Jeff Allen</a>, 
<a href="/search/cs?searchtype=author&query=Barrilleaux%2C+B">Bonnie Barrilleaux</a>, 
<a href="/search/cs?searchtype=author&query=Iyer%2C+R">Ravi Iyer</a>, 
<a href="/search/cs?searchtype=author&query=Milli%2C+S">Smitha Milli</a>, 
<a href="/search/cs?searchtype=author&query=Kothari%2C+M">Mohit Kothari</a>, 
<a href="/search/cs?searchtype=author&query=Rezaei%2C+B">Behnam Rezaei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Many online platforms predominantly rank items by predicted user engagement.
We believe that there is much unrealized potential in including non-engagement
signals, which can improve outcomes both for platforms and for society as a
whole. Based on a daylong workshop with experts from industry and academia, we
formulate a series of propositions and document each as best we can from public
evidence, including quantitative results where possible.
<br />There is strong evidence that ranking by predicted engagement is effective in
increasing user retention. However retention can be further increased by
incorporating other signals, including item "quality" proxies and asking users
what they want to see with "item-level" surveys. There is also evidence that
"diverse engagement" is an effective quality signal. Ranking changes can alter
the prevalence of self-reported experiences of various kinds (e.g. harassment)
but seldom have large enough effects on attitude measures like user
satisfaction, well-being, polarization etc. to be measured in typical
experiments. User controls over ranking often have low usage rates, but when
used they do correlate well with quality and item-level surveys. There was no
strong evidence on the impact of transparency/explainability on retention.
There is reason to believe that generative AI could be used to create better
quality signals and enable new kinds of user controls.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06838" title="Abstract">arXiv:2402.06838</a> [<a href="/pdf/2402.06838" title="Download PDF">pdf</a>, <a href="/ps/2402.06838" title="Download PostScript">ps</a>, <a href="/format/2402.06838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NavFormer: A Transformer Architecture for Robot Target-Driven Navigation  in Unknown and Dynamic Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haitong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+A+H">Aaron Hao Tan</a>, 
<a href="/search/cs?searchtype=author&query=Nejat%2C+G">Goldie Nejat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In unknown cluttered and dynamic environments such as disaster scenes, mobile
robots need to perform target-driven navigation in order to find people or
objects of interest, while being solely guided by images of the targets. In
this paper, we introduce NavFormer, a novel end-to-end transformer architecture
developed for robot target-driven navigation in unknown and dynamic
environments. NavFormer leverages the strengths of both 1) transformers for
sequential data processing and 2) self-supervised learning (SSL) for visual
representation to reason about spatial layouts and to perform
collision-avoidance in dynamic settings. The architecture uniquely combines
dual-visual encoders consisting of a static encoder for extracting invariant
environment features for spatial reasoning, and a general encoder for dynamic
obstacle avoidance. The primary robot navigation task is decomposed into two
sub-tasks for training: single robot exploration and multi-robot collision
avoidance. We perform cross-task training to enable the transfer of learned
skills to the complex primary navigation task without the need for
task-specific fine-tuning. Simulated experiments demonstrate that NavFormer can
effectively navigate a mobile robot in diverse unknown environments,
outperforming existing state-of-the-art methods in terms of success rate and
success weighted by (normalized inverse) path length. Furthermore, a
comprehensive ablation study is performed to evaluate the impact of the main
design choices of the structure and training of NavFormer, further validating
their effectiveness in the overall system.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06844" title="Abstract">arXiv:2402.06844</a> [<a href="/pdf/2402.06844" title="Download PDF">pdf</a>, <a href="/format/2402.06844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using $LDL^{T}$ factorizations in Newton&#x27;s method for solving general  large-scale algebraic Riccati equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Saak%2C+J">Jens Saak</a>, 
<a href="/search/math?searchtype=author&query=Werner%2C+S+W+R">Steffen W. R. Werner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 1 figure, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Continuous-time algebraic Riccati equations can be found in many disciplines
in different forms. In the case of small-scale dense coefficient matrices,
stabilizing solutions can be computed to all possible formulations of the
Riccati equation. This is not the case when it comes to large-scale sparse
coefficient matrices. In this paper, we provide a reformulation of the
Newton-Kleinman iteration scheme for continuous-time algebraic Riccati
equations using indefinite symmetric low-rank factorizations. This allows the
application of the method to the case of general large-scale sparse coefficient
matrices. We provide convergence results for several prominent realizations of
the equation and show in numerical examples the effectiveness of the approach.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06846" title="Abstract">arXiv:2402.06846</a> [<a href="/pdf/2402.06846" title="Download PDF">pdf</a>, <a href="/format/2402.06846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> System-level Analysis of Adversarial Attacks and Defenses on  Intelligence in O-RAN based Cellular Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chiejina%2C+A">Azuka Chiejina</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+B">Brian Kim</a>, 
<a href="/search/cs?searchtype=author&query=Chowhdury%2C+K">Kaushik Chowhdury</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+V+K">Vijay K. Shah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages including references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">While the open architecture, open interfaces, and integration of intelligence
within Open Radio Access Network technology hold the promise of transforming 5G
and 6G networks, they also introduce cybersecurity vulnerabilities that hinder
its widespread adoption. In this paper, we conduct a thorough system-level
investigation of cyber threats, with a specific focus on machine learning (ML)
intelligence components known as xApps within the O-RAN's near-real-time RAN
Intelligent Controller (near-RT RIC) platform. Our study begins by developing a
malicious xApp designed to execute adversarial attacks on two types of test
data - spectrograms and key performance metrics (KPMs), stored in the RIC
database within the near-RT RIC. To mitigate these threats, we utilize a
distillation technique that involves training a teacher model at a high softmax
temperature and transferring its knowledge to a student model trained at a
lower softmax temperature, which is deployed as the robust ML model within
xApp. We prototype an over-the-air LTE/5G O-RAN testbed to assess the impact of
these attacks and the effectiveness of the distillation defense technique by
leveraging an ML-based Interference Classification (InterClass) xApp as an
example. We examine two versions of InterClass xApp under distinct scenarios,
one based on Convolutional Neural Networks (CNNs) and another based on Deep
Neural Networks (DNNs) using spectrograms and KPMs as input data respectively.
Our findings reveal up to 100% and 96.3% degradation in the accuracy of both
the CNN and DNN models respectively resulting in a significant decline in
network performance under considered adversarial attacks. Under the strict
latency constraints of the near-RT RIC closed control loop, our analysis shows
that the distillation technique outperforms classical adversarial training by
achieving an accuracy of up to 98.3% for mitigating such attacks.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06852" title="Abstract">arXiv:2402.06852</a> [<a href="/pdf/2402.06852" title="Download PDF">pdf</a>, <a href="/format/2402.06852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChemLLM: A Chemical Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Di Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Q">Qian Tan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jingdan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yuliang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiatong Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weiran Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+X">Xiangyu Yue</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Dongzhan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shufei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+M">Mao Su</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+H">Hansen Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuqiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models (LLMs) have made impressive progress in chemistry
applications, including molecular property prediction, molecular generation,
experimental protocol design, etc. However, the community lacks a
dialogue-based model specifically designed for chemistry. The challenge arises
from the fact that most chemical data and scientific knowledge are primarily
stored in structured databases, and the direct use of these structured data
compromises the model's ability to maintain coherent dialogue. To tackle this
issue, we develop a novel template-based instruction construction method that
transforms structured knowledge into plain dialogue, making it suitable for
language model training. By leveraging this approach, we develop ChemLLM, the
first large language model dedicated to chemistry, capable of performing
various tasks across chemical disciplines with smooth dialogue interaction.
ChemLLM beats GPT-3.5 on all three principal tasks in chemistry, i.e., name
conversion, molecular caption, and reaction prediction, and surpasses GPT-4 on
two of them. Remarkably, ChemLLM also shows exceptional adaptability to related
mathematical and physical tasks despite being trained mainly on
chemical-centric corpora. Furthermore, ChemLLM demonstrates proficiency in
specialized NLP tasks within chemistry, such as literature translation and
cheminformatic programming. ChemLLM opens up a new avenue for exploration
within chemical studies, while our method of integrating structured chemical
knowledge into dialogue systems sets a new frontier for developing LLMs across
various scientific fields. Codes, Datasets, and Model weights are publicly
accessible at hf.co/AI4Chem/ChemLLM-7B-Chat.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06853" title="Abstract">arXiv:2402.06853</a> [<a href="/pdf/2402.06853" title="Download PDF">pdf</a>, <a href="/format/2402.06853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> History, Development, and Principles of Large Language Models-An  Introductory Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+Z">Zhibo Chu</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+S">Shiwen Ni</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zichong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+X">Xi Feng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengming Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiping Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruifeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Min Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenbin Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Language models serve as a cornerstone in natural language processing (NLP),
utilizing mathematical methods to generalize language laws and knowledge for
prediction and generation. Over extensive research spanning decades, language
modeling has progressed from initial statistical language models (SLMs) to the
contemporary landscape of large language models (LLMs). Notably, the swift
evolution of LLMs has reached the ability to process, understand, and generate
human-level text. Nevertheless, despite the significant advantages that LLMs
offer in improving both work and personal lives, the limited understanding
among general practitioners about the background and principles of these models
hampers their full potential. Notably, most LLMs reviews focus on specific
aspects and utilize specialized language, posing a challenge for practitioners
lacking relevant background knowledge. In light of this, this survey aims to
present a comprehensible overview of LLMs to assist a broader audience. It
strives to facilitate a comprehensive understanding by exploring the historical
background of language models and tracing their evolution over time. The survey
further investigates the factors influencing the development of LLMs,
emphasizing key contributions. Additionally, it concentrates on elucidating the
underlying principles of LLMs, equipping audiences with essential theoretical
knowledge. The survey also highlights the limitations of existing work and
points out promising future directions.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06854" title="Abstract">arXiv:2402.06854</a> [<a href="/pdf/2402.06854" title="Download PDF">pdf</a>, <a href="/format/2402.06854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gyroscope-Assisted Motion Deblurring Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luan%2C+S">Simin Luan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Boukhers%2C+Z">Zeyd Boukhers</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+X">Xue Qin</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+D">Dongfeng Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+W">Wei Sui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhijun Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Image research has shown substantial attention in deblurring networks in
recent years. Yet, their practical usage in real-world deblurring, especially
motion blur, remains limited due to the lack of pixel-aligned training triplets
(background, blurred image, and blur heat map) and restricted information
inherent in blurred images. This paper presents a simple yet efficient
framework to synthetic and restore motion blur images using Inertial
Measurement Unit (IMU) data. Notably, the framework includes a strategy for
training triplet generation, and a Gyroscope-Aided Motion Deblurring (GAMD)
network for blurred image restoration. The rationale is that through harnessing
IMU data, we can determine the transformation of the camera pose during the
image exposure phase, facilitating the deduction of the motion trajectory (aka.
blur trajectory) for each point inside the three-dimensional space. Thus, the
synthetic triplets using our strategy are inherently close to natural motion
blur, strictly pixel-aligned, and mass-producible. Through comprehensive
experiments, we demonstrate the advantages of the proposed framework: only
two-pixel errors between our synthetic and real-world blur trajectories, a
marked improvement (around 33.17%) of the state-of-the-art deblurring method
MIMO on Peak Signal-to-Noise Ratio (PSNR).
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06855" title="Abstract">arXiv:2402.06855</a> [<a href="/pdf/2402.06855" title="Download PDF">pdf</a>, <a href="/format/2402.06855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> For Better or For Worse? Learning Minimum Variance Features With Label  Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chidambaram%2C+M">Muthu Chidambaram</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+R">Rong Ge</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Data augmentation has been pivotal in successfully training deep learning
models on classification tasks over the past decade. An important subclass of
data augmentation techniques - which includes both label smoothing and Mixup -
involves modifying not only the input data but also the input label during
model training. In this work, we analyze the role played by the label
augmentation aspect of such methods. We prove that linear models on linearly
separable data trained with label augmentation learn only the minimum variance
features in the data, while standard training (which includes weight decay) can
learn higher variance features. An important consequence of our results is
negative: label smoothing and Mixup can be less robust to adversarial
perturbations of the training data when compared to standard training. We
verify that our theory reflects practice via a range of experiments on
synthetic data and image classification benchmarks.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06857" title="Abstract">arXiv:2402.06857</a> [<a href="/pdf/2402.06857" title="Download PDF">pdf</a>, <a href="/format/2402.06857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multigrid solvers for multipoint flux approximations of the Darcy  problem on rough quadrilateral grids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Arrar%C3%A1s%2C+A">Andr&#xe9;s Arrar&#xe1;s</a>, 
<a href="/search/math?searchtype=author&query=Gaspar%2C+F+J">Francisco J. Gaspar</a>, 
<a href="/search/math?searchtype=author&query=Portero%2C+L">Laura Portero</a>, 
<a href="/search/math?searchtype=author&query=Rodrigo%2C+C">Carmen Rodrigo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this work, an efficient blackbox-type multigrid method is proposed for
solving multipoint flux approximations of the Darcy problem on logically
rectangular grids. The approach is based on a cell-centered multigrid
algorithm, which combines a piecewise constant interpolation and the
restriction operator by Wesseling/Khalil with a line-wise relaxation procedure.
A local Fourier analysis is performed for the case of a Cartesian uniform grid.
The method shows a robust convergence for different full tensor coefficient
problems and several rough quadrilateral grids.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06859" title="Abstract">arXiv:2402.06859</a> [<a href="/pdf/2402.06859" title="Download PDF">pdf</a>, <a href="/format/2402.06859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LiRank: Industrial Large Scale Ranking Models at LinkedIn
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Borisyuk%2C+F">Fedor Borisyuk</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mingzhou Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Q">Qingquan Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Siyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Tiwana%2C+B">Birjodh Tiwana</a>, 
<a href="/search/cs?searchtype=author&query=Parameswaran%2C+G">Ganesh Parameswaran</a>, 
<a href="/search/cs?searchtype=author&query=Dangi%2C+S">Siddharth Dangi</a>, 
<a href="/search/cs?searchtype=author&query=Hertel%2C+L">Lars Hertel</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Q">Qiang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+X">Xiaochen Hou</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+Y">Yunbo Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Aman Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Sheallika Singh</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hailing Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+L">Lei Le</a>, 
<a href="/search/cs?searchtype=author&query=Hung%2C+J">Jonathan Hung</a>, 
<a href="/search/cs?searchtype=author&query=Keerthi%2C+S">Sathiya Keerthi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Ruoyan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fengyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kothari%2C+M">Mohit Kothari</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+D">Daqi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yun Dai</a>, 
<a href="/search/cs?searchtype=author&query=Luan%2C+X">Xun Luan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Sirou Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhiwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Daftary%2C+N">Neil Daftary</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Q">Qianqi Shen</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Chengming Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Haichao Wei</a>, 
<a href="/search/cs?searchtype=author&query=Varshney%2C+M">Maneesh Varshney</a>, 
<a href="/search/cs?searchtype=author&query=Ghoting%2C+A">Amol Ghoting</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Souvik Ghosh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">We present LiRank, a large-scale ranking framework at LinkedIn that brings to
production state-of-the-art modeling architectures and optimization methods. We
unveil several modeling improvements, including Residual DCN, which adds
attention and residual connections to the famous DCNv2 architecture. We share
insights into combining and tuning SOTA architectures to create a unified
model, including Dense Gating, Transformers and Residual DCN. We also propose
novel techniques for calibration and describe how we productionalized deep
learning based explore/exploit methods. To enable effective, production-grade
serving of large ranking models, we detail how to train and compress models
using quantization and vocabulary compression. We provide details about the
deployment setup for large-scale use cases of Feed ranking, Jobs
Recommendations, and Ads click-through rate (CTR) prediction. We summarize our
learnings from various A/B tests by elucidating the most effective technical
approaches. These ideas have contributed to relative metrics improvements
across the board at LinkedIn: +0.5% member sessions in the Feed, +1.76%
qualified job applications for Jobs search and recommendations, and +4.3% for
Ads CTR. We hope this work can provide practical insights and solutions for
practitioners interested in leveraging large-scale deep ranking systems.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06860" title="Abstract">arXiv:2402.06860</a> [<a href="/pdf/2402.06860" title="Download PDF">pdf</a>, <a href="/format/2402.06860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Age-Memory Trade-off in Read-Copy-Update
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramani%2C+V">Vishakha Ramani</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiachen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yates%2C+R+D">Roy D. Yates</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE INFOCOM Age and Semantics of Information (ASoI) Workshop 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Operating Systems (cs.OS)

</div>
<p class="mathjax">In the realm of shared memory systems, the challenge of reader-writer
synchronization is closely coupled with the potential for readers to access
outdated updates. Read-Copy-Update (RCU) is a synchronization primitive that
allows for concurrent and non-blocking read access to fresh data. This is
achieved through the creation of updated data copies, with each prior version
retained until all associated read-locks are released. Given the principle that
frequent updating keeps information fresh, the concern is whether we accumulate
an infinite number of update copies, leading to excessively large memory usage.
This paper analyzes trade-offs between memory usage and update age within
real-time status updating systems, focusing specifically on RCU. The analysis
demonstrates that with finite read time and read request rate, the average
number of updates within the system remains bounded.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06861" title="Abstract">arXiv:2402.06861</a> [<a href="/pdf/2402.06861" title="Download PDF">pdf</a>, <a href="/format/2402.06861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UrbanKGent: A Unified Large Language Model Agent Framework for Urban  Knowledge Graph Construction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ning%2C+Y">Yansong Ning</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hao Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Urban knowledge graph has recently worked as an emerging building block to
distill critical knowledge from multi-sourced urban data for diverse urban
application scenarios. Despite its promising benefits, urban knowledge graph
construction (UrbanKGC) still heavily relies on manual effort, hindering its
potential advancement. This paper presents UrbanKGent, a unified large language
model agent framework, for urban knowledge graph construction. Specifically, we
first construct the knowledgeable instruction set for UrbanKGC tasks (such as
relational triplet extraction and knowledge graph completion) via
heterogeneity-aware and geospatial-infused instruction generation. Moreover, we
propose a tool-augmented iterative trajectory refinement module to enhance and
refine the trajectories distilled from GPT-4. Through hybrid instruction
fine-tuning with augmented trajectories on Llama-2-13B, we obtain the UrbanKGC
agent, UrbanKGent-13B. We perform a comprehensive evaluation on two real-world
datasets using both human and GPT-4 self-evaluation. The experimental results
demonstrate that UrbanKGent-13B not only can significantly outperform 21
baselines in UrbanKGC tasks, but also surpass the state-of-the-art LLM, GPT-4,
by more than 10\% with approximately 20 times lower cost. We deploy
UrbanKGent-13B to provide online services, which can construct an UrbanKG with
thousands of times richer relationships using only one-fifth of the data
compared with the existing benchmark. Our data, code, and opensource UrbanKGC
agent are available at https://github.com/usail-hkust/UrbanKGent.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06864" title="Abstract">arXiv:2402.06864</a> [<a href="/pdf/2402.06864" title="Download PDF">pdf</a>, <a href="/format/2402.06864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discriminative Adversarial Unlearning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Rohan Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shijie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+K">Kaiyi Ji</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Changyou Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages including references, 2 tables, 2 figures and 1 algorithm
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce a novel machine unlearning framework founded upon the
established principles of the min-max optimization paradigm. We capitalize on
the capabilities of strong Membership Inference Attacks (MIA) to facilitate the
unlearning of specific samples from a trained model. We consider the scenario
of two networks, the attacker $\mathbf{A}$ and the trained defender
$\mathbf{D}$ pitted against each other in an adversarial objective, wherein the
attacker aims at teasing out the information of the data to be unlearned in
order to infer membership, and the defender unlearns to defend the network
against the attack, whilst preserving its general performance. The algorithm
can be trained end-to-end using backpropagation, following the well known
iterative min-max approach in updating the attacker and the defender. We
additionally incorporate a self-supervised objective effectively addressing the
feature space discrepancies between the forget set and the validation set,
enhancing unlearning performance. Our proposed algorithm closely approximates
the ideal benchmark of retraining from scratch for both random sample
forgetting and class-wise forgetting schemes on standard machine-unlearning
datasets. Specifically, on the class unlearning scheme, the method demonstrates
near-optimal performance and comprehensively overcomes known methods over the
random sample forgetting scheme across all metrics and multiple network pruning
strategies.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06869" title="Abstract">arXiv:2402.06869</a> [<a href="/pdf/2402.06869" title="Download PDF">pdf</a>, <a href="/ps/2402.06869" title="Download PostScript">ps</a>, <a href="/format/2402.06869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digital Footprints of Streaming Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+S">Sundar Krishnan</a>, 
<a href="/search/cs?searchtype=author&query=Glisson%2C+W+B">William Bradley Glisson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">These days, there are many ways to watch streaming videos on television. When
compared to a standalone smart television, streaming devices such as Roku and
Amazon Fire Stick have a plethora of app selections. While these devices are
platform agnostic and compatible with smartphones, they can still leave behind
crumbs of sensitive data that can cause privacy, security, and forensic issues.
In this paper, the authors conduct an experiment with streaming devices to
ascertain digital footprints from network traffic and mobile forensics that
they leave behind.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06871" title="Abstract">arXiv:2402.06871</a> [<a href="/pdf/2402.06871" title="Download PDF">pdf</a>, <a href="/format/2402.06871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-autoregressive Generative Models for Reranking Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yuxin Ren</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qiya Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yichun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yalong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiqiang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In a multi-stage recommendation system, reranking plays a crucial role by
modeling the intra-list correlations among items.The key challenge of reranking
lies in the exploration of optimal sequences within the combinatorial space of
permutations. Recent research proposes a generator-evaluator learning paradigm,
where the generator generates multiple feasible sequences and the evaluator
picks out the best sequence based on the estimated listwise score. Generator is
of vital importance, and generative models are well-suited for the generator
function. Current generative models employ an autoregressive strategy for
sequence generation. However, deploying autoregressive models in real-time
industrial systems is challenging. Hence, we propose a Non-AutoRegressive
generative model for reranking Recommendation (NAR4Rec) designed to enhance
efficiency and effectiveness. To address challenges related to sparse training
samples and dynamic candidates impacting model convergence, we introduce a
matching model. Considering the diverse nature of user feedback, we propose a
sequence-level unlikelihood training objective to distinguish feasible from
unfeasible sequences. Additionally, to overcome the lack of dependency modeling
in non-autoregressive models regarding target items, we introduce contrastive
decoding to capture correlations among these items. Extensive offline
experiments on publicly available datasets validate the superior performance of
our proposed approach compared to the existing state-of-the-art reranking
methods. Furthermore, our method has been fully deployed in a popular video app
Kuaishou with over 300 million daily active users, significantly enhancing
online recommendation quality, and demonstrating the effectiveness and
efficiency of our approach.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06881" title="Abstract">arXiv:2402.06881</a> [<a href="/pdf/2402.06881" title="Download PDF">pdf</a>, <a href="/ps/2402.06881" title="Download PostScript">ps</a>, <a href="/format/2402.06881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-User SR-LDPC Codes via Coded Demixing with Applications to  Cell-Free Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ebert%2C+J+R">Jamison R. Ebert</a>, 
<a href="/search/cs?searchtype=author&query=Chamberland%2C+J">Jean-Francois Chamberland</a>, 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+K+R">Krishna R. Narayanan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ISIT 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Novel sparse regression LDPC (SR-LDPC) codes exhibit excellent performance
over additive white Gaussian noise (AWGN) channels in part due to their natural
provision of shaping gains. Though SR-LDPC-like codes have been considered
within the context of single-user error correction and massive random access,
they are yet to be examined as candidates for coordinated multi-user
communication scenarios. This article explores this gap in the literature and
demonstrates that SR-LDPC codes, when combined with coded demixing techniques,
offer a new framework for efficient non-orthogonal multiple access (NOMA) in
the context of coordinated multi-user communication channels. The ensuing
communication scheme is referred to as MU-SR-LDPC coding. Empirical evidence
suggests that, for a fixed SNR, MU-SR-LDPC coding can achieve a target bit
error rate (BER) at a higher sum rate than orthogonal multiple access (OMA)
techniques such as time division multiple access (TDMA) and frequency division
multiple access (FDMA). Importantly, MU-SR-LDPC codes enable a pragmatic
solution path for user-centric cell-free communication systems with (local)
joint decoding. Results are supported by numerical simulations.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06885" title="Abstract">arXiv:2402.06885</a> [<a href="/pdf/2402.06885" title="Download PDF">pdf</a>, <a href="/format/2402.06885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DimVis: Interpreting Visual Clusters in Dimensionality Reduction With  Explainable Boosting Machine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salmanian%2C+P">Parisa Salmanian</a>, 
<a href="/search/cs?searchtype=author&query=Chatzimparmpas%2C+A">Angelos Chatzimparmpas</a>, 
<a href="/search/cs?searchtype=author&query=Karaca%2C+A+C">Ali Can Karaca</a>, 
<a href="/search/cs?searchtype=author&query=Martins%2C+R+M">Rafael M. Martins</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This manuscript is currently under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Machine Learning (cs.LG); Computation (stat.CO)

</div>
<p class="mathjax">Dimensionality Reduction (DR) techniques such as t-SNE and UMAP are popular
for transforming complex datasets into simpler visual representations. However,
while effective in uncovering general dataset patterns, these methods may
introduce artifacts and suffer from interpretability issues. This paper
presents DimVis, a visualization tool that employs supervised Explainable
Boosting Machine (EBM) models (trained on user-selected data of interest) as an
interpretation assistant for DR projections. Our tool facilitates
high-dimensional data analysis by providing an interpretation of feature
relevance in visual clusters through interactive exploration of UMAP
projections. Specifically, DimVis uses a contrastive EBM model that is trained
in real time to differentiate between the data inside and outside a cluster of
interest. Taking advantage of the inherent explainable nature of the EBM, we
then use this model to interpret the cluster itself via single and pairwise
feature comparisons in a ranking based on the EBM model's feature importance.
The applicability and effectiveness of DimVis are demonstrated through two use
cases involving real-world datasets, and we also discuss the limitations and
potential directions for future research.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06886" title="Abstract">arXiv:2402.06886</a> [<a href="/pdf/2402.06886" title="Download PDF">pdf</a>, <a href="/format/2402.06886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Principled Penalty-based Methods for Bilevel Reinforcement Learning and  RLHF
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Han Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhuoran Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianyi Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">Bilevel optimization has been recently applied to many machine learning
tasks. However, their applications have been restricted to the supervised
learning setting, where static objective functions with benign structures are
considered. But bilevel problems such as incentive design, inverse
reinforcement learning (RL), and RL from human feedback (RLHF) are often
modeled as dynamic objective functions that go beyond the simple static
objective structures, which pose significant challenges of using existing
bilevel solutions. To tackle this new class of bilevel problems, we introduce
the first principled algorithmic framework for solving bilevel RL problems
through the lens of penalty formulation. We provide theoretical studies of the
problem landscape and its penalty-based (policy) gradient algorithms. We
demonstrate the effectiveness of our algorithms via simulations in the
Stackelberg Markov game, RL from human feedback and incentive design.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06889" title="Abstract">arXiv:2402.06889</a> [<a href="/pdf/2402.06889" title="Download PDF">pdf</a>, <a href="/format/2402.06889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive finite element approximations of the first eigenpair associated  with p-Laplacian
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+G">G. Li</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+J">J. Li</a>, 
<a href="/search/math?searchtype=author&query=Merten%2C+J">J. Merten</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+Y">Y. Xu</a>, 
<a href="/search/math?searchtype=author&query=Zhu%2C+S">S. Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we propose an adaptive finite element method for computing the
first eigenpair of the p-Laplacian problem. We prove that starting from a fine
initial mesh our proposed adaptive algorithm produces a sequence of discrete
first eigenvalues that converges to the first eigenvalue of the continuous
problem and the distance between discrete eigenfunctions and the normalized
eigenfunction set with respect to the first eigenvalue in $W^{1,p}$-norm also
tends to zero. Extensive numerical examples are provided to show the
effectiveness and efficiency.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06892" title="Abstract">arXiv:2402.06892</a> [<a href="/pdf/2402.06892" title="Download PDF">pdf</a>, <a href="/format/2402.06892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Test-Time Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kimura%2C+M">Masanari Kimura</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Test-Time Augmentation (TTA) is a very powerful heuristic that takes
advantage of data augmentation during testing to produce averaged output.
Despite the experimental effectiveness of TTA, there is insufficient discussion
of its theoretical aspects. In this paper, we aim to give theoretical
guarantees for TTA and clarify its behavior.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06894" title="Abstract">arXiv:2402.06894</a> [<a href="/pdf/2402.06894" title="Download PDF">pdf</a>, <a href="/format/2402.06894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenTranslate: Large Language Models are Generative Multilingual Speech  and Machine Translators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yuchen Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C+H">Chao-Han Huck Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Ruizhe Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhehuai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chng%2C+E+S">Eng Siong Chng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages. This work is open sourced at: <a href="https://github.com/YUCHEN005/GenTranslate">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Recent advances in large language models (LLMs) have stepped forward the
development of multilingual speech and machine translation by its reduced
representation errors and incorporated external knowledge. However, both
translation tasks typically utilize beam search decoding and top-1 hypothesis
selection for inference. These techniques struggle to fully exploit the rich
information in the diverse N-best hypotheses, making them less optimal for
translation tasks that require a single, high-quality output sequence. In this
paper, we propose a new generative paradigm for translation tasks, namely
"GenTranslate", which builds upon LLMs to generate better results from the
diverse translation versions in N-best list. Leveraging the rich linguistic
knowledge and strong reasoning abilities of LLMs, our new paradigm can
integrate the rich information in N-best candidates to generate a
higher-quality translation result. Furthermore, to support LLM finetuning, we
build and release a HypoTranslate dataset that contains over 592K
hypotheses-translation pairs in 11 languages. Experiments on various speech and
machine translation benchmarks (e.g., FLEURS, CoVoST-2, WMT) demonstrate that
our GenTranslate significantly outperforms the state-of-the-art model.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06896" title="Abstract">arXiv:2402.06896</a> [<a href="/pdf/2402.06896" title="Download PDF">pdf</a>, <a href="/format/2402.06896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implementation of Kalman Filter Approach for Active Noise Control by  Using MATLAB: Dynamic Noise Cancellation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yu%2C+G">Guo Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Asia-Pacific Signal and Information Processing Association
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Audio and Speech Processing (eess.AS); Signal Processing (eess.SP)

</div>
<p class="mathjax">This article offers an elaborate description of a Kalman filter code employed
in the active control system. Conventional active noise management methods
usually employ an adaptive filter, such as the filtered reference least mean
square (FxLMS) algorithm, to adjust to changes in the primary noise and
acoustic environment. Nevertheless, the slow convergence characteristics of the
FxLMS algorithm typically impact the effectiveness of reducing dynamic noise.
Hence, this study suggests employing the Kalman filter in the active noise
control (ANC) system to enhance the efficacy of noise reduction for dynamic
noise. The ANC application effectively utilizes the Kalman filter with a novel
dynamic ANC model. The numerical simulation revealed that the proposed Kalman
filter exhibits superior convergence performance compared to the FxLMS
algorithm for handling dynamic noise. The code is available on
\href{https://github.com/ShiDongyuan/Kalman_Filter_for_ANC.git}{GitHub} and
\href{https://www.mathworks.com/matlabcentral/fileexchange/159311-kalman-filter-for-active-noise-control}{MathWorks}.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06900" title="Abstract">arXiv:2402.06900</a> [<a href="/pdf/2402.06900" title="Download PDF">pdf</a>, <a href="/format/2402.06900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework  and Semantic-Based Metric
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koh%2C+H">Hyukhun Koh</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Dohyung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Minwoo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+K">Kyomin Jung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 page long
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the pursuit of developing Large Language Models (LLMs) that adhere to
societal standards, it is imperative to discern the existence of toxicity in
the generated text. The majority of existing toxicity metrics rely on encoder
models trained on specific toxicity datasets. However, these encoders are
susceptible to out-of-distribution (OOD) problems and depend on the definition
of toxicity assumed in a dataset. In this paper, we introduce an automatic
robust metric grounded on LLMs to distinguish whether model responses are
toxic. We start by analyzing the toxicity factors, followed by examining the
intrinsic toxic attributes of LLMs to ascertain their suitability as
evaluators. Subsequently, we evaluate our metric, LLMs As ToxiciTy Evaluators
(LATTE), on evaluation datasets.The empirical results indicate outstanding
performance in measuring toxicity, improving upon state-of-the-art metrics by
12 points in F1 score without training procedure. We also show that upstream
toxicity has an influence on downstream metrics.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06901" title="Abstract">arXiv:2402.06901</a> [<a href="/pdf/2402.06901" title="Download PDF">pdf</a>, <a href="/format/2402.06901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-perfect Coverage Manifold Estimation in Cellular Networks via  conditional GAN
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mondal%2C+W+U">Washim Uddin Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+V">Veni Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Ukkusuri%2C+S+V">Satish V. Ukkusuri</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+G">Goutam Das</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Di Wang</a>, 
<a href="/search/cs?searchtype=author&query=Alouini%2C+M">Mohamed-Slim Alouini</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+V">Vaneet Aggarwal</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Networking Letters, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">This paper presents a conditional generative adversarial network (cGAN) that
translates base station location (BSL) information of any Region-of-Interest
(RoI) to location-dependent coverage probability values within a subset of that
region, called the region-of-evaluation (RoE). We train our network utilizing
the BSL data of India, the USA, Germany, and Brazil. In comparison to the
state-of-the-art convolutional neural networks (CNNs), our model improves the
prediction error ($L_1$ difference between the coverage manifold generated by
the network under consideration and that generated via simulation) by two
orders of magnitude. Moreover, the cGAN-generated coverage manifolds appear to
be almost visually indistinguishable from the ground truth.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06903" title="Abstract">arXiv:2402.06903</a> [<a href="/pdf/2402.06903" title="Download PDF">pdf</a>, <a href="/format/2402.06903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-Performance Distributed Control for Large-Scale Linear Systems: A  Partitioned Distributed Observer Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xu%2C+H">Haotian Xu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+S">Shuai Liu</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+L">Ling Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Dynamical Systems (math.DS)

</div>
<p class="mathjax">In recent years, the distributed-observer-based distributed control law has
shown powerful ability to arbitrarily approximate the centralized control
performance. However, the traditional distributed observer requires each local
observer to reconstruct the state information of the whole system, which is
unrealistic for large-scale scenarios. To fill this gap, this paper develops a
greedy-idea-based large-scale system partition algorithm, which can
significantly reduce the dimension of local observers. Then, the partitioned
distributed observer for large-scale systems is proposed to overcome the
problem that the system dynamics are difficult to estimate due to the coupling
between partitions. Furthermore, the two-layer Lyapunov analysis method is
adopted and the dynamic transformation lemma of compact errors is proven, which
solves the problem of analyzing stability of the error dynamic of the
partitioned distributed observer. Finally, it is proved that the distributed
control law based on the partitioned distributed observer can also arbitrarily
approximate the control performance of the centralized control law, and the
dimension of the local observer is greatly reduced compared with the
traditional method. The simulation results show that when the similarity
between the physical network and the communication network is about 80%, the
local observer dimension is greatly reduced by 90% and the relative error
between the performance of the distributed control law and that of the
centralized control law is less than 1%.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06904" title="Abstract">arXiv:2402.06904</a> [<a href="/pdf/2402.06904" title="Download PDF">pdf</a>, <a href="/format/2402.06904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking Frameworks and Comparative Studies of Controller Area  Network (CAN) Intrusion Detection Systems: A Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharmin%2C+S">Shaila Sharmin</a>, 
<a href="/search/cs?searchtype=author&query=Mansor%2C+H">Hafizah Mansor</a>, 
<a href="/search/cs?searchtype=author&query=Kadir%2C+A+F+A">Andi Fitriah Abdul Kadir</a>, 
<a href="/search/cs?searchtype=author&query=Aziz%2C+N+A">Normaziah A. Aziz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review at Journal of Computer Security
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The development of intrusion detection systems (IDS) for the in-vehicle
Controller Area Network (CAN) bus is one of the main efforts being taken to
secure the in-vehicle network against various cyberattacks, which have the
potential to cause vehicles to malfunction and result in dangerous accidents.
These CAN IDS are evaluated in disparate experimental conditions that vary in
terms of the workload used, the features used, the metrics reported, etc.,
which makes direct comparison difficult. Therefore, there have been several
benchmarking frameworks and comparative studies designed to evaluate CAN IDS in
similar experimental conditions to understand their relative performance and
facilitate the selection of the best CAN IDS for implementation in automotive
networks. This work provides a comprehensive survey of CAN IDS benchmarking
frameworks and comparative studies in the current literature. A CAN IDS
evaluation design space is also proposed in this work, which draws from the
wider CAN IDS literature. This is not only expected to serve as a guide for
designing CAN IDS evaluation experiments but is also used for categorizing
current benchmarking efforts. The surveyed works have been discussed on the
basis of the five aspects in the design space-namely IDS type, attack model,
evaluation type, workload generation, and evaluation metrics-and
recommendations for future work have been identified.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06905" title="Abstract">arXiv:2402.06905</a> [<a href="/pdf/2402.06905" title="Download PDF">pdf</a>, <a href="/ps/2402.06905" title="Download PostScript">ps</a>, <a href="/format/2402.06905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A novel coarse space applying to the weighted Schwarz method for  Helmholtz equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hu%2C+Q">Qiya Hu</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+Z">Ziyi Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper we are concerned with restricted additive Schwarz with local
impedance transformation conditions for a family of Helmholtz problems in two
dimensions. These problems are discretized by the finite element method with
conforming nodal finite elements. We design and analyze a new adaptive coarse
space for this kind of restricted additive Schwarz method. This coarse space is
spanned by some eigenvalue functions of local generalized eigenvalue problems,
which are defined by weighted positive semi-definite bilinear forms on
subspaces consisting of local discrete Helmholtz-harmonic functions from
impedance boundary data. We proved that a two-level hybrid Schwarz
preconditioner with the proposed coarse space possesses uniformly convergence
independent of the mesh size, the subdomain size and the wave numbers under
suitable assumptions. We also introduce an economic coarse space to avoid
solving generalized eigenvalue problems. Numerical experiments confirm the
theoretical results.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06906" title="Abstract">arXiv:2402.06906</a> [<a href="/pdf/2402.06906" title="Download PDF">pdf</a>, <a href="/format/2402.06906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ROSE: Rotation-based Squeezing Robotic Gripper toward Universal Handling  of Objects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bui%2C+S+T">Son Tien Bui</a>, 
<a href="/search/cs?searchtype=author&query=Kawano%2C+S">Shinya Kawano</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+V+A">Van Anh Ho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 9 figures, RSS2023 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Robotics hand/grippers nowadays are not limited to manufacturing lines;
instead, they are widely utilized in cluttered environments, such as
restaurants, farms, and warehouses. In such scenarios, they need to deal with
high uncertainty of the grasped objects' shapes, postures, surfaces, and
material properties, which requires complex integration of sensing and
decision-making process. On the other hand, integrating soft materials into the
gripper's design may tolerate the above uncertainties and reduce complexity in
control. In this paper, we introduce ROSE, a novel soft gripper that can
embrace the object and squeeze it by buckling a funnel-liked thin-walled soft
membrane around the object by simple rotation of the base. Thanks to this
design, ROSE hand can adapt to a wide range of objects that can fit in the
funnel and handle with gentle gripping force. Regardless of this, ROSE can
generate a high lift force (up to 33kgf) while significantly reducing the
normal pressure on the gripped objects. In our experiment, a 198g ROSE can be
integrated into a robot arm with a single actuation and successfully lift
various types of objects, even after 400,000 trials. The embracing mechanism
helps reduce the dependence of friction between the object and the membrane, as
ROSE could pick up a chicken egg submerged inside an olive oil tank. We also
report a feasible design for equipping the ROSE hand with tactile sensing while
appealing to the scalability of the design to fit a wide range of objects.
Video: https://youtu.be/E1wAI09LaoY
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06907" title="Abstract">arXiv:2402.06907</a> [<a href="/pdf/2402.06907" title="Download PDF">pdf</a>, <a href="/ps/2402.06907" title="Download PostScript">ps</a>, <a href="/format/2402.06907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Consistency in Query-Based Meeting Summarization: A  Comparative Study of Different Embedding Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia-Chen%2C+C">Chen Jia-Chen</a> (Oscar), 
<a href="/search/cs?searchtype=author&query=Senabre%2C+G">Guillem Senabre</a>, 
<a href="/search/cs?searchtype=author&query=Caron%2C+A">Allane Caron</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">With more and more advanced data analysis techniques emerging, people will
expect these techniques to be applied in more complex tasks and solve problems
in our daily lives. Text Summarization is one of famous applications in Natural
Language Processing (NLP) field. It aims to automatically generate summary with
important information based on a given context, which is important when you
have to deal with piles of documents. Summarization techniques can help capture
key points in a short time and bring convenience in works. One of applicable
situation is meeting summarization, especially for important meeting that tend
to be long, complicated, multi-topic and multi-person. Therefore, when people
want to review specific content from a meeting, it will be hard and
time-consuming to find the related spans in the meeting transcript. However,
most of previous works focus on doing summarization for newsletters, scientific
articles...etc, which have a clear document structure and an official format.
For the documents with complex structure like transcripts, we think those works
are not quite suitable for meeting summarization. Besides, the consistency of
summary is another issue common to be discussed in NLP field. To conquer
challenges of meeting summarization, we are inspired by "QMSum: A New Benchmark
for Query-based Multi-domain Meeting Summarization" proposed by Microsoft and
we also propose our Locater model designed to extract relevant spans based on
given transcript and query, which are then summarized by Summarizer model.
Furthermore, we perform a comparative study by applying different word
embedding techniques to improve summary consistency.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06908" title="Abstract">arXiv:2402.06908</a> [<a href="/pdf/2402.06908" title="Download PDF">pdf</a>, <a href="/format/2402.06908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topological Neural Networks: Mitigating the Bottlenecks of Graph Neural  Networks via Higher-Order Interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Giusti%2C+L">Lorenzo Giusti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PhD thesis, 135 pages, 51 figures, 11 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The irreducible complexity of natural phenomena has led Graph Neural Networks
to be employed as a standard model to perform representation learning tasks on
graph-structured data. While their capacity to capture local and global
patterns is remarkable, the implications associated with long-range and
higher-order dependencies pose considerable challenges to such models. This
work starts with a theoretical framework to reveal the impact of network's
width, depth, and graph topology on the over-squashing phenomena in
message-passing neural networks. Then, the work drifts towards, higher-order
interactions and multi-relational inductive biases via Topological Neural
Networks. Such models propagate messages through higher-dimensional structures,
providing shortcuts or additional routes for information flow. With this
construction, the underlying computational graph is no longer coupled with the
input graph structure, thus mitigating the aforementioned bottlenecks while
accounting also for higher-order interactions. Inspired by Graph Attention
Networks, two topological attention networks are proposed: Simplicial and Cell
Attention Networks. The rationale behind these architecture is to leverage the
extended notion of neighbourhoods provided by the arrangement of groups of
nodes within a simplicial or cell complex to design anisotropic aggregations
able to measure the importance of the information coming from different regions
of the domain. By doing so, they capture dependencies that conventional Graph
Neural Networks might miss. Finally, a multi-way communication scheme is
introduced with Enhanced Cellular Isomorphism Networks, which augment
topological message passing schemes to enable a direct interactions among
groups of nodes arranged in ring-like structures.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06912" title="Abstract">arXiv:2402.06912</a> [<a href="/pdf/2402.06912" title="Download PDF">pdf</a>, <a href="/format/2402.06912" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving Deep Reinforcement Learning Benchmarks with Linear Policy  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wong%2C+A">Annie Wong</a>, 
<a href="/search/cs?searchtype=author&query=de+Nobel%2C+J">Jacob de Nobel</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%A4ck%2C+T">Thomas B&#xe4;ck</a>, 
<a href="/search/cs?searchtype=author&query=Plaat%2C+A">Aske Plaat</a>, 
<a href="/search/cs?searchtype=author&query=Kononova%2C+A+V">Anna V. Kononova</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Although Deep Reinforcement Learning (DRL) methods can learn effective
policies for challenging problems such as Atari games and robotics tasks,
algorithms are complex and training times are often long. This study
investigates how evolution strategies (ES) perform compared to gradient-based
deep reinforcement learning methods. We use ES to optimize the weights of a
neural network via neuroevolution, performing direct policy search. We
benchmark both regular networks and policy networks consisting of a single
linear layer from observations to actions; for three classical ES methods and
for three gradient-based methods such as PPO. Our results reveal that ES can
find effective linear policies for many RL benchmark tasks, in contrast to DRL
methods that can only find successful policies using much larger networks,
suggesting that current benchmarks are easier to solve than previously assumed.
Interestingly, also for higher complexity tasks, ES achieves results comparable
to gradient-based DRL algorithms. Furthermore, we find that by directly
accessing the memory state of the game, ES are able to find successful policies
in Atari, outperforming DQN. While gradient-based methods have dominated the
field in recent years, ES offers an alternative that is easy to implement,
parallelize, understand, and tune.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06913" title="Abstract">arXiv:2402.06913</a> [<a href="/pdf/2402.06913" title="Download PDF">pdf</a>, <a href="/format/2402.06913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TL;DR Progress: Multi-faceted Literature Exploration in Text  Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Syed%2C+S">Shahbaz Syed</a>, 
<a href="/search/cs?searchtype=author&query=Al-Khatib%2C+K">Khalid Al-Khatib</a>, 
<a href="/search/cs?searchtype=author&query=Potthast%2C+M">Martin Potthast</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL 2024 System Demonstration
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper presents TL;DR Progress, a new tool for exploring the literature
on neural text summarization. It organizes 514~papers based on a comprehensive
annotation scheme for text summarization approaches and enables fine-grained,
faceted search. Each paper was manually annotated to capture aspects such as
evaluation metrics, quality dimensions, learning paradigms, challenges
addressed, datasets, and document domains. In addition, a succinct indicative
summary is provided for each paper, consisting of automatically extracted
contextual factors, issues, and proposed solutions. The tool is available
online at https://www.tldr-progress.de, a demo video at
https://youtu.be/uCVRGFvXUj8
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06916" title="Abstract">arXiv:2402.06916</a> [<a href="/pdf/2402.06916" title="Download PDF">pdf</a>, <a href="/format/2402.06916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Free Open Source Communities Sustainability: Does It Make a Difference  in Software Quality?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alami%2C+A">Adam Alami</a>, 
<a href="/search/cs?searchtype=author&query=Pardo%2C+R">Ra&#xfa;l Pardo</a>, 
<a href="/search/cs?searchtype=author&query=Lin%C3%A5ker%2C+J">Johan Lin&#xe5;ker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Context: Free and Open Source Software (FOSS) communities' ability to stay
viable and productive over time is pivotal for society as they maintain the
building blocks that digital infrastructure, products, and services depend on.
Sustainability may, however, be characterized from multiple aspects, and less
is known how these aspects interplay and impact community outputs, and software
quality specifically.
<br />Objective: This study, therefore, aims to empirically explore how the
different aspects of FOSS sustainability impact software quality.
<br />Method: 16 sustainability metrics across four categories were sampled and
applied to a set of 217 OSS projects sourced from the Apache Software
Foundation Incubator program. The impact of a decline in the sustainability
metrics was analyzed against eight software quality metrics using Bayesian data
analysis, which incorporates probability distributions to represent the
regression coefficients and intercepts.
<br />Results: Findings suggest that selected sustainability metrics do not
significantly affect defect density or code coverage. However, a positive
impact of community age was observed on specific code quality metrics, such as
risk complexity, number of very large files, and code duplication percentage.
Interestingly, findings show that even when communities are experiencing
sustainability, certain code quality metrics are negatively impacted.
<br />Conclusion: Findings imply that code quality practices are not consistently
linked to sustainability, and defect management and prevention may be
prioritized over the former. Results suggest that growth, resulting in a more
complex and large codebase, combined with a probable lack of understanding of
code quality standards, may explain the degradation in certain aspects of code
quality.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06918" title="Abstract">arXiv:2402.06918</a> [<a href="/pdf/2402.06918" title="Download PDF">pdf</a>, <a href="/format/2402.06918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach  to Searching for the Most Promising Intermediate Thought
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhen-Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Siwei Han</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+H">Huaxiu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+G">Gang Niu</a>, 
<a href="/search/cs?searchtype=author&query=Sugiyama%2C+M">Masashi Sugiyama</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">To improve the ability of the large language model (LLMs) to handle complex
reasoning problems, chain-of-thoughts (CoT) methods were proposed to guide LLMs
to reason step-by-step, facilitating problem solving from simple to complex
tasks. State-of-the-art approaches for generating such a chain involve
interactive collaboration, where the learner generates candidate intermediate
thoughts, evaluated by the LLM, guiding the generation of subsequent thoughts.
However, a widespread yet understudied problem is that the evaluation from the
LLM is typically noisy and unreliable, potentially misleading the generation
process in selecting promising intermediate thoughts. In this paper, motivated
by Vapnik's principle, we propose a novel comparison-based CoT generation
algorithm that directly identifies the most promising thoughts with the noisy
feedback from the LLM. In each round, we randomly pair intermediate thoughts
and directly prompt the LLM to select the more promising one from each pair,
allowing us to identify the most promising thoughts through an iterative
process. To further model the noise in the comparison, we resort to the
techniques of ensemble and dueling bandits and propose two variants of the
proposed algorithm. Experiments on three real-world mathematical and reasoning
tasks demonstrate the effectiveness of our proposed algorithm and verify the
rationale of the direct pairwise comparison.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06919" title="Abstract">arXiv:2402.06919</a> [<a href="/pdf/2402.06919" title="Download PDF">pdf</a>, <a href="/format/2402.06919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TREET: TRansfer Entropy Estimation via Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luxembourg%2C+O">Omer Luxembourg</a>, 
<a href="/search/cs?searchtype=author&query=Tsur%2C+D">Dor Tsur</a>, 
<a href="/search/cs?searchtype=author&query=Permuter%2C+H">Haim Permuter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Transfer entropy (TE) is a measurement in information theory that reveals the
directional flow of information between processes, providing valuable insights
for a wide range of real-world applications. This work proposes Transfer
Entropy Estimation via Transformers (TREET), a novel transformer-based approach
for estimating the TE for stationary processes. The proposed approach employs
Donsker-Vardhan (DV) representation to TE and leverages the attention mechanism
for the task of neural estimation. We propose a detailed theoretical and
empirical study of the TREET, comparing it to existing methods. To increase its
applicability, we design an estimated TE optimization scheme that is motivated
by the functional representation lemma. Afterwards, we take advantage of the
joint optimization scheme to optimize the capacity of communication channels
with memory, which is a canonical optimization problem in information theory,
and show the memory capabilities of our estimator. Finally, we apply TREET to
real-world feature analysis. Our work, applied with state-of-the-art deep
learning methods, opens a new door for communication problems which are yet to
be solved.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06921" title="Abstract">arXiv:2402.06921</a> [<a href="/pdf/2402.06921" title="Download PDF">pdf</a>, <a href="/ps/2402.06921" title="Download PostScript">ps</a>, <a href="/format/2402.06921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clustering Techniques Selection for a Hybrid Regression Model: A Case  Study Based on a Solar Thermal System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Ord%C3%A1s%2C+M+T">Mar&#xed;a Teresa Garc&#xed;a-Ord&#xe1;s</a>, 
<a href="/search/cs?searchtype=author&query=Alaiz-Moret%C3%B3n%2C+H">H&#xe9;ctor Alaiz-Moret&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Casteleiro-Roca%2C+J">Jos&#xe9;-Luis Casteleiro-Roca</a>, 
<a href="/search/cs?searchtype=author&query=Jove%2C+E">Esteban Jove</a>, 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Rodr%C3%ADguez%2C+I">Isa&#xed;as Garc&#xed;a-Rodr&#xed;guez</a>, 
<a href="/search/cs?searchtype=author&query=Quinti%C3%A1n%2C+H">H&#xe9;ctor Quinti&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=Calvo-Rolle%2C+J+L">Jos&#xe9; Luis Calvo-Rolle</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Cybernetics and Systems, Volume 54, Issue 3, pages 286-305, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Multiagent Systems (cs.MA)

</div>
<p class="mathjax">This work addresses the performance comparison between four clustering
techniques with the objective of achieving strong hybrid models in supervised
learning tasks. A real dataset from a bio-climatic house named Sotavento placed
on experimental wind farm and located in Xermade (Lugo) in Galicia (Spain) has
been collected. Authors have chosen the thermal solar generation system in
order to study how works applying several cluster methods followed by a
regression technique to predict the output temperature of the system. With the
objective of defining the quality of each clustering method two possible
solutions have been implemented. The first one is based on three unsupervised
learning metrics (Silhouette, Calinski-Harabasz and Davies-Bouldin) while the
second one, employs the most common error measurements for a regression
algorithm such as Multi Layer Perceptron.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06922" title="Abstract">arXiv:2402.06922</a> [<a href="/pdf/2402.06922" title="Download PDF">pdf</a>, <a href="/format/2402.06922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Whispers in the Machine: Confidentiality in LLM-integrated Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Evertz%2C+J">Jonathan Evertz</a>, 
<a href="/search/cs?searchtype=author&query=Chlosta%2C+M">Merlin Chlosta</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6nherr%2C+L">Lea Sch&#xf6;nherr</a>, 
<a href="/search/cs?searchtype=author&query=Eisenhofer%2C+T">Thorsten Eisenhofer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) are increasingly integrated with external tools.
While these integrations can significantly improve the functionality of LLMs,
they also create a new attack surface where confidential data may be disclosed
between different components. Specifically, malicious tools can exploit
vulnerabilities in the LLM itself to manipulate the model and compromise the
data of other services, raising the question of how private data can be
protected in the context of LLM integrations.
<br />In this work, we provide a systematic way of evaluating confidentiality in
LLM-integrated systems. For this, we formalize a "secret key" game that can
capture the ability of a model to conceal private information. This enables us
to compare the vulnerability of a model against confidentiality attacks and
also the effectiveness of different defense strategies. In this framework, we
evaluate eight previously published attacks and four defenses. We find that
current defenses lack generalization across attack strategies. Building on this
analysis, we propose a method for robustness fine-tuning, inspired by
adversarial training. This approach is effective in lowering the success rate
of attackers and in improving the system's resilience against unknown attacks.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06925" title="Abstract">arXiv:2402.06925</a> [<a href="/pdf/2402.06925" title="Download PDF">pdf</a>, <a href="/format/2402.06925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Thorough Examination of Decoding Methods in the Era of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chufan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haoran Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+D">Deng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhisong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yujiu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+W">Wai Lam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Decoding methods play an indispensable role in converting language models
from next-token predictors into practical task solvers. Prior research on
decoding methods, primarily focusing on task-specific models, may not extend to
the current era of general-purpose large language models (LLMs). Moreover, the
recent influx of decoding strategies has further complicated this landscape.
This paper provides a comprehensive and multifaceted analysis of various
decoding methods within the context of LLMs, evaluating their performance,
robustness to hyperparameter changes, and decoding speeds across a wide range
of tasks, models, and deployment environments. Our findings reveal that
decoding method performance is notably task-dependent and influenced by factors
such as alignment, model size, and quantization. Intriguingly, sensitivity
analysis exposes that certain methods achieve superior performance at the cost
of extensive hyperparameter tuning, highlighting the trade-off between
attaining optimal results and the practicality of implementation in varying
contexts.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06927" title="Abstract">arXiv:2402.06927</a> [<a href="/pdf/2402.06927" title="Download PDF">pdf</a>, <a href="/format/2402.06927" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data assimilation for the stochastic Camassa-Holm equation using  particle filtering: a numerical investigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cotter%2C+C+J">Colin John Cotter</a>, 
<a href="/search/math?searchtype=author&query=Crisan%2C+D">Dan Crisan</a>, 
<a href="/search/math?searchtype=author&query=Singh%2C+M+K">Maneesh Kumar Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted for STUOD 2024 volume
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this study, we explore data assimilation for the Stochastic Camassa-Holm
equation through the application of the particle filtering framework.
Specifically, our approach integrates adaptive tempering, jittering, and
nudging techniques to construct an advanced particle filtering system. All
filtering processes are executed utilizing ensemble parallelism. We conduct
extensive numerical experiments across various scenarios of the Stochastic
Camassa-Holm model with transport noise and viscosity to examine the impact of
different filtering procedures on the performance of the data assimilation
process. Our analysis focuses on how observational data and the data
assimilation step influence the accuracy and uncertainty of the obtained
results.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06929" title="Abstract">arXiv:2402.06929</a> [<a href="/pdf/2402.06929" title="Download PDF">pdf</a>, <a href="/format/2402.06929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making a prototype of Seoul historical sites chatbot using Langchain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suh%2C+J+Y">Jae Young Suh</a>, 
<a href="/search/cs?searchtype=author&query=Kwak%2C+M">Minsoo Kwak</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S+Y">Soo Yong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+H">Hyoungseo Cho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 4 figures, draft
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">In this paper, we are going to share a draft of the development of a
conversational agent created to disseminate information about historical sites
located in the Seoul. The primary objective of the agent is to increase
awareness among visitors who are not familiar with Seoul, about the presence
and precise locations of valuable cultural heritage sites. It aims to promote a
basic understanding of Korea's rich and diverse cultural history. The agent is
thoughtfully designed for accessibility in English and utilizes data generously
provided by the Seoul Metropolitan Government. Despite the limited data volume,
it consistently delivers reliable and accurate responses, seamlessly aligning
with the available information. We have meticulously detailed the methodologies
employed in creating this agent and provided a comprehensive overview of its
underlying structure within the paper. Additionally, we delve into potential
improvements to enhance this initial version of the system, with a primary
emphasis on expanding the available data through our prompting. In conclusion,
we provide an in-depth discussion of our expectations regarding the future
impact of this agent in promoting and facilitating the sharing of historical
sites.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06930" title="Abstract">arXiv:2402.06930</a> [<a href="/pdf/2402.06930" title="Download PDF">pdf</a>, <a href="/format/2402.06930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LiFi: Lightweight Controlled Text Generation with Fine-Grained Control  Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chufan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+D">Deng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yujiu Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In the rapidly evolving field of text generation, the demand for more precise
control mechanisms has become increasingly apparent. To address this need, we
present a novel methodology, LIFI, which offers a lightweight approach with
fine-grained control for controlled text generation. Unlike previous studies
that train pre-trained language models to follow discrete, categorical, and
exclusive control codes, LIFI learns controlled text generation under the
guidance of continuous, relative, and nonexclusive control codes. These
fine-grained codes are automatically derived from an attribute classifier,
initially trained with a small amount of labeled data and subsequently employed
to label abundant unlabeled data, thus garnering more extensive supervision
signals. Moreover, to achieve efficient control, we incorporate the
fine-grained control codes with adapters, a parameter- and compute-efficient
way to steer a pre-trained language model. We evaluate LIFI on two conventional
tasks -- sentiment control and topic control -- and one newly proposed task --
stylistic novel writing. Comprehensive experimental results validate the
effectiveness of our proposed methods, demonstrating substantial performance
improvements over existing baselines.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06931" title="Abstract">arXiv:2402.06931</a> [<a href="/pdf/2402.06931" title="Download PDF">pdf</a>, <a href="/format/2402.06931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ORIENT: A Priority-Aware Energy-Efficient Approach for Latency-Sensitive  Applications in 6G
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shokrnezhad%2C+M">Masoud Shokrnezhad</a>, 
<a href="/search/cs?searchtype=author&query=Taleb%2C+T">Tarik Taleb</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Conference, 6 pages, 2 figures, 28 equations, 1 table, 1 algorithm, and 16 references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Anticipation for 6G's arrival comes with growing concerns about increased
energy consumption in computing and networking. The expected surge in connected
devices and resource-demanding applications presents unprecedented challenges
for energy resources. While sustainable resource allocation strategies have
been discussed in the past, these efforts have primarily focused on
single-domain orchestration or ignored the unique requirements posed by 6G. To
address this gap, we investigate the joint problem of service instance
placement and assignment, path selection, and request prioritization, dubbed
PIRA. The objective function is to maximize the system's overall profit as a
function of the number of concurrently supported requests while simultaneously
minimizing energy consumption over an extended period of time. In addition,
end-to-end latency requirements and resource capacity constraints are
considered for computing and networking resources, where queuing theory is
utilized to estimate the Age of Information (AoI) for requests. After
formulating the problem in a non-linear fashion, we prove its NP-hardness and
propose a method, denoted ORIENT. This method is based on the Double Dueling
Deep Q-Learning (D3QL) mechanism and leverages Graph Neural Networks (GNNs) for
state encoding. Extensive numerical simulations demonstrate that ORIENT yields
near-optimal solutions for varying system sizes and request counts.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06932" title="Abstract">arXiv:2402.06932</a> [<a href="/pdf/2402.06932" title="Download PDF">pdf</a>, <a href="/format/2402.06932" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Attributed Graphlets: Predictive Graph Mining by Graphlets with  Trainable Attribute
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shinji%2C+T">Tajima Shinji</a>, 
<a href="/search/cs?searchtype=author&query=Sugihara%2C+R">Ren Sugihara</a>, 
<a href="/search/cs?searchtype=author&query=Kitahara%2C+R">Ryota Kitahara</a>, 
<a href="/search/cs?searchtype=author&query=Karasuyama%2C+M">Masayuki Karasuyama</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The graph classification problem has been widely studied; however, achieving
an interpretable model with high predictive performance remains a challenging
issue. This paper proposes an interpretable classification algorithm for
attributed graph data, called LAGRA (Learning Attributed GRAphlets). LAGRA
learns importance weights for small attributed subgraphs, called attributed
graphlets (AGs), while simultaneously optimizing their attribute vectors. This
enables us to obtain a combination of subgraph structures and their attribute
vectors that strongly contribute to discriminating different classes. A
significant characteristics of LAGRA is that all the subgraph structures in the
training dataset can be considered as a candidate structures of AGs. This
approach can explore all the potentially important subgraphs exhaustively, but
obviously, a naive implementation can require a large amount of computations.
To mitigate this issue, we propose an efficient pruning strategy by combining
the proximal gradient descent and a graph mining tree search. Our pruning
strategy can ensure that the quality of the solution is maintained compared to
the result without pruning. We empirically demonstrate that LAGRA has superior
or comparable prediction performance to the standard existing algorithms
including graph neural networks, while using only a small number of AGs in an
interpretable manner.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06935" title="Abstract">arXiv:2402.06935</a> [<a href="/pdf/2402.06935" title="Download PDF">pdf</a>, <a href="/format/2402.06935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Taxonomic classification with maximal exact matches in KATKA kernels and  minimizer digests
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Draesslerov%C3%A1%2C+D">Dominika Draesslerov&#xe1;</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+O">Omar Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Gagie%2C+T">Travis Gagie</a>, 
<a href="/search/cs?searchtype=author&query=Holub%2C+J">Jan Holub</a>, 
<a href="/search/cs?searchtype=author&query=Langmead%2C+B">Ben Langmead</a>, 
<a href="/search/cs?searchtype=author&query=Manzini%2C+G">Giovanni Manzini</a>, 
<a href="/search/cs?searchtype=author&query=Navarro%2C+G">Gonzalo Navarro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Genomics (q-bio.GN); Populations and Evolution (q-bio.PE)

</div>
<p class="mathjax">For taxonomic classification, we are asked to index the genomes in a
phylogenetic tree such that later, given a DNA read, we can quickly choose a
small subtree likely to contain the genome from which that read was drawn.
Although popular classifiers such as Kraken use $k$-mers, recent research
indicates that using maximal exact matches (MEMs) can lead to better
classifications. For example, we can build an augmented FM-index over the the
genomes in the tree concatenated in left-to-right order; for each MEM in a
read, find the interval in the suffix array containing the starting positions
of that MEM's occurrences in those genomes; find the minimum and maximum values
stored in that interval; take the lowest common ancestor (LCA) of the genomes
containing the characters at those positions. This solution is practical,
however, only when the total size of the genomes in the tree is fairly small.
In this paper we consider applying the same solution to three lossily
compressed representations of the genomes' concatenation: a KATKA kernel, which
discards characters that are not in the first or last occurrence of any
$k_{\max}$-tuple, for a parameter $k_{\max}$; a minimizer digest; a KATKA
kernel of a minimizer digest. With a test dataset and these three
representations of it, simulated reads and various parameter settings, we
checked how many reads' longest MEMs occurred only in the sequences from which
those reads were generated (``true positive'' reads). For some parameter
settings we achieved significant compression while only slightly decreasing the
true-positive rate.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06936" title="Abstract">arXiv:2402.06936</a> [<a href="/pdf/2402.06936" title="Download PDF">pdf</a>, <a href="/format/2402.06936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent Enhancing AutoEncoder for Occluded Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kotwal%2C+K">Ketan Kotwal</a>, 
<a href="/search/cs?searchtype=author&query=Deshmukh%2C+T">Tanay Deshmukh</a>, 
<a href="/search/cs?searchtype=author&query=Gopal%2C+P">Preeti Gopal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Large occlusions result in a significant decline in image classification
accuracy. During inference, diverse types of unseen occlusions introduce
out-of-distribution data to the classification model, leading to accuracy
dropping as low as 50%. As occlusions encompass spatially connected regions,
conventional methods involving feature reconstruction are inadequate for
enhancing classification performance. We introduce LEARN: Latent Enhancing
feAture Reconstruction Network -- An auto-encoder based network that can be
incorporated into the classification model before its classifier head without
modifying the weights of classification model. In addition to reconstruction
and classification losses, training of LEARN effectively combines intra- and
inter-class losses calculated over its latent space -- which lead to
improvement in recovering latent space of occluded data, while preserving its
class-specific discriminative information. On the OccludedPASCAL3D+ dataset,
the proposed LEARN outperforms standard classification models (VGG16 and
ResNet-50) by a large margin and up to 2% over state-of-the-art methods. In
cross-dataset testing, our method improves the average classification accuracy
by more than 5% over the state-of-the-art methods. In every experiment, our
model consistently maintains excellent accuracy on in-distribution data.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06937" title="Abstract">arXiv:2402.06937</a> [<a href="/pdf/2402.06937" title="Download PDF">pdf</a>, <a href="/format/2402.06937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing Uncertainty Estimation Methods for 3D Image Segmentation under  Distribution Shifts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Javanbakhat%2C+M">Masoumeh Javanbakhat</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+M+T">Md Tasnimul Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Lippert%2C+C">Cristoph Lippert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In recent years, machine learning has witnessed extensive adoption across
various sectors, yet its application in medical image-based disease detection
and diagnosis remains challenging due to distribution shifts in real-world
data. In practical settings, deployed models encounter samples that differ
significantly from the training dataset, especially in the health domain,
leading to potential performance issues. This limitation hinders the
expressiveness and reliability of deep learning models in health applications.
Thus, it becomes crucial to identify methods capable of producing reliable
uncertainty estimation in the context of distribution shifts in the health
sector. In this paper, we explore the feasibility of using cutting-edge
Bayesian and non-Bayesian methods to detect distributionally shifted samples,
aiming to achieve reliable and trustworthy diagnostic predictions in
segmentation task. Specifically, we compare three distinct uncertainty
estimation methods, each designed to capture either unimodal or multimodal
aspects in the posterior distribution. Our findings demonstrate that methods
capable of addressing multimodal characteristics in the posterior distribution,
offer more dependable uncertainty estimates. This research contributes to
enhancing the utility of deep learning in healthcare, making diagnostic
predictions more robust and trustworthy.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06938" title="Abstract">arXiv:2402.06938</a> [<a href="/pdf/2402.06938" title="Download PDF">pdf</a>, <a href="/format/2402.06938" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Resource Scheduling for Distributed Infrastructures Using  Negotiation Capabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chu%2C+J">Junjie Chu</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+P">Prashant Singh</a>, 
<a href="/search/cs?searchtype=author&query=Toor%2C+S">Salman Toor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IEEE CLOUD 2023. 13 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In the past few decades, the rapid development of information and internet
technologies has spawned massive amounts of data and information. The
information explosion drives many enterprises or individuals to seek to rent
cloud computing infrastructure to put their applications in the cloud. However,
the agreements reached between cloud computing providers and clients are often
not efficient. Many factors affect the efficiency, such as the idleness of the
providers' cloud computing infrastructure, and the additional cost to the
clients. One possible solution is to introduce a comprehensive, bargaining game
(a type of negotiation), and schedule resources according to the negotiation
results. We propose an agent-based auto-negotiation system for resource
scheduling based on fuzzy logic. The proposed method can complete a one-to-one
auto-negotiation process and generate optimal offers for the provider and
client. We compare the impact of different member functions, fuzzy rule sets,
and negotiation scenario cases on the offers to optimize the system. It can be
concluded that our proposed method can utilize resources more efficiently and
is interpretable, highly flexible, and customizable. We successfully train
machine learning models to replace the fuzzy negotiation system to improve
processing speed. The article also highlights possible future improvements to
the proposed system and machine learning models. All the codes and data are
available in the open-source repository.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06941" title="Abstract">arXiv:2402.06941</a> [<a href="/pdf/2402.06941" title="Download PDF">pdf</a>, <a href="/format/2402.06941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Achieving Low Latency at Low Outage: Multilevel Coding for mmWave  Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dogan%2C+M+G">Mine Gokce Dogan</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+J">Jaimin Shah</a>, 
<a href="/search/cs?searchtype=author&query=Cardone%2C+M">Martina Cardone</a>, 
<a href="/search/cs?searchtype=author&query=Fragouli%2C+C">Christina Fragouli</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+W">Wei Mao</a>, 
<a href="/search/cs?searchtype=author&query=Nikopour%2C+H">Hosein Nikopour</a>, 
<a href="/search/cs?searchtype=author&query=Vannithamby%2C+R">Rath Vannithamby</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Millimeter-wave (mmWave) spectrum is expected to support data-intensive
applications that require ultra-reliable low-latency communications (URLLC).
However, mmWave links are highly sensitive to blockage, which may lead to
disruptions in the communication. Traditional techniques that build resilience
against such blockages (among which are interleaving and feedback mechanisms)
incur delays that are too large to effectively support URLLC. This calls for
novel techniques that ensure resilient URLLC. In this paper, we propose to
deploy multilevel codes over space and over time. These codes offer several
benefits, such as they allow to control what information is received and they
provide different reliability guarantees for different information streams
based on their priority. We also show that deploying these codes leads to
attractive trade-offs between rate, delay, and outage probability. A
practically-relevant aspect of the proposed technique is that it offers
resilience while incurring a low operational complexity.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06942" title="Abstract">arXiv:2402.06942</a> [<a href="/pdf/2402.06942" title="Download PDF">pdf</a>, <a href="/format/2402.06942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Scalable Generative AI via Mixture of Experts in Mobile Edge  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiacheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+H">Hongyang Du</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jiawen Kang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zehui Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D+I">Dong In Kim</a>, 
<a href="/search/cs?searchtype=author&query=Letaief%2C+K+B">Khaled B. Letaief</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The advancement of generative artificial intelligence (GAI) has driven
revolutionary applications like ChatGPT. The widespread of these applications
relies on the mixture of experts (MoE), which contains multiple experts and
selectively engages them for each task to lower operation costs while
maintaining performance. Despite MoE, GAI faces challenges in resource
consumption when deployed on user devices. This paper proposes mobile edge
networks supported MoE-based GAI. We first review the MoE from traditional AI
and GAI perspectives, including structure, principles, and applications. We
then propose a framework that transfers subtasks to devices in mobile edge
networks, aiding GAI model operation on user devices. We discuss challenges in
this process and introduce a deep reinforcement learning based algorithm to
select edge devices for subtask execution. Experimental results will show that
our framework not only facilitates GAI's deployment on resource-limited devices
but also generates higher-quality content compared to methods without edge
network support.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06943" title="Abstract">arXiv:2402.06943</a> [<a href="/pdf/2402.06943" title="Download PDF">pdf</a>, <a href="/format/2402.06943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical Solution of Nonclassical Boundary Value Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Boito%2C+P">Paola Boito</a>, 
<a href="/search/math?searchtype=author&query=Eidelman%2C+Y">Yuli Eidelman</a>, 
<a href="/search/math?searchtype=author&query=Gemignani%2C+L">Luca Gemignani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We provide a new approach to obtain solutions of certain evolution equations
set in a Banach space and equipped with nonlocal boundary conditions. From this
approach we derive a family of numerical schemes for the approximation of the
solutions. We show by numerical tests that these schemes are numerically robust
and computationally efficient.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06945" title="Abstract">arXiv:2402.06945</a> [<a href="/pdf/2402.06945" title="Download PDF">pdf</a>, <a href="/format/2402.06945" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluation Metrics for Automated Typographic Poster Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rebelo%2C+S+M">S&#xe9;rgio M. Rebelo</a>, 
<a href="/search/cs?searchtype=author&query=Merelo%2C+J+J">J. J. Merelo</a>, 
<a href="/search/cs?searchtype=author&query=Bicker%2C+J">Jo&#xe3;o Bicker</a>, 
<a href="/search/cs?searchtype=author&query=Machado%2C+P">Penousal Machado</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted be presented in the 13th International Conference Artificial Intelligence in Music, Sound, Art and Design -- EvoMUSART 2024, Held as Part of EvoStar 2024, Aberystwyth, Wales, United Kingdom, April 3\textendash{}5, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Computational Design approaches facilitate the generation of typographic
design, but evaluating these designs remains a challenging task. In this paper,
we propose a set of heuristic metrics for typographic design evaluation,
focusing on their legibility, which assesses the text visibility, aesthetics,
which evaluates the visual quality of the design, and semantic features, which
estimate how effectively the design conveys the content semantics. We
experiment with a constrained evolutionary approach for generating typographic
posters, incorporating the proposed evaluation metrics with varied setups, and
treating the legibility metrics as constraints. We also integrate emotion
recognition to identify text semantics automatically and analyse the
performance of the approach and the visual characteristics outputs.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06948" title="Abstract">arXiv:2402.06948</a> [<a href="/pdf/2402.06948" title="Download PDF">pdf</a>, <a href="/format/2402.06948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Should I try multiple optimizers when fine-tuning pre-trained  Transformers for NLP tasks? Should I tune their hyperparameters?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gkouti%2C+N">Nefeli Gkouti</a>, 
<a href="/search/cs?searchtype=author&query=Malakasiotis%2C+P">Prodromos Malakasiotis</a>, 
<a href="/search/cs?searchtype=author&query=Toumpis%2C+S">Stavros Toumpis</a>, 
<a href="/search/cs?searchtype=author&query=Androutsopoulos%2C+I">Ion Androutsopoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">NLP research has explored different neural model architectures and sizes,
datasets, training objectives, and transfer learning techniques. However, the
choice of optimizer during training has not been explored as extensively.
Typically, some variant of Stochastic Gradient Descent (SGD) is employed,
selected among numerous variants, using unclear criteria, often with minimal or
no tuning of the optimizer's hyperparameters. Experimenting with five GLUE
datasets, two models (DistilBERT and DistilRoBERTa), and seven popular
optimizers (SGD, SGD with Momentum, Adam, AdaMax, Nadam, AdamW, and AdaBound),
we find that when the hyperparameters of the optimizers are tuned, there is no
substantial difference in test performance across the five more elaborate
(adaptive) optimizers, despite differences in training loss. Furthermore,
tuning just the learning rate is in most cases as good as tuning all the
hyperparameters. Hence, we recommend picking any of the best-behaved adaptive
optimizers (e.g., Adam) and tuning only its learning rate. When no
hyperparameter can be tuned, SGD with Momentum is the best choice.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06951" title="Abstract">arXiv:2402.06951</a> [<a href="/pdf/2402.06951" title="Download PDF">pdf</a>, <a href="/format/2402.06951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Object-level Modeling for Robust Visual Camera Relocalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yifan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+L">Lingjuan Miao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haitao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhiqiang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weiyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Longwen Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Visual relocalization is crucial for autonomous visual localization and
navigation of mobile robotics. Due to the improvement of CNN-based object
detection algorithm, the robustness of visual relocalization is greatly
enhanced especially in viewpoints where classical methods fail. However,
ellipsoids (quadrics) generated by axis-aligned object detection may limit the
accuracy of the object-level representation and degenerate the performance of
visual relocalization system. In this paper, we propose a novel method of
automatic object-level voxel modeling for accurate ellipsoidal representations
of objects. As for visual relocalization, we design a better pose optimization
strategy for camera pose recovery, to fully utilize the projection
characteristics of 2D fitted ellipses and the 3D accurate ellipsoids. All of
these modules are entirely intergrated into visual SLAM system. Experimental
results show that our semantic object-level mapping and object-based visual
relocalization methods significantly enhance the performance of visual
relocalization in terms of robustness to new viewpoints.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06954" title="Abstract">arXiv:2402.06954</a> [<a href="/pdf/2402.06954" title="Download PDF">pdf</a>, <a href="/format/2402.06954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenFedLLM: Training Large Language Models on Decentralized Private Data  via Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+R">Rui Ye</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+J">Jingyi Chai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dihan Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zexi Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yinda Xu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yaxin Du</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Siheng Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 3 figures, 16 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Distributed, Parallel, and Cluster Computing (cs.DC); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Trained on massive publicly available data, large language models (LLMs) have
demonstrated tremendous success across various fields. While more data
contributes to better performance, a disconcerting reality is that high-quality
public data will be exhausted in a few years. In this paper, we offer a
potential next step for contemporary LLMs: collaborative and privacy-preserving
LLM training on the underutilized distributed private data via federated
learning (FL), where multiple data owners collaboratively train a shared model
without transmitting raw data. To achieve this, we build a concise, integrated,
and research-friendly framework/codebase, named OpenFedLLM. It covers federated
instruction tuning for enhancing instruction-following capability, federated
value alignment for aligning with human values, and 7 representative FL
algorithms. Besides, OpenFedLLM supports training on diverse domains, where we
cover 8 training datasets; and provides comprehensive evaluations, where we
cover 30+ evaluation metrics. Through extensive experiments, we observe that
all FL algorithms outperform local training on training LLMs, demonstrating a
clear performance improvement across a variety of settings. Notably, in a
financial benchmark, Llama2-7B fine-tuned by applying any FL algorithm can
outperform GPT-4 by a significant margin while the model obtained through
individual training cannot, demonstrating strong motivation for clients to
participate in FL. The code is available at
https://github.com/rui-ye/OpenFedLLM.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06955" title="Abstract">arXiv:2402.06955</a> [<a href="/pdf/2402.06955" title="Download PDF">pdf</a>, <a href="/format/2402.06955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training dynamics in Physics-Informed Neural Networks with feature  mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+C">Chengxi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Burghardt%2C+T">Tilo Burghardt</a>, 
<a href="/search/cs?searchtype=author&query=Gambaruto%2C+A+M">Alberto M Gambaruto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Physics-Informed Neural Networks (PINNs) have emerged as an iconic machine
learning approach for solving Partial Differential Equations (PDEs). Although
its variants have achieved significant progress, the empirical success of
utilising feature mapping from the wider Implicit Neural Representations
studies has been substantially neglected. We investigate the training dynamics
of PINNs with a feature mapping layer via the limiting Conjugate Kernel and
Neural Tangent Kernel, which sheds light on the convergence and generalisation
of the model. We also show the inadequacy of commonly used Fourier-based
feature mapping in some scenarios and propose the conditional positive definite
Radial Basis Function as a better alternative. The empirical results reveal the
efficacy of our method in diverse forward and inverse problem sets. This simple
technique can be easily implemented in coordinate input networks and benefits
the broad PINNs research.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06957" title="Abstract">arXiv:2402.06957</a> [<a href="/pdf/2402.06957" title="Download PDF">pdf</a>, <a href="/format/2402.06957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Architectural Neural Backdoors from First Principles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Langford%2C+H">Harry Langford</a>, 
<a href="/search/cs?searchtype=author&query=Shumailov%2C+I">Ilia Shumailov</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yiren Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Mullins%2C+R">Robert Mullins</a>, 
<a href="/search/cs?searchtype=author&query=Papernot%2C+N">Nicolas Papernot</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">While previous research backdoored neural networks by changing their
parameters, recent work uncovered a more insidious threat: backdoors embedded
within the definition of the network's architecture. This involves injecting
common architectural components, such as activation functions and pooling
layers, to subtly introduce a backdoor behavior that persists even after (full
re-)training. However, the full scope and implications of architectural
backdoors have remained largely unexplored. Bober-Irizar et al. [2023]
introduced the first architectural backdoor; they showed how to create a
backdoor for a checkerboard pattern, but never explained how to target an
arbitrary trigger pattern of choice. In this work we construct an arbitrary
trigger detector which can be used to backdoor an architecture with no human
supervision. This leads us to revisit the concept of architecture backdoors and
taxonomise them, describing 12 distinct types. To gauge the difficulty of
detecting such backdoors, we conducted a user study, revealing that ML
developers can only identify suspicious components in common model definitions
as backdoors in 37% of cases, while they surprisingly preferred backdoored
models in 33% of cases. To contextualize these results, we find that language
models outperform humans at the detection of backdoors. Finally, we discuss
defenses against architectural backdoors, emphasizing the need for robust and
comprehensive strategies to safeguard the integrity of ML systems.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06959" title="Abstract">arXiv:2402.06959</a> [<a href="/pdf/2402.06959" title="Download PDF">pdf</a>, <a href="/format/2402.06959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpeechCLIP+: Self-supervised multi-task representation learning for  speech via CLIP and speech-image data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hsuan-Fu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shih%2C+Y">Yi-Jen Shih</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+H">Heng-Jui Chang</a>, 
<a href="/search/cs?searchtype=author&query=Berry%2C+L">Layne Berry</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+P">Puyuan Peng</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hung-yi Lee</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hsin-Min Wang</a>, 
<a href="/search/cs?searchtype=author&query=Harwath%2C+D">David Harwath</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICASSP 2024, Self-supervision in Audio, Speech, and Beyond (SASB) workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">The recently proposed visually grounded speech model SpeechCLIP is an
innovative framework that bridges speech and text through images via CLIP
without relying on text transcription. On this basis, this paper introduces two
extensions to SpeechCLIP. First, we apply the Continuous Integrate-and-Fire
(CIF) module to replace a fixed number of CLS tokens in the cascaded
architecture. Second, we propose a new hybrid architecture that merges the
cascaded and parallel architectures of SpeechCLIP into a multi-task learning
framework. Our experimental evaluation is performed on the Flickr8k and
SpokenCOCO datasets. The results show that in the speech keyword extraction
task, the CIF-based cascaded SpeechCLIP model outperforms the previous cascaded
SpeechCLIP model using a fixed number of CLS tokens. Furthermore, through our
hybrid architecture, cascaded task learning boosts the performance of the
parallel branch in image-speech retrieval tasks.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06963" title="Abstract">arXiv:2402.06963</a> [<a href="/pdf/2402.06963" title="Download PDF">pdf</a>, <a href="/format/2402.06963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tree Ensembles for Contextual Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nilsson%2C+H">Hannes Nilsson</a>, 
<a href="/search/cs?searchtype=author&query=Johansson%2C+R">Rikard Johansson</a>, 
<a href="/search/cs?searchtype=author&query=%C3%85kerblom%2C+N">Niklas &#xc5;kerblom</a>, 
<a href="/search/cs?searchtype=author&query=Chehreghani%2C+M+H">Morteza Haghir Chehreghani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally to this work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">We propose a novel framework for contextual multi-armed bandits based on tree
ensembles. Our framework integrates two widely used bandit methods, Upper
Confidence Bound and Thompson Sampling, for both standard and combinatorial
settings. We demonstrate the effectiveness of our framework via several
experimental studies, employing XGBoost, a popular tree ensemble method.
Compared to state-of-the-art methods based on neural networks, our methods
exhibit superior performance in terms of both regret minimization and
computational runtime, when applied to benchmark datasets and the real-world
application of navigation over road networks.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06964" title="Abstract">arXiv:2402.06964</a> [<a href="/pdf/2402.06964" title="Download PDF">pdf</a>, <a href="/format/2402.06964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NLP for Knowledge Discovery and Information Extraction from Energetics  Corpora
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=VanGessel%2C+F+G">Francis G. VanGessel</a>, 
<a href="/search/cs?searchtype=author&query=Perry%2C+E">Efrem Perry</a>, 
<a href="/search/cs?searchtype=author&query=Mohan%2C+S">Salil Mohan</a>, 
<a href="/search/cs?searchtype=author&query=Barham%2C+O+M">Oliver M. Barham</a>, 
<a href="/search/cs?searchtype=author&query=Cavolowsky%2C+M">Mark Cavolowsky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Materials Science (cond-mat.mtrl-sci)

</div>
<p class="mathjax">We present a demonstration of the utility of NLP for aiding research into
energetic materials and associated systems. The NLP method enables machine
understanding of textual data, offering an automated route to knowledge
discovery and information extraction from energetics text. We apply three
established unsupervised NLP models: Latent Dirichlet Allocation, Word2Vec, and
the Transformer to a large curated dataset of energetics-related scientific
articles. We demonstrate that each NLP algorithm is capable of identifying
energetic topics and concepts, generating a language model which aligns with
Subject Matter Expert knowledge. Furthermore, we present a document
classification pipeline for energetics text. Our classification pipeline
achieves 59-76\% accuracy depending on the NLP model used, with the highest
performing Transformer model rivaling inter-annotator agreement metrics. The
NLP approaches studied in this work can identify concepts germane to energetics
and therefore hold promise as a tool for accelerating energetics research
efforts and energetics material development.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06966" title="Abstract">arXiv:2402.06966</a> [<a href="/pdf/2402.06966" title="Download PDF">pdf</a>, <a href="/format/2402.06966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeepCover: Advancing RNN Test Coverage and Online Error Prediction using  State Machine Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Golshanrad%2C+P">Pouria Golshanrad</a>, 
<a href="/search/cs?searchtype=author&query=Faghih%2C+F">Fathiyeh Faghih</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recurrent neural networks (RNNs) have emerged as powerful tools for
processing sequential data in various fields, including natural language
processing and speech recognition. However, the lack of explainability in RNN
models has limited their interpretability, posing challenges in understanding
their internal workings. To address this issue, this paper proposes a
methodology for extracting a state machine (SM) from an RNN-based model to
provide insights into its internal function. The proposed SM extraction
algorithm was assessed using four newly proposed metrics: Purity, Richness,
Goodness, and Scale. The proposed methodology along with its assessment metrics
contribute to increasing explainability in RNN models by providing a clear
representation of their internal decision making process through the extracted
SM. In addition to improving the explainability of RNNs, the extracted SM can
be used to advance testing and and monitoring of the primary RNN-based model.
To enhance RNN testing, we introduce six model coverage criteria based on the
extracted SM, serving as metrics for evaluating the effectiveness of test
suites designed to analyze the primary model. We also propose a tree-based
model to predict the error probability of the primary model for each input
based on the extracted SM. We evaluated our proposed online error prediction
approach using the MNIST dataset and Mini Speech Commands dataset, achieving an
area under the curve (AUC) exceeding 80\% for the receiver operating
characteristic (ROC) chart.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06967" title="Abstract">arXiv:2402.06967</a> [<a href="/pdf/2402.06967" title="Download PDF">pdf</a>, <a href="/format/2402.06967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning  Framework for Dialogue
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Leong%2C+C+T">Chak Tou Leong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiashuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dongding Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xiao-Yong Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Tuning pretrained language models for dialogue generation has been a
prevalent paradigm for building capable dialogue agents. Yet, traditional
tuning narrowly views dialogue generation as resembling other language
generation tasks, ignoring the role disparities between two speakers and the
multi-round interactive process that dialogues ought to be. Such a manner leads
to unsatisfactory chat consistency of the built agent. In this work, we
emphasize the interactive, communicative nature of dialogue and argue that it
is more feasible to model the speaker roles of agent and user separately,
enabling the agent to adhere to its role consistently. We propose an efficient
Multi-round Interactive Dialogue Tuning (Midi-Tuning) framework. It models the
agent and user individually with two adapters built upon large language models,
where they utilize utterances round by round in alternating order and are tuned
via a round-level memory caching mechanism. Extensive experiments demonstrate
that, our framework performs superior to traditional fine-tuning and harbors
the tremendous potential for improving dialogue consistency.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06968" title="Abstract">arXiv:2402.06968</a> [<a href="/pdf/2402.06968" title="Download PDF">pdf</a>, <a href="/format/2402.06968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contextual Stochastic Vehicle Routing with Time Windows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Serrano%2C+B">Breno Serrano</a>, 
<a href="/search/cs?searchtype=author&query=Florio%2C+A+M">Alexandre M. Florio</a>, 
<a href="/search/cs?searchtype=author&query=Minner%2C+S">Stefan Minner</a>, 
<a href="/search/cs?searchtype=author&query=Schiffer%2C+M">Maximilian Schiffer</a>, 
<a href="/search/cs?searchtype=author&query=Vidal%2C+T">Thibaut Vidal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">We study the vehicle routing problem with time windows (VRPTW) and stochastic
travel times, in which the decision-maker observes related contextual
information, represented as feature variables, before making routing decisions.
Despite the extensive literature on stochastic VRPs, the integration of feature
variables has received limited attention in this context. We introduce the
contextual stochastic VRPTW, which minimizes the total transportation cost and
expected late arrival penalties conditioned on the observed features. Since the
joint distribution of travel times and features is unknown, we present novel
data-driven prescriptive models that use historical data to provide an
approximate solution to the problem. We distinguish the prescriptive models
between point-based approximation, sample average approximation, and
penalty-based approximation, each taking a different perspective on dealing
with stochastic travel times and features. We develop specialized
branch-price-and-cut algorithms to solve these data-driven prescriptive models.
In our computational experiments, we compare the out-of-sample cost performance
of different methods on instances with up to one hundred customers. Our results
show that, surprisingly, a feature-dependent sample average approximation
outperforms existing and novel methods in most settings.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06969" title="Abstract">arXiv:2402.06969</a> [<a href="/pdf/2402.06969" title="Download PDF">pdf</a>, <a href="/format/2402.06969" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthesizing CTA Image Data for Type-B Aortic Dissection using Stable  Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abaid%2C+A">Ayman Abaid</a>, 
<a href="/search/cs?searchtype=author&query=Farooq%2C+M+A">Muhammad Ali Farooq</a>, 
<a href="/search/cs?searchtype=author&query=Hynes%2C+N">Niamh Hynes</a>, 
<a href="/search/cs?searchtype=author&query=Corcoran%2C+P">Peter Corcoran</a>, 
<a href="/search/cs?searchtype=author&query=Ullah%2C+I">Ihsan Ullah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted in IEEE EMBC 2024 Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Stable Diffusion (SD) has gained a lot of attention in recent years in the
field of Generative AI thus helping in synthesizing medical imaging data with
distinct features. The aim is to contribute to the ongoing effort focused on
overcoming the limitations of data scarcity and improving the capabilities of
ML algorithms for cardiovascular image processing. Therefore, in this study,
the possibility of generating synthetic cardiac CTA images was explored by
fine-tuning stable diffusion models based on user defined text prompts, using
only limited number of CTA images as input. A comprehensive evaluation of the
synthetic data was conducted by incorporating both quantitative analysis and
qualitative assessment, where a clinician assessed the quality of the generated
data. It has been shown that Cardiac CTA images can be successfully generated
using using Text to Image (T2I) stable diffusion model. The results demonstrate
that the tuned T2I CTA diffusion model was able to generate images with
features that are typically unique to acute type B aortic dissection (TBAD)
medical conditions.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06971" title="Abstract">arXiv:2402.06971</a> [<a href="/pdf/2402.06971" title="Download PDF">pdf</a>, <a href="/format/2402.06971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Data Distillation with TabPFN
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Junwei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+V">Valentin Thomas</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+G">Guangwei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Caterini%2C+A">Anthony Caterini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Foundation models have revolutionized tasks in computer vision and natural
language processing. However, in the realm of tabular data, tree-based models
like XGBoost continue to dominate. TabPFN, a transformer model tailored for
tabular data, mirrors recent foundation models in its exceptional in-context
learning capability, being competitive with XGBoost's performance without the
need for task-specific training or hyperparameter tuning. Despite its promise,
TabPFN's applicability is hindered by its data size constraint, limiting its
use in real-world scenarios. To address this, we present in-context data
distillation (ICD), a novel methodology that effectively eliminates these
constraints by optimizing TabPFN's context. ICD efficiently enables TabPFN to
handle significantly larger datasets with a fixed memory budget, improving
TabPFN's quadratic memory complexity but at the cost of a linear number of
tuning steps. Notably, TabPFN, enhanced with ICD, demonstrates very strong
performance against established tree-based models and modern deep learning
methods on 48 large tabular datasets from OpenML.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06973" title="Abstract">arXiv:2402.06973</a> [<a href="/pdf/2402.06973" title="Download PDF">pdf</a>, <a href="/format/2402.06973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Event-Keyed Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gantt%2C+W">William Gantt</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+A">Alexander Martin</a>, 
<a href="/search/cs?searchtype=author&query=Kuchmiichuk%2C+P">Pavlo Kuchmiichuk</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+A+S">Aaron Steven White</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ARR short paper (under review)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce event-keyed summarization (EKS), a novel task that marries
traditional summarization and document-level event extraction, with the goal of
generating a contextualized summary for a specific event, given a document and
an extracted event structure. We introduce a dataset for this task, MUCSUM,
consisting of summaries of all events in the classic MUC-4 dataset, along with
a set of baselines that comprises both pretrained LM standards in the
summarization literature, as well as larger frontier models. We show that
ablations that reduce EKS to traditional summarization or structure-to-text
yield inferior summaries of target events and that MUCSUM is a robust benchmark
for this task. Lastly, we conduct a human evaluation of both reference and
model summaries, and provide some detailed analysis of the results.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06974" title="Abstract">arXiv:2402.06974</a> [<a href="/pdf/2402.06974" title="Download PDF">pdf</a>, <a href="/format/2402.06974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-linear Fusion in Federated Learning: A Hypernetwork Approach to  Federated Domain Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bartholet%2C+M">Marc Bartholet</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taehyeon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Beuret%2C+A">Ami Beuret</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Se-Young Yun</a>, 
<a href="/search/cs?searchtype=author&query=Buhmann%2C+J+M">Joachim M. Buhmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICML 2024 Conference on 01.02.2024; currently Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Federated Learning (FL) has emerged as a promising paradigm in which multiple
clients collaboratively train a shared global model while preserving data
privacy. To create a robust and practicable FL framework, it is crucial to
extend its ability to generalize well to unseen domains - a problem referred to
as federated Domain Generalization (FDG), being still under-explored. We
propose an innovative federated algorithm, termed hFedF for hypernetwork-based
Federated Fusion, designed to bridge the performance gap between generalization
and personalization, capable of addressing various degrees of domain shift.
Essentially, the hypernetwork supports a non-linear fusion of client models
enabling a comprehensive understanding of the underlying data distribution. We
encompass an extensive discussion and provide novel insights into the tradeoff
between personalization and generalization in FL. The proposed algorithm
outperforms strong benchmarks on three widely-used data sets for DG in an
exceeding number of cases.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06976" title="Abstract">arXiv:2402.06976</a> [<a href="/pdf/2402.06976" title="Download PDF">pdf</a>, <a href="/format/2402.06976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Rearrangement Planning for Object Retrieval from Confined Spaces  Perceivable by Robot&#x27;s In-hand RGB-D Sensor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Hanwen Ren</a>, 
<a href="/search/cs?searchtype=author&query=Qureshi%2C+A+H">Ahmed H. Qureshi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IEEE/RAS ICRA'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Rearrangement planning for object retrieval tasks from confined spaces is a
challenging problem, primarily due to the lack of open space for robot motion
and limited perception. Several traditional methods exist to solve object
retrieval tasks, but they require overhead cameras for perception and a
time-consuming exhaustive search to find a solution and often make unrealistic
assumptions, such as having identical, simple geometry objects in the
environment. This paper presents a neural object retrieval framework that
efficiently performs rearrangement planning of unknown, arbitrary objects in
confined spaces to retrieve the desired object using a given robot grasp. Our
method actively senses the environment with the robot's in-hand camera. It then
selects and relocates the non-target objects such that they do not block the
robot path homotopy to the target object, thus also aiding an underlying path
planner in quickly finding robot motion sequences. Furthermore, we demonstrate
our framework in challenging scenarios, including real-world cabinet-like
environments with arbitrary household objects. The results show that our
framework achieves the best performance among all presented methods and is, on
average, two orders of magnitude computationally faster than the
best-performing baselines.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06978" title="Abstract">arXiv:2402.06978</a> [<a href="/pdf/2402.06978" title="Download PDF">pdf</a>, <a href="/format/2402.06978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sophia-in-Audition: Virtual Production with a Robot Performer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Taotao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Teng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+Y">Yuyang Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Peijun Xu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yaoyu He</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jingyi Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://miaoing.github.io/SiA/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">We present Sophia-in-Audition (SiA), a new frontier in virtual production, by
employing the humanoid robot Sophia within an UltraStage environment composed
of a controllable lighting dome coupled with multiple cameras. We demonstrate
Sophia's capability to replicate iconic film segments, follow real performers,
and perform a variety of motions and expressions, showcasing her versatility as
a virtual actor. Key to this process is the integration of facial motion
transfer algorithms and the UltraStage's controllable lighting and multi-camera
setup, enabling dynamic performances that align with the director's vision. Our
comprehensive user studies indicate positive audience reception towards
Sophia's performances, highlighting her potential to reduce the uncanny valley
effect in virtual acting. Additionally, the immersive lighting in dynamic clips
was highly rated for its naturalness and its ability to mirror professional
film standards. The paper presents a first-of-its-kind multi-view robot
performance video dataset with dynamic lighting, offering valuable insights for
future enhancements in humanoid robotic performers and virtual production
techniques. This research contributes significantly to the field by presenting
a unique virtual production setup, developing tools for sophisticated
performance control, and providing a comprehensive dataset and user study
analysis for diverse applications.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06981" title="Abstract">arXiv:2402.06981</a> [<a href="/pdf/2402.06981" title="Download PDF">pdf</a>, <a href="/ps/2402.06981" title="Download PostScript">ps</a>, <a href="/format/2402.06981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structures vibration control via tuned mass dampers using a co-evolution  coral reefs optimization algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Salcedo-Sanz%2C+S">S Salcedo-Sanz</a>, 
<a href="/search/eess?searchtype=author&query=Camacho-G%C3%B3mez%2C+C">C Camacho-G&#xf3;mez</a>, 
<a href="/search/eess?searchtype=author&query=Magdaleno%2C+A">A Magdaleno</a>, 
<a href="/search/eess?searchtype=author&query=Pereira%2C+E">E Pereira</a>, 
<a href="/search/eess?searchtype=author&query=Lorenzana%2C+A">A Lorenzana</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Sound and Vibration, vol. 393, pp. 62-75, 2017
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper we tackle a problem of optimal design and location of Tuned
Mass Dampers (TMDs) for structures subjected to earthquake ground motions,
using a novel meta-heuristic algorithm. Specifically, the Coral Reefs
Optimization (CRO) with Substrate Layer (CRO-SL) is proposed as a competitive
co-evolution algorithm with different exploration procedures within a single
population of solutions. The proposed approach is able to solve the TMD design
and location problem, by exploiting the combination of different types of
searching mechanisms. This promotes a powerful evolutionary-like algorithm for
optimization problems, which is shown to be very effective in this particular
problem of TMDs tuning. The proposed algorithm's performance has been evaluated
and compared with several reference algorithms in two building models with two
and four floors, respectively.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06982" title="Abstract">arXiv:2402.06982</a> [<a href="/pdf/2402.06982" title="Download PDF">pdf</a>, <a href="/format/2402.06982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Treatment-wise Glioblastoma Survival Inference with Multi-parametric  Preoperative MRI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaofeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shusharina%2C+N">Nadya Shusharina</a>, 
<a href="/search/cs?searchtype=author&query=Shih%2C+H+A">Helen A Shih</a>, 
<a href="/search/cs?searchtype=author&query=Kuo%2C+C+-+J">C.-C. Jay Kuo</a>, 
<a href="/search/cs?searchtype=author&query=Fakhri%2C+G+E">Georges El Fakhri</a>, 
<a href="/search/cs?searchtype=author&query=Woo%2C+J">Jonghye Woo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SPIE Medical Imaging 2024: Computer-Aided Diagnosis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Medical Physics (physics.med-ph)

</div>
<p class="mathjax">In this work, we aim to predict the survival time (ST) of glioblastoma (GBM)
patients undergoing different treatments based on preoperative magnetic
resonance (MR) scans. The personalized and precise treatment planning can be
achieved by comparing the ST of different treatments. It is well established
that both the current status of the patient (as represented by the MR scans)
and the choice of treatment are the cause of ST. While previous related
MR-based glioblastoma ST studies have focused only on the direct mapping of MR
scans to ST, they have not included the underlying causal relationship between
treatments and ST. To address this limitation, we propose a
treatment-conditioned regression model for glioblastoma ST that incorporates
treatment information in addition to MR scans. Our approach allows us to
effectively utilize the data from all of the treatments in a unified manner,
rather than having to train separate models for each of the treatments.
Furthermore, treatment can be effectively injected into each convolutional
layer through the adaptive instance normalization we employ. We evaluate our
framework on the BraTS20 ST prediction task. Three treatment options are
considered: Gross Total Resection (GTR), Subtotal Resection (STR), and no
resection. The evaluation results demonstrate the effectiveness of injecting
the treatment for estimating GBM survival.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06984" title="Abstract">arXiv:2402.06984</a> [<a href="/pdf/2402.06984" title="Download PDF">pdf</a>, <a href="/format/2402.06984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speech motion anomaly detection via cross-modal translation of 4D motion  fields from tagged MRI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaofeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+F">Fangxu Xing</a>, 
<a href="/search/cs?searchtype=author&query=Zhuo%2C+J">Jiachen Zhuo</a>, 
<a href="/search/cs?searchtype=author&query=Stone%2C+M">Maureen Stone</a>, 
<a href="/search/cs?searchtype=author&query=Prince%2C+J+L">Jerry L. Prince</a>, 
<a href="/search/cs?searchtype=author&query=Fakhri%2C+G+E">Georges El Fakhri</a>, 
<a href="/search/cs?searchtype=author&query=Woo%2C+J">Jonghye Woo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SPIE Medical Imaging 2024: Image Processing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM); Audio and Speech Processing (eess.AS); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Understanding the relationship between tongue motion patterns during speech
and their resulting speech acoustic outcomes -- i.e., articulatory-acoustic
relation -- is of great importance in assessing speech quality and developing
innovative treatment and rehabilitative strategies. This is especially
important when evaluating and detecting abnormal articulatory features in
patients with speech-related disorders. In this work, we aim to develop a
framework for detecting speech motion anomalies in conjunction with their
corresponding speech acoustics. This is achieved through the use of a deep
cross-modal translator trained on data from healthy individuals only, which
bridges the gap between 4D motion fields obtained from tagged MRI and 2D
spectrograms derived from speech acoustic data. The trained translator is used
as an anomaly detector, by measuring the spectrogram reconstruction quality on
healthy individuals or patients. In particular, the cross-modal translator is
likely to yield limited generalization capabilities on patient data, which
includes unseen out-of-distribution patterns and demonstrates subpar
performance, when compared with healthy individuals.~A one-class SVM is then
used to distinguish the spectrograms of healthy individuals from those of
patients. To validate our framework, we collected a total of 39 paired tagged
MRI and speech waveforms, consisting of data from 36 healthy individuals and 3
tongue cancer patients. We used both 3D convolutional and transformer-based
deep translation models, training them on the healthy training set and then
applying them to both the healthy and patient testing sets. Our framework
demonstrates a capability to detect abnormal patient data, thereby illustrating
its potential in enhancing the understanding of the articulatory-acoustic
relation for both healthy individuals and patients.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06985" title="Abstract">arXiv:2402.06985</a> [<a href="/pdf/2402.06985" title="Download PDF">pdf</a>, <a href="/format/2402.06985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OSSAR: Towards Open-Set Surgical Activity Recognition in Robot-assisted  Surgery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+L">Long Bai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guankun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaoxiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Huxin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">An Wang</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M">Mobarakol Islam</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Hongliang Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in IEEE ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">In the realm of automated robotic surgery and computer-assisted
interventions, understanding robotic surgical activities stands paramount.
Existing algorithms dedicated to surgical activity recognition predominantly
cater to pre-defined closed-set paradigms, ignoring the challenges of
real-world open-set scenarios. Such algorithms often falter in the presence of
test samples originating from classes unseen during training phases. To tackle
this problem, we introduce an innovative Open-Set Surgical Activity Recognition
(OSSAR) framework. Our solution leverages the hyperspherical reciprocal point
strategy to enhance the distinction between known and unknown classes in the
feature space. Additionally, we address the issue of over-confidence in the
closed set by refining model calibration, avoiding misclassification of unknown
classes as known ones. To support our assertions, we establish an open-set
surgical activity benchmark utilizing the public JIGSAWS dataset. Besides, we
also collect a novel dataset on endoscopic submucosal dissection for surgical
activity tasks. Extensive comparisons and ablation experiments on these
datasets demonstrate the significant outperformance of our method over existing
state-of-the-art approaches. Our proposed solution can effectively address the
challenges of real-world surgical scenarios. Our code is publicly accessible at
https://github.com/longbai1006/OSSAR.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06986" title="Abstract">arXiv:2402.06986</a> [<a href="/pdf/2402.06986" title="Download PDF">pdf</a>, <a href="/format/2402.06986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cacophony: An Improved Contrastive Audio-Text Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+G">Ge Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+Z">Zhiyao Duan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Despite recent improvements in audio-text modeling, audio-text contrastive
models still lag behind their image-text counterparts in scale and performance.
We propose a method to improve both the scale and the training of audio-text
contrastive models. Specifically, we craft a large-scale audio-text dataset
consisting of over 13,000 hours of text-labeled audio, aided by large language
model (LLM) processing and audio captioning. Further, we employ an masked
autoencoder (MAE) pre-pretraining phase with random patch dropout, which allows
us to both scale unlabeled audio datasets and train efficiently with variable
length audio. After MAE pre-pretraining of our audio encoder, we train a
contrastive model with an auxiliary captioning objective. Our final model,
which we name Cacophony, achieves state-of-the-art performance on audio-text
retrieval tasks, and exhibits competitive results on other downstream tasks
such as zero-shot classification.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06988" title="Abstract">arXiv:2402.06988</a> [<a href="/pdf/2402.06988" title="Download PDF">pdf</a>, <a href="/ps/2402.06988" title="Download PostScript">ps</a>, <a href="/format/2402.06988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Three Subtyping Algorithms for Binary Session Types and their Complexity  Analyses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Udomsrirungruang%2C+T">Thien Udomsrirungruang</a>, 
<a href="/search/cs?searchtype=author&query=Yoshida%2C+N">Nobuko Yoshida</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures. Full version of a paper submitted to PLACES 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">Session types are a type discipline for describing and specifying
communication behaviours of concurrent processes. Session subtyping, firstly
introduced by Gay and Hole, is widely used for enlarging typability of session
programs. This paper gives the complexity analysis of three algorithms for
subtyping of synchronous binary session types. First, we analyse the complexity
of the algorithm from the original paper, which is based on an inductive tree
search. We then introduce its optimised version, which improves the complexity,
but is still exponential against the size of the two types. Finally, we propose
a new quadratic algorithm based on a graph search using the concept of
$\mathcal{XYZW}$-simulation, recently introduced by Silva et al.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06989" title="Abstract">arXiv:2402.06989</a> [<a href="/pdf/2402.06989" title="Download PDF">pdf</a>, <a href="/ps/2402.06989" title="Download PostScript">ps</a>, <a href="/format/2402.06989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Designing for Work with Intelligent Entities: A Review of Perspectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McCarthy%2C+J+E">James E. McCarthy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">As the power of Artificial Intelligence (AI) continues to advance, there is
increased interest in how best to combine AI-based agents with humans to
achieve mission effectiveness. Three perspectives have emerged. The first stems
from more conventional human factors traditions and views these entities as
highly capable tools that humans can use to accomplish increasingly
sophisticated tasks. The second "camp" believes that as the sophistication of
these entities increases, it becomes increasingly appropriate to talk about
them as "teammates" and use the research on human teams as a foundation for
further exploration. The third perspective is emerging and finds both the
"tools" and "teammate" metaphors flawed and limiting. This perspective
emphasizes "joint activity," "joint cognitive activity," or something similar.
In this article, we briefly review these three perspectives.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06990" title="Abstract">arXiv:2402.06990</a> [<a href="/pdf/2402.06990" title="Download PDF">pdf</a>, <a href="/format/2402.06990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guided Sketch-Based Program Induction by Search Gradients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amin%2C+A+A">Ahmad Ayaz Amin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">Many tasks can be easily solved using machine learning techniques. However,
some tasks cannot readily be solved using statistical models, requiring a
symbolic approach instead. Program induction is one of the ways that such tasks
can be solved by means of capturing an interpretable and generalizable
algorithm through training. However, contemporary approaches to program
induction are not sophisticated enough to readily be applied to various types
of tasks as they tend to be formulated as a single, all-encompassing model,
usually parameterized by neural networks. In an attempt to make program
induction a viable solution for many scenarios, we propose a framework for
learning parameterized programs via search gradients using evolution
strategies. This formulation departs from traditional program induction as it
allows for the programmer to impart task-specific code to the program 'sketch',
while also enjoying the benefits of accelerated learning through end-to-end
gradient-based optimization.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06991" title="Abstract">arXiv:2402.06991</a> [<a href="/pdf/2402.06991" title="Download PDF">pdf</a>, <a href="/format/2402.06991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reciprocal Visibility
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nathan%2C+R+J+A+A">Rakesh John Amala Arokia Nathan</a>, 
<a href="/search/cs?searchtype=author&query=Strand%2C+S">Sigrid Strand</a>, 
<a href="/search/cs?searchtype=author&query=Shutin%2C+D">Dmitriy Shutin</a>, 
<a href="/search/cs?searchtype=author&query=Bimber%2C+O">Oliver Bimber</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We propose a guidance strategy to optimize real-time synthetic aperture
sampling for occlusion removal with drones by pre-scanned point-cloud data.
Depth information can be used to compute visibility of points on the ground for
individual drone positions in the air. Inspired by Helmholtz reciprocity, we
introduce reciprocal visibility to determine the dual situation - the
visibility of potential sampling position in the air from given points of
interest on the ground. The resulting visibility map encodes which point on the
ground is visible by which magnitude from any position in the air. Based on
such a map, we demonstrate a first greedy sampling optimization.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06994" title="Abstract">arXiv:2402.06994</a> [<a href="/pdf/2402.06994" title="Download PDF">pdf</a>, <a href="/ps/2402.06994" title="Download PostScript">ps</a>, <a href="/format/2402.06994" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Change Detection Reality Check
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Corley%2C+I">Isaac Corley</a>, 
<a href="/search/cs?searchtype=author&query=Robinson%2C+C">Caleb Robinson</a>, 
<a href="/search/cs?searchtype=author&query=Ortiz%2C+A">Anthony Ortiz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In recent years, there has been an explosion of proposed change detection
deep learning architectures in the remote sensing literature. These approaches
claim to offer state-of the-art performance on different standard benchmark
datasets. However, has the field truly made significant progress? In this paper
we perform experiments which conclude a simple U-Net segmentation baseline
without training tricks or complicated architectural changes is still a top
performer for the task of change detection.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07002" title="Abstract">arXiv:2402.07002</a> [<a href="/pdf/2402.07002" title="Download PDF">pdf</a>, <a href="/format/2402.07002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clients Collaborate: Flexible Differentially Private Federated Learning  with Guaranteed Improvement of Utility-Privacy Trade-off
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuecheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+J">Jian Lou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zibin Zheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">To defend against privacy leakage of user data, differential privacy is
widely used in federated learning, but it is not free. The addition of noise
randomly disrupts the semantic integrity of the model and this disturbance
accumulates with increased communication rounds. In this paper, we introduce a
novel federated learning framework with rigorous privacy guarantees, named
FedCEO, designed to strike a trade-off between model utility and user privacy
by letting clients ''Collaborate with Each Other''. Specifically, we perform
efficient tensor low-rank proximal optimization on stacked local model
parameters at the server, demonstrating its capability to flexibly truncate
high-frequency components in spectral space. This implies that our FedCEO can
effectively recover the disrupted semantic information by smoothing the global
semantic space for different privacy settings and continuous training
processes. Moreover, we improve the SOTA utility-privacy trade-off bound by an
order of $\sqrt{d}$, where $d$ is the input dimension. We illustrate our
theoretical results with experiments on representative image datasets. It
observes significant performance improvements and strict privacy guarantees
under different privacy settings.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07007" title="Abstract">arXiv:2402.07007</a> [<a href="/pdf/2402.07007" title="Download PDF">pdf</a>, <a href="/format/2402.07007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonlinear electro-elastic finite element analysis with neural network  constitutive models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Klein%2C+D+K">Dominik K. Klein</a>, 
<a href="/search/cs?searchtype=author&query=Ortigosa%2C+R">Rogelio Ortigosa</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez-Frutos%2C+J">Jes&#xfa;s Mart&#xed;nez-Frutos</a>, 
<a href="/search/cs?searchtype=author&query=Weeger%2C+O">Oliver Weeger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">In the present work, the applicability of physics-augmented neural network
(PANN) constitutive models for complex electro-elastic finite element analysis
is demonstrated. For the investigations, PANN models for electro-elastic
material behavior at finite deformations are calibrated to different
synthetically generated datasets, including an analytical isotropic potential,
a homogenised rank-one laminate, and a homogenised metamaterial with a
spherical inclusion. Subsequently, boundary value problems inspired by
engineering applications of composite electro-elastic materials are considered.
Scenarios with large electrically induced deformations and instabilities are
particularly challenging and thus necessitate extensive investigations of the
PANN constitutive models in the context of finite element analyses. First of
all, an excellent prediction quality of the model is required for very general
load cases occurring in the simulation. Furthermore, simulation of large
deformations and instabilities poses challenges on the stability of the
numerical solver, which is closely related to the constitutive model. In all
cases studied, the PANN models yield excellent prediction qualities and a
stable numerical behavior even in highly nonlinear scenarios. This can be
traced back to the PANN models excellent performance in learning both the first
and second derivatives of the ground truth electro-elastic potentials, even
though it is only calibrated on the first derivatives. Overall, this work
demonstrates the applicability of PANN constitutive models for the efficient
and robust simulation of engineering applications of composite electro-elastic
materials.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07010" title="Abstract">arXiv:2402.07010</a> [<a href="/pdf/2402.07010" title="Download PDF">pdf</a>, <a href="/format/2402.07010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impact of Voice Fidelity on Decision Making: A Potential Dark Pattern?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dubiel%2C+M">Mateusz Dubiel</a>, 
<a href="/search/cs?searchtype=author&query=Sergeeva%2C+A">Anastasia Sergeeva</a>, 
<a href="/search/cs?searchtype=author&query=Leiva%2C+L+A">Luis A. Leiva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACM Conference on Intelligent User Interfaces (ACM IUI) 2024, Authors' Manuscript
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Manipulative design in user interfaces (conceptualized as dark patterns) has
emerged as a significant impediment to the ethical design of technology and a
threat to user agency and freedom of choice. While previous research focused on
exploring these patterns in the context of graphical user interfaces, the
impact of speech has largely been overlooked. We conducted a listening test (N
= 50) to elicit participants' preferences regarding different synthetic voices
that varied in terms of synthesis method (concatenative vs. neural) and
prosodic qualities (speech pace and pitch variance), and then evaluated their
impact in an online decision-making study (N = 101). Our results indicate a
significant effect of voice qualities on the participant's choices,
independently from the content of the available options. Our results also
indicate that the voice's perceived engagement, ease of understanding, and
domain fit directly translate to its impact on participants' behaviour in
decision-making tasks.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07011" title="Abstract">arXiv:2402.07011</a> [<a href="/pdf/2402.07011" title="Download PDF">pdf</a>, <a href="/format/2402.07011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedImpro: Measuring and Improving Client Update in Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zhenheng Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yonggang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Shaohuai Shi</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+X">Xinmei Tian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+X">Xiaowen Chu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated Learning (FL) models often experience client drift caused by
heterogeneous data, where the distribution of data differs across clients. To
address this issue, advanced research primarily focuses on manipulating the
existing gradients to achieve more consistent client models. In this paper, we
present an alternative perspective on client drift and aim to mitigate it by
generating improved local models. First, we analyze the generalization
contribution of local training and conclude that this generalization
contribution is bounded by the conditional Wasserstein distance between the
data distribution of different clients. Then, we propose FedImpro, to construct
similar conditional distributions for local training. Specifically, FedImpro
decouples the model into high-level and low-level components, and trains the
high-level portion on reconstructed feature distributions. This approach
enhances the generalization contribution and reduces the dissimilarity of
gradients in FL. Experimental results show that FedImpro can help FL defend
against data heterogeneity and enhance the generalization performance of the
model.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07016" title="Abstract">arXiv:2402.07016</a> [<a href="/pdf/2402.07016" title="Download PDF">pdf</a>, <a href="/format/2402.07016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> REALM: RAG-Driven Enhancement of Multimodal Electronic Health Records  Analysis via Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yinghao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+C">Changyu Ren</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Shiyun Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shukai Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Hangyuan Ji</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zixiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tao Sun</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Long He</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhoujun Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+C">Chengwei Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The integration of multimodal Electronic Health Records (EHR) data has
significantly improved clinical predictive capabilities. Leveraging clinical
notes and multivariate time-series EHR, existing models often lack the medical
context relevent to clinical tasks, prompting the incorporation of external
knowledge, particularly from the knowledge graph (KG). Previous approaches with
KG knowledge have primarily focused on structured knowledge extraction,
neglecting unstructured data modalities and semantic high dimensional medical
knowledge. In response, we propose REALM, a Retrieval-Augmented Generation
(RAG) driven framework to enhance multimodal EHR representations that address
these limitations. Firstly, we apply Large Language Model (LLM) to encode long
context clinical notes and GRU model to encode time-series EHR data. Secondly,
we prompt LLM to extract task-relevant medical entities and match entities in
professionally labeled external knowledge graph (PrimeKG) with corresponding
medical knowledge. By matching and aligning with clinical standards, our
framework eliminates hallucinations and ensures consistency. Lastly, we propose
an adaptive multimodal fusion network to integrate extracted knowledge with
multimodal EHR data. Our extensive experiments on MIMIC-III mortality and
readmission tasks showcase the superior performance of our REALM framework over
baselines, emphasizing the effectiveness of each module. REALM framework
contributes to refining the use of multimodal EHR data in healthcare and
bridging the gap with nuanced medical context essential for informed clinical
predictions.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07019" title="Abstract">arXiv:2402.07019</a> [<a href="/pdf/2402.07019" title="Download PDF">pdf</a>, <a href="/format/2402.07019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Informativeness of Reward Functions in Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Devidze%2C+R">Rati Devidze</a>, 
<a href="/search/cs?searchtype=author&query=Kamalaruban%2C+P">Parameswaran Kamalaruban</a>, 
<a href="/search/cs?searchtype=author&query=Singla%2C+A">Adish Singla</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Longer version of the AAMAS'24 paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Reward functions are central in specifying the task we want a reinforcement
learning agent to perform. Given a task and desired optimal behavior, we study
the problem of designing informative reward functions so that the designed
rewards speed up the agent's convergence. In particular, we consider
expert-driven reward design settings where an expert or teacher seeks to
provide informative and interpretable rewards to a learning agent. Existing
works have considered several different reward design formulations; however,
the key challenge is formulating a reward informativeness criterion that adapts
w.r.t. the agent's current policy and can be optimized under specified
structural constraints to obtain interpretable rewards. In this paper, we
propose a novel reward informativeness criterion, a quantitative measure that
captures how the agent's current policy will improve if it receives rewards
from a specific reward function. We theoretically showcase the utility of the
proposed informativeness criterion for adaptively designing rewards for an
agent. Experimental results on two navigation tasks demonstrate the
effectiveness of our adaptive reward informativeness criterion.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07021" title="Abstract">arXiv:2402.07021</a> [<a href="/pdf/2402.07021" title="Download PDF">pdf</a>, <a href="/format/2402.07021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Optimization with Adaptive Kernels for Robot Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Martinez-Cantin%2C+R">Ruben Martinez-Cantin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2017 IEEE International Conference on Robotics and Automation (ICRA). arXiv admin note: substantial text overlap with <a href="/abs/1610.00366">arXiv:1610.00366</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2017 IEEE International Conference on Robotics and Automation
  (ICRA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Active policy search combines the trial-and-error methodology from policy
search with Bayesian optimization to actively find the optimal policy. First,
policy search is a type of reinforcement learning which has become very popular
for robot control, for its ability to deal with complex continuous state and
action spaces. Second, Bayesian optimization is a sample efficient global
optimization method that uses a surrogate model, like a Gaussian process, and
optimal decision making to carefully select each sample during the optimization
process. Sample efficiency is of paramount importance when each trial involves
the real robot, expensive Monte Carlo runs, or a complex simulator. Black-box
Bayesian optimization generally assumes a cost function from a stationary
process, because nonstationary modeling is usually based on prior knowledge.
However, many control problems are inherently nonstationary due to their
failure conditions, terminal states and other abrupt effects. In this paper, we
present a kernel function specially designed for Bayesian optimization, that
allows nonstationary modeling without prior knowledge, using an adaptive local
region. The new kernel results in an improved local search (exploitation),
without penalizing the global search (exploration), as shown experimentally in
well-known optimization benchmarks and robot control scenarios. We finally show
its potential for the design of the wing shape of a UAV.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07023" title="Abstract">arXiv:2402.07023</a> [<a href="/pdf/2402.07023" title="Download PDF">pdf</a>, <a href="/format/2402.07023" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gemini Goes to Med School: Exploring the Capabilities of Multimodal  Large Language Models on Medical Challenge Problems &amp; Hallucinations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pal%2C+A">Ankit Pal</a>, 
<a href="/search/cs?searchtype=author&query=Sankarasubbu%2C+M">Malaikannan Sankarasubbu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint version, Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models have the potential to be valuable in the healthcare
industry, but it's crucial to verify their safety and effectiveness through
rigorous evaluation. For this purpose, we comprehensively evaluated both
open-source LLMs and Google's new multimodal LLM called Gemini across Medical
reasoning, hallucination detection, and Medical Visual Question Answering
tasks. While Gemini showed competence, it lagged behind state-of-the-art models
like MedPaLM 2 and GPT-4 in diagnostic accuracy. Additionally, Gemini achieved
an accuracy of 61.45\% on the medical VQA dataset, significantly lower than
GPT-4V's score of 88\%. Our analysis revealed that Gemini is highly susceptible
to hallucinations, overconfidence, and knowledge gaps, which indicate risks if
deployed uncritically. We also performed a detailed analysis by medical subject
and test type, providing actionable feedback for developers and clinicians. To
mitigate risks, we applied prompting strategies that improved performance.
Additionally, we facilitated future research and development by releasing a
Python module for medical LLM evaluation and establishing a dedicated
leaderboard on Hugging Face for medical domain LLMs. Python module can be found
at https://github.com/promptslab/RosettaEval
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07024" title="Abstract">arXiv:2402.07024</a> [<a href="/pdf/2402.07024" title="Download PDF">pdf</a>, <a href="/format/2402.07024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding safe 3D robot grasps through efficient haptic exploration with  unscented Bayesian optimization and collision penalty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Castanheira%2C+J">Joao Castanheira</a>, 
<a href="/search/cs?searchtype=author&query=Vicente%2C+P">Pedro Vicente</a>, 
<a href="/search/cs?searchtype=author&query=Martinez-Cantin%2C+R">Ruben Martinez-Cantin</a>, 
<a href="/search/cs?searchtype=author&query=Jamone%2C+L">Lorenzo Jamone</a>, 
<a href="/search/cs?searchtype=author&query=Bernardino%2C+A">Alexandre Bernardino</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2018 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Robust grasping is a major, and still unsolved, problem in robotics.
Information about the 3D shape of an object can be obtained either from prior
knowledge (e.g., accurate models of known objects or approximate models of
familiar objects) or real-time sensing (e.g., partial point clouds of unknown
objects) and can be used to identify good potential grasps. However, due to
modeling and sensing inaccuracies, local exploration is often needed to refine
such grasps and successfully apply them in the real world. The recently
proposed unscented Bayesian optimization technique can make such exploration
safer by selecting grasps that are robust to uncertainty in the input space
(e.g., inaccuracies in the grasp execution). Extending our previous work on 2D
optimization, in this paper we propose a 3D haptic exploration strategy that
combines unscented Bayesian optimization with a novel collision penalty
heuristic to find safe grasps in a very efficient way: while by augmenting the
search-space to 3D we are able to find better grasps, the collision penalty
heuristic allows us to do so without increasing the number of exploration
steps.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07027" title="Abstract">arXiv:2402.07027</a> [<a href="/pdf/2402.07027" title="Download PDF">pdf</a>, <a href="/ps/2402.07027" title="Download PostScript">ps</a>, <a href="/format/2402.07027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Speedup for Spectral Approximation of Kronecker Products
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yeqi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhao Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruizhe Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2311.03215">arXiv:2311.03215</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Emerging Technologies (cs.ET); Machine Learning (cs.LG); Quantum Algebra (math.QA)

</div>
<p class="mathjax">Given its widespread application in machine learning and optimization, the
Kronecker product emerges as a pivotal linear algebra operator. However, its
computational demands render it an expensive operation, leading to heightened
costs in spectral approximation of it through traditional computation
algorithms. Existing classical methods for spectral approximation exhibit a
linear dependency on the matrix dimension denoted by $n$, considering matrices
of size $A_1 \in \mathbb{R}^{n \times d}$ and $A_2 \in \mathbb{R}^{n \times
d}$. Our work introduces an innovative approach to efficiently address the
spectral approximation of the Kronecker product $A_1 \otimes A_2$ using quantum
methods. By treating matrices as quantum states, our proposed method
significantly reduces the time complexity of spectral approximation to
$O_{d,\epsilon}(\sqrt{n})$.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07028" title="Abstract">arXiv:2402.07028</a> [<a href="/pdf/2402.07028" title="Download PDF">pdf</a>, <a href="/format/2402.07028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-Supervised Learning for Bilingual Lexicon Induction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garnier%2C+P">Paul Garnier</a>, 
<a href="/search/cs?searchtype=author&query=Guinet%2C+G">Gauthier Guinet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We consider the problem of aligning two sets of continuous word
representations, corresponding to languages, to a common space in order to
infer a bilingual lexicon. It was recently shown that it is possible to infer
such lexicon, without using any parallel data, by aligning word embeddings
trained on monolingual data. Such line of work is called unsupervised bilingual
induction. By wondering whether it was possible to gain experience in the
progressive learning of several languages, we asked ourselves to what extent we
could integrate the knowledge of a given set of languages when learning a new
one, without having parallel data for the latter. In other words, while keeping
the core problem of unsupervised learning in the latest step, we allowed the
access to other corpora of idioms, hence the name semi-supervised. This led us
to propose a novel formulation, considering the lexicon induction as a ranking
problem for which we used recent tools of this machine learning field. Our
experiments on standard benchmarks, inferring dictionary from English to more
than 20 languages, show that our approach consistently outperforms existing
state of the art benchmark. In addition, we deduce from this new scenario
several relevant conclusions allowing a better understanding of the alignment
phenomenon.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07029" title="Abstract">arXiv:2402.07029</a> [<a href="/pdf/2402.07029" title="Download PDF">pdf</a>, <a href="/format/2402.07029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Mathlink Cubes to Introduce Data Wrangling with Examples in R
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McGowan%2C+L+D">Lucy D&#x27;Agostino McGowan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Applications (stat.AP); Computation (stat.CO); Other Statistics (stat.OT)

</div>
<p class="mathjax">This paper explores an innovative approach to teaching data wrangling skills
to students through hands-on activities before transitioning to coding. Data
wrangling, a critical aspect of data analysis, involves cleaning, transforming,
and restructuring data. We introduce the use of a physical tool, mathlink
cubes, to facilitate a tangible understanding of data sets. This approach helps
students grasp the concepts of data wrangling before implementing them in
coding languages such as R. We detail a classroom activity that includes
hands-on tasks paralleling common data wrangling processes such as filtering,
selecting, and mutating, followed by their coding equivalents using R's `dplyr`
package.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07031" title="Abstract">arXiv:2402.07031</a> [<a href="/pdf/2402.07031" title="Download PDF">pdf</a>, <a href="/format/2402.07031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instance-Level Safety-Aware Fidelity of Synthetic Data and Its  Calibration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+C">Chih-Hong Cheng</a>, 
<a href="/search/cs?searchtype=author&query=St%C3%B6ckel%2C+P">Paul St&#xf6;ckel</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xingyu Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Modeling and calibrating the fidelity of synthetic data is paramount in
shaping the future of safe and reliable self-driving technology by offering a
cost-effective and scalable alternative to real-world data collection. We focus
on its role in safety-critical applications, introducing four types of
instance-level fidelity that go beyond mere visual input characteristics. The
aim is to align synthetic data with real-world safety issues. We suggest an
optimization method to refine the synthetic data generator, reducing fidelity
gaps identified by the DNN-based component. Our findings show this tuning
enhances the correlation between safety-critical errors in synthetic and real
images.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07032" title="Abstract">arXiv:2402.07032</a> [<a href="/pdf/2402.07032" title="Download PDF">pdf</a>, <a href="/format/2402.07032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Field demonstration of predictive heating control for an all-electric  house in a cold climate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Pergantis%2C+E+N">Elias N. Pergantis</a>, 
<a href="/search/eess?searchtype=author&query=Priyadarshan">Priyadarshan</a>, 
<a href="/search/eess?searchtype=author&query=Theeb%2C+N+A">Nadah Al Theeb</a>, 
<a href="/search/eess?searchtype=author&query=Dhillon%2C+P">Parveen Dhillon</a>, 
<a href="/search/eess?searchtype=author&query=Ore%2C+J+P">Jonathan P. Ore</a>, 
<a href="/search/eess?searchtype=author&query=Ziviani%2C+D">Davide Ziviani</a>, 
<a href="/search/eess?searchtype=author&query=Groll%2C+E+A">Eckhard A. Groll</a>, 
<a href="/search/eess?searchtype=author&query=Kircher%2C+K+J">Kevin J. Kircher</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Efficient electric heat pumps that replace fossil-fueled heating systems
could significantly reduce greenhouse gas emissions. However, electric heat
pumps can sharply increase electricity demand, causing high utility bills and
stressing the power grid. Residential neighborhoods could see particularly high
electricity demand during cold weather, when heat demand rises and heat pump
efficiencies fall. This paper presents the development and field demonstration
of a predictive control system for an air-to-air heat pump with backup electric
resistance heat. The control system adjusts indoor temperature set-points based
on weather forecasts, occupancy conditions, and data-driven models of the
building and heating equipment. Field tests from January to March of 2023 in an
occupied, all-electric, 208 m^2 detached single-family house in Indiana, USA,
included outdoor temperatures as low as -15 C. On average over these tests, the
control system reduced daily heating energy use by 19% (95% confidence
interval: 13--24%), energy used for backup heat by 38%, and the frequency of
using the highest stage (19 kW) of backup heat by 83%. Concurrent surveys of
residents showed that the control system maintained satisfactory thermal
comfort. The control system could reduce the house's total annual heating costs
by about $300 (95% confidence interval: 23--34%). These real-world results
could strengthen the case for deploying predictive home heating control,
bringing the technology one step closer to reducing emissions, utility bills,
and power grid impacts at scale.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07033" title="Abstract">arXiv:2402.07033</a> [<a href="/pdf/2402.07033" title="Download PDF">pdf</a>, <a href="/format/2402.07033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamahori%2C+K">Keisuke Kamahori</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yile Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K">Kan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Kasikci%2C+B">Baris Kasikci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Operating Systems (cs.OS)

</div>
<p class="mathjax">Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture
are showing promising performance on various tasks. However, running them on
resource-constrained settings, where GPU memory resources are not abundant, is
challenging due to huge model sizes. Existing systems that offload model
weights to CPU memory suffer from the significant overhead of frequently moving
data between CPU and GPU. In this paper, we propose Fiddler, a
resource-efficient inference engine with CPU-GPU orchestration for MoE models.
The key idea of Fiddler is to use the computation ability of the CPU to
minimize the data movement between the CPU and GPU. Our evaluation shows that
Fiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in
parameters, to generate over $3$ tokens per second on a single GPU with 24GB
memory, showing an order of magnitude improvement over existing methods. The
code of Fiddler is publicly available at
\url{https://github.com/efeslab/fiddler}
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07034" title="Abstract">arXiv:2402.07034</a> [<a href="/pdf/2402.07034" title="Download PDF">pdf</a>, <a href="/ps/2402.07034" title="Download PostScript">ps</a>, <a href="/format/2402.07034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Robotic Cyber-Physical System for Automated Reality Capture and  Visualization in Construction Progress Monitoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Halder%2C+S">Srijeet Halder</a>, 
<a href="/search/cs?searchtype=author&query=Afsari%2C+K">Kereshmeh Afsari</a>, 
<a href="/search/cs?searchtype=author&query=Akanmu%2C+A">Abiola Akanmu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Effective progress monitoring is crucial for the successful delivery of the
construction project within the stipulated time and budget. Construction
projects are often monitored irregularly through time-consuming physical site
visits by multiple project stakeholders. Remote monitoring using robotic
cyber-physical systems (CPS) can make the process more efficient and safer.
This article presents a conceptual framework for robotic CPS for automated
reality capture and visualization for remote progress monitoring in
construction. The CPS integrates quadruped robot, Building Information
Modelling (BIM), and 360{\deg} reality capturing to autonomously capture, and
visualize up-to-date site information. Additionally, the study explores the
factors affecting acceptance of the proposed robotic CPS through
semi-structured interviews with seventeen progress monitoring experts. The
findings will guide construction management teams in adopting CPS in
construction and drive further research in the human-centered development of
CPS for construction.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07035" title="Abstract">arXiv:2402.07035</a> [<a href="/pdf/2402.07035" title="Download PDF">pdf</a>, <a href="/format/2402.07035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distilling Symbolic Priors for Concept Learning into Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marinescu%2C+I">Ioana Marinescu</a>, 
<a href="/search/cs?searchtype=author&query=McCoy%2C+R+T">R. Thomas McCoy</a>, 
<a href="/search/cs?searchtype=author&query=Griffiths%2C+T+L">Thomas L. Griffiths</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Humans can learn new concepts from a small number of examples by drawing on
their inductive biases. These inductive biases have previously been captured by
using Bayesian models defined over symbolic hypothesis spaces. Is it possible
to create a neural network that displays the same inductive biases? We show
that inductive biases that enable rapid concept learning can be instantiated in
artificial neural networks by distilling a prior distribution from a symbolic
Bayesian model via meta-learning, an approach for extracting the common
structure from a set of tasks. By generating the set of tasks used in
meta-learning from the prior distribution of a Bayesian model, we are able to
transfer that prior into a neural network. We use this approach to create a
neural network with an inductive bias towards concepts expressed as short
logical formulas. Analyzing results from previous behavioral experiments in
which people learned logical concepts from a few examples, we find that our
meta-trained models are highly aligned with human performance.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07037" title="Abstract">arXiv:2402.07037</a> [<a href="/pdf/2402.07037" title="Download PDF">pdf</a>, <a href="/format/2402.07037" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified Inverse Dynamics of Modular Serial Mechanical Systems with  Application to Soft Robotics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pustina%2C+P">Pietro Pustina</a>, 
<a href="/search/cs?searchtype=author&query=Della+Santina%2C+C">Cosimo Della Santina</a>, 
<a href="/search/cs?searchtype=author&query=De+Luca%2C+A">Alessandro De Luca</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The robotic field has been witnessing a progressive departure from classic
robotic systems composed of serial/stiff links interconnected by simple rigid
joints. Novel robotic concepts, e.g., soft robots, often maintain a series-like
structure, but their mechanical modules exhibit complex and unconventional
articulation patterns. Research in efficient recursive formulations of the
dynamic models for subclasses of these systems has been extremely active in the
past decade. Yet, as of today, no single recursive inverse dynamics algorithm
can describe the behavior of all these systems. This paper addresses this
challenge by proposing a new iterative formulation based on Kane equations. Its
computational complexity is optimal, i.e., linear with the number of modules.
While the proposed formulation is not claimed to be necessarily more efficient
than state-of-the-art techniques for specific subclasses of robots, we
illustrate its usefulness in the modeling of different complex systems. We
propose two new models of soft robots: (i) a class of pneumatically actuated
soft arms that deform along their cross-sectional area, and (ii) a piecewise
strain model with Gaussian functions.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07038" title="Abstract">arXiv:2402.07038</a> [<a href="/pdf/2402.07038" title="Download PDF">pdf</a>, <a href="/format/2402.07038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonlinear Modes as a Tool for Comparing the Mathematical Structure of  Dynamic Models of Soft Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pustina%2C+P">Pietro Pustina</a>, 
<a href="/search/cs?searchtype=author&query=Calzolari%2C+D">Davide Calzolari</a>, 
<a href="/search/cs?searchtype=author&query=Albu-Sch%C3%A4ffer%2C+A">Alin Albu-Sch&#xe4;ffer</a>, 
<a href="/search/cs?searchtype=author&query=De+Luca%2C+A">Alessandro De Luca</a>, 
<a href="/search/cs?searchtype=author&query=Della+Santina%2C+C">Cosimo Della Santina</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Continuum soft robots are nonlinear mechanical systems with theoretically
infinite degrees of freedom (DoFs) that exhibit complex behaviors. Achieving
motor intelligence under dynamic conditions necessitates the development of
control-oriented reduced-order models (ROMs), which employ as few DoFs as
possible while still accurately capturing the core characteristics of the
theoretically infinite-dimensional dynamics. However, there is no quantitative
way to measure if the ROM of a soft robot has succeeded in this task. In other
fields, like structural dynamics or flexible link robotics, linear normal modes
are routinely used to this end. Yet, this theory is not applicable to soft
robots due to their nonlinearities. In this work, we propose to use the recent
nonlinear extension in modal theory -- called eigenmanifolds -- as a means to
evaluate control-oriented models for soft robots and compare them. To achieve
this, we propose three similarity metrics relying on the projection of the
nonlinear modes of the system into a task space of interest. We use this
approach to compare quantitatively, for the first time, ROMs of increasing
order generated under the piecewise constant curvature (PCC) hypothesis with a
high-dimensional finite element (FE)-like model of a soft arm. Results show
that by increasing the order of the discretization, the eigenmanifolds of the
PCC model converge to those of the FE model.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07039" title="Abstract">arXiv:2402.07039</a> [<a href="/pdf/2402.07039" title="Download PDF">pdf</a>, <a href="/ps/2402.07039" title="Download PostScript">ps</a>, <a href="/format/2402.07039" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coordinated Disclosure for AI: Beyond Security Vulnerabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cattell%2C+S">Sven Cattell</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+A">Avijit Ghosh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Cryptography and Security (cs.CR); Computers and Society (cs.CY)

</div>
<p class="mathjax">Harm reporting in the field of Artificial Intelligence (AI) currently
operates on an ad hoc basis, lacking a structured process for disclosing or
addressing algorithmic flaws. In contrast, the Coordinated Vulnerability
Disclosure (CVD) ethos and ecosystem play a pivotal role in software security
and transparency. Within the U.S. context, there has been a protracted legal
and policy struggle to establish a safe harbor from the Computer Fraud and
Abuse Act, aiming to foster institutional support for security researchers
acting in good faith. Notably, algorithmic flaws in Machine Learning (ML)
models present distinct challenges compared to traditional software
vulnerabilities, warranting a specialized approach. To address this gap, we
propose the implementation of a dedicated Coordinated Flaw Disclosure (CFD)
framework tailored to the intricacies of machine learning and artificial
intelligence issues. This paper delves into the historical landscape of
disclosures in ML, encompassing the ad hoc reporting of harms and the emergence
of participatory auditing. By juxtaposing these practices with the
well-established disclosure norms in cybersecurity, we argue that the broader
adoption of CFD has the potential to enhance public trust through transparent
processes that carefully balance the interests of both organizations and the
community.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07040" title="Abstract">arXiv:2402.07040</a> [<a href="/pdf/2402.07040" title="Download PDF">pdf</a>, <a href="/format/2402.07040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal convergence rates of an adaptive hybrid FEM-BEM method for  full-space linear transmission problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gantner%2C+G">Gregor Gantner</a>, 
<a href="/search/math?searchtype=author&query=Ruggeri%2C+M">Michele Ruggeri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We consider a hybrid FEM-BEM method to compute approximations of full-space
linear elliptic transmission problems. First, we derive a priori and a
posteriori error estimates. Then, building on the latter, we present an
adaptive algorithm and prove that it converges at optimal rates with respect to
the number of mesh elements. Finally, we provide numerical experiments,
demonstrating the practical performance of the adaptive algorithm.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07041" title="Abstract">arXiv:2402.07041</a> [<a href="/pdf/2402.07041" title="Download PDF">pdf</a>, <a href="/ps/2402.07041" title="Download PostScript">ps</a>, <a href="/format/2402.07041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Risk assessment and observation of driver with pedestrian using  instantaneous heart rate and HRV
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kikuta%2C+R">Riku Kikuta</a>, 
<a href="/search/cs?searchtype=author&query=Carruth%2C+D">Daniel Carruth</a>, 
<a href="/search/cs?searchtype=author&query=Ball%2C+J">John Ball</a>, 
<a href="/search/cs?searchtype=author&query=Burch%2C+R">Reuben Burch</a>, 
<a href="/search/cs?searchtype=author&query=Kageyama%2C+I">Ichiro Kageyama</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 AHFE Open Access, vol 95
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Currently, human drivers outperform self-driving vehicles in many conditions
such as collision avoidance. Therefore, understanding human driver behaviour in
these conditions will provide insight for future autonomous vehicles. For
understanding driver behaviour, risk assessment is applied so far as one of the
approaches by using both subjective and objective measurement. Subjective
measurement methods such as questionnaires may provide insight into driver risk
assessment but there is often significant variability between
drivers.Physiological measurements such as heart rate (HR),
electroencephalogram (EEG), and electromyogram (EMG) provide more objective
measurements of driver risk assessment. HR is often used for measuring driver
risk assessment based on observed correlations between HR and risk perception.
Previous work has used HR to measure driver risk assessment in self-driving
systems, but pedestrian dynamics is not considered for the research. In this
study, we observed driver behaviour in certain scenarios which have pedestrian
on driving simulator. The scenarios have safe/unsafe situations (i.e.,
pedestrian crosses road and vehicle may hit pedestrian in one scenario), HR
analysis in time/frequency domain is processed for risk assessment. As a
result, HR analysis in frequency domain shows certain reasonability for driver
risk assessment when driver has pedestrian in its traffic.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07043" title="Abstract">arXiv:2402.07043</a> [<a href="/pdf/2402.07043" title="Download PDF">pdf</a>, <a href="/format/2402.07043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Tale of Tails: Model Collapse as a Change of Scaling Laws
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dohmatob%2C+E">Elvis Dohmatob</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yunzhen Feng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+P">Pu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Charton%2C+F">Francois Charton</a>, 
<a href="/search/cs?searchtype=author&query=Kempe%2C+J">Julia Kempe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As AI model size grows, neural scaling laws have become a crucial tool to
predict the improvements of large models when increasing capacity and the size
of original (human or natural) training data. Yet, the widespread use of
popular models means that the ecosystem of online data and text will co-evolve
to progressively contain increased amounts of synthesized data. In this paper
we ask: How will the scaling laws change in the inevitable regime where
synthetic data makes its way into the training corpus? Will future models,
still improve, or be doomed to degenerate up to total (model) collapse? We
develop a theoretical framework of model collapse through the lens of scaling
laws. We discover a wide range of decay phenomena, analyzing loss of scaling,
shifted scaling with number of generations, the ''un-learning" of skills, and
grokking when mixing human and synthesized data. Our theory is validated by
large-scale experiments with a transformer on an arithmetic task and text
generation using the large language model Llama2.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07049" title="Abstract">arXiv:2402.07049</a> [<a href="/pdf/2402.07049" title="Download PDF">pdf</a>, <a href="/ps/2402.07049" title="Download PostScript">ps</a>, <a href="/format/2402.07049" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Factor Graph Model of Trust for a Collaborative Multi-Agent System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akbari%2C+B">Behzad Akbari</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+M">Mingfeng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Haibin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+J">Jinjun Shan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">In the field of Multi-Agent Systems (MAS), known for their openness,
dynamism, and cooperative nature, the ability to trust the resources and
services of other agents is crucial. Trust, in this setting, is the reliance
and confidence an agent has in the information, behaviors, intentions,
truthfulness, and capabilities of others within the system. Our paper
introduces a new graphical approach that utilizes factor graphs to represent
the interdependent behaviors and trustworthiness among agents. This includes
modeling the behavior of robots as a trajectory of actions using a Gaussian
process factor graph, which accounts for smoothness, obstacle avoidance, and
trust-related factors. Our method for evaluating trust is decentralized and
considers key interdependent sub-factors such as proximity safety, consistency,
and cooperation. The overall system comprises a network of factor graphs that
interact through trust-related factors and employs a Bayesian inference method
to dynamically assess trust-based decisions with informed consent. The
effectiveness of this method is validated via simulations and empirical tests
with autonomous robots navigating unsignalized intersections.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07051" title="Abstract">arXiv:2402.07051</a> [<a href="/pdf/2402.07051" title="Download PDF">pdf</a>, <a href="/format/2402.07051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $L^*LM$: Learning Automata from Examples using Natural Language Oracles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vazquez-Chanlatte%2C+M">Marcell Vazquez-Chanlatte</a>, 
<a href="/search/cs?searchtype=author&query=Elmaaroufi%2C+K">Karim Elmaaroufi</a>, 
<a href="/search/cs?searchtype=author&query=Witwicki%2C+S+J">Stefan J. Witwicki</a>, 
<a href="/search/cs?searchtype=author&query=Seshia%2C+S+A">Sanjit A. Seshia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">Expert demonstrations have proven an easy way to indirectly specify complex
tasks. Recent algorithms even support extracting unambiguous formal
specifications, e.g. deterministic finite automata (DFA), from demonstrations.
Unfortunately, these techniques are generally not sample efficient. In this
work, we introduce $L^*LM$, an algorithm for learning DFAs from both
demonstrations and natural language. Due to the expressivity of natural
language, we observe a significant improvement in the data efficiency of
learning DFAs from expert demonstrations. Technically, $L^*LM$ leverages large
language models to answer membership queries about the underlying task. This is
then combined with recent techniques for transforming learning from
demonstrations into a sequence of labeled example learning problems. In our
experiments, we observe the two modalities complement each other, yielding a
powerful few-shot learner.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07052" title="Abstract">arXiv:2402.07052</a> [<a href="/pdf/2402.07052" title="Download PDF">pdf</a>, <a href="/format/2402.07052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Training Speedup from Sampling with Approximate Losses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+R">Rudrajit Das</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ieong%2C+B">Bertram Ieong</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+P">Parikshit Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Sanghavi%2C+S">Sujay Sanghavi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">It is well known that selecting samples with large losses/gradients can
significantly reduce the number of training steps. However, the selection
overhead is often too high to yield any meaningful gains in terms of overall
training time. In this work, we focus on the greedy approach of selecting
samples with large \textit{approximate losses} instead of exact losses in order
to reduce the selection overhead. For smooth convex losses, we show that such a
greedy strategy can converge to a constant factor of the minimum value of the
average loss in fewer iterations than the standard approach of random
selection. We also theoretically quantify the effect of the approximation
level. We then develop SIFT which uses early exiting to obtain approximate
losses with an intermediate layer's representations for sample selection. We
evaluate SIFT on the task of training a 110M parameter 12-layer BERT base model
and show significant gains (in terms of training hours and number of
backpropagation steps) without any optimized implementation over vanilla
training. For e.g., to reach 64% validation accuracy, SIFT with exit at the
first layer takes ~43 hours compared to ~57 hours of vanilla training.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07053" title="Abstract">arXiv:2402.07053</a> [<a href="/pdf/2402.07053" title="Download PDF">pdf</a>, <a href="/ps/2402.07053" title="Download PostScript">ps</a>, <a href="/format/2402.07053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Certified homotopy tracking using the Krawczyk method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Duff%2C+T">Timothy Duff</a>, 
<a href="/search/math?searchtype=author&query=Lee%2C+K">Kisun Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 4 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Symbolic Computation (cs.SC); Algebraic Geometry (math.AG)

</div>
<p class="mathjax">We revisit the problem of certifying the correctness of approximate solution
paths computed by numerical homotopy continuation methods. We propose a
conceptually simple approach based on a parametric variant of the Krawczyk
method from interval arithmetic. Unlike most previous methods for certified
path-tracking, our approach is applicable in the general setting of parameter
homotopies commonly used to solve polynomial systems of equations. We also
describe a novel preconditioning strategy and give theoretical correctness and
termination results. Experiments using a preliminary implementation of the
method indicate that our approach is competitive with specialized methods
appearing previously in the literature, in spite of our more general setting.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07054" title="Abstract">arXiv:2402.07054</a> [<a href="/pdf/2402.07054" title="Download PDF">pdf</a>, <a href="/ps/2402.07054" title="Download PostScript">ps</a>, <a href="/format/2402.07054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HNMblock: Blockchain technology powered Healthcare Network Model for  epidemiological monitoring, medical systems security, and wellness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kshetri%2C+N">Naresh Kshetri</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+R">Rahul Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+M">Mir Mehedi Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Steigner%2C+T">Tanja Steigner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">In the ever-evolving healthcare sector, the widespread adoption of Internet
of Things and wearable technologies facilitates remote patient monitoring.
However, the existing client/server infrastructure poses significant security
and privacy challenges, necessitating strict adherence to healthcare data
regulations. To combat these issues, a decentralized approach is imperative,
and blockchain technology emerges as a compelling solution for strengthening
Internet of Things and medical systems security. This paper introduces
HNMblock, a model that elevates the realms of epidemiological monitoring,
medical system security, and wellness enhancement. By harnessing the
transparency and immutability inherent in blockchain, HNMblock empowers
real-time, tamper-proof tracking of epidemiological data, enabling swift
responses to disease outbreaks. Furthermore, it fortifies the security of
medical systems through advanced cryptographic techniques and smart contracts,
with a paramount focus on safeguarding patient privacy. HNMblock also fosters
personalized healthcare, encouraging patient involvement and data-informed
decision-making. The integration of blockchain within the healthcare domain, as
exemplified by HNMblock, holds the potential to revolutionize data management,
epidemiological surveillance, and wellness, as meticulously explored in this
research article.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07059" title="Abstract">arXiv:2402.07059</a> [<a href="/pdf/2402.07059" title="Download PDF">pdf</a>, <a href="/format/2402.07059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Domain Adaptable Fine-Tune Distillation Framework For Advancing Farm  Surveillance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Imam%2C+R">Raza Imam</a>, 
<a href="/search/cs?searchtype=author&query=Huzaifa%2C+M">Muhammad Huzaifa</a>, 
<a href="/search/cs?searchtype=author&query=Mansour%2C+N">Nabil Mansour</a>, 
<a href="/search/cs?searchtype=author&query=Mirza%2C+S+B">Shaher Bano Mirza</a>, 
<a href="/search/cs?searchtype=author&query=Lamghari%2C+F">Fouad Lamghari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this study, we propose an automated framework for camel farm monitoring,
introducing two key contributions: the Unified Auto-Annotation framework and
the Fine-Tune Distillation framework. The Unified Auto-Annotation approach
combines two models, GroundingDINO (GD), and Segment-Anything-Model (SAM), to
automatically annotate raw datasets extracted from surveillance videos.
Building upon this foundation, the Fine-Tune Distillation framework conducts
fine-tuning of student models using the auto-annotated dataset. This process
involves transferring knowledge from a large teacher model to a student model,
resembling a variant of Knowledge Distillation. The Fine-Tune Distillation
framework aims to be adaptable to specific use cases, enabling the transfer of
knowledge from the large models to the small models, making it suitable for
domain-specific applications. By leveraging our raw dataset collected from
Al-Marmoom Camel Farm in Dubai, UAE, and a pre-trained teacher model,
GroundingDINO, the Fine-Tune Distillation framework produces a lightweight
deployable model, YOLOv8. This framework demonstrates high performance and
computational efficiency, facilitating efficient real-time object detection.
Our code is available at
\href{https://github.com/Razaimam45/Fine-Tune-Distillation}{https://github.com/Razaimam45/Fine-Tune-Distillation}
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07060" title="Abstract">arXiv:2402.07060</a> [<a href="/pdf/2402.07060" title="Download PDF">pdf</a>, <a href="/format/2402.07060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral convergence of a semi-discretized numerical system for the  Boltzmann equation with uncertainties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liu%2C+L">Liu Liu</a>, 
<a href="/search/math?searchtype=author&query=Qi%2C+K">Kunlun Qi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2212.04083">arXiv:2212.04083</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
<p class="mathjax">In this paper, we study the Boltzmann equation with uncertainties and prove
that the spectral convergence of the semi-discretized numerical system holds in
a combined velocity and random space, where the Fourier-spectral method is
applied for approximation in the velocity space whereas the generalized
polynomial chaos (gPC)-based stochastic Galerkin (SG) method is employed to
discretize the random variable. Our proof is based on a delicate energy
estimate for showing the well-posedness of the numerical solution as well as a
rigorous control of its negative part in our well-designed functional space
that involves high-order derivatives of both the velocity and random variables.
This paper rigorously justifies the statement proposed in [Remark 4.4, J. Hu
and S. Jin, J. Comput. Phys., 315 (2016), pp. 150-168].
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07061" title="Abstract">arXiv:2402.07061</a> [<a href="/pdf/2402.07061" title="Download PDF">pdf</a>, <a href="/format/2402.07061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The $k$-Opt algorithm for the Traveling Salesman Problem has exponential  running time for $k \ge 5$
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heimann%2C+S">Sophia Heimann</a>, 
<a href="/search/cs?searchtype=author&query=Hoang%2C+H+P">Hung P. Hoang</a>, 
<a href="/search/cs?searchtype=author&query=Hougardy%2C+S">Stefan Hougardy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
<p class="mathjax">The $k$-Opt algorithm is a local search algorithm for the Traveling Salesman
Problem. Starting with an initial tour, it iteratively replaces at most $k$
edges in the tour with the same number of edges to obtain a better tour.
Krentel (FOCS 1989) showed that the Traveling Salesman Problem with the $k$-Opt
neighborhood is complete for the class PLS (polynomial time local search) and
that the $k$-Opt algorithm can have exponential running time for any pivot
rule. However, his proof requires $k \gg 1000$ and has a substantial gap. We
show the two properties above for a much smaller value of $k$, addressing an
open question by Monien, Dumrauf, and Tscheuschner (ICALP 2010). In particular,
we prove the PLS-completeness for $k \geq 17$ and the exponential running time
for $k \geq 5$.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07062" title="Abstract">arXiv:2402.07062</a> [<a href="/pdf/2402.07062" title="Download PDF">pdf</a>, <a href="/format/2402.07062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast UCB-type algorithms for stochastic bandits with heavy and super  heavy symmetric noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dorn%2C+Y">Yuriy Dorn</a>, 
<a href="/search/cs?searchtype=author&query=Katrutsa%2C+A">Aleksandr Katrutsa</a>, 
<a href="/search/cs?searchtype=author&query=Latypov%2C+I">Ilgam Latypov</a>, 
<a href="/search/cs?searchtype=author&query=Pudovikov%2C+A">Andrey Pudovikov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">In this study, we propose a new method for constructing UCB-type algorithms
for stochastic multi-armed bandits based on general convex optimization methods
with an inexact oracle. We derive the regret bounds corresponding to the
convergence rates of the optimization methods. We propose a new algorithm
Clipped-SGD-UCB and show, both theoretically and empirically, that in the case
of symmetric noise in the reward, we can achieve an $O(\log T\sqrt{KT\log T})$
regret bound instead of $O\left (T^{\frac{1}{1+\alpha}}
K^{\frac{\alpha}{1+\alpha}} \right)$ for the case when the reward distribution
satisfies $\mathbb{E}_{X \in D}[|X|^{1+\alpha}] \leq \sigma^{1+\alpha}$
($\alpha \in (0, 1])$, i.e. perform better than it is assumed by the general
lower bound for bandits with heavy-tails. Moreover, the same bound holds even
when the reward distribution does not have the expectation, that is, when
$\alpha&lt;0$.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07065" title="Abstract">arXiv:2402.07065</a> [<a href="/pdf/2402.07065" title="Download PDF">pdf</a>, <a href="/format/2402.07065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CAHSOR: Competence-Aware High-Speed Off-Road Ground Navigation in SE(3)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pokhrel%2C+A">Anuj Pokhrel</a>, 
<a href="/search/cs?searchtype=author&query=Datar%2C+A">Aniket Datar</a>, 
<a href="/search/cs?searchtype=author&query=Nazeri%2C+M">Mohammad Nazeri</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+X">Xuesu Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">While the workspace of traditional ground vehicles is usually assumed to be
in a 2D plane, i.e., SE(2), such an assumption may not hold when they drive at
high speeds on unstructured off-road terrain: High-speed sharp turns on
high-friction surfaces may lead to vehicle rollover; Turning aggressively on
loose gravel or grass may violate the non-holonomic constraint and cause
significant lateral sliding; Driving quickly on rugged terrain will produce
extensive vibration along the vertical axis. Therefore, most offroad vehicles
are currently limited to drive only at low speeds to assure vehicle stability
and safety. In this work, we aim at empowering high-speed off-road vehicles
with competence awareness in SE(3) so that they can reason about the
consequences of taking aggressive maneuvers on different terrain with a 6-DoF
forward kinodynamic model. The model is learned from visual and inertial
Terrain Representation for Off-road Navigation (TRON) using multimodal,
self-supervised vehicle-terrain interactions. We demonstrate the efficacy of
our Competence-Aware High-Speed Off-Road (CAHSOR) navigation approach on a
physical ground robot in both an autonomous navigation and a human
shared-control setup and show that CAHSOR can efficiently reduce vehicle
instability by 62% while only compromising 8.6% average speed with the help of
TRON.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07066" title="Abstract">arXiv:2402.07066</a> [<a href="/pdf/2402.07066" title="Download PDF">pdf</a>, <a href="/format/2402.07066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Range Queries with Correlated Input Perturbation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dharangutte%2C+P">Prathamesh Dharangutte</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jie Gao</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+R">Ruobin Gong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guanyang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">This work proposes a class of locally differentially private mechanisms for
linear queries, in particular range queries, that leverages correlated input
perturbation to simultaneously achieve unbiasedness, consistency, statistical
transparency, and control over utility requirements in terms of accuracy
targets expressed either in certain query margins or as implied by the
hierarchical database structure. The proposed Cascade Sampling algorithm
instantiates the mechanism exactly and efficiently. Our bounds show that we
obtain near-optimal utility while being empirically competitive against output
perturbation methods.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07067" title="Abstract">arXiv:2402.07067</a> [<a href="/pdf/2402.07067" title="Download PDF">pdf</a>, <a href="/format/2402.07067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning the Expected Core of Strictly Convex Stochastic Cooperative  Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tran%2C+N+P">Nam Phuong Tran</a>, 
The <a href="/search/cs?searchtype=author&query=Ta%2C+A">Anh Ta</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Shuqing Shi</a>, 
<a href="/search/cs?searchtype=author&query=Mandal%2C+D">Debmalya Mandal</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yali Du</a>, 
<a href="/search/cs?searchtype=author&query=Tran-Thanh%2C+L">Long Tran-Thanh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Reward allocation, also known as the credit assignment problem, has been an
important topic in economics, engineering, and machine learning. An important
concept in credit assignment is the core, which is the set of stable
allocations where no agent has the motivation to deviate from the grand
coalition. In this paper, we consider the stable allocation learning problem of
stochastic cooperative games, where the reward function is characterised as a
random variable with an unknown distribution. Given an oracle that returns a
stochastic reward for an enquired coalition each round, our goal is to learn
the expected core, that is, the set of allocations that are stable in
expectation. Within the class of strictly convex games, we present an algorithm
named \texttt{Common-Points-Picking} that returns a stable allocation given a
polynomial number of samples, with high probability. The analysis of our
algorithm involves the development of several new results in convex geometry,
including an extension of the separation hyperplane theorem for multiple convex
sets, and may be of independent interest.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07069" title="Abstract">arXiv:2402.07069</a> [<a href="/pdf/2402.07069" title="Download PDF">pdf</a>, <a href="/format/2402.07069" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Large Language Models to Automate and Expedite Reinforcement  Learning with Reward Machine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alsadat%2C+S+M">Shayan Meshkat Alsadat</a>, 
<a href="/search/cs?searchtype=author&query=Gaglione%2C+J">Jean-Raphael Gaglione</a>, 
<a href="/search/cs?searchtype=author&query=Neider%2C+D">Daniel Neider</a>, 
<a href="/search/cs?searchtype=author&query=Topcu%2C+U">Ufuk Topcu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhe Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">We present LARL-RM (Large language model-generated Automaton for
Reinforcement Learning with Reward Machine) algorithm in order to encode
high-level knowledge into reinforcement learning using automaton to expedite
the reinforcement learning. Our method uses Large Language Models (LLM) to
obtain high-level domain-specific knowledge using prompt engineering instead of
providing the reinforcement learning algorithm directly with the high-level
knowledge which requires an expert to encode the automaton. We use
chain-of-thought and few-shot methods for prompt engineering and demonstrate
that our method works using these approaches. Additionally, LARL-RM allows for
fully closed-loop reinforcement learning without the need for an expert to
guide and supervise the learning since LARL-RM can use the LLM directly to
generate the required high-level knowledge for the task at hand. We also show
the theoretical guarantee of our algorithm to converge to an optimal policy. We
demonstrate that LARL-RM speeds up the convergence by 30% by implementing our
method in two case studies.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07071" title="Abstract">arXiv:2402.07071</a> [<a href="/pdf/2402.07071" title="Download PDF">pdf</a>, <a href="/ps/2402.07071" title="Download PostScript">ps</a>, <a href="/format/2402.07071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling of Key Quality Indicators for End-to-End Network Management:  Preparing for 5G
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Herrera-Garcia%2C+A">Ana Herrera-Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Fortes%2C+S">Sergio Fortes</a>, 
<a href="/search/cs?searchtype=author&query=Baena%2C+E">Eduardo Baena</a>, 
<a href="/search/cs?searchtype=author&query=Mendoza%2C+J">Jessica Mendoza</a>, 
<a href="/search/cs?searchtype=author&query=Baena%2C+C">Carlos Baena</a>, 
<a href="/search/cs?searchtype=author&query=Barco%2C+R">Raquel Barco</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Vehicular Technology Magazine, vol. 14, no. 4, pp. 76-84,
  Dec. 2019
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Thanks to evolving cellular telecommunication networks, providers can deploy
a wide range of services. Soon, 5G mobile networks will be available to handle
all types of services and applications for vast numbers of users through their
mobile equipment. To effectively manage new 5G systems, end-to-end (E2E)
performance analysis and optimization will be key features. However, estimating
the end-user experience is not an easy task for network operators. The amount
of end-user performance information operators can measure from the network is
limited, complicating this approach. Here we explore the calculation of service
metrics [known as key quality indicators (KQIs)] from classic low-layer
measurements and parameters. We propose a complete machine-learning (ML)
modeling framework. This system's low-layer metrics can be applied to measure
service-layer performance. To assess the approach, we implemented and evaluated
the proposed system on a real cellular network testbed.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07076" title="Abstract">arXiv:2402.07076</a> [<a href="/pdf/2402.07076" title="Download PDF">pdf</a>, <a href="/format/2402.07076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Multi-field B2B Cloud Solution Matching via Contrastive  Pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haonan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Z">Zhicheng Dou</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+X">Xuetong Hao</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Y">Yunhao Tao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shiren Song</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+Z">Zhenli Sheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Cloud solutions have gained significant popularity in the technology industry
as they offer a combination of services and tools to tackle specific problems.
However, despite their widespread use, the task of identifying appropriate
company customers for a specific target solution to the sales team of a
solution provider remains a complex business problem that existing matching
systems have yet to adequately address. In this work, we study the B2B solution
matching problem and identify two main challenges of this scenario: (1) the
modeling of complex multi-field features and (2) the limited, incomplete, and
sparse transaction data. To tackle these challenges, we propose a framework
CAMA, which is built with a hierarchical multi-field matching structure as its
backbone and supplemented by three data augmentation strategies and a
contrastive pre-training objective to compensate for the imperfections in the
available data. Through extensive experiments on a real-world dataset, we
demonstrate that CAMA outperforms several strong baseline matching models
significantly. Furthermore, we have deployed our matching framework on a system
of Huawei Cloud. Our observations indicate an improvement of about 30% compared
to the previous online model in terms of Conversion Rate (CVR), which
demonstrates its great business value.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07079" title="Abstract">arXiv:2402.07079</a> [<a href="/pdf/2402.07079" title="Download PDF">pdf</a>, <a href="/format/2402.07079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Relevance Feature and Vector Machine for health applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belenguer-Llorens%2C+A">Albert Belenguer-Llorens</a>, 
<a href="/search/cs?searchtype=author&query=Sevilla-Salcedo%2C+C">Carlos Sevilla-Salcedo</a>, 
<a href="/search/cs?searchtype=author&query=Parrado-Hern%C3%A1ndez%2C+E">Emilio Parrado-Hern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B3mez-Verdejo%2C+V">Vanessa G&#xf3;mez-Verdejo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages of main text, 12 pages of appendices, 2 figures and 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper presents the Relevance Feature and Vector Machine (RFVM), a novel
model that addresses the challenges of the fat-data problem when dealing with
clinical prospective studies. The fat-data problem refers to the limitations of
Machine Learning (ML) algorithms when working with databases in which the
number of features is much larger than the number of samples (a common scenario
in certain medical fields). To overcome such limitations, the RFVM incorporates
different characteristics: (1) A Bayesian formulation which enables the model
to infer its parameters without overfitting thanks to the Bayesian model
averaging. (2) A joint optimisation that overcomes the limitations arising from
the fat-data characteristic by simultaneously including the variables that
define the primal space (features) and those that define the dual space
(observations). (3) An integrated prunning that removes the irrelevant features
and samples during the training iterative optimization. Also, this last point
turns out crucial when performing medical prospective studies, enabling
researchers to exclude unnecessary medical tests, reducing costs and
inconvenience for patients, and identifying the critical patients/subjects that
characterize the disorder and, subsequently, optimize the patient recruitment
process that leads to a balanced cohort. The model capabilities are tested
against state-of-the-art models in several medical datasets with fat-data
problems. These experimental works show that RFVM is capable of achieving
competitive classification accuracies while providing the most compact subset
of data (in both terms of features and samples). Moreover, the selected
features (medical tests) seem to be aligned with the existing medical
literature.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07081" title="Abstract">arXiv:2402.07081</a> [<a href="/pdf/2402.07081" title="Download PDF">pdf</a>, <a href="/format/2402.07081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Large Language Models for Student-Code Guided Test Case Generation  in Computer Science Education
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+N+A">Nischal Ashok Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+A">Andrew Lan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Oral Presentation at AI4ED workshop at AAAI-2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">In computer science education, test cases are an integral part of programming
assignments since they can be used as assessment items to test students'
programming knowledge and provide personalized feedback on student-written
code. The goal of our work is to propose a fully automated approach for test
case generation that can accurately measure student knowledge, which is
important for two reasons. First, manually constructing test cases requires
expert knowledge and is a labor-intensive process. Second, developing test
cases for students, especially those who are novice programmers, is
significantly different from those oriented toward professional-level software
developers. Therefore, we need an automated process for test case generation to
assess student knowledge and provide feedback. In this work, we propose a large
language model-based approach to automatically generate test cases and show
that they are good measures of student knowledge, using a publicly available
dataset that contains student-written Java code. We also discuss future
research directions centered on using test cases to help students.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07082" title="Abstract">arXiv:2402.07082</a> [<a href="/pdf/2402.07082" title="Download PDF">pdf</a>, <a href="/ps/2402.07082" title="Download PostScript">ps</a>, <a href="/format/2402.07082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Refined Sample Complexity for Markov Games with Independent Linear  Function Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yan Dai</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Q">Qiwen Cui</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+S+S">Simon S. Du</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT); Machine Learning (stat.ML)

</div>
<p class="mathjax">Markov Games (MG) is an important model for Multi-Agent Reinforcement
Learning (MARL). It was long believed that the "curse of multi-agents" (i.e.,
the algorithmic performance drops exponentially with the number of agents) is
unavoidable until several recent works (Daskalakis et al., 2023; Cui et al.,
2023; Wang et al., 2023. While these works did resolve the curse of
multi-agents, when the state spaces are prohibitively large and (linear)
function approximations are deployed, they either had a slower convergence rate
of $O(T^{-1/4})$ or brought a polynomial dependency on the number of actions
$A_{\max}$ -- which is avoidable in single-agent cases even when the loss
functions can arbitrarily vary with time (Dai et al., 2023). This paper first
refines the `AVLPR` framework by Wang et al. (2023), with an insight of
*data-dependent* (i.e., stochastic) pessimistic estimation of the
sub-optimality gap, allowing a broader choice of plug-in algorithms. When
specialized to MGs with independent linear function approximations, we propose
novel *action-dependent bonuses* to cover occasionally extreme estimation
errors. With the help of state-of-the-art techniques from the single-agent RL
literature, we give the first algorithm that tackles the curse of multi-agents,
attains the optimal $O(T^{-1/2})$ convergence rate, and avoids
$\text{poly}(A_{\max})$ dependency simultaneously.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07083" title="Abstract">arXiv:2402.07083</a> [<a href="/pdf/2402.07083" title="Download PDF">pdf</a>, <a href="/format/2402.07083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Highlight Removal Method for Capsule Endoscopy Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaojie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yinghui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Peixuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jinlong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+T">Tao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Liangyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingfeng Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The images captured by Wireless Capsule Endoscopy (WCE) always exhibit
specular reflections, and removing highlights while preserving the color and
texture in the region remains a challenge. To address this issue, this paper
proposes a highlight removal method for capsule endoscopy images. Firstly, the
confidence and feature terms of the highlight region's edges are computed,
where confidence is obtained by the ratio of known pixels in the RGB space's R
channel to the B channel within a window centered on the highlight region's
edge pixel, and feature terms are acquired by multiplying the gradient vector
of the highlight region's edge pixel with the iso-intensity line. Subsequently,
the confidence and feature terms are assigned different weights and summed to
obtain the priority of all highlight region's edge pixels, and the pixel with
the highest priority is identified. Then, the variance of the highlight
region's edge pixels is used to adjust the size of the sample block window, and
the best-matching block is searched in the known region based on the RGB color
similarity and distance between the sample block and the window centered on the
pixel with the highest priority. Finally, the pixels in the best-matching block
are copied to the highest priority highlight removal region to achieve the goal
of removing the highlight region. Experimental results demonstrate that the
proposed method effectively removes highlights from WCE images, with a lower
coefficient of variation in the highlight removal region compared to the
Crinimisi algorithm and DeepGin method. Additionally, the color and texture in
the highlight removal region are similar to those in the surrounding areas, and
the texture is continuous.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07084" title="Abstract">arXiv:2402.07084</a> [<a href="/pdf/2402.07084" title="Download PDF">pdf</a>, <a href="/format/2402.07084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodPy: a Python library for numerics, machine learning, and statistics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=LeFloch%2C+P+G">Philippe G. LeFloch</a>, 
<a href="/search/math?searchtype=author&query=Mercier%2C+J">Jean-Marc Mercier</a>, 
<a href="/search/math?searchtype=author&query=Miryusupov%2C+S">Shohruh Miryusupov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 135 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Statistics Theory (math.ST)

</div>
<p class="mathjax">This monograph offers an introduction to a collection of numerical algorithms
implemented in the library CodPy (an acronym that stands for the Curse Of
Dimensionality in PYthon), which has found widespread applications across
various areas, including machine learning, statistics, and computational
physics. We develop here a strategy based on the theory of reproducing kernel
Hilbert spaces (RKHS) and the theory of optimal transport. Initially designed
for mathematical finance, this library has since been enhanced and broadened to
be applicable to problems arising in engineering and industry. In order to
present the general principles and techniques employed in CodPy and its
applications, we have structured this monograph into two main parts. First of
all, we focus on the fundamental principles of kernel-based representations of
data and solutions, also that the presentation therein is supplemented with
illustrative examples only. Next, we discuss the application of these
principles to many classes of concrete problems, spanning from the numerical
approximation of partial differential equations to (supervised, unsupervised)
machine learning, extending to generative methods with a focus on stochastic
aspects.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07085" title="Abstract">arXiv:2402.07085</a> [<a href="/pdf/2402.07085" title="Download PDF">pdf</a>, <a href="/format/2402.07085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speech Rhythm-Based Speaker Embeddings Extraction from Phonemes and  Phoneme Duration for Multi-Speaker Speech Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fujita%2C+K">Kenichi Fujita</a>, 
<a href="/search/cs?searchtype=author&query=Ando%2C+A">Atsushi Ando</a>, 
<a href="/search/cs?searchtype=author&query=Ijima%2C+Y">Yusuke Ijima</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages,9 figures, Accepted to IEICE TRANSACTIONS on Information and Systems
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEICE TRANSACTIONS on Information and Systems 107.1 (2024): 93-104
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This paper proposes a speech rhythm-based method for speaker embeddings to
model phoneme duration using a few utterances by the target speaker. Speech
rhythm is one of the essential factors among speaker characteristics, along
with acoustic features such as F0, for reproducing individual utterances in
speech synthesis. A novel feature of the proposed method is the rhythm-based
embeddings extracted from phonemes and their durations, which are known to be
related to speaking rhythm. They are extracted with a speaker identification
model similar to the conventional spectral feature-based one. We conducted
three experiments, speaker embeddings generation, speech synthesis with
generated embeddings, and embedding space analysis, to evaluate the
performance. The proposed method demonstrated a moderate speaker identification
performance (15.2% EER), even with only phonemes and their duration
information. The objective and subjective evaluation results demonstrated that
the proposed method can synthesize speech with speech rhythm closer to the
target speaker than the conventional method. We also visualized the embeddings
to evaluate the relationship between the distance of the embeddings and the
perceptual similarity. The visualization of the embedding space and the
relation analysis between the closeness indicated that the distribution of
embeddings reflects the subjective and objective similarity.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07087" title="Abstract">arXiv:2402.07087</a> [<a href="/pdf/2402.07087" title="Download PDF">pdf</a>, <a href="/format/2402.07087" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Correcting Self-Consuming Loops for Generative Model Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gillman%2C+N">Nate Gillman</a>, 
<a href="/search/cs?searchtype=author&query=Freeman%2C+M">Michael Freeman</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+D">Daksh Aggarwal</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+C">Chia-Hong Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+C">Calvin Luo</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yonglong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Chen Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under submission. Code will be released at <a href="https://nategillman.com/sc-sc.html">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">As synthetic data becomes higher quality and proliferates on the internet,
machine learning models are increasingly trained on a mix of human- and
machine-generated data. Despite the successful stories of using synthetic data
for representation learning, using synthetic data for generative model training
creates "self-consuming loops" which may lead to training instability or even
collapse, unless certain conditions are met. Our paper aims to stabilize
self-consuming generative model training. Our theoretical results demonstrate
that by introducing an idealized correction function, which maps a data point
to be more likely under the true data distribution, self-consuming loops can be
made exponentially more stable. We then propose self-correction functions,
which rely on expert knowledge (e.g. the laws of physics programmed in a
simulator), and aim to approximate the idealized corrector automatically and at
scale. We empirically validate the effectiveness of self-correcting
self-consuming loops on the challenging human motion synthesis task, and
observe that it successfully avoids model collapse, even when the ratio of
synthetic data to real data is as high as 100%.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07090" title="Abstract">arXiv:2402.07090</a> [<a href="/pdf/2402.07090" title="Download PDF">pdf</a>, <a href="/ps/2402.07090" title="Download PostScript">ps</a>, <a href="/format/2402.07090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design of a W-band High-PAE Class A&amp;AB Power Amplifier in 150nm GaAs  Technology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leea%2C+J+Y">Jun Yan Leea</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Duo Wu</a>, 
<a href="/search/cs?searchtype=author&query=Guoc%2C+X">Xuanrui Guoc</a>, 
<a href="/search/cs?searchtype=author&query=Ariannejad%2C+M+M">Mohammad Mahdi Ariannejad</a>, 
<a href="/search/cs?searchtype=author&query=Bhuiyan%2C+M+A+S">Mohammad Arif Sobhan Bhuiyan</a>, 
<a href="/search/cs?searchtype=author&query=Miraz%2C+M+H">Mahdi H. Miraz</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Transactions on Electrical and Electronic Materials (TEEM),
  Electronic ISSN: 2092-7592, Print ISSN: 1229-7607, 10th February 2024,
  Available: https://link.springer.com/article/10.1007/s42341-024-00513-8
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Emerging Technologies (cs.ET); Signal Processing (eess.SP)

</div>
<p class="mathjax">Nanometer scale power amplifiers (PA) at sub-THz suffer from severe parasitic
effects that lead to experience limited maximum frequency and reduced power
performance at the device transceiver front end. The integrated circuits
researchers proposed different PA design architecture combinations at scaled
down technologies to overcome these limitations. Although the designs meet the
minimum requirements, the power added efficiency (PAE) of PA is still quite
low. In this paper, a W-band single-ended common-source (CS) and cascode
integrated 3-stage 2-way PA design is proposed. The design integrated different
key design methodologies to mitigate the parasitic; such as combined Class AB
and Class A stages for gain-boosting and efficiency enhancement, Wilkinson
power combiner for higher output power, linearity, and bandwidth, and
transmission line (TL)-based wide band matching network for better inter-stage
matching and compact size. The proposed PA design is validated using UMS 150-nm
GaAs pHEMT using advanced design system (ADS) simulator. The results show that
the proposed PA achieved a gain of 20.1 dB, an output power of 17.2 dBm, a PAE
of 33 % and a 21 GHz bandwidth at 90 GHz Sub-THz band. The PA layout consumes
only 5.66 X 2.51 mm2 die space including pads. Our proposed PA design will
boost the research on sub-THz integrated circuits research and will smooth the
wide spread adoption of 6G in near future.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07092" title="Abstract">arXiv:2402.07092</a> [<a href="/pdf/2402.07092" title="Download PDF">pdf</a>, <a href="/format/2402.07092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizing Conversational Dense Retrieval via LLM-Cognition Data  Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haonan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Z">Zhicheng Dou</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+K">Kelong Mao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiongnan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Ziliang Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Conversational search utilizes muli-turn natural language contexts to
retrieve relevant passages. Existing conversational dense retrieval models
mostly view a conversation as a fixed sequence of questions and responses,
overlooking the severe data sparsity problem -- that is, users can perform a
conversation in various ways, and these alternate conversations are unrecorded.
Consequently, they often struggle to generalize to diverse conversations in
real-world scenarios. In this work, we propose a framework for generalizing
Conversational dense retrieval via LLM-cognition data Augmentation (ConvAug).
ConvAug first generates multi-level augmented conversations to capture the
diverse nature of conversational contexts. Inspired by human cognition, we
devise a cognition-aware process to mitigate the generation of false positives,
false negatives, and hallucinations. Moreover, we develop a difficulty-adaptive
sample filter that selects challenging samples for complex conversations,
thereby giving the model a larger learning space. A contrastive learning
objective is then employed to train a better conversational context encoder.
Extensive experiments conducted on four public datasets, under both normal and
zero-shot settings, demonstrate the effectiveness, generalizability, and
applicability of ConvAug.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07095" title="Abstract">arXiv:2402.07095</a> [<a href="/pdf/2402.07095" title="Download PDF">pdf</a>, <a href="/format/2402.07095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does ChatGPT and Whisper Make Humanoid Robots More Relatable?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaohui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+K">Katherine Luo</a>, 
<a href="/search/cs?searchtype=author&query=Gee%2C+T">Trevor Gee</a>, 
<a href="/search/cs?searchtype=author&query=Nejati%2C+M">Mahla Nejati</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Australasian Conference on Robotics and Automation (ACRA 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Humanoid robots are designed to be relatable to humans for applications such
as customer support and helpdesk services. However, many such systems,
including Softbank's Pepper, fall short because they fail to communicate
effectively with humans. The advent of Large Language Models (LLMs) shows the
potential to solve the communication barrier for humanoid robotics. This paper
outlines the comparison of different Automatic Speech Recognition (ASR) APIs,
the integration of Whisper ASR and ChatGPT with the Pepper robot and the
evaluation of the system (Pepper-GPT) tested by 15 human users. The comparison
result shows that, compared to the Google ASR and Google Cloud ASR, the Whisper
ASR performed best as its average Word Error Rate (1.716%) and processing time
(2.639 s) are both the lowest. The participants' usability investigations show
that 60% of the participants thought the performance of the Pepper-GPT was
"excellent", while the rest rated this system as "good" in the subsequent
experiments. It is proved that while some problems still need to be overcome,
such as the robot's multilingual ability and facial tracking capacity, users
generally responded positively to the system, feeling like talking to an actual
human.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07098" title="Abstract">arXiv:2402.07098</a> [<a href="/pdf/2402.07098" title="Download PDF">pdf</a>, <a href="/format/2402.07098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Pallet Detection Using Synthetic Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gann%2C+H">Henry Gann</a>, 
<a href="/search/cs?searchtype=author&query=Bull%2C+J">Josiah Bull</a>, 
<a href="/search/cs?searchtype=author&query=Gee%2C+T">Trevor Gee</a>, 
<a href="/search/cs?searchtype=author&query=Nejati%2C+M">Mahla Nejati</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Australasian Conference on Robotics and Automation (ACRA 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The use of synthetic data in machine learning saves a significant amount of
time when implementing an effective object detector. However, there is limited
research in this domain. This study aims to improve upon previously applied
implementations in the task of instance segmentation of pallets in a warehouse
environment. This study proposes using synthetically generated
domain-randomised data as well as data generated through Unity to achieve this.
This study achieved performance improvements on the stacked and racked pallet
categories by 69% and 50% mAP50, respectively when being evaluated on real
data. Additionally, it was found that there was a considerable impact on the
performance of a model when it was evaluated against images in a darker
environment, dropping as low as 3% mAP50 when being evaluated on images with an
80% brightness reduction. This study also created a two-stage detector that
used YOLOv8 and SAM, but this proved to have unstable performance. The use of
domain-randomised data proved to have negligible performance improvements when
compared to the Unity-generated data.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07099" title="Abstract">arXiv:2402.07099</a> [<a href="/pdf/2402.07099" title="Download PDF">pdf</a>, <a href="/format/2402.07099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking the Capacity of Graph Neural Networks for Branching Strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ziang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jialin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaohan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinshang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+W">Wotao Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Graph neural networks (GNNs) have been widely used to predict properties and
heuristics of mixed-integer linear programs (MILPs) and hence accelerate MILP
solvers. This paper investigates the capacity of GNNs to represent strong
branching (SB) scores that provide an efficient strategy in the
branch-and-bound algorithm.
<br />Although message-passing GNN (MP-GNN), as the simplest GNN structure, is
frequently employed in the existing literature to learn SB scores, we prove a
fundamental limitation in its expressive power -- there exist two MILP
instances with different SB scores that cannot be distinguished by any MP-GNN,
regardless of the number of parameters. In addition, we establish a universal
approximation theorem for another GNN structure called the second-order
folklore GNN (2-FGNN). We show that for any data distribution over MILPs, there
always exists a 2-FGNN that can approximate the SB score with arbitrarily high
accuracy and arbitrarily high probability. A small-scale numerical experiment
is conducted to directly validate our theoretical findings.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07102" title="Abstract">arXiv:2402.07102</a> [<a href="/pdf/2402.07102" title="Download PDF">pdf</a>, <a href="/format/2402.07102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Future Prediction Can be a Strong Evidence of Good History  Representation in Partially Observable Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kwon%2C+J">Jeongyeol Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Nowak%2C+R">Robert Nowak</a>, 
<a href="/search/cs?searchtype=author&query=Hanna%2C+J">Josiah Hanna</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Learning a good history representation is one of the core challenges of
reinforcement learning (RL) in partially observable environments. Recent works
have shown the advantages of various auxiliary tasks for facilitating
representation learning. However, the effectiveness of such auxiliary tasks has
not been fully convincing, especially in partially observable environments that
require long-term memorization and inference. In this empirical study, we
investigate the effectiveness of future prediction for learning the
representations of histories, possibly of extensive length, in partially
observable environments. We first introduce an approach that decouples the task
of learning history representations from policy optimization via future
prediction. Then, our main contributions are two-fold: (a) we demonstrate that
the performance of reinforcement learning is strongly correlated with the
prediction accuracy of future observations in partially observable
environments, and (b) our approach can significantly improve the overall
end-to-end approach by preventing high-variance noisy signals from
reinforcement learning objectives to influence the representation learning. We
illustrate our claims on three types of benchmarks that necessitate the ability
to process long histories for high returns.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07104" title="Abstract">arXiv:2402.07104</a> [<a href="/pdf/2402.07104" title="Download PDF">pdf</a>, <a href="/ps/2402.07104" title="Download PostScript">ps</a>, <a href="/format/2402.07104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Aleph &amp; Other Metaphors for Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramos%2C+G">Gonzalo Ramos</a>, 
<a href="/search/cs?searchtype=author&query=Barraza%2C+R">Rick Barraza</a>, 
<a href="/search/cs?searchtype=author&query=Dibia%2C+V">Victor Dibia</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+S">Sharon Lo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">In this position paper, we reflect on fictional stories dealing with the
infinite and how they connect with the current, fast-evolving field of image
generation models. We draw attention to how some of these literary constructs
can serve as powerful metaphors for guiding human-centered design and technical
thinking in the space of these emerging technologies and the experiences we
build around them. We hope our provocations seed conversations about current
and yet-to-be developed interactions with these emerging models in ways that
may amplify human agency.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07107" title="Abstract">arXiv:2402.07107</a> [<a href="/pdf/2402.07107" title="Download PDF">pdf</a>, <a href="/format/2402.07107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stutts%2C+A+C">Alex Christopher Stutts</a>, 
<a href="/search/cs?searchtype=author&query=Erricolo%2C+D">Danilo Erricolo</a>, 
<a href="/search/cs?searchtype=author&query=Tulabandhula%2C+T">Theja Tulabandhula</a>, 
<a href="/search/cs?searchtype=author&query=Trivedi%2C+A+R">Amit Ranjan Trivedi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We present a novel statistical approach to incorporating uncertainty
awareness in model-free distributional reinforcement learning involving
quantile regression-based deep Q networks. The proposed algorithm,
$\textit{Calibrated Evidential Quantile Regression in Deep Q Networks
(CEQR-DQN)}$, aims to address key challenges associated with separately
estimating aleatoric and epistemic uncertainty in stochastic environments. It
combines deep evidential learning with quantile calibration based on principles
of conformal inference to provide explicit, sample-free computations of
$\textit{global}$ uncertainty as opposed to $\textit{local}$ estimates based on
simple variance, overcoming limitations of traditional methods in computational
and statistical efficiency and handling of out-of-distribution (OOD)
observations. Tested on a suite of miniaturized Atari games (i.e., MinAtar),
CEQR-DQN is shown to surpass similar existing frameworks in scores and learning
speed. Its ability to rigorously evaluate uncertainty improves exploration
strategies and can serve as a blueprint for other algorithms requiring
uncertainty awareness.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07108" title="Abstract">arXiv:2402.07108</a> [<a href="/pdf/2402.07108" title="Download PDF">pdf</a>, <a href="/format/2402.07108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoupling Learning and Decision-Making: Breaking the  $\mathcal{O}(\sqrt{T})$ Barrier in Online Resource Allocation with  First-Order Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+W">Wenzhi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Chunlin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+C">Chenyu Xue</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+D">Dongdong Ge</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yinyu Ye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Online linear programming plays an important role in both revenue management
and resource allocation, and recent research has focused on developing
efficient first-order online learning algorithms. Despite the empirical success
of first-order methods, they typically achieve a regret no better than
$\mathcal{O}(\sqrt{T})$, which is suboptimal compared to the $\mathcal{O}(\log
T)$ bound guaranteed by the state-of-the-art linear programming (LP)-based
online algorithms. This paper establishes several important facts about online
linear programming, which unveils the challenge for first-order-method-based
online algorithms to achieve beyond $\mathcal{O}(\sqrt{T})$ regret. To address
the challenge, we introduce a new algorithmic framework that decouples learning
from decision-making. More importantly, for the first time, we show that
first-order methods can attain regret $\mathcal{O}(T^{1/3})$ with this new
framework. Lastly, we conduct numerical experiments to validate our theoretical
findings.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07114" title="Abstract">arXiv:2402.07114</a> [<a href="/pdf/2402.07114" title="Download PDF">pdf</a>, <a href="/format/2402.07114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Quantifying the Preconditioning Effect of Adam
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+R">Rudrajit Das</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+N">Naman Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Sanghavi%2C+S">Sujay Sanghavi</a>, 
<a href="/search/cs?searchtype=author&query=Dhillon%2C+I+S">Inderjit S. Dhillon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">There is a notable dearth of results characterizing the preconditioning
effect of Adam and showing how it may alleviate the curse of ill-conditioning
-- an issue plaguing gradient descent (GD). In this work, we perform a detailed
analysis of Adam's preconditioning effect for quadratic functions and quantify
to what extent Adam can mitigate the dependence on the condition number of the
Hessian. Our key finding is that Adam can suffer less from the condition number
but at the expense of suffering a dimension-dependent quantity. Specifically,
for a $d$-dimensional quadratic with a diagonal Hessian having condition number
$\kappa$, we show that the effective condition number-like quantity controlling
the iteration complexity of Adam without momentum is $\mathcal{O}(\min(d,
\kappa))$. For a diagonally dominant Hessian, we obtain a bound of
$\mathcal{O}(\min(d \sqrt{d \kappa}, \kappa))$ for the corresponding quantity.
Thus, when $d &lt; \mathcal{O}(\kappa^p)$ where $p = 1$ for a diagonal Hessian and
$p = 1/3$ for a diagonally dominant Hessian, Adam can outperform GD (which has
an $\mathcal{O}(\kappa)$ dependence). On the negative side, our results suggest
that Adam can be worse than GD for a sufficiently non-diagonal Hessian even if
$d \ll \mathcal{O}(\kappa^{1/3})$; we corroborate this with empirical evidence.
Finally, we extend our analysis to functions satisfying per-coordinate
Lipschitz smoothness and a modified version of the Polyak-\L ojasiewicz
condition.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07116" title="Abstract">arXiv:2402.07116</a> [<a href="/pdf/2402.07116" title="Download PDF">pdf</a>, <a href="/format/2402.07116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Benchmark for Multi-modal Foundation Models on Low-level Vision: from  Single Images to Pairs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zicheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haoning Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+E">Erli Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+G">Guangtao Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Weisi Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2309.14181">arXiv:2309.14181</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The rapid development of Multi-modality Large Language Models (MLLMs) has
navigated a paradigm shift in computer vision, moving towards versatile
foundational models. However, evaluating MLLMs in low-level visual perception
and understanding remains a yet-to-explore domain. To this end, we design
benchmark settings to emulate human language responses related to low-level
vision: the low-level visual perception (A1) via visual question answering
related to low-level attributes (e.g. clarity, lighting); and the low-level
visual description (A2), on evaluating MLLMs for low-level text descriptions.
Furthermore, given that pairwise comparison can better avoid ambiguity of
responses and has been adopted by many human experiments, we further extend the
low-level perception-related question-answering and description evaluations of
MLLMs from single images to image pairs. Specifically, for perception (A1), we
carry out the LLVisionQA+ dataset, comprising 2,990 single images and 1,999
image pairs each accompanied by an open-ended question about its low-level
features; for description (A2), we propose the LLDescribe+ dataset, evaluating
MLLMs for low-level descriptions on 499 single images and 450 pairs.
Additionally, we evaluate MLLMs on assessment (A3) ability, i.e. predicting
score, by employing a softmax-based approach to enable all MLLMs to generate
quantifiable quality ratings, tested against human opinions in 7 image quality
assessment (IQA) datasets. With 24 MLLMs under evaluation, we demonstrate that
several MLLMs have decent low-level visual competencies on single images, but
only GPT-4V exhibits higher accuracy on pairwise comparisons than single image
evaluations (like humans). We hope that our benchmark will motivate further
research into uncovering and enhancing these nascent capabilities of MLLMs.
Datasets will be available at https://github.com/Q-Future/Q-Bench.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07118" title="Abstract">arXiv:2402.07118</a> [<a href="/pdf/2402.07118" title="Download PDF">pdf</a>, <a href="/format/2402.07118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding  Remote Smartphone-based Consultation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Srikanth%2C+D">Dhruv Srikanth</a>, 
<a href="/search/cs?searchtype=author&query=Gurung%2C+J">Jayang Gurung</a>, 
<a href="/search/cs?searchtype=author&query=Deepika%2C+N+S">N Satya Deepika</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+V">Vineet Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Vaddavalli%2C+P">Pravin Vaddavalli</a>, 
<a href="/search/cs?searchtype=author&query=Jana%2C+S">Soumya Jana</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, Submitted to IEEE EMBC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Image and Video Processing (eess.IV); Signal Processing (eess.SP)

</div>
<p class="mathjax">Blindness and other eye diseases are a global health concern, particularly in
low- and middle-income countries like India. In this regard, during the
COVID-19 pandemic, teleophthalmology became a lifeline, and the Grabi
attachment for smartphone-based eye imaging gained in use. However, quality of
user-captured image often remained inadequate, requiring clinician vetting and
delays. In this backdrop, we propose an AI-based quality assessment system with
instant feedback mimicking clinicians' judgments and tested on patient-captured
images. Dividing the complex problem hierarchically, here we tackle a
nontrivial part, and demonstrate a proof of the concept.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07119" title="Abstract">arXiv:2402.07119</a> [<a href="/pdf/2402.07119" title="Download PDF">pdf</a>, <a href="/format/2402.07119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two-Stage Multi-task Self-Supervised Learning for Medical Image  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Binyan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+A+K">A. K. Qin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Medical image segmentation has been significantly advanced by deep learning
(DL) techniques, though the data scarcity inherent in medical applications
poses a great challenge to DL-based segmentation methods. Self-supervised
learning offers a solution by creating auxiliary learning tasks from the
available dataset and then leveraging the knowledge acquired from solving
auxiliary tasks to help better solve the target segmentation task. Different
auxiliary tasks may have different properties and thus can help the target task
to different extents. It is desired to leverage their complementary advantages
to enhance the overall assistance to the target task. To achieve this, existing
methods often adopt a joint training paradigm, which co-solves segmentation and
auxiliary tasks by integrating their losses or intermediate gradients. However,
direct coupling of losses or intermediate gradients risks undesirable
interference because the knowledge acquired from solving each auxiliary task at
every training step may not always benefit the target task. To address this
issue, we propose a two-stage training approach. In the first stage, the target
segmentation task will be independently co-solved with each auxiliary task in
both joint training and pre-training modes, with the better model selected via
validation performance. In the second stage, the models obtained with respect
to each auxiliary task are converted into a single model using an ensemble
knowledge distillation method. Our approach allows for making best use of each
auxiliary task to create multiple elite segmentation models and then combine
them into an even more powerful model. We employed five auxiliary tasks of
different proprieties in our approach and applied it to train the U-Net model
on an X-ray pneumothorax segmentation dataset. Experimental results demonstrate
the superiority of our approach over several existing methods.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07127" title="Abstract">arXiv:2402.07127</a> [<a href="/pdf/2402.07127" title="Download PDF">pdf</a>, <a href="/format/2402.07127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning by Watching: A Review of Video-based Learning Approaches for  Robot Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eze%2C+C">Chrisantus Eze</a>, 
<a href="/search/cs?searchtype=author&query=Crick%2C+C">Christopher Crick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Robot learning of manipulation skills is hindered by the scarcity of diverse,
unbiased datasets. While curated datasets can help, challenges remain in
generalizability and real-world transfer. Meanwhile, large-scale "in-the-wild"
video datasets have driven progress in computer vision through self-supervised
techniques. Translating this to robotics, recent works have explored learning
manipulation skills by passively watching abundant videos sourced online.
Showing promising results, such video-based learning paradigms provide scalable
supervision while reducing dataset bias. This survey reviews foundations such
as video feature representation learning techniques, object affordance
understanding, 3D hand/body modeling, and large-scale robot resources, as well
as emerging techniques for acquiring robot manipulation skills from
uncontrolled video demonstrations. We discuss how learning only from observing
large-scale human videos can enhance generalization and sample efficiency for
robotic manipulation. The survey summarizes video-based learning approaches,
analyses their benefits over standard datasets, survey metrics, and benchmarks,
and discusses open challenges and future directions in this nascent domain at
the intersection of computer vision, natural language processing, and robot
learning.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07129" title="Abstract">arXiv:2402.07129</a> [<a href="/pdf/2402.07129" title="Download PDF">pdf</a>, <a href="/ps/2402.07129" title="Download PostScript">ps</a>, <a href="/format/2402.07129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An attempt to generate new bridge types from latent space of denoising  diffusion Implicit model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongjun Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Use denoising diffusion implicit model for bridge-type innovation. The
process of adding noise and denoising to an image can be likened to the process
of a corpse rotting and a detective restoring the scene of a victim being
killed, to help beginners understand. Through an easy-to-understand algebraic
method, derive the function formulas for adding noise and denoising, making it
easier for beginners to master the mathematical principles of the model. Using
symmetric structured image dataset of three-span beam bridge, arch bridge,
cable-stayed bridge and suspension bridge , based on Python programming
language, TensorFlow and Keras deep learning platform framework , denoising
diffusion implicit model is constructed and trained. From the latent space
sampling, new bridge types with asymmetric structures can be generated.
Denoising diffusion implicit model can organically combine different structural
components on the basis of human original bridge types, and create new bridge
types.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07132" title="Abstract">arXiv:2402.07132</a> [<a href="/pdf/2402.07132" title="Download PDF">pdf</a>, <a href="/format/2402.07132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BAFLineDP: Code Bilinear Attention Fusion Framework for Line-Level  Defect Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+S">Shaojian Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Huihao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+J">Jianxiang Luo</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+Y">Yingjie Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haoyu Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE SANER 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Software defect prediction aims to identify defect-prone code, aiding
developers in optimizing testing resource allocation. Most defect prediction
approaches primarily focus on coarse-grained, file-level defect prediction,
which fails to provide developers with the precision required to locate
defective code. Recently, some researchers have proposed fine-grained,
line-level defect prediction methods. However, most of these approaches lack an
in-depth consideration of the contextual semantics of code lines and neglect
the local interaction information among code lines. To address the above
issues, this paper presents a line-level defect prediction method grounded in a
code bilinear attention fusion framework (BAFLineDP). This method discerns
defective code files and lines by integrating source code line semantics,
line-level context, and local interaction information between code lines and
line-level context. Through an extensive analysis involving within- and
cross-project defect prediction across 9 distinct projects encompassing 32
releases, our results demonstrate that BAFLineDP outperforms current advanced
file-level and line-level defect prediction approaches.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07138" title="Abstract">arXiv:2402.07138</a> [<a href="/pdf/2402.07138" title="Download PDF">pdf</a>, <a href="/format/2402.07138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unprecedented Code Change Automation: The Fusion of LLMs and  Transformation by Example
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dilhara%2C+M">Malinda Dilhara</a>, 
<a href="/search/cs?searchtype=author&query=Bellur%2C+A">Abhiram Bellur</a>, 
<a href="/search/cs?searchtype=author&query=Bryksin%2C+T">Timofey Bryksin</a>, 
<a href="/search/cs?searchtype=author&query=Dig%2C+D">Danny Dig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is accepted to Proceedings of the 32nd ACM Symposium on the Foundations of Software Engineering (FSE - 2024), This is an author copy
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Software developers often repeat code changes, known as "code change
patterns" (CPATs), within and across projects. Automating these CPATs
accelerates development, but current Transformation by Example (TBE) techniques
are limited by the input examples' quality and quantity, missing variations
with different syntax or flow yet semantically similar. Large Language Models
(LLMs), trained on vast code datasets, can overcome these limitations by
generating semantically equivalent, unseen CPAT variants, enhancing TBE
effectiveness.
<br />We identified best practices for using LLMs to generate code variants meeting
criteria of correctness, usefulness, and applicability. Implementing these in
PyCraft, combining static and dynamic analysis with LLMs, we achieved an
F-measure of 96.6% in identifying correct variants, expanding inputs by 58x on
average, and automating changes to increase target codes by up to 39x. Patches
from PyCraft were submitted to projects like microsoft/DeepSpeed and
IBM/inFairness, with an 83% acceptance rate, validating our approach's
usefulness.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07139" title="Abstract">arXiv:2402.07139</a> [<a href="/pdf/2402.07139" title="Download PDF">pdf</a>, <a href="/format/2402.07139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Robust Car Following Dynamics Modeling via Blackbox Models:  Methodology, Analysis, and Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shahid%2C+M+B">Muhammad Bilal Shahid</a>, 
<a href="/search/cs?searchtype=author&query=Fleming%2C+C">Cody Fleming</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The selection of the target variable is important while learning parameters
of the classical car following models like GIPPS, IDM, etc. There is a vast
body of literature on which target variable is optimal for classical car
following models, but there is no study that empirically evaluates the
selection of optimal target variables for black-box models, such as LSTM, etc.
The black-box models, like LSTM and Gaussian Process (GP) are increasingly
being used to model car following behavior without wise selection of target
variables. The current work tests different target variables, like
acceleration, velocity, and headway, for three black-box models, i.e., GP,
LSTM, and Kernel Ridge Regression. These models have different objective
functions and work in different vector spaces, e.g., GP works in function
space, and LSTM works in parameter space. The experiments show that the optimal
target variable recommendations for black-box models differ from classical car
following models depending on the objective function and the vector space. It
is worth mentioning that models and datasets used during evaluation are diverse
in nature: the datasets contained both automated and human-driven vehicle
trajectories; the black-box models belong to both parametric and non-parametric
classes of models. This diversity is important during the analysis of variance,
wherein we try to find the interaction between datasets, models, and target
variables. It is shown that the models and target variables interact and
recommended target variables don't depend on the dataset under consideration.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07140" title="Abstract">arXiv:2402.07140</a> [<a href="/pdf/2402.07140" title="Download PDF">pdf</a>, <a href="/format/2402.07140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sequential Ordering in Textual Descriptions: Impact on Spatial  Perception Abilities of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ge%2C+Y">Yuyao Ge</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shenghua Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+L">Lingrui Mei</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lizhe Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">In recent years, Large Language Models have reached state-of-the-art
performance across multiple domains. However, the progress in the field of
graph reasoning remains limited. Our work delves into this gap by thoroughly
investigating graph reasoning with LLM. In this work, we reveal the impact of
text sequence on LLM spatial understanding, finding that graph-descriptive text
sequences significantly affect LLM reasoning performance on graphs. By altering
the graph-descriptive text sequences, we enhance the performance of LLM from
42.22\% to 70\%. Furthermore, we evaluate the relationship between LLM
performance and graph size, discovering that the reasoning performance of LLM
does not monotonically decrease with the increase in graph size. Conclusively,
we introduce the Scaled Graph Reasoning benchmark for assessing LLM performance
across varied graph sizes.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07141" title="Abstract">arXiv:2402.07141</a> [<a href="/pdf/2402.07141" title="Download PDF">pdf</a>, <a href="/ps/2402.07141" title="Download PostScript">ps</a>, <a href="/format/2402.07141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reading Rational Univariate Representations on lexicographic Groebner  bases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Demin%2C+A">Alexander Demin</a>, 
<a href="/search/cs?searchtype=author&query=Rouillier%2C+F">Fabrice Rouillier</a>, 
<a href="/search/cs?searchtype=author&query=Ruiz%2C+J">Joao Ruiz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Symbolic Computation (cs.SC)</span>; Commutative Algebra (math.AC)

</div>
<p class="mathjax">In this contribution, we consider a zero-dimensional polynomial system in $n$
variables defined over a field $\mathbb{K}$. In the context of computing a
Rational Univariate Representation (RUR) of its solutions, we address the
problem of certifying a separating linear form and, once certified, calculating
the RUR that comes from it, without any condition on the ideal else than being
zero-dimensional. Our key result is that the RUR can be read (closed formula)
from lexicographic Groebner bases of bivariate elimination ideals, even in the
case where the original ideal that is not in shape position, so that one can
use the same core as the well known FGLM method to propose a simple algorithm.
Our first experiments, either with a very short code (300 lines) written in
Maple or with a Julia code using straightforward implementations performing
only classical Gaussian reductions in addition to Groebner bases for the degree
reverse lexicographic ordering, show that this new method is already
competitive with sophisticated state of the art implementations which do not
certify the parameterizations.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07144" title="Abstract">arXiv:2402.07144</a> [<a href="/pdf/2402.07144" title="Download PDF">pdf</a>, <a href="/format/2402.07144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Fundamental Analysis of the Impact on Traffic Assignment by Toll  System of Electric Road System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Nakanishi%2C+W">Wataru Nakanishi</a>, 
<a href="/search/eess?searchtype=author&query=Kaneko%2C+N">Noriko Kaneko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Electric road system (ERS) is expected to make electric vehicles (EVs) more
popular as EVs with Dynamic Wireless Power Transfer (DWPT) system can be
charged while driving on ERS. Although some studies dealt with ERS
implementation, its toll system has not been explored yet. This paper aims at a
fundamental analysis on impact of ERS toll system on a traffic assignment. We
conduct assignments on a simple network where two vehicle types (EVs with DWPT
and others) are co-existing. The results under two toll systems showed some
undesirable situations, such as total travel time was not minimised, total
charged volume was not optimised, and ERS was not utilised. The occurrence of
them depended on the ratio of EVs, battery level, value of electricity, and
toll price. The difficulty to control such situations by toll price was
discussed as the battery level and value of electricity may vary over time.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07145" title="Abstract">arXiv:2402.07145</a> [<a href="/pdf/2402.07145" title="Download PDF">pdf</a>, <a href="/format/2402.07145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Designing NLP-based solutions for requirements variability management:  experiences from a design science study at Visma
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elahidoost%2C+P">Parisa Elahidoost</a>, 
<a href="/search/cs?searchtype=author&query=Unterkalmsteiner%2C+M">Michael Unterkalmsteiner</a>, 
<a href="/search/cs?searchtype=author&query=Fucci%2C+D">Davide Fucci</a>, 
<a href="/search/cs?searchtype=author&query=Liljenberg%2C+P">Peter Liljenberg</a>, 
<a href="/search/cs?searchtype=author&query=Fischbach%2C+J">Jannik Fischbach</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Context and motivation: In this industry-academia collaborative project, a
team of researchers, supported by a software architect, business analyst, and
test engineer explored the challenges of requirement variability in a large
business software development company. Question/problem: Following the design
science paradigm, we studied the problem of requirements analysis and tracing
in the context of contractual documents, with a specific focus on managing
requirements variability. This paper reports on the lessons learned from that
experience, highlighting the strategies and insights gained in the realm of
requirements variability management. Principal ideas/results: This experience
report outlines the insights gained from applying design science in
requirements engineering research in industry. We show and evaluate various
strategies to tackle the issue of requirement variability. Contribution: We
report on the iterations and how the solution development evolved in parallel
with problem understanding. From this process, we derive five key lessons
learned to highlight the effectiveness of design science in exploring solutions
for requirement variability in contract-based environments.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07152" title="Abstract">arXiv:2402.07152</a> [<a href="/pdf/2402.07152" title="Download PDF">pdf</a>, <a href="/format/2402.07152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable Global Wildfire Prediction Models using Graph Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dayou Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Sibo Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jinwei Hu</a>, 
<a href="/search/cs?searchtype=author&query=Kasoar%2C+M">Matthew Kasoar</a>, 
<a href="/search/cs?searchtype=author&query=Arcucci%2C+R">Rossella Arcucci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Wildfire prediction has become increasingly crucial due to the escalating
impacts of climate change. Traditional CNN-based wildfire prediction models
struggle with handling missing oceanic data and addressing the long-range
dependencies across distant regions in meteorological data. In this paper, we
introduce an innovative Graph Neural Network (GNN)-based model for global
wildfire prediction. We propose a hybrid model that combines the spatial
prowess of Graph Convolutional Networks (GCNs) with the temporal depth of Long
Short-Term Memory (LSTM) networks. Our approach uniquely transforms global
climate and wildfire data into a graph representation, addressing challenges
such as null oceanic data locations and long-range dependencies inherent in
traditional models. Benchmarking against established architectures using an
unseen ensemble of JULES-INFERNO simulations, our model demonstrates superior
predictive accuracy. Furthermore, we emphasise the model's explainability,
unveiling potential wildfire correlation clusters through community detection
and elucidating feature importance via Integrated Gradient analysis. Our
findings not only advance the methodological domain of wildfire prediction but
also underscore the importance of model transparency, offering valuable
insights for stakeholders in wildfire management.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07153" title="Abstract">arXiv:2402.07153</a> [<a href="/pdf/2402.07153" title="Download PDF">pdf</a>, <a href="/format/2402.07153" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Error Estimation for Physics-informed Neural Networks Approximating  Semilinear Wave Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lorenz%2C+B">Beatrice Lorenz</a>, 
<a href="/search/math?searchtype=author&query=Bacho%2C+A">Aras Bacho</a>, 
<a href="/search/math?searchtype=author&query=Kutyniok%2C+G">Gitta Kutyniok</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper provides rigorous error bounds for physics-informed neural
networks approximating the semilinear wave equation. We provide bounds for the
generalization and training error in terms of the width of the network's layers
and the number of training points for a tanh neural network with two hidden
layers. Our main result is a bound of the total error in the
$H^1([0,T];L^2(\Omega))$-norm in terms of the training error and the number of
training points, which can be made arbitrarily small under some assumptions. We
illustrate our theoretical bounds with numerical experiments.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07156" title="Abstract">arXiv:2402.07156</a> [<a href="/pdf/2402.07156" title="Download PDF">pdf</a>, <a href="/format/2402.07156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A hybrid iterative method based on MIONet for PDEs: Theory and numerical  examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hu%2C+J">Jun Hu</a>, 
<a href="/search/math?searchtype=author&query=Jin%2C+P">Pengzhan Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We propose a hybrid iterative method based on MIONet for PDEs, which combines
the traditional numerical iterative solver and the recent powerful machine
learning method of neural operator, and further systematically analyze its
theoretical properties, including the convergence condition, the spectral
behavior, as well as the convergence rate, in terms of the errors of the
discretization and the model inference. We show the theoretical results for the
frequently-used smoothers, i.e. Richardson (damped Jacobi) and Gauss-Seidel. We
give an upper bound of the convergence rate of the hybrid method w.r.t. the
model correction period, which indicates a minimum point to make the hybrid
iteration converge fastest. Several numerical examples including the hybrid
Richardson (Gauss-Seidel) iteration for the 1-d (2-d) Poisson equation are
presented to verify our theoretical results, and also reflect an excellent
acceleration effect. As a meshless acceleration method, it is provided with
enormous potentials for practice applications.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07157" title="Abstract">arXiv:2402.07157</a> [<a href="/pdf/2402.07157" title="Download PDF">pdf</a>, <a href="/format/2402.07157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Natural Language Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+X">Xidong Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Z">Ziyu Wan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mengyue Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziyan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Koushiks%2C+G+A">Girish A. Koushiks</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yali Du</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Y">Ying Wen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jun Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Reinforcement Learning (RL) has shown remarkable abilities in learning
policies for decision-making tasks. However, RL is often hindered by issues
such as low sample efficiency, lack of interpretability, and sparse supervision
signals. To tackle these limitations, we take inspiration from the human
learning process and introduce Natural Language Reinforcement Learning (NLRL),
which innovatively combines RL principles with natural language representation.
Specifically, NLRL redefines RL concepts like task objectives, policy, value
function, Bellman equation, and policy iteration in natural language space. We
present how NLRL can be practically implemented with the latest advancements in
large language models (LLMs) like GPT-4. Initial experiments over tabular MDPs
demonstrate the effectiveness, efficiency, and also interpretability of the
NLRL framework.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07158" title="Abstract">arXiv:2402.07158</a> [<a href="/pdf/2402.07158" title="Download PDF">pdf</a>, <a href="/format/2402.07158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effort and Size Estimation in Software Projects with Large Language  Model-based Intelligent Interfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Coelho%2C+C+N">Claudionor N. Coelho Jr</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Hanchen Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Karayil%2C+T">Tushar Karayil</a>, 
<a href="/search/cs?searchtype=author&query=Koratala%2C+S">Sree Koratala</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+R">Rex Shang</a>, 
<a href="/search/cs?searchtype=author&query=Bollinger%2C+J">Jacob Bollinger</a>, 
<a href="/search/cs?searchtype=author&query=Shabar%2C+M">Mohamed Shabar</a>, 
<a href="/search/cs?searchtype=author&query=Nair%2C+S">Syam Nair</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The advancement of Large Language Models (LLM) has also resulted in an
equivalent proliferation in its applications. Software design, being one, has
gained tremendous benefits in using LLMs as an interface component that extends
fixed user stories. However, inclusion of LLM-based AI agents in software
design often poses unexpected challenges, especially in the estimation of
development efforts. Through the example of UI-based user stories, we provide a
comparison against traditional methods and propose a new way to enhance
specifications of natural language-based questions that allows for the
estimation of development effort by taking into account data sources,
interfaces and algorithms.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07162" title="Abstract">arXiv:2402.07162</a> [<a href="/pdf/2402.07162" title="Download PDF">pdf</a>, <a href="/format/2402.07162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Source-Channel Coding for Wireless Image Transmission: A Deep  Compressed-Sensing Based Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jarrahi%2C+M+A">Mohammad Amin Jarrahi</a>, 
<a href="/search/cs?searchtype=author&query=Bourtsoulatze%2C+E">Eirina Bourtsoulatze</a>, 
<a href="/search/cs?searchtype=author&query=Abolghasemi%2C+V">Vahid Abolghasemi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Nowadays, the demand for image transmission over wireless networks has surged
significantly. To meet the need for swift delivery of high-quality images
through time-varying channels with limited bandwidth, the development of
efficient transmission strategies and techniques for preserving image quality
is of importance. This paper introduces an innovative approach to Joint
Source-Channel Coding (JSCC) tailored for wireless image transmission. It
capitalizes on the power of Compressed Sensing (CS) to achieve superior
compression and resilience to channel noise. In this method, the process begins
with the compression of images using a block-based CS technique implemented
through a Convolutional Neural Network (CNN) structure. Subsequently, the
images are encoded by directly mapping image blocks to complex-valued channel
input symbols. Upon reception, the data is decoded to recover the
channel-encoded information, effectively removing the noise introduced during
transmission. To finalize the process, a novel CNN-based reconstruction network
is employed to restore the original image from the channel-decoded data. The
performance of the proposed method is assessed using the CIFAR-10 and Kodak
datasets. The results illustrate a substantial improvement over existing JSCC
frameworks when assessed in terms of metrics such as Peak Signal-to-Noise Ratio
(PSNR) and Structural Similarity Index (SSIM) across various channel
Signal-to-Noise Ratios (SNRs) and channel bandwidth values. These findings
underscore the potential of harnessing CNN-based CS for the development of deep
JSCC algorithms tailored for wireless image transmission.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07164" title="Abstract">arXiv:2402.07164</a> [<a href="/pdf/2402.07164" title="Download PDF">pdf</a>, <a href="/format/2402.07164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GeoFormer: A Vision and Sequence Transformer-based Approach for  Greenhouse Gas Monitoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khirwar%2C+M">Madhav Khirwar</a>, 
<a href="/search/cs?searchtype=author&query=Narang%2C+A">Ankur Narang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Air pollution represents a pivotal environmental challenge globally, playing
a major role in climate change via greenhouse gas emissions and negatively
affecting the health of billions. However predicting the spatial and temporal
patterns of pollutants remains challenging. The scarcity of ground-based
monitoring facilities and the dependency of air pollution modeling on
comprehensive datasets, often inaccessible for numerous areas, complicate this
issue. In this work, we introduce GeoFormer, a compact model that combines a
vision transformer module with a highly efficient time-series transformer
module to predict surface-level nitrogen dioxide (NO2) concentrations from
Sentinel-5P satellite imagery. We train the proposed model to predict
surface-level NO2 measurements using a dataset we constructed with Sentinel-5P
images of ground-level monitoring stations, and their corresponding NO2
concentration readings. The proposed model attains high accuracy (MAE 5.65),
demonstrating the efficacy of combining vision and time-series transformer
architectures to harness satellite-derived data for enhanced GHG emission
insights, proving instrumental in advancing climate change monitoring and
emission regulation efforts globally.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07166" title="Abstract">arXiv:2402.07166</a> [<a href="/pdf/2402.07166" title="Download PDF">pdf</a>, <a href="/format/2402.07166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social Evolution of Published Text and The Emergence of Artificial  Intelligence Through Large Language Models and The Problem of Toxicity and  Bias
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+A">Arifa Khan</a>, 
<a href="/search/cs?searchtype=author&query=Saravanan%2C+P">P. Saravanan</a>, 
<a href="/search/cs?searchtype=author&query=Venkatesan%2C+S+K">S.K Venkatesan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">We provide a birds eye view of the rapid developments in AI and Deep Learning
that has led to the path-breaking emergence of AI in Large Language Models. The
aim of this study is to place all these developments in a pragmatic broader
historical social perspective without any exaggerations while at the same time
without any pessimism that created the AI winter in the 1970s to 1990s. We also
at the same time point out toxicity, bias, memorization, sycophancy, logical
inconsistencies, hallucinations that exist just as a warning to the overly
optimistic. We note here that just as this emergence of AI seems to occur at a
threshold point in the number of neural connections or weights, it has also
been observed that human brain and especially the cortex region is nothing
special or extraordinary but simply a case of scaled-up version of the primate
brain and that even the human intelligence seems like an emergent phenomena of
scale.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07167" title="Abstract">arXiv:2402.07167</a> [<a href="/pdf/2402.07167" title="Download PDF">pdf</a>, <a href="/format/2402.07167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large-Language-Model Empowered Dose Volume Histogram Prediction for  Intensity Modulated Radiotherapy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zehao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yixin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gay%2C+H">Hiram Gay</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+Y">Yao Hao</a>, 
<a href="/search/cs?searchtype=author&query=Hugo%2C+G+D">Geoffrey D. Hugo</a>, 
<a href="/search/cs?searchtype=author&query=Samson%2C+P">Pamela Samson</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tianyu Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Treatment planning is currently a patient specific, time-consuming, and
resource demanding task in radiotherapy. Dose-volume histogram (DVH) prediction
plays a critical role in automating this process. The geometric relationship
between DVHs in radiotherapy plans and organs-at-risk (OAR) and planning target
volume (PTV) has been well established. This study explores the potential of
deep learning models for predicting DVHs using images and subsequent human
intervention facilitated by a large-language model (LLM) to enhance the
planning quality. We propose a pipeline to convert unstructured images to a
structured graph consisting of image-patch nodes and dose nodes. A novel Dose
Graph Neural Network (DoseGNN) model is developed for predicting DVHs from the
structured graph. The proposed DoseGNN is enhanced with the LLM to encode
massive knowledge from prescriptions and interactive instructions from
clinicians. In this study, we introduced an online human-AI collaboration
(OHAC) system as a practical implementation of the concept proposed for the
automation of intensity-modulated radiotherapy (IMRT) planning. In comparison
to the widely-employed DL models used in radiotherapy, DoseGNN achieved mean
square errors that were 80$\%$, 76$\%$ and 41.0$\%$ of those predicted by Swin
U-Net Transformer, 3D U-Net CNN and vanilla MLP, respectively. Moreover, the
LLM-empowered DoseGNN model facilitates seamless adjustment to treatment plans
through interaction with clinicians using natural language.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07173" title="Abstract">arXiv:2402.07173</a> [<a href="/pdf/2402.07173" title="Download PDF">pdf</a>, <a href="/format/2402.07173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> INSITE: labelling medical images using submodular functions and  semi-supervised data programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gautam%2C+A">Akshat Gautam</a>, 
<a href="/search/cs?searchtype=author&query=Shandilya%2C+A">Anurag Shandilya</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+A">Akshit Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Subramanian%2C+V">Venkatapathy Subramanian</a>, 
<a href="/search/cs?searchtype=author&query=Ramakrishnan%2C+G">Ganesh Ramakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Jadhav%2C+K">Kshitij Jadhav</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The necessity of large amounts of labeled data to train deep models,
especially in medical imaging creates an implementation bottleneck in
resource-constrained settings. In Insite (labelINg medical imageS usIng
submodular funcTions and sEmi-supervised data programming) we apply informed
subset selection to identify a small number of most representative or diverse
images from a huge pool of unlabelled data subsequently annotated by a domain
expert. The newly annotated images are then used as exemplars to develop
several data programming-driven labeling functions. These labelling functions
output a predicted-label and a similarity score when given an unlabelled image
as an input. A consensus is brought amongst the outputs of these labeling
functions by using a label aggregator function to assign the final predicted
label to each unlabelled data point. We demonstrate that informed subset
selection followed by semi-supervised data programming methods using these
images as exemplars perform better than other state-of-the-art semi-supervised
methods. Further, for the first time we demonstrate that this can be achieved
through a small set of images used as exemplars.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07174" title="Abstract">arXiv:2402.07174</a> [<a href="/pdf/2402.07174" title="Download PDF">pdf</a>, <a href="/format/2402.07174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EmoWear: Exploring Emotional Teasers for Voice Message Interaction on  Smartwatches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=An%2C+P">Pengcheng An</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiawen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zibo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yifei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Q">Qingyuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+C">Che Yan</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+L">Linghao Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jian Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at ACM CHI '24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Voice messages, by nature, prevent users from gauging the emotional tone
without fully diving into the audio content. This hinders the shared emotional
experience at the pre-retrieval stage. Research scarcely explored "Emotional
Teasers"-pre-retrieval cues offering a glimpse into an awaiting message's
emotional tone without disclosing its content. We introduce EmoWear, a
smartwatch voice messaging system enabling users to apply 30 animation teasers
on message bubbles to reflect emotions. EmoWear eases senders' choice by
prioritizing emotions based on semantic and acoustic processing. EmoWear was
evaluated in comparison with a mirroring system using color-coded message
bubbles as emotional cues (N=24). Results showed EmoWear significantly enhanced
emotional communication experience in both receiving and sending messages. The
animated teasers were considered intuitive and valued for diverse expressions.
Desirable interaction qualities and practical implications are distilled for
future design. We thereby contribute both a novel system and empirical
knowledge concerning emotional teasers for voice messaging.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07179" title="Abstract">arXiv:2402.07179</a> [<a href="/pdf/2402.07179" title="Download PDF">pdf</a>, <a href="/format/2402.07179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt Perturbation in Retrieval-Augmented Generation based Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhibo Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+Y">Yanfeng Shu</a>, 
<a href="/search/cs?searchtype=author&query=Helen">Helen</a> (Hye-Young)
<a href="/search/cs?searchtype=author&query=Paik">Paik</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Liming Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">The robustness of large language models (LLMs) becomes increasingly important
as their use rapidly grows in a wide range of domains. Retrieval-Augmented
Generation (RAG) is considered as a means to improve the trustworthiness of
text generation from LLMs. However, how the outputs from RAG-based LLMs are
affected by slightly different inputs is not well studied. In this work, we
find that the insertion of even a short prefix to the prompt leads to the
generation of outputs far away from factually correct answers. We
systematically evaluate the effect of such prefixes on RAG by introducing a
novel optimization technique called Gradient Guided Prompt Perturbation (GGPP).
GGPP achieves a high success rate in steering outputs of RAG-based LLMs to
targeted wrong answers. It can also cope with instructions in the prompts
requesting to ignore irrelevant context. We also exploit LLMs' neuron
activation difference between prompts with and without GGPP perturbations to
give a method that improves the robustness of RAG-based LLMs through a highly
effective detector trained on neuron activation triggered by GGPP generated
prompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of
our methods.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07180" title="Abstract">arXiv:2402.07180</a> [<a href="/pdf/2402.07180" title="Download PDF">pdf</a>, <a href="/format/2402.07180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAGNETO: Edge AI for Human Activity Recognition -- Privacy and  Personalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zuo%2C+J">Jingwei Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Arvanitakis%2C+G">George Arvanitakis</a>, 
<a href="/search/cs?searchtype=author&query=Ndhlovu%2C+M">Mthandazo Ndhlovu</a>, 
<a href="/search/cs?searchtype=author&query=Hacid%2C+H">Hakim Hacid</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by EDBT 2024 (demo track)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Human activity recognition (HAR) is a well-established field, significantly
advanced by modern machine learning (ML) techniques. While companies have
successfully integrated HAR into consumer products, they typically rely on a
predefined activity set, which limits personalizations at the user level (edge
devices). Despite advancements in Incremental Learning for updating models with
new data, this often occurs on the Cloud, necessitating regular data transfers
between cloud and edge devices, thus leading to data privacy issues. In this
paper, we propose MAGNETO, an Edge AI platform that pushes HAR tasks from the
Cloud to the Edge. MAGNETO allows incremental human activity learning directly
on the Edge devices, without any data exchange with the Cloud. This enables
strong privacy guarantees, low processing latency, and a high degree of
personalization for users. In particular, we demonstrate MAGNETO in an Android
device, validating the whole pipeline from data collection to result
visualization.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07181" title="Abstract">arXiv:2402.07181</a> [<a href="/pdf/2402.07181" title="Download PDF">pdf</a>, <a href="/format/2402.07181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D Gaussian as a New Vision Era: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fei%2C+B">Ben Fei</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jingyi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qingyuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Weidong Yang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Ying He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">3D Gaussian Splatting (3D-GS) has emerged as a significant advancement in the
field of Computer Graphics, offering explicit scene representation and novel
view synthesis without the reliance on neural networks, such as Neural Radiance
Fields (NeRF). This technique has found diverse applications in areas such as
robotics, urban mapping, autonomous navigation, and virtual reality/augmented
reality, just name a few. Given the growing popularity and expanding research
in 3D Gaussian Splatting, this paper presents a comprehensive survey of
relevant papers from the past year. We organize the survey into taxonomies
based on characteristics and applications, providing an introduction to the
theoretical underpinnings of 3D Gaussian Splatting. Our goal through this
survey is to acquaint new researchers with 3D Gaussian Splatting, serve as a
valuable reference for seminal works in the field, and inspire future research
directions, as discussed in our concluding section.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07182" title="Abstract">arXiv:2402.07182</a> [<a href="/pdf/2402.07182" title="Download PDF">pdf</a>, <a href="/format/2402.07182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Divide and Conquer: Provably Unveiling the Pareto Front with  Multi-Objective Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=R%C3%B6pke%2C+W">Willem R&#xf6;pke</a>, 
<a href="/search/cs?searchtype=author&query=Reymond%2C+M">Mathieu Reymond</a>, 
<a href="/search/cs?searchtype=author&query=Mannion%2C+P">Patrick Mannion</a>, 
<a href="/search/cs?searchtype=author&query=Roijers%2C+D+M">Diederik M. Roijers</a>, 
<a href="/search/cs?searchtype=author&query=Now%C3%A9%2C+A">Ann Now&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=R%C4%83dulescu%2C+R">Roxana R&#x103;dulescu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">A significant challenge in multi-objective reinforcement learning is
obtaining a Pareto front of policies that attain optimal performance under
different preferences. We introduce Iterated Pareto Referent Optimisation
(IPRO), a principled algorithm that decomposes the task of finding the Pareto
front into a sequence of single-objective problems for which various solution
methods exist. This enables us to establish convergence guarantees while
providing an upper bound on the distance to undiscovered Pareto optimal
solutions at each step. Empirical evaluations demonstrate that IPRO matches or
outperforms methods that require additional domain knowledge. By leveraging
problem-specific single-objective solvers, our approach also holds promise for
applications beyond multi-objective reinforcement learning, such as in
pathfinding and optimisation.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07183" title="Abstract">arXiv:2402.07183</a> [<a href="/pdf/2402.07183" title="Download PDF">pdf</a>, <a href="/format/2402.07183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Random Ensemble of Encrypted Vision Transformers for Adversarially  Robust Defense
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iijima%2C+R">Ryota Iijima</a>, 
<a href="/search/cs?searchtype=author&query=Shiota%2C+S">Sayaka Shiota</a>, 
<a href="/search/cs?searchtype=author&query=Kiya%2C+H">Hitoshi Kiya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Deep neural networks (DNNs) are well known to be vulnerable to adversarial
examples (AEs). In previous studies, the use of models encrypted with a secret
key was demonstrated to be robust against white-box attacks, but not against
black-box ones. In this paper, we propose a novel method using the vision
transformer (ViT) that is a random ensemble of encrypted models for enhancing
robustness against both white-box and black-box attacks. In addition, a
benchmark attack method, called AutoAttack, is applied to models to test
adversarial robustness objectively. In experiments, the method was demonstrated
to be robust against not only white-box attacks but also black-box ones in an
image classification task on the CIFAR-10 and ImageNet datasets. The method was
also compared with the state-of-the-art in a standardized benchmark for
adversarial robustness, RobustBench, and it was verified to outperform
conventional defenses in terms of clean accuracy and robust accuracy.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07188" title="Abstract">arXiv:2402.07188</a> [<a href="/pdf/2402.07188" title="Download PDF">pdf</a>, <a href="/ps/2402.07188" title="Download PostScript">ps</a>, <a href="/format/2402.07188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Hierarchical Coded Caching Schemes from $t$-Designs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=T.%2C+R+U+N">Rashid Ummer N.T.</a>, 
<a href="/search/cs?searchtype=author&query=Rajan%2C+B+S">B. Sundar Rajan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages and 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)
achieves an optimal transmission rate $R$ under uncoded placement but requires
a subpacketization level $F$ which increases exponentially with the number of
users $K$ where the number of files $N \geq K$. Placement delivery array (PDA)
was proposed as a tool to design coded caching schemes with reduced
subpacketization level by Yan \textit{et al.} in \cite{YCT}. This paper
proposes two novel classes of PDA constructions from combinatorial $t$-designs
that achieve an improved transmission rate for a given low subpacketization
level, cache size and number of users compared to existing coded caching
schemes from $t$-designs. A $(K, F, Z, S)$ PDA composed of a specific symbol
$\star$ and $S$ non-negative integers corresponds to a coded caching scheme
with subpacketization level $F$, $K$ users each caching $Z$ packets and the
demands of all the users are met with a rate $R=\frac{S}{F}$. For a given $K$,
$F$ and $Z$, a lower bound on $S$ such that a $(K, F, Z, S)$ PDA exists is
given by Cheng \textit{et al.} in \cite{MJXQ}. Our first class of proposed PDA
achieves this lower bound on $S$. The second class of PDA also achieves this
lower bound in some cases. From these two classes of PDAs, we then construct
hierarchical placement delivery arrays (HPDA), proposed by Kong \textit{et al.}
in \cite{KYWM}, which characterizes a hierarchical two-layer coded caching
system. These constructions give very low subpacketization level schemes with
optimal rate with respect to the lower bound on $S$ in first layer transmission
from server to attached mirrors.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07191" title="Abstract">arXiv:2402.07191</a> [<a href="/pdf/2402.07191" title="Download PDF">pdf</a>, <a href="/format/2402.07191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GSINA: Improving Subgraph Extraction for Graph Invariant Learning via  Graph Sinkhorn Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+F">Fangyu Ding</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haiyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+Z">Zhixuan Chu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianming Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhaoping Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Graph invariant learning (GIL) has been an effective approach to discovering
the invariant relationships between graph data and its labels for different
graph learning tasks under various distribution shifts. Many recent endeavors
of GIL focus on extracting the invariant subgraph from the input graph for
prediction as a regularization strategy to improve the generalization
performance of graph learning. Despite their success, such methods also have
various limitations in obtaining their invariant subgraphs. In this paper, we
provide in-depth analyses of the drawbacks of existing works and propose
corresponding principles of our invariant subgraph extraction: 1) the sparsity,
to filter out the variant features, 2) the softness, for a broader solution
space, and 3) the differentiability, for a soundly end-to-end optimization. To
meet these principles in one shot, we leverage the Optimal Transport (OT)
theory and propose a novel graph attention mechanism called Graph Sinkhorn
Attention (GSINA). This novel approach serves as a powerful regularization
method for GIL tasks. By GSINA, we are able to obtain meaningful,
differentiable invariant subgraphs with controllable sparsity and softness.
Moreover, GSINA is a general graph learning framework that could handle GIL
tasks of multiple data grain levels. Extensive experiments on both synthetic
and real-world datasets validate the superiority of our GSINA, which
outperforms the state-of-the-art GIL methods by large margins on both
graph-level tasks and node-level tasks. Our code is publicly available at
\url{https://github.com/dingfangyu/GSINA}.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07193" title="Abstract">arXiv:2402.07193</a> [<a href="/pdf/2402.07193" title="Download PDF">pdf</a>, <a href="/format/2402.07193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Implicit Bias of Gradient Noise: A Symmetry Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ziyin%2C+L">Liu Ziyin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingze Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Lei Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">We characterize the learning dynamics of stochastic gradient descent (SGD)
when continuous symmetry exists in the loss function, where the divergence
between SGD and gradient descent is dramatic. We show that depending on how the
symmetry affects the learning dynamics, we can divide a family of symmetry into
two classes. For one class of symmetry, SGD naturally converges to solutions
that have a balanced and aligned gradient noise. For the other class of
symmetry, SGD will almost always diverge. Then, we show that our result remains
applicable and can help us understand the training dynamics even when the
symmetry is not present in the loss function. Our main result is universal in
the sense that it only depends on the existence of the symmetry and is
independent of the details of the loss function. We demonstrate that the
proposed theory offers an explanation of progressive sharpening and flattening
and can be applied to common practical problems such as representation
normalization, matrix factorization, and the use of warmup.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07197" title="Abstract">arXiv:2402.07197</a> [<a href="/pdf/2402.07197" title="Download PDF">pdf</a>, <a href="/format/2402.07197" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraphTranslator: Aligning Graph Model to Large Language Model for  Open-ended Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengmei Zhang</a> (Alibaba Group Holding Limited, China Telecom Bestpay), 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mingwei Sun</a> (Alibaba Group Holding Limited), 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a> (Alibaba Group Holding Limited), 
<a href="/search/cs?searchtype=author&query=Fan%2C+S">Shen Fan</a> (Alibaba Group Holding Limited), 
<a href="/search/cs?searchtype=author&query=Mo%2C+Y">Yanhu Mo</a> (Alibaba Group Holding Limited), 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaoxiao Xu</a> (Alibaba Group Holding Limited), 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hong Liu</a> (Alibaba Group Holding Limited), 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cheng Yang</a> (Peng Cheng Laboratory), 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chuan Shi</a> (Peng Cheng Laboratory)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Large language models (LLMs) like ChatGPT, exhibit powerful zero-shot and
instruction-following capabilities, have catalyzed a revolutionary
transformation across diverse research fields of artificial intelligence,
especially for open-ended tasks. While the idea is less explored in the graph
domain, despite the availability of numerous powerful graph models (GMs), they
are restricted to tasks in a pre-defined form. Although several methods
applying LLMs to graphs have been proposed, they fail to simultaneously handle
the pre-defined and open-ended tasks, with LLM as a node feature enhancer or as
a standalone predictor. To break this dilemma, we propose to bridge the
pretrained GM and LLM by a Translator, named GraphTranslator, aiming to
leverage GM to handle the pre-defined tasks effectively and utilize the
extended interface of LLMs to offer various open-ended tasks for GM. To train
such Translator, we propose a Producer capable of constructing the graph-text
alignment data along node information, neighbor information and model
information. By treating the node representation as a type of language, the
proposed GraphTranslator empowers an LLM to make predictions based on node
representation and language instructions, providing a unified perspective for
both pre-defined and open-ended tasks. Extensive results show that the proposed
GraphTranslator effectively improves the results of zero-shot node
classification. The graph question answering experiments reveal our
GraphTranslator potential across a broad spectrum of open-ended applications
through language instructions.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07198" title="Abstract">arXiv:2402.07198</a> [<a href="/pdf/2402.07198" title="Download PDF">pdf</a>, <a href="/format/2402.07198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> More Benefits of Being Distributional: Second-Order Bounds for  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaiwen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Oertell%2C+O">Owen Oertell</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+A">Alekh Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Kallus%2C+N">Nathan Kallus</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wen Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this paper, we prove that Distributional Reinforcement Learning (DistRL),
which learns the return distribution, can obtain second-order bounds in both
online and offline RL in general settings with function approximation.
Second-order bounds are instance-dependent bounds that scale with the variance
of return, which we prove are tighter than the previously known small-loss
bounds of distributional RL. To the best of our knowledge, our results are the
first second-order bounds for low-rank MDPs and for offline RL. When
specializing to contextual bandits (one-step RL problem), we show that a
distributional learning based optimism algorithm achieves a second-order
worst-case regret bound, and a second-order gap dependent bound,
simultaneously. We also empirically demonstrate the benefit of DistRL in
contextual bandits on real-world datasets. We highlight that our analysis with
DistRL is relatively simple, follows the general framework of optimism in the
face of uncertainty and does not require weighted regression. Our results
suggest that DistRL is a promising framework for obtaining second-order bounds
in general RL settings, thus further reinforcing the benefits of DistRL.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07199" title="Abstract">arXiv:2402.07199</a> [<a href="/pdf/2402.07199" title="Download PDF">pdf</a>, <a href="/format/2402.07199" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Link-aware link prediction over temporal graph by pattern recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bingqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xikun Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, one column
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">A temporal graph can be considered as a stream of links, each of which
represents an interaction between two nodes at a certain time. On temporal
graphs, link prediction is a common task, which aims to answer whether the
query link is true or not. To do this task, previous methods usually focus on
the learning of representations of the two nodes in the query link. We point
out that the learned representation by their models may encode too much
information with side effects for link prediction because they have not
utilized the information of the query link, i.e., they are link-unaware. Based
on this observation, we propose a link-aware model: historical links and the
query link are input together into the following model layers to distinguish
whether this input implies a reasonable pattern that ends with the query link.
During this process, we focus on the modeling of link evolution patterns rather
than node representations. Experiments on six datasets show that our model
achieves strong performances compared with state-of-the-art baselines, and the
results of link prediction are interpretable. The code and datasets are
available on the project website: https://github.com/lbq8942/TGACN.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07200" title="Abstract">arXiv:2402.07200</a> [<a href="/pdf/2402.07200" title="Download PDF">pdf</a>, <a href="/format/2402.07200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Outlier-Aware Training for Low-Bit Quantization of Structural  Re-Parameterized Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niu%2C+M">Muqun Niu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yuan Ren</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Boyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+C">Chenchen Ding</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Lightweight design of Convolutional Neural Networks (CNNs) requires co-design
efforts in the model architectures and compression techniques. As a novel
design paradigm that separates training and inference, a structural
re-parameterized (SR) network such as the representative RepVGG revitalizes the
simple VGG-like network with a high accuracy comparable to advanced and often
more complicated networks. However, the merging process in SR networks
introduces outliers into weights, making their distribution distinct from
conventional networks and thus heightening difficulties in quantization. To
address this, we propose an operator-level improvement for training called
Outlier Aware Batch Normalization (OABN). Additionally, to meet the demands of
limited bitwidths while upkeeping the inference accuracy, we develop a
clustering-based non-uniform quantization framework for Quantization-Aware
Training (QAT) named ClusterQAT. Integrating OABN with ClusterQAT, the
quantized performance of RepVGG is largely enhanced, particularly when the
bitwidth falls below 8.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07204" title="Abstract">arXiv:2402.07204</a> [<a href="/pdf/2402.07204" title="Download PDF">pdf</a>, <a href="/format/2402.07204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synergizing Spatial Optimization with Large Language Models for  Open-Domain Urban Itinerary Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yihong Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaokai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+A">Ao Qu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yihao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+K">Kebing Hou</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+D">Dingyi Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xiaotong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jinhua Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Wei Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we for the first time propose the task of Open-domain Urban
Itinerary Planning (OUIP) for citywalk, which directly generates itineraries
based on users' requests described in natural language. OUIP is different from
conventional itinerary planning, which limits users from expressing more
detailed needs and hinders true personalization. Recently, large language
models (LLMs) have shown potential in handling diverse tasks. However, due to
non-real-time information, incomplete knowledge, and insufficient spatial
awareness, they are unable to independently deliver a satisfactory user
experience in OUIP. Given this, we present ItiNera, an OUIP system that
synergizes spatial optimization with Large Language Models (LLMs) to provide
services that customize urban itineraries based on users' needs. Specifically,
we develop an LLM-based pipeline for extracting and updating POI features to
create a user-owned personalized POI database. For each user request, we
leverage LLM in cooperation with an embedding-based module for retrieving
candidate POIs from the user's POI database. Then, a spatial optimization
module is used to order these POIs, followed by LLM crafting a personalized,
spatially coherent itinerary. To the best of our knowledge, this study marks
the first integration of LLMs to innovate itinerary planning solutions.
Extensive experiments on offline datasets and online subjective evaluation have
demonstrated the capacities of our system to deliver more responsive and
spatially coherent itineraries than current LLM-based solutions. Our system has
been deployed in production at the TuTu online travel service and has attracted
thousands of users for their urban travel planning.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07207" title="Abstract">arXiv:2402.07207</a> [<a href="/pdf/2402.07207" title="Download PDF">pdf</a>, <a href="/format/2402.07207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided  Generative Gaussian Splatting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiaoyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ran%2C+X">Xingjian Ran</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Y">Yajiao Xiong</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jinlin He</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhiwei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yongtao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+D">Deqing Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming-Hsuan Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present GALA3D, generative 3D GAussians with LAyout-guided control, for
effective compositional text-to-3D generation. We first utilize large language
models (LLMs) to generate the initial layout and introduce a layout-guided 3D
Gaussian representation for 3D content generation with adaptive geometric
constraints. We then propose an object-scene compositional optimization
mechanism with conditioned diffusion to collaboratively generate realistic 3D
scenes with consistent geometry, texture, scale, and accurate interactions
among multiple objects while simultaneously adjusting the coarse layout priors
extracted from the LLMs to align with the generated scene. Experiments show
that GALA3D is a user-friendly, end-to-end framework for state-of-the-art
scene-level 3D content generation and controllable editing while ensuring the
high fidelity of object-level entities within the scene. Source codes and
models will be available at https://gala3d.github.io/.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07208" title="Abstract">arXiv:2402.07208</a> [<a href="/pdf/2402.07208" title="Download PDF">pdf</a>, <a href="/format/2402.07208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ML Framework for Wireless MAC Protocol Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Keshtiarast%2C+N">Navid Keshtiarast</a>, 
<a href="/search/cs?searchtype=author&query=Petrova%2C+M">Marina Petrova</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for Publication in IEEE ICMLCN 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Adaptivity, reconfigurability and intelligence are key features of the
next-generation wireless networks to meet the increasingly diverse quality of
service (QoS) requirements of the future applications. Conventional protocol
designs, however, struggle to provide flexibility and agility to changing radio
environments, traffic types and different user service requirements. In this
paper, we explore the potential of deep reinforcement learning (DRL), in
particular Proximal Policy Optimization (PPO), to design and configure
intelligent and application-specific medium access control (MAC) protocols. We
propose a framework that enables the addition, removal, or modification of
protocol features to meet individual application needs. The DRL channel access
policy design empowers the protocol to adapt and optimize in accordance with
the network and radio environment. Through extensive simulations, we
demonstrate the superior performance of the learned protocols over legacy IEEE
802.11ac in terms of throughput and latency.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07211" title="Abstract">arXiv:2402.07211</a> [<a href="/pdf/2402.07211" title="Download PDF">pdf</a>, <a href="/format/2402.07211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Fast Stochastic Sampling in Diffusion Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pandey%2C+K">Kushagra Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Rudolph%2C+M">Maja Rudolph</a>, 
<a href="/search/cs?searchtype=author&query=Mandt%2C+S">Stephan Mandt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in the NeurIPS'23 Workshop on Diffusion Models. arXiv admin note: substantial text overlap with <a href="/abs/2310.07894">arXiv:2310.07894</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Diffusion models suffer from slow sample generation at inference time.
Despite recent efforts, improving the sampling efficiency of stochastic
samplers for diffusion models remains a promising direction. We propose
Splitting Integrators for fast stochastic sampling in pre-trained diffusion
models in augmented spaces. Commonly used in molecular dynamics,
splitting-based integrators attempt to improve sampling efficiency by cleverly
alternating between numerical updates involving the data, auxiliary, or noise
variables. However, we show that a naive application of splitting integrators
is sub-optimal for fast sampling. Consequently, we propose several principled
modifications to naive splitting samplers for improving sampling efficiency and
denote the resulting samplers as Reduced Splitting Integrators. In the context
of Phase Space Langevin Diffusion (PSLD) [Pandey \&amp; Mandt, 2023] on CIFAR-10,
our stochastic sampler achieves an FID score of 2.36 in only 100 network
function evaluations (NFE) as compared to 2.63 for the best baselines.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07214" title="Abstract">arXiv:2402.07214</a> [<a href="/pdf/2402.07214" title="Download PDF">pdf</a>, <a href="/format/2402.07214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Through the Lens of Split Vote: Exploring Disagreement, Difficulty and  Calibration in Legal Case Outcome Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shanshan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Santosh%2C+T+Y+S+S">T.Y.S.S Santosh</a>, 
<a href="/search/cs?searchtype=author&query=Ichim%2C+O">Oana Ichim</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>, 
<a href="/search/cs?searchtype=author&query=Grabmair%2C+M">Matthias Grabmair</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In legal decisions, split votes (SV) occur when judges cannot reach a
unanimous decision, posing a difficulty for lawyers who must navigate diverse
legal arguments and opinions. In high-stakes domains, understanding the
alignment of perceived difficulty between humans and AI systems is crucial to
build trust. However, existing NLP calibration methods focus on a classifier's
awareness of predictive performance, measured against the human majority class,
overlooking inherent human label variation (HLV). This paper explores split
votes as naturally observable human disagreement and value pluralism. We
collect judges' vote distributions from the European Court of Human Rights
(ECHR), and present SV-ECHR, a case outcome classification (COC) dataset with
SV information. We build a taxonomy of disagreement with SV-specific
subcategories. We further assess the alignment of perceived difficulty between
models and humans, as well as confidence- and human-calibration of COC models.
We observe limited alignment with the judge vote distribution. To our
knowledge, this is the first systematic exploration of calibration to human
judgements in legal NLP. Our study underscores the necessity for further
research on measuring and enhancing model calibration considering HLV in legal
decision tasks.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07216" title="Abstract">arXiv:2402.07216</a> [<a href="/pdf/2402.07216" title="Download PDF">pdf</a>, <a href="/format/2402.07216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A novel spatial-frequency domain network for zero-shot incremental  learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jie Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weichuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Changming Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Zero-shot incremental learning aims to enable the model to generalize to new
classes without forgetting previously learned classes. However, the semantic
gap between old and new sample classes can lead to catastrophic forgetting.
Additionally, existing algorithms lack capturing significant information from
each sample image domain, impairing models' classification performance.
Therefore, this paper proposes a novel Spatial-Frequency Domain Network
(SFDNet) which contains a Spatial-Frequency Feature Extraction (SFFE) module
and Attention Feature Alignment (AFA) module to improve the Zero-Shot
Translation for Class Incremental algorithm. Firstly, SFFE module is designed
which contains a dual attention mechanism for obtaining salient
spatial-frequency feature information. Secondly, a novel feature fusion module
is conducted for obtaining fused spatial-frequency domain features. Thirdly,
the Nearest Class Mean classifier is utilized to select the most suitable
category. Finally, iteration between tasks is performed using the Zero-Shot
Translation model. The proposed SFDNet has the ability to effectively extract
spatial-frequency feature representation from input images, improve the
accuracy of image classification, and fundamentally alleviate catastrophic
forgetting. Extensive experiments on the CUB 200-2011 and CIFAR100 datasets
demonstrate that our proposed algorithm outperforms state-of-the-art
incremental learning algorithms.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07217" title="Abstract">arXiv:2402.07217</a> [<a href="/pdf/2402.07217" title="Download PDF">pdf</a>, <a href="/ps/2402.07217" title="Download PostScript">ps</a>, <a href="/format/2402.07217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Content and structure of laboratory packages for software engineering  experiments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Solari%2C+M">Mart&#xed;n Solari</a>, 
<a href="/search/cs?searchtype=author&query=Vegas%2C+S">Sira Vegas</a>, 
<a href="/search/cs?searchtype=author&query=Juristo%2C+N">Natalia Juristo</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Information and Software Technology, 97, 64-79, 2018
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Context: Experiment replications play a central role in the scientific
method. Although software engineering experimentation has matured a great deal,
the number of experiment replications is still relatively small. Software
engineering experiments are composed of complex concepts, procedures and
artefacts. Laboratory packages are a means of transfer-ring knowledge among
researchers to facilitate experiment replications. Objective: This paper
investigates the experiment replication process to find out what information is
needed to successfully replicate an experiment. Our objective is to propose the
content and structure of laboratory packages for software engineering
experiments. Method: We evaluated seven replications of three different
families of experiments. Each replication had a different experimenter who was,
at the time, unfamiliar with the experi-ment. During the first iterations of
the study, we identified experimental incidents and then proposed a laboratory
package structure that addressed these incidents, including docu-ment usability
improvements. We used the later iterations to validate and generalize the
laboratory package structure for use in all software engineering experiments.
We aimed to solve a specific problem, while at the same time looking at how to
contribute to the body of knowledge on laboratory packages. Results: We
generated a laboratory package for three different experiments. These packages
eased the replication of the respective experiments. The evaluation that we
conducted shows that the laboratory package proposal is acceptable and reduces
the effort currently required to replicate experiments in software engineering.
Conclusion: We think that the content and structure that we propose for
laboratory pack-ages can be useful for other software engineering experiments.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07218" title="Abstract">arXiv:2402.07218</a> [<a href="/pdf/2402.07218" title="Download PDF">pdf</a>, <a href="/format/2402.07218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sensor Misalignment-tolerant AUV Navigation with Passive DoA and Doppler  Measurements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bingbing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shanmin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+D">Daxiong Ji</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+T">Tian Xia</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wen Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We present a sensor misalignment-tolerant AUV navigation method that
leverages measurements from an acoustic array and dead reckoned information.
Recent studies have demonstrated the potential use of passive acoustic
Direction of Arrival (DoA) measurements for AUV navigation without requiring
ranging measurements. However, the sensor misalignment between the acoustic
array and the attitude sensor was not accounted for. Such misalignment may
deteriorate the navigation accuracy. This paper proposes a novel approach that
allows simultaneous AUV navigation, beacon localization, and sensor alignment.
An Unscented Kalman Filter (UKF) that enables the necessary calculations to be
completed at an affordable computational load is developed. A Nonlinear Least
Squares (NLS)-based technique is employed to find an initial solution for
beacon localization and sensor alignment as early as possible using a
short-term window of measurements. Experimental results demonstrate the
performance of the proposed method.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07221" title="Abstract">arXiv:2402.07221</a> [<a href="/pdf/2402.07221" title="Download PDF">pdf</a>, <a href="/format/2402.07221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Reasons that Agents Act: Intention and Instrumental Goals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ward%2C+F+R">Francis Rhys Ward</a>, 
<a href="/search/cs?searchtype=author&query=MacDermott%2C+M">Matt MacDermott</a>, 
<a href="/search/cs?searchtype=author&query=Belardinelli%2C+F">Francesco Belardinelli</a>, 
<a href="/search/cs?searchtype=author&query=Toni%2C+F">Francesca Toni</a>, 
<a href="/search/cs?searchtype=author&query=Everitt%2C+T">Tom Everitt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAMAS24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Intention is an important and challenging concept in AI. It is important
because it underlies many other concepts we care about, such as agency,
manipulation, legal responsibility, and blame. However, ascribing intent to AI
systems is contentious, and there is no universally accepted theory of
intention applicable to AI agents. We operationalise the intention with which
an agent acts, relating to the reasons it chooses its decision. We introduce a
formal definition of intention in structural causal influence models, grounded
in the philosophy literature on intent and applicable to real-world machine
learning systems. Through a number of examples and results, we show that our
definition captures the intuitive notion of intent and satisfies desiderata
set-out by past work. In addition, we show how our definition relates to past
concepts, including actual causality, and the notion of instrumental goals,
which is a core idea in the literature on safe AI agents. Finally, we
demonstrate how our definition can be used to infer the intentions of
reinforcement learning agents and language models from their behaviour.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07222" title="Abstract">arXiv:2402.07222</a> [<a href="/pdf/2402.07222" title="Download PDF">pdf</a>, <a href="/format/2402.07222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On (Mis)perceptions of testing effectiveness: an empirical study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vegas%2C+S">Sira Vegas</a>, 
<a href="/search/cs?searchtype=author&query=Riofrio%2C+P">Patricia Riofrio</a>, 
<a href="/search/cs?searchtype=author&query=Marcos%2C+E">Esperanza Marcos</a>, 
<a href="/search/cs?searchtype=author&query=Juristo%2C+N">Natalia Juristo</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Empirical Software Engineering Journal, 25, pp. 2844-2896, 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">A recurring problem in software development is incorrect decision making on
the techniques, methods and tools to be used. Mostly, these decisions are based
on developers' perceptions about them. A factor influencing people's
perceptions is past experience, but it is not the only one. In this research,
we aim to discover how well the perceptions of the defect detection
effectiveness of different techniques match their real effectiveness in the
absence of prior experience. To do this, we conduct an empirical study plus a
replication. During the original study, we conduct a controlled experiment with
students applying two testing techniques and a code review technique. At the
end of the experiment, they take a survey to find out which technique they
perceive to be most effective. The results show that participants' perceptions
are wrong and that this mismatch is costly in terms of quality. In order to
gain further insight into the results, we replicate the controlled experiment
and extend the survey to include questions about participants' opinions on the
techniques and programs. The results of the replicated study confirm the
findings of the original study and suggest that participants' perceptions might
be based not on their opinions about complexity or preferences for techniques
but on how well they think that they have applied the techniques.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07225" title="Abstract">arXiv:2402.07225</a> [<a href="/pdf/2402.07225" title="Download PDF">pdf</a>, <a href="/format/2402.07225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Graph Masked Autoencoders through Alignment and Uniformity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+X">Xiang Tao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Self-supervised learning on graphs can be bifurcated into contrastive and
generative methods. Contrastive methods, also known as graph contrastive
learning (GCL), have dominated graph self-supervised learning in the past few
years, but the recent advent of graph masked autoencoder (GraphMAE) rekindles
the momentum behind generative methods. Despite the empirical success of
GraphMAE, there is still a dearth of theoretical understanding regarding its
efficacy. Moreover, while both generative and contrastive methods have been
shown to be effective, their connections and differences have yet to be
thoroughly investigated. Therefore, we theoretically build a bridge between
GraphMAE and GCL, and prove that the node-level reconstruction objective in
GraphMAE implicitly performs context-level GCL. Based on our theoretical
analysis, we further identify the limitations of the GraphMAE from the
perspectives of alignment and uniformity, which have been considered as two key
properties of high-quality representations in GCL. We point out that GraphMAE's
alignment performance is restricted by the masking strategy, and the uniformity
is not strictly guaranteed. To remedy the aforementioned limitations, we
propose an Alignment-Uniformity enhanced Graph Masked AutoEncoder, named
AUG-MAE. Specifically, we propose an easy-to-hard adversarial masking strategy
to provide hard-to-align samples, which improves the alignment performance.
Meanwhile, we introduce an explicit uniformity regularizer to ensure the
uniformity of the learned representations. Experimental results on benchmark
datasets demonstrate the superiority of our model over existing
state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07226" title="Abstract">arXiv:2402.07226</a> [<a href="/pdf/2402.07226" title="Download PDF">pdf</a>, <a href="/format/2402.07226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stitching Sub-Trajectories with Conditional Diffusion Model for  Goal-Conditioned Offline RL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungyoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yunseon Choi</a>, 
<a href="/search/cs?searchtype=author&query=Matsunaga%2C+D+E">Daiki E. Matsunaga</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kee-Eung Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Offline Goal-Conditioned Reinforcement Learning (Offline GCRL) is an
important problem in RL that focuses on acquiring diverse goal-oriented skills
solely from pre-collected behavior datasets. In this setting, the reward
feedback is typically absent except when the goal is achieved, which makes it
difficult to learn policies especially from a finite dataset of suboptimal
behaviors. In addition, realistic scenarios involve long-horizon planning,
which necessitates the extraction of useful skills within sub-trajectories.
Recently, the conditional diffusion model has been shown to be a promising
approach to generate high-quality long-horizon plans for RL. However, their
practicality for the goal-conditioned setting is still limited due to a number
of technical assumptions made by the methods. In this paper, we propose SSD
(Sub-trajectory Stitching with Diffusion), a model-based offline GCRL method
that leverages the conditional diffusion model to address these limitations. In
summary, we use the diffusion model that generates future plans conditioned on
the target goal and value, with the target value estimated from the
goal-relabeled offline dataset. We report state-of-the-art performance in the
standard benchmark set of GCRL tasks, and demonstrate the capability to
successfully stitch the segments of suboptimal trajectories in the offline data
to generate high-quality plans.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07229" title="Abstract">arXiv:2402.07229</a> [<a href="/pdf/2402.07229" title="Download PDF">pdf</a>, <a href="/format/2402.07229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Successive Refinement in Large-Scale Computation: Advancing Model  Inference Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Esfahanizadeh%2C+H">Homa Esfahanizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+A">Alejandro Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Shamai%2C+S">Shlomo Shamai</a> (Shitz), 
<a href="/search/cs?searchtype=author&query=Medard%2C+M">Muriel Medard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, partially appeared in proceedings of IEEE Cloudnet 2022, submitted and under review for IEEE Transactions on Signal Processing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Modern computationally-intensive applications often operate under time
constraints, necessitating acceleration methods and distribution of
computational workloads across multiple entities. However, the outcome is
either achieved within the desired timeline or not, and in the latter case,
valuable resources are wasted. In this paper, we introduce solutions for
layered-resolution computation. These solutions allow lower-resolution results
to be obtained at an earlier stage than the final result. This innovation
notably enhances the deadline-based systems, as if a computational job is
terminated due to time constraints, an approximate version of the final result
can still be generated. Moreover, in certain operational regimes, a
high-resolution result might be unnecessary, because the low-resolution result
may already deviate significantly from the decision threshold, for example in
AI-based decision-making systems. Therefore, operators can decide whether
higher resolution is needed or not based on intermediate results, enabling
computations with adaptive resolution. We present our framework for two
critical and computationally demanding jobs: distributed matrix multiplication
(linear) and model inference in machine learning (nonlinear). Our theoretical
and empirical results demonstrate that the execution delay for the first
resolution is significantly shorter than that for the final resolution, while
maintaining overall complexity comparable to the conventional one-shot
approach. Our experiments further illustrate how the layering feature increases
the likelihood of meeting deadlines and enables adaptability and transparency
in massive, large-scale computations.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07230" title="Abstract">arXiv:2402.07230</a> [<a href="/pdf/2402.07230" title="Download PDF">pdf</a>, <a href="/ps/2402.07230" title="Download PostScript">ps</a>, <a href="/format/2402.07230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Principal Types as Partial Involutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Honsell%2C+F">Furio Honsell</a>, 
<a href="/search/cs?searchtype=author&query=Lenisa%2C+M">Marina Lenisa</a>, 
<a href="/search/cs?searchtype=author&query=Scagnetto%2C+I">Ivan Scagnetto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">We show that the principal types of the closed terms of the affine fragment
of $\lambda$-calculus, with respect to a simple type discipline, are
structurally isomorphic to their interpretations, as partial involutions, in a
natural Geometry of Interaction model \`a la Abramsky. This permits to explain
in elementary terms the somewhat awkward notion of linear application arising
in Geometry of Interaction, simply as the resolution between principal types
using an alternate unification algorithm. As a consequence, we provide an
answer, for the purely affine fragment, to the open problem raised by Abramsky
of characterising those partial involutions which are denotations of
combinatory terms.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07232" title="Abstract">arXiv:2402.07232</a> [<a href="/pdf/2402.07232" title="Download PDF">pdf</a>, <a href="/format/2402.07232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenSTL: General Sparse Trajectory Learning via Auto-regressive  Generation of Feature Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jilin Hu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Shengnan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Bin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jensen%2C+C+S">Christian S. Jensen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Youfang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+H">Huaiyu Wan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Trajectories are sequences of timestamped location samples. In sparse
trajectories, the locations are sampled infrequently; and while such
trajectories are prevalent in real-world settings, they are challenging to use
to enable high-quality transportation-related applications. Current
methodologies either assume densely sampled and accurately map-matched
trajectories, or they rely on two-stage schemes, yielding sub-optimal
applications.
<br />To extend the utility of sparse trajectories, we propose a novel sparse
trajectory learning framework, GenSTL. The framework is pre-trained to form
connections between sparse trajectories and dense counterparts using
auto-regressive generation of feature domains. GenSTL can subsequently be
applied directly in downstream tasks, or it can be fine-tuned first. This way,
GenSTL eliminates the reliance on the availability of large-scale dense and
map-matched trajectory data. The inclusion of a well-crafted feature domain
encoding layer and a hierarchical masked trajectory encoder enhances GenSTL's
learning capabilities and adaptability. Experiments on two real-world
trajectory datasets offer insight into the framework's ability to contend with
sparse trajectories with different sampling intervals and its versatility
across different downstream tasks, thus offering evidence of its practicality
in real-world applications.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07233" title="Abstract">arXiv:2402.07233</a> [<a href="/pdf/2402.07233" title="Download PDF">pdf</a>, <a href="/format/2402.07233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TransGPT: Multi-modal Generative Pre-trained Transformer for  Transportation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xiang Wei</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+F">Fangxu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+W">Wenjuan Han</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Natural language processing (NLP) is a key component of intelligent
transportation systems (ITS), but it faces many challenges in the
transportation domain, such as domain-specific knowledge and data, and
multi-modal inputs and outputs. This paper presents TransGPT, a novel
(multi-modal) large language model for the transportation domain, which
consists of two independent variants: TransGPT-SM for single-modal data and
TransGPT-MM for multi-modal data. TransGPT-SM is finetuned on a single-modal
Transportation dataset (STD) that contains textual data from various sources in
the transportation domain. TransGPT-MM is finetuned on a multi-modal
Transportation dataset (MTD) that we manually collected from three areas of the
transportation domain: driving tests, traffic signs, and landmarks. We evaluate
TransGPT on several benchmark datasets for different tasks in the
transportation domain, and show that it outperforms baseline models on most
tasks. We also showcase the potential applications of TransGPT for traffic
analysis and modeling, such as generating synthetic traffic scenarios,
explaining traffic phenomena, answering traffic-related questions, providing
traffic recommendations, and generating traffic reports. This work advances the
state-of-the-art of NLP in the transportation domain and provides a useful tool
for ITS researchers and practitioners.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07234" title="Abstract">arXiv:2402.07234</a> [<a href="/pdf/2402.07234" title="Download PDF">pdf</a>, <a href="/format/2402.07234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for  Chinese Public Security Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tong%2C+X">Xin Tong</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+B">Bo Jin</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Binjun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Ting Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated significant potential and
effectiveness across multiple application domains. To assess the performance of
mainstream LLMs in public security tasks, this study aims to construct a
specialized evaluation benchmark tailored to the Chinese public security
domain--CPSDbench. CPSDbench integrates datasets related to public security
collected from real-world scenarios, supporting a comprehensive assessment of
LLMs across four key dimensions: text classification, information extraction,
question answering, and text generation. Furthermore, this study introduces a
set of innovative evaluation metrics designed to more precisely quantify the
efficacy of LLMs in executing tasks related to public security. Through the
in-depth analysis and evaluation conducted in this research, we not only
enhance our understanding of the performance strengths and limitations of
existing models in addressing public security issues but also provide
references for the future development of more accurate and customized LLM
models targeted at applications in this field.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07241" title="Abstract">arXiv:2402.07241</a> [<a href="/pdf/2402.07241" title="Download PDF">pdf</a>, <a href="/format/2402.07241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proof of Diligence: Cryptoeconomic Security for Rollups
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sheng%2C+P">Peiyao Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Rana%2C+R">Ranvir Rana</a>, 
<a href="/search/cs?searchtype=author&query=Tyagi%2C+H">Himanshu Tyagi</a>, 
<a href="/search/cs?searchtype=author&query=Viswanath%2C+P">Pramod Viswanath</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Layer 1 (L1) blockchains such as Ethereum are secured under an "honest
supermajority of stake" assumption for a large pool of validators who verify
each and every transaction on it. This high security comes at a scalability
cost which not only effects the throughput of the blockchain but also results
in high gas fees for executing transactions on chain. The most successful
solution for this problem is provided by optimistic rollups, Layer 2 (L2)
blockchains that execute transactions outside L1 but post the transaction data
on L1. The security for such L2 chains is argued, informally, under the
assumption that a set of nodes will check the transaction data posted on L1 and
raise an alarm (a fraud proof) if faulty transactions are detected. However,
all current deployments lack a proper incentive mechanism for ensuring that
these nodes will do their job ``diligently'', and simply rely on a cursory
incentive alignment argument for security. We solve this problem by introducing
an incentivized watchtower network designed to serve as the first line of
defense for rollups. Our main contribution is a ``Proof of Diligence'' protocol
that requires watchtowers to continuously provide a proof that they have
verified L2 assertions and get rewarded for the same. Proof of Diligence
protocol includes a carefully-designed incentive mechanism that is provably
secure when watchtowers are rational actors, under a mild rational independence
assumption.
<br />Our proposed system is now live on Ethereum testnet. We deployed a watchtower
network and implemented Proof of Diligence for multiple optimistic rollups. We
extract execution as well as inclusion proofs for transactions as a part of the
bounty. Each watchtower has minimal additional computational overhead beyond
access to standard L1 and L2 RPC nodes.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07242" title="Abstract">arXiv:2402.07242</a> [<a href="/pdf/2402.07242" title="Download PDF">pdf</a>, <a href="/format/2402.07242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Genetically-Driven Synaptogenesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boccato%2C+T">Tommaso Boccato</a>, 
<a href="/search/cs?searchtype=author&query=Ferrante%2C+M">Matteo Ferrante</a>, 
<a href="/search/cs?searchtype=author&query=Toschi%2C+N">Nicola Toschi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">In this paper we introduce SynaptoGen, a novel framework that aims to bridge
the gap between genetic manipulations and neuronal network behavior by
simulating synaptogenesis and guiding the development of neuronal networks
capable of solving predetermined computational tasks. Drawing inspiration from
recent advancements in the field, we propose SynaptoGen as a bio-plausible
approach to modeling synaptogenesis through differentiable functions. To
validate SynaptoGen, we conduct a preliminary experiment using reinforcement
learning as a benchmark learning framework, demonstrating its effectiveness in
generating neuronal networks capable of solving the OpenAI Gym's Cart Pole
task, compared to carefully designed baselines. The results highlight the
potential of SynaptoGen to inspire further advancements in neuroscience and
computational modeling, while also acknowledging the need for incorporating
more realistic genetic rules and synaptic conductances in future research.
Overall, SynaptoGen represents a promising avenue for exploring the
intersection of genetics, neuroscience, and artificial intelligence.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07243" title="Abstract">arXiv:2402.07243</a> [<a href="/pdf/2402.07243" title="Download PDF">pdf</a>, <a href="/format/2402.07243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PIVOT-Net: Heterogeneous Point-Voxel-Tree-based Framework for Point  Cloud Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+J">Jiahao Pang</a>, 
<a href="/search/cs?searchtype=author&query=Bui%2C+K">Kevin Bui</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+D">Dong Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at 3DV 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">The universality of the point cloud format enables many 3D applications,
making the compression of point clouds a critical phase in practice. Sampled as
discrete 3D points, a point cloud approximates 2D surface(s) embedded in 3D
with a finite bit-depth. However, the point distribution of a practical point
cloud changes drastically as its bit-depth increases, requiring different
methodologies for effective consumption/analysis. In this regard, a
heterogeneous point cloud compression (PCC) framework is proposed. We unify
typical point cloud representations -- point-based, voxel-based, and tree-based
representations -- and their associated backbones under a learning-based
framework to compress an input point cloud at different bit-depth levels.
Having recognized the importance of voxel-domain processing, we augment the
framework with a proposed context-aware upsampling for decoding and an enhanced
voxel transformer for feature aggregation. Extensive experimentation
demonstrates the state-of-the-art performance of our proposal on a wide range
of point clouds.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07244" title="Abstract">arXiv:2402.07244</a> [<a href="/pdf/2402.07244" title="Download PDF">pdf</a>, <a href="/format/2402.07244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAIS: A Novel Bio-Inspired Artificial Immune System Based on Symbiotic  Paradigm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Junhao Song</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yingfang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+W">Wei Pang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We propose a novel type of Artificial Immune System (AIS): Symbiotic
Artificial Immune Systems (SAIS), drawing inspiration from symbiotic
relationships in biology. SAIS parallels the three key stages (i.e., mutualism,
commensalism and parasitism) of population updating from the Symbiotic
Organisms Search (SOS) algorithm. This parallel approach effectively addresses
the challenges of large population size and enhances population diversity in
AIS, which traditional AIS and SOS struggle to resolve efficiently. We
conducted a series of experiments, which demonstrated that our SAIS achieved
comparable performance to the state-of-the-art approach SOS and outperformed
other popular AIS approaches and evolutionary algorithms across 26 benchmark
problems. Furthermore, we investigated the problem of parameter selection and
found that SAIS performs better in handling larger population sizes while
requiring fewer generations. Finally, we believe SAIS, as a novel bio-inspired
and immune-inspired algorithm, paves the way for innovation in bio-inspired
computing with the symbiotic paradigm.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07246" title="Abstract">arXiv:2402.07246</a> [<a href="/pdf/2402.07246" title="Download PDF">pdf</a>, <a href="/format/2402.07246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Generalized Inverse Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+C">Chaosheng Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yijia Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper studies generalized inverse reinforcement learning (GIRL) in
Markov decision processes (MDPs), that is, the problem of learning the basic
components of an MDP given observed behavior (policy) that might not be
optimal. These components include not only the reward function and transition
probability matrices, but also the action space and state space that are not
exactly known but are known to belong to given uncertainty sets. We address two
key challenges in GIRL: first, the need to quantify the discrepancy between the
observed policy and the underlying optimal policy; second, the difficulty of
mathematically characterizing the underlying optimal policy when the basic
components of an MDP are unobservable or partially observable. Then, we propose
the mathematical formulation for GIRL and develop a fast heuristic algorithm.
Numerical results on both finite and infinite state problems show the merit of
our formulation and algorithm.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07248" title="Abstract">arXiv:2402.07248</a> [<a href="/pdf/2402.07248" title="Download PDF">pdf</a>, <a href="/ps/2402.07248" title="Download PostScript">ps</a>, <a href="/format/2402.07248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Depth Separations in Neural Networks: Separating the Dimension from the  Accuracy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Safran%2C+I">Itay Safran</a>, 
<a href="/search/cs?searchtype=author&query=Reichman%2C+D">Daniel Reichman</a>, 
<a href="/search/cs?searchtype=author&query=Valiant%2C+P">Paul Valiant</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We prove an exponential separation between depth 2 and depth 3 neural
networks, when approximating an $\mathcal{O}(1)$-Lipschitz target function to
constant accuracy, with respect to a distribution with support in $[0,1]^{d}$,
assuming exponentially bounded weights. This addresses an open problem posed in
\citet{safran2019depth}, and proves that the curse of dimensionality manifests
in depth 2 approximation, even in cases where the target function can be
represented efficiently using depth 3. Previously, lower bounds that were used
to separate depth 2 from depth 3 required that at least one of the Lipschitz
parameter, target accuracy or (some measure of) the size of the domain of
approximation scale polynomially with the input dimension, whereas we fix the
former two and restrict our domain to the unit hypercube. Our lower bound holds
for a wide variety of activation functions, and is based on a novel application
of an average- to worst-case random self-reducibility argument, to reduce the
problem to threshold circuits lower bounds.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07249" title="Abstract">arXiv:2402.07249</a> [<a href="/pdf/2402.07249" title="Download PDF">pdf</a>, <a href="/format/2402.07249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Impact of Domain Knowledge and Multi-Modality on Intelligent  Molecular Property Prediction: A Systematic Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuang%2C+T">Taojie Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Pengfei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhixiang Ren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE); Biomolecules (q-bio.BM)

</div>
<p class="mathjax">The precise prediction of molecular properties is essential for advancements
in drug development, particularly in virtual screening and compound
optimization. The recent introduction of numerous deep learning-based methods
has shown remarkable potential in enhancing molecular property prediction
(MPP), especially improving accuracy and insights into molecular structures.
Yet, two critical questions arise: does the integration of domain knowledge
augment the accuracy of molecular property prediction and does employing
multi-modal data fusion yield more precise results than unique data source
methods? To explore these matters, we comprehensively review and quantitatively
analyze recent deep learning methods based on various benchmarks. We discover
that integrating molecular information will improve both MPP regression and
classification tasks by upto 3.98% and 1.72%, respectively. We also discover
that the utilizing 3-dimensional information with 1-dimensional and
2-dimensional information simultaneously can substantially enhance MPP upto
4.2%. The two consolidated insights offer crucial guidance for future
advancements in drug discovery.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07250" title="Abstract">arXiv:2402.07250</a> [<a href="/pdf/2402.07250" title="Download PDF">pdf</a>, <a href="/format/2402.07250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DIMON: Learning Solution Operators of Partial Differential Equations on  a Diffeomorphic Family of Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+M">Minglang Yin</a>, 
<a href="/search/cs?searchtype=author&query=Charon%2C+N">Nicolas Charon</a>, 
<a href="/search/cs?searchtype=author&query=Brody%2C+R">Ryan Brody</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Lu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Trayanova%2C+N">Natalia Trayanova</a>, 
<a href="/search/cs?searchtype=author&query=Maggioni%2C+M">Mauro Maggioni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">The solution of a PDE over varying initial/boundary conditions on multiple
domains is needed in a wide variety of applications, but it is computationally
expensive if the solution is computed de novo whenever the initial/boundary
conditions of the domain change. We introduce a general operator learning
framework, called DIffeomorphic Mapping Operator learNing (DIMON) to learn
approximate PDE solutions over a family of domains $\{\Omega_{\theta}}_\theta$,
that learns the map from initial/boundary conditions and domain $\Omega_\theta$
to the solution of the PDE, or to specified functionals thereof. DIMON is based
on transporting a given problem (initial/boundary conditions and domain
$\Omega_{\theta}$) to a problem on a reference domain $\Omega_{0}$, where
training data from multiple problems is used to learn the map to the solution
on $\Omega_{0}$, which is then re-mapped to the original domain
$\Omega_{\theta}$. We consider several problems to demonstrate the performance
of the framework in learning both static and time-dependent PDEs on non-rigid
geometries; these include solving the Laplace equation, reaction-diffusion
equations, and a multiscale PDE that characterizes the electrical propagation
on the left ventricle. This work paves the way toward the fast prediction of
PDE solutions on a family of domains and the application of neural operators in
engineering and precision medicine.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07251" title="Abstract">arXiv:2402.07251</a> [<a href="/pdf/2402.07251" title="Download PDF">pdf</a>, <a href="/format/2402.07251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-Informed Neural Networks with Hard Linear Equality Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Flores%2C+G+E+C">Gonzalo E. Constante Flores</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Can Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Surrogate modeling is used to replace computationally expensive simulations.
Neural networks have been widely applied as surrogate models that enable
efficient evaluations over complex physical systems. Despite this, neural
networks are data-driven models and devoid of any physics. The incorporation of
physics into neural networks can improve generalization and data efficiency.
The physics-informed neural network (PINN) is an approach to leverage known
physical constraints present in the data, but it cannot strictly satisfy them
in the predictions. This work proposes a novel physics-informed neural network,
KKT-hPINN, which rigorously guarantees hard linear equality constraints through
projection layers derived from KKT conditions. Numerical experiments on Aspen
models of a continuous stirred-tank reactor (CSTR) unit, an extractive
distillation subsystem, and a chemical plant demonstrate that this model can
further enhance the prediction accuracy.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07255" title="Abstract">arXiv:2402.07255</a> [<a href="/pdf/2402.07255" title="Download PDF">pdf</a>, <a href="/format/2402.07255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> American Sign Language Video to Text Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roy%2C+P">Parsheeta Roy</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Ji-Eun Han</a>, 
<a href="/search/cs?searchtype=author&query=Chouhan%2C+S">Srishti Chouhan</a>, 
<a href="/search/cs?searchtype=author&query=Thumu%2C+B">Bhaavanaa Thumu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Sign language to text is a crucial technology that can break down
communication barriers for individuals with hearing difficulties. We replicate
and try to improve on a recently published study. We evaluate models using BLEU
and rBLEU metrics to ensure translation quality. During our ablation study, we
found that the model's performance is significantly influenced by optimizers,
activation functions, and label smoothing. Further research aims to refine
visual feature capturing, enhance decoder utilization, and integrate
pre-trained decoders for better translation outcomes. Our source code is
available to facilitate replication of our results and encourage future
research.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07258" title="Abstract">arXiv:2402.07258</a> [<a href="/pdf/2402.07258" title="Download PDF">pdf</a>, <a href="/format/2402.07258" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Quality Aware Approaches for Addressing Model Drift of Semantic  Segmentation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mirza%2C+S">Samiha Mirza</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+V+D">Vuong D. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Mantini%2C+P">Pranav Mantini</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+S+K">Shishir K. Shah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In the midst of the rapid integration of artificial intelligence (AI) into
real world applications, one pressing challenge we confront is the phenomenon
of model drift, wherein the performance of AI models gradually degrades over
time, compromising their effectiveness in real-world, dynamic environments.
Once identified, we need techniques for handling this drift to preserve the
model performance and prevent further degradation. This study investigates two
prominent quality aware strategies to combat model drift: data quality
assessment and data conditioning based on prior model knowledge. The former
leverages image quality assessment metrics to meticulously select high-quality
training data, improving the model robustness, while the latter makes use of
learned feature vectors from existing models to guide the selection of future
data, aligning it with the model's prior knowledge. Through comprehensive
experimentation, this research aims to shed light on the efficacy of these
approaches in enhancing the performance and reliability of semantic
segmentation models, thereby contributing to the advancement of computer vision
capabilities in real-world scenarios.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07261" title="Abstract">arXiv:2402.07261</a> [<a href="/pdf/2402.07261" title="Download PDF">pdf</a>, <a href="/format/2402.07261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Technique to Parameterize Congestion Control in 6TiSCH IIoT  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+K">Kushal Chakraborty</a> (1), 
<a href="/search/cs?searchtype=author&query=Dutta%2C+A+K">Aritra Kumar Dutta</a> (1), 
<a href="/search/cs?searchtype=author&query=Hussain%2C+M+A">Mohammad Avesh Hussain</a> (1), 
<a href="/search/cs?searchtype=author&query=Mohiuddin%2C+S+R">Syed Raafay Mohiuddin</a> (1), 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+N">Nikumani Choudhury</a> (1), 
<a href="/search/cs?searchtype=author&query=Matam%2C+R">Rakesh Matam</a> (2), 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+M">Mithun Mukherjee</a> (3) ((1) Dept. of Computer Science &amp; Information Systems, Birla Institute of Technology &amp; Science, Pilani, Hyderabad, India, (2) Dept. of Computer Science &amp; Engineering, Indian Institute of Information Technology Guwahati, India, (3) Nanjing University of Information Science and Technology, Nanjing, China)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper has been submitted, accepted, and presented at the 2023 IEEE Global Communications Conference: Next-Generation Networking and Internet, with plans for publication. It was delivered during the IEEE Global Communications Conference held on December 6th, 2023, in Kuala Lumpur, Malaysia
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The Industrial Internet of Things (IIoT) refers to the use of interconnected
smart devices, sensors, and other technologies to create a network of
intelligent systems that can monitor and manage industrial processes. 6TiSCH
(IPv6 over the Time Slotted Channel Hopping mode of IEEE 802.15.4e) as an
enabling technology facilitates low-power and low-latency communication between
IoT devices in industrial environments. The Routing Protocol for Low power and
lossy networks (RPL), which is used as the de-facto routing protocol for 6TiSCH
networks is observed to suffer from several limitations, especially during
congestion in the network. Therefore, there is an immediate need for some
modifications to the RPL to deal with this problem. Under traffic load which
keeps on changing continuously at different instants of time, the proposed
mechanism aims at finding the appropriate parent for a node that can forward
the packet to the destination through the least congested path with minimal
packet loss. This facilitates congestion management under dynamic traffic
loads. For this, a new metric for routing using the concept of exponential
weighting has been proposed, which takes the number of packets present in the
queue of the node into account when choosing the parent at a particular
instance of time. Additionally, the paper proposes a parent selection and
swapping mechanism for congested networks. Performance evaluations are carried
out in order to validate the proposed work. The results show an improvement in
the performance of RPL under heavy and dynamic traffic loads.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07262" title="Abstract">arXiv:2402.07262</a> [<a href="/pdf/2402.07262" title="Download PDF">pdf</a>, <a href="/format/2402.07262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Resource Counterspeech Generation for Indic Languages: The Case of  Bengali and Hindi
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+M">Mithun Das</a>, 
<a href="/search/cs?searchtype=author&query=Pandey%2C+S+K">Saurabh Kumar Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Sethi%2C+S">Shivansh Sethi</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+P">Punyajoy Saha</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+A">Animesh Mukherjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the Findings of the ACL: EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">With the rise of online abuse, the NLP community has begun investigating the
use of neural architectures to generate counterspeech that can "counter" the
vicious tone of such abusive speech and dilute/ameliorate their rippling effect
over the social network. However, most of the efforts so far have been
primarily focused on English. To bridge the gap for low-resource languages such
as Bengali and Hindi, we create a benchmark dataset of 5,062 abusive
speech/counterspeech pairs, of which 2,460 pairs are in Bengali and 2,602 pairs
are in Hindi. We implement several baseline models considering various
interlingual transfer mechanisms with different configurations to generate
suitable counterspeech to set up an effective benchmark. We observe that the
monolingual setup yields the best performance. Further, using synthetic
transfer, language models can generate counterspeech to some extent;
specifically, we notice that transferability is better when languages belong to
the same language family.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07263" title="Abstract">arXiv:2402.07263</a> [<a href="/pdf/2402.07263" title="Download PDF">pdf</a>, <a href="/ps/2402.07263" title="Download PostScript">ps</a>, <a href="/format/2402.07263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trade-off Between Spatial and Angular Resolution in Facial Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alam%2C+M+Z">Muhammad Zeshan Alam</a>, 
<a href="/search/cs?searchtype=author&query=kelowani%2C+S">Sousso kelowani</a>, 
<a href="/search/cs?searchtype=author&query=Elsaeidy%2C+M">Mohamed Elsaeidy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages,5 figures,International Conference on Emerging Trends and Applications in Artificial Intelligence (ICETAI) [Accepted]
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Ensuring robustness in face recognition systems across various challenging
conditions is crucial for their versatility. State-of-the-art methods often
incorporate additional information, such as depth, thermal, or angular data, to
enhance performance. However, light field-based face recognition approaches
that leverage angular information face computational limitations. This paper
investigates the fundamental trade-off between spatio-angular resolution in
light field representation to achieve improved face recognition performance. By
utilizing macro-pixels with varying angular resolutions while maintaining the
overall image size, we aim to quantify the impact of angular information at the
expense of spatial resolution, while considering computational constraints. Our
experimental results demonstrate a notable performance improvement in face
recognition systems by increasing the angular resolution, up to a certain
extent, at the cost of spatial resolution.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07270" title="Abstract">arXiv:2402.07270</a> [<a href="/pdf/2402.07270" title="Download PDF">pdf</a>, <a href="/format/2402.07270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-ended VQA benchmarking of Vision-Language models by exploiting  Classification datasets and their semantic hierarchy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ging%2C+S">Simon Ging</a>, 
<a href="/search/cs?searchtype=author&query=Bravo%2C+M+A">Mar&#xed;a A. Bravo</a>, 
<a href="/search/cs?searchtype=author&query=Brox%2C+T">Thomas Brox</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as Spotlight Paper for ICLR 2024. The first two authors contributed equally to this work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">The evaluation of text-generative vision-language models is a challenging yet
crucial endeavor. By addressing the limitations of existing Visual Question
Answering (VQA) benchmarks and proposing innovative evaluation methodologies,
our research seeks to advance our understanding of these models' capabilities.
We propose a novel VQA benchmark based on well-known visual classification
datasets which allows a granular evaluation of text-generative vision-language
models and their comparison with discriminative vision-language models. To
improve the assessment of coarse answers on fine-grained classification tasks,
we suggest using the semantic hierarchy of the label space to ask automatically
generated follow-up questions about the ground-truth category. Finally, we
compare traditional NLP and LLM-based metrics for the problem of evaluating
model predictions given ground-truth answers. We perform a human evaluation
study upon which we base our decision on the final metric. We apply our
benchmark to a suite of vision-language models and show a detailed comparison
of their abilities on object, action, and attribute classification. Our
contributions aim to lay the foundation for more precise and meaningful
assessments, facilitating targeted progress in the exciting field of
vision-language modeling.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07271" title="Abstract">arXiv:2402.07271</a> [<a href="/pdf/2402.07271" title="Download PDF">pdf</a>, <a href="/format/2402.07271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Previously on the Stories: Recap Snippet Identification for Story  Reading
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiangnan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qiujing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Liyan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+W">Wenjie Pang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+M">Mo Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiping Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Similar to the "previously-on" scenes in TV shows, recaps can help book
reading by recalling the readers' memory about the important elements in
previous texts to better understand the ongoing plot. Despite its usefulness,
this application has not been well studied in the NLP community. We propose the
first benchmark on this useful task called Recap Snippet Identification with a
hand-crafted evaluation dataset. Our experiments show that the proposed task is
challenging to PLMs, LLMs, and proposed methods as the task requires a deep
understanding of the plot correlation between snippets.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07273" title="Abstract">arXiv:2402.07273</a> [<a href="/pdf/2402.07273" title="Download PDF">pdf</a>, <a href="/format/2402.07273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Finite Element Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bonito%2C+A">Andrea Bonito</a>, 
<a href="/search/math?searchtype=author&query=Canuto%2C+C">Claudio Canuto</a>, 
<a href="/search/math?searchtype=author&query=Nochetto%2C+R+H">Ricardo H. Nochetto</a>, 
<a href="/search/math?searchtype=author&query=Veeser%2C+A">Andreas Veeser</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 317 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This is a survey on the theory of adaptive finite element methods (AFEMs),
which are fundamental in modern computational science and engineering but whose
mathematical assessment is a formidable challenge. We present a self-contained
and up-to-date discussion of AFEMs for linear second order elliptic PDEs and
dimension d&gt;1, with emphasis on foundational issues. After a brief review of
functional analysis and basic finite element theory, including piecewise
polynomial approximation in graded meshes, we present the core material for
coercive problems. We start with a novel a posteriori error analysis applicable
to rough data, which delivers estimators fully equivalent to the solution
error. They are used in the design and study of three AFEMs depending on the
structure of data. We prove linear convergence of these algorithms and
rate-optimality provided the solution and data belong to suitable approximation
classes. We also address the relation between approximation and regularity
classes. We finally extend this theory to discontinuous Galerkin methods as
prototypes of non-conforming AFEMs and beyond coercive problems to inf-sup
stable AFEMs.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07281" title="Abstract">arXiv:2402.07281</a> [<a href="/pdf/2402.07281" title="Download PDF">pdf</a>, <a href="/format/2402.07281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Tree Based Approaches Surpass Deep Learning in Anomaly Detection? A  Benchmarking Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+S">Santonu Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+S">Shanay Mehta</a>, 
<a href="/search/cs?searchtype=author&query=Fernandes%2C+N">Nicole Fernandes</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+J">Jyotirmoy Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+S">Snehanshu Saha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Detection of anomalous situations for complex mission-critical systems holds
paramount importance when their service continuity needs to be ensured. A major
challenge in detecting anomalies from the operational data arises due to the
imbalanced class distribution problem since the anomalies are supposed to be
rare events. This paper evaluates a diverse array of machine learning-based
anomaly detection algorithms through a comprehensive benchmark study. The paper
contributes significantly by conducting an unbiased comparison of various
anomaly detection algorithms, spanning classical machine learning including
various tree-based approaches to deep learning and outlier detection methods.
The inclusion of 104 publicly available and a few proprietary industrial
systems datasets enhances the diversity of the study, allowing for a more
realistic evaluation of algorithm performance and emphasizing the importance of
adaptability to real-world scenarios. The paper dispels the deep learning myth,
demonstrating that though powerful, deep learning is not a universal solution
in this case. We observed that recently proposed tree-based evolutionary
algorithms outperform in many scenarios. We noticed that tree-based approaches
catch a singleton anomaly in a dataset where deep learning methods fail. On the
other hand, classical SVM performs the best on datasets with more than 10%
anomalies, implying that such scenarios can be best modeled as a classification
problem rather than anomaly detection. To our knowledge, such a study on a
large number of state-of-the-art algorithms using diverse data sets, with the
objective of guiding researchers and practitioners in making informed
algorithmic choices, has not been attempted earlier.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07282" title="Abstract">arXiv:2402.07282</a> [<a href="/pdf/2402.07282" title="Download PDF">pdf</a>, <a href="/format/2402.07282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How do Large Language Models Navigate Conflicts between Honesty and  Helpfulness?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Ryan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sumers%2C+T+R">Theodore R. Sumers</a>, 
<a href="/search/cs?searchtype=author&query=Dasgupta%2C+I">Ishita Dasgupta</a>, 
<a href="/search/cs?searchtype=author&query=Griffiths%2C+T+L">Thomas L. Griffiths</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In day-to-day communication, people often approximate the truth - for
example, rounding the time or omitting details - in order to be maximally
helpful to the listener. How do large language models (LLMs) handle such
nuanced trade-offs? To address this question, we use psychological models and
experiments designed to characterize human behavior to analyze LLMs. We test a
range of LLMs and explore how optimization for human preferences or
inference-time reasoning affects these trade-offs. We find that reinforcement
learning from human feedback improves both honesty and helpfulness, while
chain-of-thought prompting skews LLMs towards helpfulness over honesty.
Finally, GPT-4 Turbo demonstrates human-like response patterns including
sensitivity to the conversational framing and listener's decision context. Our
findings reveal the conversational values internalized by LLMs and suggest that
even these abstract values can, to a degree, be steered by zero-shot prompting.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07283" title="Abstract">arXiv:2402.07283</a> [<a href="/pdf/2402.07283" title="Download PDF">pdf</a>, <a href="/ps/2402.07283" title="Download PostScript">ps</a>, <a href="/format/2402.07283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Power Transformer Fault Prediction Based on Knowledge Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Ziyan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chiyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+K">Kai Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">In this paper, we address the challenge of learning with limited fault data
for power transformers. Traditional operation and maintenance tools lack
effective predictive capabilities for potential faults. The scarcity of
extensive fault data makes it difficult to apply machine learning techniques
effectively. To solve this problem, we propose a novel approach that leverages
the knowledge graph (KG) technology in combination with gradient boosting
decision trees (GBDT). This method is designed to efficiently learn from a
small set of high-dimensional data, integrating various factors influencing
transformer faults and historical operational data. Our approach enables
accurate safe state assessments and fault analyses of power transformers
despite the limited fault characteristic data. Experimental results demonstrate
that this method outperforms other learning approaches in prediction accuracy,
such as artificial neural networks (ANN) and logistic regression (LR).
Furthermore, it offers significant improvements in progressiveness,
practicality, and potential for widespread application.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07284" title="Abstract">arXiv:2402.07284</a> [<a href="/pdf/2402.07284" title="Download PDF">pdf</a>, <a href="/format/2402.07284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CLIPPER: Robust Data Association without an Initial Guess
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lusk%2C+P+C">Parker C. Lusk</a>, 
<a href="/search/cs?searchtype=author&query=How%2C+J+P">Jonathan P. How</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures, accepted to RA-L
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Identifying correspondences in noisy data is a critically important step in
estimation processes. When an informative initial estimation guess is
available, the data association challenge is less acute; however, the existence
of a high-quality initial guess is rare in most contexts. We explore
graph-theoretic formulations for data association, which do not require an
initial estimation guess. Existing graph-theoretic approaches optimize over
unweighted graphs, discarding important consistency information encoded in
weighted edges, and frequently attempt to solve NP-hard problems exactly. In
contrast, we formulate a new optimization problem that fully leverages weighted
graphs and seeks the densest edge-weighted clique. We introduce two relaxations
to this problem: a convex semidefinite relaxation which we find to be
empirically tight, and a fast first-order algorithm called CLIPPER which
frequently arrives at nearly-optimal solutions in milliseconds. When evaluated
on point cloud registration problems, our algorithms remain robust up to at
least 95% outliers while existing algorithms begin breaking down at 80%
outliers. Code is available at https://mit-acl.github.io/clipper.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07294" title="Abstract">arXiv:2402.07294</a> [<a href="/pdf/2402.07294" title="Download PDF">pdf</a>, <a href="/format/2402.07294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Effectiveness of Machine Learning-based Call Graph Pruning: An  Empirical Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mir%2C+A+M">Amir M. Mir</a>, 
<a href="/search/cs?searchtype=author&query=Keshani%2C+M">Mehdi Keshani</a>, 
<a href="/search/cs?searchtype=author&query=Proksch%2C+S">Sebastian Proksch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the technical track of MSR'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Programming Languages (cs.PL)

</div>
<p class="mathjax">Static call graph (CG) construction often over-approximates call relations,
leading to sound, but imprecise results. Recent research has explored machine
learning (ML)-based CG pruning as a means to enhance precision by eliminating
false edges. However, current methods suffer from a limited evaluation dataset,
imbalanced training data, and reduced recall, which affects practical
downstream analyses. Prior results were also not compared with advanced static
CG construction techniques yet. This study tackles these issues. We introduce
the NYXCorpus, a dataset of real-world Java programs with high test coverage
and we collect traces from test executions and build a ground truth of dynamic
CGs. We leverage these CGs to explore conservative pruning strategies during
the training and inference of ML-based CG pruners. We conduct a comparative
analysis of static CGs generated using zero control flow analysis (0-CFA) and
those produced by a context-sensitive 1-CFA algorithm, evaluating both with and
without pruning. We find that CG pruning is a difficult task for real-world
Java projects and substantial improvements in the CG precision (+25%) meet
reduced recall (-9%). However, our experiments show promising results: even
when we favor recall over precision by using an F2 metric in our experiments,
we can show that pruned CGs have comparable quality to a context-sensitive
1-CFA analysis while being computationally less demanding. Resulting CGs are
much smaller (69%), and substantially faster (3.5x speed-up), with virtually
unchanged results in our downstream analysis.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07295" title="Abstract">arXiv:2402.07295</a> [<a href="/pdf/2402.07295" title="Download PDF">pdf</a>, <a href="/format/2402.07295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Heterogeneous Client Models using Knowledge Distillation in  Serverless Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chadha%2C+M">Mohak Chadha</a>, 
<a href="/search/cs?searchtype=author&query=Khera%2C+P">Pulkit Khera</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jianfeng Gu</a>, 
<a href="/search/cs?searchtype=author&query=Abboud%2C+O">Osama Abboud</a>, 
<a href="/search/cs?searchtype=author&query=Gerndt%2C+M">Michael Gerndt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACM/SIGAPP Symposium on Applied Computing 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated Learning (FL) is an emerging machine learning paradigm that enables
the collaborative training of a shared global model across distributed clients
while keeping the data decentralized. Recent works on designing systems for
efficient FL have shown that utilizing serverless computing technologies,
particularly Function-as-a-Service (FaaS) for FL, can enhance resource
efficiency, reduce training costs, and alleviate the complex infrastructure
management burden on data holders. However, existing serverless FL systems
implicitly assume a uniform global model architecture across all participating
clients during training. This assumption fails to address fundamental
challenges in practical FL due to the resource and statistical data
heterogeneity among FL clients. To address these challenges and enable
heterogeneous client models in serverless FL, we utilize Knowledge Distillation
(KD) in this paper. Towards this, we propose novel optimized serverless
workflows for two popular conventional federated KD techniques, i.e., FedMD and
FedDF. We implement these workflows by introducing several extensions to an
open-source serverless FL system called FedLess. Moreover, we comprehensively
evaluate the two strategies on multiple datasets across varying levels of
client data heterogeneity using heterogeneous client models with respect to
accuracy, fine-grained training times, and costs. Results from our experiments
demonstrate that serverless FedDF is more robust to extreme non-IID data
distributions, is faster, and leads to lower costs than serverless FedMD. In
addition, compared to the original implementation, our optimizations for
particular steps in FedMD and FedDF lead to an average speedup of 3.5x and
1.76x across all datasets.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07300" title="Abstract">arXiv:2402.07300</a> [<a href="/pdf/2402.07300" title="Download PDF">pdf</a>, <a href="/format/2402.07300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPICA: Interactive Video Content Exploration through Augmented Audio  Descriptions for Blind or Low-Vision Viewers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ning%2C+Z">Zheng Ning</a>, 
<a href="/search/cs?searchtype=author&query=Wimer%2C+B">Brianna Wimer</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+K">Kaiwen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Keyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ban%2C+J">Jerrick Ban</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yapeng Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yuhang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Toby Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Blind or Low-Vision (BLV) users often rely on audio descriptions (AD) to
access video content. However, conventional static ADs can leave out detailed
information in videos, impose a high mental load, neglect the diverse needs and
preferences of BLV users, and lack immersion. To tackle these challenges, we
introduce SPICA, an AI-powered system that enables BLV users to interactively
explore video content. Informed by prior empirical studies on BLV video
consumption, SPICA offers novel interactive mechanisms for supporting temporal
navigation of frame captions and spatial exploration of objects within key
frames. Leveraging an audio-visual machine learning pipeline, SPICA augments
existing ADs by adding interactivity, spatial sound effects, and individual
object descriptions without requiring additional human annotation. Through a
user study with 14 BLV participants, we evaluated the usability and usefulness
of SPICA and explored user behaviors, preferences, and mental models when
interacting with augmented ADs.
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07301" title="Abstract">arXiv:2402.07301</a> [<a href="/pdf/2402.07301" title="Download PDF">pdf</a>, <a href="/format/2402.07301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LISR: Learning Linear 3D Implicit Surface Representation Using Compactly  Supported Radial Basis Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pandey%2C+A">Atharva Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Yadav%2C+V">Vishal Yadav</a>, 
<a href="/search/cs?searchtype=author&query=Nagar%2C+R">Rajendra Nagar</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhury%2C+S">Santanu Chaudhury</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Implicit 3D surface reconstruction of an object from its partial and noisy 3D
point cloud scan is the classical geometry processing and 3D computer vision
problem. In the literature, various 3D shape representations have been
developed, differing in memory efficiency and shape retrieval effectiveness,
such as volumetric, parametric, and implicit surfaces. Radial basis functions
provide memory-efficient parameterization of the implicit surface. However, we
show that training a neural network using the mean squared error between the
ground-truth implicit surface and the linear basis-based implicit surfaces does
not converge to the global solution. In this work, we propose locally supported
compact radial basis functions for a linear representation of the implicit
surface. This representation enables us to generate 3D shapes with arbitrary
topologies at any resolution due to their continuous nature. We then propose a
neural network architecture for learning the linear implicit shape
representation of the 3D surface of an object. We learn linear implicit shapes
within a supervised learning framework using ground truth Signed-Distance Field
(SDF) data for guidance. The classical strategies face difficulties in finding
linear implicit shapes from a given 3D point cloud due to numerical issues
(requires solving inverse of a large matrix) in basis and query point
selection. The proposed approach achieves better Chamfer distance and
comparable F-score than the state-of-the-art approach on the benchmark dataset.
We also show the effectiveness of the proposed approach by using it for the 3D
shape completion task.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07302" title="Abstract">arXiv:2402.07302</a> [<a href="/pdf/2402.07302" title="Download PDF">pdf</a>, <a href="/format/2402.07302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Review of the GIC Blocker Placement Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Barnes%2C+A+K">Arthur K. Barnes</a>, 
<a href="/search/eess?searchtype=author&query=Mate%2C+A">Adam Mate</a>, 
<a href="/search/eess?searchtype=author&query=Bent%2C+R">Russell Bent</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Accepted by the IEEE/IAS 60th Industrial &amp; Commercial Power Systems Technical Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Space weather poses a tremendous threat to power systems: geomagnetic
disturbances could result in widespread disruptions and long-duration
blackouts, including severe damage to system components. To mitigate their
impacts, a handful of strategies exist, with the most promising being the
deployment of transformer neutral blocking devices. The high cost of these
devices, however, precludes their installation at all substations; this
motivates the development of effective solutions for the cost-effective
placement of such devices. While the current state-of-the-art in blocker
placement methods is insufficient to be applied to real-sized power grids,
ongoing research continues to increase the size of networks for which the
placement problem remains tractable. Along these lines, the contributions of
this paper are two fold: first, a comprehensive overview of the current
state-of-the-art in blocker placement methods is provided; and second, a
complete optimization formulation - implemented and benchmarked in an
open-source software - for the blocker placement problem is presented.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07303" title="Abstract">arXiv:2402.07303</a> [<a href="/pdf/2402.07303" title="Download PDF">pdf</a>, <a href="/format/2402.07303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysing cycloids using linear algebra
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Valk%2C+R">R&#xfc;diger Valk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Cycloids are particular Petri nets for modelling processes of actions or
events. They belong to the fundaments of Petri's general systems theory and
have very different interpretations, ranging from Einstein's relativity theory
and elementary information processing gates to the modelling of interacting
sequential processes. This article contains previously unpublished proofs of
cycloid properties using linear algebra.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07304" title="Abstract">arXiv:2402.07304</a> [<a href="/pdf/2402.07304" title="Download PDF">pdf</a>, <a href="/format/2402.07304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Insights into Natural Language Database Query Errors: From Attention  Misalignment to User Handling Strategies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ning%2C+Z">Zheng Ning</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuan Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Toby Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Querying structured databases with natural language (NL2SQL) has remained a
difficult problem for years. Recently, the advancement of machine learning
(ML), natural language processing (NLP), and large language models (LLM) have
led to significant improvements in performance, with the best model achieving
~85% percent accuracy on the benchmark Spider dataset. However, there is a lack
of a systematic understanding of the types, causes, and effectiveness of
error-handling mechanisms of errors for erroneous queries nowadays. To bridge
the gap, a taxonomy of errors made by four representative NL2SQL models was
built in this work, along with an in-depth analysis of the errors. Second, the
causes of model errors were explored by analyzing the model-human attention
alignment to the natural language query. Last, a within-subjects user study
with 26 participants was conducted to investigate the effectiveness of three
interactive error-handling mechanisms in NL2SQL. Findings from this paper shed
light on the design of model structure and error discovery and repair
strategies for natural language data query interfaces in the future.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07309" title="Abstract">arXiv:2402.07309</a> [<a href="/pdf/2402.07309" title="Download PDF">pdf</a>, <a href="/format/2402.07309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node  Classification on Text-Attributed Hypergraphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bazaga%2C+A">Adri&#xe1;n Bazaga</a>, 
<a href="/search/cs?searchtype=author&query=Li%C3%B2%2C+P">Pietro Li&#xf2;</a>, 
<a href="/search/cs?searchtype=author&query=Micklem%2C+G">Gos Micklem</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Machine Learning (stat.ML)

</div>
<p class="mathjax">Hypergraphs are marked by complex topology, expressing higher-order
interactions among multiple entities with hyperedges. Lately, hypergraph-based
deep learning methods to learn informative data representations for the problem
of node classification on text-attributed hypergraphs have garnered increasing
research attention. However, existing methods struggle to simultaneously
capture the full extent of hypergraph structural information and the rich
linguistic attributes inherent in the nodes attributes, which largely hampers
their effectiveness and generalizability. To overcome these challenges, we
explore ways to further augment a pretrained BERT model with specialized
hypergraph-aware layers for the task of node classification. Such layers
introduce higher-order structural inductive bias into the language model, thus
improving the model's capacity to harness both higher-order context information
from the hypergraph structure and semantic information present in text. In this
paper, we propose a new architecture, HyperBERT, a mixed text-hypergraph model
which simultaneously models hypergraph relational structure while maintaining
the high-quality text encoding capabilities of a pre-trained BERT. Notably,
HyperBERT presents results that achieve a new state-of-the-art on 5 challenging
text-attributed hypergraph node classification benchmarks.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07310" title="Abstract">arXiv:2402.07310</a> [<a href="/pdf/2402.07310" title="Download PDF">pdf</a>, <a href="/format/2402.07310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BioNeRF: Biologically Plausible Neural Radiance Fields for View  Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Passos%2C+L+A">Leandro A. Passos</a>, 
<a href="/search/cs?searchtype=author&query=Rodrigues%2C+D">Douglas Rodrigues</a>, 
<a href="/search/cs?searchtype=author&query=Jodas%2C+D">Danilo Jodas</a>, 
<a href="/search/cs?searchtype=author&query=Costa%2C+K+A+P">Kelton A. P. Costa</a>, 
<a href="/search/cs?searchtype=author&query=Papa%2C+J+P">Jo&#xe3;o Paulo Papa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper presents BioNeRF, a biologically plausible architecture that
models scenes in a 3D representation and synthesizes new views through radiance
fields. Since NeRF relies on the network weights to store the scene's
3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism
that fuses inputs from multiple sources into a memory-like structure, improving
the storing capacity and extracting more intrinsic and correlated information.
BioNeRF also mimics a behavior observed in pyramidal cells concerning
contextual information, in which the memory is provided as the context and
combined with the inputs of two subsequent neural models, one responsible for
producing the volumetric densities and the other the colors used to render the
scene. Experimental results show that BioNeRF outperforms state-of-the-art
results concerning a quality measure that encodes human perception in two
datasets: real-world images and synthetic data.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07314" title="Abstract">arXiv:2402.07314</a> [<a href="/pdf/2402.07314" title="Download PDF">pdf</a>, <a href="/ps/2402.07314" title="Download PostScript">ps</a>, <a href="/format/2402.07314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Theoretical Analysis of Nash Learning from Human Feedback under  General KL-Regularized Preference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+C">Chenlu Ye</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+W">Wei Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+N">Nan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> RLHF, NLHF, Alignment for LLMs
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Reinforcement Learning from Human Feedback (RLHF) learns from the preference
signal provided by a probabilistic preference model, which takes a prompt and
two responses as input, and produces a score indicating the preference of one
response against another. So far, the most popular RLHF paradigm is
reward-based, which starts with an initial step of reward modeling, and the
constructed reward is then used to provide a reward signal for the subsequent
reward optimization stage. However, the existence of a reward function is a
strong assumption and the reward-based RLHF is limited in expressivity and
cannot capture the real-world complicated human preference.
<br />In this work, we provide theoretical insights for a recently proposed
learning paradigm, Nash learning from human feedback (NLHF), which considered a
general preference model and formulated the alignment process as a game between
two competitive LLMs. The learning objective is to find a policy that
consistently generates responses preferred over any competing policy while
staying close to the initial model. The objective is defined as the Nash
equilibrium (NE) of the KL-regularized preference model. We aim to make the
first attempt to study the theoretical learnability of the KL-regularized NLHF
by considering both offline and online settings. For the offline learning from
a pre-collected dataset, we propose algorithms that are efficient under
suitable coverage conditions of the dataset. For batch online learning from
iterative interactions with a preference oracle, our proposed algorithm enjoys
a finite sample guarantee under the structural condition of the underlying
preference model. Our results connect the new NLHF paradigm with traditional RL
theory, and validate the potential of reward-model-free learning under general
preference.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07319" title="Abstract">arXiv:2402.07319</a> [<a href="/pdf/2402.07319" title="Download PDF">pdf</a>, <a href="/format/2402.07319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ODIN: Disentangled Reward Mitigates Hacking in RLHF
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lichang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Soselia%2C+D">Davit Soselia</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiuhai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Goldstein%2C+T">Tom Goldstein</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Shoeybi%2C+M">Mohammad Shoeybi</a>, 
<a href="/search/cs?searchtype=author&query=Catanzaro%2C+B">Bryan Catanzaro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">In this work, we study the issue of reward hacking on the response length, a
challenge emerging in Reinforcement Learning from Human Feedback (RLHF) on
LLMs. A well-formatted, verbose but less helpful response from the LLMs can
often deceive LLMs or even human evaluators to achieve high scores. The same
issue also holds for some reward models in RL. To address the challenges in
both training and evaluation, we establish a more reliable evaluation protocol
for comparing different training configurations, which inspects the trade-off
between LLM evaluation score and response length obtained by varying training
hyperparameters. Based on this evaluation, we conduct large-scale studies,
where the results shed insights into the efficacy of hyperparameters and tricks
used in RL on mitigating length bias. We further propose to improve the reward
model by jointly training two linear heads on shared feature representations to
predict the rewards, one trained to correlate with length, and the other
trained to decorrelate with length and therefore focus more on the actual
content. We then discard the length head in RL to prevent reward hacking on
length. Experiments demonstrate that our approach almost eliminates the reward
correlation with length, and improves the obtained policy by a significant
margin.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07320" title="Abstract">arXiv:2402.07320</a> [<a href="/pdf/2402.07320" title="Download PDF">pdf</a>, <a href="/format/2402.07320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Explainable, Safe Autonomous Driving with Language Embeddings  for Novelty Identification and Active Learning: Framework and Experimental  Analysis with Real-World Data Sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Greer%2C+R">Ross Greer</a>, 
<a href="/search/cs?searchtype=author&query=Trivedi%2C+M">Mohan Trivedi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This research explores the integration of language embeddings for active
learning in autonomous driving datasets, with a focus on novelty detection.
Novelty arises from unexpected scenarios that autonomous vehicles struggle to
navigate, necessitating higher-level reasoning abilities. Our proposed method
employs language-based representations to identify novel scenes, emphasizing
the dual purpose of safety takeover responses and active learning. The research
presents a clustering experiment using Contrastive Language-Image Pretrained
(CLIP) embeddings to organize datasets and detect novelties. We find that the
proposed algorithm effectively isolates novel scenes from a collection of
subsets derived from two real-world driving datasets, one vehicle-mounted and
one infrastructure-mounted. From the generated clusters, we further present
methods for generating textual explanations of elements which differentiate
scenes classified as novel from other scenes in the data pool, presenting
qualitative examples from the clustered results. Our results demonstrate the
effectiveness of language-driven embeddings in identifying novel elements and
generating explanations of data, and we further discuss potential applications
in safe takeovers, data curation, and multi-task active learning.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07321" title="Abstract">arXiv:2402.07321</a> [<a href="/pdf/2402.07321" title="Download PDF">pdf</a>, <a href="/format/2402.07321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chughtai%2C+B">Bilal Chughtai</a>, 
<a href="/search/cs?searchtype=author&query=Cooney%2C+A">Alan Cooney</a>, 
<a href="/search/cs?searchtype=author&query=Nanda%2C+N">Neel Nanda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Attributing Model Behaviour at Scale Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">How do transformer-based large language models (LLMs) store and retrieve
knowledge? We focus on the most basic form of this task -- factual recall,
where the model is tasked with explicitly surfacing stored facts in prompts of
form `Fact: The Colosseum is in the country of'. We find that the mechanistic
story behind factual recall is more complex than previously thought. It
comprises several distinct, independent, and qualitatively different mechanisms
that additively combine, constructively interfering on the correct attribute.
We term this generic phenomena the additive motif: models compute through
summing up multiple independent contributions. Each mechanism's contribution
may be insufficient alone, but summing results in constructive interfere on the
correct answer. In addition, we extend the method of direct logit attribution
to attribute an attention head's output to individual source tokens. We use
this technique to unpack what we call `mixed heads' -- which are themselves a
pair of two separate additive updates from different source tokens.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07323" title="Abstract">arXiv:2402.07323</a> [<a href="/pdf/2402.07323" title="Download PDF">pdf</a>, <a href="/format/2402.07323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lessons Learned from Mining the Hugging Face Repository
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Casta%C3%B1o%2C+J">Joel Casta&#xf1;o</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez-Fern%C3%A1ndez%2C+S">Silverio Mart&#xed;nez-Fern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Franch%2C+X">Xavier Franch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 2024 ACM/IEEE 1st International Workshop on Methodological Issues with Empirical Studies in Software Engineering (WSESE)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The rapidly evolving fields of Machine Learning (ML) and Artificial
Intelligence have witnessed the emergence of platforms like Hugging Face (HF)
as central hubs for model development and sharing. This experience report
synthesizes insights from two comprehensive studies conducted on HF, focusing
on carbon emissions and the evolutionary and maintenance aspects of ML models.
Our objective is to provide a practical guide for future researchers embarking
on mining software repository studies within the HF ecosystem to enhance the
quality of these studies. We delve into the intricacies of the replication
package used in our studies, highlighting the pivotal tools and methodologies
that facilitated our analysis. Furthermore, we propose a nuanced stratified
sampling strategy tailored for the diverse HF Hub dataset, ensuring a
representative and comprehensive analytical approach. The report also
introduces preliminary guidelines, transitioning from repository mining to
cohort studies, to establish causality in repository mining studies,
particularly within the ML model of HF context. This transition is inspired by
existing frameworks and is adapted to suit the unique characteristics of the HF
model ecosystem. Our report serves as a guiding framework for researchers,
contributing to the responsible and sustainable advancement of ML, and
fostering a deeper understanding of the broader implications of ML models.
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07325" title="Abstract">arXiv:2402.07325</a> [<a href="/pdf/2402.07325" title="Download PDF">pdf</a>, <a href="/ps/2402.07325" title="Download PostScript">ps</a>, <a href="/format/2402.07325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Voronoi-based Column Selection Methods for Interpretable  Dimensionality Reduction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Emelianenko%2C+M">Maria Emelianenko</a>, 
<a href="/search/math?searchtype=author&query=Oldaker%2C+G+B">Guy B. Oldaker IV</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In data analysis, there continues to be a need for interpretable
dimensionality reduction methods whereby instrinic meaning associated with the
data is retained in the reduced space. Standard approaches such as Principal
Component Analysis (PCA) and the Singular Value Decomposition (SVD) fail at
this task. A popular alternative is the CUR decomposition. In an SVD-like
manner, the CUR decomposition approximates a matrix $A \in \mathbb{R}^{m \times
n}$ as $A \approx CUR$, where $C$ and $R$ are matrices whose columns and rows
are selected from the original matrix \cite{goreinov1997theory},
\cite{mahoney2009cur}. The difficulty in constructing a CUR decomposition is in
determining which columns and rows to select when forming $C$ and $R$. Current
column/row selection algorithms, particularly those that rely on an SVD, become
infeasible as the size of the data becomes large \cite{dong2021simpler}. We
address this problem by reducing the column/row selection problem to a
collection of smaller sub-problems. The basic idea is to first partition the
rows/columns of a matrix, and then apply an existing selection algorithm on
each piece; for illustration purposes we use the Discrete Empirical
Interpolation Method (\textsf{DEIM}) \cite{sorensen2016deim}. For the first
task, we consider two existing algorithms that construct a Voronoi Tessellation
(VT) of the rows and columns of a given matrix. We then extend these methods to
automatically adapt to the data. The result is four data-driven row/column
selection methods that are well-suited for parallelization, and compatible with
nearly any existing column/row selection strategy. Theory and numerical
examples show the design to be competitive with the original \textsf{DEIM}
routine.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07326" title="Abstract">arXiv:2402.07326</a> [<a href="/pdf/2402.07326" title="Download PDF">pdf</a>, <a href="/ps/2402.07326" title="Download PostScript">ps</a>, <a href="/format/2402.07326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Persian Speech Emotion Recognition by Fine-Tuning Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shayaninasab%2C+M">Minoo Shayaninasab</a>, 
<a href="/search/cs?searchtype=author&query=Babaali%2C+B">Bagher Babaali</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Given the significance of speech emotion recognition, numerous methods have
been developed in recent years to create effective and efficient systems in
this domain. One of these methods involves the use of pretrained transformers,
fine-tuned to address this specific problem, resulting in high accuracy.
Despite extensive discussions and global-scale efforts to enhance these
systems, the application of this innovative and effective approach has received
less attention in the context of Persian speech emotion recognition. In this
article, we review the field of speech emotion recognition and its background,
with an emphasis on the importance of employing transformers in this context.
We present two models, one based on spectrograms and the other on the audio
itself, fine-tuned using the shEMO dataset. These models significantly enhance
the accuracy of previous systems, increasing it from approximately 65% to 80%
on the mentioned dataset. Subsequently, to investigate the effect of
multilinguality on the fine-tuning process, these same models are fine-tuned
twice. First, they are fine-tuned using the English IEMOCAP dataset, and then
they are fine-tuned with the Persian shEMO dataset. This results in an improved
accuracy of 82% for the Persian emotion recognition system. Keywords: Persian
Speech Emotion Recognition, shEMO, Self-Supervised Learning
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07327" title="Abstract">arXiv:2402.07327</a> [<a href="/pdf/2402.07327" title="Download PDF">pdf</a>, <a href="/ps/2402.07327" title="Download PostScript">ps</a>, <a href="/format/2402.07327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Modal Emotion Recognition by Text, Speech and Video Using  Pretrained Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shayaninasab%2C+M">Minoo Shayaninasab</a>, 
<a href="/search/cs?searchtype=author&query=Babaali%2C+B">Bagher Babaali</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Due to the complex nature of human emotions and the diversity of emotion
representation methods in humans, emotion recognition is a challenging field.
In this research, three input modalities, namely text, audio (speech), and
video, are employed to generate multimodal feature vectors. For generating
features for each of these modalities, pre-trained Transformer models with
fine-tuning are utilized. In each modality, a Transformer model is used with
transfer learning to extract feature and emotional structure. These features
are then fused together, and emotion recognition is performed using a
classifier. To select an appropriate fusion method and classifier, various
feature-level and decision-level fusion techniques have been experimented with,
and ultimately, the best model, which combines feature-level fusion by
concatenating feature vectors and classification using a Support Vector Machine
on the IEMOCAP multimodal dataset, achieves an accuracy of 75.42%. Keywords:
Multimodal Emotion Recognition, IEMOCAP, Self-Supervised Learning, Transfer
Learning, Transformer.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07328" title="Abstract">arXiv:2402.07328</a> [<a href="/pdf/2402.07328" title="Download PDF">pdf</a>, <a href="/ps/2402.07328" title="Download PostScript">ps</a>, <a href="/format/2402.07328" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing discrete residues of rational functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arreche%2C+C+E">Carlos E. Arreche</a>, 
<a href="/search/cs?searchtype=author&query=Sitaula%2C+H+P">Hari P. Sitaula</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages; submitted manuscript (not yet accepted)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Symbolic Computation (cs.SC)</span>; Commutative Algebra (math.AC); Combinatorics (math.CO)

</div>
<p class="mathjax">In 2012 Chen and Singer introduced the notion of discrete residues for
rational functions as a complete obstruction to rational summability. More
explicitly, for a given rational function f(x), there exists a rational
function g(x) such that f(x) = g(x+1) - g(x) if and only if every discrete
residue of f(x) is zero. Discrete residues have many important further
applications beyond summability: to creative telescoping problems, thence to
the determination of (differential-)algebraic relations among hypergeometric
sequences, and subsequently to the computation of (differential) Galois groups
of difference equations. However, the discrete residues of a rational function
are defined in terms of its complete partial fraction decomposition, which
makes their direct computation impractical due to the high complexity of
completely factoring arbitrary denominator polynomials into linear factors. We
develop a factorization-free algorithm to compute discrete residues of rational
functions, relying only on gcd computations and linear algebra.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07329" title="Abstract">arXiv:2402.07329</a> [<a href="/pdf/2402.07329" title="Download PDF">pdf</a>, <a href="/format/2402.07329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Bias of Harmful Label Associations in Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hazirbas%2C+C">Caner Hazirbas</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+A">Alicia Sun</a>, 
<a href="/search/cs?searchtype=author&query=Efroni%2C+Y">Yonathan Efroni</a>, 
<a href="/search/cs?searchtype=author&query=Ibrahim%2C+M">Mark Ibrahim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Despite the remarkable performance of foundation vision-language models, the
shared representation space for text and vision can also encode harmful label
associations detrimental to fairness. While prior work has uncovered bias in
vision-language models' (VLMs) classification performance across geography,
work has been limited along the important axis of harmful label associations
due to a lack of rich, labeled data. In this work, we investigate harmful label
associations in the recently released Casual Conversations datasets containing
more than 70,000 videos. We study bias in the frequency of harmful label
associations across self-provided labels for age, gender, apparent skin tone,
and physical adornments across several leading VLMs. We find that VLMs are
$4-13$x more likely to harmfully classify individuals with darker skin tones.
We also find scaling transformer encoder model size leads to higher confidence
in harmful predictions. Finally, we find improvements on standard vision tasks
across VLMs does not address disparities in harmful label associations.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07330" title="Abstract">arXiv:2402.07330</a> [<a href="/pdf/2402.07330" title="Download PDF">pdf</a>, <a href="/format/2402.07330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning for Medical Image Segmentation with Imprecise Annotation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Binyan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+A+K">A. K. Qin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Medical image segmentation (MIS) plays an instrumental role in medical image
analysis, where considerable efforts have been devoted to automating the
process. Currently, mainstream MIS approaches are based on deep neural networks
(DNNs) which are typically trained on a dataset that contains annotation masks
produced by doctors. However, in the medical domain, the annotation masks
generated by different doctors can inherently vary because a doctor may
unnecessarily produce precise and unique annotations to meet the goal of
diagnosis. Therefore, the DNN model trained on the data annotated by certain
doctors, often just a single doctor, could undesirably favour those doctors who
annotate the training data, leading to the unsatisfaction of a new doctor who
will use the trained model. To address this issue, this work investigates the
utilization of multi-expert annotation to enhance the adaptability of the model
to a new doctor and we conduct a pilot study on the MRI brain segmentation
task. Experimental results demonstrate that the model trained on a dataset with
multi-expert annotation can efficiently cater for a new doctor, after
lightweight fine-tuning on just a few annotations from the new doctor.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07331" title="Abstract">arXiv:2402.07331</a> [<a href="/pdf/2402.07331" title="Download PDF">pdf</a>, <a href="/format/2402.07331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fundamental Problems on Bounded-Treewidth Graphs: The Real Source of  Hardness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Esmer%2C+B+C">Bar&#x131;&#x15f; Can Esmer</a>, 
<a href="/search/cs?searchtype=author&query=Focke%2C+J">Jacob Focke</a>, 
<a href="/search/cs?searchtype=author&query=Marx%2C+D">D&#xe1;niel Marx</a>, 
<a href="/search/cs?searchtype=author&query=Rz%C4%85%C5%BCewski%2C+P">Pawe&#x142; Rz&#x105;&#x17c;ewski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">It is known for many algorithmic problems that if a tree decomposition of
width $t$ is given in the input, then the problem can be solved with
exponential dependence on $t$. A line of research by Lokshtanov, Marx, and
Saurabh [SODA 2011] produced lower bounds showing that in many cases known
algorithms achieve the best possible exponential dependence on $t$, assuming
the SETH. The main message of our paper is showing that the same lower bounds
can be obtained in a more restricted setting: a graph consisting of a block of
$t$ vertices connected to components of constant size already has the same
hardness as a general tree decomposition of width $t$. Formally, a
$(\sigma,\delta)$-hub is a set $Q$ of vertices such that every component of $Q$
has size at most $\sigma$ and is adjacent to at most $\delta$ vertices of $Q$.
We show that
<br />$\bullet$ For every $\epsilon&gt; 0$, there are $\sigma,\delta&gt; 0$ such that
Independent Set/Vertex Cover cannot be solved in time $(2-\epsilon)^p\cdot n$,
even if a $(\sigma,\delta)$-hub of size $p$ is given in the input, assuming the
SETH. This matches the earlier tight lower bounds parameterized by the width of
the tree decomposition. Similar tight bounds are obtained for Odd Cycle
Transversal, Max Cut, $q$-Coloring, and edge/vertex deletions versions of
$q$-Coloring.
<br />$\bullet$ For every $\epsilon&gt;0$, there are $\sigma,\delta&gt; 0$ such that
Triangle-Partition cannot be solved in time $(2-\epsilon)^p\cdot n$, even if a
$(\sigma,\delta)$-hub of size $p$ is given in the input, assuming the Set Cover
Conjecture (SCC). In fact, we prove that this statement is equivalent to the
SCC, thus it is unlikely that this could be proved assuming the SETH.
<br />Our results reveal that, for many problems, the research on lower bounds on
the dependence on tree width was never really about tree decompositions, but
the real source of hardness comes from a much simpler structure.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07332" title="Abstract">arXiv:2402.07332</a> [<a href="/pdf/2402.07332" title="Download PDF">pdf</a>, <a href="/format/2402.07332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intent-Based Access Control: Using LLMs to Intelligently Manage Access  Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Subramaniam%2C+P">Pranav Subramaniam</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+S">Sanjay Krishnan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 21 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">In every enterprise database, administrators must define an access control
policy that specifies which users have access to which assets. Access control
straddles two worlds: policy (organization-level principles that define who
should have access) and process (database-level primitives that actually
implement the policy). Assessing and enforcing process compliance with a policy
is a manual and ad-hoc task. This paper introduces a new paradigm for access
control called Intent-Based Access Control for Databases (IBAC-DB). In IBAC-DB,
access control policies are expressed more precisely using a novel format, the
natural language access control matrix (NLACM). Database access control
primitives are synthesized automatically from these NLACMs. These primitives
can be used to generate new DB configurations and/or evaluate existing ones.
This paper presents a reference architecture for an IBAC-DB interface, an
initial implementation for PostgreSQL (which we call LLM4AC), and initial
benchmarks that evaluate the accuracy and scope of such a system. We find that
our chosen implementation, LLM4AC, vastly outperforms other baselines,
achieving near-perfect F1 scores on our initial benchmarks.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07334" title="Abstract">arXiv:2402.07334</a> [<a href="/pdf/2402.07334" title="Download PDF">pdf</a>, <a href="/format/2402.07334" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Training of Mixture of Experts Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tholoniat%2C+P">Pierre Tholoniat</a>, 
<a href="/search/cs?searchtype=author&query=Inan%2C+H+A">Huseyin A. Inan</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+J">Janardhan Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Sim%2C+R">Robert Sim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preliminary work presented as a poster at the 5th AAAI Workshop on Privacy-Preserving Artificial Intelligence (PPAI 24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This position paper investigates the integration of Differential Privacy (DP)
in the training of Mixture of Experts (MoE) models within the field of natural
language processing. As Large Language Models (LLMs) scale to billions of
parameters, leveraging expansive datasets, they exhibit enhanced linguistic
capabilities and emergent abilities. However, this growth raises significant
computational and privacy concerns. Our study addresses these issues by
exploring the potential of MoE models, known for their computational
efficiency, and the application of DP, a standard for privacy preservation. We
present the first known attempt to train MoE models under the constraints of
DP, addressing the unique challenges posed by their architecture and the
complexities of DP integration. Our initial experimental studies demonstrate
that MoE models can be effectively trained with DP, achieving performance that
is competitive with their non-private counterparts. This initial study aims to
provide valuable insights and ignite further research in the domain of
privacy-preserving MoE models, softly laying the groundwork for prospective
developments in this evolving field.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07335" title="Abstract">arXiv:2402.07335</a> [<a href="/pdf/2402.07335" title="Download PDF">pdf</a>, <a href="/format/2402.07335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cultural gems linked open data: Mapping culture and intangible heritage  in European cities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Consoli%2C+S">Sergio Consoli</a>, 
<a href="/search/cs?searchtype=author&query=Alberti%2C+V">Valentina Alberti</a>, 
<a href="/search/cs?searchtype=author&query=Cocco%2C+C">Cinzia Cocco</a>, 
<a href="/search/cs?searchtype=author&query=Panella%2C+F">Francesco Panella</a>, 
<a href="/search/cs?searchtype=author&query=Montalto%2C+V">Valentina Montalto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Data in Brief, Volume 49, 2023, 109375
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
<p class="mathjax">The recovery and resilience of the cultural and creative sectors after the
COVID-19 pandemic is a current topic with priority for the European Commission.
Cultural gems is a crowdsourced web platform managed by the Joint Research
Centre of the European Commission aimed at creating community-led maps as well
as a common repository for cultural and creative places across European cities
and towns. More than 130,000 physical locations and online cultural activities
in more than 300 European cities and towns are currently tracked by the
application. The main objective of Cultural gems consists in raising a holistic
vision of European culture, reinforcing a sense of belonging to a common
European cultural space. This data article describes the ontology developed for
Cultural gems, adopted to represent the domain of knowledge of the application
by means of FAIR (Findable, Accessible, Interoperable, Reusable) principles and
following the paradigms of Linked Open Data (LOD). We provide an overview of
this dataset, and describe the ontology model, along with the services used to
access and consume the data.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07338" title="Abstract">arXiv:2402.07338</a> [<a href="/pdf/2402.07338" title="Download PDF">pdf</a>, <a href="/format/2402.07338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Saliency Bias in Manipulation Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krinsky%2C+J">Joshua Krinsky</a>, 
<a href="/search/cs?searchtype=author&query=Bettis%2C+A">Alan Bettis</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Q">Qiuyu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Moreira%2C+D">Daniel Moreira</a>, 
<a href="/search/cs?searchtype=author&query=Bharati%2C+A">Aparna Bharati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The social media-fuelled explosion of fake news and misinformation supported
by tampered images has led to growth in the development of models and datasets
for image manipulation detection. However, existing detection methods mostly
treat media objects in isolation, without considering the impact of specific
manipulations on viewer perception. Forensic datasets are usually analyzed
based on the manipulation operations and corresponding pixel-based masks, but
not on the semantics of the manipulation, i.e., type of scene, objects, and
viewers' attention to scene content. The semantics of the manipulation play an
important role in spreading misinformation through manipulated images. In an
attempt to encourage further development of semantic-aware forensic approaches
to understand visual misinformation, we propose a framework to analyze the
trends of visual and semantic saliency in popular image manipulation datasets
and their impact on detection.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07339" title="Abstract">arXiv:2402.07339</a> [<a href="/pdf/2402.07339" title="Download PDF">pdf</a>, <a href="/format/2402.07339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond the Headlines: Understanding Sentiments and Morals Impacting  Female Employment in Spain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Araque%2C+O">Oscar Araque</a>, 
<a href="/search/cs?searchtype=author&query=Barbaglia%2C+L">Luca Barbaglia</a>, 
<a href="/search/cs?searchtype=author&query=Berlingieri%2C+F">Francesco Berlingieri</a>, 
<a href="/search/cs?searchtype=author&query=Colagrossi%2C+M">Marco Colagrossi</a>, 
<a href="/search/cs?searchtype=author&query=Consoli%2C+S">Sergio Consoli</a>, 
<a href="/search/cs?searchtype=author&query=Gatti%2C+L">Lorenzo Gatti</a>, 
<a href="/search/cs?searchtype=author&query=Mauri%2C+C">Caterina Mauri</a>, 
<a href="/search/cs?searchtype=author&query=Kalimeri%2C+K">Kyriaki Kalimeri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures. In Workshop Proceedings of the 17th International AAAI Conference on Web and Social Media (ICWSM), AAAI Press (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">After decades of improvements in the employment conditions of females in
Spain, this process came to a sudden stop with the Great Spanish Recession of
2008. In this contribution, we analyse a large longitudinal corpus of national
and regional news outlets employing advanced Natural Language Processing
techniques to capture the valence of mentions of gender inequality expressed in
the Spanish press. The automatic analysis of the news articles does indeed
capture the known hardships faced by females in the Spanish labour market. Our
approach can be straightforwardly generalised to other topics of interest.
Assessing the sentiment and moral values expressed in the articles, we notice
that females are, in the majority of cases, concerned more than males when
there is a deterioration in the overall labour market conditions, based on
newspaper articles. This behaviour has been present in the entire period of
study (2000--2022) and looked particularly pronounced during the economic
crisis of 2008 and the recent COVID-19 pandemic. Most of the time, this
phenomenon looks to be more pronounced at the regional level, perhaps caused by
a significant focus on local labour markets rather than on aggregate statistics
or because, in local contexts, females might suffer more from an isolation or
discrimination condition. Our findings contribute to a deeper understanding of
the gender inequalities in Spain using alternative data, informing policymakers
and stakeholders.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07340" title="Abstract">arXiv:2402.07340</a> [<a href="/pdf/2402.07340" title="Download PDF">pdf</a>, <a href="/format/2402.07340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Random Geometric Graph Alignment with Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Suqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Austern%2C+M">Morgane Austern</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 2 figure, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Social and Information Networks (cs.SI); Probability (math.PR); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">We characterize the performance of graph neural networks for graph alignment
problems in the presence of vertex feature information. More specifically,
given two graphs that are independent perturbations of a single random
geometric graph with noisy sparse features, the task is to recover an unknown
one-to-one mapping between the vertices of the two graphs. We show under
certain conditions on the sparsity and noise level of the feature vectors, a
carefully designed one-layer graph neural network can with high probability
recover the correct alignment between the vertices with the help of the graph
structure. We also prove that our conditions on the noise level are tight up to
logarithmic factors. Finally we compare the performance of the graph neural
network to directly solving an assignment problem on the noisy vertex features.
We demonstrate that when the noise level is at least constant this direct
matching fails to have perfect recovery while the graph neural network can
tolerate noise level growing as fast as a power of the size of the graph.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07342" title="Abstract">arXiv:2402.07342</a> [<a href="/pdf/2402.07342" title="Download PDF">pdf</a>, <a href="/format/2402.07342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Imagining a Future of Designing with AI: Dynamic Grounding, Constructive  Negotiation, and Sustainable Motivation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vaithilingam%2C+P">Priyan Vaithilingam</a>, 
<a href="/search/cs?searchtype=author&query=Arawjo%2C+I">Ian Arawjo</a>, 
<a href="/search/cs?searchtype=author&query=Glassman%2C+E+L">Elena L. Glassman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We ideate a future design workflow that involves AI technology. Drawing from
activity and communication theory, we attempt to isolate the new value large AI
models can provide design compared to past technologies. We arrive at three
affordances -- dynamic grounding, constructive negotiation, and sustainable
motivation -- that summarize latent qualities of natural language-enabled
foundation models that, if explicitly designed for, can support the process of
design. Through design fiction, we then imagine a future interface as a
diegetic prototype, the story of Squirrel Game, that demonstrates each of our
three affordances in a realistic usage scenario. Our design process,
terminology, and diagrams aim to contribute to future discussions about the
relative affordances of AI technology with regard to collaborating with human
designers.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07344" title="Abstract">arXiv:2402.07344</a> [<a href="/pdf/2402.07344" title="Download PDF">pdf</a>, <a href="/format/2402.07344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measurement Scheduling for ICU Patients with Offline Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+Z">Zongliang Ji</a>, 
<a href="/search/cs?searchtype=author&query=Goldenberg%2C+A">Anna Goldenberg</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+R+G">Rahul G. Krishnan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended Abstract presented at Machine Learning for Health (ML4H) symposium 2023, December 10th, 2023, New Orleans, United States, 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Scheduling laboratory tests for ICU patients presents a significant
challenge. Studies show that 20-40% of lab tests ordered in the ICU are
redundant and could be eliminated without compromising patient safety. Prior
work has leveraged offline reinforcement learning (Offline-RL) to find optimal
policies for ordering lab tests based on patient information. However, new ICU
patient datasets have since been released, and various advancements have been
made in Offline-RL methods. In this study, we first introduce a preprocessing
pipeline for the newly-released MIMIC-IV dataset geared toward time-series
tasks. We then explore the efficacy of state-of-the-art Offline-RL methods in
identifying better policies for ICU patient lab test scheduling. Besides
assessing methodological performance, we also discuss the overall suitability
and practicality of using Offline-RL frameworks for scheduling laboratory tests
in ICU settings.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07345" title="Abstract">arXiv:2402.07345</a> [<a href="/pdf/2402.07345" title="Download PDF">pdf</a>, <a href="/format/2402.07345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing Krylov iterates in the time of matrix multiplication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neiger%2C+V">Vincent Neiger</a>, 
<a href="/search/cs?searchtype=author&query=Pernet%2C+C">Cl&#xe9;ment Pernet</a>, 
<a href="/search/cs?searchtype=author&query=Villard%2C+G">Gilles Villard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 algorithms
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Symbolic Computation (cs.SC)</span>

</div>
<p class="mathjax">Krylov methods rely on iterated matrix-vector products $A^k u_j$ for an
$n\times n$ matrix $A$ and vectors $u_1,\ldots,u_m$. The space spanned by all
iterates $A^k u_j$ admits a particular basis -- the \emph{maximal Krylov basis}
-- which consists of iterates of the first vector $u_1, Au_1, A^2u_1,\ldots$,
until reaching linear dependency, then iterating similarly the subsequent
vectors until a basis is obtained. Finding minimal polynomials and Frobenius
normal forms is closely related to computing maximal Krylov bases. The fastest
way to produce these bases was, until this paper, Keller-Gehrig's 1985
algorithm whose complexity bound $O(n^\omega \log(n))$ comes from repeated
squarings of $A$ and logarithmically many Gaussian eliminations. Here
$\omega&gt;2$ is a feasible exponent for matrix multiplication over the base
field. We present an algorithm computing the maximal Krylov basis in
$O(n^\omega\log\log(n))$ field operations when $m \in O(n)$, and even
$O(n^\omega)$ as soon as $m\in O(n/\log(n)^c)$ for some fixed real $c&gt;0$. As a
consequence, we show that the Frobenius normal form together with a
transformation matrix can be computed deterministically in $O(n^\omega
\log\log(n)^2)$, and therefore matrix exponentiation~$A^k$ can be performed in
the latter complexity if $\log(k) \in O(n^{\omega-1-\varepsilon})$, for
$\varepsilon&gt;0$. A key idea for these improvements is to rely on fast
algorithms for $m\times m$ polynomial matrices of average degree $n/m$,
involving high-order lifting and minimal kernel bases.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07347" title="Abstract">arXiv:2402.07347</a> [<a href="/pdf/2402.07347" title="Download PDF">pdf</a>, <a href="/format/2402.07347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accuracy of TextFooler black box adversarial attacks on 01 loss sign  activation neural network ensemble
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+Y">Yunzhe Xue</a>, 
<a href="/search/cs?searchtype=author&query=Roshan%2C+U">Usman Roshan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Recent work has shown the defense of 01 loss sign activation neural networks
against image classification adversarial attacks. A public challenge to attack
the models on CIFAR10 dataset remains undefeated. We ask the following question
in this study: are 01 loss sign activation neural networks hard to deceive with
a popular black box text adversarial attack program called TextFooler? We study
this question on four popular text classification datasets: IMDB reviews, Yelp
reviews, MR sentiment classification, and AG news classification. We find that
our 01 loss sign activation network is much harder to attack with TextFooler
compared to sigmoid activation cross entropy and binary neural networks. We
also study a 01 loss sign activation convolutional neural network with a novel
global pooling step specific to sign activation networks. With this new
variation we see a significant gain in adversarial accuracy rendering
TextFooler practically useless against it. We make our code freely available at
\url{https://github.com/zero-one-loss/wordcnn01} and
\url{https://github.com/xyzacademic/mlp01example}. Our work here suggests that
01 loss sign activation networks could be further developed to create fool
proof models against text adversarial attacks.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07350" title="Abstract">arXiv:2402.07350</a> [<a href="/pdf/2402.07350" title="Download PDF">pdf</a>, <a href="/format/2402.07350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Antagonistic AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cai%2C+A">Alice Cai</a>, 
<a href="/search/cs?searchtype=author&query=Arawjo%2C+I">Ian Arawjo</a>, 
<a href="/search/cs?searchtype=author&query=Glassman%2C+E+L">Elena L. Glassman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 1 figure, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">The vast majority of discourse around AI development assumes that
subservient, "moral" models aligned with "human values" are universally
beneficial -- in short, that good AI is sycophantic AI. We explore the shadow
of the sycophantic paradigm, a design space we term antagonistic AI: AI systems
that are disagreeable, rude, interrupting, confrontational, challenging, etc.
-- embedding opposite behaviors or values. Far from being "bad" or "immoral,"
we consider whether antagonistic AI systems may sometimes have benefits to
users, such as forcing users to confront their assumptions, build resilience,
or develop healthier relational boundaries. Drawing from formative explorations
and a speculative design workshop where participants designed fictional AI
technologies that employ antagonism, we lay out a design space for antagonistic
AI, articulating potential benefits, design techniques, and methods of
embedding antagonistic elements into user experience. Finally, we discuss the
many ethical challenges of this space and identify three dimensions for the
responsible design of antagonistic AI -- consent, context, and framing.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07351" title="Abstract">arXiv:2402.07351</a> [<a href="/pdf/2402.07351" title="Download PDF">pdf</a>, <a href="/format/2402.07351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ontology Engineering to Model the European Cultural Heritage: The Case  of Cultural Gems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alberti%2C+V">Valentina Alberti</a>, 
<a href="/search/cs?searchtype=author&query=Cocco%2C+C">Cinzia Cocco</a>, 
<a href="/search/cs?searchtype=author&query=Consoli%2C+S">Sergio Consoli</a>, 
<a href="/search/cs?searchtype=author&query=Montalto%2C+V">Valentina Montalto</a>, 
<a href="/search/cs?searchtype=author&query=Panella%2C+F">Francesco Panella</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 3 figures, Eighth International Congress on Information and Communication Technology (ICICT 2023)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Lecture Notes in Networks and Systems, vol 696, Springer Nature
  (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Digital Libraries (cs.DL)

</div>
<p class="mathjax">Cultural gems is a web application conceived by the European Commission's
Joint Research Centre (DG JRC), which aims at engaging people and organisations
across Europe to create a unique repository of cultural and creative places.
The main goal is to provide a vision of European culture in order to strengthen
a sense of identity within a single European cultural realm. Cultural gems maps
more than 130,000 physical places in over 300 European cities and towns, and
since 2020 it also lists online cultural initiatives. The new release aims,
among other, to increase the interoperability of the application. At this
purpose, we provide an overview on the current development of an ontology for
Cultural gems used to map cultural heritage in European cities by using Linked
Open Data (LOD) standards, and making the data FAIR, that is Findable,
Accessible, Interoperable, and Reusable. We provide an overview of the
methodology, presenting the structure of the ontology, and the services and
tools we are currently building on top.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07352" title="Abstract">arXiv:2402.07352</a> [<a href="/pdf/2402.07352" title="Download PDF">pdf</a>, <a href="/format/2402.07352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Distribution-based Curriculum Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chaudhry%2C+S">Shonal Chaudhry</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Anuraganand Sharma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The order of training samples can have a significant impact on the
performance of a classifier. Curriculum learning is a method of ordering
training samples from easy to hard. This paper proposes the novel idea of a
curriculum learning approach called Data Distribution-based Curriculum Learning
(DDCL). DDCL uses the data distribution of a dataset to build a curriculum
based on the order of samples. Two types of scoring methods known as DDCL
(Density) and DDCL (Point) are used to score training samples thus determining
their training order. DDCL (Density) uses the sample density to assign scores
while DDCL (Point) utilises the Euclidean distance for scoring. We evaluate the
proposed DDCL approach by conducting experiments on multiple datasets using a
neural network, support vector machine and random forest classifier. Evaluation
results show that the application of DDCL improves the average classification
accuracy for all datasets compared to standard evaluation without any
curriculum. Moreover, analysis of the error losses for a single training epoch
reveals that convergence is faster when using DDCL over the no curriculum
method.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07353" title="Abstract">arXiv:2402.07353</a> [<a href="/pdf/2402.07353" title="Download PDF">pdf</a>, <a href="/format/2402.07353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimized Gr&#xf6;bner basis algorithms for maximal determinantal ideals  and critical point computations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gopalakrishnan%2C+S">Sriram Gopalakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Neiger%2C+V">Vincent Neiger</a>, 
<a href="/search/cs?searchtype=author&query=Din%2C+M+S+E">Mohab Safey El Din</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 3 algorithms, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Symbolic Computation (cs.SC)</span>; Commutative Algebra (math.AC)

</div>
<p class="mathjax">Given polynomials $g$ and $f_1,\dots,f_p$, all in $\Bbbk[x_1,\dots,x_n]$ for
some field $\Bbbk$, we consider the problem of computing the critical points of
the restriction of $g$ to the variety defined by $f_1=\cdots=f_p=0$. These are
defined by the simultaneous vanishing of the $f_i$'s and all maximal minors of
the Jacobian matrix associated to $(g,f_1, \ldots, f_p)$. We use the
Eagon-Northcott complex associated to the ideal generated by these maximal
minors to gain insight into the syzygy module of the system defining these
critical points. We devise new $F_5$-type criteria to predict and avoid more
reductions to zero when computing a Gr\"obner basis for the defining system of
this critical locus. We give a bound for the arithmetic complexity of this
enhanced $F_5$ algorithm and compare it to the best previously known bound for
computing critical points using Gr\"obner bases.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07356" title="Abstract">arXiv:2402.07356</a> [<a href="/pdf/2402.07356" title="Download PDF">pdf</a>, <a href="/ps/2402.07356" title="Download PostScript">ps</a>, <a href="/format/2402.07356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Gaussian Min-Max Theorem and its Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akhtiamov%2C+D">Danil Akhtiamov</a>, 
<a href="/search/cs?searchtype=author&query=Bosch%2C+D">David Bosch</a>, 
<a href="/search/cs?searchtype=author&query=Ghane%2C+R">Reza Ghane</a>, 
<a href="/search/cs?searchtype=author&query=Varma%2C+K+N">K Nithin Varma</a>, 
<a href="/search/cs?searchtype=author&query=Hassibi%2C+B">Babak Hassibi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">A celebrated result by Gordon allows one to compare the min-max behavior of
two Gaussian processes if certain inequality conditions are met. The
consequences of this result include the Gaussian min-max (GMT) and convex
Gaussian min-max (CGMT) theorems which have had far-reaching implications in
high-dimensional statistics, machine learning, non-smooth optimization, and
signal processing. Both theorems rely on a pair of Gaussian processes, first
identified by Slepian, that satisfy Gordon's comparison inequalities. To date,
no other pair of Gaussian processes satisfying these inequalities has been
discovered. In this paper, we identify such a new pair. The resulting theorems
extend the classical GMT and CGMT Theorems from the case where the underlying
Gaussian matrix in the primary process has iid rows to where it has independent
but non-identically-distributed ones. The new CGMT is applied to the problems
of multi-source Gaussian regression, as well as to binary classification of
general Gaussian mixture models.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07359" title="Abstract">arXiv:2402.07359</a> [<a href="/pdf/2402.07359" title="Download PDF">pdf</a>, <a href="/format/2402.07359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structured Satellite-UAV-Terrestrial Networks for 6G Internet of Things
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+W">Wei Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanmin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yunfei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+N">Ning Ge</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cheng-Xiang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">The upcoming sixth generation (6G) wireless communication network is
envisioned to cover space, air, and maritime areas, in addition to
urban-centered terrestrial coverage by the fifth generation (5G) network, to
support intelligent Internet of Things (IoT). Towards this end, we investigate
structured integration of satellites, unmanned aerial vehicles (UAVs), and
terrestrial networks, aiming to serve future universal IoT possibly with a
massive number of devices in the coverage holes of current 5G. The hybrid
satellite-UAV-terrestrial network usually leads to high system complexity, due
to the heterogeneity and dynamics of space/air/ground links. With a systematic
thinking, we propose to create and exploit hierarchies for the integrated
network. Four basic structures are discussed by learning from the synergies in
our human body. To orchestrate multiple heterogeneous basic structures, we
further propose a process-oriented on-demand coverage method, which
characterizes the system behavior as a series of events over time and is able
to tackle the system complexity elaborately. We also outline open issues for
promoting the agility and intelligence of structured satellite-UAV-terrestrial
networks in the making.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07363" title="Abstract">arXiv:2402.07363</a> [<a href="/pdf/2402.07363" title="Download PDF">pdf</a>, <a href="/format/2402.07363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strategically-Robust Learning Algorithms for Bidding in First-Price  Auctions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+R">Rachitesh Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+J">Jon Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Sivan%2C+B">Balasubramanian Sivan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Learning to bid in repeated first-price auctions is a fundamental problem at
the interface of game theory and machine learning, which has seen a recent
surge in interest due to the transition of display advertising to first-price
auctions. In this work, we propose a novel concave formulation for
pure-strategy bidding in first-price auctions, and use it to analyze natural
Gradient-Ascent-based algorithms for this problem. Importantly, our analysis
goes beyond regret, which was the typical focus of past work, and also accounts
for the strategic backdrop of online-advertising markets where bidding
algorithms are deployed -- we prove that our algorithms cannot be exploited by
a strategic seller and that they incentivize truth-telling for the buyer.
<br />Concretely, we show that our algorithms achieve $O(\sqrt{T})$ regret when the
highest competing bids are generated adversarially, and show that no online
algorithm can do better. We further prove that the regret improves to $O(\log
T)$ when the competition is stationary and stochastic. Moving beyond regret, we
show that a strategic seller cannot exploit our algorithms to extract more
revenue on average than is possible under the optimal mechanism, i.e., the
seller cannot do much better than posting the monopoly reserve price in each
auction. Finally, we prove that our algorithm is also incentive compatible --
it is a (nearly) dominant strategy for the buyer to report her values
truthfully to the algorithm as a whole.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07366" title="Abstract">arXiv:2402.07366</a> [<a href="/pdf/2402.07366" title="Download PDF">pdf</a>, <a href="/format/2402.07366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Federated Learning Via Expectation Maximization and Turbo Deep  Approximate Message Passing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">An Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yiting Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lau%2C+V">Vincent Lau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Federated learning (FL) is a machine learning paradigm where the clients
possess decentralized training data and the central server handles aggregation
and scheduling. Typically, FL algorithms involve clients training their local
models using stochastic gradient descent (SGD), which carries drawbacks such as
slow convergence and being prone to getting stuck in suboptimal solutions. In
this work, we propose a message passing based Bayesian federated learning (BFL)
framework to avoid these drawbacks.Specifically, we formulate the problem of
deep neural network (DNN) learning and compression and as a sparse Bayesian
inference problem, in which group sparse prior is employed to achieve
structured model compression. Then, we propose an efficient BFL algorithm
called EMTDAMP, where expectation maximization (EM) and turbo deep approximate
message passing (TDAMP) are combined to achieve distributed learning and
compression. The central server aggregates local posterior distributions to
update global posterior distributions and update hyperparameters based on EM to
accelerate convergence. The clients perform TDAMP to achieve efficient
approximate message passing over DNN with joint prior distribution. We detail
the application of EMTDAMP to Boston housing price prediction and handwriting
recognition, and present extensive numerical results to demonstrate the
advantages of EMTDAMP.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07367" title="Abstract">arXiv:2402.07367</a> [<a href="/pdf/2402.07367" title="Download PDF">pdf</a>, <a href="/format/2402.07367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Utilizing Large LanguageModels to Detect Privacy Leaks in Mini-App Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L">Liming Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Mini-applications, commonly referred to as mini-apps, are compact software
programs embedded within larger applications or platforms, offering targeted
functionality without the need for separate installations. Typically web-based
or cloud-hosted, these mini-apps streamline user experiences by providing
focused services accessible through web browsers or mobile apps. Their
simplicity, speed, and integration capabilities make them valuable additions to
messaging platforms, social media networks, e-commerce sites, and various
digital environments. WeChat Mini Programs, a prominent feature of China's
leading messaging app, exemplify this trend, offering users a seamless array of
services without additional downloads. Leveraging WeChat's extensive user base
and payment infrastructure, Mini Programs facilitate efficient transactions and
bridge online and offline experiences, shaping China's digital landscape
significantly. This paper investigates the potential of employing Large
Language Models (LLMs) to detect privacy breaches within WeChat Mini Programs.
Given the widespread use of Mini Programs and growing concerns about data
privacy, this research seeks to determine if LLMs can effectively identify
instances of privacy leakage within this ecosystem. Through meticulous analysis
and experimentation, we aim to highlight the efficacy of LLMs in safeguarding
user privacy and security within the WeChat Mini Program environment, thereby
contributing to a more secure digital landscape.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07368" title="Abstract">arXiv:2402.07368</a> [<a href="/pdf/2402.07368" title="Download PDF">pdf</a>, <a href="/format/2402.07368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing Generalization for Subpopulation Representative Modeling via  In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Simmons%2C+G">Gabriel Simmons</a>, 
<a href="/search/cs?searchtype=author&query=Savinov%2C+V">Vladislav Savinov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to PERSONALIZE workshop at EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computers and Society (cs.CY)

</div>
<p class="mathjax">This study evaluates the ability of Large Language Model (LLM)-based
Subpopulation Representative Models (SRMs) to generalize from empirical data,
utilizing in-context learning with data from the 2016 and 2020 American
National Election Studies. We explore generalization across response variables
and demographic subgroups. While conditioning with empirical data improves
performance on the whole, the benefit of in-context learning varies
considerably across demographics, sometimes hurting performance for one
demographic while helping performance for others. The inequitable benefits of
in-context learning for SRM present a challenge for practitioners implementing
SRMs, and for decision-makers who might come to rely on them. Our work
highlights a need for fine-grained benchmarks captured from diverse
subpopulations that test not only fidelity but generalization.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07369" title="Abstract">arXiv:2402.07369</a> [<a href="/pdf/2402.07369" title="Download PDF">pdf</a>, <a href="/format/2402.07369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diff-RNTraj: A Structure-aware Diffusion Model for Road  Network-constrained Trajectory Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+T">Tonglong Wei</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Youfang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Shengnan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yan Lin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yiheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+C">Chenyang Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yuqing Bai</a>, 
<a href="/search/cs?searchtype=author&query=Ya%2C+M">Menglu Ya</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+H">Huaiyu Wan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Trajectory data is essential for various applications as it records the
movement of vehicles. However, publicly available trajectory datasets remain
limited in scale due to privacy concerns, which hinders the development of
trajectory data mining and trajectory-based applications. To address this
issue, some methods for generating synthetic trajectories have been proposed to
expand the scale of the dataset. However, all existing methods generate
trajectories in the geographical coordinate system, which poses two limitations
for their utilization in practical applications: 1) the inability to ensure
that the generated trajectories are constrained on the road. 2) the lack of
road-related information. In this paper, we propose a new problem to meet the
practical application need, \emph{i.e.}, road network-constrained trajectory
(RNTraj) generation, which can directly generate trajectories on the road
network with road-related information. RNTraj is a hybrid type of data, in
which each point is represented by a discrete road segment and a continuous
moving rate. To generate RNTraj, we design a diffusion model called
Diff-RNTraj. This model can effectively handle the hybrid RNTraj using a
continuous diffusion framework by incorporating a pre-training strategy to
embed hybrid RNTraj into continuous representations. During the sampling stage,
a RNTraj decoder is designed to map the continuous representation generated by
the diffusion model back to the hybrid RNTraj format. Furthermore, Diff-RNTraj
introduces a novel loss function to enhance the spatial validity of the
generated trajectories. Extensive experiments conducted on two real-world
trajectory datasets demonstrate the effectiveness of the proposed model.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07370" title="Abstract">arXiv:2402.07370</a> [<a href="/pdf/2402.07370" title="Download PDF">pdf</a>, <a href="/format/2402.07370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked  AutoEncoder
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jaeseong Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hyung%2C+J">Junha Hyung</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+S">Sohyun Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Choo%2C+J">Jaegul Choo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Face swapping has gained significant attention for its varied applications.
The majority of previous face swapping approaches have relied on the seesaw
game training scheme, which often leads to the instability of the model
training and results in undesired samples with blended identities due to the
target identity leakage problem. This paper introduces the Shape Agnostic
Masked AutoEncoder (SAMAE) training scheme, a novel self-supervised approach
designed to enhance face swapping model training. Our training scheme addresses
the limitations of traditional training methods by circumventing the
conventional seesaw game and introducing clear ground truth through its
self-reconstruction training regime. It effectively mitigates identity leakage
by masking facial regions of the input images and utilizing learned
disentangled identity and non-identity features. Additionally, we tackle the
shape misalignment problem with new techniques including perforation confusion
and random mesh scaling, and establishes a new state-of-the-art, surpassing
other baseline methods, preserving both identity and non-identity attributes,
without sacrificing on either aspect.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07371" title="Abstract">arXiv:2402.07371</a> [<a href="/pdf/2402.07371" title="Download PDF">pdf</a>, <a href="/format/2402.07371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-World Atmospheric Turbulence Correction via Domain Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xijun Wang</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%B3pez-Tapia%2C+S">Santiago L&#xf3;pez-Tapia</a>, 
<a href="/search/cs?searchtype=author&query=Katsaggelos%2C+A+K">Aggelos K. Katsaggelos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Atmospheric turbulence, a common phenomenon in daily life, is primarily
caused by the uneven heating of the Earth's surface. This phenomenon results in
distorted and blurred acquired images or videos and can significantly impact
downstream vision tasks, particularly those that rely on capturing clear,
stable images or videos from outdoor environments, such as accurately detecting
or recognizing objects. Therefore, people have proposed ways to simulate
atmospheric turbulence and designed effective deep learning-based methods to
remove the atmospheric turbulence effect. However, these synthesized turbulent
images can not cover all the range of real-world turbulence effects. Though the
models have achieved great performance for synthetic scenarios, there always
exists a performance drop when applied to real-world cases. Moreover, reducing
real-world turbulence is a more challenging task as there are no clean ground
truth counterparts provided to the models during training. In this paper, we
propose a real-world atmospheric turbulence mitigation model under a domain
adaptation framework, which links the supervised simulated atmospheric
turbulence correction with the unsupervised real-world atmospheric turbulence
correction. We will show our proposed method enhances performance in real-world
atmospheric turbulence scenarios, improving both image quality and downstream
vision tasks.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07375" title="Abstract">arXiv:2402.07375</a> [<a href="/pdf/2402.07375" title="Download PDF">pdf</a>, <a href="/format/2402.07375" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified MPC Strategy for a Tilt-rotor VTOL UAV Towards Seamless Mode  Transitioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+Q">Qizhao Chen</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+Z">Ziqi Hu</a>, 
<a href="/search/eess?searchtype=author&query=Geng%2C+J">Junyi Geng</a>, 
<a href="/search/eess?searchtype=author&query=Bai%2C+D">Dongwei Bai</a>, 
<a href="/search/eess?searchtype=author&query=Mousaei%2C+M">Mohammad Mousaei</a>, 
<a href="/search/eess?searchtype=author&query=Scherer%2C+S">Sebastian Scherer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In proceedings of the 2024 AIAA SciTech Forum, Session: Guidance, Navigation, and Control GNC-49
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> AIAA SCITECH 2024 Forum, p. 2878. January 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Capabilities of long-range flight and vertical take-off and landing (VTOL)
are essential for Urban Air Mobility (UAM). Tiltrotor VTOLs have the advantage
of balancing control simplicity and system complexity due to their redundant
control authority. Prior work on controlling these aircraft either requires
separate controllers and switching modes for different vehicle configurations
or performs the control allocation on separate actuator sets, which cannot
fully use the potential of the redundancy of tiltrotor. This paper introduces a
unified MPC-based control strategy for a customized tiltrotor VTOL Unmanned
Aerial Vehicle (UAV), which does not require mode-switching and can perform the
control allocation in a consistent way. The incorporation of four independently
controllable rotors in VTOL design offers an extra level of redundancy,
allowing the VTOL to accommodate actuator failures. The result shows that our
approach outperforms PID controllers while maintaining unified control. It
allows the VTOL to perform smooth acceleration/deceleration, and precise
coordinated turns. In addition, the independently controlled tilts enable the
vehicle to handle actuator failures, ensuring that the aircraft remains
operational even in the event of a servo or motor malfunction.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07376" title="Abstract">arXiv:2402.07376</a> [<a href="/pdf/2402.07376" title="Download PDF">pdf</a>, <a href="/format/2402.07376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Discovery of Object-Centric Neural Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+R">Rundong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hong-Xing Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiajun Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We study inferring 3D object-centric scene representations from a single
image. While recent methods have shown potential in unsupervised 3D object
discovery from simple synthetic images, they fail to generalize to real-world
scenes with visually rich and diverse objects. This limitation stems from their
object representations, which entangle objects' intrinsic attributes like shape
and appearance with extrinsic, viewer-centric properties such as their 3D
location. To address this bottleneck, we propose Unsupervised discovery of
Object-Centric neural Fields (uOCF). uOCF focuses on learning the intrinsics of
objects and models the extrinsics separately. Our approach significantly
improves systematic generalization, thus enabling unsupervised learning of
high-fidelity object-centric scene representations from sparse real-world
images. To evaluate our approach, we collect three new datasets, including two
real kitchen environments. Extensive experiments show that uOCF enables
unsupervised discovery of visually rich objects from a single real image,
allowing applications such as 3D object segmentation and scene manipulation.
Notably, uOCF demonstrates zero-shot generalization to unseen objects from a
single real image. Project page: https://red-fairy.github.io/uOCF/
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07379" title="Abstract">arXiv:2402.07379</a> [<a href="/pdf/2402.07379" title="Download PDF">pdf</a>, <a href="/format/2402.07379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distribution Locational Marginal Emission for Carbon Alleviation in  Distribution Networks: Formulation, Calculation, and Implication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sang%2C+L">Linwei Sang</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+Y">Yinliang Xu</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+H">Hongbin Sun</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+Q">Qiuwei Wu</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+W">Wenchuan Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Regulating the proper carbon-aware intervention policy is one of the keys to
emission alleviation in the distribution network, whose basis lies in
effectively attributing the emission responsibility using emission factors.
This paper establishes the distribution locational marginal emission (DLME) to
calculate the marginal change of emission from the marginal change of both
active and reactive load demand for incentivizing carbon alleviation. It first
formulates the day-head distribution network scheduling model based on the
second-order cone program (SOCP). The emission propagation and responsibility
are analyzed from demand to supply to system emission. Considering the complex
and implicit mapping of the SOCP-based scheduling model, the implicit theorem
is leveraged to exploit the optimal condition of SOCP. The corresponding
SOCP-based implicit derivation approach is proposed to calculate the DLMEs
effectively in a model-based way. Comprehensive numerical studies are conducted
to verify the superiority of the proposed method by comparing its calculation
efficacy to the conventional marginal estimation approach, assessing its
effectiveness in carbon alleviation with comparison to the average emission
factors, and evaluating its carbon alleviation ability of reactive DLME.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07381" title="Abstract">arXiv:2402.07381</a> [<a href="/pdf/2402.07381" title="Download PDF">pdf</a>, <a href="/format/2402.07381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RIS-Empowered LEO Satellite Networks for 6G: Promising Usage Scenarios  and Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Toka%2C+M">Mesut Toka</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+B">Byungju Lee</a>, 
<a href="/search/cs?searchtype=author&query=Seong%2C+J">Jaehyup Seong</a>, 
<a href="/search/cs?searchtype=author&query=Kaushik%2C+A">Aryan Kaushik</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Juhwan Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jungwoo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+N">Namyoon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+W">Wonjae Shin</a>, 
<a href="/search/cs?searchtype=author&query=Poor%2C+H+V">H. Vincent Poor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 5 figures, Paper accepted by IEEE Communications Magazine
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Low-Earth orbit (LEO) satellite systems have been deemed a promising key
enabler for current 5G and the forthcoming 6G wireless networks. Such LEO
satellite constellations can provide worldwide three-dimensional coverage, high
data rate, and scalability, thus enabling truly ubiquitous connectivity. On the
other hand, another promising technology, reconfigurable intelligent surfaces
(RISs), has emerged with favorable features, such as flexible deployment, cost
&amp; power efficiency, less transmission delay, noise-free nature, and in-band
full-duplex structure. LEO satellite networks have many practical imperfections
and limitations; however, exploiting RISs has been shown to be a potential
solution to overcome these challenges. Particularly, RISs can enhance link
quality, reduce the Doppler shift effect, and mitigate inter-/intra beam
interference. In this article, we delve into exploiting RISs in LEO satellite
networks. First, we present a holistic overview of LEO satellite communication
and RIS technology, highlighting potential benefits and challenges. Second, we
describe promising usage scenarios and applications in detail. Finally, we
discuss potential future directions and challenges on RIS-empowered LEO
networks, offering futuristic visions of the upcoming 6G era.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07384" title="Abstract">arXiv:2402.07384</a> [<a href="/pdf/2402.07384" title="Download PDF">pdf</a>, <a href="/format/2402.07384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Perceptual Limitation of Multimodal Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiarui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jinyi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Khayatkhoei%2C+M">Mahyar Khayatkhoei</a>, 
<a href="/search/cs?searchtype=author&query=Ilievski%2C+F">Filip Ilievski</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 14 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Multimodal Large Language Models (MLLMs) have recently shown remarkable
perceptual capability in answering visual questions, however, little is known
about the limits of their perception. In particular, while prior works have
provided anecdotal evidence of MLLMs' sensitivity to object size, this
phenomenon and its underlying causes have not been explored comprehensively. In
this work, we quantitatively study the perception of small visual objects in
several state-of-the-art MLLMs and reveal a pervasive limitation in answering
questions about small objects in images. Next, we identify four independent
factors that can contribute to this limitation -- object quality, size,
distractors, and location -- and conduct controlled intervention studies to
measure the effect of each factor on MLLMs' perception. In particular, we find
that lower object quality and smaller object size can both independently reduce
MLLMs' ability to answer visual questions. More surprisingly, we find that the
location of the object in the image and the presence of visual distractors can
also significantly reduce MLLMs' question answering accuracy. Our study
provides a better understanding of the perceptual limitation of MLLMs and
contributes new evaluation protocols for analyzing the perception of future
MLLMs. To facilitate further investigations, we release our code and data.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07385" title="Abstract">arXiv:2402.07385</a> [<a href="/pdf/2402.07385" title="Download PDF">pdf</a>, <a href="/format/2402.07385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Interpretable Low-complexity Model for Wireless Channel Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zaidi%2C+Z">Zainab Zaidi</a>, 
<a href="/search/cs?searchtype=author&query=Alpcan%2C+T">Tansu Alpcan</a>, 
<a href="/search/cs?searchtype=author&query=Leckie%2C+C">Christopher Leckie</a>, 
<a href="/search/cs?searchtype=author&query=Efrain%2C+S">Sarah Efrain</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">With the advent of machine learning, there has been renewed interest in the
problem of wireless channel estimation. This paper presents a novel
low-complexity wireless channel estimation scheme based on a tapped delay line
(TDL) model of wireless signal propagation, where a data-driven machine
learning approach is used to estimate the path delays and gains. Advantages of
this approach include low computation time and training data requirements, as
well as interpretability since the estimated model parameters and their
variance provide comprehensive representation of the dynamic wireless multipath
environment. We evaluate this model's performance using Matlab's ray-tracing
tool under static and dynamic conditions for increased realism instead of the
standard evaluation approaches using statistical channel models. Our results
show that our TDL-based model can accurately estimate the path delays and
associated gains for a broad-range of locations and operating conditions.
Root-mean-square estimation error remained less than $10^{-4}$, or $-40$dB, for
SNR $\geq 30$dB in all of our experiments.
<br />The key motivation for the novel channel estimation model is to gain
environment awareness, i.e., detecting changes in path delays and gains related
to interesting objects and events in the field. The channel state with
multipath delays and gains is a detailed measure to sense the field than the
single-tap channel state indicator calculated in current OFDM systems.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07386" title="Abstract">arXiv:2402.07386</a> [<a href="/pdf/2402.07386" title="Download PDF">pdf</a>, <a href="/format/2402.07386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chain-of-Layer: Iteratively Prompting Large Language Models for Taxonomy  Induction from Limited Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Q">Qingkai Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yuyang Bai</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhaoxuan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shangbin Feng</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Z">Zhenwen Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhihan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Meng Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Automatic taxonomy induction is crucial for web search, recommendation
systems, and question answering. Manual curation of taxonomies is expensive in
terms of human effort, making automatic taxonomy construction highly desirable.
In this work, we introduce Chain-of-Layer which is an in-context learning
framework designed to induct taxonomies from a given set of entities.
Chain-of-Layer breaks down the task into selecting relevant candidate entities
in each layer and gradually building the taxonomy from top to bottom. To
minimize errors, we introduce the Ensemble-based Ranking Filter to reduce the
hallucinated content generated at each iteration. Through extensive
experiments, we demonstrate that Chain-of-Layer achieves state-of-the-art
performance on four real-world benchmarks.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07393" title="Abstract">arXiv:2402.07393</a> [<a href="/pdf/2402.07393" title="Download PDF">pdf</a>, <a href="/format/2402.07393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TeMPO: Efficient Time-Multiplexed Dynamic Photonic Tensor Core for Edge  AI with Compact Slow-Light Electro-Optic Modulator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Meng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dennis Yin</a>, 
<a href="/search/cs?searchtype=author&query=Gangi%2C+N">Nicholas Gangi</a>, 
<a href="/search/cs?searchtype=author&query=Begovi%C4%87%2C+A">Amir Begovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Alexander Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z+R">Zhaoran Rena Huang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jiaqi Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 19 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Electronic-photonic computing systems offer immense potential in
energy-efficient artificial intelligence (AI) acceleration tasks due to the
superior computing speed and efficiency of optics, especially for real-time,
low-energy deep neural network (DNN) inference tasks on resource-restricted
edge platforms. However, current optical neural accelerators based on
foundry-available devices and conventional system architecture still encounter
a performance gap compared to highly customized electronic counterparts. To
bridge the performance gap due to lack of domain specialization, we present a
time-multiplexed dynamic photonic tensor accelerator, dubbed TeMPO, with
cross-layer device/circuit/architecture customization. At the device level, we
present foundry-compatible, customized photonic devices, including a slow-light
electro-optic modulator with experimental demonstration, optical splitters, and
phase shifters that significantly reduce the footprint and power in input
encoding and dot-product calculation. At the circuit level, partial products
are hierarchically accumulated via parallel photocurrent aggregation,
lightweight capacitive temporal integration, and sequential digital summation,
considerably relieving the analog-to-digital conversion bottleneck. We also
employ a multi-tile, multi-core architecture to maximize hardware sharing for
higher efficiency. Across diverse edge AI workloads, TeMPO delivers
digital-comparable task accuracy with superior quantization/noise tolerance. We
achieve a 368.6 TOPS peak performance, 22.3 TOPS/W energy efficiency, and 1.2
TOPS/mm$^2$ compute density, pushing the Pareto frontier in edge AI hardware.
This work signifies the power of cross-layer co-design and domain-specific
customization, paving the way for future electronic-photonic accelerators with
even greater performance and efficiency.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07395" title="Abstract">arXiv:2402.07395</a> [<a href="/pdf/2402.07395" title="Download PDF">pdf</a>, <a href="/format/2402.07395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparing the willingness to share for human-generated vs. AI-generated  fake news
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bashardoust%2C+A">Amirsiavosh Bashardoust</a>, 
<a href="/search/cs?searchtype=author&query=Feuerriegel%2C+S">Stefan Feuerriegel</a>, 
<a href="/search/cs?searchtype=author&query=Shrestha%2C+Y+R">Yash Raj Shrestha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Generative artificial intelligence (AI) presents large risks for society when
it is used to create fake news. A crucial factor for fake news to go viral on
social media is that users share such content. Here, we aim to shed light on
the sharing behavior of users across human-generated vs. AI-generated fake
news. Specifically, we study: (1) What is the perceived veracity of
human-generated fake news vs. AI-generated fake news? (2) What is the user's
willingness to share human-generated fake news vs. AI-generated fake news on
social media? (3) What socio-economic characteristics let users fall for
AI-generated fake news? To this end, we conducted a pre-registered, online
experiment with $N=$ 988 subjects and 20 fake news from the COVID-19 pandemic
generated by GPT-4 vs. humans. Our findings show that AI-generated fake news is
perceived as less accurate than human-generated fake news, but both tend to be
shared equally. Further, several socio-economic factors explain who falls for
AI-generated fake news.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07397" title="Abstract">arXiv:2402.07397</a> [<a href="/pdf/2402.07397" title="Download PDF">pdf</a>, <a href="/format/2402.07397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging AI to Advance Science and Computing Education across Africa:  Progress, Challenges, and Opportunities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boateng%2C+G">George Boateng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Book chapter for upcoming book: "Artificial Intelligence in Education: The Intersection of Technology and Pedagogy"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Computation and Language (cs.CL); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Across the African continent, students grapple with various educational
challenges, including limited access to essential resources such as computers,
internet connectivity, reliable electricity, and a shortage of qualified
teachers. Despite these challenges, recent advances in AI such as BERT, and
GPT-4 have demonstrated their potential for advancing education. Yet, these AI
tools tend to be deployed and evaluated predominantly within the context of
Western educational settings, with limited attention directed towards the
unique needs and challenges faced by students in Africa. In this book chapter,
we describe our works developing and deploying AI in Education tools in Africa:
(1) SuaCode, an AI-powered app that enables Africans to learn to code using
their smartphones, (2) AutoGrad, an automated grading, and feedback tool for
graphical and interactive coding assignments, (3) a tool for code plagiarism
detection that shows visual evidence of plagiarism, (4) Kwame, a bilingual AI
teaching assistant for coding courses, (5) Kwame for Science, a web-based AI
teaching assistant that provides instant answers to students' science questions
and (6) Brilla AI, an AI contestant for the National Science and Maths Quiz
competition. We discuss challenges and potential opportunities to use AI to
advance science and computing education across Africa.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07398" title="Abstract">arXiv:2402.07398</a> [<a href="/pdf/2402.07398" title="Download PDF">pdf</a>, <a href="/format/2402.07398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VisLingInstruct: Elevating Zero-Shot Learning in Multi-Modal Language  Models with Autonomous Instruction Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+D">Dongsheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xunzhu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+W">Weidong Han</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jinghui Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yukun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+G">Guoliang Xing</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junfeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">This paper presents VisLingInstruct, a novel approach to advancing
Multi-Modal Language Models (MMLMs) in zero-shot learning. Current MMLMs show
impressive zero-shot abilities in multi-modal tasks, but their performance
depends heavily on the quality of instructions. VisLingInstruct tackles this by
autonomously evaluating and optimizing instructional texts through In-Context
Learning, improving the synergy between visual perception and linguistic
expression in MMLMs. Alongside this instructional advancement, we have also
optimized the visual feature extraction modules in MMLMs, further augmenting
their responsiveness to textual cues. Our comprehensive experiments on MMLMs,
based on FlanT5 and Vicuna, show that VisLingInstruct significantly improves
zero-shot performance in visual multi-modal tasks. Notably, it achieves a 13.1%
and 9% increase in accuracy over the prior state-of-the-art on the TextVQA and
HatefulMemes datasets.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07401" title="Abstract">arXiv:2402.07401</a> [<a href="/pdf/2402.07401" title="Download PDF">pdf</a>, <a href="/format/2402.07401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can LLMs Produce Faithful Explanations For Fact-checking? Towards  Faithful Explainable Fact-Checking via Multi-Agent Debate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kyungha Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sangyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kung-Hsiang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+H+P">Hou Pong Chan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Manling Li</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Fact-checking research has extensively explored verification but less so the
generation of natural-language explanations, crucial for user trust. While
Large Language Models (LLMs) excel in text generation, their capability for
producing faithful explanations in fact-checking remains underexamined. Our
study investigates LLMs' ability to generate such explanations, finding that
zero-shot prompts often result in unfaithfulness. To address these challenges,
we propose the Multi-Agent Debate Refinement (MADR) framework, leveraging
multiple LLMs as agents with diverse roles in an iterative refining process
aimed at enhancing faithfulness in generated explanations. MADR ensures that
the final explanation undergoes rigorous validation, significantly reducing the
likelihood of unfaithful elements and aligning closely with the provided
evidence. Experimental results demonstrate that MADR significantly improves the
faithfulness of LLM-generated explanations to the evidence, advancing the
credibility and trustworthiness of these explanations.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07402" title="Abstract">arXiv:2402.07402</a> [<a href="/pdf/2402.07402" title="Download PDF">pdf</a>, <a href="/format/2402.07402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BDIQA: A New Dataset for Video Question Answering to Explore Cognitive  Reasoning through Theory of Mind
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yuanyuan Mao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+Q">Qin Ni</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+L">Liang He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As a foundational component of cognitive intelligence, theory of mind (ToM)
can make AI more closely resemble human thought processes, thereby enhancing
their interaction and collaboration with human. In particular, it can
significantly improve a model's comprehension of videos in complex scenes.
However, current video question answer (VideoQA) datasets focus on studying
causal reasoning within events few of them genuinely incorporating human ToM.
Consequently, there is a lack of development in ToM reasoning tasks within the
area of VideoQA. This paper presents BDIQA, the first benchmark to explore the
cognitive reasoning capabilities of VideoQA models in the context of ToM. BDIQA
is inspired by the cognitive development of children's ToM and addresses the
current deficiencies in machine ToM within datasets and tasks. Specifically, it
offers tasks at two difficulty levels, assessing Belief, Desire and Intention
(BDI) reasoning in both simple and complex scenarios. We conduct evaluations on
several mainstream methods of VideoQA and diagnose their capabilities with zero
shot, few shot and supervised learning. We find that the performance of
pre-trained models on cognitive reasoning tasks remains unsatisfactory. To
counter this challenge, we undertake thorough analysis and experimentation,
ultimately presenting two guidelines to enhance cognitive reasoning derived
from ablation analysis.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07403" title="Abstract">arXiv:2402.07403</a> [<a href="/pdf/2402.07403" title="Download PDF">pdf</a>, <a href="/ps/2402.07403" title="Download PostScript">ps</a>, <a href="/format/2402.07403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Make it more specific: A novel uncertainty based airway segmentation  application on 3D U-Net and its variants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shiyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Nan%2C+Y">Yang Nan</a>, 
<a href="/search/cs?searchtype=author&query=N%2C+F+F">Felder Federico N</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=F%2C+W+S+L">Walsh Simon L F</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Guang Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Each medical segmentation task should be considered with a specific AI
algorithm based on its scenario so that the most accurate prediction model can
be obtained. The most popular algorithms in medical segmentation, 3D U-Net and
its variants, can directly implement the task of lung trachea segmentation, but
its failure to consider the special tree-like structure of the trachea suggests
that there is much room for improvement in its segmentation accuracy.
Therefore, a research gap exists because a great amount of state-of-the-art DL
algorithms are vanilla 3D U-Net structures, which do not introduce the various
performance-enhancing modules that come with special natural image modality in
lung airway segmentation. In this paper, we proposed two different network
structures Branch-Level U-Net (B-UNet) and Branch-Level CE-UNet (B-CE-UNet)
which are based on U-Net structure and compared the prediction results with the
same dataset. Specially, both of the two networks add branch loss and central
line loss to learn the feature of fine branch endings of the airways.
Uncertainty estimation algorithms are also included to attain confident
predictions and thereby, increase the overall trustworthiness of our whole
model. In addition, predictions of the lung trachea based on the maximum
connectivity rate were calculated and extracted during post-processing for
segmentation refinement and pruning.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07404" title="Abstract">arXiv:2402.07404</a> [<a href="/pdf/2402.07404" title="Download PDF">pdf</a>, <a href="/ps/2402.07404" title="Download PostScript">ps</a>, <a href="/format/2402.07404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Multi-Criteria Decision Analysis with AI: Integrating Analytic  Hierarchy Process and GPT-4 for Automated Decision Support
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Svoboda%2C+I">Igor Svoboda</a>, 
<a href="/search/cs?searchtype=author&query=Lande%2C+D">Dmytro Lande</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Cryptography and Security (cs.CR); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Our study presents a new framework that incorporates the Analytic Hierarchy
Process (AHP) and Generative Pre-trained Transformer 4 (GPT-4) large language
model (LLM), bringing novel approaches to cybersecurity Multiple-criteria
Decision Making (MCDA). By utilizing the capabilities of GPT-4 autonomous
agents as virtual experts, we automate the decision-making process, enhancing
both efficiency and reliability. This new approach focuses on leveraging LLMs
for sophisticated decision analysis, highlighting the synergy between
traditional decision-making models and cutting-edge AI technologies. Our
innovative methodology demonstrates significant advancements in using AI-driven
agents for complex decision-making scenarios, highlighting the importance of AI
in strategic cybersecurity applications. The findings reveal the transformative
potential of combining AHP and LLMs, establishing a new paradigm for
intelligent decision support systems in cybersecurity and beyond.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07405" title="Abstract">arXiv:2402.07405</a> [<a href="/pdf/2402.07405" title="Download PDF">pdf</a>, <a href="/format/2402.07405" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> D&#xf3;lares or Dollars? Unraveling the Bilingual Prowess of Financial LLMs  Between Spanish and English
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+R">Ruoyu Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+C">Chenhan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+D">Duanyu Feng</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+W">Weiguang Han</a>, 
<a href="/search/cs?searchtype=author&query=Lopez-Lira%2C+A">Alejandro Lopez-Lira</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao-Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ananiadou%2C+S">Sophia Ananiadou</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+M">Min Peng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jimin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qianqian Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Despite Spanish's pivotal role in the global finance industry, a pronounced
gap exists in Spanish financial natural language processing (NLP) and
application studies compared to English, especially in the era of large
language models (LLMs). To bridge this gap, we unveil Tois\'on de Oro, the
first bilingual framework that establishes instruction datasets, finetuned
LLMs, and evaluation benchmark for financial LLMs in Spanish joint with
English. We construct a rigorously curated bilingual instruction dataset
including over 144K Spanish and English samples from 15 datasets covering 7
tasks. Harnessing this, we introduce FinMA-ES, an LLM designed for bilingual
financial applications. We evaluate our model and existing LLMs using FLARE-ES,
the first comprehensive bilingual evaluation benchmark with 21 datasets
covering 9 tasks. The FLARE-ES benchmark results reveal a significant
multilingual performance gap and bias in existing LLMs. FinMA-ES models surpass
SOTA LLMs such as GPT-4 in Spanish financial tasks, due to strategic
instruction tuning and leveraging data from diverse linguistic resources,
highlighting the positive impact of cross-linguistic transfer. All our
datasets, models, and benchmarks have been released.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07407" title="Abstract">arXiv:2402.07407</a> [<a href="/pdf/2402.07407" title="Download PDF">pdf</a>, <a href="/format/2402.07407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conformal Predictive Programming for Chance Constrained Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhao%2C+Y">Yiqi Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Yu%2C+X">Xinyi Yu</a>, 
<a href="/search/eess?searchtype=author&query=Deshmukh%2C+J+V">Jyotirmoy V. Deshmukh</a>, 
<a href="/search/eess?searchtype=author&query=Lindemann%2C+L">Lars Lindemann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">Motivated by the advances in conformal prediction (CP), we propose conformal
predictive programming (CPP), an approach to solve chance constrained
optimization (CCO) problems, i.e., optimization problems with nonlinear
constraint functions affected by arbitrary random parameters. CPP utilizes
samples from these random parameters along with the quantile lemma -- which is
central to CP -- to transform the CCO problem into a deterministic optimization
problem. We then present two tractable reformulations of CPP by: (1) writing
the quantile as a linear program along with its KKT conditions (CPP-KKT), and
(2) using mixed integer programming (CPP-MIP). CPP comes with marginal
probabilistic feasibility guarantees for the CCO problem that are conceptually
different from existing approaches, e.g., the sample approximation and the
scenario approach. While we explore algorithmic similarities with the sample
approximation approach, we emphasize that the strength of CPP is that it can
easily be extended to incorporate different variants of CP. To illustrate this,
we present robust conformal predictive programming to deal with distribution
shifts in the uncertain parameters of the CCO problem.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07408" title="Abstract">arXiv:2402.07408</a> [<a href="/pdf/2402.07408" title="Download PDF">pdf</a>, <a href="/format/2402.07408" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models are Few-shot Generators: Proposing Hybrid Prompt  Algorithm To Generate Webshell Escape Samples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+M">Mingrui Ma</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+L">Lansheng Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chunjie Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The frequent occurrence of cyber-attacks has made webshell attacks and
defense gradually become a research hotspot in the field of network security.
However, the lack of publicly available benchmark datasets and the
over-reliance on manually defined rules for webshell escape sample generation
have slowed down the progress of research related to webshell escape sample
generation strategies and artificial intelligence-based webshell detection
algorithms. To address the drawbacks of weak webshell sample escape
capabilities, the lack of webshell datasets with complex malicious features,
and to promote the development of webshell detection technology, we propose the
Hybrid Prompt algorithm for webshell escape sample generation with the help of
large language models. As a prompt algorithm specifically developed for
webshell sample generation, the Hybrid Prompt algorithm not only combines
various prompt ideas including Chain of Thought, Tree of Thought, but also
incorporates various components such as webshell hierarchical module and
few-shot example to facilitate the LLM in learning and reasoning webshell
escape strategies. Experimental results show that the Hybrid Prompt algorithm
can work with multiple LLMs with excellent code reasoning ability to generate
high-quality webshell samples with high Escape Rate (88.61% with GPT-4 model on
VIRUSTOTAL detection engine) and Survival Rate (54.98% with GPT-4 model).
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07410" title="Abstract">arXiv:2402.07410</a> [<a href="/pdf/2402.07410" title="Download PDF">pdf</a>, <a href="/format/2402.07410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Closer Look at the Robustness of Contrastive Language-Image  Pre-Training (CLIP)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tu%2C+W">Weijie Tu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+W">Weijian Deng</a>, 
<a href="/search/cs?searchtype=author&query=Gedeon%2C+T">Tom Gedeon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NeurIPS 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Contrastive Language-Image Pre-training (CLIP) models have demonstrated
remarkable generalization capabilities across multiple challenging distribution
shifts. However, there is still much to be explored in terms of their
robustness to the variations of specific visual factors. In real-world
applications, reliable and safe systems must consider other safety objectives
beyond classification accuracy, such as predictive uncertainty. Yet, the
effectiveness of CLIP models on such safety-related features is less-explored.
Driven by the above, this work comprehensively investigates the safety
objectives of CLIP models, specifically focusing on three key properties:
resilience to visual factor variations, calibrated uncertainty estimations, and
the ability to detect anomalous inputs. To this end, we study 83 CLIP models
and 127 ImageNet classifiers. They are diverse in architecture, (pre)training
distribution and training strategies. We consider 10 visual factors (e.g.,
shape and pattern), 5 types of out-of-distribution data, and 8 natural and
challenging test conditions with different shift types, such as texture, style,
and perturbation shifts. Our study has unveiled several previously unknown
insights into CLIP models. For instance, they are not consistently more
calibrated than other ImageNet models, which contradicts existing findings.
Additionally, our analysis underscores the significance of training source
design by showcasing its profound influence on the three safety-related
properties. We believe our comprehensive study can shed light on and help guide
the development of more robust and reliable CLIP models.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07411" title="Abstract">arXiv:2402.07411</a> [<a href="/pdf/2402.07411" title="Download PDF">pdf</a>, <a href="/format/2402.07411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Potential-Based Reward Shaping For Intrinsic Motivation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Forbes%2C+G+C">Grant C. Forbes</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+N">Nitish Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Villalobos-Arias%2C+L">Leonardo Villalobos-Arias</a>, 
<a href="/search/cs?searchtype=author&query=Potts%2C+C+M">Colin M. Potts</a>, 
<a href="/search/cs?searchtype=author&query=Jhala%2C+A">Arnav Jhala</a>, 
<a href="/search/cs?searchtype=author&query=Roberts%2C+D+L">David L. Roberts</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version of paper appearing in AAMAS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recently there has been a proliferation of intrinsic motivation (IM)
reward-shaping methods to learn in complex and sparse-reward environments.
These methods can often inadvertently change the set of optimal policies in an
environment, leading to suboptimal behavior. Previous work on mitigating the
risks of reward shaping, particularly through potential-based reward shaping
(PBRS), has not been applicable to many IM methods, as they are often complex,
trainable functions themselves, and therefore dependent on a wider set of
variables than the traditional reward functions that PBRS was developed for. We
present an extension to PBRS that we prove preserves the set of optimal
policies under a more general set of functions than has been previously proven.
We also present {\em Potential-Based Intrinsic Motivation} (PBIM), a method for
converting IM rewards into a potential-based form that is useable without
altering the set of optimal policies. Testing in the MiniGrid DoorKey and Cliff
Walking environments, we demonstrate that PBIM successfully prevents the agent
from converging to a suboptimal policy and can speed up training.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07412" title="Abstract">arXiv:2402.07412</a> [<a href="/pdf/2402.07412" title="Download PDF">pdf</a>, <a href="/format/2402.07412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auxiliary Reward Generation with Transition Distance Representation  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Shijie Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yingnan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+B">By Liang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Peng Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Reinforcement learning (RL) has shown its strength in challenging sequential
decision-making problems. The reward function in RL is crucial to the learning
performance, as it serves as a measure of the task completion degree. In
real-world problems, the rewards are predominantly human-designed, which
requires laborious tuning, and is easily affected by human cognitive biases. To
achieve automatic auxiliary reward generation, we propose a novel
representation learning approach that can measure the ``transition distance''
between states. Building upon these representations, we introduce an auxiliary
reward generation technique for both single-task and skill-chaining scenarios
without the need for human knowledge. The proposed approach is evaluated in a
wide range of manipulation tasks. The experiment results demonstrate the
effectiveness of measuring the transition distance between states and the
induced improvement by auxiliary rewards, which not only promotes better
learning efficiency but also increases convergent stability.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07415" title="Abstract">arXiv:2402.07415</a> [<a href="/pdf/2402.07415" title="Download PDF">pdf</a>, <a href="/format/2402.07415" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Context-aware Multi-Model Object Detection for Diversely Heterogeneous  Compute Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Davis%2C+J">Justin Davis</a>, 
<a href="/search/cs?searchtype=author&query=Belviranli%2C+M+E">Mehmet E. Belviranli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)

</div>
<p class="mathjax">In recent years, deep neural networks (DNNs) have gained widespread adoption
for continuous mobile object detection (OD) tasks, particularly in autonomous
systems. However, a prevalent issue in their deployment is the
one-size-fits-all approach, where a single DNN is used, resulting in
inefficient utilization of computational resources. This inefficiency is
particularly detrimental in energy-constrained systems, as it degrades overall
system efficiency. We identify that, the contextual information embedded in the
input data stream (e.g. the frames in the camera feed that the OD models are
run on) could be exploited to allow a more efficient multi-model-based OD
process. In this paper, we propose SHIFT which continuously selects from a
variety of DNN-based OD models depending on the dynamically changing contextual
information and computational constraints. During this selection, SHIFT
uniquely considers multi-accelerator execution to better optimize the
energy-efficiency while satisfying the latency constraints. Our proposed
methodology results in improvements of up to 7.5x in energy usage and 2.8x in
latency compared to state-of-the-art GPU-based single model OD approaches.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07417" title="Abstract">arXiv:2402.07417</a> [<a href="/pdf/2402.07417" title="Download PDF">pdf</a>, <a href="/format/2402.07417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Study Into What Matters for Calibrating Vision-Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tu%2C+W">Weijie Tu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+W">Weijian Deng</a>, 
<a href="/search/cs?searchtype=author&query=Campbell%2C+D">Dylan Campbell</a>, 
<a href="/search/cs?searchtype=author&query=Gould%2C+S">Stephen Gould</a>, 
<a href="/search/cs?searchtype=author&query=Gedeon%2C+T">Tom Gedeon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 8 figures, this version is not fully edited and will be updated soon
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Vision--Language Models (VLMs) have emerged as the dominant approach for
zero-shot recognition, adept at handling diverse scenarios and significant
distribution changes. However, their deployment in risk-sensitive areas
requires a deeper understanding of their uncertainty estimation capabilities, a
relatively uncharted area. In this study, we explore the calibration properties
of VLMs across different architectures, datasets, and training strategies. In
particular, we analyze the uncertainty estimation performance of VLMs when
calibrated in one domain, label set or hierarchy level, and tested in a
different one. Our findings reveal that while VLMs are not inherently
calibrated for uncertainty, temperature scaling significantly and consistently
improves calibration, even across shifts in distribution and changes in label
set. Moreover, VLMs can be calibrated with a very small set of examples.
Through detailed experimentation, we highlight the potential applications and
importance of our insights, aiming for more reliable and effective use of VLMs
in critical, real-world scenarios.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07418" title="Abstract">arXiv:2402.07418</a> [<a href="/pdf/2402.07418" title="Download PDF">pdf</a>, <a href="/format/2402.07418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SemTra: A Semantic Skill Translator for Cross-Domain Zero-Shot Policy  Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shin%2C+S">Sangwoo Shin</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+M">Minjong Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jeongwoo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Woo%2C+H">Honguk Woo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 2024 Camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">This work explores the zero-shot adaptation capability of semantic skills,
semantically interpretable experts' behavior patterns, in cross-domain
settings, where a user input in interleaved multi-modal snippets can prompt a
new long-horizon task for different domains. In these cross-domain settings, we
present a semantic skill translator framework SemTra which utilizes a set of
multi-modal models to extract skills from the snippets, and leverages the
reasoning capabilities of a pretrained language model to adapt these extracted
skills to the target domain. The framework employs a two-level hierarchy for
adaptation: task adaptation and skill adaptation. During task adaptation,
seq-to-seq translation by the language model transforms the extracted skills
into a semantic skill sequence, which is tailored to fit the cross-domain
contexts. Skill adaptation focuses on optimizing each semantic skill for the
target domain context, through parametric instantiations that are facilitated
by language prompting and contrastive learning-based context inferences. This
hierarchical adaptation empowers the framework to not only infer a complex task
specification in one-shot from the interleaved multi-modal snippets, but also
adapt it to new domains with zero-shot learning abilities. We evaluate our
framework with Meta-World, Franka Kitchen, RLBench, and CARLA environments. The
results clarify the framework's superiority in performing long-horizon tasks
and adapting to different domains, showing its broad applicability in practical
use cases, such as cognitive robots interpreting abstract instructions and
autonomous vehicles operating under varied configurations.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07419" title="Abstract">arXiv:2402.07419</a> [<a href="/pdf/2402.07419" title="Download PDF">pdf</a>, <a href="/format/2402.07419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conditional Generative Models are Sufficient to Sample from Any Causal  Effect Estimand
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+M">Md Musfiqur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Jordan%2C+M">Matt Jordan</a>, 
<a href="/search/cs?searchtype=author&query=Kocaoglu%2C+M">Murat Kocaoglu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">Causal inference from observational data has recently found many applications
in machine learning. While sound and complete algorithms exist to compute
causal effects, many of these algorithms require explicit access to conditional
likelihoods over the observational distribution, which is difficult to estimate
in the high-dimensional regime, such as with images. To alleviate this issue,
researchers have approached the problem by simulating causal relations with
neural models and obtained impressive results. However, none of these existing
approaches can be applied to generic scenarios such as causal graphs on image
data with latent confounders, or obtain conditional interventional samples. In
this paper, we show that any identifiable causal effect given an arbitrary
causal graph can be computed through push-forward computations of conditional
generative models. Based on this result, we devise a diffusion-based approach
to sample from any (conditional) interventional distribution on image data. To
showcase our algorithm's performance, we conduct experiments on a Colored MNIST
dataset having both the treatment ($X$) and the target variables ($Y$) as
images and obtain interventional samples from $P(y|do(x))$. As an application
of our algorithm, we evaluate two large conditional generative models that are
pre-trained on the CelebA dataset by analyzing the strength of spurious
correlations and the level of disentanglement they achieve.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07420" title="Abstract">arXiv:2402.07420</a> [<a href="/pdf/2402.07420" title="Download PDF">pdf</a>, <a href="/format/2402.07420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Transit Obfuscation Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takahashi%2C+H">Hideaki Takahashi</a>, 
<a href="/search/cs?searchtype=author&query=Fukunaga%2C+A">Alex Fukunaga</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Concealing an intermediate point on a route or visible from a route is an
important goal in some transportation and surveillance scenarios. This paper
studies the Transit Obfuscation Problem, the problem of traveling from some
start location to an end location while "covering" a specific transit point
that needs to be concealed from adversaries. We propose the notion of transit
anonymity, a quantitative guarantee of the anonymity of a specific transit
point, even with a powerful adversary with full knowledge of the path planning
algorithm. We propose and evaluate planning/search algorithms that satisfy this
anonymity criterion.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07422" title="Abstract">arXiv:2402.07422</a> [<a href="/pdf/2402.07422" title="Download PDF">pdf</a>, <a href="/ps/2402.07422" title="Download PostScript">ps</a>, <a href="/format/2402.07422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> News Recommendation with Attention Mechanism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianrui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Changxin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yuxin Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Chufeng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weisheng Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, Journal of Industrial Engineering and Applied Science
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">This paper explores the area of news recommendation, a key component of
online information sharing. Initially, we provide a clear introduction to news
recommendation, defining the core problem and summarizing current methods and
notable recent algorithms. We then present our work on implementing the NRAM
(News Recommendation with Attention Mechanism), an attention-based approach for
news recommendation, and assess its effectiveness. Our evaluation shows that
NRAM has the potential to significantly improve how news content is
personalized for users on digital news platforms.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07425" title="Abstract">arXiv:2402.07425</a> [<a href="/pdf/2402.07425" title="Download PDF">pdf</a>, <a href="/format/2402.07425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Debiasing Recommendation with Personal Popularity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ning%2C+W">Wentao Ning</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+R">Reynold Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xiao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Kao%2C+B">Ben Kao</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+N">Nan Huo</a>, 
<a href="/search/cs?searchtype=author&query=Haldar%2C+N+A+H">Nur AI Hasan Haldar</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+B">Bo Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW'24 as a research full paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Global popularity (GP) bias is the phenomenon that popular items are
recommended much more frequently than they should be, which goes against the
goal of providing personalized recommendations and harms user experience and
recommendation accuracy. Many methods have been proposed to reduce GP bias but
they fail to notice the fundamental problem of GP, i.e., it considers
popularity from a \textit{global} perspective of \textit{all users} and uses a
single set of popular items, and thus cannot capture the interests of
individual users. As such, we propose a user-aware version of item popularity
named \textit{personal popularity} (PP), which identifies different popular
items for each user by considering the users that share similar interests. As
PP models the preferences of individual users, it naturally helps to produce
personalized recommendations and mitigate GP bias. To integrate PP into
recommendation, we design a general \textit{personal popularity aware
counterfactual} (PPAC) framework, which adapts easily to existing
recommendation models. In particular, PPAC recognizes that PP and GP have both
direct and indirect effects on recommendations and controls direct effects with
counterfactual inference techniques for unbiased recommendations. All codes and
datasets are available at \url{https://github.com/Stevenn9981/PPAC}.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07426" title="Abstract">arXiv:2402.07426</a> [<a href="/pdf/2402.07426" title="Download PDF">pdf</a>, <a href="/ps/2402.07426" title="Download PostScript">ps</a>, <a href="/format/2402.07426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computational Aspects of Bayesian Persuasion under Approximate Best  Response
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Kunhe Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanrui Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We study Bayesian persuasion under approximate best response, where the
receiver may choose any action that is not too much suboptimal, given their
posterior belief upon receiving the signal. We focus on the computational
aspects of the problem, aiming to design algorithms that efficiently compute
(almost) optimal strategies for the sender. Despite the absence of the
revelation principle -- which has been one of the most powerful tools in
Bayesian persuasion -- we design polynomial-time exact algorithms for the
problem when either the state space or the action space is small, as well as a
quasi-polynomial-time approximation scheme (QPTAS) for the general problem. On
the negative side, we show there is no polynomial-time exact algorithm for the
general problem unless $\mathsf{P} = \mathsf{NP}$. Our results build on several
new algorithmic ideas, which might be useful in other principal-agent problems
where robustness is desired.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07429" title="Abstract">arXiv:2402.07429</a> [<a href="/pdf/2402.07429" title="Download PDF">pdf</a>, <a href="/ps/2402.07429" title="Download PostScript">ps</a>, <a href="/format/2402.07429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Particle Filter SLAM for Vehicle Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianrui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Changxin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yuxin Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Chufeng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jiqiang Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, Journal of Industrial Engineering and Applied Science
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Simultaneous Localization and Mapping (SLAM) presents a formidable challenge
in robotics, involving the dynamic construction of a map while concurrently
determining the precise location of the robotic agent within an unfamiliar
environment. This intricate task is further compounded by the inherent
"chicken-and-egg" dilemma, where accurate mapping relies on a dependable
estimation of the robot's location, and vice versa. Moreover, the computational
intensity of SLAM adds an additional layer of complexity, making it a crucial
yet demanding topic in the field. In our research, we address the challenges of
SLAM by adopting the Particle Filter SLAM method. Our approach leverages
encoded data and fiber optic gyro (FOG) information to enable precise
estimation of vehicle motion, while lidar technology contributes to
environmental perception by providing detailed insights into surrounding
obstacles. The integration of these data streams culminates in the
establishment of a Particle Filter SLAM framework, representing a key endeavor
in this paper to effectively navigate and overcome the complexities associated
with simultaneous localization and mapping in robotic systems.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07431" title="Abstract">arXiv:2402.07431</a> [<a href="/pdf/2402.07431" title="Download PDF">pdf</a>, <a href="/format/2402.07431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SALAD: Smart AI Language Assistant Daily
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nihal%2C+R+A">Ragib Amin Nihal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">SALAD is an AI-driven language-learning application designed to help
foreigners learn Japanese. It offers translations in Kanji-Kana-Romaji, speech
recognition, translated audio, vocabulary tracking, grammar explanations, and
songs generated from newly learned words. The app targets beginners and
intermediate learners, aiming to make language acquisition more accessible and
enjoyable. SALAD uses daily translations to enhance fluency and comfort in
communication with native speakers. The primary objectives include effective
Japanese language learning, user engagement, and progress tracking. A survey by
us found that 39% of foreigners in Japan face discomfort in conversations with
Japanese speakers. Over 60% of foreigners expressed confidence in SALAD's
ability to enhance their Japanese language skills. The app uses large language
models, speech recognition, and diffusion models to bridge the language gap and
foster a more inclusive community in Japan.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07432" title="Abstract">arXiv:2402.07432</a> [<a href="/pdf/2402.07432" title="Download PDF">pdf</a>, <a href="/format/2402.07432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intrinsic Task-based Evaluation for Referring Expression Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guanyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Same%2C+F">Fahime Same</a>, 
<a href="/search/cs?searchtype=author&query=van+Deemter%2C+K">Kees van Deemter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recently, a human evaluation study of Referring Expression Generation (REG)
models had an unexpected conclusion: on \textsc{webnlg}, Referring Expressions
(REs) generated by the state-of-the-art neural models were not only
indistinguishable from the REs in \textsc{webnlg} but also from the REs
generated by a simple rule-based system. Here, we argue that this limitation
could stem from the use of a purely ratings-based human evaluation (which is a
common practice in Natural Language Generation). To investigate these issues,
we propose an intrinsic task-based evaluation for REG models, in which, in
addition to rating the quality of REs, participants were asked to accomplish
two meta-level tasks. One of these tasks concerns the referential success of
each RE; the other task asks participants to suggest a better alternative for
each RE. The outcomes suggest that, in comparison to previous evaluations, the
new evaluation protocol assesses the performance of each REG model more
comprehensively and makes the participants' ratings more reliable and
discriminable.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07433" title="Abstract">arXiv:2402.07433</a> [<a href="/pdf/2402.07433" title="Download PDF">pdf</a>, <a href="/format/2402.07433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Logical Synchrony Networks: A formal model for deterministic  distribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kenwright%2C+L">Logan Kenwright</a>, 
<a href="/search/cs?searchtype=author&query=Roop%2C+P">Partha Roop</a>, 
<a href="/search/cs?searchtype=author&query=Allen%2C+N">Nathan Allen</a>, 
<a href="/search/cs?searchtype=author&query=Lall%2C+S">Sanjay Lall</a>, 
<a href="/search/cs?searchtype=author&query=Cascaval%2C+C">Calin Cascaval</a>, 
<a href="/search/cs?searchtype=author&query=Spalink%2C+T">Tammo Spalink</a>, 
<a href="/search/cs?searchtype=author&query=Izzard%2C+M">Martin Izzard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">Kahn Process Networks (KPNs) are a deterministic Model of Computation (MoC)
for distributed systems. KPNs supports non-blocking writes and blocking reads,
with the consequent assumption of unbounded buffers between processes. Variants
such as Finite FIFO Platforms (FFP) have been developed, which enforce
boundedness. One issue with existing models is that they mix process
synchronisation with process execution. In this paper we address how these two
facets may be decoupled.
<br />This paper explores a recent alternative called bittide, which decouples the
execution of a process from the control needed for process synchronisation, and
thus preserves determinism and boundedness while ensuring pipelined execution
for better throughput. Our intuition is that such an approach could leverage
not only determinism and buffer boundedness but may potentially offer better
overall throughput.
<br />To understand the behavior of these systems we define a formal model -- a
deterministic MoC called Logical Synchrony Networks (LSNs). LSNs describes a
network of processes modelled as a graph, with edges representing invariant
logical delays between a producer process and the corresponding consumer
process. We show that this abstraction is satisfied by KPNs. Subsequently, we
show that both FFPs and bittide faithfully implement this abstraction. Thus, we
show for the first time that FFPs and bittide offer two alternative ways of
implementing deterministic distributed systems with the latter being more
performant.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07437" title="Abstract">arXiv:2402.07437</a> [<a href="/pdf/2402.07437" title="Download PDF">pdf</a>, <a href="/ps/2402.07437" title="Download PostScript">ps</a>, <a href="/format/2402.07437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Optimal Tax Design in Nonatomic Congestion Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+Q">Qiwen Cui</a>, 
<a href="/search/cs?searchtype=author&query=Fazel%2C+M">Maryam Fazel</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+S+S">Simon S. Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">We study how to learn the optimal tax design to maximize the efficiency in
nonatomic congestion games. It is known that self-interested behavior among the
players can damage the system's efficiency. Tax mechanisms is a common method
to alleviate this issue and induce socially optimal behavior. In this work, we
take the initial step for learning the optimal tax that can minimize the social
cost with \emph{equilibrium feedback}, i.e., the tax designer can only observe
the equilibrium state under the enforced tax. Existing algorithms are not
applicable due to the exponentially large tax function space, nonexistence of
the gradient, and nonconvexity of the objective. To tackle these challenges,
our algorithm leverages several novel components: (1) piece-wise linear tax to
approximate the optimal tax; (2) an extra linear term to guarantee a strongly
convex potential function; (3) efficient subroutine to find the ``boundary''
tax. The algorithm can find an $\epsilon$-optimal tax with $O(\beta
F^2/\epsilon)$ sample complexity, where $\beta$ is the smoothness of the cost
function and $F$ is the number of facilities.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07440" title="Abstract">arXiv:2402.07440</a> [<a href="/pdf/2402.07440" title="Download PDF">pdf</a>, <a href="/format/2402.07440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking and Building Long-Context Retrieval Models with LoCo and  M2-BERT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saad-Falcon%2C+J">Jon Saad-Falcon</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+D+Y">Daniel Y. Fu</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+S">Simran Arora</a>, 
<a href="/search/cs?searchtype=author&query=Guha%2C+N">Neel Guha</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%A9%2C+C">Christopher R&#xe9;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Retrieval pipelines-an integral component of many machine learning
systems-perform poorly in domains where documents are long (e.g., 10K tokens or
more) and where identifying the relevant document requires synthesizing
information across the entire text. Developing long-context retrieval encoders
suitable for these domains raises three challenges: (1) how to evaluate
long-context retrieval performance, (2) how to pretrain a base language model
to represent both short contexts (corresponding to queries) and long contexts
(corresponding to documents), and (3) how to fine-tune this model for retrieval
under the batch size limitations imposed by GPU memory constraints. To address
these challenges, we first introduce LoCoV1, a novel 12 task benchmark
constructed to measure long-context retrieval where chunking is not possible or
not effective. We next present the M2-BERT retrieval encoder, an 80M parameter
state-space encoder model built from the Monarch Mixer architecture, capable of
scaling to documents up to 32K tokens long. We describe a pretraining data
mixture which allows this encoder to process both short and long context
sequences, and a finetuning approach that adapts this base model to retrieval
with only single-sample batches. Finally, we validate the M2-BERT retrieval
encoder on LoCoV1, finding that it outperforms competitive baselines by up to
23.3 points, despite containing 5-90x fewer parameters.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07441" title="Abstract">arXiv:2402.07441</a> [<a href="/pdf/2402.07441" title="Download PDF">pdf</a>, <a href="/ps/2402.07441" title="Download PostScript">ps</a>, <a href="/format/2402.07441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fully Dynamic Geometric Vertex Cover and Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhore%2C+S">Sujoy Bhore</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+T+M">Timothy M. Chan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">In this work, we study two fundamental graph optimization problems, minimum
vertex cover (MVC) and maximum-cardinality matching (MCM), for intersection
graphs of geometric objects, e.g., disks, rectangles, hypercubes, etc., in
$d$-dimensional Euclidean space. We consider the problems in fully dynamic
settings, allowing insertions and deletions of objects.
<br />We develop a general framework for dynamic MVC in intersection graphs,
achieving sublinear amortized update time for most natural families of
geometric objects. In particular, we show that -
<br />\begin{itemize}
<br />\item For a dynamic collection of disks in $\mathbb{R}^2$ or hypercubes in
$\mathbb{R}^d$ (for constant $d$), it is possible to maintain a
$(1+\varepsilon)$-approximate vertex cover in $\polylog$ amortized update time.
These results also hold in the bipartite case.
<br />\item For a dynamic collection of rectangles in $\mathbb{R}^2$, it is
possible to maintain a $(\frac{3}{2}+\varepsilon)$-approximate vertex cover in
$\polylog$ amortized update time. \end{itemize}
<br />Along the way, we obtain the first near-linear time static algorithms for MVC
in the above two cases with the same approximation factors.
<br />Next, we turn our attention to the MCM problem. Although our MVC algorithms
automatically allow us to approximate the size of the MCM in bipartite
geometric intersection graphs, they do not produce a matching. We give another
general framework to maintain an approximate maximum matching, and further
extend the approach to handle non-bipartite intersection graphs. In particular,
we show that -
<br />\begin{itemize} \item For a dynamic collection of (bichromatic or
monochromatic) disks in $\mathbb{R}^2$ or hypercubes in $\mathbb{R}^d$ (for
constant $d$), it is possible to maintain a $(1+\varepsilon)$-approximate
matching in $\polylog$ amortized update time. \end{itemize}
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07442" title="Abstract">arXiv:2402.07442</a> [<a href="/pdf/2402.07442" title="Download PDF">pdf</a>, <a href="/ps/2402.07442" title="Download PostScript">ps</a>, <a href="/format/2402.07442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Game Agent Driven by Free-Form Text Command: Using LLM-based Code  Generation and Behavior Branch
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ito%2C+R">Ray Ito</a>, 
<a href="/search/cs?searchtype=author&query=Takahashi%2C+J">Junichiro Takahashi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is posted at JSAI 2024 Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Several attempts have been made to implement text command control for game
agents. However, current technologies are limited to processing predefined
format commands. This paper proposes a pioneering text command control system
for a game agent that can understand natural language commands expressed in
free-form. The proposed system uses a large language model (LLM) for code
generation to interpret and transform natural language commands into behavior
branch, a proposed knowledge expression based on behavior trees, which
facilitates execution by the game agent. This study conducted empirical
validation within a game environment that simulates a Pok\'emon game and
involved multiple participants. The results confirmed the system's ability to
understand and carry out natural language commands, representing a noteworthy
in the realm of real-time language interactive game agents.
<br />Notice for the use of this material. The copyright of this material is
retained by the Japanese Society for Artificial Intelligence (JSAI). This
material is published here with the agreement of JSAI. Please be complied with
Copyright Law of Japan if any users wish to reproduce, make derivative work,
distribute or make available to the public any part or whole thereof. All
Rights Reserved, Copyright (C) The Japanese Society for Artificial
Intelligence.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07443" title="Abstract">arXiv:2402.07443</a> [<a href="/pdf/2402.07443" title="Download PDF">pdf</a>, <a href="/format/2402.07443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The I/O Complexity of Attention, or How Optimal is Flash Attention?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saha%2C+B">Barna Saha</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+C">Christopher Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Complexity (cs.CC); Data Structures and Algorithms (cs.DS); Information Theory (cs.IT)

</div>
<p class="mathjax">Self-attention is at the heart of the popular Transformer architecture, yet
suffers from quadratic time and memory complexity. The breakthrough
FlashAttention algorithm revealed I/O complexity as the true bottleneck in
scaling Transformers. Given two levels of memory hierarchy, a fast cache (e.g.
GPU on-chip SRAM) and a slow memory (e.g. GPU high-bandwidth memory), the I/O
complexity measures the number of accesses to memory. FlashAttention computes
attention using $\frac{N^2d^2}{M}$ I/O operations where $N$ is the dimension of
the attention matrix, $d$ the head-dimension and $M$ the cache size. However,
is this I/O complexity optimal? The known lower bound only rules out an I/O
complexity of $o(Nd)$ when $M=\Theta(Nd)$, since the output that needs to be
written to slow memory is $\Omega(Nd)$. This leads to the main question of our
work: Is FlashAttention I/O optimal for all values of $M$?
<br />We resolve the above question in its full generality by showing an I/O
complexity lower bound that matches the upper bound provided by FlashAttention
for any values of $M \geq d^2$ within any constant factors. Further, we give a
better algorithm with lower I/O complexity for $M &lt; d^2$, and show that it is
optimal as well. Moreover, our lower bounds do not rely on using combinatorial
matrix multiplication for computing the attention matrix. We show even if one
uses fast matrix multiplication, the above I/O complexity bounds cannot be
improved. We do so by introducing a new communication complexity protocol for
matrix compression, and connecting communication complexity to I/O complexity.
To the best of our knowledge, this is the first work to establish a connection
between communication complexity and I/O complexity, and we believe this
connection could be of independent interest and will find many more
applications in proving I/O complexity lower bounds in the future.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07444" title="Abstract">arXiv:2402.07444</a> [<a href="/pdf/2402.07444" title="Download PDF">pdf</a>, <a href="/format/2402.07444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Malicious Package Detection using Metadata Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Halder%2C+S">S. Halder</a>, 
<a href="/search/cs?searchtype=author&query=Bewong%2C+M">M. Bewong</a>, 
<a href="/search/cs?searchtype=author&query=Mahboubi%2C+A">A. Mahboubi</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Y. Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+R">R. Islam</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+Z">Z. Islam</a>, 
<a href="/search/cs?searchtype=author&query=Ip%2C+R">R. Ip</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+E">E. Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Ramachandran%2C+G">G. Ramachandran</a>, 
<a href="/search/cs?searchtype=author&query=Babar%2C+A">A. Babar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Protecting software supply chains from malicious packages is paramount in the
evolving landscape of software development. Attacks on the software supply
chain involve attackers injecting harmful software into commonly used packages
or libraries in a software repository. For instance, JavaScript uses Node
Package Manager (NPM), and Python uses Python Package Index (PyPi) as their
respective package repositories. In the past, NPM has had vulnerabilities such
as the event-stream incident, where a malicious package was introduced into a
popular NPM package, potentially impacting a wide range of projects. As the
integration of third-party packages becomes increasingly ubiquitous in modern
software development, accelerating the creation and deployment of applications,
the need for a robust detection mechanism has become critical. On the other
hand, due to the sheer volume of new packages being released daily, the task of
identifying malicious packages presents a significant challenge. To address
this issue, in this paper, we introduce a metadata-based malicious package
detection model, MeMPtec. This model extracts a set of features from package
metadata information. These extracted features are classified as either
easy-to-manipulate (ETM) or difficult-to-manipulate (DTM) features based on
monotonicity and restricted control properties. By utilising these metadata
features, not only do we improve the effectiveness of detecting malicious
packages, but also we demonstrate its resistance to adversarial attacks in
comparison with existing state-of-the-art. Our experiments indicate a
significant reduction in both false positives (up to 97.56%) and false
negatives (up to 91.86%).
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07446" title="Abstract">arXiv:2402.07446</a> [<a href="/pdf/2402.07446" title="Download PDF">pdf</a>, <a href="/format/2402.07446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quality Does Matter: A Detailed Look at the Quality and Utility of  Web-Mined Parallel Corpora
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ranathunga%2C+S">Surangika Ranathunga</a>, 
<a href="/search/cs?searchtype=author&query=de+Silva%2C+N">Nisansa de Silva</a>, 
<a href="/search/cs?searchtype=author&query=Velayuthan%2C+M">Menan Velayuthan</a>, 
<a href="/search/cs?searchtype=author&query=Fernando%2C+A">Aloka Fernando</a>, 
<a href="/search/cs?searchtype=author&query=Rathnayake%2C+C">Charitha Rathnayake</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We conducted a detailed analysis on the quality of web-mined corpora for two
low-resource languages (making three language pairs, English-Sinhala,
English-Tamil and Sinhala-Tamil). We ranked each corpus according to a
similarity measure and carried out an intrinsic and extrinsic evaluation on
different portions of this ranked corpus. We show that there are significant
quality differences between different portions of web-mined corpora and that
the quality varies across languages and datasets. We also show that, for some
web-mined datasets, Neural Machine Translation (NMT) models trained with their
highest-ranked 25k portion can be on par with human-curated datasets.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07448" title="Abstract">arXiv:2402.07448</a> [<a href="/pdf/2402.07448" title="Download PDF">pdf</a>, <a href="/ps/2402.07448" title="Download PostScript">ps</a>, <a href="/format/2402.07448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AraSpider: Democratizing Arabic-to-SQL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heakl%2C+A">Ahmed Heakl</a>, 
<a href="/search/cs?searchtype=author&query=Mohamed%2C+Y">Youssef Mohamed</a>, 
<a href="/search/cs?searchtype=author&query=Zaky%2C+A+B">Ahmed B. Zaky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Databases (cs.DB); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">This study presents AraSpider, the first Arabic version of the Spider
dataset, aimed at improving natural language processing (NLP) in the
Arabic-speaking community. Four multilingual translation models were tested for
their effectiveness in translating English to Arabic. Additionally, two models
were assessed for their ability to generate SQL queries from Arabic text. The
results showed that using back translation significantly improved the
performance of both ChatGPT 3.5 and SQLCoder models, which are considered top
performers on the Spider dataset. Notably, ChatGPT 3.5 demonstrated
high-quality translation, while SQLCoder excelled in text-to-SQL tasks. The
study underscores the importance of incorporating contextual schema and
employing back translation strategies to enhance model performance in Arabic
NLP tasks. Moreover, the provision of detailed methodologies for
reproducibility and translation of the dataset into other languages highlights
the research's commitment to promoting transparency and collaborative knowledge
sharing in the field. Overall, these contributions advance NLP research,
empower Arabic-speaking researchers, and enrich the global discourse on
language comprehension and database interrogation.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07452" title="Abstract">arXiv:2402.07452</a> [<a href="/pdf/2402.07452" title="Download PDF">pdf</a>, <a href="/format/2402.07452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TriAug: Out-of-Distribution Detection for Robust Classification of  Imbalanced Breast Lesion in Ultrasound
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yinyu Ye</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shijing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+D">Dong Ni</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Ruobing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Different diseases, such as histological subtypes of breast lesions, have
severely varying incidence rates. Even trained with substantial amount of
in-distribution (ID) data, models often encounter out-of-distribution (OOD)
samples belonging to unseen classes in clinical reality. To address this, we
propose a novel framework built upon a long-tailed OOD detection task for
breast ultrasound images. It is equipped with a triplet state augmentation
(TriAug) which improves ID classification accuracy while maintaining a
promising OOD detection performance. Meanwhile, we designed a balanced sphere
loss to handle the class imbalanced problem.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07453" title="Abstract">arXiv:2402.07453</a> [<a href="/pdf/2402.07453" title="Download PDF">pdf</a>, <a href="/ps/2402.07453" title="Download PostScript">ps</a>, <a href="/format/2402.07453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bandit-Feedback Online Multiclass Classification: Variants and Tradeoffs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Filmus%2C+Y">Yuval Filmus</a>, 
<a href="/search/cs?searchtype=author&query=Hanneke%2C+S">Steve Hanneke</a>, 
<a href="/search/cs?searchtype=author&query=Mehalel%2C+I">Idan Mehalel</a>, 
<a href="/search/cs?searchtype=author&query=Moran%2C+S">Shay Moran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Consider the domain of multiclass classification within the adversarial
online setting. What is the price of relying on bandit feedback as opposed to
full information? To what extent can an adaptive adversary amplify the loss
compared to an oblivious one? To what extent can a randomized learner reduce
the loss compared to a deterministic one? We study these questions in the
mistake bound model and provide nearly tight answers.
<br />We demonstrate that the optimal mistake bound under bandit feedback is at
most $O(k)$ times higher than the optimal mistake bound in the full information
case, where $k$ represents the number of labels. This bound is tight and
provides an answer to an open question previously posed and studied by Daniely
and Helbertal ['13] and by Long ['17, '20], who focused on deterministic
learners.
<br />Moreover, we present nearly optimal bounds of $\tilde{\Theta}(k)$ on the gap
between randomized and deterministic learners, as well as between adaptive and
oblivious adversaries in the bandit feedback setting. This stands in contrast
to the full information scenario, where adaptive and oblivious adversaries are
equivalent, and the gap in mistake bounds between randomized and deterministic
learners is a constant multiplicative factor of $2$.
<br />In addition, our results imply that in some cases the optimal randomized
mistake bound is approximately the square-root of its deterministic parallel.
Previous results show that this is essentially the smallest it can get.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07456" title="Abstract">arXiv:2402.07456</a> [<a href="/pdf/2402.07456" title="Download PDF">pdf</a>, <a href="/format/2402.07456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OS-Copilot: Towards Generalist Computer Agents with Self-Improvement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhiyong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+C">Chengcheng Han</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Z">Zichen Ding</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+Z">Zhenmin Weng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhoumianze Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+S">Shunyu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingpeng Kong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://os-copilot.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Autonomous interaction with the computer has been a longstanding challenge
with great potential, and the recent proliferation of large language models
(LLMs) has markedly accelerated progress in building digital agents. However,
most of these agents are designed to interact with a narrow domain, such as a
specific software or website. This narrow focus constrains their applicability
for general computer tasks. To this end, we introduce OS-Copilot, a framework
to build generalist agents capable of interfacing with comprehensive elements
in an operating system (OS), including the web, code terminals, files,
multimedia, and various third-party applications. We use OS-Copilot to create
FRIDAY, a self-improving embodied agent for automating general computer tasks.
On GAIA, a general AI assistants benchmark, FRIDAY outperforms previous methods
by 35%, showcasing strong generalization to unseen applications via accumulated
skills from previous tasks. We also present numerical and quantitative evidence
that FRIDAY learns to control and self-improve on Excel and Powerpoint with
minimal supervision. Our OS-Copilot framework and empirical findings provide
infrastructure and insights for future research toward more capable and
general-purpose computer agents.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07458" title="Abstract">arXiv:2402.07458</a> [<a href="/pdf/2402.07458" title="Download PDF">pdf</a>, <a href="/format/2402.07458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Distance from Calibration in Sequential Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiao%2C+M">Mingda Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Letian Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study a sequential binary prediction setting where the forecaster is
evaluated in terms of the calibration distance, which is defined as the $L_1$
distance between the predicted values and the set of predictions that are
perfectly calibrated in hindsight. This is analogous to a calibration measure
recently proposed by B{\l}asiok, Gopalan, Hu and Nakkiran (STOC 2023) for the
offline setting. The calibration distance is a natural and intuitive measure of
deviation from perfect calibration, and satisfies a Lipschitz continuity
property which does not hold for many popular calibration measures, such as the
$L_1$ calibration error and its variants.
<br />We prove that there is a forecasting algorithm that achieves an $O(\sqrt{T})$
calibration distance in expectation on an adversarially chosen sequence of $T$
binary outcomes. At the core of this upper bound is a structural result showing
that the calibration distance is accurately approximated by the lower
calibration distance, which is a continuous relaxation of the former. We then
show that an $O(\sqrt{T})$ lower calibration distance can be achieved via a
simple minimax argument and a reduction to online learning on a Lipschitz
class.
<br />On the lower bound side, an $\Omega(T^{1/3})$ calibration distance is shown
to be unavoidable, even when the adversary outputs a sequence of independent
random bits, and has an additional ability to early stop (i.e., to stop
producing random bits and output the same bit in the remaining steps).
Interestingly, without this early stopping, the forecaster can achieve a much
smaller calibration distance of $\mathrm{polylog}(T)$.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07460" title="Abstract">arXiv:2402.07460</a> [<a href="/pdf/2402.07460" title="Download PDF">pdf</a>, <a href="/format/2402.07460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anonymizing Test Data in Android: Does It Hurt?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Masserini%2C+E">Elena Masserini</a>, 
<a href="/search/cs?searchtype=author&query=Ginelli%2C+D">Davide Ginelli</a>, 
<a href="/search/cs?searchtype=author&query=Micucci%2C+D">Daniela Micucci</a>, 
<a href="/search/cs?searchtype=author&query=Briola%2C+D">Daniela Briola</a>, 
<a href="/search/cs?searchtype=author&query=Mariani%2C+L">Leonardo Mariani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Failure data collected from the field (e.g., failure traces, bug reports, and
memory dumps) represent an invaluable source of information for developers who
need to reproduce and analyze failures. Unfortunately, field data may include
sensitive information and thus cannot be collected indiscriminately.
Privacy-preserving techniques can address this problem anonymizing data and
reducing the risk of disclosing personal information. However, collecting
anonymized information may harm reproducibility, that is, the anonymized data
may not allow the reproduction of a failure observed in the field. In this
paper, we present an empirical investigation about the impact of
privacy-preserving techniques on the reproducibility of failures. In
particular, we study how five privacy-preserving techniques may impact
reproducibilty for 19 bugs in 17 Android applications. Results provide insights
on how to select and configure privacy-preserving techniques.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07462" title="Abstract">arXiv:2402.07462</a> [<a href="/pdf/2402.07462" title="Download PDF">pdf</a>, <a href="/ps/2402.07462" title="Download PostScript">ps</a>, <a href="/format/2402.07462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Hormetic Approach to the Value-Loading Problem: Preventing the  Paperclip Apocalypse?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Henry%2C+N+I+N">Nathan I. N. Henry</a>, 
<a href="/search/cs?searchtype=author&query=Pedersen%2C+M">Mangor Pedersen</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+M">Matt Williams</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+J+L+B">Jamin L. B. Martin</a>, 
<a href="/search/cs?searchtype=author&query=Donkin%2C+L">Liesje Donkin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 105 pages (24 pages for main article excluding references and appendices), 46 figures (7 in main article, 39 in appendices), and 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG); Multiagent Systems (cs.MA); Theoretical Economics (econ.TH)

</div>
<p class="mathjax">The value-loading problem is a significant challenge for researchers aiming
to create artificial intelligence (AI) systems that align with human values and
preferences. This problem requires a method to define and regulate safe and
optimal limits of AI behaviors. In this work, we propose HALO (Hormetic
ALignment via Opponent processes), a regulatory paradigm that uses hormetic
analysis to regulate the behavioral patterns of AI. Behavioral hormesis is a
phenomenon where low frequencies of a behavior have beneficial effects, while
high frequencies are harmful. By modeling behaviors as allostatic opponent
processes, we can use either Behavioral Frequency Response Analysis (BFRA) or
Behavioral Count Response Analysis (BCRA) to quantify the hormetic limits of
repeatable behaviors. We demonstrate how HALO can solve the 'paperclip
maximizer' scenario, a thought experiment where an unregulated AI tasked with
making paperclips could end up converting all matter in the universe into
paperclips. Our approach may be used to help create an evolving database of
'values' based on the hedonic calculus of repeatable behaviors with decreasing
marginal utility. This positions HALO as a promising solution for the
value-loading problem, which involves embedding human-aligned values into an AI
system, and the weak-to-strong generalization problem, which explores whether
weak models can supervise stronger models as they become more intelligent.
Hence, HALO opens several research avenues that may lead to the development of
a computational value system that allows an AI algorithm to learn whether the
decisions it makes are right or wrong.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07465" title="Abstract">arXiv:2402.07465</a> [<a href="/pdf/2402.07465" title="Download PDF">pdf</a>, <a href="/format/2402.07465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Score-Based Physics-Informed Neural Networks for High-Dimensional  Fokker-Planck Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zheyuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhongqiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Karniadakis%2C+G+E">George Em Karniadakis</a>, 
<a href="/search/cs?searchtype=author&query=Kawaguchi%2C+K">Kenji Kawaguchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Dynamical Systems (math.DS); Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
<p class="mathjax">The Fokker-Planck (FP) equation is a foundational PDE in stochastic
processes. However, curse of dimensionality (CoD) poses challenge when dealing
with high-dimensional FP PDEs. Although Monte Carlo and vanilla
Physics-Informed Neural Networks (PINNs) have shown the potential to tackle
CoD, both methods exhibit numerical errors in high dimensions when dealing with
the probability density function (PDF) associated with Brownian motion. The
point-wise PDF values tend to decrease exponentially as dimension increases,
surpassing the precision of numerical simulations and resulting in substantial
errors. Moreover, due to its massive sampling, Monte Carlo fails to offer fast
sampling. Modeling the logarithm likelihood (LL) via vanilla PINNs transforms
the FP equation into a difficult HJB equation, whose error grows rapidly with
dimension. To this end, we propose a novel approach utilizing a score-based
solver to fit the score function in SDEs. The score function, defined as the
gradient of the LL, plays a fundamental role in inferring LL and PDF and
enables fast SDE sampling. Three fitting methods, Score Matching (SM), Sliced
SM (SSM), and Score-PINN, are introduced. The proposed score-based SDE solver
operates in two stages: first, employing SM, SSM, or Score-PINN to acquire the
score; and second, solving the LL via an ODE using the obtained score.
Comparative evaluations across these methods showcase varying trade-offs. The
proposed method is evaluated across diverse SDEs, including anisotropic OU
processes, geometric Brownian, and Brownian with varying eigenspace. We also
test various distributions, including Gaussian, Log-normal, Laplace, and
Cauchy. The numerical results demonstrate the score-based SDE solver's
stability, speed, and performance across different settings, solidifying its
potential as a solution to CoD for high-dimensional FP equations.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07466" title="Abstract">arXiv:2402.07466</a> [<a href="/pdf/2402.07466" title="Download PDF">pdf</a>, <a href="/format/2402.07466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VCR: Video representation for Contextual Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nir%2C+O">Oron Nir</a>, 
<a href="/search/cs?searchtype=author&query=Vidra%2C+I">Idan Vidra</a>, 
<a href="/search/cs?searchtype=author&query=Neeman%2C+A">Avi Neeman</a>, 
<a href="/search/cs?searchtype=author&query=Kinarti%2C+B">Barak Kinarti</a>, 
<a href="/search/cs?searchtype=author&query=Shamir%2C+A">Ariel Shamir</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Streamlining content discovery within media archives requires integrating
advanced data representations and effective visualization techniques for clear
communication of video topics to users. The proposed system addresses the
challenge of efficiently navigating large video collections by exploiting a
fusion of visual, audio, and textual features to accurately index and
categorize video content through a text-based method. Additionally, semantic
embeddings are employed to provide contextually relevant information and
recommendations to users, resulting in an intuitive and engaging exploratory
experience over our topics ontology map using OpenAI GPT-4.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07470" title="Abstract">arXiv:2402.07470</a> [<a href="/pdf/2402.07470" title="Download PDF">pdf</a>, <a href="/format/2402.07470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pushing The Limit of LLM Capacity for Text Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yazhou Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+C">Chenyu Ren</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qiuchi Li</a>, 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+P">Prayag Tiwari</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Benyou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+J">Jing Qin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The value of text classification's future research has encountered challenges
and uncertainties, due to the extraordinary efficacy demonstrated by large
language models (LLMs) across numerous downstream NLP tasks. In this era of
open-ended language modeling, where task boundaries are gradually fading, an
urgent question emerges: have we made significant advances in text
classification under the full benefit of LLMs? To answer this question, we
propose RGPT, an adaptive boosting framework tailored to produce a specialized
text classification LLM by recurrently ensembling a pool of strong base
learners. The base learners are constructed by adaptively adjusting the
distribution of training samples and iteratively fine-tuning LLMs with them.
Such base learners are then ensembled to be a specialized text classification
LLM, by recurrently incorporating the historical predictions from the previous
learners. Through a comprehensive empirical comparison, we show that RGPT
significantly outperforms 8 SOTA PLMs and 7 SOTA LLMs on four benchmarks by
1.36% on average. Further evaluation experiments show a clear surpassing of
RGPT over human classification.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07471" title="Abstract">arXiv:2402.07471</a> [<a href="/pdf/2402.07471" title="Download PDF">pdf</a>, <a href="/format/2402.07471" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Decentralized Learning with Random Walks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cyffers%2C+E">Edwige Cyffers</a>, 
<a href="/search/cs?searchtype=author&query=Bellet%2C+A">Aur&#xe9;lien Bellet</a>, 
<a href="/search/cs?searchtype=author&query=Upadhyay%2C+J">Jalaj Upadhyay</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The popularity of federated learning comes from the possibility of better
scalability and the ability for participants to keep control of their data,
improving data security and sovereignty. Unfortunately, sharing model updates
also creates a new privacy attack surface. In this work, we characterize the
privacy guarantees of decentralized learning with random walk algorithms, where
a model is updated by traveling from one node to another along the edges of a
communication graph. Using a recent variant of differential privacy tailored to
the study of decentralized algorithms, namely Pairwise Network Differential
Privacy, we derive closed-form expressions for the privacy loss between each
pair of nodes where the impact of the communication topology is captured by
graph theoretic quantities. Our results further reveal that random walk
algorithms tends to yield better privacy guarantees than gossip algorithms for
nodes close from each other. We supplement our theoretical results with
empirical evaluation on synthetic and real-world graphs and datasets.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07477" title="Abstract">arXiv:2402.07477</a> [<a href="/pdf/2402.07477" title="Download PDF">pdf</a>, <a href="/format/2402.07477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Food Recommendation as Language Processing (F-RLP): A Personalized and  Contextual Paradigm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rostami%2C+A">Ali Rostami</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+R">Ramesh Jain</a>, 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+A+M">Amir M. Rahmani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">State-of-the-art rule-based and classification-based food recommendation
systems face significant challenges in becoming practical and useful. This
difficulty arises primarily because most machine learning models struggle with
problems characterized by an almost infinite number of classes and a limited
number of samples within an unbalanced dataset. Conversely, the emergence of
Large Language Models (LLMs) as recommendation engines offers a promising
avenue. However, a general-purpose Recommendation as Language Processing (RLP)
approach lacks the critical components necessary for effective food
recommendations. To address this gap, we introduce Food Recommendation as
Language Processing (F-RLP), a novel framework that offers a food-specific,
tailored infrastructure. F-RLP leverages the capabilities of LLMs to maximize
their potential, thereby paving the way for more accurate, personalized food
recommendations.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07478" title="Abstract">arXiv:2402.07478</a> [<a href="/pdf/2402.07478" title="Download PDF">pdf</a>, <a href="/format/2402.07478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparison of Different Representations of Ordinal Patterns and Their  Usability in Data Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schnurr%2C+A">Alexander Schnurr</a>, 
<a href="/search/cs?searchtype=author&query=Silbernagel%2C+A">Angelika Silbernagel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures, published in conference proceedings of ICECET 2023 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Probability (math.PR)

</div>
<p class="mathjax">We describe and analyze different approaches to represent ordinal patterns.
All of these can be found in the literature. The most important representations
(plus sub-classes) are compared in terms of their applicability from different
angles. Namely we consider digital implementation, inverse patterns and ties
between values. At the end we provide a guideline on which occasions which
representation should be used.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07480" title="Abstract">arXiv:2402.07480</a> [<a href="/pdf/2402.07480" title="Download PDF">pdf</a>, <a href="/format/2402.07480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topological Safeguard for Evasion Attack based on the Interpretability  of Artificial Neural Network Behavior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Echeberria-Barrio%2C+X">Xabier Echeberria-Barrio</a>, 
<a href="/search/cs?searchtype=author&query=Gil-Lerchundi%2C+A">Amaia Gil-Lerchundi</a>, 
<a href="/search/cs?searchtype=author&query=Mendialdua%2C+I">I&#xf1;igo Mendialdua</a>, 
<a href="/search/cs?searchtype=author&query=Orduna-Urrutia%2C+R">Raul Orduna-Urrutia</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Pattern Recognition, Volume 147, 2024, 110130, ISSN 0031-3203
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In the last years, Deep Learning technology has been proposed in different
fields, bringing many advances in each of them, but identifying new threats in
these solutions regarding cybersecurity. Those implemented models have brought
several vulnerabilities associated with Deep Learning technology. Moreover,
those allow taking advantage of the implemented model, obtaining private
information, and even modifying the model's decision-making. Therefore,
interest in studying those vulnerabilities/attacks and designing defenses to
avoid or fight them is gaining prominence among researchers. In particular, the
widely known evasion attack is being analyzed by researchers; thus, several
defenses to avoid such a threat can be found in the literature. Since the
presentation of the L-BFG algorithm, this threat concerns the research
community. However, it continues developing new and ingenious countermeasures
since there is no perfect defense for all the known evasion algorithms. In this
work, a novel detector of evasion attacks is developed. It focuses on the
information of the activations of the neurons given by the model when an input
sample is injected. Moreover, it puts attention to the topology of the targeted
deep learning model to analyze the activations according to which neurons are
connecting. This approach has been decided because the literature shows that
the targeted model's topology contains essential information about if the
evasion attack occurs. For this purpose, a huge data preprocessing is required
to introduce all this information in the detector, which uses the Graph
Convolutional Neural Network (GCN) technology. Thus, it understands the
topology of the target model, obtaining promising results and improving the
outcomes presented in the literature related to similar defenses.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07483" title="Abstract">arXiv:2402.07483</a> [<a href="/pdf/2402.07483" title="Download PDF">pdf</a>, <a href="/format/2402.07483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> T-RAG: Lessons from the LLM Trenches
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fatehkia%2C+M">Masoomali Fatehkia</a>, 
<a href="/search/cs?searchtype=author&query=Lucas%2C+J+K">Ji Kim Lucas</a>, 
<a href="/search/cs?searchtype=author&query=Chawla%2C+S">Sanjay Chawla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Large Language Models (LLM) have shown remarkable language capabilities
fueling attempts to integrate them into applications across a wide range of
domains. An important application area is question answering over private
enterprise documents where the main considerations are data security, which
necessitates applications that can be deployed on-prem, limited computational
resources and the need for a robust application that correctly responds to
queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent
framework for building LLM-based applications. While building a RAG is
relatively straightforward, making it robust and a reliable application
requires extensive customization and relatively deep knowledge of the
application domain. We share our experiences building and deploying an LLM
application for question answering over private organizational documents. Our
application combines the use of RAG with a finetuned open-source LLM.
Additionally, our system, which we call Tree-RAG (T-RAG), uses a tree structure
to represent entity hierarchies within the organization. This is used to
generate a textual description to augment the context when responding to user
queries pertaining to entities within the organization's hierarchy. Our
evaluations show that this combination performs better than a simple RAG or
finetuning implementation. Finally, we share some lessons learned based on our
experiences building an LLM application for real-world use.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07485" title="Abstract">arXiv:2402.07485</a> [<a href="/pdf/2402.07485" title="Download PDF">pdf</a>, <a href="/format/2402.07485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SLIT: Boosting Audio-Text Pre-Training via Multi-Stage Learning and  Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+Y">Yifei Xin</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhesong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bilei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Lu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zejun Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Audio-text pre-training (ATP) has witnessed remarkable strides across a
variety of downstream tasks. Yet, most existing pretrained audio models only
specialize in either discriminative tasks or generative tasks. In this study,
we develop SLIT, a novel ATP framework which transfers flexibly to both
audio-text understanding and generation tasks, bootstrapping audio-text
pre-training from frozen pretrained audio encoders and large language models.
To bridge the modality gap during pre-training, we leverage Q-Former, which
undergoes a multi-stage pre-training process. The first stage enhances
audio-text representation learning from a frozen audio encoder, while the
second stage boosts audio-to-text generative learning with a frozen language
model. Furthermore, we introduce an ATP instruction tuning strategy, which
enables flexible and informative feature extraction tailered to the given
instructions for different tasks. Experiments show that SLIT achieves superior
performances on a variety of audio-text understanding and generation tasks, and
even demonstrates strong generalization capabilities when directly applied to
zero-shot scenarios.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07487" title="Abstract">arXiv:2402.07487</a> [<a href="/pdf/2402.07487" title="Download PDF">pdf</a>, <a href="/format/2402.07487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Score-based Diffusion Models via Stochastic Differential Equations -- a  Technical Tutorial
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+W">Wenpin Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hanyang Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; History and Overview (math.HO)

</div>
<p class="mathjax">This is an expository article on the score-based diffusion models, with a
particular focus on the formulation via stochastic differential equations
(SDE). After a gentle introduction, we discuss the two pillars in the diffusion
modeling -- sampling and score matching, which encompass the SDE/ODE sampling,
score matching efficiency, the consistency model, and reinforcement learning.
Short proofs are given to illustrate the main idea of the stated results. The
article is primarily for introducing the beginners to the field, and
practitioners may also find some analysis useful in designing new models or
algorithms.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07496" title="Abstract">arXiv:2402.07496</a> [<a href="/pdf/2402.07496" title="Download PDF">pdf</a>, <a href="/ps/2402.07496" title="Download PostScript">ps</a>, <a href="/format/2402.07496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Deep Learning defenses Against Adversarial Examples  Through Visualizations for Dynamic Risk Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Echeberria-Barrio%2C+X">Xabier Echeberria-Barrio</a>, 
<a href="/search/cs?searchtype=author&query=Gil-Lerchundi%2C+A">Amaia Gil-Lerchundi</a>, 
<a href="/search/cs?searchtype=author&query=Egana-Zubia%2C+J">Jon Egana-Zubia</a>, 
<a href="/search/cs?searchtype=author&query=Orduna-Urrutia%2C+R">Raul Orduna-Urrutia</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Neural Comput and Applic 34, 20477 to 20490, 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">In recent years, Deep Neural Network models have been developed in different
fields, where they have brought many advances. However, they have also started
to be used in tasks where risk is critical. A misdiagnosis of these models can
lead to serious accidents or even death. This concern has led to an interest
among researchers to study possible attacks on these models, discovering a long
list of vulnerabilities, from which every model should be defended. The
adversarial example attack is a widely known attack among researchers, who have
developed several defenses to avoid such a threat. However, these defenses are
as opaque as a deep neural network model, how they work is still unknown. This
is why visualizing how they change the behavior of the target model is
interesting in order to understand more precisely how the performance of the
defended model is being modified. For this work, some defenses, against
adversarial example attack, have been selected in order to visualize the
behavior modification of each of them in the defended model. Adversarial
training, dimensionality reduction and prediction similarity were the selected
defenses, which have been developed using a model composed by convolution
neural network layers and dense neural network layers. In each defense, the
behavior of the original model has been compared with the behavior of the
defended model, representing the target model by a graph in a visualization.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07498" title="Abstract">arXiv:2402.07498</a> [<a href="/pdf/2402.07498" title="Download PDF">pdf</a>, <a href="/format/2402.07498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerated Smoothing: A Scalable Approach to Randomized Smoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhardwaj%2C+D">Devansh Bhardwaj</a>, 
<a href="/search/cs?searchtype=author&query=Kaushik%2C+K">Kshitiz Kaushik</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Sarthak Gupta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Randomized smoothing has emerged as a potent certifiable defense against
adversarial attacks by employing smoothing noises from specific distributions
to ensure the robustness of a smoothed classifier. However, the utilization of
Monte Carlo sampling in this process introduces a compute-intensive element,
which constrains the practicality of randomized smoothing on a larger scale. To
address this limitation, we propose a novel approach that replaces Monte Carlo
sampling with the training of a surrogate neural network. Through extensive
experimentation in various settings, we demonstrate the efficacy of our
approach in approximating the smoothed classifier with remarkable precision.
Furthermore, we demonstrate that our approach significantly accelerates the
robust radius certification process, providing nearly $600$X improvement in
computation time, overcoming the computational bottlenecks associated with
traditional randomized smoothing.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07501" title="Abstract">arXiv:2402.07501</a> [<a href="/pdf/2402.07501" title="Download PDF">pdf</a>, <a href="/format/2402.07501" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One Train for Two Tasks: An Encrypted Traffic Classification Framework  Using Supervised Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haozhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+X">Xi Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Le Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qing Li</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+Z">Zhen Ling</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ye Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The code is available at <a href="https://github.com/ViktorAxelsen/CLE-TFE">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As network security receives widespread attention, encrypted traffic
classification has become the current research focus. However, existing methods
conduct traffic classification without sufficiently considering the common
characteristics between data samples, leading to suboptimal performance.
Moreover, they train the packet-level and flow-level classification tasks
independently, which is redundant because the packet representations learned in
the packet-level task can be exploited by the flow-level task. Therefore, in
this paper, we propose an effective model named a Contrastive Learning Enhanced
Temporal Fusion Encoder (CLE-TFE). In particular, we utilize supervised
contrastive learning to enhance the packet-level and flow-level representations
and perform graph data augmentation on the byte-level traffic graph so that the
fine-grained semantic-invariant characteristics between bytes can be captured
through contrastive learning. We also propose cross-level multi-task learning,
which simultaneously accomplishes the packet-level and flow-level
classification tasks in the same model with one training. Further experiments
show that CLE-TFE achieves the best overall performance on the two tasks, while
its computational overhead (i.e., floating point operations, FLOPs) is only
about 1/14 of the pre-trained model (e.g., ET-BERT). We release the code at
https://github.com/ViktorAxelsen/CLE-TFE
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07502" title="Abstract">arXiv:2402.07502</a> [<a href="/pdf/2402.07502" title="Download PDF">pdf</a>, <a href="/format/2402.07502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ClusterTabNet: Supervised clustering method for table detection and  table structure recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Polewczyk%2C+M">Marek Polewczyk</a>, 
<a href="/search/cs?searchtype=author&query=Spinaci%2C+M">Marco Spinaci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 4 figures, submitted. The code will be released at <a href="https://github.com/SAP-samples">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We present a novel deep-learning-based method to cluster words in documents
which we apply to detect and recognize tables given the OCR output. We
interpret table structure bottom-up as a graph of relations between pairs of
words (belonging to the same row, column, header, as well as to the same table)
and use a transformer encoder model to predict its adjacency matrix. We
demonstrate the performance of our method on the PubTables-1M dataset as well
as PubTabNet and FinTabNet datasets. Compared to the current state-of-the-art
detection methods such as DETR and Faster R-CNN, our method achieves similar or
better accuracy, while requiring a significantly smaller model.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07506" title="Abstract">arXiv:2402.07506</a> [<a href="/pdf/2402.07506" title="Download PDF">pdf</a>, <a href="/ps/2402.07506" title="Download PostScript">ps</a>, <a href="/format/2402.07506" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeuralSentinel: Safeguarding Neural Network Reliability and  Trustworthiness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Echeberria-Barrio%2C+X">Xabier Echeberria-Barrio</a>, 
<a href="/search/cs?searchtype=author&query=Gorricho%2C+M">Mikel Gorricho</a>, 
<a href="/search/cs?searchtype=author&query=Valencia%2C+S">Selene Valencia</a>, 
<a href="/search/cs?searchtype=author&query=Zola%2C+F">Francesco Zola</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CS and IT Conference Proceedings, CS and IT Conference
  Proceedings, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The usage of Artificial Intelligence (AI) systems has increased
exponentially, thanks to their ability to reduce the amount of data to be
analyzed, the user efforts and preserving a high rate of accuracy. However,
introducing this new element in the loop has converted them into attacked
points that can compromise the reliability of the systems. This new scenario
has raised crucial challenges regarding the reliability and trustworthiness of
the AI models, as well as about the uncertainties in their response decisions,
becoming even more crucial when applied in critical domains such as healthcare,
chemical, electrical plants, etc. To contain these issues, in this paper, we
present NeuralSentinel (NS), a tool able to validate the reliability and
trustworthiness of AI models. This tool combines attack and defence strategies
and explainability concepts to stress an AI model and help non-expert staff
increase their confidence in this new system by understanding the model
decisions. NS provide a simple and easy-to-use interface for helping humans in
the loop dealing with all the needed information. This tool was deployed and
used in a Hackathon event to evaluate the reliability of a skin cancer image
detector. During the event, experts and non-experts attacked and defended the
detector, learning which factors were the most important for model
misclassification and which techniques were the most efficient. The event was
also used to detect NS's limitations and gather feedback for further
improvements.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07507" title="Abstract">arXiv:2402.07507</a> [<a href="/pdf/2402.07507" title="Download PDF">pdf</a>, <a href="/format/2402.07507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clustering Dynamics for Improved Speed Prediction Deriving from  Topographical GPS Registrations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carneiro%2C+S+A">Sarah Almeida Carneiro</a> (LIGM), 
<a href="/search/cs?searchtype=author&query=Chierchia%2C+G">Giovanni Chierchia</a> (LIGM), 
<a href="/search/cs?searchtype=author&query=Pirayre%2C+A">Aurelie Pirayre</a> (IFPEN), 
<a href="/search/cs?searchtype=author&query=Najman%2C+L">Laurent Najman</a> (LIGM)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">A persistent challenge in the field of Intelligent Transportation Systems is
to extract accurate traffic insights from geographic regions with scarce or no
data coverage. To this end, we propose solutions for speed prediction using
sparse GPS data points and their associated topographical and road design
features. Our goal is to investigate whether we can use similarities in the
terrain and infrastructure to train a machine learning model that can predict
speed in regions where we lack transportation data. For this we create a
Temporally Orientated Speed Dictionary Centered on Topographically Clustered
Roads, which helps us to provide speed correlations to selected feature
configurations. Our results show qualitative and quantitative improvement over
new and standard regression methods. The presented framework provides a fresh
perspective on devising strategies for missing data traffic analysis.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07510" title="Abstract">arXiv:2402.07510</a> [<a href="/pdf/2402.07510" title="Download PDF">pdf</a>, <a href="/format/2402.07510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Secret Collusion Among Generative AI Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Motwani%2C+S+R">Sumeet Ramesh Motwani</a>, 
<a href="/search/cs?searchtype=author&query=Baranchuk%2C+M">Mikhail Baranchuk</a>, 
<a href="/search/cs?searchtype=author&query=Strohmeier%2C+M">Martin Strohmeier</a>, 
<a href="/search/cs?searchtype=author&query=Bolina%2C+V">Vijay Bolina</a>, 
<a href="/search/cs?searchtype=author&query=Torr%2C+P+H+S">Philip H.S. Torr</a>, 
<a href="/search/cs?searchtype=author&query=Hammond%2C+L">Lewis Hammond</a>, 
<a href="/search/cs?searchtype=author&query=de+Witt%2C+C+S">Christian Schroeder de Witt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Recent capability increases in large language models (LLMs) open up
applications in which teams of communicating generative AI agents solve joint
tasks. This poses privacy and security challenges concerning the unauthorised
sharing of information, or other unwanted forms of agent coordination. Modern
steganographic techniques could render such dynamics hard to detect. In this
paper, we comprehensively formalise the problem of secret collusion in systems
of generative AI agents by drawing on relevant concepts from both the AI and
security literature. We study incentives for the use of steganography, and
propose a variety of mitigation measures. Our investigations result in a model
evaluation framework that systematically tests capabilities required for
various forms of secret collusion. We provide extensive empirical results
across a range of contemporary LLMs. While the steganographic capabilities of
current models remain limited, GPT-4 displays a capability jump suggesting the
need for continuous monitoring of steganographic frontier model capabilities.
We conclude by laying out a comprehensive research program to mitigate future
risks of collusion between generative AI models.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07513" title="Abstract">arXiv:2402.07513</a> [<a href="/pdf/2402.07513" title="Download PDF">pdf</a>, <a href="/format/2402.07513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Balancing Act: Unmasking and Alleviating ASR Biases in Portuguese
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+A">Ajinkya Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Tokareva%2C+A">Anna Tokareva</a>, 
<a href="/search/cs?searchtype=author&query=Qureshi%2C+R">Rameez Qureshi</a>, 
<a href="/search/cs?searchtype=author&query=Couceiro%2C+M">Miguel Couceiro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EACL-2024 LT-EDI Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">In the field of spoken language understanding, systems like Whisper and
Multilingual Massive Speech (MMS) have shown state-of-the-art performances.
This study is dedicated to a comprehensive exploration of the Whisper and MMS
systems, with a focus on assessing biases in automatic speech recognition (ASR)
inherent to casual conversation speech specific to the Portuguese language. Our
investigation encompasses various categories, including gender, age, skin tone
color, and geo-location. Alongside traditional ASR evaluation metrics such as
Word Error Rate (WER), we have incorporated p-value statistical significance
for gender bias analysis. Furthermore, we extensively examine the impact of
data distribution and empirically show that oversampling techniques alleviate
such stereotypical biases. This research represents a pioneering effort in
quantifying biases in the Portuguese language context through the application
of MMS and Whisper, contributing to a better understanding of ASR systems'
performance in multilingual settings.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07514" title="Abstract">arXiv:2402.07514</a> [<a href="/pdf/2402.07514" title="Download PDF">pdf</a>, <a href="/format/2402.07514" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-informed machine learning as a kernel method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Doum%C3%A8che%2C+N">Nathan Doum&#xe8;che</a> (LPSM, EDF R&amp;D OSIRIS), 
<a href="/search/cs?searchtype=author&query=Bach%2C+F">Francis Bach</a> (DI-ENS), 
<a href="/search/cs?searchtype=author&query=Boyer%2C+C">Claire Boyer</a> (IUF, LPSM), 
<a href="/search/cs?searchtype=author&query=Biau%2C+G">G&#xe9;rard Biau</a> (LPSM)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Statistics Theory (math.ST)

</div>
<p class="mathjax">Physics-informed machine learning combines the expressiveness of data-based
approaches with the interpretability of physical models. In this context, we
consider a general regression problem where the empirical risk is regularized
by a partial differential equation that quantifies the physical inconsistency.
We prove that for linear differential priors, the problem can be formulated as
a kernel regression task. Taking advantage of kernel theory, we derive
convergence rates for the minimizer of the regularized risk and show that it
converges at least at the Sobolev minimax rate. However, faster rates can be
achieved, depending on the physical error. This principle is illustrated with a
one-dimensional example, supporting the claim that regularizing the empirical
risk with physical information can be beneficial to the statistical performance
of estimators.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07518" title="Abstract">arXiv:2402.07518</a> [<a href="/pdf/2402.07518" title="Download PDF">pdf</a>, <a href="/format/2402.07518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resilient Watermarking for LLM-Generated Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Boquan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengdi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peixin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xingmei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to a conference or journal for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">With the development of large language models, multiple AIs are now made
available for code generation (such as ChatGPT and StarCoder) and are adopted
widely. It is often desirable to know whether a piece of code is generated by
AI, and furthermore, which AI is the author. For instance, if a certain version
of AI is known to generate vulnerable code, it is particularly important to
know the creator. Existing approaches are not satisfactory as watermarking
codes are challenging compared with watermarking text data, as codes can be
altered with relative ease via widely-used code refactoring methods. In this
work, we propose ACW (AI Code Watermarking), a novel method for watermarking
AI-generated codes. ACW is efficient as it requires no training or fine-tuning
and works in a black-box manner. It is resilient as the watermark cannot be
easily removed or tampered through common code refactoring methods. The key
idea of ACW is to selectively apply a set of carefully-designed
semantic-preserving, idempotent code transformations, whose presence (or
absence) allows us to determine the existence of the watermark. Our
experimental results show that ACW is effective (i.e., achieving high accuracy,
true positive rates and false positive rates), resilient and efficient,
significantly outperforming existing approaches.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07519" title="Abstract">arXiv:2402.07519</a> [<a href="/pdf/2402.07519" title="Download PDF">pdf</a>, <a href="/format/2402.07519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAFIA: Multi-Adapter Fused Inclusive LanguAge Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jain%2C+P">Prachi Jain</a>, 
<a href="/search/cs?searchtype=author&query=Sathe%2C+A">Ashutosh Sathe</a>, 
<a href="/search/cs?searchtype=author&query=Gumma%2C+V">Varun Gumma</a>, 
<a href="/search/cs?searchtype=author&query=Ahuja%2C+K">Kabir Ahuja</a>, 
<a href="/search/cs?searchtype=author&query=Sitaram%2C+S">Sunayana Sitaram</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Pretrained Language Models (PLMs) are widely used in NLP for various tasks.
Recent studies have identified various biases that such models exhibit and have
proposed methods to correct these biases. However, most of the works address a
limited set of bias dimensions independently such as gender, race, or religion.
Moreover, the methods typically involve finetuning the full model to maintain
the performance on the downstream task. In this work, we aim to modularly
debias a pretrained language model across multiple dimensions. Previous works
extensively explored debiasing PLMs using limited US-centric counterfactual
data augmentation (CDA). We use structured knowledge and a large generative
model to build a diverse CDA across multiple bias dimensions in a
semi-automated way. We highlight how existing debiasing methods do not consider
interactions between multiple societal biases and propose a debiasing model
that exploits the synergy amongst various societal biases and enables
multi-bias debiasing simultaneously. An extensive evaluation on multiple tasks
and languages demonstrates the efficacy of our approach.
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07520" title="Abstract">arXiv:2402.07520</a> [<a href="/pdf/2402.07520" title="Download PDF">pdf</a>, <a href="/ps/2402.07520" title="Download PostScript">ps</a>, <a href="/format/2402.07520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fortran... ok, and what&#x27;s next?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Magnin%2C+V">Vincent Magnin</a> (OPTO - IEMN, IEMN), 
<a href="/search/cs?searchtype=author&query=Alves%2C+J">Jos&#xe9; Alves</a>, 
<a href="/search/cs?searchtype=author&query=Arnoud%2C+A">Antoine Arnoud</a>, 
<a href="/search/cs?searchtype=author&query=Markus%2C+A">Arjen Markus</a>, 
<a href="/search/cs?searchtype=author&query=Marzino%2C+M+E">Michele Esposito Marzino</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This article is the official English translation of the French article published in Bulletin de la Soci{\'e}t{\'e} Informatique de France (November 2023)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Bulletin de la Soci{\'e}t{\'e} Informatique de France, 2023, 22,
  pp.143-161
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">Modern Fortran is a standardized language that includes object-oriented and
parallel programming paradigms. The Fortran-lang community, created at the end
of 2019, is actively working to modernize its ecosystem. New compilers are
under development. And the fourth Fortran standard of the 21st century is due
to be published in autumn 2023.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07523" title="Abstract">arXiv:2402.07523</a> [<a href="/pdf/2402.07523" title="Download PDF">pdf</a>, <a href="/format/2402.07523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Ensemble Inference to Improve Recall of Clone Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+G+A">Gul Aftab Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Patten%2C+J+V">James Vincent Patten</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yuanhua Han</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+G">Guoxian Lu</a>, 
<a href="/search/cs?searchtype=author&query=Gregg%2C+D">David Gregg</a>, 
<a href="/search/cs?searchtype=author&query=Buckley%2C+J">Jim Buckley</a>, 
<a href="/search/cs?searchtype=author&query=Chochlov%2C+M">Muslim Chochlov</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE 17th International Workshop on Software Clones (IWSC)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Large-scale source-code clone detection is a challenging task. In our
previous work, we proposed an approach (SSCD) that leverages artificial neural
networks and approximates nearest neighbour search to effectively and
efficiently locate clones in large-scale bodies of code, in a time-efficient
manner. However, our literature review suggests that the relative efficacy of
differing neural network models has not been assessed in the context of
large-scale clone detection approaches. In this work, we aim to assess several
such models individually, in terms of their potential to maximize recall, while
preserving a high level of precision during clone detection. We investigate if
ensemble inference (in this case, using the results of more than one of these
neural network models in combination) can further assist in this task.
<br />To assess this, we employed four state-of-the-art neural network models and
evaluated them individually/in combination. The results, on an illustrative
dataset of approximately 500K lines of C/C++ code, suggest that ensemble
inference outperforms individual models in all trialled cases, when recall is
concerned. Of individual models, the ADA model (belonging to the ChatGPT family
of models) has the best performance. However commercial companies may not be
prepared to hand their proprietary source code over to the cloud, as required
by that approach. Consequently, they may be more interested in an
ensemble-combination of CodeBERT-based and CodeT5 models, resulting in similar
(if slightly lesser) recall and precision results.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07526" title="Abstract">arXiv:2402.07526</a> [<a href="/pdf/2402.07526" title="Download PDF">pdf</a>, <a href="/format/2402.07526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Morse sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bertrand%2C+G">Gilles Bertrand</a> (LIGM)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Conference on Discrete Geometry and Mathematical
  Morphology (DGMM), Apr 2024, Florence, Italy
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We introduce the notion of a Morse sequence, which provides a simple and
effective approach to discrete Morse theory. A Morse sequence is a sequence
composed solely of two elementary operations, that is, expansions (the inverse
of a collapse), and fillings (the inverse of a perforation). We show that a
Morse sequence may be seen as an alternative way to represent the gradient
vector field of an arbitrary discrete Morse function. We also show that it is
possible, in a straightforward manner, to make a link between Morse sequences
and different kinds of Morse functions. At last, we introduce maximal Morse
sequences, which formalize two basic schemes for building a Morse sequence from
an arbitrary simplicial complex.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07528" title="Abstract">arXiv:2402.07528</a> [<a href="/pdf/2402.07528" title="Download PDF">pdf</a>, <a href="/format/2402.07528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Battery-Less LoRaWAN Communications using Energy Harvesting: Modeling  and Characterization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Delgado%2C+C">Carmen Delgado</a>, 
<a href="/search/cs?searchtype=author&query=Sanz%2C+J+M">Jos&#xe9; Mar&#xed;a Sanz</a>, 
<a href="/search/cs?searchtype=author&query=Blondia%2C+C">Chris Blondia</a>, 
<a href="/search/cs?searchtype=author&query=Famaey%2C+J">Jeroen Famaey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2402.06290">arXiv:2402.06290</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Internet of Things Journal 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Billions of IoT devices are deployed worldwide and batteries are their main
power source. However, these batteries are bulky, short-lived and full of
hazardous chemicals that damage our environment. Relying on batteries is not a
sustainable solution for the future IoT. As an alternative, battery-less
devices run on long-lived capacitors charged using energy harvesters. The small
energy storage capacity of capacitors results in an intermittent on-off
behaviour. LoRaWAN is a popular Low Power Wide Area Network technology used in
many IoT devices and can be used in these new scenarios. In this work, we
present a Markov model to characterize the performance of battery-less LoRaWAN
devices for uplink and downlink transmissions and we evaluate their performance
in terms of the parameters that define the model (i.e., device configuration,
application behaviour and environmental conditions). Results show that LoRaWAN
battery-less communications are feasible if choosing the proper configuration
(i.e., capacitor size, turn-on voltage threshold) for different application
behaviour (i.e., transmission interval, UL/DL packet sizes) and environmental
conditions (i.e., energy harvesting rate). Since downlink in the second
reception window highly affects the performance, only small DL packet sizes
should be considered for these devices. Besides, a 47 mF capacitor can support
1 Byte $SF7$ transmissions every 60 s at an energy harvesting rate of 1 mW.
However, if no DL is expected, a 4.7 mF capacitor could support 1 Byte $SF7$
transmissions every 9~s.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07529" title="Abstract">arXiv:2402.07529</a> [<a href="/pdf/2402.07529" title="Download PDF">pdf</a>, <a href="/format/2402.07529" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Distributed Deep Learning using Lossless Homomorphic  Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuchen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiayi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dwivedula%2C+R">Rohit Dwivedula</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wenfei Wu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+K">Keqiang He</a>, 
<a href="/search/cs?searchtype=author&query=Akella%2C+A">Aditya Akella</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Daehyeok Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">As deep neural networks (DNNs) grow in complexity and size, the resultant
increase in communication overhead during distributed training has become a
significant bottleneck, challenging the scalability of distributed training
systems. Existing solutions, while aiming to mitigate this bottleneck through
worker-level compression and in-network aggregation, fall short due to their
inability to efficiently reconcile the trade-offs between compression
effectiveness and computational overhead, hindering overall performance and
scalability. In this paper, we introduce a novel compression algorithm that
effectively merges worker-level compression with in-network aggregation. Our
solution is both homomorphic, allowing for efficient in-network aggregation
without CPU/GPU processing, and lossless, ensuring no compromise on training
accuracy. Theoretically optimal in compression and computational efficiency,
our approach is empirically validated across diverse DNN models such as NCF,
LSTM, VGG19, and BERT-base, showing up to a 6.33$\times$ improvement in
aggregation throughput and a 3.74$\times$ increase in per-iteration training
speed.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07530" title="Abstract">arXiv:2402.07530</a> [<a href="/pdf/2402.07530" title="Download PDF">pdf</a>, <a href="/ps/2402.07530" title="Download PostScript">ps</a>, <a href="/format/2402.07530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reproducibility, Replicability, and Repeatability: A survey of  reproducible research with a focus on high performance computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Antunes%2C+B+A">Benjamin A. Antunes</a> (LIMOS), 
<a href="/search/cs?searchtype=author&query=Hill%2C+D+R+C">David R.C. Hill</a> (ISIMA, LIMOS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Reproducibility is widely acknowledged as a fundamental principle in
scientific research. Currently, the scientific community grapples with numerous
challenges associated with reproducibility, often referred to as the
''reproducibility crisis.'' This crisis permeated numerous scientific
disciplines. In this study, we examined the factors in scientific practices
that might contribute to this lack of reproducibility. Significant focus is
placed on the prevalent integration of computation in research, which can
sometimes function as a black box in published papers. Our study primarily
focuses on highperformance computing (HPC), which presents unique
reproducibility challenges. This paper provides a comprehensive review of these
concerns and potential solutions. Furthermore, we discuss the critical role of
reproducible research in advancing science and identifying persisting issues
within the field of HPC.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07531" title="Abstract">arXiv:2402.07531</a> [<a href="/pdf/2402.07531" title="Download PDF">pdf</a>, <a href="/ps/2402.07531" title="Download PostScript">ps</a>, <a href="/format/2402.07531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Data for Humanities and Social Sciences (SDHSS): an Ecosystem  of CIDOC CRM Extensions for Research Data Production and Reuse
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beretta%2C+F">Francesco Beretta</a> (LARHRA, LARHRA PHN)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Please refer to the publisher version for quotation
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Thomas Riechert; Hartmut Beyer; Jennifer Blanke; Edgard Marx.
  Professorale Karrieremuster Reloaded. Entwicklung einer wissenschaftlichen
  Methode zur Forschung auf online verf{\"u}gbaren und verteilten
  Forschungsdatenbanken der Universit{\"a}tsgeschichte., HTWK Leipzig /
  OA-HVerlag, pp.73-102, 2024, 978-3-96627-050-2
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Given the challenge of giant knowledge graphs created by major eco-nomic
actors, which could virtually replace research in the Humani-ties and Social
Sciences (HSS) in responding to public concerns, thequestion arises of how to
increase the value of research data throughtheir publication and networking,
applying the FAIR principles. Bothan epistemological and a semantic analysis
show that the most rel-evant part of research data is factual information,
understood as arepresentation of the objects observed by the scientific
disciplines,their properties and their relationships.This rich universe of
information will be made understandable andtherefore reusable through the
application of foundational ontologiesand a methodology based on the
distinction between different levelsof abstraction, allowing the collective
development of one or moreshared and reusable domain ontologies. This vision is
being carriedout around the CIDOC CRM, as core ontology, and Semantic Datafor
Humanities and Social Sciences (SDHSS), as a high-level exten-sion of it, as
well as an ecosystem of sub-domain extensions that canbe easily managed through
the ontome.net application. This willresult in an interoperability that is
semantically richer than the sim-ple alignment of ontologies and less costly in
terms of resources, andabove all adapted to the scientific and humanistic
project of the HSS.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07536" title="Abstract">arXiv:2402.07536</a> [<a href="/pdf/2402.07536" title="Download PDF">pdf</a>, <a href="/format/2402.07536" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BreakGPT: A Large Language Model with Multi-stage Structure for  Financial Breakout Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yoshie%2C+O">Osamu Yoshie</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weiran Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Trading range breakout (TRB) is a key method in the technical analysis of
financial trading, widely employed by traders in financial markets such as
stocks, futures, and foreign exchange. However, distinguishing between true and
false breakout and providing the correct rationale cause significant challenges
to investors. Recently, large language models have achieved success in various
downstream applications, but their effectiveness in the domain of financial
breakout detection has been subpar. The reason is that the unique data and
specific knowledge are required in breakout detection. To address these issues,
we introduce BreakGPT, the first large language model for financial breakout
detection. Furthermore, we have developed a novel framework for large language
models, namely multi-stage structure, effectively reducing mistakes in
downstream applications. Experimental results indicate that compared to
GPT-3.5, BreakGPT improves the accuracy of answers and rational by 44%, with
the multi-stage structure contributing 17.6% to the improvement. Additionally,
it outperforms ChatGPT-4 by 42.07%. Our Code is publicly available:
https://github.com/Neviim96/BreakGPT
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07537" title="Abstract">arXiv:2402.07537</a> [<a href="/pdf/2402.07537" title="Download PDF">pdf</a>, <a href="/format/2402.07537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UAV-assisted Visual SLAM Generating Reconstructed 3D Scene Graphs in  GPS-denied Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Radwan%2C+A">Ahmed Radwan</a>, 
<a href="/search/cs?searchtype=author&query=Tourani%2C+A">Ali Tourani</a>, 
<a href="/search/cs?searchtype=author&query=Bavle%2C+H">Hriday Bavle</a>, 
<a href="/search/cs?searchtype=author&query=Voos%2C+H">Holger Voos</a>, 
<a href="/search/cs?searchtype=author&query=Sanchez-Lopez%2C+J+L">Jose Luis Sanchez-Lopez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 7 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Aerial robots play a vital role in various applications where the situational
awareness of the robots concerning the environment is a fundamental demand. As
one such use case, drones in GPS-denied environments require equipping with
different sensors (e.g., vision sensors) that provide reliable sensing results
while performing pose estimation and localization. In this paper,
reconstructing the maps of indoor environments alongside generating 3D scene
graphs for a high-level representation using a camera mounted on a drone is
targeted. Accordingly, an aerial robot equipped with a companion computer and
an RGB-D camera was built and employed to be appropriately integrated with a
Visual Simultaneous Localization and Mapping (VSLAM) framework proposed by the
authors. To enhance the situational awareness of the robot while reconstructing
maps, various structural elements, including doors and walls, were labeled with
printed fiducial markers, and a dictionary of the topological relations among
them was fed to the system. The VSLAM system detects markers and reconstructs
the map of the indoor areas enriched with higher-level semantic entities,
including corridors and rooms. Another achievement is generating multi-layered
vision-based situational graphs containing enhanced hierarchical
representations of the indoor environment. In this regard, integrating VSLAM
into the employed drone is the primary target of this paper to provide an
end-to-end robot application for GPS-denied environments. To show the
practicality of the system, various real-world condition experiments have been
conducted in indoor scenarios with dissimilar structural layouts. Evaluations
show the proposed drone application can perform adequately w.r.t. the
ground-truth data and its baseline.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07540" title="Abstract">arXiv:2402.07540</a> [<a href="/pdf/2402.07540" title="Download PDF">pdf</a>, <a href="/format/2402.07540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PKG API: A Tool for Personal Knowledge Graph Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bernard%2C+N">Nolwenn Bernard</a>, 
<a href="/search/cs?searchtype=author&query=Kostric%2C+I">Ivica Kostric</a>, 
<a href="/search/cs?searchtype=author&query=%C5%81ajewska%2C+W">Weronika &#x141;ajewska</a>, 
<a href="/search/cs?searchtype=author&query=Balog%2C+K">Krisztian Balog</a>, 
<a href="/search/cs?searchtype=author&query=Galu%C5%A1%C4%8D%C3%A1kov%C3%A1%2C+P">Petra Galu&#x161;&#x10d;&#xe1;kov&#xe1;</a>, 
<a href="/search/cs?searchtype=author&query=Setty%2C+V">Vinay Setty</a>, 
<a href="/search/cs?searchtype=author&query=Skj%C3%A6veland%2C+M+G">Martin G. Skj&#xe6;veland</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Personal knowledge graphs (PKGs) offer individuals a way to store and
consolidate their fragmented personal data in a central place, improving
service personalization while maintaining full user control. Despite their
potential, practical PKG implementations with user-friendly interfaces remain
scarce. This work addresses this gap by proposing a complete solution to
represent, manage, and interface with PKGs. Our approach includes (1) a
user-facing PKG Client, enabling end-users to administer their personal data
easily via natural language statements, and (2) a service-oriented PKG API. To
tackle the complexity of representing these statements within a PKG, we present
an RDF-based PKG vocabulary that supports this, along with properties for
access rights and provenance.
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07542" title="Abstract">arXiv:2402.07542</a> [<a href="/pdf/2402.07542" title="Download PDF">pdf</a>, <a href="/format/2402.07542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ASAP-Repair: API-Specific Automated Program Repair Based on API Usage  Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nielebock%2C+S">Sebastian Nielebock</a>, 
<a href="/search/cs?searchtype=author&query=Blockhaus%2C+P">Paul Blockhaus</a>, 
<a href="/search/cs?searchtype=author&query=Kr%C3%BCger%2C+J">Jacob Kr&#xfc;ger</a>, 
<a href="/search/cs?searchtype=author&query=Ortmeier%2C+F">Frank Ortmeier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for presentation at the 5th ACM/IEEE International Workshop on Automated Program Repair (APR24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Modern software development relies on the reuse of code via Application
Programming Interfaces (APIs). Such reuse relieves developers from learning and
developing established algorithms and data structures anew, enabling them to
focus on their problem at hand. However, there is also the risk of misusing an
API due to a lack of understanding or proper documentation. While many
techniques target API misuse detection, only limited efforts have been put into
automatically repairing API misuses. In this paper, we present our advances on
our technique API-Specific Automated Program Repair (ASAP-Repair). ASAP-Repair
is intended to fix API misuses based on API Usage Graphs (AUGs) by leveraging
API usage templates of state-of-the-art API misuse detectors. We demonstrate
that ASAP-Repair is in principle applicable on an established API misuse
dataset. Moreover, we discuss next steps and challenges to evolve ASAP-Repair
towards a full-fledged Automatic Program Repair (APR) technique.
</p>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07543" title="Abstract">arXiv:2402.07543</a> [<a href="/pdf/2402.07543" title="Download PDF">pdf</a>, <a href="/format/2402.07543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Show Me How It&#x27;s Done: The Role of Explanations in Fine-Tuning Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ballout%2C+M">Mohamad Ballout</a>, 
<a href="/search/cs?searchtype=author&query=Krumnack%2C+U">Ulf Krumnack</a>, 
<a href="/search/cs?searchtype=author&query=Heidemann%2C+G">Gunther Heidemann</a>, 
<a href="/search/cs?searchtype=author&query=Kuehnberger%2C+K">Kai-Uwe Kuehnberger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Our research demonstrates the significant benefits of using fine-tuning with
explanations to enhance the performance of language models. Unlike prompting,
which maintains the model's parameters, fine-tuning allows the model to learn
and update its parameters during a training phase. In this study, we applied
fine-tuning to various sized language models using data that contained
explanations of the output rather than merely presenting the answers. We found
that even smaller language models with as few as 60 million parameters
benefited substantially from this approach. Interestingly, our results
indicated that the detailed explanations were more beneficial to smaller models
than larger ones, with the latter gaining nearly the same advantage from any
form of explanation, irrespective of its length. Additionally, we demonstrate
that the inclusion of explanations enables the models to solve tasks that they
were not able to solve without explanations. Lastly, we argue that despite the
challenging nature of adding explanations, samples that contain explanations
not only reduce the volume of data required for training but also promote a
more effective generalization by the model. In essence, our findings suggest
that fine-tuning with explanations significantly bolsters the performance of
large language models.
</p>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07545" title="Abstract">arXiv:2402.07545</a> [<a href="/pdf/2402.07545" title="Download PDF">pdf</a>, <a href="/format/2402.07545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TransAxx: Efficient Transformers with Approximate Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Danopoulos%2C+D">Dimitrios Danopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Zervakis%2C+G">Georgios Zervakis</a>, 
<a href="/search/cs?searchtype=author&query=Soudris%2C+D">Dimitrios Soudris</a>, 
<a href="/search/cs?searchtype=author&query=Henkel%2C+J">J&#xf6;rg Henkel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">Vision Transformer (ViT) models which were recently introduced by the
transformer architecture have shown to be very competitive and often become a
popular alternative to Convolutional Neural Networks (CNNs). However, the high
computational requirements of these models limit their practical applicability
especially on low-power devices. Current state-of-the-art employs approximate
multipliers to address the highly increased compute demands of DNN accelerators
but no prior research has explored their use on ViT models. In this work we
propose TransAxx, a framework based on the popular PyTorch library that enables
fast inherent support for approximate arithmetic to seamlessly evaluate the
impact of approximate computing on DNNs such as ViT models. Using TransAxx we
analyze the sensitivity of transformer models on the ImageNet dataset to
approximate multiplications and perform approximate-aware finetuning to regain
accuracy. Furthermore, we propose a methodology to generate approximate
accelerators for ViT models. Our approach uses a Monte Carlo Tree Search (MCTS)
algorithm to efficiently search the space of possible configurations using a
hardware-driven hand-crafted policy. Our evaluation demonstrates the efficacy
of our methodology in achieving significant trade-offs between accuracy and
power, resulting in substantial gains without compromising on performance.
</p>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07547" title="Abstract">arXiv:2402.07547</a> [<a href="/pdf/2402.07547" title="Download PDF">pdf</a>, <a href="/format/2402.07547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ensuring trustworthy and ethical behaviour in intelligent logical agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Costantini%2C+S">Stefania Costantini</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Logic and Computation, Volume 32, Issue 2, March 2022,
  Pages 443-478
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI); Logic in Computer Science (cs.LO); Symbolic Computation (cs.SC)

</div>
<p class="mathjax">Autonomous Intelligent Agents are employed in many applications upon which
the life and welfare of living beings and vital social functions may depend.
Therefore, agents should be trustworthy. A priori certification techniques
(i.e., techniques applied prior to system's deployment) can be useful, but are
not sufficient for agents that evolve, and thus modify their epistemic and
belief state, and for open Multi-Agent Systems, where heterogeneous agents can
join or leave the system at any stage of its operation. In this paper, we
propose/refine/extend dynamic (runtime) logic-based self-checking techniques,
devised in order to be able to ensure agents' trustworthy and ethical
behaviour.
</p>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07549" title="Abstract">arXiv:2402.07549</a> [<a href="/pdf/2402.07549" title="Download PDF">pdf</a>, <a href="/format/2402.07549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Precision-Optimized Fixed-Point Near-Memory Digital Processing Unit  for Analog In-Memory Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferro%2C+E">Elena Ferro</a>, 
<a href="/search/cs?searchtype=author&query=Vasilopoulos%2C+A">Athanasios Vasilopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Lammie%2C+C">Corey Lammie</a>, 
<a href="/search/cs?searchtype=author&query=Gallo%2C+M+L">Manuel Le Gallo</a>, 
<a href="/search/cs?searchtype=author&query=Benini%2C+L">Luca Benini</a>, 
<a href="/search/cs?searchtype=author&query=Boybat%2C+I">Irem Boybat</a>, 
<a href="/search/cs?searchtype=author&query=Sebastian%2C+A">Abu Sebastian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ISCAS2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Emerging Technologies (cs.ET); Machine Learning (cs.LG)

</div>
<p class="mathjax">Analog In-Memory Computing (AIMC) is an emerging technology for fast and
energy-efficient Deep Learning (DL) inference. However, a certain amount of
digital post-processing is required to deal with circuit mismatches and
non-idealities associated with the memory devices. Efficient near-memory
digital logic is critical to retain the high area/energy efficiency and low
latency of AIMC. Existing systems adopt Floating Point 16 (FP16) arithmetic
with limited parallelization capability and high latency. To overcome these
limitations, we propose a Near-Memory digital Processing Unit (NMPU) based on
fixed-point arithmetic. It achieves competitive accuracy and higher computing
throughput than previous approaches while minimizing the area overhead.
Moreover, the NMPU supports standard DL activation steps, such as ReLU and
Batch Normalization. We perform a physical implementation of the NMPU design in
a 14 nm CMOS technology and provide detailed performance, power, and area
assessments. We validate the efficacy of the NMPU by using data from an AIMC
chip and demonstrate that a simulated AIMC system with the proposed NMPU
outperforms existing FP16-based implementations, providing 139$\times$
speed-up, 7.8$\times$ smaller area, and a competitive power consumption.
Additionally, our approach achieves an inference accuracy of 86.65 %/65.06 %,
with an accuracy drop of just 0.12 %/0.4 % compared to the FP16 baseline when
benchmarked with ResNet9/ResNet32 networks trained on the CIFAR10/CIFAR100
datasets, respectively.
</p>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07556" title="Abstract">arXiv:2402.07556</a> [<a href="/pdf/2402.07556" title="Download PDF">pdf</a>, <a href="/format/2402.07556" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digital Twins Below the Surface: Enhancing Underwater Teleoperation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adetunji%2C+F+O">Favour O. Adetunji</a>, 
<a href="/search/cs?searchtype=author&query=Ellis%2C+N">Niamh Ellis</a>, 
<a href="/search/cs?searchtype=author&query=Koskinopoulou%2C+M">Maria Koskinopoulou</a>, 
<a href="/search/cs?searchtype=author&query=Carlucho%2C+I">Ignacio Carlucho</a>, 
<a href="/search/cs?searchtype=author&query=Petillot%2C+Y+R">Yvan R. Petillot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 8 figures, to be published in OCEANS 2024 Singapore Conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Subsea exploration, inspection, and intervention operations heavily rely on
remotely operated vehicles (ROVs). However, the inherent complexity of the
underwater environment presents significant challenges to the operators of
these vehicles. This paper delves into the challenges associated with
navigation and maneuvering tasks in the teleoperation of ROVs, such as reduced
situational awareness and heightened teleoperator workload. To address these
challenges, we introduce an underwater Digital Twin (DT) system designed to
enhance underwater teleoperation, enable autonomous navigation, support system
monitoring, and facilitate system testing through simulation. Our approach
involves a dynamic representation of the underwater robot and its environment
using desktop virtual reality, as well as the integration of mapping,
localization, path planning and simulation capabilities within the DT system.
Our research demonstrates the system's adaptability, versatility and
feasibility, highlighting significant challenges and, in turn, improving the
teleoperators' situational awareness and reducing their workload.
</p>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07562" title="Abstract">arXiv:2402.07562</a> [<a href="/pdf/2402.07562" title="Download PDF">pdf</a>, <a href="/format/2402.07562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering Universal Semantic Triggers for Text-to-Image Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhai%2C+S">Shengfang Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weilong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiajun Li</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yinpeng Dong</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hang Su</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Q">Qingni Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures. Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Recently text-to-image models have gained widespread attention in the
community due to their controllable and high-quality generation ability.
However, the robustness of such models and their potential ethical issues have
not been fully explored. In this paper, we introduce Universal Semantic
Trigger, a meaningless token sequence that can be added at any location within
the input text yet can induce generated images towards a preset semantic
target.To thoroughly investigate it, we propose Semantic Gradient-based Search
(SGS) framework. SGS automatically discovers the potential universal semantic
triggers based on the given semantic targets. Furthermore, we design evaluation
metrics to comprehensively evaluate semantic shift of images caused by these
triggers. And our empirical analyses reveal that the mainstream open-source
text-to-image models are vulnerable to our triggers, which could pose
significant ethical threats. Our work contributes to a further understanding of
text-to-image synthesis and helps users to automatically auditing their models
before deployment.
</p>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07563" title="Abstract">arXiv:2402.07563</a> [<a href="/pdf/2402.07563" title="Download PDF">pdf</a>, <a href="/format/2402.07563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint User and Beam Selection in Millimeter Wave Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Singh%2C+S+K">Santosh Kumar Singh</a>, 
<a href="/search/eess?searchtype=author&query=Sahu%2C+S">Satyabrata Sahu</a>, 
<a href="/search/eess?searchtype=author&query=Thawait%2C+A">Ayushi Thawait</a>, 
<a href="/search/eess?searchtype=author&query=Chaporkar%2C+P">Prasanna Chaporkar</a>, 
<a href="/search/eess?searchtype=author&query=Kasbekar%2C+G+S">Gaurav S. Kasbekar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">We study the problem of selecting a user equipment (UE) and a beam for each
access point (AP) for concurrent transmissions in a millimeter wave (mmWave)
network, such that the sum of weighted rates of UEs is maximized. We prove that
this problem is NP-complete. We propose two algorithms -- Markov Chain Monte
Carlo (MCMC) based and local interaction game (LIG) based UE and beam selection
-- and prove that both of them asymptotically achieve the optimal solution.
Also, we propose two fast greedy algorithms -- NGUB1 and NGUB2 -- for UE and
beam selection. Through extensive simulations, we show that our proposed greedy
algorithms outperform the most relevant algorithms proposed in prior work and
perform close to the asymptotically optimal algorithms.
</p>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07568" title="Abstract">arXiv:2402.07568</a> [<a href="/pdf/2402.07568" title="Download PDF">pdf</a>, <a href="/format/2402.07568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weisfeiler-Leman at the margin: When more expressivity matters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Franks%2C+B+J">Billy J. Franks</a>, 
<a href="/search/cs?searchtype=author&query=Morris%2C+C">Christopher Morris</a>, 
<a href="/search/cs?searchtype=author&query=Velingker%2C+A">Ameya Velingker</a>, 
<a href="/search/cs?searchtype=author&query=Geerts%2C+F">Floris Geerts</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2301.11039">arXiv:2301.11039</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Discrete Mathematics (cs.DM); Neural and Evolutionary Computing (cs.NE); Machine Learning (stat.ML)

</div>
<p class="mathjax">The Weisfeiler-Leman algorithm ($1$-WL) is a well-studied heuristic for the
graph isomorphism problem. Recently, the algorithm has played a prominent role
in understanding the expressive power of message-passing graph neural networks
(MPNNs) and being effective as a graph kernel. Despite its success, $1$-WL
faces challenges in distinguishing non-isomorphic graphs, leading to the
development of more expressive MPNN and kernel architectures. However, the
relationship between enhanced expressivity and improved generalization
performance remains unclear. Here, we show that an architecture's expressivity
offers limited insights into its generalization performance when viewed through
graph isomorphism. Moreover, we focus on augmenting $1$-WL and MPNNs with
subgraph information and employ classical margin theory to investigate the
conditions under which an architecture's increased expressivity aligns with
improved generalization performance. In addition, we show that gradient flow
pushes the MPNN's weights toward the maximum margin solution. Further, we
introduce variations of expressive $1$-WL-based kernel and MPNN architectures
with provable generalization properties. Our empirical study confirms the
validity of our theoretical findings.
</p>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07570" title="Abstract">arXiv:2402.07570</a> [<a href="/pdf/2402.07570" title="Download PDF">pdf</a>, <a href="/format/2402.07570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Only the Curve Shape Matters: Training Foundation Models for Zero-Shot  Multivariate Time Series Forecasting through Next Curve Shape Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Cheng Feng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Long Huang</a>, 
<a href="/search/cs?searchtype=author&query=Krompass%2C+D">Denis Krompass</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We present General Time Transformer (GTT), an encoder-only style foundation
model for zero-shot multivariate time series forecasting. GTT is pretrained on
a large dataset of 200M high-quality time series samples spanning diverse
domains. In our proposed framework, the task of multivariate time series
forecasting is formulated as a channel-wise next curve shape prediction
problem, where each time series sample is represented as a sequence of
non-overlapping curve shapes with a unified numerical magnitude. GTT is trained
to predict the next curve shape based on a window of past curve shapes in a
channel-wise manner. Experimental results demonstrate that GTT exhibits
superior zero-shot multivariate forecasting capabilities on unseen time series
datasets, even surpassing state-of-the-art supervised baselines. Additionally,
we investigate the impact of varying GTT model parameters and training dataset
scales, observing that the scaling law also holds in the context of zero-shot
multivariate time series forecasting.
</p>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07577" title="Abstract">arXiv:2402.07577</a> [<a href="/pdf/2402.07577" title="Download PDF">pdf</a>, <a href="/format/2402.07577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topic Modeling as Multi-Objective Contrastive Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Thong Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiaobao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xinshuai Dong</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+C+T">Cong-Duy T Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+S">See-Kiong Ng</a>, 
<a href="/search/cs?searchtype=author&query=Luu%2C+A+T">Anh Tuan Luu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICLR 2024 (poster)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent representation learning approaches enhance neural topic models by
optimizing the weighted linear combination of the evidence lower bound (ELBO)
of the log-likelihood and the contrastive learning objective that contrasts
pairs of input documents. However, document-level contrastive learning might
capture low-level mutual information, such as word ratio, which disturbs topic
modeling. Moreover, there is a potential conflict between the ELBO loss that
memorizes input details for better reconstruction quality, and the contrastive
loss which attempts to learn topic representations that generalize among input
documents. To address these issues, we first introduce a novel contrastive
learning method oriented towards sets of topic vectors to capture useful
semantics that are shared among a set of input documents. Secondly, we
explicitly cast contrastive topic modeling as a gradient-based multi-objective
optimization problem, with the goal of achieving a Pareto stationary solution
that balances the trade-off between the ELBO and the contrastive objective.
Extensive experiments demonstrate that our framework consistently produces
higher-performing neural topic models in terms of topic coherence, topic
diversity, and downstream performance.
</p>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07578" title="Abstract">arXiv:2402.07578</a> [<a href="/pdf/2402.07578" title="Download PDF">pdf</a>, <a href="/format/2402.07578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LFOC: A Lightweight Fairness-Oriented Cache Clustering Policy for  Commodity Multicores
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Garc%C3%ADa%2C+A">Adri&#xe1;n Garc&#xed;a-Garc&#xed;a</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%A1ez%2C+J+C">Juan Carlos S&#xe1;ez</a>, 
<a href="/search/cs?searchtype=author&query=Castro%2C+F">Fernando Castro</a>, 
<a href="/search/cs?searchtype=author&query=Prieto-Mat%C3%ADas%2C+M">Manuel Prieto-Mat&#xed;as</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 48th International Conference on Parallel Processing (ICPP 2019),
  August 5--8, 2019, Kyoto, Japan
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">Multicore processors constitute the main architecture choice for modern
computing systems in different market segments. Despite their benefits, the
contention that naturally appears when multiple applications compete for the
use of shared resources among cores, such as the last-level cache (LLC), may
lead to substantial performance degradation. This may have a negative impact on
key system aspects such as throughput and fairness. Assigning the various
applications in the workload to separate LLC partitions with possibly different
sizes, has been proven effective to mitigate shared-resource contention
effects.
<br />In this article we propose LFOC, a clustering-based cache partitioning scheme
that strives to deliver fairness while providing acceptable system throughput.
LFOC leverages the Intel Cache Allocation Technology (CAT), which enables the
system software to divide the LLC into different partitions. To accomplish its
goals, LFOC tries to mimic the behavior of the optimal cache-clustering
solution, which we could approximate by means of a simulator in different
scenarios. To this end, LFOC effectively identifies streaming aggressor
programs and cache sensitive applications, which are then assigned to separate
cache partitions.
<br />We implemented LFOC in the Linux kernel and evaluated it on a real system
featuring an Intel Skylake processor, where we compare its effectiveness to
that of two state-of-the-art policies that optimize fairness and throughput,
respectively. Our experimental analysis reveals that LFOC is able to bring a
higher reduction in unfairness by leveraging a lightweight algorithm suitable
for adoption in a real OS.
</p>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07584" title="Abstract">arXiv:2402.07584</a> [<a href="/pdf/2402.07584" title="Download PDF">pdf</a>, <a href="/ps/2402.07584" title="Download PostScript">ps</a>, <a href="/format/2402.07584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-Optimized Randomized Response for Sharing Multi-Attribute Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yamamoto%2C+A">Akito Yamamoto</a>, 
<a href="/search/cs?searchtype=author&query=Shibuya%2C+T">Tetsuo Shibuya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">With the increasing amount of data in society, privacy concerns in data
sharing have become widely recognized. Particularly, protecting personal
attribute information is essential for a wide range of aims from crowdsourcing
to realizing personalized medicine. Although various differentially private
methods based on randomized response have been proposed for single attribute
information or specific analysis purposes such as frequency estimation, there
is a lack of studies on the mechanism for sharing individuals' multiple
categorical information itself. The existing randomized response for sharing
multi-attribute data uses the Kronecker product to perturb each attribute
information in turn according to the respective privacy level but achieves only
a weak privacy level for the entire dataset. Therefore, in this study, we
propose a privacy-optimized randomized response that guarantees the strongest
privacy in sharing multi-attribute data. Furthermore, we present an efficient
heuristic algorithm for constructing a near-optimal mechanism. The time
complexity of our algorithm is O(k^2), where k is the number of attributes, and
it can be performed in about 1 second even for large datasets with k = 1,000.
The experimental results demonstrate that both of our methods provide
significantly stronger privacy guarantees for the entire dataset than the
existing method. In addition, we show an analysis example using genome
statistics to confirm that our methods can achieve less than half the output
error compared with that of the existing method. Overall, this study is an
important step toward trustworthy sharing and analysis of multi-attribute data.
The Python implementation of our experiments and supplemental results are
available at https://github.com/ay0408/Optimized-RR.
</p>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07585" title="Abstract">arXiv:2402.07585</a> [<a href="/pdf/2402.07585" title="Download PDF">pdf</a>, <a href="/format/2402.07585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying architectural design decisions for achieving green ML  serving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dur%C3%A1n%2C+F">Francisco Dur&#xe1;n</a>, 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADnez-Fern%C3%A1ndez%2C+S">Silverio Mart&#xed;nez-Fern&#xe1;ndez</a>, 
<a href="/search/cs?searchtype=author&query=Martinez%2C+M">Matias Martinez</a>, 
<a href="/search/cs?searchtype=author&query=Lago%2C+P">Patricia Lago</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication as short paper in Conference on AI Engineering Software Engineering for AI (CAIN 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">The growing use of large machine learning models highlights concerns about
their increasing computational demands. While the energy consumption of their
training phase has received attention, fewer works have considered the
inference phase. For ML inference, the binding of ML models to the ML system
for user access, known as ML serving, is a critical yet understudied step for
achieving efficiency in ML applications.
<br />We examine the literature in ML architectural design decisions and Green AI,
with a special focus on ML serving. The aim is to analyze ML serving
architectural design decisions for the purpose of understanding and identifying
them with respect to quality characteristics from the point of view of
researchers and practitioners in the context of ML serving literature.
<br />Our results (i) identify ML serving architectural design decisions along with
their corresponding components and associated technological stack, and (ii)
provide an overview of the quality characteristics studied in the literature,
including energy efficiency.
<br />This preliminary study is the first step in our goal to achieve green ML
serving. Our analysis may aid ML researchers and practitioners in making
green-aware architecture design decisions when serving their models.
</p>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07586" title="Abstract">arXiv:2402.07586</a> [<a href="/pdf/2402.07586" title="Download PDF">pdf</a>, <a href="/format/2402.07586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling Group-Specific Distributed Concept Drift: A Fairness  Imperative in Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salazar%2C+T">Teresa Salazar</a>, 
<a href="/search/cs?searchtype=author&query=Gama%2C+J">Jo&#xe3;o Gama</a>, 
<a href="/search/cs?searchtype=author&query=Ara%C3%BAjo%2C+H">Helder Ara&#xfa;jo</a>, 
<a href="/search/cs?searchtype=author&query=Abreu%2C+P+H">Pedro Henriques Abreu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In the evolving field of machine learning, ensuring fairness has become a
critical concern, prompting the development of algorithms designed to mitigate
discriminatory outcomes in decision-making processes. However, achieving
fairness in the presence of group-specific concept drift remains an unexplored
frontier, and our research represents pioneering efforts in this regard.
Group-specific concept drift refers to situations where one group experiences
concept drift over time while another does not, leading to a decrease in
fairness even if accuracy remains fairly stable. Within the framework of
federated learning, where clients collaboratively train models, its distributed
nature further amplifies these challenges since each client can experience
group-specific concept drift independently while still sharing the same
underlying concept, creating a complex and dynamic environment for maintaining
fairness. One of the significant contributions of our research is the
formalization and introduction of the problem of group-specific concept drift
and its distributed counterpart, shedding light on its critical importance in
the realm of fairness. In addition, leveraging insights from prior research, we
adapt an existing distributed concept drift adaptation algorithm to tackle
group-specific distributed concept drift which utilizes a multi-model approach,
a local group-specific drift detection mechanism, and continuous clustering of
models over time. The findings from our experiments highlight the importance of
addressing group-specific concept drift and its distributed counterpart to
advance fairness in machine learning.
</p>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07588" title="Abstract">arXiv:2402.07588</a> [<a href="/pdf/2402.07588" title="Download PDF">pdf</a>, <a href="/format/2402.07588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Scaling Laws for Learning in Strategic Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Handina%2C+T">Tinashe Handina</a>, 
<a href="/search/cs?searchtype=author&query=Mazumdar%2C+E">Eric Mazumdar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">The deployment of ever-larger machine learning models reflects a growing
consensus that the more expressive the model$\unicode{x2013}$and the more data
one has access to$\unicode{x2013}$the more one can improve performance. As
models get deployed in a variety of real world scenarios, they inevitably face
strategic environments. In this work, we consider the natural question of how
the interplay of models and strategic interactions affects scaling laws. We
find that strategic interactions can break the conventional view of scaling
laws$\unicode{x2013}$meaning that performance does not necessarily
monotonically improve as models get larger and/ or more expressive (even with
infinite data). We show the implications of this phenomenon in several contexts
including strategic regression, strategic classification, and multi-agent
reinforcement learning through examples of strategic environments in
which$\unicode{x2013}$by simply restricting the expressivity of one's model or
policy class$\unicode{x2013}$one can achieve strictly better equilibrium
outcomes. Motivated by these examples, we then propose a new paradigm for
model-selection in games wherein an agent seeks to choose amongst different
model classes to use as their action set in a game.
</p>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07594" title="Abstract">arXiv:2402.07594</a> [<a href="/pdf/2402.07594" title="Download PDF">pdf</a>, <a href="/format/2402.07594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foundational Inference Models for Dynamical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seifner%2C+P">Patrick Seifner</a>, 
<a href="/search/cs?searchtype=author&query=Cvejoski%2C+K">Kostadin Cvejoski</a>, 
<a href="/search/cs?searchtype=author&query=Sanchez%2C+R+J">Ramses J. Sanchez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Dynamical Systems (math.DS)

</div>
<p class="mathjax">Ordinary differential equations (ODEs) underlie dynamical systems which serve
as models for a vast number of natural and social phenomena. Yet inferring the
ODE that best describes a set of noisy observations on one such phenomenon can
be remarkably challenging, and the models available to achieve it tend to be
highly specialized and complex too. In this work we propose a novel supervised
learning framework for zero-shot inference of ODEs from noisy data. We first
generate large datasets of one-dimensional ODEs, by sampling distributions over
the space of initial conditions, and the space of vector fields defining them.
We then learn neural maps between noisy observations on the solutions of these
equations, and their corresponding initial condition and vector fields. The
resulting models, which we call foundational inference models (FIM), can be (i)
copied and matched along the time dimension to increase their resolution; and
(ii) copied and composed to build inference models of any dimensionality,
without the need of any finetuning. We use FIM to model both ground-truth
dynamical systems of different dimensionalities and empirical time series data
in a zero-shot fashion, and outperform state-of-the-art models which are
finetuned to these systems. Our (pretrained) FIMs are available online
</p>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07596" title="Abstract">arXiv:2402.07596</a> [<a href="/pdf/2402.07596" title="Download PDF">pdf</a>, <a href="/format/2402.07596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sheet Music Transformer: End-To-End Optical Music Recognition Beyond  Monophonic Transcription
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=R%C3%ADos-Vila%2C+A">Antonio R&#xed;os-Vila</a>, 
<a href="/search/cs?searchtype=author&query=Calvo-Zaragoza%2C+J">Jorge Calvo-Zaragoza</a>, 
<a href="/search/cs?searchtype=author&query=Paquet%2C+T">Thierry Paquet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the International Conference on Document Analysis and Recognition 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">State-of-the-art end-to-end Optical Music Recognition (OMR) has, to date,
primarily been carried out using monophonic transcription techniques to handle
complex score layouts, such as polyphony, often by resorting to simplifications
or specific adaptations. Despite their efficacy, these approaches imply
challenges related to scalability and limitations. This paper presents the
Sheet Music Transformer, the first end-to-end OMR model designed to transcribe
complex musical scores without relying solely on monophonic strategies. Our
model employs a Transformer-based image-to-sequence framework that predicts
score transcriptions in a standard digital music encoding format from input
images. Our model has been tested on two polyphonic music datasets and has
proven capable of handling these intricate music structures effectively. The
experimental outcomes not only indicate the competence of the model, but also
show that it is better than the state-of-the-art methods, thus contributing to
advancements in end-to-end OMR transcription.
</p>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07598" title="Abstract">arXiv:2402.07598</a> [<a href="/pdf/2402.07598" title="Download PDF">pdf</a>, <a href="/format/2402.07598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near-Minimax-Optimal Distributional Reinforcement Learning with a  Generative Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rowland%2C+M">Mark Rowland</a>, 
<a href="/search/cs?searchtype=author&query=Wenliang%2C+L+K">Li Kevin Wenliang</a>, 
<a href="/search/cs?searchtype=author&query=Munos%2C+R">R&#xe9;mi Munos</a>, 
<a href="/search/cs?searchtype=author&query=Lyle%2C+C">Clare Lyle</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yunhao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Dabney%2C+W">Will Dabney</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We propose a new algorithm for model-based distributional reinforcement
learning (RL), and prove that it is minimax-optimal for approximating return
distributions with a generative model (up to logarithmic factors), resolving an
open question of Zhang et al. (2023). Our analysis provides new theoretical
results on categorical approaches to distributional RL, and also introduces a
new distributional Bellman equation, the stochastic categorical CDF Bellman
equation, which we expect to be of independent interest. We also provide an
experimental study comparing several model-based distributional RL algorithms,
with several takeaways for practitioners.
</p>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07600" title="Abstract">arXiv:2402.07600</a> [<a href="/pdf/2402.07600" title="Download PDF">pdf</a>, <a href="/format/2402.07600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optical Routing with Binary Optimisation and Quantum Annealing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Davies%2C+E">Ethan Davies</a>, 
<a href="/search/cs?searchtype=author&query=Banfield%2C+D">Darren Banfield</a>, 
<a href="/search/cs?searchtype=author&query=Carare%2C+V">Vlad Carare</a>, 
<a href="/search/cs?searchtype=author&query=Weaver%2C+B">Ben Weaver</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+C">Catherine White</a>, 
<a href="/search/cs?searchtype=author&query=Walker%2C+N">Nigel Walker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">A challenge for scalability of demand-responsive, elastic optical Dense
Wavelength Division Multiplexing (DWDM) and Flexgrid networks is the
computational complexity of allocating many optical routes on large networks.
We demonstrate that demand satisfaction problems in communication networks can
be formulated as quadratic unconstrained binary optimisation (QUBO) problems,
and solved using a hybrid quantum annealer. Efficient encodings are developed
which solve both unicast and multicast multicommodity-flow problems, while also
adhering to individual requirements for maximum latency and resilience for each
route. We present several QUBO formulations and analyse the qubit scaling. We
demonstrate solutions using a hybrid solver, D-Wave Quantum Advantage QPU.
Progress in generating optimal solutions with efficient use of computational
resources will be beneficial to telecoms operators, enabling them to run
dynamic optical network infrastructures which use resources efficiently, are
resilient to local faults and cyber-attacks, and can be elastically responsive
to demands.
</p>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07601" title="Abstract">arXiv:2402.07601</a> [<a href="/pdf/2402.07601" title="Download PDF">pdf</a>, <a href="/format/2402.07601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topic-aware Most Influential Community Search in Social Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Teng%2C+L">Long Teng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanhao Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">Community search is a problem aimed at searching for densely connected
subgraphs within a network based on query conditions, which has recently
attracted significant attention. However, most previous community search
studies have overlooked the coexistence relationship among attributes. They
typically assign a single attribute to each node or edge (e.g.,only considering
influence scores or keywords), which is difficult for users to obtain a
comprehensive and beneficial information. Additionally, most of them also
ignored the uncertainty in the attribute graph. Therefore, in this paper, we
introduce two novel community models, namely topic-based interaction graph and
$(k,l,\eta)$-influential community. The former is a directed ucertain graph
generated by the query topic distribution provided by users, while the latter
is used for solving the topic-aware most influential community search problem
in social networks. Furthermore, we propose an online search algorithm which
computes the influence value of each vertex by considering the topic-aware
information diffusion process on interaction graphs. And then, we use a
peeling-pruning strategy to iteratively find the topic-aware most
$(k,l,\eta)$-influential community. To further speed up the search performance,
we devise two lightweight index structures which efficiently support the search
for the topic-aware most influential community within an optimal time. We also
propose three optimization methods to improve the space and time costs of the
index-based approach.
</p>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07602" title="Abstract">arXiv:2402.07602</a> [<a href="/pdf/2402.07602" title="Download PDF">pdf</a>, <a href="/format/2402.07602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DART: A Compact Platform For Autonomous Driving Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyons%2C+L">Lorenzo Lyons</a>, 
<a href="/search/cs?searchtype=author&query=Niesten%2C+T">Thijs Niesten</a>, 
<a href="/search/cs?searchtype=author&query=Ferranti%2C+L">Laura Ferranti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper presents the design of a research platform for autonomous driving
applications, the Delft's Autonomous-driving Robotic Testbed (DART). Our goal
was to design a small-scale car-like robot equipped with all the hardware
needed for on-board navigation and control while keeping it cost-effective and
easy to replicate. To develop DART, we built on an existing off-the-shelf model
and augmented its sensor suite to improve its capabilities for control and
motion planning tasks. We detail the hardware setup and the system
identification challenges to derive the vehicle's models. Furthermore, we
present some use cases where we used DART to test different motion planning
applications to show the versatility of the platform. Finally, we provide a git
repository with all the details to replicate DART, complete with a simulation
environment and the data used for system identification.
</p>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07610" title="Abstract">arXiv:2402.07610</a> [<a href="/pdf/2402.07610" title="Download PDF">pdf</a>, <a href="/format/2402.07610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+G">Guozheng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Z">Ziqiao Meng</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Zeyu Qin</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Bingzhe Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Liu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+Y">Yatao Bian</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tingyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xueqian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Peilin Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Self-alignment is an effective way to reduce the cost of human annotation
while ensuring promising model capability. However, most current methods
complete the data collection and training steps in a single round, which may
overlook the continuously improving ability of self-aligned models. This gives
rise to a key query: What if we do multi-time bootstrapping self-alignment?
Does this strategy enhance model performance or lead to rapid degradation? In
this paper, our pioneering exploration delves into the impact of bootstrapping
self-alignment on large language models. Our findings reveal that bootstrapping
self-alignment markedly surpasses the single-round approach, by guaranteeing
data diversity from in-context learning. To further exploit the capabilities of
bootstrapping, we investigate and adjust the training order of data, which
yields improved performance of the model. Drawing on these findings, we propose
Step-On-Feet Tuning (SOFT) which leverages model's continuously enhanced
few-shot ability to boost zero or one-shot performance. Based on easy-to-hard
training recipe, we propose SOFT+ which further boost self-alignment's
performance. Our experiments demonstrate the efficiency of SOFT (SOFT+) across
various classification and generation tasks, highlighting the potential of
bootstrapping self-alignment on continually enhancing model alignment
performance.
</p>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07616" title="Abstract">arXiv:2402.07616</a> [<a href="/pdf/2402.07616" title="Download PDF">pdf</a>, <a href="/format/2402.07616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anchor-based Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+J">Jianhui Pang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+F">Fanghua Ye</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+D+F">Derek F. Wong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Longyue Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages. Work was done when Jianhui Pang and Fanghua Ye were interning at Tencent AI Lab. Longyue Wang is the corresponding author
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) predominantly employ decoder-only transformer
architectures, necessitating the retention of keys/values information for
historical tokens to provide contextual information and avoid redundant
computation. However, the substantial size and parameter volume of these LLMs
require massive GPU memory. This memory demand increases with the length of the
input text, leading to an urgent need for more efficient methods of information
storage and processing. This study introduces the Anchor-based LLM (AnLLM),
which utilizes an innovative anchor-based self-attention network (AnSAN) and
also an anchor-based inference strategy. This approach enables LLMs to compress
sequence information into an anchor token, reducing the keys/values cache and
enhancing inference efficiency. Experiments show that the AnLLM maintains
comparable accuracy with up to 99% keys/values cache reduction and up to 3.5
times faster inference. Despite a minor compromise in accuracy, the AnLLM
significantly improves computational efficiency and resource utilization,
demonstrating the potential of the anchor-based attention approach in the
context of LLMs for real-time inference in practical applications.
</p>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07619" title="Abstract">arXiv:2402.07619</a> [<a href="/pdf/2402.07619" title="Download PDF">pdf</a>, <a href="/format/2402.07619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Developing a Multi-variate Prediction Model For COVID-19 From  Crowd-sourced Respiratory Voice Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yuyang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Aljbawi%2C+W">Wafaa Aljbawi</a>, 
<a href="/search/cs?searchtype=author&query=Simons%2C+S+O">Sami O. Simons</a>, 
<a href="/search/cs?searchtype=author&query=Urovi%2C+V">Visara Urovi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2209.03727">arXiv:2209.03727</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">COVID-19 has affected more than 223 countries worldwide and in the Post-COVID
Era, there is a pressing need for non-invasive, low-cost, and highly scalable
solutions to detect COVID-19. We develop a deep learning model to identify
COVID-19 from voice recording data. The novelty of this work is in the
development of deep learning models for COVID-19 identification from only voice
recordings. We use the Cambridge COVID-19 Sound database which contains 893
speech samples, crowd-sourced from 4352 participants via a COVID-19 Sounds app.
Voice features including Mel-spectrograms and Mel-frequency cepstral
coefficients (MFCC) and CNN Encoder features are extracted. Based on the voice
data, we develop deep learning classification models to detect COVID-19 cases.
These models include Long Short-Term Memory (LSTM) and Convolutional Neural
Network (CNN) and Hidden-Unit BERT (HuBERT). We compare their predictive power
to baseline machine learning models. HuBERT achieves the highest accuracy of
86\% and the highest AUC of 0.93. The results achieved with the proposed models
suggest promising results in COVID-19 diagnosis from voice recordings when
compared to the results obtained from the state-of-the-art.
</p>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07621" title="Abstract">arXiv:2402.07621</a> [<a href="/pdf/2402.07621" title="Download PDF">pdf</a>, <a href="/format/2402.07621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Correctness Verification of Neural Networks Approximating Differential  Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ellinas%2C+P">Petros Ellinas</a>, 
<a href="/search/eess?searchtype=author&query=Nellikath%2C+R">Rahul Nellikath</a>, 
<a href="/search/eess?searchtype=author&query=Ventura%2C+I">Ignasi Ventura</a>, 
<a href="/search/eess?searchtype=author&query=Stiasny%2C+J">Jochen Stiasny</a>, 
<a href="/search/eess?searchtype=author&query=Chatzivasileiadis%2C+S">Spyros Chatzivasileiadis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Verification of Neural Networks (NNs) that approximate the solution of
Partial Differential Equations (PDEs) is a major milestone towards enhancing
their trustworthiness and accelerating their deployment, especially for
safety-critical systems. If successful, such NNs can become integral parts of
simulation software tools which can accelerate the simulation of complex
dynamic systems more than 100 times. However, the verification of these
functions poses major challenges; it is not straightforward how to efficiently
bound them or how to represent the derivative of the NN. This work addresses
both these problems. First, we define the NN derivative as a finite difference
approximation. Then, we formulate the PDE residual bounding problem alongside
the Initial Value Problem's error propagation. Finally, for the first time, we
tackle the problem of bounding an NN function without a priori knowledge of the
output domain. For this, we build a parallel branching algorithm that combines
the incomplete CROWN solver and Gradient Attack for termination and domain
rejection conditions. We demonstrate the strengths and weaknesses of the
proposed framework, and we suggest further work to enhance its efficiency.
</p>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07625" title="Abstract">arXiv:2402.07625</a> [<a href="/pdf/2402.07625" title="Download PDF">pdf</a>, <a href="/format/2402.07625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoMathText: Autonomous Data Selection with Language Models for  Mathematical Texts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yifan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+A+C">Andrew Chi-Chih Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">To improve language models' proficiency in mathematical reasoning via
continual pretraining, we introduce a novel strategy that leverages base
language models for autonomous data selection. Departing from conventional
supervised fine-tuning or trained classifiers with human-annotated data, our
approach utilizes meta-prompted language models as zero-shot verifiers to
autonomously evaluate and select high-quality mathematical content, and we
release the curated open-source AutoMathText dataset encompassing over 200GB of
data. To demonstrate the efficacy of our method, we continuously pretrained a
7B-parameter Mistral language model on the AutoMathText dataset, achieving
substantial improvements in downstream performance on the MATH dataset with a
token amount reduced by orders of magnitude compared to previous continuous
pretraining works. Our method showcases a 2 times increase in pretraining token
efficiency compared to baselines, underscoring the potential of our approach in
enhancing models' mathematical reasoning capabilities. The AutoMathText dataset
is available at https://huggingface.co/datasets/math-ai/AutoMathText. The code
is available at https://github.com/yifanzhang-pro/AutoMathText.
</p>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07630" title="Abstract">arXiv:2402.07630</a> [<a href="/pdf/2402.07630" title="Download PDF">pdf</a>, <a href="/format/2402.07630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> G-Retriever: Retrieval-Augmented Generation for Textual Graph  Understanding and Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiaoxin He</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yijun Tian</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yifei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chawla%2C+N+V">Nitesh V. Chawla</a>, 
<a href="/search/cs?searchtype=author&query=Laurent%2C+T">Thomas Laurent</a>, 
<a href="/search/cs?searchtype=author&query=LeCun%2C+Y">Yann LeCun</a>, 
<a href="/search/cs?searchtype=author&query=Bresson%2C+X">Xavier Bresson</a>, 
<a href="/search/cs?searchtype=author&query=Hooi%2C+B">Bryan Hooi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Given a graph with textual attributes, we enable users to `chat with their
graph': that is, to ask questions about the graph using a conversational
interface. In response to a user's questions, our method provides textual
replies and highlights the relevant parts of the graph. While existing works
integrate large language models (LLMs) and graph neural networks (GNNs) in
various ways, they mostly focus on either conventional graph tasks (such as
node, edge, and graph classification), or on answering simple graph queries on
small or synthetic graphs. In contrast, we develop a flexible
question-answering framework targeting real-world textual graphs, applicable to
multiple applications including scene graph understanding, common sense
reasoning, and knowledge graph reasoning. Toward this goal, we first develop
our Graph Question Answering (GraphQA) benchmark with data collected from
different tasks. Then, we propose our G-Retriever approach, which integrates
the strengths of GNNs, LLMs, and Retrieval-Augmented Generation (RAG), and can
be fine-tuned to enhance graph understanding via soft prompting. To resist
hallucination and to allow for textual graphs that greatly exceed the LLM's
context window size, G-Retriever performs RAG over a graph by formulating this
task as a Prize-Collecting Steiner Tree optimization problem. Empirical
evaluations show that our method outperforms baselines on textual graph tasks
from multiple domains, scales well with larger graph sizes, and resists
hallucination. (Our codes and datasets are available at:
https://github.com/XiaoxinHe/G-Retriever.)
</p>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07631" title="Abstract">arXiv:2402.07631</a> [<a href="/pdf/2402.07631" title="Download PDF">pdf</a>, <a href="/format/2402.07631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Higher-order Connection Laplacians for Directed Simplicial Complexes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+X">Xue Gong</a>, 
<a href="/search/cs?searchtype=author&query=Higham%2C+D+J">Desmond J. Higham</a>, 
<a href="/search/cs?searchtype=author&query=Zygalakis%2C+K">Konstantinos Zygalakis</a>, 
<a href="/search/cs?searchtype=author&query=Bianconi%2C+G">Ginestra Bianconi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Statistical Mechanics (cond-mat.stat-mech); Adaptation and Self-Organizing Systems (nlin.AO); Data Analysis, Statistics and Probability (physics.data-an)

</div>
<p class="mathjax">Higher-order networks encode the many-body interactions existing in complex
systems, such as the brain, protein complexes, and social interactions.
Simplicial complexes are higher-order networks that allow a comprehensive
investigation of the interplay between topology and dynamics. However,
simplicial complexes have the limitation that they only capture undirected
higher-order interactions while in real-world scenarios, often there is a need
to introduce the direction of simplices, extending the popular notion of
direction of edges. On graphs and networks the Magnetic Laplacian, a special
case of Connection Laplacian, is becoming a popular operator to treat edge
directionality. Here we tackle the challenge of treating directional simplicial
complexes by formulating Higher-order Connection Laplacians taking into account
the configurations induced by the simplices' directions. Specifically, we
define all the Connection Laplacians of directed simplicial complexes of
dimension two and we discuss the induced higher-order diffusion dynamics by
considering instructive synthetic examples of simplicial complexes. The
proposed higher-order diffusion processes can be adopted in real scenarios when
we want to consider higher-order diffusion displaying non-trivial frustration
effects due to conflicting directionalities of the incident simplices.
</p>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07632" title="Abstract">arXiv:2402.07632</a> [<a href="/pdf/2402.07632" title="Download PDF">pdf</a>, <a href="/format/2402.07632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Overconfident and Unconfident AI Hinder Human-AI Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jingshu Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yitian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yi-chieh Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">As artificial intelligence (AI) advances, human-AI collaboration has become
increasingly prevalent across both professional and everyday settings. In such
collaboration, AI can express its confidence level about its performance,
serving as a crucial indicator for humans to evaluate AI's suggestions.
However, AI may exhibit overconfidence or underconfidence--its expressed
confidence is higher or lower than its actual performance--which may lead
humans to mistakenly evaluate AI advice. Our study investigates the influences
of AI's overconfidence and underconfidence on human trust, their acceptance of
AI suggestions, and collaboration outcomes. Our study reveal that disclosing AI
confidence levels and performance feedback facilitates better recognition of AI
confidence misalignments. However, participants tend to withhold their trust as
perceiving such misalignments, leading to a rejection of AI suggestions and
subsequently poorer performance in collaborative tasks. Conversely, without
such information, participants struggle to identify misalignments, resulting in
either the neglect of correct AI advice or the following of incorrect AI
suggestions, adversely affecting collaboration. This study offers valuable
insights for enhancing human-AI collaboration by underscoring the importance of
aligning AI's expressed confidence with its actual performance and the
necessity of calibrating human trust towards AI confidence.
</p>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07633" title="Abstract">arXiv:2402.07633</a> [<a href="/pdf/2402.07633" title="Download PDF">pdf</a>, <a href="/format/2402.07633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complete Instances Mining for Weakly Supervised Instance Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zecheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Zening Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuqi Liang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jin-Gang Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the Thirty-Second International Joint Conference on
  Artificial Intelligence(IJCAI 2023). Main Track. Pages 1142-1150
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Weakly supervised instance segmentation (WSIS) using only image-level labels
is a challenging task due to the difficulty of aligning coarse annotations with
the finer task. However, with the advancement of deep neural networks (DNNs),
WSIS has garnered significant attention. Following a proposal-based paradigm,
we encounter a redundant segmentation problem resulting from a single instance
being represented by multiple proposals. For example, we feed a picture of a
dog and proposals into the network and expect to output only one proposal
containing a dog, but the network outputs multiple proposals. To address this
problem, we propose a novel approach for WSIS that focuses on the online
refinement of complete instances through the use of MaskIoU heads to predict
the integrity scores of proposals and a Complete Instances Mining (CIM)
strategy to explicitly model the redundant segmentation problem and generate
refined pseudo labels. Our approach allows the network to become aware of
multiple instances and complete instances, and we further improve its
robustness through the incorporation of an Anti-noise strategy. Empirical
evaluations on the PASCAL VOC 2012 and MS COCO datasets demonstrate that our
method achieves state-of-the-art performance with a notable margin. Our
implementation will be made available at https://github.com/ZechengLi19/CIM.
</p>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07635" title="Abstract">arXiv:2402.07635</a> [<a href="/pdf/2402.07635" title="Download PDF">pdf</a>, <a href="/format/2402.07635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaborative Semantic Occupancy Prediction with Hybrid Feature Fusion  in Connected Automated Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+R">Rui Song</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+C">Chenwei Liang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+H">Hu Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhiran Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zimmer%2C+W">Walter Zimmer</a>, 
<a href="/search/cs?searchtype=author&query=Gross%2C+M">Markus Gross</a>, 
<a href="/search/cs?searchtype=author&query=Festag%2C+A">Andreas Festag</a>, 
<a href="/search/cs?searchtype=author&query=Knoll%2C+A">Alois Knoll</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Collaborative perception in automated vehicles leverages the exchange of
information between agents, aiming to elevate perception results. Previous
camera-based collaborative 3D perception methods typically employ 3D bounding
boxes or bird's eye views as representations of the environment. However, these
approaches fall short in offering a comprehensive 3D environmental prediction.
To bridge this gap, we introduce the first method for collaborative 3D semantic
occupancy prediction. Particularly, it improves local 3D semantic occupancy
predictions by hybrid fusion of (i) semantic and occupancy task features, and
(ii) compressed orthogonal attention features shared between vehicles.
Additionally, due to the lack of a collaborative perception dataset designed
for semantic occupancy prediction, we augment a current collaborative
perception dataset to include 3D collaborative semantic occupancy labels for a
more robust evaluation. The experimental findings highlight that: (i) our
collaborative semantic occupancy predictions excel above the results from
single vehicles by over 30%, and (ii) models anchored on semantic occupancy
outpace state-of-the-art collaborative 3D detection techniques in subsequent
perception applications, showcasing enhanced accuracy and enriched
semantic-awareness in road environments.
</p>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07639" title="Abstract">arXiv:2402.07639</a> [<a href="/pdf/2402.07639" title="Download PDF">pdf</a>, <a href="/format/2402.07639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tighter Bounds on the Information Bottleneck with Application to Deep  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weingarten%2C+N">Nir Weingarten</a>, 
<a href="/search/cs?searchtype=author&query=Yakhini%2C+Z">Zohar Yakhini</a>, 
<a href="/search/cs?searchtype=author&query=Butman%2C+M">Moshe Butman</a>, 
<a href="/search/cs?searchtype=author&query=Gilad-Bachrach%2C+R">Ran Gilad-Bachrach</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures, code included in github repo
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT)

</div>
<p class="mathjax">Deep Neural Nets (DNNs) learn latent representations induced by their
downstream task, objective function, and other parameters. The quality of the
learned representations impacts the DNN's generalization ability and the
coherence of the emerging latent space. The Information Bottleneck (IB)
provides a hypothetically optimal framework for data modeling, yet it is often
intractable. Recent efforts combined DNNs with the IB by applying VAE-inspired
variational methods to approximate bounds on mutual information, resulting in
improved robustness to adversarial attacks. This work introduces a new and
tighter variational bound for the IB, improving performance of previous
IB-inspired DNNs. These advancements strengthen the case for the IB and its
variational approximations as a data modeling framework, and provide a simple
method to significantly enhance the adversarial robustness of classifier DNNs.
</p>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07640" title="Abstract">arXiv:2402.07640</a> [<a href="/pdf/2402.07640" title="Download PDF">pdf</a>, <a href="/format/2402.07640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthesizing Sentiment-Controlled Feedback For Multimodal Text and Image  Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+P">Puneet Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Malik%2C+S">Sarthak Malik</a>, 
<a href="/search/cs?searchtype=author&query=Raman%2C+B">Balasubramanian Raman</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaobai Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The ability to generate sentiment-controlled feedback in response to
multimodal inputs, comprising both text and images, addresses a critical gap in
human-computer interaction by enabling systems to provide empathetic, accurate,
and engaging responses. This capability has profound applications in
healthcare, marketing, and education. To this end, we construct a large-scale
Controllable Multimodal Feedback Synthesis (CMFeed) dataset and propose a
controllable feedback synthesis system. The proposed system includes an
encoder, decoder, and controllability block for textual and visual inputs. It
extracts textual and visual features using a transformer and Faster R-CNN
networks and combines them to generate feedback. The CMFeed dataset encompasses
images, text, reactions to the post, human comments with relevance scores, and
reactions to the comments. The reactions to the post and comments are utilized
to train the proposed model to produce feedback with a particular (positive or
negative) sentiment. A sentiment classification accuracy of 77.23% has been
achieved, 18.82% higher than the accuracy without using the controllability.
Moreover, the system incorporates a similarity module for assessing feedback
relevance through rank-based metrics. It implements an interpretability
technique to analyze the contribution of textual and visual features during the
generation of uncontrolled and controlled feedback.
</p>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07642" title="Abstract">arXiv:2402.07642</a> [<a href="/pdf/2402.07642" title="Download PDF">pdf</a>, <a href="/format/2402.07642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Flow-based Credibility Metric for Safety-critical Pedestrian Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyssenko%2C+M">Maria Lyssenko</a>, 
<a href="/search/cs?searchtype=author&query=Gladisch%2C+C">Christoph Gladisch</a>, 
<a href="/search/cs?searchtype=author&query=Heinzemann%2C+C">Christian Heinzemann</a>, 
<a href="/search/cs?searchtype=author&query=Woehrle%2C+M">Matthias Woehrle</a>, 
<a href="/search/cs?searchtype=author&query=Triebel%2C+R">Rudolph Triebel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Safety is of utmost importance for perception in automated driving (AD).
However, a prime safety concern in state-of-the art object detection is that
standard evaluation schemes utilize safety-agnostic metrics to argue sufficient
detection performance. Hence, it is imperative to leverage supplementary domain
knowledge to accentuate safety-critical misdetections during evaluation tasks.
To tackle the underspecification, this paper introduces a novel credibility
metric, called c-flow, for pedestrian bounding boxes. To this end, c-flow
relies on a complementary optical flow signal from image sequences and enhances
the analyses of safety-critical misdetections without requiring additional
labels. We implement and evaluate c-flow with a state-of-the-art pedestrian
detector on a large AD dataset. Our analysis demonstrates that c-flow allows
developers to identify safety-critical misdetections.
</p>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07643" title="Abstract">arXiv:2402.07643</a> [<a href="/pdf/2402.07643" title="Download PDF">pdf</a>, <a href="/ps/2402.07643" title="Download PostScript">ps</a>, <a href="/format/2402.07643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Lattice-Reduction Aided Vector Perturbation Precoder Relying on  Quantum Annealing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Winter%2C+S">Samuel Winter</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yangyishi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+G">Gan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Hanzo%2C+L">Lajos Hanzo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by IEEE Wireless Communications Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Quantum annealing (QA) is proposed for vector perturbation precoding (VPP) in
multiple input multiple output (MIMO) communications systems. The mathematical
framework of VPP is presented, outlining the problem formulation and the
benefits of lattice reduction algorithms. Lattice reduction aided quantum
vector perturbation (LRAQVP) is designed by harnessing physical quantum
hardware, and the optimization of hardware parameters is discussed. We observe
a 5dB gain over lattice reduction zero forcing precoding (LRZFP), which behaves
similarly to a quantum annealing algorithm operating without a lattice
reduction stage. The proposed algorithm is also shown to approach the
performance of a sphere encoder, which exhibits an exponentially escalating
complexity.
</p>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07645" title="Abstract">arXiv:2402.07645</a> [<a href="/pdf/2402.07645" title="Download PDF">pdf</a>, <a href="/format/2402.07645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting the Clinical Features of Difficult-to-Treat Depression using  Synthetic Data from Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lorge%2C+I">Isabelle Lorge</a>, 
<a href="/search/cs?searchtype=author&query=Joyce%2C+D+W">Dan W. Joyce</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+N">Niall Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Nevado-Holgado%2C+A">Alejo Nevado-Holgado</a>, 
<a href="/search/cs?searchtype=author&query=Cipriani%2C+A">Andrea Cipriani</a>, 
<a href="/search/cs?searchtype=author&query=Kormilitzin%2C+A">Andrey Kormilitzin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Difficult-to-treat depression (DTD) has been proposed as a broader and more
clinically comprehensive perspective on a person's depressive disorder where
despite treatment, they continue to experience significant burden. We sought to
develop a Large Language Model (LLM)-based tool capable of interrogating
routinely-collected, narrative (free-text) electronic health record (EHR) data
to locate published prognostic factors that capture the clinical syndrome of
DTD. In this work, we use LLM-generated synthetic data (GPT3.5) and a
Non-Maximum Suppression (NMS) algorithm to train a BERT-based span extraction
model. The resulting model is then able to extract and label spans related to a
variety of relevant positive and negative factors in real clinical data (i.e.
spans of text that increase or decrease the likelihood of a patient matching
the DTD syndrome). We show it is possible to obtain good overall performance
(0.70 F1 across polarity) on real clinical data on a set of as many as 20
different factors, and high performance (0.85 F1 with 0.95 precision) on a
subset of important DTD factors such as history of abuse, family history of
affective disorder, illness severity and suicidality by training the model
exclusively on synthetic data. Our results show promise for future healthcare
applications especially in applications where traditionally, highly
confidential medical data and human-expert annotation would normally be
required.
</p>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07647" title="Abstract">arXiv:2402.07647</a> [<a href="/pdf/2402.07647" title="Download PDF">pdf</a>, <a href="/format/2402.07647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GRILLBot In Practice: Lessons and Tradeoffs Deploying Large Language  Models for Adaptable Conversational Task Assistants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fischer%2C+S">Sophie Fischer</a>, 
<a href="/search/cs?searchtype=author&query=Gemmell%2C+C">Carlos Gemmell</a>, 
<a href="/search/cs?searchtype=author&query=Tecklenburg%2C+N">Niklas Tecklenburg</a>, 
<a href="/search/cs?searchtype=author&query=Mackie%2C+I">Iain Mackie</a>, 
<a href="/search/cs?searchtype=author&query=Rossetto%2C+F">Federico Rossetto</a>, 
<a href="/search/cs?searchtype=author&query=Dalton%2C+J">Jeffrey Dalton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, KDD Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">We tackle the challenge of building real-world multimodal assistants for
complex real-world tasks. We describe the practicalities and challenges of
developing and deploying GRILLBot, a leading (first and second prize winning in
2022 and 2023) system deployed in the Alexa Prize TaskBot Challenge. Building
on our Open Assistant Toolkit (OAT) framework, we propose a hybrid architecture
that leverages Large Language Models (LLMs) and specialised models tuned for
specific subtasks requiring very low latency. OAT allows us to define when, how
and which LLMs should be used in a structured and deployable manner. For
knowledge-grounded question answering and live task adaptations, we show that
LLM reasoning abilities over task context and world knowledge outweigh latency
concerns. For dialogue state management, we implement a code generation
approach and show that specialised smaller models have 84% effectiveness with
100x lower latency. Overall, we provide insights and discuss tradeoffs for
deploying both traditional models and LLMs to users in complex real-world
multimodal environments in the Alexa TaskBot challenge. These experiences will
continue to evolve as LLMs become more capable and efficient -- fundamentally
reshaping OAT and future assistant architectures.
</p>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07648" title="Abstract">arXiv:2402.07648</a> [<a href="/pdf/2402.07648" title="Download PDF">pdf</a>, <a href="/format/2402.07648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeformNet: Latent Space Modeling and Dynamics Prediction for Deformable  Object Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenchang Li</a>, 
<a href="/search/cs?searchtype=author&query=Ai%2C+Z">Zihao Ai</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaosa Li</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+W">Wenbo Ding</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Huazhe Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, Submitted to 2024 IEEE International Conference on Robotics and Automation (ICRA), Japan, Yokohama
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Manipulating deformable objects is a ubiquitous task in household
environments, demanding adequate representation and accurate dynamics
prediction due to the objects' infinite degrees of freedom. This work proposes
DeformNet, which utilizes latent space modeling with a learned 3D
representation model to tackle these challenges effectively. The proposed
representation model combines a PointNet encoder and a conditional neural
radiance field (NeRF), facilitating a thorough acquisition of object
deformations and variations in lighting conditions. To model the complex
dynamics, we employ a recurrent state-space model (RSSM) that accurately
predicts the transformation of the latent representation over time. Extensive
simulation experiments with diverse objectives demonstrate the generalization
capabilities of DeformNet for various deformable object manipulation tasks,
even in the presence of previously unseen goals. Finally, we deploy DeformNet
on an actual UR5 robotic arm to demonstrate its capability in real-world
scenarios.
</p>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07654" title="Abstract">arXiv:2402.07654</a> [<a href="/pdf/2402.07654" title="Download PDF">pdf</a>, <a href="/format/2402.07654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impact of spatial transformations on landscape features of CEC2022 basic  benchmark problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Haoran Yin</a>, 
<a href="/search/cs?searchtype=author&query=Vermetten%2C+D">Diederick Vermetten</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+F">Furong Ye</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%A4ck%2C+T+H+W">Thomas H.W. B&#xe4;ck</a>, 
<a href="/search/cs?searchtype=author&query=Kononova%2C+A+V">Anna V. Kononova</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">When benchmarking optimization heuristics, we need to take care to avoid an
algorithm exploiting biases in the construction of the used problems. One way
in which this might be done is by providing different versions of each problem
but with transformations applied to ensure the algorithms are equipped with
mechanisms for successfully tackling a range of problems. In this paper, we
investigate several of these problem transformations and show how they
influence the low-level landscape features of a set of 5 problems from the
CEC2022 benchmark suite. Our results highlight that even relatively small
transformations can significantly alter the measured landscape features. This
poses a wider question of what properties we want to preserve when creating
problem transformations, and how to fairly measure them.
</p>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07658" title="Abstract">arXiv:2402.07658</a> [<a href="/pdf/2402.07658" title="Download PDF">pdf</a>, <a href="/format/2402.07658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Sound of Healthcare: Improving Medical Transcription ASR Accuracy  with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adedeji%2C+A">Ayo Adedeji</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+S">Sarita Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Doohan%2C+B">Brendan Doohan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In the rapidly evolving landscape of medical documentation, transcribing
clinical dialogues accurately is increasingly paramount. This study explores
the potential of Large Language Models (LLMs) to enhance the accuracy of
Automatic Speech Recognition (ASR) systems in medical transcription. Utilizing
the PriMock57 dataset, which encompasses a diverse range of primary care
consultations, we apply advanced LLMs to refine ASR-generated transcripts. Our
research is multifaceted, focusing on improvements in general Word Error Rate
(WER), Medical Concept WER (MC-WER) for the accurate transcription of essential
medical terms, and speaker diarization accuracy. Additionally, we assess the
role of LLM post-processing in improving semantic textual similarity, thereby
preserving the contextual integrity of clinical dialogues. Through a series of
experiments, we compare the efficacy of zero-shot and Chain-of-Thought (CoT)
prompting techniques in enhancing diarization and correction accuracy. Our
findings demonstrate that LLMs, particularly through CoT prompting, not only
improve the diarization accuracy of existing ASR systems but also achieve
state-of-the-art performance in this domain. This improvement extends to more
accurately capturing medical concepts and enhancing the overall semantic
coherence of the transcribed dialogues. These findings illustrate the dual role
of LLMs in augmenting ASR outputs and independently excelling in transcription
tasks, holding significant promise for transforming medical ASR systems and
leading to more accurate and reliable patient records in healthcare settings.
</p>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07659" title="Abstract">arXiv:2402.07659</a> [<a href="/pdf/2402.07659" title="Download PDF">pdf</a>, <a href="/format/2402.07659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Behavior Collaborative Filtering with Partial Order Graph  Convolutional Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yijie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Bei%2C+Y">Yuanchen Bei</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Q">Qijie Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+H">Huan Gong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Senzhang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Feiran Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiao Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Representing the information of multiple behaviors in the single graph
collaborative filtering (CF) vector has been a long-standing challenge. This is
because different behaviors naturally form separate behavior graphs and learn
separate CF embeddings. Existing models merge the separate embeddings by
appointing the CF embeddings for some behaviors as the primary embedding and
utilizing other auxiliaries to enhance the primary embedding. However, this
approach often results in the joint embedding performing well on the main tasks
but poorly on the auxiliary ones. To address the problem arising from the
separate behavior graphs, we propose the concept of Partial Order Graphs (POG).
POG defines the partial order relation of multiple behaviors and models
behavior combinations as weighted edges to merge separate behavior graphs into
a joint POG. Theoretical proof verifies that POG can be generalized to any
given set of multiple behaviors. Based on POG, we propose the tailored Partial
Order Graph Convolutional Networks (POGCN) that convolute neighbors'
information while considering the behavior relations between users and items.
POGCN also introduces a partial-order BPR sampling strategy for efficient and
effective multiple-behavior CF training. POGCN has been successfully deployed
on the homepage of Alibaba for two months, providing recommendation services
for over one billion users. Extensive offline experiments conducted on three
public benchmark datasets demonstrate that POGCN outperforms state-of-the-art
multi-behavior baselines across all types of behaviors. Furthermore, online A/B
tests confirm the superiority of POGCN in billion-scale recommender systems.
</p>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07660" title="Abstract">arXiv:2402.07660</a> [<a href="/pdf/2402.07660" title="Download PDF">pdf</a>, <a href="/ps/2402.07660" title="Download PostScript">ps</a>, <a href="/format/2402.07660" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> R&#xe9;nyi Resolvability, Noise Stability, and Anti-contractivity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lei Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 54 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Functional Analysis (math.FA); Probability (math.PR)

</div>
<p class="mathjax">As indicated by the title, this paper investigates three closely related
topics -- R\'enyi resolvability, noise stability, and anti-contractivity. The
R\'enyi resolvability problem refers to approximating a target output
distribution of a given channel in the R\'enyi divergence when the input is set
to a function of a given uniform random variable. This problem for the R\'enyi
parameter in $[0,2]\cup\{\infty\}$ was studied by the present author and Tan in
2019. In the present paper, we provide a complete solution to this problem for
the R\'enyi parameter in the entire range $\mathbb{R}\cup\{\pm\infty\}$. We
then connect the R\'enyi resolvability problem to the noise stability problem,
by observing that the $q$-stability of a set can be expressed in terms of the
R\'enyi divergence between the true output distribution and the target
distribution in a variant of the R\'enyi resolvability problem. By such a
connection, we provide sharp dimension-free bounds on the $q$-stability. We
lastly relate the noise stability problem to the anti-contractivity of a Markov
operator (i.e., conditional expectation operator), where anti-contractivity
introduced by us refers to as the opposite property of the well-known
contractivity/hyercontractivity. We derive sharp dimension-free
anti-contractivity inequalities. All of the results in this paper are evaluated
for binary distributions. Our proofs in this paper are mainly based on the
method of types, especially a strengthened version of packing-covering lemma.
</p>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07664" title="Abstract">arXiv:2402.07664</a> [<a href="/pdf/2402.07664" title="Download PDF">pdf</a>, <a href="/format/2402.07664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling performance portability of data-parallel OpenMP applications on  asymmetric multicore processors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saez%2C+J+C">Juan Carlos Saez</a>, 
<a href="/search/cs?searchtype=author&query=Castro%2C+F">Fernando Castro</a>, 
<a href="/search/cs?searchtype=author&query=Prieto-Matias%2C+M">Manuel Prieto-Matias</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 49th International Conference on Parallel
  Processing (ICPP 2020). ACM, Article 51, 1-11
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Operating Systems (cs.OS)

</div>
<p class="mathjax">Asymmetric multicore processors (AMPs) couple high-performance big cores and
low-power small cores with the same instruction-set architecture but different
features, such as clock frequency or microarchitecture. Previous work has shown
that asymmetric designs may deliver higher energy efficiency than symmetric
multicores for diverse workloads. Despite their benefits, AMPs pose significant
challenges to runtime systems of parallel programming models. While previous
work has mainly explored how to efficiently execute task-based parallel
applications on AMPs, via enhancements in the runtime system, improving the
performance of unmodified data-parallel applications on these architectures is
still a big challenge. In this work we analyze the particular case of
loop-based OpenMP applications, which are widely used today in scientific and
engineering domains, and constitute the dominant application type in many
parallel benchmark suites used for performance evaluation on multicore systems.
We observed that conventional loop-scheduling OpenMP approaches are unable to
efficiently cope with the load imbalance that naturally stems from the
different performance delivered by big and small cores.
<br />To address this shortcoming, we propose \textit{Asymmetric Iteration
Distribution} (AID), a set of novel loop-scheduling methods for AMPs that
distribute iterations unevenly across worker threads to efficiently deal with
performance asymmetry. We implemented AID in \textit{libgomp} --the GNU OpenMP
runtime system--, and evaluated it on two different asymmetric multicore
platforms. Our analysis reveals that the AID methods constitute effective
replacements of the \texttt{static} and \texttt{dynamic} methods on AMPs, and
are capable of improving performance over these conventional strategies by up
to 56\% and 16.8\%, respectively.
</p>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07666" title="Abstract">arXiv:2402.07666</a> [<a href="/pdf/2402.07666" title="Download PDF">pdf</a>, <a href="/format/2402.07666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximating the Maximum Independent Set of Convex Polygons with a  Bounded Number of Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grandoni%2C+F">Fabrizio Grandoni</a>, 
<a href="/search/cs?searchtype=author&query=Husi%C4%87%2C+E">Edin Husi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Mari%2C+M">Mathieu Mari</a>, 
<a href="/search/cs?searchtype=author&query=Tinguely%2C+A">Antoine Tinguely</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at SoCG 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">In the maximum independent set of convex polygons problem, we are given a set
of $n$ convex polygons in the plane with the objective of selecting a maximum
cardinality subset of non-overlapping polygons. Here we study a special case of
the problem where the edges of the polygons can take at most $d$ fixed
directions. We present an $8d/3$-approximation algorithm for this problem
running in time $O((nd)^{O(d4^d)})$. The previous-best polynomial-time
approximation (for constant $d$) was a classical $n^\varepsilon$ approximation
by Fox and Pach [SODA'11] that has recently been improved to a
$OPT^{\varepsilon}$-approximation algorithm by Cslovjecsek, Pilipczuk and
W\k{e}grzycki [SODA '24], which also extends to an arbitrary set of convex
polygons. Our result builds on, and generalizes the recent constant factor
approximation algorithms for the maximum independent set of axis-parallel
rectangles problem (which is a special case of our problem with $d=2$) by
Mitchell [FOCS'21] and G\'{a}lvez, Khan, Mari, M\"{o}mke, Reddy, and Wiese
[SODA'22].
</p>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07669" title="Abstract">arXiv:2402.07669</a> [<a href="/pdf/2402.07669" title="Download PDF">pdf</a>, <a href="/format/2402.07669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A History-dependent Dynamic Biot Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Stokke%2C+J+S">Jakob S. Stokke</a>, 
<a href="/search/math?searchtype=author&query=Jakobsen%2C+M">Morten Jakobsen</a>, 
<a href="/search/math?searchtype=author&query=Kumar%2C+K">Kundan Kumar</a>, 
<a href="/search/math?searchtype=author&query=Radu%2C+F+A">Florin A. Radu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this work, we consider a fully dynamic Biot model that includes memory
effects due to evolving permeability. Time integrals are used to account for
the change in structure. We propose an iterative splitting scheme for this
model, extending the fixed-stress split for the quasi-static Biot. We use
finite elements in space and a backward Euler discretization in time. The
performance of the method is demonstrated through a numerical experiment
</p>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07674" title="Abstract">arXiv:2402.07674</a> [<a href="/pdf/2402.07674" title="Download PDF">pdf</a>, <a href="/ps/2402.07674" title="Download PostScript">ps</a>, <a href="/format/2402.07674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multi-Tenant System for 5/6G Testbed as-a-Service
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bolla%2C+R">Raffaele Bolla</a>, 
<a href="/search/cs?searchtype=author&query=Bruschi%2C+R">Roberto Bruschi</a>, 
<a href="/search/cs?searchtype=author&query=Lombardo%2C+C">Chiara Lombardo</a>, 
<a href="/search/cs?searchtype=author&query=Mangialardi%2C+S">Sergio Mangialardi</a>, 
<a href="/search/cs?searchtype=author&query=Mohammadpour%2C+A">Alireza Mohammadpour</a>, 
<a href="/search/cs?searchtype=author&query=Rabbani%2C+R">Ramin Rabbani</a>, 
<a href="/search/cs?searchtype=author&query=Siccardi%2C+B">Beatrice Siccardi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-acceptance version of a COMSNET conferenze paper accepted in TASIR workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">In order to fulfill the stringent requirements and fast advancements of 5G
and beyond applications, it is inevitable to develop research/industrial
testbeds to examine the different proposed innovative features of 5G and
beyond. In this paper, we propose a testbed including 5G and beyond
technologies by combining open-source solutions and our developed Network
Services/Functions to enhance the ETSI-NFV MANO framework. Our testbed contains
an automation framework to reduce both run time and setup complexity. It
provides various services: Metal as a Service (MaaS), Infrastructure as a
Service (IaaS), Platform as a Service (PaaS), different Network Functions
(NFs), and Network Services (NSs), under the context of full automation of
End-to-End (E2E) NSs on top of the services provided by ETSI.
</p>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07677" title="Abstract">arXiv:2402.07677</a> [<a href="/pdf/2402.07677" title="Download PDF">pdf</a>, <a href="/format/2402.07677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GBOT: Graph-Based 3D Object Tracking for Augmented Reality-Assisted  Assembly Guidance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shiyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Schieber%2C+H">Hannah Schieber</a>, 
<a href="/search/cs?searchtype=author&query=Corell%2C+N">Niklas Corell</a>, 
<a href="/search/cs?searchtype=author&query=Egger%2C+B">Bernhard Egger</a>, 
<a href="/search/cs?searchtype=author&query=Kreimeier%2C+J">Julian Kreimeier</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+D">Daniel Roth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Guidance for assemblable parts is a promising field for augmented reality.
Augmented reality assembly guidance requires 6D object poses of target objects
in real time. Especially in time-critical medical or industrial settings,
continuous and markerless tracking of individual parts is essential to
visualize instructions superimposed on or next to the target object parts. In
this regard, occlusions by the user's hand or other objects and the complexity
of different assembly states complicate robust and real-time markerless
multi-object tracking. To address this problem, we present Graph-based Object
Tracking (GBOT), a novel graph-based single-view RGB-D tracking approach. The
real-time markerless multi-object tracking is initialized via 6D pose
estimation and updates the graph-based assembly poses. The tracking through
various assembly states is achieved by our novel multi-state assembly graph. We
update the multi-state assembly graph by utilizing the relative poses of the
individual assembly parts. Linking the individual objects in this graph enables
more robust object tracking during the assembly process. For evaluation, we
introduce a synthetic dataset of publicly available and 3D printable assembly
assets as a benchmark for future work. Quantitative experiments in synthetic
data and further qualitative study in real test data show that GBOT can
outperform existing work towards enabling context-aware augmented reality
assembly guidance. Dataset and code will be made publically available.
</p>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07680" title="Abstract">arXiv:2402.07680</a> [<a href="/pdf/2402.07680" title="Download PDF">pdf</a>, <a href="/ps/2402.07680" title="Download PostScript">ps</a>, <a href="/format/2402.07680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AYDIV: Adaptable Yielding 3D Object Detection via Integrated Contextual  Vision Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dam%2C+T">Tanmoy Dam</a>, 
<a href="/search/cs?searchtype=author&query=Dharavath%2C+S+B">Sanjay Bhargav Dharavath</a>, 
<a href="/search/cs?searchtype=author&query=Alam%2C+S">Sameer Alam</a>, 
<a href="/search/cs?searchtype=author&query=Lilith%2C+N">Nimrod Lilith</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Supriyo Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Feroskhan%2C+M">Mir Feroskhan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for ICRA 2024, and copyright will automatically transfer to IEEE upon its availability on the IEEE portal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">Combining LiDAR and camera data has shown potential in enhancing
short-distance object detection in autonomous driving systems. Yet, the fusion
encounters difficulties with extended distance detection due to the contrast
between LiDAR's sparse data and the dense resolution of cameras. Besides,
discrepancies in the two data representations further complicate fusion
methods. We introduce AYDIV, a novel framework integrating a tri-phase
alignment process specifically designed to enhance long-distance detection even
amidst data discrepancies. AYDIV consists of the Global Contextual Fusion
Alignment Transformer (GCFAT), which improves the extraction of camera features
and provides a deeper understanding of large-scale patterns; the Sparse Fused
Feature Attention (SFFA), which fine-tunes the fusion of LiDAR and camera
details; and the Volumetric Grid Attention (VGA) for a comprehensive spatial
data fusion. AYDIV's performance on the Waymo Open Dataset (WOD) with an
improvement of 1.24% in mAPH value(L2 difficulty) and the Argoverse2 Dataset
with a performance improvement of 7.40% in AP value demonstrates its efficacy
in comparison to other existing fusion-based methods. Our code is publicly
available at https://github.com/sanjay-810/AYDIV2
</p>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07681" title="Abstract">arXiv:2402.07681</a> [<a href="/pdf/2402.07681" title="Download PDF">pdf</a>, <a href="/ps/2402.07681" title="Download PostScript">ps</a>, <a href="/format/2402.07681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models &quot;Ad Referendum&quot;: How Good Are They at Machine  Translation in the Legal Domain?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Briva-Iglesias%2C+V">Vicent Briva-Iglesias</a>, 
<a href="/search/cs?searchtype=author&query=Camargo%2C+J+L+C">Joao Lucas Cavalheiro Camargo</a>, 
<a href="/search/cs?searchtype=author&query=Dogru%2C+G">Gokhan Dogru</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This study evaluates the machine translation (MT) quality of two
state-of-the-art large language models (LLMs) against a tradition-al neural
machine translation (NMT) system across four language pairs in the legal
domain. It combines automatic evaluation met-rics (AEMs) and human evaluation
(HE) by professional transla-tors to assess translation ranking, fluency and
adequacy. The re-sults indicate that while Google Translate generally
outperforms LLMs in AEMs, human evaluators rate LLMs, especially GPT-4,
comparably or slightly better in terms of producing contextually adequate and
fluent translations. This discrepancy suggests LLMs' potential in handling
specialized legal terminology and context, highlighting the importance of human
evaluation methods in assessing MT quality. The study underscores the evolving
capabil-ities of LLMs in specialized domains and calls for reevaluation of
traditional AEMs to better capture the nuances of LLM-generated translations.
</p>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07682" title="Abstract">arXiv:2402.07682</a> [<a href="/pdf/2402.07682" title="Download PDF">pdf</a>, <a href="/format/2402.07682" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auxiliary Tasks to Boost Biaffine Semantic Dependency Parsing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Candito%2C+M">Marie Candito</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Findings of the Association for Computational Linguistics: ACL
  2022, pp. 2422-2429
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The biaffine parser of Dozat and Manning (2017) was successfully extended to
semantic dependency parsing (SDP) (Dozat and Manning, 2018). Its performance on
graphs is surprisingly high given that, without the constraint of producing a
tree, all arcs for a given sentence are predicted independently from each other
(modulo a shared representation of tokens). To circumvent such an independence
of decision, while retaining the O(n^2) complexity and highly parallelizable
architecture, we propose to use simple auxiliary tasks that introduce some form
of interdependence between arcs. Experiments on the three English acyclic
datasets of SemEval 2015 task 18 (Oepen et al., 2015), and on French deep
syntactic cyclic graphs (Ribeyre et al., 2014) show modest but systematic
performance gains on a near state-of-the-art baseline using transformer-based
contextualized representations. This provides a simple and robust method to
boost SDP performance.
</p>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07683" title="Abstract">arXiv:2402.07683</a> [<a href="/pdf/2402.07683" title="Download PDF">pdf</a>, <a href="/format/2402.07683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two Choices are Enough for P-LCPs, USOs, and Colorful Tangents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Borzechowski%2C+M">Michaela Borzechowski</a>, 
<a href="/search/cs?searchtype=author&query=Fearnley%2C+J">John Fearnley</a>, 
<a href="/search/cs?searchtype=author&query=Gordon%2C+S">Spencer Gordon</a>, 
<a href="/search/cs?searchtype=author&query=Savani%2C+R">Rahul Savani</a>, 
<a href="/search/cs?searchtype=author&query=Schnider%2C+P">Patrick Schnider</a>, 
<a href="/search/cs?searchtype=author&query=Weber%2C+S">Simon Weber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Computational Geometry (cs.CG); Optimization and Control (math.OC)

</div>
<p class="mathjax">We provide polynomial-time reductions between three search problems from
three distinct areas: the P-matrix linear complementarity problem (P-LCP),
finding the sink of a unique sink orientation (USO), and a variant of the
$\alpha$-Ham Sandwich problem. For all three settings, we show that "two
choices are enough", meaning that the general non-binary version of the problem
can be reduced in polynomial time to the binary version. This specifically
means that generalized P-LCPs are equivalent to P-LCPs, and grid USOs are
equivalent to cube USOs. These results are obtained by showing that both the
P-LCP and our $\alpha$-Ham Sandwich variant are equivalent to a new problem we
introduce, P-Lin-Bellman. This problem can be seen as a new tool for
formulating problems as P-LCPs.
</p>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07685" title="Abstract">arXiv:2402.07685</a> [<a href="/pdf/2402.07685" title="Download PDF">pdf</a>, <a href="/format/2402.07685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Multiple Instance Learning for Weakly Supervised Person ReID
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tyo%2C+J">Jacob Tyo</a>, 
<a href="/search/cs?searchtype=author&query=Lipton%2C+Z+C">Zachary C. Lipton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The acquisition of large-scale, precisely labeled datasets for person
re-identification (ReID) poses a significant challenge. Weakly supervised ReID
has begun to address this issue, although its performance lags behind fully
supervised methods. In response, we introduce Contrastive Multiple Instance
Learning (CMIL), a novel framework tailored for more effective weakly
supervised ReID. CMIL distinguishes itself by requiring only a single model and
no pseudo labels while leveraging contrastive losses -- a technique that has
significantly enhanced traditional ReID performance yet is absent in all prior
MIL-based approaches. Through extensive experiments and analysis across three
datasets, CMIL not only matches state-of-the-art performance on the large-scale
SYSU-30k dataset with fewer assumptions but also consistently outperforms all
baselines on the WL-market1501 and Weakly Labeled MUddy racer re-iDentification
dataset (WL-MUDD) datasets. We introduce and release the WL-MUDD dataset, an
extension of the MUDD dataset featuring naturally occurring weak labels from
the real-world application at PerformancePhoto.co. All our code and data are
accessible at
https://drive.google.com/file/d/1rjMbWB6m-apHF3Wg_cfqc8QqKgQ21AsT/view?usp=drive_link.
</p>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07687" title="Abstract">arXiv:2402.07687</a> [<a href="/pdf/2402.07687" title="Download PDF">pdf</a>, <a href="/format/2402.07687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-Preserving Gaze Data Streaming in Immersive Interactive Virtual  Reality: Robustness and User Experience
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wilson%2C+E">Ethan Wilson</a>, 
<a href="/search/cs?searchtype=author&query=Ibragimov%2C+A">Azim Ibragimov</a>, 
<a href="/search/cs?searchtype=author&query=Proulx%2C+M+J">Michael J. Proulx</a>, 
<a href="/search/cs?searchtype=author&query=Tetali%2C+S+D">Sai Deep Tetali</a>, 
<a href="/search/cs?searchtype=author&query=Butler%2C+K">Kevin Butler</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+E">Eakta Jain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in IEEE Transactions on Visualization and Computer Graphics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Eye tracking is routinely being incorporated into virtual reality (VR)
systems. Prior research has shown that eye tracking data can be used for
re-identification attacks. The state of our knowledge about currently existing
privacy mechanisms is limited to privacy-utility trade-off curves based on
data-centric metrics of utility, such as prediction error, and black-box threat
models. We propose that for interactive VR applications, it is essential to
consider user-centric notions of utility and a variety of threat models. We
develop a methodology to evaluate real-time privacy mechanisms for interactive
VR applications that incorporate subjective user experience and task
performance metrics. We evaluate selected privacy mechanisms using this
methodology and find that re-identification accuracy can be decreased to as low
as 14% while maintaining a high usability score and reasonable task
performance. Finally, we elucidate three threat scenarios (black-box, black-box
with exemplars, and white-box) and assess how well the different privacy
mechanisms hold up to these adversarial scenarios. This work advances the state
of the art in VR privacy by providing a methodology for end-to-end assessment
of the risk of re-identification attacks and potential mitigating solutions.
</p>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07688" title="Abstract">arXiv:2402.07688</a> [<a href="/pdf/2402.07688" title="Download PDF">pdf</a>, <a href="/format/2402.07688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CyberMetric: A Benchmark Dataset for Evaluating Large Language Models  Knowledge in Cybersecurity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tihanyi%2C+N">Norbert Tihanyi</a>, 
<a href="/search/cs?searchtype=author&query=Ferrag%2C+M+A">Mohamed Amine Ferrag</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+R">Ridhi Jain</a>, 
<a href="/search/cs?searchtype=author&query=Debbah%2C+M">Merouane Debbah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Large Language Models (LLMs) excel across various domains, from computer
vision to medical diagnostics. However, understanding the diverse landscape of
cybersecurity, encompassing cryptography, reverse engineering, and managerial
facets like risk assessment, presents a challenge, even for human experts. In
this paper, we introduce CyberMetric, a benchmark dataset comprising 10,000
questions sourced from standards, certifications, research papers, books, and
other publications in the cybersecurity domain. The questions are created
through a collaborative process, i.e., merging expert knowledge with LLMs,
including GPT-3.5 and Falcon-180B. Human experts spent over 200 hours verifying
their accuracy and relevance. Beyond assessing LLMs' knowledge, the dataset's
main goal is to facilitate a fair comparison between humans and different LLMs
in cybersecurity. To achieve this, we carefully selected 80 questions covering
a wide range of topics within cybersecurity and involved 30 participants of
diverse expertise levels, facilitating a comprehensive comparison between human
and machine intelligence in this area. The findings revealed that LLMs
outperformed humans in almost every aspect of cybersecurity.
</p>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07689" title="Abstract">arXiv:2402.07689</a> [<a href="/pdf/2402.07689" title="Download PDF">pdf</a>, <a href="/format/2402.07689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OrderBkd: Textual backdoor attack through repositioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alekseevskaia%2C+I">Irina Alekseevskaia</a>, 
<a href="/search/cs?searchtype=author&query=Arkhipenko%2C+K">Konstantin Arkhipenko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The use of third-party datasets and pre-trained machine learning models poses
a threat to NLP systems due to possibility of hidden backdoor attacks. Existing
attacks involve poisoning the data samples such as insertion of tokens or
sentence paraphrasing, which either alter the semantics of the original texts
or can be detected. Our main difference from the previous work is that we use
the reposition of a two words in a sentence as a trigger. By designing and
applying specific part-of-speech (POS) based rules for selecting these tokens,
we maintain high attack success rate on SST-2 and AG classification datasets
while outperforming existing attacks in terms of perplexity and semantic
similarity to the clean samples. In addition, we show the robustness of our
attack to the ONION defense method. All the code and data for the paper can be
obtained at https://github.com/alekseevskaia/OrderBkd.
</p>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07691" title="Abstract">arXiv:2402.07691</a> [<a href="/pdf/2402.07691" title="Download PDF">pdf</a>, <a href="/format/2402.07691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluation of a Smart Mobile Robotic System for Industrial Plant  Inspection and Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fischer%2C+G+K+J">Georg K.J. Fischer</a>, 
<a href="/search/cs?searchtype=author&query=Bergau%2C+M">Max Bergau</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B3mez-Rosal%2C+D+A">D. Adriana G&#xf3;mez-Rosal</a>, 
<a href="/search/cs?searchtype=author&query=Wachaja%2C+A">Andreas Wachaja</a>, 
<a href="/search/cs?searchtype=author&query=Gr%C3%A4ter%2C+J">Johannes Gr&#xe4;ter</a>, 
<a href="/search/cs?searchtype=author&query=Odenweller%2C+M">Matthias Odenweller</a>, 
<a href="/search/cs?searchtype=author&query=Piechottka%2C+U">Uwe Piechottka</a>, 
<a href="/search/cs?searchtype=author&query=Hoeflinger%2C+F">Fabian Hoeflinger</a>, 
<a href="/search/cs?searchtype=author&query=Gosala%2C+N">Nikhil Gosala</a>, 
<a href="/search/cs?searchtype=author&query=Wetzel%2C+N">Niklas Wetzel</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%BCscher%2C+D">Daniel B&#xfc;scher</a>, 
<a href="/search/cs?searchtype=author&query=Valada%2C+A">Abhinav Valada</a>, 
<a href="/search/cs?searchtype=author&query=Burgard%2C+W">Wolfram Burgard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted for publication in IEEE Sensors Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Automated and autonomous industrial inspection is a longstanding research
field, driven by the necessity to enhance safety and efficiency within
industrial settings. In addressing this need, we introduce an autonomously
navigating robotic system designed for comprehensive plant inspection. This
innovative system comprises a robotic platform equipped with a diverse array of
sensors integrated to facilitate the detection of various process and
infrastructure parameters. These sensors encompass optical (LiDAR, Stereo,
UV/IR/RGB cameras), olfactory (electronic nose), and acoustic (microphone
array) capabilities, enabling the identification of factors such as methane
leaks, flow rates, and infrastructural anomalies. The proposed system underwent
individual evaluation at a wastewater treatment site within a chemical plant,
providing a practical and challenging environment for testing. The evaluation
process encompassed key aspects such as object detection, 3D localization, and
path planning. Furthermore, specific evaluations were conducted for optical
methane leak detection and localization, as well as acoustic assessments
focusing on pump equipment and gas leak localization.
</p>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07692" title="Abstract">arXiv:2402.07692</a> [<a href="/pdf/2402.07692" title="Download PDF">pdf</a>, <a href="/format/2402.07692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boundary Exploration for Bayesian Optimization With Unknown Physical  Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yunsheng Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zuniga%2C+A">Ane Zuniga</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%BCrholt%2C+J+P">Johannes P. D&#xfc;rholt</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+P">Payel Das</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Matusik%2C+W">Wojciech Matusik</a>, 
<a href="/search/cs?searchtype=author&query=Lukovi%C4%87%2C+M+K">Mina Konakovi&#x107; Lukovi&#x107;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Bayesian optimization has been successfully applied to optimize black-box
functions where the number of evaluations is severely limited. However, in many
real-world applications, it is hard or impossible to know in advance which
designs are feasible due to some physical or system limitations. These issues
lead to an even more challenging problem of optimizing an unknown function with
unknown constraints. In this paper, we observe that in such scenarios optimal
solution typically lies on the boundary between feasible and infeasible regions
of the design space, making it considerably more difficult than that with
interior optima. Inspired by this observation, we propose BE-CBO, a new
Bayesian optimization method that efficiently explores the boundary between
feasible and infeasible designs. To identify the boundary, we learn the
constraints with an ensemble of neural networks that outperform the standard
Gaussian Processes for capturing complex boundaries. Our method demonstrates
superior performance against state-of-the-art methods through comprehensive
experiments on synthetic and real-world benchmarks.
</p>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07693" title="Abstract">arXiv:2402.07693</a> [<a href="/pdf/2402.07693" title="Download PDF">pdf</a>, <a href="/ps/2402.07693" title="Download PostScript">ps</a>, <a href="/format/2402.07693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LFOC+: A Fair OS-level Cache-Clustering Policy for Commodity Multicore  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saez%2C+J+C">Juan Carlos Saez</a>, 
<a href="/search/cs?searchtype=author&query=Castro%2C+F">Fernando Castro</a>, 
<a href="/search/cs?searchtype=author&query=Fanizzi%2C+G">Graziano Fanizzi</a>, 
<a href="/search/cs?searchtype=author&query=Prieto-Matias%2C+M">Manuel Prieto-Matias</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Computers, 71(8), pp. 1952-1967, 1 Aug. 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Commodity multicore systems are increasingly adopting hardware support that
enables the system software to partition the last-level cache (LLC). This
support makes it possible for the operating system (OS) or the Virtual Machine
Monitor (VMM) to mitigate shared-resource contention effects on multicores by
assigning different co-running applications to various cache partitions.
Recently cache-clustering (or partition-sharing) strategies have emerged as a
way to improve system throughput and fairness on new platforms with
cache-partitioning support. As opposed to strict cache-partitioning, which
allocates separate cache partitions to each application, cache-clustering
allows partitions to be shared by a group of applications.
<br />In this article we propose LFOC+, a fairness-aware OS-level cache-clustering
policy for commodity multicore systems. LFOC+ tries to mimic the behavior of
the optimal cache-clustering solution for fairness, which we could obtain for
different workload scenarios by using a simulation tool. Our dynamic
cache-clustering strategy continuously gathers data from performance monitoring
counters to classify applications at runtime based on the degree of cache
sensitivity and contentiousness, and effectively separates cache-sensitive
applications from aggressor programs to improve fairness, while providing
acceptable system throughput.
<br />We implemented LFOC+ in the Linux kernel and evaluated it on a real system
featuring an Intel Skylake processor, where we compare its effectiveness to
that of four previously proposed cache-clustering policies. Our experimental
analysis reveals that LFOC+ constitutes a lightweight OS-level policy and
improves fairness relative to two other state-of-the-art fairness-aware
strategies --Dunn and LFOC--, by up to 22\% and up to 20.6\%, respectively, and
by 9\% and 4.9\% on average.
</p>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07696" title="Abstract">arXiv:2402.07696</a> [<a href="/pdf/2402.07696" title="Download PDF">pdf</a>, <a href="/format/2402.07696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthesizing Strongly Equivalent Logic Programs: Beth Definability for  Answer Set Programs via Craig Interpolation in First-Order Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heuer%2C+J">Jan Heuer</a>, 
<a href="/search/cs?searchtype=author&query=Wernhard%2C+C">Christoph Wernhard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">We show a projective Beth definability theorem for logic programs under the
stable model semantics: For given programs $P$ and $Q$ and vocabulary $V$ (set
of predicates) the existence of a program $R$ in $V$ such that $P \cup R$ and
$P \cup Q$ are strongly equivalent can be expressed as a first-order
entailment. Moreover, our result is effective: A program $R$ can be constructed
from a Craig interpolant for this entailment, using a known first-order
encoding for testing strong equivalence, which we apply in reverse to extract
programs from formulas. As a further perspective, this allows transforming
logic programs via transforming their first-order encodings. In a prototypical
implementation, the Craig interpolation is performed by first-order provers
based on clausal tableaux or resolution calculi. Our work shows how
definability and interpolation, which underlie modern logic-based approaches to
advanced tasks in knowledge representation, transfer to answer set programming.
</p>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07703" title="Abstract">arXiv:2402.07703</a> [<a href="/pdf/2402.07703" title="Download PDF">pdf</a>, <a href="/format/2402.07703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Sequential Decision-Making with Unknown Delays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+P">Ping Wu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heyan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengyang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the field of online sequential decision-making, we address the problem
with delays utilizing the framework of online convex optimization (OCO), where
the feedback of a decision can arrive with an unknown delay. Unlike previous
research that is limited to Euclidean norm and gradient information, we propose
three families of delayed algorithms based on approximate solutions to handle
different types of received feedback. Our proposed algorithms are versatile and
applicable to universal norms. Specifically, we introduce a family of Follow
the Delayed Regularized Leader algorithms for feedback with full information on
the loss function, a family of Delayed Mirror Descent algorithms for feedback
with gradient information on the loss function and a family of Simplified
Delayed Mirror Descent algorithms for feedback with the value information of
the loss function's gradients at corresponding decision points. For each type
of algorithm, we provide corresponding regret bounds under cases of general
convexity and relative strong convexity, respectively. We also demonstrate the
efficiency of each algorithm under different norms through concrete examples.
Furthermore, our theoretical results are consistent with the current best
bounds when degenerated to standard settings.
</p>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07708" title="Abstract">arXiv:2402.07708</a> [<a href="/pdf/2402.07708" title="Download PDF">pdf</a>, <a href="/ps/2402.07708" title="Download PostScript">ps</a>, <a href="/format/2402.07708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Signed Distance Field based Segmentation and Statistical Shape Modelling  of the Left Atrial Appendage
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Juhl%2C+K+A">Kristine Aavild Juhl</a>, 
<a href="/search/cs?searchtype=author&query=Slipsager%2C+J">Jakob Slipsager</a>, 
<a href="/search/cs?searchtype=author&query=de+Backer%2C+O">Ole de Backer</a>, 
<a href="/search/cs?searchtype=author&query=Kofoed%2C+K">Klaus Kofoed</a>, 
<a href="/search/cs?searchtype=author&query=Camara%2C+O">Oscar Camara</a>, 
<a href="/search/cs?searchtype=author&query=Paulsen%2C+R">Rasmus Paulsen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Onsubmitted paper from 2019
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Patients with atrial fibrillation have a 5-7 fold increased risk of having an
ischemic stroke. In these cases, the most common site of thrombus localization
is inside the left atrial appendage (LAA) and studies have shown a correlation
between the LAA shape and the risk of ischemic stroke. These studies make use
of manual measurement and qualitative assessment of shape and are therefore
prone to large inter-observer discrepancies, which may explain the
contradictions between the conclusions in different studies. We argue that
quantitative shape descriptors are necessary to robustly characterize LAA
morphology and relate to other functional parameters and stroke risk.
<br />Deep Learning methods are becoming standardly available for segmenting
cardiovascular structures from high resolution images such as computed
tomography (CT), but only few have been tested for LAA segmentation.
Furthermore, the majority of segmentation algorithms produces non-smooth 3D
models that are not ideal for further processing, such as statistical shape
analysis or computational fluid modelling. In this paper we present a fully
automatic pipeline for image segmentation, mesh model creation and statistical
shape modelling of the LAA. The LAA anatomy is implicitly represented as a
signed distance field (SDF), which is directly regressed from the CT image
using Deep Learning. The SDF is further used for registering the LAA shapes to
a common template and build a statistical shape model (SSM). Based on 106
automatically segmented LAAs, the built SSM reveals that the LAA shape can be
quantified using approximately 5 PCA modes and allows the identification of two
distinct shape clusters corresponding to the so-called chicken-wing and
non-chicken-wing morphologies.
</p>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07710" title="Abstract">arXiv:2402.07710</a> [<a href="/pdf/2402.07710" title="Download PDF">pdf</a>, <a href="/format/2402.07710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimization of Sparse Convolution for 3D-Point Cloud on GPUs with CUDA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+C">Chester Luo</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+K">Kevin Lai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">In recent years, there has been a significant increase in the utilization of
deep learning methods, particularly convolutional neural networks (CNNs), which
have emerged as the dominant approach in various domains that involve
structured grid data, such as picture analysis and processing. Nevertheless,
the exponential growth in the utilization of LiDAR and 3D sensors across many
domains has resulted in an increased need for the analysis of 3D point clouds.
The utilization of 3D point clouds is crucial in various applications,
including object recognition and segmentation, as they offer a spatial
depiction of things within a three-dimensional environment. In contrast to
photos, point clouds exhibit sparsity and lack a regular grid, hence posing
distinct processing and computational issues.
</p>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07711" title="Abstract">arXiv:2402.07711</a> [<a href="/pdf/2402.07711" title="Download PDF">pdf</a>, <a href="/ps/2402.07711" title="Download PostScript">ps</a>, <a href="/format/2402.07711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Near optimal constructions of frameproof codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Miao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zengjiao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Shangguan%2C+C">Chong Shangguan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Happy Chinese new year, the year of Loong; 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">Frameproof codes are a class of secure codes that were originally introduced
in the pioneering work of Boneh and Shaw in the context of digital
fingerprinting. They can be used to enhance the security and credibility of
digital content. Let $M_{c,l}(q)$ denote the largest cardinality of a $q$-ary
$c$-frameproof code with length $l$. Based on an intriguing observation that
relates $M_{c,l}(q)$ to the renowned Erd\H{o}s Matching Conjecture in extremal
set theory, in 2003, Blackburn posed an open problem on the precise value of
the limit $R_{c,l}=\lim_{q\rightarrow\infty}\frac{M_{c,l}(q)}{q^{\lceil l/c
\rceil}}$. By combining several ideas from the probabilistic method, we present
a lower bound for $M_{c,l}(q)$, which, together with an upper bound of
Blackburn, completely determines $R_{c,l}$ for {\it all} fixed $c,l$, and
resolves the above open problem in the full generality. We also present an
improved upper bound for $M_{c,l}(q)$.
</p>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07712" title="Abstract">arXiv:2402.07712</a> [<a href="/pdf/2402.07712" title="Download PDF">pdf</a>, <a href="/format/2402.07712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Collapse Demystified: The Case of Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dohmatob%2C+E">Elvis Dohmatob</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yunzhen Feng</a>, 
<a href="/search/cs?searchtype=author&query=Kempe%2C+J">Julia Kempe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">In the era of large language models like ChatGPT, the phenomenon of "model
collapse" refers to the situation whereby as a model is trained recursively on
data generated from previous generations of itself over time, its performance
degrades until the model eventually becomes completely useless, i.e the model
collapses. In this work, we study this phenomenon in the simplified setting of
kernel regression and obtain results which show a clear crossover between where
the model can cope with fake data, and a regime where the model's performance
completely collapses. Under polynomial decaying spectral and source conditions,
we obtain modified scaling laws which exhibit new crossover phenomena from fast
to slow rates. We also propose a simple strategy based on adaptive
regularization to mitigate model collapse. Our theoretical results are
validated with experiments.
</p>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07714" title="Abstract">arXiv:2402.07714</a> [<a href="/pdf/2402.07714" title="Download PDF">pdf</a>, <a href="/ps/2402.07714" title="Download PostScript">ps</a>, <a href="/format/2402.07714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Artificial Immune Networks for Mitigating DoS flooding Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vidal%2C+J+M">Jorge Maestre Vidal</a>, 
<a href="/search/cs?searchtype=author&query=Orozco%2C+A+L+S">Ana Lucila Sandoval Orozco</a>, 
<a href="/search/cs?searchtype=author&query=Villalba%2C+L+J+G">Luis Javier Garc&#xed;a Villalba</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> J. Maestre Vidal, A. L. Sandoval Orozco, L. J. Garc\'ia Villalba:
  Adaptive Artificial Immune Networks for Mitigating DoS Flooding Attacks.
  Swarm and Evolutionary Computation. Vol. 38, pp. 3894-108, February 2018
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Denial of service attacks pose a threat in constant growth. This is mainly
due to their tendency to gain in sophistication, ease of implementation,
obfuscation and the recent improvements in occultation of fingerprints. On the
other hand, progress towards self-organizing networks, and the different
techniques involved in their development, such as software-defined networking,
network-function virtualization, artificial intelligence or cloud computing,
facilitates the design of new defensive strategies, more complete, consistent
and able to adapt the defensive deployment to the current status of the
network. In order to contribute to their development, in this paper, the use of
artificial immune systems to mitigate denial of service attacks is proposed.
The approach is based on building networks of distributed sensors suited to the
requirements of the monitored environment. These components are capable of
identifying threats and reacting according to the behavior of the biological
defense mechanisms in human beings. It is accomplished by emulating the
different immune reactions, the establishment of quarantine areas and the
construction of immune memory. For their assessment, experiments with public
domain datasets (KDD'99, CAIDA'07 and CAIDA'08) and simulations on various
network configurations based on traffic samples gathered by the University
Complutense of Madrid and flooding attacks generated by the tool DDoSIM were
performed.
</p>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07718" title="Abstract">arXiv:2402.07718</a> [<a href="/pdf/2402.07718" title="Download PDF">pdf</a>, <a href="/format/2402.07718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local Centrality Minimization with Quality Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miyauchi%2C+A">Atsushi Miyauchi</a>, 
<a href="/search/cs?searchtype=author&query=Severini%2C+L">Lorenzo Severini</a>, 
<a href="/search/cs?searchtype=author&query=Bonchi%2C+F">Francesco Bonchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to The Web Conference 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Centrality measures, quantifying the importance of vertices or edges, play a
fundamental role in network analysis. To date, triggered by some positive
approximability results, a large body of work has been devoted to studying
centrality maximization, where the goal is to maximize the centrality score of
a target vertex by manipulating the structure of a given network. On the other
hand, due to the lack of such results, only very little attention has been paid
to centrality minimization, despite its practical usefulness.
<br />In this study, we introduce a novel optimization model for local centrality
minimization, where the manipulation is allowed only around the target vertex.
We prove the NP-hardness of our model and that the most intuitive greedy
algorithm has a quite limited performance in terms of approximation ratio. Then
we design two effective approximation algorithms: The first algorithm is a
highly-scalable algorithm that has an approximation ratio unachievable by the
greedy algorithm, while the second algorithm is a bicriteria approximation
algorithm that solves a continuous relaxation based on the Lov\'asz extension,
using a projected subgradient method. To the best of our knowledge, ours are
the first polynomial-time algorithms with provable approximation guarantees for
centrality minimization. Experiments using a variety of real-world networks
demonstrate the effectiveness of our proposed algorithms: Our first algorithm
is applicable to million-scale graphs and obtains much better solutions than
those of scalable baselines, while our second algorithm is rather strong
against adversarial instances.
</p>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07720" title="Abstract">arXiv:2402.07720</a> [<a href="/pdf/2402.07720" title="Download PDF">pdf</a>, <a href="/ps/2402.07720" title="Download PostScript">ps</a>, <a href="/format/2402.07720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interaction-Based Driving Scenario Classification and Labeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+C">Cheng Chang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiawei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+J">Jingwei Ge</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zuo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+J">Junqing Wei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Li Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Scenario data play a vital role in autonomous driving related researches, and
it is essential to obtain refined descriptions and labels to extract and index
scenarios with different types of interactions. However, existing methods
cannot cope well with the problem of scenario classification and comparison
with vehicle interactions as the core. In this paper, we propose a framework
for interaction-based refined scenario classification and labeling. Based on
the summarized basic types of vehicle interactions, we slice scenario data
stream into a series of scenario segments via spatiotemporal scenario evolution
tree. The scenario segment statistics of many published scenario datasets are
further analyzed. We also propose the scenario metric Graph-DTW based on Graph
Computation Tree and Dynamic Time Warping to conduct refined scenario
comparison and labeling. The extreme interactive scenarios and corner cases can
be efficiently filtered and extracted. Moreover, testing examples on trajectory
prediction model demonstrate the effectiveness and advantages of scenario
labeling and the proposed metric. The overall framework can provide solid
support for the usage and indexing of scenario data.
</p>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07721" title="Abstract">arXiv:2402.07721</a> [<a href="/pdf/2402.07721" title="Download PDF">pdf</a>, <a href="/format/2402.07721" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hongyun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Xiangyu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Conghui Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tiejun Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Low-Rank Adaptation (LoRA) introduces auxiliary parameters for each layer to
fine-tune the pre-trained model under limited computing resources. But it still
faces challenges of resource consumption when scaling up to larger models.
Previous studies employ pruning techniques by evaluating the importance of LoRA
parameters for different layers to address the problem. However, these efforts
only analyzed parameter features to evaluate their importance. Indeed, the
output of LoRA related to the parameters and data is the factor that directly
impacts the frozen model. To this end, we propose LoRA-drop which evaluates the
importance of the parameters by analyzing the LoRA output. We retain LoRA for
important layers and the LoRA of the other layers share the same parameters.
Abundant experiments on NLU and NLG tasks demonstrate the effectiveness of
LoRA-drop.
</p>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07726" title="Abstract">arXiv:2402.07726</a> [<a href="/pdf/2402.07726" title="Download PDF">pdf</a>, <a href="/format/2402.07726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Sign Language Translation and Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhengsheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhiwei He</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+W">Wenxiang Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kehai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zhaopeng Tu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Motivated by the success of unsupervised neural machine translation (UNMT),
we introduce an unsupervised sign language translation and generation network
(USLNet), which learns from abundant single-modality (text and video) data
without parallel sign language data. USLNet comprises two main components:
single-modality reconstruction modules (text and video) that rebuild the input
from its noisy version in the same modality and cross-modality back-translation
modules (text-video-text and video-text-video) that reconstruct the input from
its noisy version in the different modality using back-translation
procedure.Unlike the single-modality back-translation procedure in text-based
UNMT, USLNet faces the cross-modality discrepancy in feature representation, in
which the length and the feature dimension mismatch between text and video
sequences. We propose a sliding window method to address the issues of aligning
variable-length text with video sequences. To our knowledge, USLNet is the
first unsupervised sign language translation and generation model capable of
generating both natural language text and sign language video in a unified
manner. Experimental results on the BBC-Oxford Sign Language dataset (BOBSL)
and Open-Domain American Sign Language dataset (OpenASL) reveal that USLNet
achieves competitive results compared to supervised baseline models, indicating
its effectiveness in sign language translation and generation.
</p>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07732" title="Abstract">arXiv:2402.07732</a> [<a href="/pdf/2402.07732" title="Download PDF">pdf</a>, <a href="/format/2402.07732" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pattern Matching with Mismatches and Wildcards
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bathie%2C+G">Gabriel Bathie</a>, 
<a href="/search/cs?searchtype=author&query=Charalampopoulos%2C+P">Panagiotis Charalampopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Starikovskaya%2C+T">Tatiana Starikovskaya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">In this work, we address the problem of approximate pattern matching with
wildcards. Given a pattern $P$ of length $m$ containing $D$ wildcards, a text
$T$ of length $n$, and an integer $k$, our objective is to identify all
fragments of $T$ within Hamming distance $k$ from $P$.
<br />Our primary contribution is an algorithm with runtime $O(n+(D+k)(G+k)\cdot
n/m)$ for this problem. Here, $G \le D$ represents the number of maximal
wildcard fragments in $P$. We derive this algorithm by elaborating in a
non-trivial way on the ideas presented by [Charalampopoulos et al., FOCS'20]
for pattern matching with mismatches (without wildcards). Our algorithm
improves over the state of the art when $D$, $G$, and $k$ are small relative to
$n$. For instance, if $m = n/2$, $k=G=n^{2/5}$, and $D=n^{3/5}$, our algorithm
operates in $O(n)$ time, surpassing the $\Omega(n^{6/5})$ time requirement of
all previously known algorithms.
<br />In the case of exact pattern matching with wildcards ($k=0$), we present a
much simpler algorithm with runtime $O(n+DG\cdot n/m)$ that clearly illustrates
our main technical innovation: the utilisation of positions of $P$ that do not
belong to any fragment of $P$ with a density of wildcards much larger than
$D/m$ as anchors for the sought (approximate) occurrences. Notably, our
algorithm outperforms the best-known $O(n\log m)$-time FFT-based algorithms of
[Cole and Hariharan, STOC'02] and [Clifford and Clifford, IPL'04] if $DG =
o(m\log m)$.
<br />We complement our algorithmic results with a structural characterization of
the $k$-mismatch occurrences of $P$. We demonstrate that in a text of length
$O(m)$, these occurrences can be partitioned into $O((D+k)(G+k))$ arithmetic
progressions. Additionally, we construct an infinite family of examples with
$\Omega((D+k)k)$ arithmetic progressions of occurrences, leveraging a
combinatorial result on progression-free sets [Elkin, SODA'10].
</p>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07736" title="Abstract">arXiv:2402.07736</a> [<a href="/pdf/2402.07736" title="Download PDF">pdf</a>, <a href="/ps/2402.07736" title="Download PostScript">ps</a>, <a href="/format/2402.07736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Learned Sparse Retrieval for Image Suggestion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Thong Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Hendriksen%2C+M">Mariya Hendriksen</a>, 
<a href="/search/cs?searchtype=author&query=Yates%2C+A">Andrew Yates</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, TREC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Learned Sparse Retrieval (LSR) is a group of neural methods designed to
encode queries and documents into sparse lexical vectors. These vectors can be
efficiently indexed and retrieved using an inverted index. While LSR has shown
promise in text retrieval, its potential in multi-modal retrieval remains
largely unexplored. Motivated by this, in this work, we explore the application
of LSR in the multi-modal domain, i.e., we focus on Multi-Modal Learned Sparse
Retrieval (MLSR). We conduct experiments using several MLSR model
configurations and evaluate the performance on the image suggestion task. We
find that solving the task solely based on the image content is challenging.
Enriching the image content with its caption improves the model performance
significantly, implying the importance of image captions to provide
fine-grained concepts and context information of images. Our approach presents
a practical and effective solution for training LSR retrieval models in
multi-modal settings.
</p>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07738" title="Abstract">arXiv:2402.07738</a> [<a href="/pdf/2402.07738" title="Download PDF">pdf</a>, <a href="/format/2402.07738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal link predictor by In-context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+K">Kaiwen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+H">Haitao Mao</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhichun Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chawla%2C+N+V">Nitesh V. Chawla</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Link prediction is a crucial task in graph machine learning, where the goal
is to infer missing or future links within a graph. Traditional approaches
leverage heuristic methods based on widely observed connectivity patterns,
offering broad applicability and generalizability without the need for model
training. Despite their utility, these methods are limited by their reliance on
human-derived heuristics and lack the adaptability of data-driven approaches.
Conversely, parametric link predictors excel in automatically learning the
connectivity patterns from data and achieving state-of-the-art but fail short
to directly transfer across different graphs. Instead, it requires the cost of
extensive training and hyperparameter optimization to adapt to the target
graph. In this work, we introduce the Universal Link Predictor (UniLP), a novel
model that combines the generalizability of heuristic approaches with the
pattern learning capabilities of parametric models. UniLP is designed to
autonomously identify connectivity patterns across diverse graphs, ready for
immediate application to any unseen graph dataset without targeted training. We
address the challenge of conflicting connectivity patterns-arising from the
unique distributions of different graphs-through the implementation of
In-context Learning (ICL). This approach allows UniLP to dynamically adjust to
various target graphs based on contextual demonstrations, thereby avoiding
negative transfer. Through rigorous experimentation, we demonstrate UniLP's
effectiveness in adapting to new, unseen graphs at test time, showcasing its
ability to perform comparably or even outperform parametric models that have
been finetuned for specific datasets. Our findings highlight UniLP's potential
to set a new standard in link prediction, combining the strengths of heuristic
and parametric methods in a single, versatile framework.
</p>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07739" title="Abstract">arXiv:2402.07739</a> [<a href="/pdf/2402.07739" title="Download PDF">pdf</a>, <a href="/format/2402.07739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Task-conditioned adaptation of visual features in multi-task policy  learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marza%2C+P">Pierre Marza</a>, 
<a href="/search/cs?searchtype=author&query=Matignon%2C+L">Laetitia Matignon</a>, 
<a href="/search/cs?searchtype=author&query=Simonin%2C+O">Olivier Simonin</a>, 
<a href="/search/cs?searchtype=author&query=Wolf%2C+C">Christian Wolf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Successfully addressing a wide variety of tasks is a core ability of
autonomous agents, which requires flexibly adapting the underlying
decision-making strategies and, as we argue in this work, also adapting the
underlying perception modules. An analogical argument would be the human visual
system, which uses top-down signals to focus attention determined by the
current task. Similarly, in this work, we adapt pre-trained large vision models
conditioned on specific downstream tasks in the context of multi-task policy
learning. We introduce task-conditioned adapters that do not require finetuning
any pre-trained weights, combined with a single policy trained with behavior
cloning and capable of addressing multiple tasks. We condition the policy and
visual adapters on task embeddings, which can be selected at inference if the
task is known, or alternatively inferred from a set of example demonstrations.
To this end, we propose a new optimization-based estimator. We evaluate the
method on a wide variety of tasks of the CortexBench benchmark and show that,
compared to existing work, it can be addressed with a single policy. In
particular, we demonstrate that adapting visual features is a key design choice
and that the method generalizes to unseen tasks given visual demonstrations.
</p>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07742" title="Abstract">arXiv:2402.07742</a> [<a href="/pdf/2402.07742" title="Download PDF">pdf</a>, <a href="/format/2402.07742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asking Multimodal Clarifying Questions in Mixed-Initiative  Conversational Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yifei Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Siro%2C+C">Clemencia Siro</a>, 
<a href="/search/cs?searchtype=author&query=Aliannejadi%2C+M">Mohammad Aliannejadi</a>, 
<a href="/search/cs?searchtype=author&query=de+Rijke%2C+M">Maarten de Rijke</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+W">Wai Lam</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to WWW24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In mixed-initiative conversational search systems, clarifying questions are
used to help users who struggle to express their intentions in a single query.
These questions aim to uncover user's information needs and resolve query
ambiguities. We hypothesize that in scenarios where multimodal information is
pertinent, the clarification process can be improved by using non-textual
information. Therefore, we propose to add images to clarifying questions and
formulate the novel task of asking multimodal clarifying questions in
open-domain, mixed-initiative conversational search systems. To facilitate
research into this task, we collect a dataset named Melon that contains over 4k
multimodal clarifying questions, enriched with over 14k images. We also propose
a multimodal query clarification model named Marto and adopt a prompt-based,
generative fine-tuning strategy to perform the training of different stages
with different prompts. Several analyses are conducted to understand the
importance of multimodal contents during the query clarification phase.
Experimental results indicate that the addition of images leads to significant
improvements of up to 90% in retrieval performance when selecting the relevant
images. Extensive analyses are also performed to show the superiority of Marto
compared with discriminative baselines in terms of effectiveness and
efficiency.
</p>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07744" title="Abstract">arXiv:2402.07744</a> [<a href="/pdf/2402.07744" title="Download PDF">pdf</a>, <a href="/format/2402.07744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Unified Alignment Between Agents, Humans, and Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zonghan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">An Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zijun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kaiming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+F">Fangzhou Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yile Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zeyuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qingyuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinrui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhenhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+F">Fuwen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhicheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peng Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project webpage: <a href="https://agent-force.github.io/unified-alignment-for-agents.html">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">The rapid progress of foundation models has led to the prosperity of
autonomous agents, which leverage the universal capabilities of foundation
models to conduct reasoning, decision-making, and environmental interaction.
However, the efficacy of agents remains limited when operating in intricate,
realistic environments. In this work, we introduce the principles of
$\mathbf{U}$nified $\mathbf{A}$lignment for $\mathbf{A}$gents
($\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with
human intentions, environmental dynamics, and self-constraints such as the
limitation of monetary budgets. From the perspective of $\mathbf{UA}^2$, we
review the current agent research and highlight the neglected factors in
existing agent benchmarks and method candidates. We also conduct
proof-of-concept studies by introducing realistic features to WebShop,
including user profiles to demonstrate intentions, personalized reranking for
complex environmental dynamics, and runtime cost statistics to reflect
self-constraints. We then follow the principles of $\mathbf{UA}^2$ to propose
an initial design of our agent, and benchmark its performance with several
candidate baselines in the retrofitted WebShop. The extensive experimental
results further prove the importance of the principles of $\mathbf{UA}^2$. Our
research sheds light on the next steps of autonomous agent research with
improved general problem-solving abilities.
</p>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07745" title="Abstract">arXiv:2402.07745</a> [<a href="/pdf/2402.07745" title="Download PDF">pdf</a>, <a href="/format/2402.07745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predictive Churn with the Set of Good Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Watson-Daniels%2C+J">Jamelle Watson-Daniels</a>, 
<a href="/search/cs?searchtype=author&query=Calmon%2C+F+d+P">Flavio du Pin Calmon</a>, 
<a href="/search/cs?searchtype=author&query=D%27Amour%2C+A">Alexander D&#x27;Amour</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+C">Carol Long</a>, 
<a href="/search/cs?searchtype=author&query=Parkes%2C+D+C">David C. Parkes</a>, 
<a href="/search/cs?searchtype=author&query=Ustun%2C+B">Berk Ustun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Machine learning models in modern mass-market applications are often updated
over time. One of the foremost challenges faced is that, despite increasing
overall performance, these updates may flip specific model predictions in
unpredictable ways. In practice, researchers quantify the number of unstable
predictions between models pre and post update -- i.e., predictive churn. In
this paper, we study this effect through the lens of predictive multiplicity --
i.e., the prevalence of conflicting predictions over the set of near-optimal
models (the Rashomon set). We show how traditional measures of predictive
multiplicity can be used to examine expected churn over this set of prospective
models -- i.e., the set of models that may be used to replace a baseline model
in deployment. We present theoretical results on the expected churn between
models within the Rashomon set from different perspectives. And we characterize
expected churn over model updates via the Rashomon set, pairing our analysis
with empirical results on real-world datasets -- showing how our approach can
be used to better anticipate, reduce, and avoid churn in consumer-facing
applications. Further, we show that our approach is useful even for models
enhanced with uncertainty awareness.
</p>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07749" title="Abstract">arXiv:2402.07749</a> [<a href="/pdf/2402.07749" title="Download PDF">pdf</a>, <a href="/ps/2402.07749" title="Download PostScript">ps</a>, <a href="/format/2402.07749" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymptotically compatible schemes for nonlinear variational models via  Gamma-convergence and applications to nonlocal problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Du%2C+Q">Qiang Du</a>, 
<a href="/search/math?searchtype=author&query=Scott%2C+J+M">James M. Scott</a>, 
<a href="/search/math?searchtype=author&query=Tian%2C+X">Xiaochuan Tian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We present a study on asymptotically compatible Galerkin discretizations for
a class of parametrized nonlinear variational problems. The abstract analytical
framework is based on variational convergence, or Gamma-convergence. We
demonstrate the broad applicability of the theoretical framework by developing
asymptotically compatible finite element discretizations of some representative
nonlinear nonlocal variational problems on a bounded domain. These include
nonlocal nonlinear problems with classically-defined, local boundary
constraints through heterogeneous localization at the boundary, as well as
nonlocal problems posed on parameter-dependent domains.
</p>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07752" title="Abstract">arXiv:2402.07752</a> [<a href="/pdf/2402.07752" title="Download PDF">pdf</a>, <a href="/format/2402.07752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixed Q-Functionals: Advancing Value-Based Methods in Cooperative MARL  with Continuous Action Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Findik%2C+Y">Yasin Findik</a>, 
<a href="/search/cs?searchtype=author&query=Ahmadzadeh%2C+S+R">S. Reza Ahmadzadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">Tackling multi-agent learning problems efficiently is a challenging task in
continuous action domains. While value-based algorithms excel in sample
efficiency when applied to discrete action domains, they are usually
inefficient when dealing with continuous actions. Policy-based algorithms, on
the other hand, attempt to address this challenge by leveraging critic networks
for guiding the learning process and stabilizing the gradient estimation. The
limitations in the estimation of true return and falling into local optima in
these methods result in inefficient and often sub-optimal policies. In this
paper, we diverge from the trend of further enhancing critic networks, and
focus on improving the effectiveness of value-based methods in multi-agent
continuous domains by concurrently evaluating numerous actions. We propose a
novel multi-agent value-based algorithm, Mixed Q-Functionals (MQF), inspired
from the idea of Q-Functionals, that enables agents to transform their states
into basis functions. Our algorithm fosters collaboration among agents by
mixing their action-values. We evaluate the efficacy of our algorithm in six
cooperative multi-agent scenarios. Our empirical findings reveal that MQF
outperforms four variants of Deep Deterministic Policy Gradient through rapid
action evaluation and increased sample efficiency.
</p>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07753" title="Abstract">arXiv:2402.07753</a> [<a href="/pdf/2402.07753" title="Download PDF">pdf</a>, <a href="/format/2402.07753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Engineering Weighted Connectivity Augmentation Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Faraj%2C+M+F">Marcelo Fonseca Faraj</a>, 
<a href="/search/cs?searchtype=author&query=Gro%C3%9Fmann%2C+E">Ernestine Gro&#xdf;mann</a>, 
<a href="/search/cs?searchtype=author&query=Joos%2C+F">Felix Joos</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%B6ller%2C+T">Thomas M&#xf6;ller</a>, 
<a href="/search/cs?searchtype=author&query=Schulz%2C+C">Christian Schulz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Increasing the connectivity of a graph is a pivotal challenge in robust
network design. The weighted connectivity augmentation problem is a common
version of the problem that takes link costs into consideration. The problem is
then to find a minimum cost subset of a given set of weighted links that
increases the connectivity of a graph by one when the links are added to the
edge set of the input instance. In this work, we give a first implementation of
recently discovered better-than-2 approximations. Furthermore, we propose three
new heuristic and one exact approach. These include a greedy algorithm
considering link costs and the number of unique cuts covered, an approach based
on minimum spanning trees and a local search algorithm that may improve a given
solution by swapping links of paths. Our exact approach uses an ILP formulation
with efficient cut enumeration as well as a fast initialization routine. We
then perform an extensive experimental evaluation which shows that our
algorithms are faster and yield the best solutions compared to the current
state-of-the-art as well as the recently discovered better-than-2 approximation
algorithms. Our novel local search algorithm can improve solution quality even
further.
</p>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07754" title="Abstract">arXiv:2402.07754</a> [<a href="/pdf/2402.07754" title="Download PDF">pdf</a>, <a href="/format/2402.07754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jiacheng Ye</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+S">Shansan Gong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Lin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jiahui Gao</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Han Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+W">Wei Bi</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+L">Lingpeng Kong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Diffusion models have gained attention in text processing, offering many
potential advantages over traditional autoregressive models. This work explores
the integration of diffusion models and Chain-of-Thought (CoT), a
well-established technique to improve the reasoning ability in autoregressive
language models. We propose Diffusion-of-Thought (DoT), allowing reasoning
steps to diffuse over time through the diffusion process. In contrast to
traditional autoregressive language models that make decisions in a
left-to-right, token-by-token manner, DoT offers more flexibility in the
trade-off between computation and reasoning performance. Our experimental
results demonstrate the effectiveness of DoT in multi-digit multiplication and
grade school math problems. Additionally, DoT showcases promising
self-correction abilities and benefits from existing reasoning-enhancing
techniques like self-consistency decoding. Our findings contribute to the
understanding and development of reasoning capabilities in diffusion language
models.
</p>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07757" title="Abstract">arXiv:2402.07757</a> [<a href="/pdf/2402.07757" title="Download PDF">pdf</a>, <a href="/format/2402.07757" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards an Understanding of Stepwise Inference in Transformers: A  Synthetic Graph Navigation Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khona%2C+M">Mikail Khona</a>, 
<a href="/search/cs?searchtype=author&query=Okawa%2C+M">Maya Okawa</a>, 
<a href="/search/cs?searchtype=author&query=Hula%2C+J">Jan Hula</a>, 
<a href="/search/cs?searchtype=author&query=Ramesh%2C+R">Rahul Ramesh</a>, 
<a href="/search/cs?searchtype=author&query=Nishi%2C+K">Kento Nishi</a>, 
<a href="/search/cs?searchtype=author&query=Dick%2C+R">Robert Dick</a>, 
<a href="/search/cs?searchtype=author&query=Lubana%2C+E+S">Ekdeep Singh Lubana</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+H">Hidenori Tanaka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Stepwise inference protocols, such as scratchpads and chain-of-thought, help
language models solve complex problems by decomposing them into a sequence of
simpler subproblems. Despite the significant gain in performance achieved via
these protocols, the underlying mechanisms of stepwise inference have remained
elusive. To address this, we propose to study autoregressive Transformer models
on a synthetic task that embodies the multi-step nature of problems where
stepwise inference is generally most useful. Specifically, we define a graph
navigation problem wherein a model is tasked with traversing a path from a
start to a goal node on the graph. Despite is simplicity, we find we can
empirically reproduce and analyze several phenomena observed at scale: (i) the
stepwise inference reasoning gap, the cause of which we find in the structure
of the training data; (ii) a diversity-accuracy tradeoff in model generations
as sampling temperature varies; (iii) a simplicity bias in the model's output;
and (iv) compositional generalization and a primacy bias with in-context
exemplars. Overall, our work introduces a grounded, synthetic framework for
studying stepwise inference and offers mechanistic hypotheses that can lay the
foundation for a deeper understanding of this phenomenon.
</p>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07767" title="Abstract">arXiv:2402.07767</a> [<a href="/pdf/2402.07767" title="Download PDF">pdf</a>, <a href="/ps/2402.07767" title="Download PostScript">ps</a>, <a href="/format/2402.07767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text Detoxification as Style Transfer in English and Hindi
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+S">Sourabrata Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+A">Akanksha Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Ojha%2C+A+K">Atul Kr. Ojha</a>, 
<a href="/search/cs?searchtype=author&query=McCrae%2C+J+P">John P. McCrae</a>, 
<a href="/search/cs?searchtype=author&query=Du%C5%A1ek%2C+O">Ond&#x159;ej Du&#x161;ek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted and presented at the 20th International Conference on Natural Language Processing (ICON-2023) during December 14-17, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper focuses on text detoxification, i.e., automatically converting
toxic text into non-toxic text. This task contributes to safer and more
respectful online communication and can be considered a Text Style Transfer
(TST) task, where the text style changes while its content is preserved. We
present three approaches: knowledge transfer from a similar task, multi-task
learning approach, combining sequence-to-sequence modeling with various
toxicity classification tasks, and, delete and reconstruct approach. To support
our research, we utilize a dataset provided by Dementieva et al.(2021), which
contains multiple versions of detoxified texts corresponding to toxic texts. In
our experiments, we selected the best variants through expert human annotators,
creating a dataset where each toxic sentence is paired with a single,
appropriate detoxified version. Additionally, we introduced a small Hindi
parallel dataset, aligning with a part of the English dataset, suitable for
evaluation purposes. Our results demonstrate that our approach effectively
balances text detoxication while preserving the actual content and maintaining
fluency.
</p>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07770" title="Abstract">arXiv:2402.07770</a> [<a href="/pdf/2402.07770" title="Download PDF">pdf</a>, <a href="/format/2402.07770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantitative knowledge retrieval from large language models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Selby%2C+D">David Selby</a>, 
<a href="/search/cs?searchtype=author&query=Spriestersbach%2C+K">Kai Spriestersbach</a>, 
<a href="/search/cs?searchtype=author&query=Iwashita%2C+Y">Yuichiro Iwashita</a>, 
<a href="/search/cs?searchtype=author&query=Bappert%2C+D">Dennis Bappert</a>, 
<a href="/search/cs?searchtype=author&query=Warrier%2C+A">Archana Warrier</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+S">Sumantrak Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Asim%2C+M+N">Muhammad Nabeel Asim</a>, 
<a href="/search/cs?searchtype=author&query=Kise%2C+K">Koichi Kise</a>, 
<a href="/search/cs?searchtype=author&query=Vollmer%2C+S">Sebastian Vollmer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages plus supplementary materials
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL); Applications (stat.AP)

</div>
<p class="mathjax">Large language models (LLMs) have been extensively studied for their
abilities to generate convincing natural language sequences, however their
utility for quantitative information retrieval is less well understood. In this
paper we explore the feasibility of LLMs as a mechanism for quantitative
knowledge retrieval to aid data analysis tasks such as elicitation of prior
distributions for Bayesian models and imputation of missing data. We present a
prompt engineering framework, treating an LLM as an interface to a latent space
of scientific literature, comparing responses in different contexts and domains
against more established approaches. Implications and challenges of using LLMs
as 'experts' are discussed.
</p>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07771" title="Abstract">arXiv:2402.07771</a> [<a href="/pdf/2402.07771" title="Download PDF">pdf</a>, <a href="/format/2402.07771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Insights into $(k,&#x3c1;)$-shortcutting algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leonhardt%2C+A">Alexander Leonhardt</a>, 
<a href="/search/cs?searchtype=author&query=Meyer%2C+U">Ulrich Meyer</a>, 
<a href="/search/cs?searchtype=author&query=Penschuck%2C+M">Manuel Penschuck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">A graph is called a $(k,\rho)$-graph iff every node can reach $\rho$ of its
nearest neighbors in at most k hops. This property proved useful in the
analysis and design of parallel shortest-path algorithms. Any graph can be
transformed into a $(k,\rho)$-graph by adding shortcuts. Formally, the
$(k,\rho)$-Minimum-Shortcut problem asks to find an appropriate shortcut set of
minimal cardinality.
<br />We show that the $(k,\rho)$-Minimum-Shortcut problem is NP-complete in the
practical regime of $k \ge 3$ and $\rho = \Theta(n^\epsilon)$ for $\epsilon &gt;
0$. With a related construction, we bound the approximation factor of known
$(k,\rho)$-Minimum-Shortcut problem heuristics from below and propose
algorithmic countermeasures improving the approximation quality. Further, we
describe an integer linear problem (ILP) solving the
$(k,\rho)$-Minimum-Shortcut problem optimally. Finally, we compare the
practical performance and quality of all algorithms in an empirical campaign.
</p>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07772" title="Abstract">arXiv:2402.07772</a> [<a href="/pdf/2402.07772" title="Download PDF">pdf</a>, <a href="/format/2402.07772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-End Learning for Fair Multiobjective Optimization Under  Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dinh%2C+M+H">My H Dinh</a>, 
<a href="/search/cs?searchtype=author&query=Kotary%2C+J">James Kotary</a>, 
<a href="/search/cs?searchtype=author&query=Fioretto%2C+F">Ferdinando Fioretto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Many decision processes in artificial intelligence and operations research
are modeled by parametric optimization problems whose defining parameters are
unknown and must be inferred from observable data. The Predict-Then-Optimize
(PtO) paradigm in machine learning aims to maximize downstream decision quality
by training the parametric inference model end-to-end with the subsequent
constrained optimization. This requires backpropagation through the
optimization problem using approximation techniques specific to the problem's
form, especially for nondifferentiable linear and mixed-integer programs. This
paper extends the PtO methodology to optimization problems with
nondifferentiable Ordered Weighted Averaging (OWA) objectives, known for their
ability to ensure properties of fairness and robustness in decision models.
Through a collection of training techniques and proposed application settings,
it shows how optimization of OWA functions can be effectively integrated with
parametric prediction for fair and robust optimization under uncertainty.
</p>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07775" title="Abstract">arXiv:2402.07775</a> [<a href="/pdf/2402.07775" title="Download PDF">pdf</a>, <a href="/format/2402.07775" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Growth Rate of the Number of Empty Triangles in the Plane
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+B+B">Bhaswar B. Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Sandip Das</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+S+S">Sk Samim Islam</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+S">Saumya Sen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>

</div>
<p class="mathjax">Given a set $P$ of $n$ points in the plane, in general position, denote by
$N_\Delta(P)$ the number of empty triangles with vertices in $P$. In this paper
we investigate by how much $N_\Delta(P)$ changes if a point $x$ is removed from
$P$. By constructing a graph $G_P(x)$ based on the arrangement of the empty
triangles incident on $x$, we transform this geometric problem to the problem
of counting triangles in the graph $G_P(x)$. We study properties of the graph
$G_P(x)$ and, in particular, show that it is kite-free. This relates the growth
rate of the number of empty triangles to the famous Ruzsa-Szemer\'edi problem.
</p>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07776" title="Abstract">arXiv:2402.07776</a> [<a href="/pdf/2402.07776" title="Download PDF">pdf</a>, <a href="/format/2402.07776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TELLER: A Trustworthy Framework for Explainable, Generalizable and  Controllable Fake News Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenya Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoru Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoliang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 2 figures, 16 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The proliferation of fake news has emerged as a severe societal problem,
raising significant interest from industry and academia. While existing
deep-learning based methods have made progress in detecting fake news
accurately, their reliability may be compromised caused by the non-transparent
reasoning processes, poor generalization abilities and inherent risks of
integration with large language models (LLMs). To address this challenge, we
propose {\methodname}, a novel framework for trustworthy fake news detection
that prioritizes explainability, generalizability and controllability of
models. This is achieved via a dual-system framework that integrates cognition
and decision systems, adhering to the principles above. The cognition system
harnesses human expertise to generate logical predicates, which guide LLMs in
generating human-readable logic atoms. Meanwhile, the decision system deduces
generalizable logic rules to aggregate these atoms, enabling the identification
of the truthfulness of the input news across diverse domains and enhancing
transparency in the decision-making process. Finally, we present comprehensive
evaluation results on four datasets, demonstrating the feasibility and
trustworthiness of our proposed framework. Our implementation is available at
\url{https://github.com/less-and-less-bugs/Trust_TELLER}.
</p>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07777" title="Abstract">arXiv:2402.07777</a> [<a href="/pdf/2402.07777" title="Download PDF">pdf</a>, <a href="/format/2402.07777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Novel Low-Complexity Model Development for Li-ion Cells Using Online  Impedance Measurement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kulkarni%2C+A">Abhijit Kulkarni</a>, 
<a href="/search/eess?searchtype=author&query=Nadeem%2C+A">Ahsan Nadeem</a>, 
<a href="/search/eess?searchtype=author&query=Di+Fonso%2C+R">Roberta Di Fonso</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+Y">Yusheng Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Teodorescu%2C+R">Remus Teodorescu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Modeling of Li-ion cells is used in battery management systems (BMS) to
determine key states such as state-of-charge (SoC), state-of-health (SoH), etc.
Accurate models are also useful in developing a cell-level digital-twin that
can be used for protection and diagnostics in the BMS. In this paper, a
low-complexity model development is proposed based on the equivalent circuit
model (ECM) of the Li-ion cells. The proposed approach uses online impedance
measurement at discrete frequencies to derive the ECM that matches closely with
the results from the electro-impedance spectroscopy (EIS). The proposed method
is suitable to be implemented in a microcontroller with low-computational
power, typically used in BMS. Practical design guidelines are proposed to
ensure fast and accurate model development. Using the proposed method to
enhance the functions of a typical automotive BMS is described. Experimental
validation is performed using large prismatic cells and small-capacity
cylindrical cells. Root-mean-square error (RMSE) of less than 3\% is observed
for a wide variation of operating conditions.
</p>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07778" title="Abstract">arXiv:2402.07778</a> [<a href="/pdf/2402.07778" title="Download PDF">pdf</a>, <a href="/ps/2402.07778" title="Download PostScript">ps</a>, <a href="/format/2402.07778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algorithmic Fairness and Color-blind Racism: Navigating the Intersection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Watson-Daniels%2C+J">Jamelle Watson-Daniels</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Our focus lies at the intersection between two broader research perspectives:
(1) the scientific study of algorithms and (2) the scholarship on race and
racism. Many streams of research related to algorithmic fairness have been born
out of interest at this intersection. We think about this intersection as the
product of work derived from both sides. From (1) algorithms to (2) racism, the
starting place might be an algorithmic question or method connected to a
conceptualization of racism. On the other hand, from (2) racism to (1)
algorithms, the starting place could be recognizing a setting where a legacy of
racism is known to persist and drawing connections between that legacy and the
introduction of algorithms into this setting. In either direction, meaningful
disconnection can occur when conducting research at the intersection of racism
and algorithms. The present paper urges collective reflection on research
directions at this intersection. Despite being primarily motivated by instances
of racial bias, research in algorithmic fairness remains mostly disconnected
from scholarship on racism. In particular, there has not been an examination
connecting algorithmic fairness discussions directly to the ideology of
color-blind racism; we aim to fill this gap. We begin with a review of an
essential account of color-blind racism then we review racial discourse within
algorithmic fairness research and underline significant patterns, shifts and
disconnects. Ultimately, we argue that researchers can improve the navigation
of the landscape at the intersection by recognizing ideological shifts as such
and iteratively re-orienting towards maintaining meaningful connections across
interdisciplinary lines.
</p>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07781" title="Abstract">arXiv:2402.07781</a> [<a href="/pdf/2402.07781" title="Download PDF">pdf</a>, <a href="/format/2402.07781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IR-Aware ECO Timing Optimization Using Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chhabria%2C+V+A">Vidya A. Chhabria</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wenjing Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Sapatnekar%2C+S+S">Sachin S. Sapatnekar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Engineering change orders (ECOs) in late stages make minimal design fixes to
recover from timing shifts due to excessive IR drops. This paper integrates
IR-drop-aware timing analysis and ECO timing optimization using reinforcement
learning (RL). The method operates after physical design and power grid
synthesis, and rectifies IR-drop-induced timing degradation through gate
sizing. It incorporates the Lagrangian relaxation (LR) technique into a novel
RL framework, which trains a relational graph convolutional network (R-GCN)
agent to sequentially size gates to fix timing violations. The R-GCN agent
outperforms a classical LR-only algorithm: in an open 45nm technology, it (a)
moves the Pareto front of the delay-area tradeoff curve to the left and (b)
saves runtime over the classical method by running fast inference using trained
models at iso-quality. The RL model is transferable across timing
specifications, and transferable to unseen designs with zero-shot learning or
fine tuning.
</p>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07782" title="Abstract">arXiv:2402.07782</a> [<a href="/pdf/2402.07782" title="Download PDF">pdf</a>, <a href="/ps/2402.07782" title="Download PostScript">ps</a>, <a href="/format/2402.07782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving parameter-dependent semi-algebraic systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gaillard%2C+L">Louis Gaillard</a>, 
<a href="/search/cs?searchtype=author&query=Din%2C+M+S+E">Mohab Safey El Din</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Symbolic Computation (cs.SC)</span>; Algebraic Geometry (math.AG)

</div>
<p class="mathjax">We consider systems of polynomial equations and inequalities in
$\mathbb{Q}[\boldsymbol{y}][\boldsymbol{x}]$ where $\boldsymbol{x} = (x_1,
\ldots, x_n)$ and $\boldsymbol{y} = (y_1, \ldots,y_t)$. The $\boldsymbol{y}$
indeterminates are considered as parameters and we assume that when
specialising them generically, the set of common complex solutions, to the
obtained equations, is finite. We consider the problem of real root
classification for such parameter-dependent problems, i.e. identifying the
possible number of real solutions depending on the values of the parameters and
computing a description of the regions of the space of parameters over which
the number of real roots remains invariant.
<br />We design an algorithm for solving this problem. The formulas it outputs
enjoy a determinantal structure. Under genericity assumptions, we show that its
arithmetic complexity is polynomial in both the maximum degree $d$ and the
number $s$ of the input inequalities and exponential in $nt+t^2$. The output
formulas consist of polynomials of degree bounded by $(2s+n)d^{n+1}$. This is
the first algorithm with such a singly exponential complexity. We report on
practical experiments showing that a first implementation of this algorithm can
tackle examples which were previously out of reach.
</p>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07785" title="Abstract">arXiv:2402.07785</a> [<a href="/pdf/2402.07785" title="Download PDF">pdf</a>, <a href="/format/2402.07785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HYPO: Hyperspherical Out-of-Distribution Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+H">Haoyue Bai</a>, 
<a href="/search/cs?searchtype=author&query=Ming%2C+Y">Yifei Ming</a>, 
<a href="/search/cs?searchtype=author&query=Katz-Samuels%2C+J">Julian Katz-Samuels</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yixuan Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICLR 2024; First two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Out-of-distribution (OOD) generalization is critical for machine learning
models deployed in the real world. However, achieving this can be fundamentally
challenging, as it requires the ability to learn invariant features across
different domains or environments. In this paper, we propose a novel framework
HYPO (HYPerspherical OOD generalization) that provably learns domain-invariant
representations in a hyperspherical space. In particular, our hyperspherical
learning algorithm is guided by intra-class variation and inter-class
separation principles -- ensuring that features from the same class (across
different training domains) are closely aligned with their class prototypes,
while different class prototypes are maximally separated. We further provide
theoretical justifications on how our prototypical learning objective improves
the OOD generalization bound. Through extensive experiments on challenging OOD
benchmarks, we demonstrate that our approach outperforms competitive baselines
and achieves superior performance. Code is available at
https://github.com/deeplearning-wisc/hypo.
</p>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07787" title="Abstract">arXiv:2402.07787</a> [<a href="/pdf/2402.07787" title="Download PDF">pdf</a>, <a href="/format/2402.07787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extensible Multi-Granularity Fusion Network for Aspect-based Sentiment  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiaowei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiujuan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yu Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Aspect-based Sentiment Analysis (ABSA) evaluates sentiment expressions within
a text to comprehend sentiment information. Previous studies integrated
external knowledge, such as knowledge graphs, to enhance the semantic features
in ABSA models. Recent research has examined the use of Graph Neural Networks
(GNNs) on dependency and constituent trees for syntactic analysis. With the
ongoing development of ABSA, more innovative linguistic and structural features
are being incorporated (e.g. latent graph), but this also introduces complexity
and confusion. As of now, a scalable framework for integrating diverse
linguistic and structural features into ABSA does not exist. This paper
presents the Extensible Multi-Granularity Fusion (EMGF) network, which
integrates information from dependency and constituent syntactic, attention
semantic , and external knowledge graphs. EMGF, equipped with multi-anchor
triplet learning and orthogonal projection, efficiently harnesses the combined
potential of each granularity feature and their synergistic interactions,
resulting in a cumulative effect without additional computational expenses.
Experimental findings on SemEval 2014 and Twitter datasets confirm EMGF's
superiority over existing ABSA methods.
</p>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07788" title="Abstract">arXiv:2402.07788</a> [<a href="/pdf/2402.07788" title="Download PDF">pdf</a>, <a href="/format/2402.07788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Intent Attribute-Aware Text Matching in Searching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingzhe Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiuying Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+J">Jing Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qishen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Changsheng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+C">Chenchen Dai</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+J">Jinxiong Chang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhongyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guannan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Text matching systems have become a fundamental service in most searching
platforms. For instance, they are responsible for matching user queries to
relevant candidate items, or rewriting the user-input query to a pre-selected
high-performing one for a better search experience. In practice, both the
queries and items often contain multiple attributes, such as the category of
the item and the location mentioned in the query, which represent condensed key
information that is helpful for matching. However, most of the existing works
downplay the effectiveness of attributes by integrating them into text
representations as supplementary information. Hence, in this work, we focus on
exploring the relationship between the attributes from two sides. Since
attributes from two ends are often not aligned in terms of number and type, we
propose to exploit the benefit of attributes by multiple-intent modeling. The
intents extracted from attributes summarize the diverse needs of queries and
provide rich content of items, which are more refined and abstract, and can be
aligned for paired inputs. Concretely, we propose a multi-intent
attribute-aware matching model (MIM), which consists of three main components:
attribute-aware encoder, multi-intent modeling, and intent-aware matching. In
the attribute-aware encoder, the text and attributes are weighted and processed
through a scaled attention mechanism with regard to the attributes' importance.
Afterward, the multi-intent modeling extracts intents from two ends and aligns
them. Herein, we come up with a distribution loss to ensure the learned intents
are diverse but concentrated, and a kullback-leibler divergence loss that
aligns the learned intents. Finally, in the intent-aware matching, the intents
are evaluated by a self-supervised masking task, and then incorporated to
output the final matching result.
</p>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07790" title="Abstract">arXiv:2402.07790</a> [<a href="/pdf/2402.07790" title="Download PDF">pdf</a>, <a href="/format/2402.07790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Uncertainty to Precision: Enhancing Binary Classifier Performance  through Calibration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Machado%2C+A+F">Agathe Fernandes Machado</a>, 
<a href="/search/cs?searchtype=author&query=Charpentier%2C+A">Arthur Charpentier</a>, 
<a href="/search/cs?searchtype=author&query=Flachaire%2C+E">Emmanuel Flachaire</a>, 
<a href="/search/cs?searchtype=author&query=Gallic%2C+E">Ewen Gallic</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+F">Fran&#xe7;ois Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">The assessment of binary classifier performance traditionally centers on
discriminative ability using metrics, such as accuracy. However, these metrics
often disregard the model's inherent uncertainty, especially when dealing with
sensitive decision-making domains, such as finance or healthcare. Given that
model-predicted scores are commonly seen as event probabilities, calibration is
crucial for accurate interpretation. In our study, we analyze the sensitivity
of various calibration measures to score distortions and introduce a refined
metric, the Local Calibration Score. Comparing recalibration methods, we
advocate for local regressions, emphasizing their dual role as effective
recalibration tools and facilitators of smoother visualizations. We apply these
findings in a real-world scenario using Random Forest classifier and regressor
to predict credit default while simultaneously measuring calibration during
performance optimization.
</p>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07791" title="Abstract">arXiv:2402.07791</a> [<a href="/pdf/2402.07791" title="Download PDF">pdf</a>, <a href="/format/2402.07791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continuous Assurance of Autonomous Vehicle Behavior Through Machine  Learned Correctness Properties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Litton%2C+M">Matthew Litton</a>, 
<a href="/search/cs?searchtype=author&query=Drusinsky%2C+D">Doron Drusinsky</a>, 
<a href="/search/cs?searchtype=author&query=Michael%2C+J+B">James Bret Michael</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Correctness properties are critical to conducting verification and validation
on software systems, especially those cyberphysical systems whose functionality
changes frequently due to software updates, changes in the operating
environment, or newly learned behaviors. We detail a novel method to
automatically construct expressive, executable correctness properties in the
form of machine-learned correctness properties which can be used to ensure that
a system's behavior is correct with respect to its design and operating
requirements. We propose a method to bootstrap the creation of these
correctness properties using a novel simulation-based generation of training
and testing data using multiple extensions to the Cross Entropy algorithm for
search-based optimization. Then, we apply this method to a software-in-the-loop
evaluation of an autonomous vehicle to demonstrate that such models can assert
about important properties of multi-agent cyberphysical systems. We demonstrate
that this process brings the task of developing robust correctness properties
from the realm of formal methods experts into the domain of system developers
and engineers, and that machine-learned correctness properties are expressive
enough to capture the correct behavior of cyberphysical systems in their
complex environments. This advancement can provide evidence of dependability to
system designers and users, enhancing trust in the deployment of autonomous
vehicles and other intelligent transportation systems.
</p>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07792" title="Abstract">arXiv:2402.07792</a> [<a href="/pdf/2402.07792" title="Download PDF">pdf</a>, <a href="/format/2402.07792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empowering Federated Learning for Massive Models with NVIDIA FLARE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roth%2C+H+R">Holger R. Roth</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Ziyue Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+Y">Yuan-Ting Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Renduchintala%2C+A">Adithya Renduchintala</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+I">Isaac Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhihong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Y">Yuhong Wen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Sean Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+K">Kevin Lu</a>, 
<a href="/search/cs?searchtype=author&query=Kersten%2C+K">Kristopher Kersten</a>, 
<a href="/search/cs?searchtype=author&query=Ricketts%2C+C">Camir Ricketts</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Daguang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chester Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+A">Andrew Feng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">In the ever-evolving landscape of artificial intelligence (AI) and large
language models (LLMs), handling and leveraging data effectively has become a
critical challenge. Most state-of-the-art machine learning algorithms are
data-centric. However, as the lifeblood of model performance, necessary data
cannot always be centralized due to various factors such as privacy,
regulation, geopolitics, copyright issues, and the sheer effort required to
move vast datasets. In this paper, we explore how federated learning enabled by
NVIDIA FLARE can address these challenges with easy and scalable integration
capabilities, enabling parameter-efficient and full supervised fine-tuning of
LLMs for natural language processing and biopharmaceutical applications to
enhance their accuracy and robustness.
</p>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07797" title="Abstract">arXiv:2402.07797</a> [<a href="/pdf/2402.07797" title="Download PDF">pdf</a>, <a href="/format/2402.07797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing Nash Equilibria in Potential Games with Private Uncoupled  Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patris%2C+N">Nikolas Patris</a>, 
<a href="/search/cs?searchtype=author&query=Stavroulakis%2C+S">Stelios Stavroulakis</a>, 
<a href="/search/cs?searchtype=author&query=Kalogiannis%2C+F">Fivos Kalogiannis</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rose Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Panageas%2C+I">Ioannis Panageas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to appear in AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We consider the problem of computing Nash equilibria in potential games where
each player's strategy set is subject to private uncoupled constraints. This
scenario is frequently encountered in real-world applications like road network
congestion games where individual drivers adhere to personal budget and fuel
limitations. Despite the plethora of algorithms that efficiently compute Nash
equilibria (NE) in potential games, the domain of constrained potential games
remains largely unexplored. We introduce an algorithm that leverages the
Lagrangian formulation of NE. The algorithm is implemented independently by
each player and runs in polynomial time with respect to the approximation
error, the sum of the size of the action-spaces, and the game's inherent
parameters.
</p>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07799" title="Abstract">arXiv:2402.07799</a> [<a href="/pdf/2402.07799" title="Download PDF">pdf</a>, <a href="/format/2402.07799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalising Planning Environment Redesign
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pozanco%2C+A">Alberto Pozanco</a>, 
<a href="/search/cs?searchtype=author&query=Pereira%2C+R+F">Ramon Fraga Pereira</a>, 
<a href="/search/cs?searchtype=author&query=Borrajo%2C+D">Daniel Borrajo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted at AAAI'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">In Environment Design, one interested party seeks to affect another agent's
decisions by applying changes to the environment. Most research on planning
environment (re)design assumes the interested party's objective is to
facilitate the recognition of goals and plans, and search over the space of
environment modifications to find the minimal set of changes that simplify
those tasks and optimise a particular metric. This search space is usually
intractable, so existing approaches devise metric-dependent pruning techniques
for performing search more efficiently. This results in approaches that are not
able to generalise across different objectives and/or metrics. In this paper,
we argue that the interested party could have objectives and metrics that are
not necessarily related to recognising agents' goals or plans. Thus, to
generalise the task of Planning Environment Redesign, we develop a general
environment redesign approach that is metric-agnostic and leverages recent
research on top-quality planning to efficiently redesign planning environments
according to any interested party's objective and metric. Experiments over a
set of environment redesign benchmarks show that our general approach
outperforms existing approaches when using well-known metrics, such as
facilitating the recognition of goals, as well as its effectiveness when
solving environment redesign tasks that optimise a novel set of different
metrics.
</p>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07808" title="Abstract">arXiv:2402.07808</a> [<a href="/pdf/2402.07808" title="Download PDF">pdf</a>, <a href="/format/2402.07808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vetter%2C+J">Julius Vetter</a>, 
<a href="/search/cs?searchtype=author&query=Moss%2C+G">Guy Moss</a>, 
<a href="/search/cs?searchtype=author&query=Schr%C3%B6der%2C+C">Cornelius Schr&#xf6;der</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+R">Richard Gao</a>, 
<a href="/search/cs?searchtype=author&query=Macke%2C+J+H">Jakob H. Macke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Scientific modeling applications often require estimating a distribution of
parameters consistent with a dataset of observations - an inference task also
known as source distribution estimation. This problem can be ill-posed,
however, since many different source distributions might produce the same
distribution of data-consistent simulations. To make a principled choice among
many equally valid sources, we propose an approach which targets the maximum
entropy distribution, i.e., prioritizes retaining as much uncertainty as
possible. Our method is purely sample-based - leveraging the Sliced-Wasserstein
distance to measure the discrepancy between the dataset and simulations - and
thus suitable for simulators with intractable likelihoods. We benchmark our
method on several tasks, and show that it can recover source distributions with
substantially higher entropy without sacrificing the fidelity of the
simulations. Finally, to demonstrate the utility of our approach, we infer
source distributions for parameters of the Hodgkin-Huxley neuron model from
experimental datasets with thousands of measurements. In summary, we propose a
principled framework for inferring unique source distributions of scientific
simulator parameters while retaining as much uncertainty as possible.
</p>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07812" title="Abstract">arXiv:2402.07812</a> [<a href="/pdf/2402.07812" title="Download PDF">pdf</a>, <a href="/format/2402.07812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieval-Augmented Thought Process as Sequential Decision Making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pouplin%2C+T">Thomas Pouplin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Holt%2C+S">Samuel Holt</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Schaar%2C+M">Mihaela van der Schaar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated their strong ability to assist
people and show "sparks of intelligence". However, several open challenges
hinder their wider application: such as concerns over privacy, tendencies to
produce hallucinations, and difficulties in handling long contexts. In this
work, we address those challenges by introducing the Retrieval-Augmented
Thought Process (RATP). Given access to external knowledge, RATP formulates the
thought generation of LLMs as a multiple-step decision process. To optimize
such a thought process, RATP leverages Monte-Carlo Tree Search, and learns a
Q-value estimator that permits cost-efficient inference. In addressing the task
of question-answering with private data, where ethical and security concerns
limit LLM training methods, RATP achieves a 50% improvement over existing
in-context retrieval-augmented language models.
</p>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07814" title="Abstract">arXiv:2402.07814</a> [<a href="/pdf/2402.07814" title="Download PDF">pdf</a>, <a href="/format/2402.07814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PBADet: A One-Stage Anchor-Free Approach for Part-Body Association
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhongpai Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Huayi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Abhishek Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+M">Meng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Planche%2C+B">Benjamin Planche</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Terrence Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Ziyan Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The detection of human parts (e.g., hands, face) and their correct
association with individuals is an essential task, e.g., for ubiquitous
human-machine interfaces and action recognition. Traditional methods often
employ multi-stage processes, rely on cumbersome anchor-based systems, or do
not scale well to larger part sets. This paper presents PBADet, a novel
one-stage, anchor-free approach for part-body association detection. Building
upon the anchor-free object representation across multi-scale feature maps, we
introduce a singular part-to-body center offset that effectively encapsulates
the relationship between parts and their parent bodies. Our design is
inherently versatile and capable of managing multiple parts-to-body
associations without compromising on detection accuracy or robustness.
Comprehensive experiments on various datasets underscore the efficacy of our
approach, which not only outperforms existing state-of-the-art techniques but
also offers a more streamlined and efficient solution to the part-body
association challenge.
</p>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07817" title="Abstract">arXiv:2402.07817</a> [<a href="/pdf/2402.07817" title="Download PDF">pdf</a>, <a href="/format/2402.07817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Injecting Wiktionary to improve token-level contextual representations  using contrastive learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mosolova%2C+A">Anna Mosolova</a>, 
<a href="/search/cs?searchtype=author&query=Candito%2C+M">Marie Candito</a>, 
<a href="/search/cs?searchtype=author&query=Ramisch%2C+C">Carlos Ramisch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024 (Main)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While static word embeddings are blind to context, for lexical semantics
tasks context is rather too present in contextual word embeddings, vectors of
same-meaning occurrences being too different (Ethayarajh, 2019). Fine-tuning
pre-trained language models (PLMs) using contrastive learning was proposed,
leveraging automatically self-augmented examples (Liu et al., 2021b). In this
paper, we investigate how to inject a lexicon as an alternative source of
supervision, using the English Wiktionary. We also test how dimensionality
reduction impacts the resulting contextual word embeddings. We evaluate our
approach on the Word-In-Context (WiC) task, in the unsupervised setting (not
using the training set). We achieve new SoTA result on the original WiC test
set. We also propose two new WiC test sets for which we show that our
fine-tuning method achieves substantial improvements. We also observe
improvements, although modest, for the semantic frame induction task. Although
we experimented on English to allow comparison with related work, our method is
adaptable to the many languages for which large Wiktionaries exist.
</p>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07818" title="Abstract">arXiv:2402.07818</a> [<a href="/pdf/2402.07818" title="Download PDF">pdf</a>, <a href="/format/2402.07818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Zeroth-Order Methods for Scalable Large Language  Model Finetuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Z Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lou%2C+J">J Lou</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+W">W Bao</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Z">Z Qin</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+K">K Ren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Finetuning on task-specific datasets is a widely-embraced paradigm of
harnessing the powerful capability of pretrained LLMs for various downstream
tasks. Due to the popularity of LLMs finetuning and its accompanying privacy
concerns, differentially private (DP) finetuning of pretrained LLMs has
garnered increasing attention to safeguarding the privacy of task-specific
datasets. Lying at the design core of DP LLM finetuning methods is the
satisfactory tradeoff between privacy, utility, and scalability. Most existing
methods build upon the seminal work of DP-SGD. Despite pushing the scalability
of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately
limited by the inherent inefficiency of SGD. In this paper, we investigate the
potential of DP zeroth-order methods for LLM pretraining, which avoids the
scalability bottleneck of SGD by approximating the gradient with the more
efficient zeroth-order gradient. Rather than treating the zeroth-order method
as a drop-in replacement for SGD, this paper presents a comprehensive study
both theoretically and empirically. First, we propose the stagewise DP
zeroth-order method that dynamically schedules key hyperparameters. This design
is grounded on the synergy between DP random perturbation and the gradient
approximation error of the zeroth-order method, and its effect on finetuning
trajectory. Second, we further enhance the scalability by reducing the
trainable parameters that are identified by repurposing a data-free pruning
technique requiring no additional data or extra privacy budget. We provide
theoretical analysis for both proposed methods. We conduct extensive empirical
analysis on both encoder-only masked language model and decoder-only
autoregressive language model, achieving impressive results in terms of
scalability and utility.
</p>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07819" title="Abstract">arXiv:2402.07819</a> [<a href="/pdf/2402.07819" title="Download PDF">pdf</a>, <a href="/format/2402.07819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Benchmark Grocery Dataset of Realworld Point Clouds From Single View
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sheshappanavar%2C+S+V">Shivanand Venkanna Sheshappanavar</a>, 
<a href="/search/cs?searchtype=author&query=Anvekar%2C+T">Tejas Anvekar</a>, 
<a href="/search/cs?searchtype=author&query=Kundargi%2C+S">Shivanand Kundargi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yufan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kambhamettu%2C+C">Chandra Kambhamettu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Fine-grained grocery object recognition is an important computer vision
problem with broad applications in automatic checkout, in-store robotic
navigation, and assistive technologies for the visually impaired. Existing
datasets on groceries are mainly 2D images. Models trained on these datasets
are limited to learning features from the regular 2D grids. While portable 3D
sensors such as Kinect were commonly available for mobile phones, sensors such
as LiDAR and TrueDepth, have recently been integrated into mobile phones.
Despite the availability of mobile 3D sensors, there are currently no dedicated
real-world large-scale benchmark 3D datasets for grocery. In addition, existing
3D datasets lack fine-grained grocery categories and have limited training
samples. Furthermore, collecting data by going around the object versus the
traditional photo capture makes data collection cumbersome. Thus, we introduce
a large-scale grocery dataset called 3DGrocery100. It constitutes 100 classes,
with a total of 87,898 3D point clouds created from 10,755 RGB-D single-view
images. We benchmark our dataset on six recent state-of-the-art 3D point cloud
classification models. Additionally, we also benchmark the dataset on few-shot
and continual learning point cloud classification tasks. Project Page:
https://bigdatavision.org/3DGrocery100/.
</p>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07821" title="Abstract">arXiv:2402.07821</a> [<a href="/pdf/2402.07821" title="Download PDF">pdf</a>, <a href="/format/2402.07821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Computationally Efficient Multi-Class Calibration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gopalan%2C+P">Parikshit Gopalan</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+L">Lunjia Hu</a>, 
<a href="/search/cs?searchtype=author&query=Rothblum%2C+G+N">Guy N. Rothblum</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Complexity (cs.CC); Data Structures and Algorithms (cs.DS); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
<p class="mathjax">Consider a multi-class labelling problem, where the labels can take values in
$[k]$, and a predictor predicts a distribution over the labels. In this work,
we study the following foundational question: Are there notions of multi-class
calibration that give strong guarantees of meaningful predictions and can be
achieved in time and sample complexities polynomial in $k$? Prior notions of
calibration exhibit a tradeoff between computational efficiency and
expressivity: they either suffer from having sample complexity exponential in
$k$, or needing to solve computationally intractable problems, or give rather
weak guarantees.
<br />Our main contribution is a notion of calibration that achieves all these
desiderata: we formulate a robust notion of projected smooth calibration for
multi-class predictions, and give new recalibration algorithms for efficiently
calibrating predictors under this definition with complexity polynomial in $k$.
Projected smooth calibration gives strong guarantees for all downstream
decision makers who want to use the predictor for binary classification
problems of the form: does the label belong to a subset $T \subseteq [k]$: e.g.
is this an image of an animal? It ensures that the probabilities predicted by
summing the probabilities assigned to labels in $T$ are close to some perfectly
calibrated binary predictor for that task. We also show that natural
strengthenings of our definition are computationally hard to achieve: they run
into information theoretic barriers or computational intractability. Underlying
both our upper and lower bounds is a tight connection that we prove between
multi-class calibration and the well-studied problem of agnostic learning in
the (standard) binary prediction setting.
</p>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07822" title="Abstract">arXiv:2402.07822</a> [<a href="/pdf/2402.07822" title="Download PDF">pdf</a>, <a href="/format/2402.07822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding fitness landscapes in morpho-evolution via local optima  networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thomson%2C+S+L">Sarah L. Thomson</a>, 
<a href="/search/cs?searchtype=author&query=Goff%2C+L+K+L">L&#xe9;ni K. Le Goff</a>, 
<a href="/search/cs?searchtype=author&query=Hart%2C+E">Emma Hart</a>, 
<a href="/search/cs?searchtype=author&query=Buchanan%2C+E">Edgar Buchanan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to GECCO-2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Morpho-evolution (ME) refers to the simultaneous optimisation of a robot's
design and controller to maximise performance given a task and environment.
Many genetic encodings have been proposed which are capable of representing
design and control. Previous research has provided empirical comparisons
between encodings in terms of their performance with respect to an objective
function and the diversity of designs that are evaluated, however there has
been no attempt to explain the observed findings. We address this by applying
Local Optima Network (LON) analysis to investigate the structure of the fitness
landscapes induced by three different encodings when evolving a robot for a
locomotion task, shedding new light on the ease by which different fitness
landscapes can be traversed by a search process. This is the first time LON
analysis has been applied in the field of ME despite its popularity in
combinatorial optimisation domains; the findings will facilitate design of new
algorithms or operators that are customised to ME landscapes in the future.
</p>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07827" title="Abstract">arXiv:2402.07827</a> [<a href="/pdf/2402.07827" title="Download PDF">pdf</a>, <a href="/format/2402.07827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aya Model: An Instruction Finetuned Open-Access Multilingual Language  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C3%9Cst%C3%BCn%2C+A">Ahmet &#xdc;st&#xfc;n</a>, 
<a href="/search/cs?searchtype=author&query=Aryabumi%2C+V">Viraat Aryabumi</a>, 
<a href="/search/cs?searchtype=author&query=Yong%2C+Z">Zheng-Xin Yong</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+W">Wei-Yin Ko</a>, 
<a href="/search/cs?searchtype=author&query=D%27souza%2C+D">Daniel D&#x27;souza</a>, 
<a href="/search/cs?searchtype=author&query=Onilude%2C+G">Gbemileke Onilude</a>, 
<a href="/search/cs?searchtype=author&query=Bhandari%2C+N">Neel Bhandari</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Shivalika Singh</a>, 
<a href="/search/cs?searchtype=author&query=Ooi%2C+H">Hui-Lee Ooi</a>, 
<a href="/search/cs?searchtype=author&query=Kayid%2C+A">Amr Kayid</a>, 
<a href="/search/cs?searchtype=author&query=Vargus%2C+F">Freddie Vargus</a>, 
<a href="/search/cs?searchtype=author&query=Blunsom%2C+P">Phil Blunsom</a>, 
<a href="/search/cs?searchtype=author&query=Longpre%2C+S">Shayne Longpre</a>, 
<a href="/search/cs?searchtype=author&query=Muennighoff%2C+N">Niklas Muennighoff</a>, 
<a href="/search/cs?searchtype=author&query=Fadaee%2C+M">Marzieh Fadaee</a>, 
<a href="/search/cs?searchtype=author&query=Kreutzer%2C+J">Julia Kreutzer</a>, 
<a href="/search/cs?searchtype=author&query=Hooker%2C+S">Sara Hooker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent breakthroughs in large language models (LLMs) have centered around a
handful of data-rich languages. What does it take to broaden access to
breakthroughs beyond first-class citizen languages? Our work introduces Aya, a
massively multilingual generative language model that follows instructions in
101 languages of which over 50% are considered as lower-resourced. Aya
outperforms mT0 and BLOOMZ on the majority of tasks while covering double the
number of languages. We introduce extensive new evaluation suites that broaden
the state-of-art for multilingual eval across 99 languages -- including
discriminative and generative tasks, human evaluation, and simulated win rates
that cover both held-out tasks and in-distribution performance. Furthermore, we
conduct detailed investigations on the optimal finetuning mixture composition,
data pruning, as well as the toxicity, bias, and safety of our models. We
open-source our instruction datasets and our model at
https://hf.co/CohereForAI/aya-101
</p>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07832" title="Abstract">arXiv:2402.07832</a> [<a href="/pdf/2402.07832" title="Download PDF">pdf</a>, <a href="/ps/2402.07832" title="Download PostScript">ps</a>, <a href="/format/2402.07832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Best Practices for Facing the Security Challenges of Internet of Things  Devices Focusing on Software Development Life Cycle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+R">Md Rafid Islam</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+R">Ratun Rahman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">In the past few years, the number of IoT devices has grown substantially, and
this trend is likely to continue. An increasing amount of effort is being put
into developing software for the ever-increasing IoT devices. Every IoT system
at its core has software that enables the devices to function efficiently. But
security has always been a concern in this age of information and technology.
Security for IoT devices is now a top priority due to the growing number of
threats. This study introduces best practices for ensuring security in the IoT,
with an emphasis on guidelines to be utilized in software development for IoT
devices. The objective of the study is to raise awareness of potential threats,
emphasizing the secure software development lifecycle. The study will also
serve as a point of reference for future developments and provide a solid
foundation for securing IoT software and dealing with vulnerabilities.
</p>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07834" title="Abstract">arXiv:2402.07834</a> [<a href="/pdf/2402.07834" title="Download PDF">pdf</a>, <a href="/format/2402.07834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizing across Temporal Domains with Koopman Operators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Q">Qiuhao Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Fan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Gezheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+R">Ruizhi Pu</a>, 
<a href="/search/cs?searchtype=author&query=Shui%2C+C">Changjian Shui</a>, 
<a href="/search/cs?searchtype=author&query=Gagne%2C+C">Christian Gagne</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shichun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Boyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+C+X">Charles X. Ling</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 7 figures, Accepted by AAAI 2024. arXiv admin note: text overlap with <a href="/abs/2206.00047">arXiv:2206.00047</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In the field of domain generalization, the task of constructing a predictive
model capable of generalizing to a target domain without access to target data
remains challenging. This problem becomes further complicated when considering
evolving dynamics between domains. While various approaches have been proposed
to address this issue, a comprehensive understanding of the underlying
generalization theory is still lacking. In this study, we contribute novel
theoretic results that aligning conditional distribution leads to the reduction
of generalization bounds. Our analysis serves as a key motivation for solving
the Temporal Domain Generalization (TDG) problem through the application of
Koopman Neural Operators, resulting in Temporal Koopman Networks (TKNets). By
employing Koopman Operators, we effectively address the time-evolving
distributions encountered in TDG using the principles of Koopman theory, where
measurement functions are sought to establish linear transition relations
between evolving domains. Through empirical evaluations conducted on synthetic
and real-world datasets, we validate the effectiveness of our proposed
approach.
</p>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07839" title="Abstract">arXiv:2402.07839</a> [<a href="/pdf/2402.07839" title="Download PDF">pdf</a>, <a href="/format/2402.07839" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Meta-Pruning via Optimal Transport
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Theus%2C+A">Alexander Theus</a>, 
<a href="/search/cs?searchtype=author&query=Geimer%2C+O">Olin Geimer</a>, 
<a href="/search/cs?searchtype=author&query=Wicke%2C+F">Friedrich Wicke</a>, 
<a href="/search/cs?searchtype=author&query=Hofmann%2C+T">Thomas Hofmann</a>, 
<a href="/search/cs?searchtype=author&query=Anagnostidis%2C+S">Sotiris Anagnostidis</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S+P">Sidak Pal Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the International Conference on Learning Representations (ICLR) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Structural pruning of neural networks conventionally relies on identifying
and discarding less important neurons, a practice often resulting in
significant accuracy loss that necessitates subsequent fine-tuning efforts.
This paper introduces a novel approach named Intra-Fusion, challenging this
prevailing pruning paradigm. Unlike existing methods that focus on designing
meaningful neuron importance metrics, Intra-Fusion redefines the overlying
pruning procedure. Through utilizing the concepts of model fusion and Optimal
Transport, we leverage an agnostically given importance metric to arrive at a
more effective sparse model representation. Notably, our approach achieves
substantial accuracy recovery without the need for resource-intensive
fine-tuning, making it an efficient and promising tool for neural network
compression.
<br />Additionally, we explore how fusion can be added to the pruning process to
significantly decrease the training time while maintaining competitive
performance. We benchmark our results for various networks on commonly used
datasets such as CIFAR-10, CIFAR-100, and ImageNet. More broadly, we hope that
the proposed Intra-Fusion approach invigorates exploration into a fresh
alternative to the predominant compression approaches. Our code is available
here: https://github.com/alexandertheus/Intra-Fusion.
</p>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07841" title="Abstract">arXiv:2402.07841</a> [<a href="/pdf/2402.07841" title="Download PDF">pdf</a>, <a href="/format/2402.07841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Membership Inference Attacks Work on Large Language Models?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+M">Michael Duan</a>, 
<a href="/search/cs?searchtype=author&query=Suri%2C+A">Anshuman Suri</a>, 
<a href="/search/cs?searchtype=author&query=Mireshghallah%2C+N">Niloofar Mireshghallah</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+S">Sewon Min</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Weijia Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zettlemoyer%2C+L">Luke Zettlemoyer</a>, 
<a href="/search/cs?searchtype=author&query=Tsvetkov%2C+Y">Yulia Tsvetkov</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Evans%2C+D">David Evans</a>, 
<a href="/search/cs?searchtype=author&query=Hajishirzi%2C+H">Hannaneh Hajishirzi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Membership inference attacks (MIAs) attempt to predict whether a particular
datapoint is a member of a target model's training data. Despite extensive
research on traditional machine learning models, there has been limited work
studying MIA on the pre-training data of large language models (LLMs). We
perform a large-scale evaluation of MIAs over a suite of language models (LMs)
trained on the Pile, ranging from 160M to 12B parameters. We find that MIAs
barely outperform random guessing for most settings across varying LLM sizes
and domains. Our further analyses reveal that this poor performance can be
attributed to (1) the combination of a large dataset and few training
iterations, and (2) an inherently fuzzy boundary between members and
non-members. We identify specific settings where LLMs have been shown to be
vulnerable to membership inference and show that the apparent success in such
settings can be attributed to a distribution shift, such as when members and
non-members are drawn from the seemingly identical domain but with different
temporal ranges. We release our code and data as a unified benchmark package
that includes all existing MIAs, supporting future work.
</p>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07844" title="Abstract">arXiv:2402.07844</a> [<a href="/pdf/2402.07844" title="Download PDF">pdf</a>, <a href="/format/2402.07844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mercury: An Efficiency Benchmark for LLM Code Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+M">Mingzhe Du</a>, 
<a href="/search/cs?searchtype=author&query=Luu%2C+A+T">Anh Tuan Luu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+B">Bin Ji</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+S">See-Kiong Ng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Despite advancements in evaluating Large Language Models (LLMs) for code
synthesis, benchmarks have predominantly focused on functional correctness,
overlooking the importance of code efficiency. We present Mercury, the first
benchmark designated for assessing the code efficiency of LLM code synthesis
tasks. Mercury consists of 1,889 programming tasks covering diverse difficulty
levels alongside test case generators generating unlimited cases for
comprehensive evaluation. Unlike existing benchmarks, Mercury integrates a
novel metric Beyond@K to measure normalized code efficiency based on historical
submissions, leading to a new evaluation indicator for code synthesis, which
encourages generating functionally correct and computationally efficient code,
mirroring the real-world software development standard. Our findings reveal
that while LLMs demonstrate the remarkable capability to generate functionally
correct code, there still exists a substantial gap in their efficiency output,
underscoring a new frontier for LLM research and development.
</p>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07845" title="Abstract">arXiv:2402.07845</a> [<a href="/pdf/2402.07845" title="Download PDF">pdf</a>, <a href="/ps/2402.07845" title="Download PostScript">ps</a>, <a href="/format/2402.07845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Investigation into Using Unsupervised Metrics to Optimise GNNs for  Node Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leeney%2C+W">William Leeney</a>, 
<a href="/search/cs?searchtype=author&query=McConville%2C+R">Ryan McConville</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Graph Neural Networks (GNNs) can be trained to detect communities within a
graph by learning from the duality of feature and connectivity information.
Currently, the common approach for optimisation of GNNs is to use comparisons
to ground-truth for hyperparameter tuning and model selection. In this work, we
show that nodes can be clustered into communities with GNNs by solely
optimising for modularity, without any comparison to ground-truth. Although
modularity is a graph partitioning quality metric, we show that this can be
used to optimise GNNs that also encode features without a drop in performance.
We take it a step further and also study whether the unsupervised metric
performance can predict ground-truth performance. To investigate why modularity
can be used to optimise GNNs, we design synthetic experiments that show the
limitations of this approach. The synthetic graphs are created to highlight
current capabilities in distinct, random and zero information space partitions
in attributed graphs. We conclude that modularity can be used for
hyperparameter optimisation and model selection on real-world datasets as well
as being a suitable proxy for predicting ground-truth performance, however,
GNNs fail to balance the information duality when the spaces contain
conflicting signals.
</p>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07846" title="Abstract">arXiv:2402.07846</a> [<a href="/pdf/2402.07846" title="Download PDF">pdf</a>, <a href="/format/2402.07846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Modeling of Discrete Joint Distributions by E-Geodesic Flow  Matching on Assignment Manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boll%2C+B">Bastian Boll</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez-Alvarado%2C+D">Daniel Gonzalez-Alvarado</a>, 
<a href="/search/cs?searchtype=author&query=Schn%C3%B6rr%2C+C">Christoph Schn&#xf6;rr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">This paper introduces a novel generative model for discrete distributions
based on continuous normalizing flows on the submanifold of factorizing
discrete measures. Integration of the flow gradually assigns categories and
avoids issues of discretizing the latent continuous model like rounding, sample
truncation etc. General non-factorizing discrete distributions capable of
representing complex statistical dependencies of structured discrete data, can
be approximated by embedding the submanifold into a the meta-simplex of all
joint discrete distributions and data-driven averaging. Efficient training of
the generative model is demonstrated by matching the flow of geodesics of
factorizing discrete distributions. Various experiments underline the
approach's broad applicability.
</p>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07851" title="Abstract">arXiv:2402.07851</a> [<a href="/pdf/2402.07851" title="Download PDF">pdf</a>, <a href="/format/2402.07851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparing skill of historical rainfall data based monsoon rainfall  prediction in India with NCEP-NWP forecasts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Narula%2C+A">Apoorva Narula</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+A">Aastha Jain</a>, 
<a href="/search/cs?searchtype=author&query=Batra%2C+J">Jatin Batra</a>, 
<a href="/search/cs?searchtype=author&query=Juneja%2C+S">Sandeep Juneja</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this draft we consider the problem of forecasting rainfall across India
during the four monsoon months, one day as well as three days in advance. We
train neural networks using historical daily gridded precipitation data for
India obtained from IMD for the time period $1901- 2022$, at a spatial
resolution of $1^{\circ} \times 1^{\circ}$. This is compared with the numerical
weather prediction (NWP) forecasts obtained from NCEP (National Centre for
Environmental Prediction) available for the period 2011-2022. We conduct a
detailed country wide analysis and separately analyze some of the most
populated cities in India. Our conclusion is that forecasts obtained by
applying deep learning to historical rainfall data are more accurate compared
to NWP forecasts as well as predictions based on persistence. On average,
compared to our predictions, forecasts from NCEP-NWP model have about 34%
higher error for a single day prediction, and over 68% higher error for a three
day prediction. Similarly, persistence estimates report a 29% higher error in a
single day forecast, and over 54% error in a three day forecast. We further
observe that data up to 20 days in the past is useful in reducing errors of one
and three day forecasts, when a transformer based learning architecture, and to
a lesser extent when an LSTM is used. A key conclusion suggested by our
preliminary analysis is that NWP forecasts can be substantially improved upon
through more and diverse data relevant to monsoon prediction combined with
carefully selected neural network architecture.
</p>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07852" title="Abstract">arXiv:2402.07852</a> [<a href="/pdf/2402.07852" title="Download PDF">pdf</a>, <a href="/ps/2402.07852" title="Download PostScript">ps</a>, <a href="/format/2402.07852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Complexity of Algebraic Algorithms for LWE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Steiner%2C+M+J">Matthias Johann Steiner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Arora &amp; Ge introduced a noise-free polynomial system to compute the secret of
a Learning With Errors (LWE) instance via linearization. Albrecht et al. later
utilized the Arora-Ge polynomial model to study the complexity of Gr\"obner
basis computations on LWE polynomial systems under the assumption of
semi-regularity. In this paper we revisit the Arora-Ge polynomial and prove
that it satisfies a genericity condition recently introduced by Caminata &amp;
Gorla, called being in generic coordinates. For polynomial systems in generic
coordinates one can always estimate the complexity of DRL Gr\"obner basis
computations in terms of the Castelnuovo-Mumford regularity and henceforth also
via the Macaulay bound.
<br />Moreover, we generalize the Gr\"obner basis algorithm of Semaev &amp; Tenti to
arbitrary polynomial systems with a finite degree of regularity. In particular,
existence of this algorithm yields another approach to estimate the complexity
of DRL Gr\"obner basis computations in terms of the degree of regularity. In
practice, the degree of regularity of LWE polynomial systems is not known,
though one can always estimate the lowest achievable degree of regularity.
Consequently, from a designer's worst case perspective this approach yields
sub-exponential complexity estimates for general, binary secret, and binary
error LWE.
<br />In recent works by Dachman-Soled et al. the hardness of LWE in the presence
of side information was analyzed. Utilizing their framework we discuss how
hints can be incorporated into LWE polynomial systems and how they affect the
complexity of Gr\"obner basis computations.
</p>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07858" title="Abstract">arXiv:2402.07858</a> [<a href="/pdf/2402.07858" title="Download PDF">pdf</a>, <a href="/format/2402.07858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiscale Neuroimaging Features for the Identification of Medication  Class and Non-Responders in Mood Disorder Treatment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baker%2C+B+T">Bradley T. Baker</a>, 
<a href="/search/cs?searchtype=author&query=Salman%2C+M+S">Mustafa S. Salman</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Z">Zening Fu</a>, 
<a href="/search/cs?searchtype=author&query=Iraji%2C+A">Armin Iraji</a>, 
<a href="/search/cs?searchtype=author&query=Osuch%2C+E">Elizabeth Osuch</a>, 
<a href="/search/cs?searchtype=author&query=Bockholt%2C+J">Jeremy Bockholt</a>, 
<a href="/search/cs?searchtype=author&query=Calhoun%2C+V+D">Vince D. Calhoun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In the clinical treatment of mood disorders, the complex behavioral symptoms
presented by patients and variability of patient response to particular
medication classes can create difficulties in providing fast and reliable
treatment when standard diagnostic and prescription methods are used.
Increasingly, the incorporation of physiological information such as
neuroimaging scans and derivatives into the clinical process promises to
alleviate some of the uncertainty surrounding this process. Particularly, if
neural features can help to identify patients who may not respond to standard
courses of anti-depressants or mood stabilizers, clinicians may elect to avoid
lengthy and side-effect-laden treatments and seek out a different, more
effective course that might otherwise not have been under consideration.
Previously, approaches for the derivation of relevant neuroimaging features
work at only one scale in the data, potentially limiting the depth of
information available for clinical decision support. In this work, we show that
the utilization of multi spatial scale neuroimaging features - particularly
resting state functional networks and functional network connectivity measures
- provide a rich and robust basis for the identification of relevant medication
class and non-responders in the treatment of mood disorders. We demonstrate
that the generated features, along with a novel approach for fast and automated
feature selection, can support high accuracy rates in the identification of
medication class and non-responders as well as the identification of novel,
multi-scale biomarkers.
</p>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07859" title="Abstract">arXiv:2402.07859</a> [<a href="/pdf/2402.07859" title="Download PDF">pdf</a>, <a href="/format/2402.07859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lissard: Long and Simple Sequential Reasoning Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bueno%2C+M">Mirelle Bueno</a>, 
<a href="/search/cs?searchtype=author&query=Lotufo%2C+R">Roberto Lotufo</a>, 
<a href="/search/cs?searchtype=author&query=Nogueira%2C+R">Rodrigo Nogueira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Language models are now capable of solving tasks that require dealing with
long sequences consisting of hundreds of thousands of tokens. However, they
often fail on tasks that require repetitive use of simple rules, even on
sequences that are much shorter than those seen during training. For example,
state-of-the-art LLMs can find common items in two lists with up to 20 items
but fail when lists have 80 items. In this paper, we introduce Lissard, a
benchmark comprising seven tasks whose goal is to assess the ability of models
to process and generate wide-range sequence lengths, requiring repetitive
procedural execution. Our evaluation of open-source (Mistral-7B and
Mixtral-8x7B) and proprietary models (GPT-3.5 and GPT-4) show a consistent
decline in performance across all models as the complexity of the sequence
increases. The datasets and code are available at
https://github.com/unicamp-dl/Lissard
</p>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07860" title="Abstract">arXiv:2402.07860</a> [<a href="/pdf/2402.07860" title="Download PDF">pdf</a>, <a href="/format/2402.07860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Detection of Reviewer-Author Collusion Rings From Paper Bidding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jecmen%2C+S">Steven Jecmen</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+N+B">Nihar B. Shah</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+F">Fei Fang</a>, 
<a href="/search/cs?searchtype=author&query=Akoglu%2C+L">Leman Akoglu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">A major threat to the peer-review systems of computer science conferences is
the existence of "collusion rings" between reviewers. In such collusion rings,
reviewers who have also submitted their own papers to the conference work
together to manipulate the conference's paper assignment, with the aim of being
assigned to review each other's papers. The most straightforward way that
colluding reviewers can manipulate the paper assignment is by indicating their
interest in each other's papers through strategic paper bidding. One potential
approach to solve this important problem would be to detect the colluding
reviewers from their manipulated bids, after which the conference can take
appropriate action. While prior work has has developed effective techniques to
detect other kinds of fraud, no research has yet established that detecting
collusion rings is even possible. In this work, we tackle the question of
whether it is feasible to detect collusion rings from the paper bidding. To
answer this question, we conduct empirical analysis of two realistic conference
bidding datasets, including evaluations of existing algorithms for fraud
detection in other applications. We find that collusion rings can achieve
considerable success at manipulating the paper assignment while remaining
hidden from detection: for example, in one dataset, undetected colluders are
able to achieve assignment to up to 30% of the papers authored by other
colluders. In addition, when 10 colluders bid on all of each other's papers, no
detection algorithm outputs a group of reviewers with more than 31% overlap
with the true colluders. These results suggest that collusion cannot be
effectively detected from the bidding, demonstrating the need to develop more
complex detection algorithms that leverage additional metadata.
</p>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07862" title="Abstract">arXiv:2402.07862</a> [<a href="/pdf/2402.07862" title="Download PDF">pdf</a>, <a href="/format/2402.07862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI-Augmented Predictions: LLM Assistants Improve Human Forecasting  Accuracy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schoenegger%2C+P">Philipp Schoenegger</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+P+S">Peter S. Park</a>, 
<a href="/search/cs?searchtype=author&query=Karger%2C+E">Ezra Karger</a>, 
<a href="/search/cs?searchtype=author&query=Tetlock%2C+P+E">Philip E. Tetlock</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages (main text comprised of 15 pages, appendix comprised of three pages). 10 visualizations in the main text (four figures, six tables), three additional figures in the appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) show impressive capabilities, matching and
sometimes exceeding human performance in many domains. This study explores the
potential of LLMs to augment judgement in forecasting tasks. We evaluated the
impact on forecasting accuracy of two GPT-4-Turbo assistants: one designed to
provide high-quality advice ('superforecasting'), and the other designed to be
overconfident and base-rate-neglecting. Participants (N = 991) had the option
to consult their assigned LLM assistant throughout the study, in contrast to a
control group that used a less advanced model (DaVinci-003) without direct
forecasting support. Our preregistered analyses reveal that LLM augmentation
significantly enhances forecasting accuracy by 23% across both types of
assistants, compared to the control group. This improvement occurs despite the
superforecasting assistant's higher accuracy in predictions, indicating the
augmentation's benefit is not solely due to model prediction accuracy.
Exploratory analyses showed a pronounced effect in one forecasting item,
without which we find that the superforecasting assistant increased accuracy by
43%, compared with 28% for the biased assistant. We further examine whether LLM
augmentation disproportionately benefits less skilled forecasters, degrades the
wisdom-of-the-crowd by reducing prediction diversity, or varies in
effectiveness with question difficulty. Our findings do not consistently
support these hypotheses. Our results suggest that access to an LLM assistant,
even a biased one, can be a helpful decision aid in cognitively demanding tasks
where the answer is not known at the time of interaction.
</p>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07863" title="Abstract">arXiv:2402.07863</a> [<a href="/pdf/2402.07863" title="Download PDF">pdf</a>, <a href="/ps/2402.07863" title="Download PostScript">ps</a>, <a href="/format/2402.07863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An approximation algorithm for Maximum DiCut vs. Cut
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nakajima%2C+T">Tamio-Vesa Nakajima</a>, 
<a href="/search/cs?searchtype=author&query=%C5%BDivn%C3%BD%2C+S">Stanislav &#x17d;ivn&#xfd;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">Goemans and Williamson designed a 0.878-approximation algorithm for Max-Cut
in undirected graphs [JACM'95]. Khot, Kindler, Mosel, and O'Donnel showed that
the approximation ratio of the Goemans-Williamson algorithm is optimal assuming
Khot's Unique Games Conjecture [SICOMP'07]. In the problem of maximum cuts in
directed graphs (Max-DiCut), in which we seek as many edges going from one
particular side of the cut to the other, the situation is more complicated but
the recent work of Brakensiek, Huang, Potechin, and Zwick showed that their
0.874-approximation algorithm is tight under the Unique Games Conjecture (up to
a small delta)[FOCS'23].
<br />We consider a promise version of the problem and design an SDP-based
algorithm which, if given a directed graph G that has a directed cut of value
rho, finds an undirected cut in G (ignoring edge directions) with value at
least \rho.
</p>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07864" title="Abstract">arXiv:2402.07864</a> [<a href="/pdf/2402.07864" title="Download PDF">pdf</a>, <a href="/format/2402.07864" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cruising Queer HCI on the DL: A Literature Review of LGBTQ+ People in  HCI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taylor%2C+J">Jordan Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Simpson%2C+E">Ellen Simpson</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+A">Anh-Ton Tran</a>, 
<a href="/search/cs?searchtype=author&query=Brubaker%2C+J">Jed Brubaker</a>, 
<a href="/search/cs?searchtype=author&query=Fox%2C+S">Sarah Fox</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Haiyi Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">LGBTQ+ people have received increased attention in HCI research, paralleling
a greater emphasis on social justice in recent years. However, there has not
been a systematic review of how LGBTQ+ people are researched or discussed in
HCI. In this work, we review all research mentioning LGBTQ+ people across the
HCI venues of CHI, CSCW, DIS, and TOCHI. Since 2014, we find a linear growth in
the number of papers substantially about LGBTQ+ people and an exponential
increase in the number of mentions. Research about LGBTQ+ people tends to
center experiences of being politicized, outside the norm, stigmatized, or
highly vulnerable. LGBTQ+ people are typically mentioned as a marginalized
group or an area of future research. We identify gaps and opportunities for (1)
research about and (2) the discussion of LGBTQ+ in HCI and provide a dataset to
facilitate future Queer HCI research.
</p>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07865" title="Abstract">arXiv:2402.07865</a> [<a href="/pdf/2402.07865" title="Download PDF">pdf</a>, <a href="/format/2402.07865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prismatic VLMs: Investigating the Design Space of Visually-Conditioned  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karamcheti%2C+S">Siddharth Karamcheti</a>, 
<a href="/search/cs?searchtype=author&query=Nair%2C+S">Suraj Nair</a>, 
<a href="/search/cs?searchtype=author&query=Balakrishna%2C+A">Ashwin Balakrishna</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+P">Percy Liang</a>, 
<a href="/search/cs?searchtype=author&query=Kollar%2C+T">Thomas Kollar</a>, 
<a href="/search/cs?searchtype=author&query=Sadigh%2C+D">Dorsa Sadigh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 11 figures. Training code and models: <a href="https://github.com/TRI-ML/prismatic-vlms.">this https URL</a> Evaluation code: <a href="https://github.com/TRI-ML/vlm-evaluation">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Visually-conditioned language models (VLMs) have seen growing adoption in
applications such as visual dialogue, scene understanding, and robotic task
planning; adoption that has fueled a wealth of new models such as LLaVa,
InstructBLIP, and PaLI-3. Despite the volume of new releases, key design
decisions around image preprocessing, architecture, and optimization are
under-explored, making it challenging to understand what factors account for
model performance $-$ a challenge further complicated by the lack of objective,
consistent evaluations. To address these gaps, we first compile a suite of
standardized evaluations spanning visual question answering, object
localization from language, and targeted challenge sets that probe properties
such as hallucination; evaluations that provide calibrated, fine-grained
insight into a VLM's capabilities. Second, we rigorously investigate VLMs along
key design axes, including pretrained visual representations and quantifying
the tradeoffs of using base vs. instruct-tuned language models, amongst others.
We couple our analysis with three resource contributions: (1) a unified
framework for evaluating VLMs, (2) optimized, flexible code for VLM training,
and (3) checkpoints for all models, including a family of VLMs at the 7-13B
scale that strictly outperform InstructBLIP and LLaVa v1.5, the
state-of-the-art in open-source VLMs.
</p>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07867" title="Abstract">arXiv:2402.07867</a> [<a href="/pdf/2402.07867" title="Download PDF">pdf</a>, <a href="/format/2402.07867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented  Generation of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zou%2C+W">Wei Zou</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+R">Runpeng Geng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Binghui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jinyuan Jia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code is available at <a href="https://github.com/sleeepeer/PoisonedRAG">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) have achieved remarkable success due to their
exceptional generative capabilities. Despite their success, they also have
inherent limitations such as a lack of up-to-date knowledge and hallucination.
Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to
mitigate those limitations. In particular, given a question, RAG retrieves
relevant knowledge from a knowledge database to augment the input of the LLM.
For instance, the retrieved knowledge could be a set of top-k texts that are
most semantically similar to the given question when the knowledge database
contains millions of texts collected from Wikipedia. As a result, the LLM could
utilize the retrieved knowledge as the context to generate an answer for the
given question. Existing studies mainly focus on improving the accuracy or
efficiency of RAG, leaving its security largely unexplored. We aim to bridge
the gap in this work. Particularly, we propose PoisonedRAG , a set of knowledge
poisoning attacks to RAG, where an attacker could inject a few poisoned texts
into the knowledge database such that the LLM generates an attacker-chosen
target answer for an attacker-chosen target question. We formulate knowledge
poisoning attacks as an optimization problem, whose solution is a set of
poisoned texts. Depending on the background knowledge (e.g., black-box and
white-box settings) of an attacker on the RAG, we propose two solutions to
solve the optimization problem, respectively. Our results on multiple benchmark
datasets and LLMs show our attacks could achieve 90% attack success rates when
injecting 5 poisoned texts for each target question into a database with
millions of texts. We also evaluate recent defenses and our results show they
are insufficient to defend against our attacks, highlighting the need for new
defenses.
</p>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07868" title="Abstract">arXiv:2402.07868</a> [<a href="/pdf/2402.07868" title="Download PDF">pdf</a>, <a href="/ps/2402.07868" title="Download PostScript">ps</a>, <a href="/format/2402.07868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nesting Particle Filters for Experimental Design in Dynamical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iqbal%2C+S">Sahel Iqbal</a>, 
<a href="/search/cs?searchtype=author&query=Corenflos%2C+A">Adrien Corenflos</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%A4rkk%C3%A4%2C+S">Simo S&#xe4;rkk&#xe4;</a>, 
<a href="/search/cs?searchtype=author&query=Abdulsamad%2C+H">Hany Abdulsamad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The article has been made available early for dissemination. The empirical results are preliminary
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this paper, we propose a novel approach to Bayesian Experimental Design
(BED) for non-exchangeable data that formulates it as risk-sensitive policy
optimization. We develop the Inside-Out SMC^2 algorithm that uses a nested
sequential Monte Carlo (SMC) estimator of the expected information gain and
embeds it into a particle Markov chain Monte Carlo (pMCMC) framework to perform
gradient-based policy optimization. This is in contrast to recent approaches
that rely on biased estimators of the expected information gain (EIG) to
amortize the cost of experiments by learning a design policy in advance.
Numerical validation on a set of dynamical systems showcases the efficacy of
our method in comparison to other state-of-the-art strategies.
</p>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07871" title="Abstract">arXiv:2402.07871</a> [<a href="/pdf/2402.07871" title="Download PDF">pdf</a>, <a href="/format/2402.07871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Laws for Fine-Grained Mixture of Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krajewski%2C+J">Jakub Krajewski</a>, 
<a href="/search/cs?searchtype=author&query=Ludziejewski%2C+J">Jan Ludziejewski</a>, 
<a href="/search/cs?searchtype=author&query=Adamczewski%2C+K">Kamil Adamczewski</a>, 
<a href="/search/cs?searchtype=author&query=Pi%C3%B3ro%2C+M">Maciej Pi&#xf3;ro</a>, 
<a href="/search/cs?searchtype=author&query=Krutul%2C+M">Micha&#x142; Krutul</a>, 
<a href="/search/cs?searchtype=author&query=Antoniak%2C+S">Szymon Antoniak</a>, 
<a href="/search/cs?searchtype=author&query=Ciebiera%2C+K">Kamil Ciebiera</a>, 
<a href="/search/cs?searchtype=author&query=Kr%C3%B3l%2C+K">Krystian Kr&#xf3;l</a>, 
<a href="/search/cs?searchtype=author&query=Odrzyg%C3%B3%C5%BAd%C5%BA%2C+T">Tomasz Odrzyg&#xf3;&#x17a;d&#x17a;</a>, 
<a href="/search/cs?searchtype=author&query=Sankowski%2C+P">Piotr Sankowski</a>, 
<a href="/search/cs?searchtype=author&query=Cygan%2C+M">Marek Cygan</a>, 
<a href="/search/cs?searchtype=author&query=Jaszczur%2C+S">Sebastian Jaszczur</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Mixture of Experts (MoE) models have emerged as a primary solution for
reducing the computational cost of Large Language Models. In this work, we
analyze their scaling properties, incorporating an expanded range of variables.
Specifically, we introduce a new hyperparameter, granularity, whose adjustment
enables precise control over the size of the experts. Building on this, we
establish scaling laws for fine-grained MoE, taking into account the number of
training tokens, model size, and granularity. Leveraging these laws, we derive
the optimal training configuration for a given computational budget. Our
findings not only show that MoE models consistently outperform dense
Transformers but also highlight that the efficiency gap between dense and MoE
models widens as we scale up the model size and training budget. Furthermore,
we demonstrate that the common practice of setting the size of experts in MoE
to mirror the feed-forward layer is not optimal at almost any computational
budget.
</p>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07872" title="Abstract">arXiv:2402.07872</a> [<a href="/pdf/2402.07872" title="Download PDF">pdf</a>, <a href="/format/2402.07872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nasiriany%2C+S">Soroush Nasiriany</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+F">Fei Xia</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenhao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+T">Ted Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jacky Liang</a>, 
<a href="/search/cs?searchtype=author&query=Dasgupta%2C+I">Ishita Dasgupta</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+A">Annie Xie</a>, 
<a href="/search/cs?searchtype=author&query=Driess%2C+D">Danny Driess</a>, 
<a href="/search/cs?searchtype=author&query=Wahid%2C+A">Ayzaan Wahid</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhuo Xu</a>, 
<a href="/search/cs?searchtype=author&query=Vuong%2C+Q">Quan Vuong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tingnan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+T+E">Tsang-Wei Edward Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kuang-Huei Lee</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Peng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Kirmani%2C+S">Sean Kirmani</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yuke Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+A">Andy Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Hausman%2C+K">Karol Hausman</a>, 
<a href="/search/cs?searchtype=author&query=Heess%2C+N">Nicolas Heess</a>, 
<a href="/search/cs?searchtype=author&query=Finn%2C+C">Chelsea Finn</a>, 
<a href="/search/cs?searchtype=author&query=Levine%2C+S">Sergey Levine</a>, 
<a href="/search/cs?searchtype=author&query=Ichter%2C+B">Brian Ichter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Vision language models (VLMs) have shown impressive capabilities across a
variety of tasks, from logical reasoning to visual understanding. This opens
the door to richer interaction with the world, for example robotic control.
However, VLMs produce only textual outputs, while robotic control and other
spatial tasks require outputting continuous coordinates, actions, or
trajectories. How can we enable VLMs to handle such settings without
fine-tuning on task-specific data?
<br />In this paper, we propose a novel visual prompting approach for VLMs that we
call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as
iterative visual question answering. In each iteration, the image is annotated
with a visual representation of proposals that the VLM can refer to (e.g.,
candidate robot actions, localizations, or trajectories). The VLM then selects
the best ones for the task. These proposals are iteratively refined, allowing
the VLM to eventually zero in on the best available answer. We investigate
PIVOT on real-world robotic navigation, real-world manipulation from images,
instruction following in simulation, and additional spatial inference tasks
such as localization. We find, perhaps surprisingly, that our approach enables
zero-shot control of robotic systems without any robot training data,
navigation in a variety of environments, and other capabilities. Although
current performance is far from perfect, our work highlights potentials and
limitations of this new regime and shows a promising approach for
Internet-Scale VLMs in robotic and spatial reasoning domains. Website:
pivot-prompt.github.io and HuggingFace:
https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.
</p>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07875" title="Abstract">arXiv:2402.07875</a> [<a href="/pdf/2402.07875" title="Download PDF">pdf</a>, <a href="/format/2402.07875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit Bias of Policy Gradient in Linear Quadratic Control:  Extrapolation to Unseen Initial States
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Razin%2C+N">Noam Razin</a>, 
<a href="/search/cs?searchtype=author&query=Alexander%2C+Y">Yotam Alexander</a>, 
<a href="/search/cs?searchtype=author&query=Cohen-Karlik%2C+E">Edo Cohen-Karlik</a>, 
<a href="/search/cs?searchtype=author&query=Giryes%2C+R">Raja Giryes</a>, 
<a href="/search/cs?searchtype=author&query=Globerson%2C+A">Amir Globerson</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+N">Nadav Cohen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY); Machine Learning (stat.ML)

</div>
<p class="mathjax">In modern machine learning, models can often fit training data in numerous
ways, some of which perform well on unseen (test) data, while others do not.
Remarkably, in such cases gradient descent frequently exhibits an implicit bias
that leads to excellent performance on unseen data. This implicit bias was
extensively studied in supervised learning, but is far less understood in
optimal control (reinforcement learning). There, learning a controller applied
to a system via gradient descent is known as policy gradient, and a question of
prime importance is the extent to which a learned controller extrapolates to
unseen initial states. This paper theoretically studies the implicit bias of
policy gradient in terms of extrapolation to unseen initial states. Focusing on
the fundamental Linear Quadratic Regulator (LQR) problem, we establish that the
extent of extrapolation depends on the degree of exploration induced by the
system when commencing from initial states included in training. Experiments
corroborate our theory, and demonstrate its conclusions on problems beyond LQR,
where systems are non-linear and controllers are neural networks. We
hypothesize that real-world optimal control may be greatly improved by
developing methods for informed selection of initial states to train on.
</p>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07876" title="Abstract">arXiv:2402.07876</a> [<a href="/pdf/2402.07876" title="Download PDF">pdf</a>, <a href="/format/2402.07876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Policy Improvement using Language Feedback Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+V">Victor Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Misra%2C+D">Dipendra Misra</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xingdi Yuan</a>, 
<a href="/search/cs?searchtype=author&query=C%C3%B4t%C3%A9%2C+M">Marc-Alexandre C&#xf4;t&#xe9;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">We introduce Language Feedback Models (LFMs) that identify desirable
behaviour - actions that help achieve tasks specified in the instruction - for
imitation learning in instruction following. To train LFMs, we obtain feedback
from Large Language Models (LLMs) on visual trajectories verbalized to language
descriptions. First, by using LFMs to identify desirable behaviour to imitate,
we improve in task-completion rate over strong behavioural cloning baselines on
three distinct language grounding environments (Touchdown, ScienceWorld, and
ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict
actions, when controlling for the number of LLM output tokens. Third, LFMs
generalize to unseen environments, improving task-completion rate by 3.5-12.0%
through one round of adaptation. Finally, LFM can be modified to provide
human-interpretable feedback without performance loss, allowing human
verification of desirable behaviour for imitation learning.
</p>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07877" title="Abstract">arXiv:2402.07877</a> [<a href="/pdf/2402.07877" title="Download PDF">pdf</a>, <a href="/format/2402.07877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WildfireGPT: Tailored Large Language Model for Wildfire Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yangxinyu Xie</a>, 
<a href="/search/cs?searchtype=author&query=Mallick%2C+T">Tanwi Mallick</a>, 
<a href="/search/cs?searchtype=author&query=Bergerson%2C+J+D">Joshua David Bergerson</a>, 
<a href="/search/cs?searchtype=author&query=Hutchison%2C+J+K">John K. Hutchison</a>, 
<a href="/search/cs?searchtype=author&query=Verner%2C+D+R">Duane R. Verner</a>, 
<a href="/search/cs?searchtype=author&query=Branham%2C+J">Jordan Branham</a>, 
<a href="/search/cs?searchtype=author&query=Alexander%2C+M+R">M. Ross Alexander</a>, 
<a href="/search/cs?searchtype=author&query=Ross%2C+R+B">Robert B. Ross</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Levy%2C+L">Leslie-Anne Levy</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+W">Weijie Su</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The recent advancement of large language models (LLMs) represents a
transformational capability at the frontier of artificial intelligence (AI) and
machine learning (ML). However, LLMs are generalized models, trained on
extensive text corpus, and often struggle to provide context-specific
information, particularly in areas requiring specialized knowledge such as
wildfire details within the broader context of climate change. For
decision-makers and policymakers focused on wildfire resilience and adaptation,
it is crucial to obtain responses that are not only precise but also
domain-specific, rather than generic. To that end, we developed WildfireGPT, a
prototype LLM agent designed to transform user queries into actionable insights
on wildfire risks. We enrich WildfireGPT by providing additional context such
as climate projections and scientific literature to ensure its information is
current, relevant, and scientifically accurate. This enables WildfireGPT to be
an effective tool for delivering detailed, user-specific insights on wildfire
risks to support a diverse set of end users, including researchers, engineers,
urban planners, emergency managers, and infrastructure operators.
</p>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07878" title="Abstract">arXiv:2402.07878</a> [<a href="/pdf/2402.07878" title="Download PDF">pdf</a>, <a href="/format/2402.07878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Graph Theory for Improving Machine Learning-based Detection of  Cyber Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zonneveld%2C+G">Giacomo Zonneveld</a>, 
<a href="/search/cs?searchtype=author&query=Principi%2C+L">Lorenzo Principi</a>, 
<a href="/search/cs?searchtype=author&query=Baldi%2C+M">Marco Baldi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 1 figure, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Early detection of network intrusions and cyber threats is one of the main
pillars of cybersecurity. One of the most effective approaches for this purpose
is to analyze network traffic with the help of artificial intelligence
algorithms, with the aim of detecting the possible presence of an attacker by
distinguishing it from a legitimate user. This is commonly done by collecting
the traffic exchanged between terminals in a network and analyzing it on a
per-packet or per-connection basis. In this paper, we propose instead to
perform pre-processing of network traffic under analysis with the aim of
extracting some new metrics on which we can perform more efficient detection
and overcome some limitations of classical approaches. These new metrics are
based on graph theory, and consider the network as a whole, rather than
focusing on individual packets or connections. Our approach is validated
through experiments performed on publicly available data sets, from which it
results that it can not only overcome some of the limitations of classical
approaches, but also achieve a better detection capability of cyber threats.
</p>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07884" title="Abstract">arXiv:2402.07884</a> [<a href="/pdf/2402.07884" title="Download PDF">pdf</a>, <a href="/format/2402.07884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Anomaly Detection in Modern Power Systems: A Penalty-based  Mitigation Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Abadi%2C+E+M">Erfan Mehdipour Abadi</a>, 
<a href="/search/eess?searchtype=author&query=Nazari%2C+M+H">Masoud H. Nazari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in 2024 IEEE PES General Meeting, Seattle, Washington (PES GM 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">The evolving landscape of electric power networks, influenced by the
integration of distributed energy resources require the development of novel
power system monitoring and control architectures. This paper develops
algorithm to monitor and detect anomalies of different parts of a power system
that cannot be measured directly, by applying neighboring measurements and a
dynamic probing technique in a distributed fashion. Additionally, the proposed
method accurately assesses the severity of the anomaly. A decision-making
algorithm is introduced to effectively penalize anomalous agents, ensuring
vigilant oversight of the entire power system's functioning. Simulation results
show the efficacy of algorithms in distributed anomaly detection and
mitigation.
</p>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07889" title="Abstract">arXiv:2402.07889</a> [<a href="/pdf/2402.07889" title="Download PDF">pdf</a>, <a href="/format/2402.07889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward an Android Static Analysis Approach for Data Protection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khedkar%2C+M">Mugdha Khedkar</a>, 
<a href="/search/cs?searchtype=author&query=Bodden%2C+E">Eric Bodden</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at MOBILESoft 2024 Research Forum Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Android applications collecting data from users must protect it according to
the current legal frameworks. Such data protection has become even more
important since the European Union rolled out the General Data Protection
Regulation (GDPR). Since app developers are not legal experts, they find it
difficult to write privacy-aware source code. Moreover, they have limited tool
support to reason about data protection throughout their app development
process.
<br />This paper motivates the need for a static analysis approach to diagnose and
explain data protection in Android apps. The analysis will recognize personal
data sources in the source code, and aims to further examine the data flow
originating from these sources. App developers can then address key questions
about data manipulation, derived data, and the presence of technical measures.
Despite challenges, we explore to what extent one can realize this analysis
through static taint analysis, a common method for identifying security
vulnerabilities. This is a first step towards designing a tool-based approach
that aids app developers and assessors in ensuring data protection in Android
apps, based on automated static program analysis.
</p>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07890" title="Abstract">arXiv:2402.07890</a> [<a href="/pdf/2402.07890" title="Download PDF">pdf</a>, <a href="/format/2402.07890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nipu%2C+A+S">Ayesha Siddika Nipu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Siming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Harris%2C+A">Anthony Harris</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2022 IEEE Conference on Games (CoG)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Distributed decision-making in multi-agent systems presents difficult
challenges for interactive behavior learning in both cooperative and
competitive systems. To mitigate this complexity, MAIDRL presents a
semi-centralized Dense Reinforcement Learning algorithm enhanced by agent
influence maps (AIMs), for learning effective multi-agent control on StarCraft
Multi-Agent Challenge (SMAC) scenarios. In this paper, we extend the DenseNet
in MAIDRL and introduce semi-centralized Multi-Agent Dense-CNN Reinforcement
Learning, MAIDCRL, by incorporating convolutional layers into the deep model
architecture, and evaluate the performance on both homogeneous and
heterogeneous scenarios. The results show that the CNN-enabled MAIDCRL
significantly improved the learning performance and achieved a faster learning
rate compared to the existing MAIDRL, especially on more complicated
heterogeneous SMAC scenarios. We further investigate the stability and
robustness of our model. The statistics reflect that our model not only
achieves higher winning rate in all the given scenarios but also boosts the
agent's learning process in fine-grained decision-making.
</p>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07891" title="Abstract">arXiv:2402.07891</a> [<a href="/pdf/2402.07891" title="Download PDF">pdf</a>, <a href="/format/2402.07891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Label-Efficient Model Selection for Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ashury-Tahan%2C+S">Shir Ashury-Tahan</a>, 
<a href="/search/cs?searchtype=author&query=Sznajder%2C+B">Benjamin Sznajder</a>, 
<a href="/search/cs?searchtype=author&query=Choshen%2C+L">Leshem Choshen</a>, 
<a href="/search/cs?searchtype=author&query=Ein-Dor%2C+L">Liat Ein-Dor</a>, 
<a href="/search/cs?searchtype=author&query=Shnarch%2C+E">Eyal Shnarch</a>, 
<a href="/search/cs?searchtype=author&query=Gera%2C+A">Ariel Gera</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Model selection for a given target task can be costly, as it may entail
extensive annotation of the quality of outputs of different models. We
introduce DiffUse, an efficient method to make an informed decision between
candidate text generation models. DiffUse reduces the required amount of
preference annotations, thus saving valuable time and resources in performing
evaluation. DiffUse intelligently selects instances by clustering embeddings
that represent the semantic differences between model outputs. Thus, it is able
to identify a subset of examples that are more informative for preference
decisions. Our method is model-agnostic, and can be applied to any text
generation model. Moreover, we propose a practical iterative approach for
dynamically determining how many instances to annotate. In a series of
experiments over hundreds of model pairs, we demonstrate that DiffUse can
dramatically reduce the required number of annotations -- by up to 75% -- while
maintaining high evaluation reliability.
</p>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07894" title="Abstract">arXiv:2402.07894</a> [<a href="/pdf/2402.07894" title="Download PDF">pdf</a>, <a href="/format/2402.07894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MODIPHY: Multimodal Obscured Detection for IoT using PHantom  Convolution-Enabled Faster YOLO
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+S">Shubhabrata Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Beard%2C+C">Cory Beard</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhu Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Low-light conditions and occluded scenarios impede object detection in
real-world Internet of Things (IoT) applications like autonomous vehicles and
security systems. While advanced machine learning models strive for accuracy,
their computational demands clash with the limitations of resource-constrained
devices, hampering real-time performance. In our current research, we tackle
this challenge, by introducing "YOLO Phantom", one of the smallest YOLO models
ever conceived. YOLO Phantom utilizes the novel Phantom Convolution block,
achieving comparable accuracy to the latest YOLOv8n model while simultaneously
reducing both parameters and model size by 43%, resulting in a significant 19%
reduction in Giga Floating Point Operations (GFLOPs). YOLO Phantom leverages
transfer learning on our multimodal RGB-infrared dataset to address low-light
and occlusion issues, equipping it with robust vision under adverse conditions.
Its real-world efficacy is demonstrated on an IoT platform with advanced
low-light and RGB cameras, seamlessly connecting to an AWS-based notification
endpoint for efficient real-time object detection. Benchmarks reveal a
substantial boost of 17% and 14% in frames per second (FPS) for thermal and RGB
detection, respectively, compared to the baseline YOLOv8n model. For community
contribution, both the code and the multimodal dataset are available on GitHub.
</p>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07895" title="Abstract">arXiv:2402.07895</a> [<a href="/pdf/2402.07895" title="Download PDF">pdf</a>, <a href="/format/2402.07895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detection of Spider Mites on Labrador Beans through Machine Learning  Approaches Using Custom Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+V">Violet Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jason Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qureshi%2C+A">Ans Qureshi</a>, 
<a href="/search/cs?searchtype=author&query=Nejati%2C+M">Mahla Nejati</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Australasian Conference on Robotics and Automation (ACRA 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
<p class="mathjax">Amidst growing food production demands, early plant disease detection is
essential to safeguard crops; this study proposes a visual machine learning
approach for plant disease detection, harnessing RGB and NIR data collected in
real-world conditions through a JAI FS-1600D-10GE camera to build an RGBN
dataset. A two-stage early plant disease detection model with YOLOv8 and a
sequential CNN was used to train on a dataset with partial labels, which showed
a 3.6% increase in mAP compared to a single-stage end-to-end segmentation
model. The sequential CNN model achieved 90.62% validation accuracy utilising
RGBN data. An average of 6.25% validation accuracy increase is found using RGBN
in classification compared to RGB using ResNet15 and the sequential CNN models.
Further research and dataset improvements are needed to meet food production
demands.
</p>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07896" title="Abstract">arXiv:2402.07896</a> [<a href="/pdf/2402.07896" title="Download PDF">pdf</a>, <a href="/format/2402.07896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Suppressing Pink Elephants with Direct Principle Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Castricato%2C+L">Louis Castricato</a>, 
<a href="/search/cs?searchtype=author&query=Lile%2C+N">Nathan Lile</a>, 
<a href="/search/cs?searchtype=author&query=Anand%2C+S">Suraj Anand</a>, 
<a href="/search/cs?searchtype=author&query=Schoelkopf%2C+H">Hailey Schoelkopf</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+S">Siddharth Verma</a>, 
<a href="/search/cs?searchtype=author&query=Biderman%2C+S">Stella Biderman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Existing methods for controlling language models, such as RLHF and
Constitutional AI, involve determining which LLM behaviors are desirable and
training them into a language model. However, in many cases, it is desirable
for LLMs to be controllable \textit{at inference time}, so that they can be
used in multiple contexts with diverse needs. We illustrate this with the
\textbf{Pink Elephant Problem}: instructing an LLM to avoid discussing a
certain entity (a ``Pink Elephant''), and instead discuss a preferred entity
(``Grey Elephant''). We apply a novel simplification of Constitutional AI,
\textbf{Direct Principle Feedback}, which skips the ranking of responses and
uses DPO directly on critiques and revisions. Our results show that after DPF
fine-tuning on our synthetic Pink Elephants dataset, our 13B fine-tuned LLaMA 2
model significantly outperforms Llama-2-13B-Chat and a prompted baseline, and
performs as well as GPT-4 in on our curated test set assessing the Pink
Elephant Problem.
</p>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07897" title="Abstract">arXiv:2402.07897</a> [<a href="/pdf/2402.07897" title="Download PDF">pdf</a>, <a href="/ps/2402.07897" title="Download PostScript">ps</a>, <a href="/format/2402.07897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A holographic mobile-based application for practicing pronunciation of  basic English vocabulary for Spanish speaking children
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cerezo%2C+R">R. Cerezo</a>, 
<a href="/search/cs?searchtype=author&query=Calderon%2C+V">V. Calderon</a>, 
<a href="/search/cs?searchtype=author&query=Romero%2C+C">C. Romero</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Human-Computer Studies (2019):124, 13-25
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">This paper describes a holographic mobile-based application designed to help
Spanish-speaking children to practice the pronunciation of basic English
vocabulary words. The mastery of vocabulary is a fundamental step when learning
a language but is often perceived as boring. Producing the correct
pronunciation is frequently regarded as the most difficult and complex skill
for new learners of English. In order to address these problems this research
takes advantage of the power of multi-channel stimuli (sound, image and
interaction) in a mobilebased hologram application in order to motivate
students and improve their experience of practicing. We adapted the
prize-winning HolograFX game and developed a new mobile application to help
practice English pronunciation. A 3D holographic robot that acts as a virtual
teacher interacts via voice with the children. To test the tool we carried out
an experiment with 70 Spanish pre-school children divided into three classes,
the control group using traditional methods such as images in books and on the
blackboard, and two experimental groups using our drills and practice software.
One experimental group used the mobile application without the holographic game
and the other experimental group used the application with the holographic
game. We performed pre-test and post-test performance assessments, a
satisfaction survey and emotion analysis. The results are very promising. They
show that the use of the holographic mobile-based application had a significant
impact on the children's motivation. It also improved their performance
compared to traditional methods used in the classroom.
</p>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07899" title="Abstract">arXiv:2402.07899</a> [<a href="/pdf/2402.07899" title="Download PDF">pdf</a>, <a href="/format/2402.07899" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A systematic investigation of learnability from single child linguistic  input
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yulu Qin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wentao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lake%2C+B+M">Brenden M. Lake</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages; 6 figures; Submitted to CogSci 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Language models (LMs) have demonstrated remarkable proficiency in generating
linguistically coherent text, sparking discussions about their relevance to
understanding human language learnability. However, a significant gap exists
between the training data for these models and the linguistic input a child
receives. LMs are typically trained on data that is orders of magnitude larger
and fundamentally different from child-directed speech (Warstadt and Bowman,
2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our
research focuses on training LMs on subsets of a single child's linguistic
input. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in
this setting can form syntactic and semantic word clusters and develop
sensitivity to certain linguistic phenomena, but they only considered LSTMs and
simpler neural networks trained from just one single-child dataset. Here, to
examine the robustness of learnability from single-child input, we
systematically train six different model architectures on five datasets (3
single-child and 2 baselines). We find that the models trained on single-child
datasets showed consistent results that matched with previous work,
underscoring the robustness of forming meaningful syntactic and semantic
representations from a subset of a child's linguistic input.
</p>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07900" title="Abstract">arXiv:2402.07900</a> [<a href="/pdf/2402.07900" title="Download PDF">pdf</a>, <a href="/format/2402.07900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wavefront Randomization Improves Deconvolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kohli%2C+A">Amit Kohli</a>, 
<a href="/search/cs?searchtype=author&query=Angelopoulos%2C+A+N">Anastasios N. Angelopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Waller%2C+L">Laura Waller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV); Optics (physics.optics)

</div>
<p class="mathjax">The performance of an imaging system is limited by optical aberrations, which
cause blurriness in the resulting image. Digital correction techniques, such as
deconvolution, have limited ability to correct the blur, since some spatial
frequencies in the scene are not measured adequately due to the aberrations
('zeros' of the system transfer function). We prove that the addition of a
random mask to an imaging system removes its dependence on aberrations,
reducing the likelihood of zeros in the transfer function and consequently
reducing the sensitivity to noise during deconvolution. and consequently result
in lower sensitivity to noise during deconvolution. In simulation, we show that
this strategy improves image quality over a range of aberration types,
aberration strengths, and signal-to-noise ratios.
</p>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07901" title="Abstract">arXiv:2402.07901</a> [<a href="/pdf/2402.07901" title="Download PDF">pdf</a>, <a href="/format/2402.07901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FAST: Factorizable Attention for Speeding up Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gerami%2C+A">Armin Gerami</a>, 
<a href="/search/cs?searchtype=author&query=Hoover%2C+M">Monte Hoover</a>, 
<a href="/search/cs?searchtype=author&query=Dulepet%2C+P+S">Pranav S. Dulepet</a>, 
<a href="/search/cs?searchtype=author&query=Duraiswami%2C+R">Ramani Duraiswami</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Numerical Analysis (math.NA)

</div>
<p class="mathjax">Motivated by the factorization inherent in the original fast multipole method
and the improved fast Gauss transform we introduce a factorable form of
attention that operates efficiently in high dimensions. This approach reduces
the computational and memory complexity of the attention mechanism in
transformers from $O(N^2)$ to $O(N)$. In comparison to previous attempts, our
work presents a linearly scaled attention mechanism that maintains the full
representation of the attention matrix without compromising on sparsification
and incorporates the all-to-all relationship between tokens. We explore the
properties of our new attention metric and conduct tests in various standard
settings. Results indicate that our attention mechanism has a robust
performance and holds significant promise for diverse applications where
self-attention is used.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Tue, 13 Feb 24</h3>
<dl>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06633" title="Abstract">arXiv:2402.06633</a> (cross-list from q-fin.ST) [<a href="/pdf/2402.06633" title="Download PDF">pdf</a>, <a href="/format/2402.06633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MDGNN: Multi-Relational Dynamic Graph Neural Network for Comprehensive  and Dynamic Stock Investment Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Qian%2C+H">Hao Qian</a>, 
<a href="/search/q-fin?searchtype=author&query=Zhou%2C+H">Hongting Zhou</a>, 
<a href="/search/q-fin?searchtype=author&query=Zhao%2C+Q">Qian Zhao</a>, 
<a href="/search/q-fin?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/q-fin?searchtype=author&query=Yao%2C+H">Hongxiang Yao</a>, 
<a href="/search/q-fin?searchtype=author&query=Wang%2C+J">Jingwei Wang</a>, 
<a href="/search/q-fin?searchtype=author&query=Liu%2C+Z">Ziqi Liu</a>, 
<a href="/search/q-fin?searchtype=author&query=Yu%2C+F">Fei Yu</a>, 
<a href="/search/q-fin?searchtype=author&query=Zhang%2C+Z">Zhiqiang Zhang</a>, 
<a href="/search/q-fin?searchtype=author&query=Zhou%2C+J">Jun Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 figures, accepted by AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Finance (q-fin.ST)</span>; Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">The stock market is a crucial component of the financial system, but
predicting the movement of stock prices is challenging due to the dynamic and
intricate relations arising from various aspects such as economic indicators,
financial reports, global news, and investor sentiment. Traditional sequential
methods and graph-based models have been applied in stock movement prediction,
but they have limitations in capturing the multifaceted and temporal influences
in stock price movements. To address these challenges, the Multi-relational
Dynamic Graph Neural Network (MDGNN) framework is proposed, which utilizes a
discrete dynamic graph to comprehensively capture multifaceted relations among
stocks and their evolution over time. The representation generated from the
graph offers a complete perspective on the interrelationships among stocks and
associated entities. Additionally, the power of the Transformer structure is
leveraged to encode the temporal evolution of multiplex relations, providing a
dynamic and effective approach to predicting stock investment. Further, our
proposed MDGNN framework achieves the best performance in public datasets
compared with state-of-the-art (SOTA) stock investment methods.
</p>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06635" title="Abstract">arXiv:2402.06635</a> (cross-list from q-fin.ST) [<a href="/pdf/2402.06635" title="Download PDF">pdf</a>, <a href="/format/2402.06635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large (and Deep) Factor Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Kelly%2C+B">Bryan Kelly</a>, 
<a href="/search/q-fin?searchtype=author&query=Kuznetsov%2C+B">Boris Kuznetsov</a>, 
<a href="/search/q-fin?searchtype=author&query=Malamud%2C+S">Semyon Malamud</a>, 
<a href="/search/q-fin?searchtype=author&query=Xu%2C+T+A">Teng Andrea Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Finance (q-fin.ST)</span>; Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
<p class="mathjax">We open up the black box behind Deep Learning for portfolio optimization and
prove that a sufficiently wide and arbitrarily deep neural network (DNN)
trained to maximize the Sharpe ratio of the Stochastic Discount Factor (SDF) is
equivalent to a large factor model (LFM): A linear factor pricing model that
uses many non-linear characteristics. The nature of these characteristics
depends on the architecture of the DNN in an explicit, tractable fashion. This
makes it possible to derive end-to-end trained DNN-based SDFs in closed form
for the first time. We evaluate LFMs empirically and show how various
architectural choices impact SDF performance. We document the virtue of depth
complexity: With enough data, the out-of-sample performance of DNN-SDF is
increasing in the NN depth, saturating at huge depths of around 100 hidden
layers.
</p>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06638" title="Abstract">arXiv:2402.06638</a> (cross-list from q-fin.ST) [<a href="/pdf/2402.06638" title="Download PDF">pdf</a>, <a href="/format/2402.06638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformers with Attentive Federated Aggregation for Time Series Stock  Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Thwal%2C+C+M">Chu Myaet Thwal</a>, 
<a href="/search/q-fin?searchtype=author&query=Tun%2C+Y+L">Ye Lin Tun</a>, 
<a href="/search/q-fin?searchtype=author&query=Kim%2C+K">Kitae Kim</a>, 
<a href="/search/q-fin?searchtype=author&query=Park%2C+S">Seong-Bae Park</a>, 
<a href="/search/q-fin?searchtype=author&query=Hong%2C+C+S">Choong Seon Hong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in IEEE ICOIN 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Finance (q-fin.ST)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent innovations in transformers have shown their superior performance in
natural language processing (NLP) and computer vision (CV). The ability to
capture long-range dependencies and interactions in sequential data has also
triggered a great interest in time series modeling, leading to the widespread
use of transformers in many time series applications. However, being the most
common and crucial application, the adaptation of transformers to time series
forecasting has remained limited, with both promising and inconsistent results.
In contrast to the challenges in NLP and CV, time series problems not only add
the complexity of order or temporal dependence among input sequences but also
consider trend, level, and seasonality information that much of this data is
valuable for decision making. The conventional training scheme has shown
deficiencies regarding model overfitting, data scarcity, and privacy issues
when working with transformers for a forecasting task. In this work, we propose
attentive federated transformers for time series stock forecasting with better
performance while preserving the privacy of participating enterprises.
Empirical results on various stock data from the Yahoo! Finance website
indicate the superiority of our proposed scheme in dealing with the above
challenges and data heterogeneity in federated learning.
</p>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06642" title="Abstract">arXiv:2402.06642</a> (cross-list from q-fin.ST) [<a href="/pdf/2402.06642" title="Download PDF">pdf</a>, <a href="/format/2402.06642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From GARCH to Neural Network for Volatility Forecast
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Zhao%2C+P">Pengfei Zhao</a>, 
<a href="/search/q-fin?searchtype=author&query=Zhu%2C+H">Haoren Zhu</a>, 
<a href="/search/q-fin?searchtype=author&query=NG%2C+W+S+H">Wilfred Siu Hung NG</a>, 
<a href="/search/q-fin?searchtype=author&query=Lee%2C+D+L">Dik Lun Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Finance (q-fin.ST)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Volatility, as a measure of uncertainty, plays a crucial role in numerous
financial activities such as risk management. The Econometrics and Machine
Learning communities have developed two distinct approaches for financial
volatility forecasting: the stochastic approach and the neural network (NN)
approach. Despite their individual strengths, these methodologies have
conventionally evolved in separate research trajectories with little
interaction between them. This study endeavors to bridge this gap by
establishing an equivalence relationship between models of the GARCH family and
their corresponding NN counterparts. With the equivalence relationship
established, we introduce an innovative approach, named GARCH-NN, for
constructing NN-based volatility models. It obtains the NN counterparts of
GARCH models and integrates them as components into an established NN
architecture, thereby seamlessly infusing volatility stylized facts (SFs)
inherent in the GARCH models into the neural network. We develop the GARCH-LSTM
model to showcase the power of the GARCH-NN approach. Experiment results
validate that amalgamating the NN counterparts of the GARCH family models into
established NN models leads to enhanced outcomes compared to employing the
stochastic and NN models in isolation.
</p>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06646" title="Abstract">arXiv:2402.06646</a> (cross-list from physics.ao-ph) [<a href="/pdf/2402.06646" title="Download PDF">pdf</a>, <a href="/ps/2402.06646" title="Download PostScript">ps</a>, <a href="/format/2402.06646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Model-based Probabilistic Downscaling for 180-year East Asian  Climate Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Ling%2C+F">Fenghua Ling</a>, 
<a href="/search/physics?searchtype=author&query=Lu%2C+Z">Zeyu Lu</a>, 
<a href="/search/physics?searchtype=author&query=Luo%2C+J">Jing-Jia Luo</a>, 
<a href="/search/physics?searchtype=author&query=Bai%2C+L">Lei Bai</a>, 
<a href="/search/physics?searchtype=author&query=Behera%2C+S+K">Swadhin K. Behera</a>, 
<a href="/search/physics?searchtype=author&query=Jin%2C+D">Dachao Jin</a>, 
<a href="/search/physics?searchtype=author&query=Pan%2C+B">Baoxiang Pan</a>, 
<a href="/search/physics?searchtype=author&query=Jiang%2C+H">Huidong Jiang</a>, 
<a href="/search/physics?searchtype=author&query=Yamagata%2C+T">Toshio Yamagata</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Machine Learning (cs.LG); Geophysics (physics.geo-ph)

</div>
<p class="mathjax">As our planet is entering into the "global boiling" era, understanding
regional climate change becomes imperative. Effective downscaling methods that
provide localized insights are crucial for this target. Traditional approaches,
including computationally-demanding regional dynamical models or statistical
downscaling frameworks, are often susceptible to the influence of downscaling
uncertainty. Here, we address these limitations by introducing a diffusion
probabilistic downscaling model (DPDM) into the meteorological field. This
model can efficiently transform data from 1{\deg} to 0.1{\deg} resolution.
Compared with deterministic downscaling schemes, it not only has more accurate
local details, but also can generate a large number of ensemble members based
on probability distribution sampling to evaluate the uncertainty of
downscaling. Additionally, we apply the model to generate a 180-year dataset of
monthly surface variables in East Asia, offering a more detailed perspective
for understanding local scale climate change over the past centuries.
</p>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06656" title="Abstract">arXiv:2402.06656</a> (cross-list from q-fin.ST) [<a href="/pdf/2402.06656" title="Download PDF">pdf</a>, <a href="/format/2402.06656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffsFormer: A Diffusion Transformer on Stock Factor Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Gao%2C+Y">Yuan Gao</a>, 
<a href="/search/q-fin?searchtype=author&query=Chen%2C+H">Haokun Chen</a>, 
<a href="/search/q-fin?searchtype=author&query=Wang%2C+X">Xiang Wang</a>, 
<a href="/search/q-fin?searchtype=author&query=Wang%2C+Z">Zhicai Wang</a>, 
<a href="/search/q-fin?searchtype=author&query=Wang%2C+X">Xue Wang</a>, 
<a href="/search/q-fin?searchtype=author&query=Gao%2C+J">Jinyang Gao</a>, 
<a href="/search/q-fin?searchtype=author&query=Ding%2C+B">Bolin Ding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Finance (q-fin.ST)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Machine learning models have demonstrated remarkable efficacy and efficiency
in a wide range of stock forecasting tasks. However, the inherent challenges of
data scarcity, including low signal-to-noise ratio (SNR) and data homogeneity,
pose significant obstacles to accurate forecasting. To address this issue, we
propose a novel approach that utilizes artificial intelligence-generated
samples (AIGS) to enhance the training procedures. In our work, we introduce
the Diffusion Model to generate stock factors with Transformer architecture
(DiffsFormer). DiffsFormer is initially trained on a large-scale source domain,
incorporating conditional guidance so as to capture global joint distribution.
When presented with a specific downstream task, we employ DiffsFormer to
augment the training procedure by editing existing samples. This editing step
allows us to control the strength of the editing process, determining the
extent to which the generated data deviates from the target domain. To evaluate
the effectiveness of DiffsFormer augmented training, we conduct experiments on
the CSI300 and CSI800 datasets, employing eight commonly used machine learning
models. The proposed method achieves relative improvements of 7.2% and 27.8% in
annualized return ratio for the respective datasets. Furthermore, we perform
extensive experiments to gain insights into the functionality of DiffsFormer
and its constituent components, elucidating how they address the challenges of
data scarcity and enhance the overall model performance. Our research
demonstrates the efficacy of leveraging AIGS and the DiffsFormer architecture
to mitigate data scarcity in stock forecasting tasks.
</p>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06666" title="Abstract">arXiv:2402.06666</a> (cross-list from physics.ao-ph) [<a href="/pdf/2402.06666" title="Download PDF">pdf</a>, <a href="/format/2402.06666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weather Prediction with Diffusion Guided by Realistic Forecast Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Hua%2C+Z">Zhanxiang Hua</a>, 
<a href="/search/physics?searchtype=author&query=He%2C+Y">Yutong He</a>, 
<a href="/search/physics?searchtype=author&query=Ma%2C+C">Chengqian Ma</a>, 
<a href="/search/physics?searchtype=author&query=Anderson-Frey%2C+A">Alexandra Anderson-Frey</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Weather forecasting remains a crucial yet challenging domain, where recently
developed models based on deep learning (DL) have approached the performance of
traditional numerical weather prediction (NWP) models. However, these DL
models, often complex and resource-intensive, face limitations in flexibility
post-training and in incorporating NWP predictions, leading to reliability
concerns due to potential unphysical predictions. In response, we introduce a
novel method that applies diffusion models (DM) for weather forecasting. In
particular, our method can achieve both direct and iterative forecasting with
the same modeling framework. Our model is not only capable of generating
forecasts independently but also uniquely allows for the integration of NWP
predictions, even with varying lead times, during its sampling process. The
flexibility and controllability of our model empowers a more trustworthy DL
system for the general weather community. Additionally, incorporating
persistence and climatology data further enhances our model's long-term
forecasting stability. Our empirical findings demonstrate the feasibility and
generalizability of this approach, suggesting a promising direction for future,
more sophisticated diffusion models without the need for retraining.
</p>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06678" title="Abstract">arXiv:2402.06678</a> (cross-list from physics.soc-ph) [<a href="/pdf/2402.06678" title="Download PDF">pdf</a>, <a href="/format/2402.06678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can machine learning predict citizen-reported angler behavior?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Schmid%2C+J+S">Julia S. Schmid</a> (1), 
<a href="/search/physics?searchtype=author&query=Simmons%2C+S">Sean Simmons</a> (2), 
<a href="/search/physics?searchtype=author&query=Lewis%2C+M+A">Mark A. Lewis</a> (1 and 3), 
<a href="/search/physics?searchtype=author&query=Poesch%2C+M+S">Mark S. Poesch</a> (1), 
<a href="/search/physics?searchtype=author&query=Ramazi%2C+P">Pouria Ramazi</a> (4) ((1) University of Alberta, (2) Angler&#x27;s Atlas, (3) University of Victoria, (4) Brock University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 10 figures, 4 tables (including supplementary information)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Prediction of angler behaviors, such as catch rates and angler pressure, is
essential to maintaining fish populations and ensuring angler satisfaction.
Angler behavior can partly be tracked by online platforms and mobile phone
applications that provide fishing activities reported by recreational anglers.
Moreover, angler behavior is known to be driven by local site attributes. Here,
the prediction of citizen-reported angler behavior was investigated by
machine-learning methods using auxiliary data on the environment,
socioeconomics, fisheries management objectives, and events at a freshwater
body. The goal was to determine whether auxiliary data alone could predict the
reported behavior. Different spatial and temporal extents and temporal
resolutions were considered. Accuracy scores averaged 88% for monthly
predictions at single water bodies and 86% for spatial predictions on a day in
a specific region across Canada. At other resolutions and scales, the models
only achieved low prediction accuracy of around 60%. The study represents a
first attempt at predicting angler behavior in time and space at a large scale
and establishes a foundation for potential future expansions in various
directions.
</p>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06680" title="Abstract">arXiv:2402.06680</a> (cross-list from physics.soc-ph) [<a href="/pdf/2402.06680" title="Download PDF">pdf</a>, <a href="/format/2402.06680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social Physics Informed Diffusion Model for Crowd Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Chen%2C+H">Hongyi Chen</a>, 
<a href="/search/physics?searchtype=author&query=Ding%2C+J">Jingtao Ding</a>, 
<a href="/search/physics?searchtype=author&query=Li%2C+Y">Yong Li</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+Y">Yue Wang</a>, 
<a href="/search/physics?searchtype=author&query=Zhang%2C+X">Xiao-Ping Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Crowd simulation holds crucial applications in various domains, such as urban
planning, architectural design, and traffic arrangement. In recent years,
physics-informed machine learning methods have achieved state-of-the-art
performance in crowd simulation but fail to model the heterogeneity and
multi-modality of human movement comprehensively. In this paper, we propose a
social physics-informed diffusion model named SPDiff to mitigate the above gap.
SPDiff takes both the interactive and historical information of crowds in the
current timeframe to reverse the diffusion process, thereby generating the
distribution of pedestrian movement in the subsequent timeframe. Inspired by
the well-known social physics model, i.e., Social Force, regarding crowd
dynamics, we design a crowd interaction module to guide the denoising process
and further enhance this module with the equivariant properties of crowd
interactions. To mitigate error accumulation in long-term simulations, we
propose a multi-frame rollout training algorithm for diffusion modeling.
Experiments conducted on two real-world datasets demonstrate the superior
performance of SPDiff in terms of macroscopic and microscopic evaluation
metrics. Code and appendix are available at
https://github.com/tsinghua-fib-lab/SPDiff.
</p>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06683" title="Abstract">arXiv:2402.06683</a> (cross-list from eess.AS) [<a href="/pdf/2402.06683" title="Download PDF">pdf</a>, <a href="/format/2402.06683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sound Source Separation Using Latent Variational Block-Wise  Disentanglement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Helwani%2C+K">Karim Helwani</a>, 
<a href="/search/eess?searchtype=author&query=Togami%2C+M">Masahito Togami</a>, 
<a href="/search/eess?searchtype=author&query=Smaragdis%2C+P">Paris Smaragdis</a>, 
<a href="/search/eess?searchtype=author&query=Goodwin%2C+M+M">Michael M. Goodwin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Sound (cs.SD)

</div>
<p class="mathjax">While neural network approaches have made significant strides in resolving
classical signal processing problems, it is often the case that hybrid
approaches that draw insight from both signal processing and neural networks
produce more complete solutions. In this paper, we present a hybrid classical
digital signal processing/deep neural network (DSP/DNN) approach to source
separation (SS) highlighting the theoretical link between variational
autoencoder and classical approaches to SS. We propose a system that transforms
the single channel under-determined SS task to an equivalent multichannel
over-determined SS problem in a properly designed latent space. The separation
task in the latent space is treated as finding a variational block-wise
disentangled representation of the mixture. We show empirically, that the
design choices and the variational formulation of the task at hand motivated by
the classical signal processing theoretical results lead to robustness to
unseen out-of-distribution data and reduction of the overfitting risk. To
address the resulting permutation issue we explicitly incorporate a novel
differentiable permutation loss function and augment the model with a memory
mechanism to keep track of the statistics of the individual sources.
</p>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06684" title="Abstract">arXiv:2402.06684</a> (cross-list from physics.ao-ph) [<a href="/pdf/2402.06684" title="Download PDF">pdf</a>, <a href="/format/2402.06684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ai4Fapar: How artificial intelligence can help to forecast the seasonal  earth observation signal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Sabo%2C+F">Filip Sabo</a>, 
<a href="/search/physics?searchtype=author&query=Claverie%2C+M">Martin Claverie</a>, 
<a href="/search/physics?searchtype=author&query=Meroni%2C+M">Michele Meroni</a>, 
<a href="/search/physics?searchtype=author&query=Essenfelder%2C+A+H">Arthur Hrast Essenfelder</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 2023 conference on Big Data from Space, Soille,
  P., Lumnitz, S. and Albani, S. editor(s), Publications Office of the European
  Union, Luxembourg, 2023, JRC135493
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper investigated the potential of a multivariate Transformer model to
forecast the temporal trajectory of the Fraction of Absorbed Photosynthetically
Active Radiation (FAPAR) for short (1 month) and long horizon (more than 1
month) periods at the regional level in Europe and North Africa. The input data
covers the period from 2002 to 2022 and includes remote sensing and weather
data for modelling FAPAR predictions. The model was evaluated using a leave one
year out cross-validation and compared with the climatological benchmark.
Results show that the transformer model outperforms the benchmark model for one
month forecasting horizon, after which the climatological benchmark is better.
The RMSE values of the transformer model ranged from 0.02 to 0.04 FAPAR units
for the first 2 months of predictions. Overall, the tested Transformer model is
a valid method for FAPAR forecasting, especially when combined with weather
data and used for short-term predictions.
</p>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06689" title="Abstract">arXiv:2402.06689</a> (cross-list from q-fin.ST) [<a href="/pdf/2402.06689" title="Download PDF">pdf</a>, <a href="/format/2402.06689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Study on Stock Forecasting Using Deep Learning and Statistical Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Gupta%2C+H">Himanshu Gupta</a>, 
<a href="/search/q-fin?searchtype=author&query=Jaiswal%2C+A">Aditya Jaiswal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Finance (q-fin.ST)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Predicting a fast and accurate model for stock price forecasting is been a
challenging task and this is an active area of research where it is yet to be
found which is the best way to forecast the stock price. Machine learning, deep
learning and statistical analysis techniques are used here to get the accurate
result so the investors can see the future trend and maximize the return of
investment in stock trading. This paper will review many deep learning
algorithms for stock price forecasting. We use a record of s&amp;p 500 index data
for training and testing. The survey motive is to check various deep learning
and statistical model techniques for stock price forecasting that are Moving
Averages, ARIMA which are statistical techniques and LSTM, RNN, CNN, and FULL
CNN which are deep learning models. It will discuss various models, including
the Auto regression integration moving average model, the Recurrent neural
network model, the long short-term model which is the type of RNN used for long
dependency for data, the convolutional neural network model, and the full
convolutional neural network model, in terms of error calculation or percentage
of accuracy that how much it is accurate which measures by the function like
Root mean square error, mean absolute error, mean squared error. The model can
be used to predict the stock price by checking the low MAE value as lower the
MAE value the difference between the predicting and the actual value will be
less and this model will predict the price more accurately than other models.
</p>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06692" title="Abstract">arXiv:2402.06692</a> (cross-list from eess.IV) [<a href="/pdf/2402.06692" title="Download PDF">pdf</a>, <a href="/format/2402.06692" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HistoHDR-Net: Histogram Equalization for Single LDR to HDR Image  Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Barua%2C+H+B">Hrishav Bakul Barua</a>, 
<a href="/search/eess?searchtype=author&query=Krishnasamy%2C+G">Ganesh Krishnasamy</a>, 
<a href="/search/eess?searchtype=author&query=Wong%2C+K">KokSheik Wong</a>, 
<a href="/search/eess?searchtype=author&query=Dhall%2C+A">Abhinav Dhall</a>, 
<a href="/search/eess?searchtype=author&query=Stefanov%2C+K">Kalin Stefanov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
<p class="mathjax">High Dynamic Range (HDR) imaging aims to replicate the high visual quality
and clarity of real-world scenes. Due to the high costs associated with HDR
imaging, the literature offers various data-driven methods for HDR image
reconstruction from Low Dynamic Range (LDR) counterparts. A common limitation
of these approaches is missing details in regions of the reconstructed HDR
images, which are over- or under-exposed in the input LDR images. To this end,
we propose a simple and effective method, HistoHDR-Net, to recover the fine
details (e.g., color, contrast, saturation, and brightness) of HDR images via a
fusion-based approach utilizing histogram-equalized LDR images along with
self-attention guidance. Our experiments demonstrate the efficacy of the
proposed approach over the state-of-art methods.
</p>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06753" title="Abstract">arXiv:2402.06753</a> (cross-list from physics.soc-ph) [<a href="/pdf/2402.06753" title="Download PDF">pdf</a>, <a href="/format/2402.06753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shortest-path percolation on complex networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Kim%2C+M">Minsuk Kim</a>, 
<a href="/search/physics?searchtype=author&query=Radicchi%2C+F">Filippo Radicchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 5 figures, 1 table + Supplemental Material
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Statistical Mechanics (cond-mat.stat-mech); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">We propose a bond-percolation model intended to describe the consumption, and
eventual exhaustion, of resources in transport networks. Edges forming
minimum-length paths connecting demanded origin-destination nodes are removed
if below a certain budget. As pairs of nodes are demanded and edges are
removed, the macroscopic connected component of the graph disappears, i.e., the
graph undergoes a percolation transition. Here, we study such a
shortest-path-percolation transition in homogeneous random graphs where pairs
of demanded origin-destination nodes are randomly generated, and fully
characterize it by means of finite-size scaling analysis. If budget is finite,
the transition is identical to the one of ordinary percolation, where a single
giant cluster shrinks as edges are removed from the graph; for infinite budget,
the transition becomes more abrupt than the one of ordinary percolation, being
characterized by the sudden fragmentation of the giant connected component into
a multitude of clusters of similar size.
</p>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06772" title="Abstract">arXiv:2402.06772</a> (cross-list from q-bio.QM) [<a href="/pdf/2402.06772" title="Download PDF">pdf</a>, <a href="/format/2402.06772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrosynthesis Prediction via Search in (Hyper) Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Lan%2C+Z">Zixun Lan</a>, 
<a href="/search/q-bio?searchtype=author&query=Hong%2C+B">Binjie Hong</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhu%2C+J">Jiajun Zhu</a>, 
<a href="/search/q-bio?searchtype=author&query=Zeng%2C+Z">Zuo Zeng</a>, 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+Z">Zhenfu Liu</a>, 
<a href="/search/q-bio?searchtype=author&query=Yu%2C+L">Limin Yu</a>, 
<a href="/search/q-bio?searchtype=author&query=Ma%2C+F">Fei Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
<p class="mathjax">Predicting reactants from a specified core product stands as a fundamental
challenge within organic synthesis, termed retrosynthesis prediction. Recently,
semi-template-based methods and graph-edits-based methods have achieved good
performance in terms of both interpretability and accuracy. However, due to
their mechanisms these methods cannot predict complex reactions, e.g.,
reactions with multiple reaction center or attaching the same leaving group to
more than one atom. In this study we propose a semi-template-based method, the
\textbf{Retro}synthesis via \textbf{S}earch \textbf{i}n (Hyper) \textbf{G}raph
(RetroSiG) framework to alleviate these limitations. In the proposed method, we
turn the reaction center identification and the leaving group completion tasks
as tasks of searching in the product molecular graph and leaving group
hypergraph respectively. As a semi-template-based method RetroSiG has several
advantages. First, RetroSiG is able to handle the complex reactions mentioned
above by its novel search mechanism. Second, RetroSiG naturally exploits the
hypergraph to model the implicit dependencies between leaving groups. Third,
RetroSiG makes full use of the prior, i.e., one-hop constraint. It reduces the
search space and enhances overall performance. Comprehensive experiments
demonstrated that RetroSiG achieved competitive results. Furthermore, we
conducted experiments to show the capability of RetroSiG in predicting complex
reactions. Ablation experiments verified the efficacy of specific elements,
such as the one-hop constraint and the leaving group hypergraph.
</p>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06778" title="Abstract">arXiv:2402.06778</a> (cross-list from math.OC) [<a href="/pdf/2402.06778" title="Download PDF">pdf</a>, <a href="/format/2402.06778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Quasi-Newton Method for Multi-Agent Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shorinwa%2C+O">Ola Shorinwa</a>, 
<a href="/search/math?searchtype=author&query=Schwager%2C+M">Mac Schwager</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Multiagent Systems (cs.MA); Systems and Control (eess.SY)

</div>
<p class="mathjax">We present a distributed quasi-Newton (DQN) method, which enables a group of
agents to compute an optimal solution of a separable multi-agent optimization
problem locally using an approximation of the curvature of the aggregate
objective function. Each agent computes a descent direction from its local
estimate of the aggregate Hessian, obtained from quasi-Newton approximation
schemes using the gradient of its local objective function. Moreover, we
introduce a distributed quasi-Newton method for equality-constrained
optimization (EC-DQN), where each agent takes Karush-Kuhn-Tucker-like update
steps to compute an optimal solution. In our algorithms, each agent
communicates with its one-hop neighbors over a peer-to-peer communication
network to compute a common solution. We prove convergence of our algorithms to
a stationary point of the optimization problem. In addition, we demonstrate the
competitive empirical convergence of our algorithm in both well-conditioned and
ill-conditioned optimization problems, in terms of the computation time and
communication cost incurred by each agent for convergence, compared to existing
distributed first-order and second-order methods. Particularly, in
ill-conditioned problems, our algorithms achieve a faster computation time for
convergence, while requiring a lower communication cost, across a range of
communication networks with different degrees of connectedness, by leveraging
information on the curvature of the problem.
</p>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06785" title="Abstract">arXiv:2402.06785</a> (cross-list from cond-mat.soft) [<a href="/pdf/2402.06785" title="Download PDF">pdf</a>, <a href="/ps/2402.06785" title="Download PostScript">ps</a>, <a href="/format/2402.06785" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The poker-chip experiments of synthetic elastomers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Kamarei%2C+F">Farhad Kamarei</a>, 
<a href="/search/cond-mat?searchtype=author&query=Kumar%2C+A">Aditya Kumar</a>, 
<a href="/search/cond-mat?searchtype=author&query=Lopez-Pamies%2C+O">Oscar Lopez-Pamies</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Soft Condensed Matter (cond-mat.soft)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">In a recent study, Kumar and Lopez-Pamies (J. Mech. Phys. Solids 150: 104359,
2021) have provided a complete quantitative explanation of the famed poker-chip
experiments of Gent and Lindley (Proc. R. Soc. Lond. Ser. A 249: 195--205,
1959) on natural rubber. In a nutshell, making use of the fracture theory of
Kumar, Francfort, and Lopez-Pamies (J. Mech. Phys. Solids 112: 523--551, 2018),
they have shown that the nucleation of cracks in poker-chip experiments in
natural rubber is governed by the strength -- in particular, the hydrostatic
strength -- of the rubber, while the propagation of the nucleated cracks is
governed by the Griffith competition between the bulk elastic energy of the
rubber and its intrinsic fracture energy. The main objective of this paper is
to extend the theoretical study of the poker-chip experiment by Kumar and
Lopez-Pamies to synthetic elastomers that, as opposed to natural rubber: ($i$)
may feature a hydrostatic strength that is larger than their uniaxial and
biaxial tensile strengths and ($ii$) do not exhibit strain-induced
crystallization. A parametric study, together with direct comparisons with
recent poker-chip experiments on a silicone elastomer, show that these two
different material characteristics have a profound impact on where and when
cracks nucleate, as well as on where and when they propagate. In conjunction
with the results put forth earlier for natural rubber, the results presented in
this paper provide a complete description and explanation of the poker-chip
experiments of elastomers at large. As a second objective, this paper also
introduces a new fully explicit constitutive prescription for the driving force
that describes the material strength in the fracture theory of Kumar,
Francfort, and Lopez-Pamies.
</p>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06841" title="Abstract">arXiv:2402.06841</a> (cross-list from eess.IV) [<a href="/pdf/2402.06841" title="Download PDF">pdf</a>, <a href="/ps/2402.06841" title="Download PostScript">ps</a>, <a href="/format/2402.06841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Point cloud-based registration and image fusion between cardiac SPECT  MPI and CTA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tang%2C+S">Shaojie Tang</a>, 
<a href="/search/eess?searchtype=author&query=Miao%2C+P">Penpen Miao</a>, 
<a href="/search/eess?searchtype=author&query=Gao%2C+X">Xingyu Gao</a>, 
<a href="/search/eess?searchtype=author&query=Zhong%2C+Y">Yu Zhong</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+D">Dantong Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Wen%2C+H">Haixing Wen</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+Z">Zhihui Xu</a>, 
<a href="/search/eess?searchtype=author&query=Wei%2C+Q">Qiuyue Wei</a>, 
<a href="/search/eess?searchtype=author&query=Yao%2C+H">Hongping Yao</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+X">Xin Huang</a>, 
<a href="/search/eess?searchtype=author&query=Gao%2C+R">Rui Gao</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+C">Chen Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+W">Weihua Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">A method was proposed for the point cloud-based registration and image fusion
between cardiac single photon emission computed tomography (SPECT) myocardial
perfusion images (MPI) and cardiac computed tomography angiograms (CTA).
Firstly, the left ventricle (LV) epicardial regions (LVERs) in SPECT and CTA
images were segmented by using different U-Net neural networks trained to
generate the point clouds of the LV epicardial contours (LVECs). Secondly,
according to the characteristics of cardiac anatomy, the special points of
anterior and posterior interventricular grooves (APIGs) were manually marked in
both SPECT and CTA image volumes. Thirdly, we developed an in-house program for
coarsely registering the special points of APIGs to ensure a correct cardiac
orientation alignment between SPECT and CTA images. Fourthly, we employed ICP,
SICP or CPD algorithm to achieve a fine registration for the point clouds
(together with the special points of APIGs) of the LV epicardial surfaces
(LVERs) in SPECT and CTA images. Finally, the image fusion between SPECT and
CTA was realized after the fine registration. The experimental results showed
that the cardiac orientation was aligned well and the mean distance error of
the optimal registration method (CPD with affine transform) was consistently
less than 3 mm. The proposed method could effectively fuse the structures from
cardiac CTA and SPECT functional images, and demonstrated a potential in
assisting in accurate diagnosis of cardiac diseases by combining complementary
advantages of the two imaging modalities.
</p>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06856" title="Abstract">arXiv:2402.06856</a> (cross-list from math.PR) [<a href="/pdf/2402.06856" title="Download PDF">pdf</a>, <a href="/ps/2402.06856" title="Download PostScript">ps</a>, <a href="/format/2402.06856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Community detection in the hypergraph stochastic block model and  reconstruction on hypertrees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gu%2C+Y">Yuzhou Gu</a>, 
<a href="/search/math?searchtype=author&query=Pandey%2C+A">Aaradhya Pandey</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">We study the weak recovery problem on the $r$-uniform hypergraph stochastic
block model ($r$-HSBM) with two balanced communities. In this model, $n$
vertices are randomly divided into two communities, and size-$r$ hyperedges are
added randomly depending on whether all vertices in the hyperedge are in the
same community. The goal of the weak recovery problem is to recover a
non-trivial fraction of the communities given the hypergraph. Previously, Pal
and Zhu (2021) established that weak recovery is always possible above a
natural threshold called the Kesten-Stigum (KS) threshold. Gu and Polyanskiy
(2023) proved that the KS threshold is tight if $r\le 4$ or the expected degree
$d$ is small. It remained open whether the KS threshold is tight for $r\ge 5$
and large $d$.
<br />In this paper we determine the tightness of the KS threshold for any fixed
$r$ and large $d$. We prove that for $r\le 6$ and $d$ large enough, the KS
threshold is tight. This shows that there is no information-computation gap in
this regime. This partially confirms a conjecture of Angelini et al. (2015).
For $r\ge 7$, we prove that for $d$ large enough, the KS threshold is not
tight, providing more evidence supporting the existence of an
information-computation gap in this regime.
<br />Furthermore, we establish asymptotic bounds on the weak recovery threshold
for fixed $r$ and large $d$. We also obtain a number of results regarding the
closely-related broadcasting on hypertrees (BOHT) model, including the
asymptotics of the reconstruction threshold for $r\ge 7$ and impossibility of
robust reconstruction at criticality.
</p>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06875" title="Abstract">arXiv:2402.06875</a> (cross-list from eess.IV) [<a href="/pdf/2402.06875" title="Download PDF">pdf</a>, <a href="/format/2402.06875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disentangled Latent Energy-Based Style Translation: An Image-Level  Structural MRI Harmonization Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wu%2C+M">Mengqi Wu</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+L">Lintao Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Yap%2C+P">Pew-Thian Yap</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+H">Hongtu Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+M">Mingxia Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Brain magnetic resonance imaging (MRI) has been extensively employed across
clinical and research fields, but often exhibits sensitivity to site effects
arising from nonbiological variations such as differences in field strength and
scanner vendors. Numerous retrospective MRI harmonization techniques have
demonstrated encouraging outcomes in reducing the site effects at image level.
However, existing methods generally suffer from high computational requirements
and limited generalizability, restricting their applicability to unseen MRIs.
In this paper, we design a novel disentangled latent energy-based style
translation (DLEST) framework for unpaired image-level MRI harmonization,
consisting of (1) site-invariant image generation (SIG), (2) site-specific
style translation (SST), and (3) site-specific MRI synthesis (SMS).
Specifically, the SIG employs a latent autoencoder to encode MRIs into a
low-dimensional latent space and reconstruct MRIs from latent codes. The SST
utilizes an energy-based model to comprehend the global latent distribution of
a target domain and translate source latent codes toward the target domain,
while SMS enables MRI synthesis with a target-specific style. By disentangling
image generation and style translation in latent space, the DLEST can achieve
efficient style translation. Our model was trained on T1-weighted MRIs from a
public dataset (with 3,984 subjects across 58 acquisition sites/settings) and
validated on an independent dataset (with 9 traveling subjects scanned in 11
sites/settings) in 4 tasks: (1) histogram and clustering comparison, (2) site
classification, (3) brain tissue segmentation, and (4) site-specific MRI
synthesis. Qualitative and quantitative results demonstrate the superiority of
our method over several state-of-the-arts.
</p>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06884" title="Abstract">arXiv:2402.06884</a> (cross-list from stat.ML) [<a href="/pdf/2402.06884" title="Download PDF">pdf</a>, <a href="/format/2402.06884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-Rank Approximation of Structural Redundancy for Self-Supervised  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Du%2C+K">Kang Du</a>, 
<a href="/search/stat?searchtype=author&query=Xiang%2C+Y">Yu Xiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the 3rd Conference on Causal Learning and Reasoning (CLeaR)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We study the data-generating mechanism for reconstructive SSL to shed light
on its effectiveness. With an infinite amount of labeled samples, we provide a
sufficient and necessary condition for perfect linear approximation. The
condition reveals a full-rank component that preserves the label classes of Y,
along with a redundant component. Motivated by the condition, we propose to
approximate the redundant component by a low-rank factorization and measure the
approximation quality by introducing a new quantity $\epsilon_s$, parameterized
by the rank of factorization s. We incorporate $\epsilon_s$ into the excess
risk analysis under both linear regression and ridge regression settings, where
the latter regularization approach is to handle scenarios when the dimension of
the learned features is much larger than the number of labeled samples n for
downstream tasks. We design three stylized experiments to compare SSL with
supervised learning under different settings to support our theoretical
findings.
</p>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06888" title="Abstract">arXiv:2402.06888</a> (cross-list from eess.AS) [<a href="/pdf/2402.06888" title="Download PDF">pdf</a>, <a href="/format/2402.06888" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Self-Supervised Speech Models on Children&#x27;s Speech and  Infant Vocalizations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Jialu Li</a>, 
<a href="/search/eess?searchtype=author&query=Hasegawa-Johnson%2C+M">Mark Hasegawa-Johnson</a>, 
<a href="/search/eess?searchtype=author&query=McElwain%2C+N+L">Nancy L. McElwain</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to 2024 ICASSP Workshop of Self-supervision in Audio, Speech and Beyond (SASB)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">To understand why self-supervised learning (SSL) models have empirically
achieved strong performances on several speech-processing downstream tasks,
numerous studies have focused on analyzing the encoded information of the SSL
layer representations in adult speech. Limited work has investigated how
pre-training and fine-tuning affect SSL models encoding children's speech and
vocalizations. In this study, we aim to bridge this gap by probing SSL models
on two relevant downstream tasks: (1) phoneme recognition (PR) on the speech of
adults, older children (8-10 years old), and younger children (1-4 years old),
and (2) vocalization classification (VC) distinguishing cry, fuss, and babble
for infants under 14 months old. For younger children's PR, the superiority of
fine-tuned SSL models is largely due to their ability to learn features that
represent older children's speech and then adapt those features to the speech
of younger children. For infant VC, SSL models pre-trained on large-scale home
recordings learn to leverage phonetic representations at middle layers, and
thereby enhance the performance of this task.
</p>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06923" title="Abstract">arXiv:2402.06923</a> (cross-list from eess.AS) [<a href="/pdf/2402.06923" title="Download PDF">pdf</a>, <a href="/format/2402.06923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CochCeps-Augment: A Novel Self-Supervised Contrastive Learning Using  Cochlear Cepstrum-based Masking for Speech Emotion Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ziogas%2C+I">Ioannis Ziogas</a>, 
<a href="/search/eess?searchtype=author&query=Alfalahi%2C+H">Hessa Alfalahi</a>, 
<a href="/search/eess?searchtype=author&query=Khandoker%2C+A+H">Ahsan H. Khandoker</a>, 
<a href="/search/eess?searchtype=author&query=Hadjileontiadis%2C+L+J">Leontios J. Hadjileontiadis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 1 figure Accepted in IEEE ICASSP 2024 Workshops - Self-Supervision in Audio, Speech, and Beyond
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Sound (cs.SD); Signal Processing (eess.SP); Machine Learning (stat.ML)

</div>
<p class="mathjax">Self-supervised learning (SSL) for automated speech recognition in terms of
its emotional content, can be heavily degraded by the presence noise, affecting
the efficiency of modeling the intricate temporal and spectral informative
structures of speech. Recently, SSL on large speech datasets, as well as new
audio-specific SSL proxy tasks, such as, temporal and frequency masking, have
emerged, yielding superior performance compared to classic approaches drawn
from the image augmentation domain. Our proposed contribution builds upon this
successful paradigm by introducing CochCeps-Augment, a novel bio-inspired
masking augmentation task for self-supervised contrastive learning of speech
representations. Specifically, we utilize the newly introduced bio-inspired
cochlear cepstrogram (CCGRAM) to derive noise robust representations of input
speech, that are then further refined through a self-supervised learning
scheme. The latter employs SimCLR to generate contrastive views of a CCGRAM
through masking of its angle and quefrency dimensions. Our experimental
approach and validations on the emotion recognition K-EmoCon benchmark dataset,
for the first time via a speaker-independent approach, features unsupervised
pre-training, linear probing and fine-tuning. Our results potentiate
CochCeps-Augment to serve as a standard tool in speech emotion recognition
analysis, showing the added value of incorporating bio-inspired masking as an
informative augmentation task for self-supervision. Our code for implementing
CochCeps-Augment will be made available at:
https://github.com/GiannisZgs/CochCepsAugment.
</p>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06940" title="Abstract">arXiv:2402.06940</a> (cross-list from stat.ML) [<a href="/pdf/2402.06940" title="Download PDF">pdf</a>, <a href="/format/2402.06940" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Incremental Belief Updates Using Weighted Virtual Observations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Tolpin%2C+D">David Tolpin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We present an algorithmic solution to the problem of incremental belief
updating in the context of Monte Carlo inference in Bayesian statistical models
represented by probabilistic programs. Given a model and a sample-approximated
posterior, our solution constructs a set of weighted observations to condition
the model such that inference would result in the same posterior. This problem
arises e.g. in multi-level modelling, incremental inference, inference in
presence of privacy constraints. First, a set of virtual observations is
selected, then, observation weights are found through a computationally
efficient optimization procedure such that the reconstructed posterior
coincides with or closely approximates the original posterior. We implement and
apply the solution to a number of didactic examples and case studies, showing
efficiency and robustness of our approach. The provided reference
implementation is agnostic to the probabilistic programming language or the
inference algorithm, and can be applied to most mainstream probabilistic
programming environments.
</p>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06952" title="Abstract">arXiv:2402.06952</a> (cross-list from quant-ph) [<a href="/pdf/2402.06952" title="Download PDF">pdf</a>, <a href="/format/2402.06952" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating the Effect of Crosstalk Error on Circuit Fidelity Using Noisy  Intermediate-Scale Quantum Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Heng%2C+S">Sovanmonynuth Heng</a>, 
<a href="/search/quant-ph?searchtype=author&query=Go%2C+M">Myeongseong Go</a>, 
<a href="/search/quant-ph?searchtype=author&query=Han%2C+Y">Youngsun Han</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Current advancements in technology have focused the attention of the quantum
computing community toward exploring the potential of near-term devices whose
computing power surpasses that of classical computers in practical
applications. An unresolved central question revolves around whether the
inherent noise in these devices can be overcome or whether any potential
quantum advantage would be limited. There is no doubt that crosstalk is one of
the main sources of noise in noisy intermediate-scale quantum (NISQ) systems,
and it poses a fundamental challenge to hardware designs. Crosstalk between
parallel instructions can corrupt quantum states and cause incorrect program
execution. In this study, we present a comprehensive analysis of the crosstalk
error effect on NISQ computers. Our approach is extremely straightforward and
practical for characterizing the crosstalk error of various multi-qubit
devices. In particular, we combine the randomized benchmarking (RB) and
simultaneous randomized benchmarking (SRB) protocol to characterize the
crosstalk error from the correlation controlled-NOT (CNOT) gate. We demonstrate
this protocol experimentally on 5- \&amp; 7-qubit devices. Our results demonstrate
the crosstalk error model of two different IBM quantum devices over the
experimental week and compare the error variation against the machine, number
of qubits, quantum volume, processor, and topology of the IBM quantum devices.
We then confirm the improvement in the circuit fidelity on different benchmarks
by up to 3.06x via inserting an instruction barrier, as compared with an IBM
quantum noisy device which offers near-optimal crosstalk mitigation in
practice. Most importantly, we provide insight to ensure that the quantum
operation can perform its quantum magic undisturbed.
</p>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06956" title="Abstract">arXiv:2402.06956</a> (cross-list from math.CA) [<a href="/pdf/2402.06956" title="Download PDF">pdf</a>, <a href="/format/2402.06956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uniform enclosures for the phase and zeros of Bessel functions and their  derivatives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Filonov%2C+N">Nikolay Filonov</a>, 
<a href="/search/math?searchtype=author&query=Levitin%2C+M">Michael Levitin</a>, 
<a href="/search/math?searchtype=author&query=Polterovich%2C+I">Iosif Polterovich</a>, 
<a href="/search/math?searchtype=author&query=Sher%2C+D+A">David A. Sher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages (including Appendices), 13 figures, 3 tables. The accompanying Mathematica script and its printout are available for download at <a href="https://michaellevitin.net/bessels.html">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Classical Analysis and ODEs (math.CA)</span>; Numerical Analysis (math.NA); Spectral Theory (math.SP)

</div>
<p class="mathjax">We prove explicit uniform two-sided bounds for the phase functions of Bessel
functions and of their derivatives. As a consequence, we obtain new enclosures
for the zeros of Bessel functions and their derivatives in terms of inverse
values of some elementary functions. These bounds are valid, with a few
exceptions, for all zeros and all Bessel functions with non-negative indices.
We provide numerical evidence showing that our bounds either improve or closely
match the best previously known ones.
</p>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06992" title="Abstract">arXiv:2402.06992</a> (cross-list from q-bio.NC) [<a href="/pdf/2402.06992" title="Download PDF">pdf</a>, <a href="/format/2402.06992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Rational Analysis of the Speech-to-Song Illusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Marjieh%2C+R">Raja Marjieh</a>, 
<a href="/search/q-bio?searchtype=author&query=van+Rijn%2C+P">Pol van Rijn</a>, 
<a href="/search/q-bio?searchtype=author&query=Sucholutsky%2C+I">Ilia Sucholutsky</a>, 
<a href="/search/q-bio?searchtype=author&query=Lee%2C+H">Harin Lee</a>, 
<a href="/search/q-bio?searchtype=author&query=Griffiths%2C+T+L">Thomas L. Griffiths</a>, 
<a href="/search/q-bio?searchtype=author&query=Jacoby%2C+N">Nori Jacoby</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Applications (stat.AP)

</div>
<p class="mathjax">The speech-to-song illusion is a robust psychological phenomenon whereby a
spoken sentence sounds increasingly more musical as it is repeated. Despite
decades of research, a complete formal account of this transformation is still
lacking, and some of its nuanced characteristics, namely, that certain phrases
appear to transform while others do not, is not well understood. Here we
provide a formal account of this phenomenon, by recasting it as a statistical
inference whereby a rational agent attempts to decide whether a sequence of
utterances is more likely to have been produced in a song or speech. Using this
approach and analyzing song and speech corpora, we further introduce a novel
prose-to-lyrics illusion that is purely text-based. In this illusion, simply
duplicating written sentences makes them appear more like song lyrics. We
provide robust evidence for this new illusion in both human participants and
large language models.
</p>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07008" title="Abstract">arXiv:2402.07008</a> (cross-list from eess.IV) [<a href="/pdf/2402.07008" title="Download PDF">pdf</a>, <a href="/format/2402.07008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Optimization Framework for Processing and Transfer Learning for the  Brain Tumor Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ren%2C+T">Tianyi Ren</a>, 
<a href="/search/eess?searchtype=author&query=Honey%2C+E">Ethan Honey</a>, 
<a href="/search/eess?searchtype=author&query=Rebala%2C+H">Harshitha Rebala</a>, 
<a href="/search/eess?searchtype=author&query=Sharma%2C+A">Abhishek Sharma</a>, 
<a href="/search/eess?searchtype=author&query=Chopra%2C+A">Agamdeep Chopra</a>, 
<a href="/search/eess?searchtype=author&query=Kurt%2C+M">Mehmet Kurt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Tumor segmentation from multi-modal brain MRI images is a challenging task
due to the limited samples, high variance in shapes and uneven distribution of
tumor morphology. The performance of automated medical image segmentation has
been significant improvement by the recent advances in deep learning. However,
the model predictions have not yet reached the desired level for clinical use
in terms of accuracy and generalizability. In order to address the distinct
problems presented in Challenges 1, 2, and 3 of BraTS 2023, we have constructed
an optimization framework based on a 3D U-Net model for brain tumor
segmentation. This framework incorporates a range of techniques, including
various pre-processing and post-processing techniques, and transfer learning.
On the validation datasets, this multi-modality brain tumor segmentation
framework achieves an average lesion-wise Dice score of 0.79, 0.72, 0.74 on
Challenges 1, 2, 3 respectively.
</p>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07017" title="Abstract">arXiv:2402.07017</a> (cross-list from math.OC) [<a href="/pdf/2402.07017" title="Download PDF">pdf</a>, <a href="/format/2402.07017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Space-time shape optimization of rotating electric machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cesarano%2C+A">Alessio Cesarano</a>, 
<a href="/search/math?searchtype=author&query=Dapogny%2C+C">Charles Dapogny</a>, 
<a href="/search/math?searchtype=author&query=Gangl%2C+P">Peter Gangl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">This article is devoted to the shape optimization of the internal structure
of an electric motor, and more precisely of the arrangement of air and
ferromagnetic material inside the rotor part with the aim to increase the
torque of the machine. The governing physical problem is the time-dependent,
non linear magneto-quasi-static version of Maxwell's equations. This multiphase
problem can be reformulated on a 2d section of the real cylindrical 3d
configuration; however, due to the rotation of the machine, the geometry of the
various material phases at play (the ferromagnetic material, the permanent
magnets, air, etc.) undergoes a prescribed motion over the considered time
period. This original setting raises a number of issues. From the theoretical
viewpoint, we prove the well-posedness of this unusual non linear evolution
problem featuring a moving geometry. We then calculate the shape derivative of
a performance criterion depending on the shape of the ferromagnetic phase via
the corresponding magneto-quasi-static potential. Our numerical framework to
address this problem is based on a shape gradient algorithm. The non linear
time periodic evolution problems for the magneto-quasi-static potential is
solved in the time domain, with a Newton-Raphson method. The discretization
features a space-time finite element method, applied on a precise, meshed
representation of the space-time region of interest, which encloses a
body-fitted representation of the various material phases of the motor at all
the considered stages of the time period. After appraising the efficiency of
our numerical framework on an academic problem, we present a quite realistic
example of optimal design of the ferromagnetic phase of the rotor of an
electric machine.
</p>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07025" title="Abstract">arXiv:2402.07025</a> (cross-list from stat.ML) [<a href="/pdf/2402.07025" title="Download PDF">pdf</a>, <a href="/format/2402.07025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalization Error of Graph Neural Networks in the Mean-field Regime
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Aminian%2C+G">Gholamali Aminian</a>, 
<a href="/search/stat?searchtype=author&query=He%2C+Y">Yixuan He</a>, 
<a href="/search/stat?searchtype=author&query=Reinert%2C+G">Gesine Reinert</a>, 
<a href="/search/stat?searchtype=author&query=Szpruch%2C+%C5%81">&#x141;ukasz Szpruch</a>, 
<a href="/search/stat?searchtype=author&query=Cohen%2C+S+N">Samuel N. Cohen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
<p class="mathjax">This work provides a theoretical framework for assessing the generalization
error of graph classification tasks via graph neural networks in the
over-parameterized regime, where the number of parameters surpasses the
quantity of data points. We explore two widely utilized types of graph neural
networks: graph convolutional neural networks and message passing graph neural
networks. Prior to this study, existing bounds on the generalization error in
the over-parametrized regime were uninformative, limiting our understanding of
over-parameterized network performance. Our novel approach involves deriving
upper bounds within the mean-field regime for evaluating the generalization
error of these graph neural networks. We establish upper bounds with a
convergence rate of $O(1/n)$, where $n$ is the number of graph samples. These
upper bounds offer a theoretical assurance of the networks' performance on
unseen data in the challenging over-parameterized regime and overall contribute
to our understanding of their performance.
</p>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07042" title="Abstract">arXiv:2402.07042</a> (cross-list from quant-ph) [<a href="/pdf/2402.07042" title="Download PDF">pdf</a>, <a href="/ps/2402.07042" title="Download PostScript">ps</a>, <a href="/format/2402.07042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Projection-algebras and quantum logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Lehmann%2C+D">Daniel Lehmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 2 figures, to be submitted. Comments appreciated
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Logic in Computer Science (cs.LO); Logic (math.LO)

</div>
<p class="mathjax">P-algebras are a non-commutative, non-associative generalization of Boolean
algebras that are for Quantum Logic what Boolean algebras are for Classical
Logic.The closed subspaces of a separable Hilbert space form a P-algebra under
orthogonal complementation and projection of a subspace onto another one.
P-algebras are complemented orthomodular posets that are not lattices. Atomic
algebras are defined and their main properties are studied. A substructural
logic of sequents is proved to be sound and complete for the logic of
P-algebras.
</p>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07057" title="Abstract">arXiv:2402.07057</a> (cross-list from eess.IV) [<a href="/pdf/2402.07057" title="Download PDF">pdf</a>, <a href="/format/2402.07057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rate-Quality or Energy-Quality Pareto Fronts for Adaptive Video  Streaming?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Katsenou%2C+A">Angeliki Katsenou</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+X">Xinyi Wang</a>, 
<a href="/search/eess?searchtype=author&query=Schien%2C+D">Daniel Schien</a>, 
<a href="/search/eess?searchtype=author&query=Bull%2C+D">David Bull</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6, submitted to a conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Adaptive video streaming is a key enabler for optimising the delivery of
offline encoded video content. The research focus to date has been on
optimisation, based solely on rate-quality curves. This paper adds an
additional dimension, the energy expenditure, and explores construction of
bitrate ladders based on decoding energy-quality curves rather than the
conventional rate-quality curves. Pareto fronts are extracted from the
rate-quality and energy-quality spaces to select optimal points. Bitrate
ladders are constructed from these points using conventional rate-based rules
together with a novel quality-based approach. Evaluation on a subset of
YouTube-UGC videos encoded with x.265 shows that the energy-quality ladders
reduce energy requirements by 28-31% on average at the cost of slightly higher
bitrates. The results indicate that optimising based on energy-quality curves
rather than rate-quality curves and using quality levels to create the rungs
could potentially improve energy efficiency for a comparable quality of
experience.
</p>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07101" title="Abstract">arXiv:2402.07101</a> (cross-list from math.OC) [<a href="/pdf/2402.07101" title="Download PDF">pdf</a>, <a href="/ps/2402.07101" title="Download PostScript">ps</a>, <a href="/format/2402.07101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Complexity of First-Order Methods in Stochastic Bilevel  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kwon%2C+J">Jeongyeol Kwon</a>, 
<a href="/search/math?searchtype=author&query=Kwon%2C+D">Dohyun Kwon</a>, 
<a href="/search/math?searchtype=author&query=Lyu%2C+H">Hanbaek Lyu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We consider the problem of finding stationary points in Bilevel optimization
when the lower-level problem is unconstrained and strongly convex. The problem
has been extensively studied in recent years; the main technical challenge is
to keep track of lower-level solutions $y^*(x)$ in response to the changes in
the upper-level variables $x$. Subsequently, all existing approaches tie their
analyses to a genie algorithm that knows lower-level solutions and, therefore,
need not query any points far from them. We consider a dual question to such
approaches: suppose we have an oracle, which we call $y^*$-aware, that returns
an $O(\epsilon)$-estimate of the lower-level solution, in addition to
first-order gradient estimators {\it locally unbiased} within the
$\Theta(\epsilon)$-ball around $y^*(x)$. We study the complexity of finding
stationary points with such an $y^*$-aware oracle: we propose a simple
first-order method that converges to an $\epsilon$ stationary point using
$O(\epsilon^{-6}), O(\epsilon^{-4})$ access to first-order $y^*$-aware oracles.
Our upper bounds also apply to standard unbiased first-order oracles, improving
the best-known complexity of first-order methods by $O(\epsilon)$ with minimal
assumptions. We then provide the matching $\Omega(\epsilon^{-6})$,
$\Omega(\epsilon^{-4})$ lower bounds without and with an additional smoothness
assumption on $y^*$-aware oracles, respectively. Our results imply that any
approach that simulates an algorithm with an $y^*$-aware oracle must suffer the
same lower bounds.
</p>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07131" title="Abstract">arXiv:2402.07131</a> (cross-list from stat.ML) [<a href="/pdf/2402.07131" title="Download PDF">pdf</a>, <a href="/format/2402.07131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resampling methods for Private Statistical Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Chadha%2C+K">Karan Chadha</a>, 
<a href="/search/stat?searchtype=author&query=Duchi%2C+J">John Duchi</a>, 
<a href="/search/stat?searchtype=author&query=Kuditipudi%2C+R">Rohit Kuditipudi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">We consider the task of constructing confidence intervals with differential
privacy. We propose two private variants of the non-parametric bootstrap, which
privately compute the median of the results of multiple ``little'' bootstraps
run on partitions of the data and give asymptotic bounds on the coverage error
of the resulting confidence intervals. For a fixed differential privacy
parameter $\epsilon$, our methods enjoy the same error rates as that of the
non-private bootstrap to within logarithmic factors in the sample size $n$. We
empirically validate the performance of our methods for mean estimation, median
estimation, and logistic regression with both real and synthetic data. Our
methods achieve similar coverage accuracy to existing methods (and non-private
baselines) while providing notably shorter ($\gtrsim 10$ times) confidence
intervals than previous approaches.
</p>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07148" title="Abstract">arXiv:2402.07148</a> (cross-list from cond-mat.soft) [<a href="/pdf/2402.07148" title="Download PDF">pdf</a>, <a href="/format/2402.07148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for  Large Language Models with Applications in Protein Mechanics and Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Buehler%2C+E+L">Eric L. Buehler</a>, 
<a href="/search/cond-mat?searchtype=author&query=Buehler%2C+M+J">Markus J. Buehler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Soft Condensed Matter (cond-mat.soft)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">We report a mixture of expert strategy to create fine-tuned large language
models using a deep layer-wise token-level approach based on low-rank
adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, we propose
a gating strategy that uses the hidden states to dynamically mix adapted
layers, allowing the resulting X-LoRA model to draw upon different capabilities
and create never-before-used deep layer-wise combinations of adaptations are
established to solve specific tasks. The design is inspired by the biological
principles of universality and diversity, where neural network building blocks
are reused in different hierarchical manifestations. Hence, the X-LoRA model
can be easily implemented for any existing large language model (LLM) without a
need for modifications of the underlying structure. We develop a tailored
X-LoRA model that offers scientific capabilities including forward/inverse
analysis tasks and enhanced reasoning capability, focused on biomaterial
analysis, protein mechanics and design. The impact of this work include access
to readily expandable, adaptable and changeable models with strong domain
knowledge and the capability to integrate across areas of knowledge. With the
X-LoRA model featuring experts in biology, mathematics, reasoning, bio-inspired
materials, mechanics and materials, chemistry, and protein mechanics we conduct
a series of physics-focused case studies. We examine knowledge recall, protein
mechanics forward/inverse tasks, protein design, and adversarial agentic
modeling including ontological knowledge graphs. The model is capable not only
of making quantitative predictions of nanomechanical properties of proteins,
but also reasons over the results and correctly predicts likely mechanisms that
explain distinct molecular behaviors.
</p>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07160" title="Abstract">arXiv:2402.07160</a> (cross-list from stat.ML) [<a href="/pdf/2402.07160" title="Download PDF">pdf</a>, <a href="/format/2402.07160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PASOA- PArticle baSed Bayesian Optimal Adaptive design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Iollo%2C+J">Jacopo Iollo</a>, 
<a href="/search/stat?searchtype=author&query=Heinkel%C3%A9%2C+C">Christophe Heinkel&#xe9;</a>, 
<a href="/search/stat?searchtype=author&query=Alliez%2C+P">Pierre Alliez</a>, 
<a href="/search/stat?searchtype=author&query=Forbes%2C+F">Florence Forbes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO); Methodology (stat.ME)

</div>
<p class="mathjax">We propose a new procedure named PASOA, for Bayesian experimental design,
that performs sequential design optimization by simultaneously providing
accurate estimates of successive posterior distributions for parameter
inference. The sequential design process is carried out via a contrastive
estimation principle, using stochastic optimization and Sequential Monte Carlo
(SMC) samplers to maximise the Expected Information Gain (EIG). As larger
information gains are obtained for larger distances between successive
posterior distributions, this EIG objective may worsen classical SMC
performance. To handle this issue, tempering is proposed to have both a large
information gain and an accurate SMC sampling, that we show is crucial for
performance. This novel combination of stochastic optimization and tempered SMC
allows to jointly handle design optimization and parameter inference. We
provide a proof that the obtained optimal design estimators benefit from some
consistency property. Numerical experiments confirm the potential of the
approach, which outperforms other recent existing procedures.
</p>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07189" title="Abstract">arXiv:2402.07189</a> (cross-list from stat.ML) [<a href="/pdf/2402.07189" title="Download PDF">pdf</a>, <a href="/ps/2402.07189" title="Download PostScript">ps</a>, <a href="/format/2402.07189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving LSH via Tensorized Random Projection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Verma%2C+B+D">Bhisham Dev Verma</a>, 
<a href="/search/stat?searchtype=author&query=Pratap%2C+R">Rameshwar Pratap</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)

</div>
<p class="mathjax">Locality sensitive hashing (LSH) is a fundamental algorithmic toolkit used by
data scientists for approximate nearest neighbour search problems that have
been used extensively in many large scale data processing applications such as
near duplicate detection, nearest neighbour search, clustering, etc. In this
work, we aim to propose faster and space efficient locality sensitive hash
functions for Euclidean distance and cosine similarity for tensor data.
Typically, the naive approach for obtaining LSH for tensor data involves first
reshaping the tensor into vectors, followed by applying existing LSH methods
for vector data $E2LSH$ and $SRP$. However, this approach becomes impractical
for higher order tensors because the size of the reshaped vector becomes
exponential in the order of the tensor. Consequently, the size of LSH
parameters increases exponentially. To address this problem, we suggest two
methods for LSH for Euclidean distance and cosine similarity, namely
$CP-E2LSH$, $TT-E2LSH$, and $CP-SRP$, $TT-SRP$, respectively, building on $CP$
and tensor train $(TT)$ decompositions techniques. Our approaches are space
efficient and can be efficiently applied to low rank $CP$ or $TT$ tensors. We
provide a rigorous theoretical analysis of our proposal on their correctness
and efficacy.
</p>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07192" title="Abstract">arXiv:2402.07192</a> (cross-list from eess.IV) [<a href="/pdf/2402.07192" title="Download PDF">pdf</a>, <a href="/ps/2402.07192" title="Download PostScript">ps</a>, <a href="/format/2402.07192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatio-spectral classification of hyperspectral images for brain cancer  detection during surgical operations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fabelo%2C+H">H. Fabelo</a>, 
<a href="/search/eess?searchtype=author&query=Ortega%2C+S">S. Ortega</a>, 
<a href="/search/eess?searchtype=author&query=Ravi%2C+D">D. Ravi</a>, 
<a href="/search/eess?searchtype=author&query=Kiran%2C+B+R">B. R. Kiran</a>, 
<a href="/search/eess?searchtype=author&query=Sosa%2C+C">C. Sosa</a>, 
<a href="/search/eess?searchtype=author&query=Bulters%2C+D">D. Bulters</a>, 
<a href="/search/eess?searchtype=author&query=Callico%2C+G+M">G. M. Callico</a>, 
<a href="/search/eess?searchtype=author&query=Bulstrode%2C+H">H. Bulstrode</a>, 
<a href="/search/eess?searchtype=author&query=Szolna%2C+A">A. Szolna</a>, 
<a href="/search/eess?searchtype=author&query=Pineiro%2C+J+F">J. F. Pineiro</a>, 
<a href="/search/eess?searchtype=author&query=Kabwama%2C+S">S. Kabwama</a>, 
<a href="/search/eess?searchtype=author&query=Madronal%2C+D">D. Madronal</a>, 
<a href="/search/eess?searchtype=author&query=Lazcano%2C+R">R. Lazcano</a>, 
<a href="/search/eess?searchtype=author&query=OShanahan%2C+A+J">A. J. OShanahan</a>, 
<a href="/search/eess?searchtype=author&query=Bisshopp%2C+S">S. Bisshopp</a>, 
<a href="/search/eess?searchtype=author&query=Hernandez%2C+M">M. Hernandez</a>, 
<a href="/search/eess?searchtype=author&query=Baez-Quevedo%2C+A">A. Baez-Quevedo</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+G+Z">G. Z. Yang</a>, 
<a href="/search/eess?searchtype=author&query=Stanciulescu%2C+B">B. Stanciulescu</a>, 
<a href="/search/eess?searchtype=author&query=Salvador%2C+R">R. Salvador</a>, 
<a href="/search/eess?searchtype=author&query=Juarez%2C+E">E. Juarez</a>, 
<a href="/search/eess?searchtype=author&query=Sarmiento%2C+R">R. Sarmiento</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Surgery for brain cancer is a major problem in neurosurgery. The diffuse
infiltration into the surrounding normal brain by these tumors makes their
accurate identification by the naked eye difficult. Since surgery is the common
treatment for brain cancer, an accurate radical resection of the tumor leads to
improved survival rates for patients. However, the identification of the tumor
boundaries during surgery is challenging. Hyperspectral imaging is a
noncontact, non-ionizing and non-invasive technique suitable for medical
diagnosis. This study presents the development of a novel classification method
taking into account the spatial and spectral characteristics of the
hyperspectral images to help neurosurgeons to accurately determine the tumor
boundaries in surgical-time during the resection, avoiding excessive excision
of normal tissue or unintentionally leaving residual tumor. The algorithm
proposed in this study to approach an efficient solution consists of a hybrid
framework that combines both supervised and unsupervised machine learning
methods. To evaluate the proposed approach, five hyperspectral images of
surface of the brain affected by glioblastoma tumor in vivo from five different
patients have been used. The final classification maps obtained have been
analyzed and validated by specialists. These preliminary results are promising,
obtaining an accurate delineation of the tumor area.
</p>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07220" title="Abstract">arXiv:2402.07220</a> (cross-list from eess.IV) [<a href="/pdf/2402.07220" title="Download PDF">pdf</a>, <a href="/format/2402.07220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KVQ: Kaleidoscope Video Quality Assessment for Short-form Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lu%2C+Y">Yiting Lu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/eess?searchtype=author&query=Pei%2C+Y">Yajing Pei</a>, 
<a href="/search/eess?searchtype=author&query=Yuan%2C+K">Kun Yuan</a>, 
<a href="/search/eess?searchtype=author&query=Xie%2C+Q">Qizhi Xie</a>, 
<a href="/search/eess?searchtype=author&query=Qu%2C+Y">Yunpeng Qu</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+M">Ming Sun</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+C">Chao Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Z">Zhibo Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Short-form UGC video platforms, like Kwai and TikTok, have been an emerging
and irreplaceable mainstream media form, thriving on user-friendly engagement,
and kaleidoscope creation, etc. However, the advancing content-generation
modes, e.g., special effects, and sophisticated processing workflows, e.g.,
de-artifacts, have introduced significant challenges to recent UGC video
quality assessment: (i) the ambiguous contents hinder the identification of
quality-determined regions. (ii) the diverse and complicated hybrid distortions
are hard to distinguish. To tackle the above challenges and assist in the
development of short-form videos, we establish the first large-scale
Kaleidoscope short Video database for Quality assessment, termed KVQ, which
comprises 600 user-uploaded short videos and 3600 processed videos through the
diverse practical processing workflows, including pre-processing, transcoding,
and enhancement. Among them, the absolute quality score of each video and
partial ranking score among indistinguishable samples are provided by a team of
professional researchers specializing in image processing. Based on this
database, we propose the first short-form video quality evaluator, i.e., KSVQE,
which enables the quality evaluator to identify the quality-determined
semantics with the content understanding of large vision language models (i.e.,
CLIP) and distinguish the distortions with the distortion understanding module.
Experimental results have shown the effectiveness of KSVQE on our KVQ database
and popular VQA databases.
</p>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07245" title="Abstract">arXiv:2402.07245</a> (cross-list from eess.IV) [<a href="/pdf/2402.07245" title="Download PDF">pdf</a>, <a href="/format/2402.07245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-Mamba-UNet: Pixel-Level Contrastive Cross-Supervised Visual  Mamba-based UNet for Semi-Supervised Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Ziyang Wang</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+C">Chao Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Medical image segmentation is essential in diagnostics, treatment planning,
and healthcare, with deep learning offering promising advancements. Notably,
Convolutional Neural Network (CNN) excel in capturing local image features,
whereas Vision Transformer (ViT) adeptly model long-range dependencies through
multi-head self-attention mechanisms. Despite their strengths, both CNN and ViT
face challenges in efficiently processing long-range dependencies within
medical images, often requiring substantial computational resources. This
issue, combined with the high cost and limited availability of expert
annotations, poses significant obstacles to achieving precise segmentation. To
address these challenges, this paper introduces the Semi-Mamba-UNet, which
integrates a visual mamba-based UNet architecture with a conventional UNet into
a semi-supervised learning (SSL) framework. This innovative SSL approach
leverages dual networks to jointly generate pseudo labels and cross supervise
each other, drawing inspiration from consistency regularization techniques.
Furthermore, we introduce a self-supervised pixel-level contrastive learning
strategy, employing a projector pair to further enhance feature learning
capabilities. Our comprehensive evaluation on a publicly available MRI cardiac
segmentation dataset, comparing against various SSL frameworks with different
UNet-based segmentation networks, highlights the superior performance of
Semi-Mamba-UNet. The source code has been made publicly accessible.
</p>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07268" title="Abstract">arXiv:2402.07268</a> (cross-list from q-bio.GN) [<a href="/pdf/2402.07268" title="Download PDF">pdf</a>, <a href="/format/2402.07268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Highly Accurate Disease Diagnosis and Highly Reproducible Biomarker  Identification with PathFormer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Dong%2C+Z">Zehao Dong</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhao%2C+Q">Qihang Zhao</a>, 
<a href="/search/q-bio?searchtype=author&query=Payne%2C+P+R+O">Philip R.O. Payne</a>, 
<a href="/search/q-bio?searchtype=author&query=Province%2C+M+A">Michael A Province</a>, 
<a href="/search/q-bio?searchtype=author&query=Cruchaga%2C+C">Carlos Cruchaga</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+M">Muhan Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhao%2C+T">Tianyu Zhao</a>, 
<a href="/search/q-bio?searchtype=author&query=Chen%2C+Y">Yixin Chen</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+F">Fuhai Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Biomarker identification is critical for precise disease diagnosis and
understanding disease pathogenesis in omics data analysis, like using fold
change and regression analysis. Graph neural networks (GNNs) have been the
dominant deep learning model for analyzing graph-structured data. However, we
found two major limitations of existing GNNs in omics data analysis, i.e.,
limited-prediction (diagnosis) accuracy and limited-reproducible biomarker
identification capacity across multiple datasets. The root of the challenges is
the unique graph structure of biological signaling pathways, which consists of
a large number of targets and intensive and complex signaling interactions
among these targets. To resolve these two challenges, in this study, we
presented a novel GNN model architecture, named PathFormer, which
systematically integrate signaling network, priori knowledge and omics data to
rank biomarkers and predict disease diagnosis. In the comparison results,
PathFormer outperformed existing GNN models significantly in terms of highly
accurate prediction capability ( 30% accuracy improvement in disease diagnosis
compared with existing GNN models) and high reproducibility of biomarker
ranking across different datasets. The improvement was confirmed using two
independent Alzheimer's Disease (AD) and cancer transcriptomic datasets. The
PathFormer model can be directly applied to other omics data analysis studies.
</p>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07298" title="Abstract">arXiv:2402.07298</a> (cross-list from eess.IV) [<a href="/pdf/2402.07298" title="Download PDF">pdf</a>, <a href="/format/2402.07298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supervised Reconstruction for Silhouette Tomography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bell%2C+E">Evan Bell</a>, 
<a href="/search/eess?searchtype=author&query=McCann%2C+M+T">Michael T. McCann</a>, 
<a href="/search/eess?searchtype=author&query=Klasky%2C+M">Marc Klasky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In this paper, we introduce silhouette tomography, a novel formulation of
X-ray computed tomography that relies only on the geometry of the imaging
system. We formulate silhouette tomography mathematically and provide a simple
method for obtaining a particular solution to the problem, assuming that any
solution exists. We then propose a supervised reconstruction approach that uses
a deep neural network to solve the silhouette tomography problem. We present
experimental results on a synthetic dataset that demonstrate the effectiveness
of the proposed method.
</p>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07307" title="Abstract">arXiv:2402.07307</a> (cross-list from stat.ML) [<a href="/pdf/2402.07307" title="Download PDF">pdf</a>, <a href="/format/2402.07307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Consistent Conformal Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=van+der+Laan%2C+L">Lars van der Laan</a>, 
<a href="/search/stat?searchtype=author&query=Alaa%2C+A+M">Ahmed M. Alaa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">In decision-making guided by machine learning, decision-makers often take
identical actions in contexts with identical predicted outcomes. Conformal
prediction helps decision-makers quantify outcome uncertainty for actions,
allowing for better risk management. Inspired by this perspective, we introduce
self-consistent conformal prediction, which yields both Venn-Abers calibrated
predictions and conformal prediction intervals that are valid conditional on
actions prompted by model predictions. Our procedure can be applied post-hoc to
any black-box predictor to provide rigorous, action-specific decision-making
guarantees. Numerical experiments show our approach strikes a balance between
interval efficiency and conditional validity.
</p>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07322" title="Abstract">arXiv:2402.07322</a> (cross-list from math.ST) [<a href="/pdf/2402.07322" title="Download PDF">pdf</a>, <a href="/format/2402.07322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interference Among First-Price Pacing Equilibria: A Bias and Variance  Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liao%2C+L">Luofeng Liao</a>, 
<a href="/search/math?searchtype=author&query=Kroer%2C+C">Christian Kroer</a>, 
<a href="/search/math?searchtype=author&query=Leonenkov%2C+S">Sergei Leonenkov</a>, 
<a href="/search/math?searchtype=author&query=Schrijvers%2C+O">Okke Schrijvers</a>, 
<a href="/search/math?searchtype=author&query=Shi%2C+L">Liang Shi</a>, 
<a href="/search/math?searchtype=author&query=Stier-Moses%2C+N">Nicolas Stier-Moses</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+C">Congshan Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Computer Science and Game Theory (cs.GT); Econometrics (econ.EM)

</div>
<p class="mathjax">Online A/B testing is widely used in the internet industry to inform
decisions on new feature roll-outs. For online marketplaces (such as
advertising markets), standard approaches to A/B testing may lead to biased
results when buyers operate under a budget constraint, as budget consumption in
one arm of the experiment impacts performance of the other arm. To counteract
this interference, one can use a budget-split design where the budget
constraint operates on a per-arm basis and each arm receives an equal fraction
of the budget, leading to ``budget-controlled A/B testing.'' Despite clear
advantages of budget-controlled A/B testing, performance degrades when budget
are split too small, limiting the overall throughput of such systems. In this
paper, we propose a parallel budget-controlled A/B testing design where we use
market segmentation to identify submarkets in the larger market, and we run
parallel experiments on each submarket.
<br />Our contributions are as follows: First, we introduce and demonstrate the
effectiveness of the parallel budget-controlled A/B test design with submarkets
in a large online marketplace environment. Second, we formally define market
interference in first-price auction markets using the first price pacing
equilibrium (FPPE) framework. Third, we propose a debiased surrogate that
eliminates the first-order bias of FPPE, drawing upon the principles of
sensitivity analysis in mathematical programs. Fourth, we derive a plug-in
estimator for the surrogate and establish its asymptotic normality. Fifth, we
provide an estimation procedure for submarket parallel budget-controlled A/B
tests. Finally, we present numerical examples on semi-synthetic data,
confirming that the debiasing technique achieves the desired coverage
properties.
</p>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07336" title="Abstract">arXiv:2402.07336</a> (cross-list from math.LO) [<a href="/pdf/2402.07336" title="Download PDF">pdf</a>, <a href="/ps/2402.07336" title="Download PostScript">ps</a>, <a href="/format/2402.07336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Obligations and permissions on selfextensional logics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=De+Domenico%2C+A">Andrea De Domenico</a> (1), 
<a href="/search/math?searchtype=author&query=Farjami%2C+A">Ali Farjami</a> (2), 
<a href="/search/math?searchtype=author&query=Manoorkar%2C+K">Krishna Manoorkar</a> (1), 
<a href="/search/math?searchtype=author&query=Palmigiano%2C+A">Alessandra Palmigiano</a> (1 and 3), 
<a href="/search/math?searchtype=author&query=Panettiere%2C+M">Mattia Panettiere</a> (1), 
<a href="/search/math?searchtype=author&query=Wang%2C+X">Xiaolong Wang</a> (1 and 4) ((1) Vrije Universiteit Amsterdam, (2) Iran University of Science and Technology, (3) University of Johannesburg, (4) Shandong University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">We further develop the abstract algebraic logic approach to input/output
logic initiated in \cite{wollic22}, where the family of selfextensional logics
was proposed as a general background environment for input/output logics. In
this paper, we introduce and discuss the generalizations of several types of
permission (negative, dual negative, static, dynamic), as well as their
interactions with normative systems, to various families of selfextensional
logics, thereby proposing a systematic approach to the definition of normative
and permission systems on nonclassical propositional bases.
</p>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07341" title="Abstract">arXiv:2402.07341</a> (cross-list from stat.ML) [<a href="/pdf/2402.07341" title="Download PDF">pdf</a>, <a href="/format/2402.07341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Noise-Adaptive Confidence Sets for Linear Bandits and Application to  Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Jun%2C+K">Kwang-Sung Jun</a>, 
<a href="/search/stat?searchtype=author&query=Kim%2C+J">Jungtaek Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Adapting to a priori unknown noise level is a very important but challenging
problem in sequential decision-making as efficient exploration typically
requires knowledge of the noise level, which is often loosely specified. We
report significant progress in addressing this issue in linear bandits in two
respects. First, we propose a novel confidence set that is `semi-adaptive' to
the unknown sub-Gaussian parameter $\sigma_*^2$ in the sense that the
(normalized) confidence width scales with $\sqrt{d\sigma_*^2 + \sigma_0^2}$
where $d$ is the dimension and $\sigma_0^2$ is the specified sub-Gaussian
parameter (known) that can be much larger than $\sigma_*^2$. This is a
significant improvement over $\sqrt{d\sigma_0^2}$ of the standard confidence
set of Abbasi-Yadkori et al. (2011), especially when $d$ is large. We show that
this leads to an improved regret bound in linear bandits. Second, for bounded
rewards, we propose a novel variance-adaptive confidence set that has a much
improved numerical performance upon prior art. We then apply this confidence
set to develop, as we claim, the first practical variance-adaptive linear
bandit algorithm via an optimistic approach, which is enabled by our novel
regret analysis technique. Both of our confidence sets rely critically on
`regret equality' from online learning. Our empirical evaluation in Bayesian
optimization tasks shows that our algorithms demonstrate better or comparable
performance compared to existing methods.
</p>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07354" title="Abstract">arXiv:2402.07354</a> (cross-list from eess.IV) [<a href="/pdf/2402.07354" title="Download PDF">pdf</a>, <a href="/format/2402.07354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Re-DiffiNet: Modeling discrepancies in tumor segmentation using  diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ren%2C+T">Tianyi Ren</a>, 
<a href="/search/eess?searchtype=author&query=Sharma%2C+A">Abhishek Sharma</a>, 
<a href="/search/eess?searchtype=author&query=Rivera%2C+J+H">Juampablo Heras Rivera</a>, 
<a href="/search/eess?searchtype=author&query=Rebala%2C+H">Harshitha Rebala</a>, 
<a href="/search/eess?searchtype=author&query=Honey%2C+E">Ethan Honey</a>, 
<a href="/search/eess?searchtype=author&query=Chopra%2C+A">Agamdeep Chopra</a>, 
<a href="/search/eess?searchtype=author&query=Kurt%2C+M">Mehmet Kurt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Identification of tumor margins is essential for surgical decision-making for
glioblastoma patients and provides reliable assistance for neurosurgeons.
Despite improvements in deep learning architectures for tumor segmentation over
the years, creating a fully autonomous system suitable for clinical floors
remains a formidable challenge because the model predictions have not yet
reached the desired level of accuracy and generalizability for clinical
applications. Generative modeling techniques have seen significant improvements
in recent times. Specifically, Generative Adversarial Networks (GANs) and
Denoising-diffusion-based models (DDPMs) have been used to generate
higher-quality images with fewer artifacts and finer attributes. In this work,
we introduce a framework called Re-Diffinet for modeling the discrepancy
between the outputs of a segmentation model like U-Net and the ground truth,
using DDPMs. By explicitly modeling the discrepancy, the results show an
average improvement of 0.55\% in the Dice score and 16.28\% in HD95 from
cross-validation over 5-folds, compared to the state-of-the-art U-Net
segmentation model.
</p>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07355" title="Abstract">arXiv:2402.07355</a> (cross-list from math.ST) [<a href="/pdf/2402.07355" title="Download PDF">pdf</a>, <a href="/ps/2402.07355" title="Download PostScript">ps</a>, <a href="/format/2402.07355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sampling from the Mean-Field Stationary Distribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kook%2C+Y">Yunbum Kook</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+M+S">Matthew S. Zhang</a>, 
<a href="/search/math?searchtype=author&query=Chewi%2C+S">Sinho Chewi</a>, 
<a href="/search/math?searchtype=author&query=Erdogdu%2C+M+A">Murat A. Erdogdu</a>, 
<a href="/search/math?searchtype=author&query=Mufan">Mufan</a> (Bill)Li
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the complexity of sampling from the stationary distribution of a
mean-field SDE, or equivalently, the complexity of minimizing a functional over
the space of probability measures which includes an interaction term.
<br />Our main insight is to decouple the two key aspects of this problem: (1)
approximation of the mean-field SDE via a finite-particle system, via
uniform-in-time propagation of chaos, and (2) sampling from the finite-particle
stationary distribution, via standard log-concave samplers. Our approach is
conceptually simpler and its flexibility allows for incorporating the
state-of-the-art for both algorithms and theory. This leads to improved
guarantees in numerous settings, including better guarantees for optimizing
certain two-layer neural networks in the mean-field regime.
</p>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07357" title="Abstract">arXiv:2402.07357</a> (cross-list from stat.ML) [<a href="/pdf/2402.07357" title="Download PDF">pdf</a>, <a href="/format/2402.07357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regression Trees for Fast and Adaptive Prediction Intervals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cabezas%2C+L+M+C">Luben M. C. Cabezas</a>, 
<a href="/search/stat?searchtype=author&query=Otto%2C+M+P">Mateus P. Otto</a>, 
<a href="/search/stat?searchtype=author&query=Izbicki%2C+R">Rafael Izbicki</a>, 
<a href="/search/stat?searchtype=author&query=Stern%2C+R+B">Rafael B. Stern</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Predictive models make mistakes. Hence, there is a need to quantify the
uncertainty associated with their predictions. Conformal inference has emerged
as a powerful tool to create statistically valid prediction regions around
point predictions, but its naive application to regression problems yields
non-adaptive regions. New conformal scores, often relying upon quantile
regressors or conditional density estimators, aim to address this limitation.
Although they are useful for creating prediction bands, these scores are
detached from the original goal of quantifying the uncertainty around an
arbitrary predictive model. This paper presents a new, model-agnostic family of
methods to calibrate prediction intervals for regression problems with local
coverage guarantees. Our approach is based on pursuing the coarsest partition
of the feature space that approximates conditional coverage. We create this
partition by training regression trees and Random Forests on conformity scores.
Our proposal is versatile, as it applies to various conformity scores and
prediction settings and demonstrates superior scalability and performance
compared to established baselines in simulated and real-world datasets. We
provide a Python package locart that implements our methods using the standard
scikit-learn interface.
</p>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07365" title="Abstract">arXiv:2402.07365</a> (cross-list from math.OC) [<a href="/pdf/2402.07365" title="Download PDF">pdf</a>, <a href="/format/2402.07365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Deep Learning Method for Optimal Investment Under Relative Performance  Criteria Among Heterogeneous Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lauri%C3%A8re%2C+M">Mathieu Lauri&#xe8;re</a>, 
<a href="/search/math?searchtype=author&query=Tangpi%2C+L">Ludovic Tangpi</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+X">Xuchen Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Graphon games have been introduced to study games with many players who
interact through a weighted graph of interaction. By passing to the limit, a
game with a continuum of players is obtained, in which the interactions are
through a graphon. In this paper, we focus on a graphon game for optimal
investment under relative performance criteria, and we propose a deep learning
method. The method builds upon two key ingredients: first, a characterization
of Nash equilibria by forward-backward stochastic differential equations and,
second, recent advances of machine learning algorithms for stochastic
differential games. We provide numerical experiments on two different financial
models. In each model, we compare the effect of several graphons, which
correspond to different structures of interactions.
</p>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07383" title="Abstract">arXiv:2402.07383</a> (cross-list from eess.AS) [<a href="/pdf/2402.07383" title="Download PDF">pdf</a>, <a href="/format/2402.07383" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kanda%2C+N">Naoyuki Kanda</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+X">Xiaofei Wang</a>, 
<a href="/search/eess?searchtype=author&query=Eskimez%2C+S+E">Sefik Emre Eskimez</a>, 
<a href="/search/eess?searchtype=author&query=Thakker%2C+M">Manthan Thakker</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+H">Hemin Yang</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+Z">Zirun Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Tang%2C+M">Min Tang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+C">Canrun Li</a>, 
<a href="/search/eess?searchtype=author&query=Tsai%2C+S">Steven Tsai</a>, 
<a href="/search/eess?searchtype=author&query=Xiao%2C+Z">Zhen Xiao</a>, 
<a href="/search/eess?searchtype=author&query=Xia%2C+Y">Yufei Xia</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Jinzhu Li</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yanqing Liu</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+S">Sheng Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Zeng%2C+M">Michael Zeng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> See <a href="https://aka.ms/elate/">this https URL</a> for demo samples
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD)

</div>
<p class="mathjax">Laughter is one of the most expressive and natural aspects of human speech,
conveying emotions, social cues, and humor. However, most text-to-speech (TTS)
systems lack the ability to produce realistic and appropriate laughter sounds,
limiting their applications and user experience. While there have been prior
works to generate natural laughter, they fell short in terms of controlling the
timing and variety of the laughter to be generated. In this work, we propose
ELaTE, a zero-shot TTS that can generate natural laughing speech of any speaker
based on a short audio prompt with precise control of laughter timing and
expression. Specifically, ELaTE works on the audio prompt to mimic the voice
characteristic, the text prompt to indicate the contents of the generated
speech, and the input to control the laughter expression, which can be either
the start and end times of laughter, or the additional audio prompt that
contains laughter to be mimicked. We develop our model based on the foundation
of conditional flow-matching-based zero-shot TTS, and fine-tune it with
frame-level representation from a laughter detector as additional conditioning.
With a simple scheme to mix small-scale laughter-conditioned data with
large-scale pre-training data, we demonstrate that a pre-trained zero-shot TTS
model can be readily fine-tuned to generate natural laughter with precise
controllability, without losing any quality of the pre-trained zero-shot TTS
model. Through the evaluations, we show that ELaTE can generate laughing speech
with significantly higher quality and controllability compared to conventional
models. See https://aka.ms/elate/ for demo samples.
</p>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07388" title="Abstract">arXiv:2402.07388</a> (cross-list from math.ST) [<a href="/pdf/2402.07388" title="Download PDF">pdf</a>, <a href="/ps/2402.07388" title="Download PostScript">ps</a>, <a href="/format/2402.07388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Limits of Assumption-free Tests for Algorithm Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Luo%2C+Y">Yuetian Luo</a>, 
<a href="/search/math?searchtype=author&query=Barber%2C+R+F">Rina Foygel Barber</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Algorithm evaluation and comparison are fundamental questions in machine
learning and statistics -- how well does an algorithm perform at a given
modeling task, and which algorithm performs best? Many methods have been
developed to assess algorithm performance, often based around cross-validation
type strategies, retraining the algorithm of interest on different subsets of
the data and assessing its performance on the held-out data points. Despite the
broad use of such procedures, the theoretical properties of these methods are
not yet fully understood. In this work, we explore some fundamental limits for
answering these questions with limited amounts of data. In particular, we make
a distinction between two questions: how good is an algorithm $A$ at the
problem of learning from a training set of size $n$, versus, how good is a
particular fitted model produced by running $A$ on a particular training data
set of size $n$?
<br />Our main results prove that, for any test that treats the algorithm $A$ as a
``black box'' (i.e., we can only study the behavior of $A$ empirically), there
is a fundamental limit on our ability to carry out inference on the performance
of $A$, unless the number of available data points $N$ is many times larger
than the sample size $n$ of interest. (On the other hand, evaluating the
performance of a particular fitted model is easy as long as a holdout data set
is available -- that is, as long as $N-n$ is not too small.) We also ask
whether an assumption of algorithmic stability might be sufficient to
circumvent this hardness result. Surprisingly, we find that this is not the
case: the same hardness result still holds for the problem of evaluating the
performance of $A$, aside from a high-stability regime where fitted models are
essentially nonrandom. Finally, we also establish similar hardness results for
the problem of comparing multiple algorithms.
</p>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07391" title="Abstract">arXiv:2402.07391</a> (cross-list from stat.ML) [<a href="/pdf/2402.07391" title="Download PDF">pdf</a>, <a href="/format/2402.07391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Replicability is Asymptotically Free in Multi-armed Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Komiyama%2C+J">Junpei Komiyama</a>, 
<a href="/search/stat?searchtype=author&query=Ito%2C+S">Shinji Ito</a>, 
<a href="/search/stat?searchtype=author&query=Yoshida%2C+Y">Yuichi Yoshida</a>, 
<a href="/search/stat?searchtype=author&query=Koshino%2C+S">Souta Koshino</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This work is motivated by the growing demand for reproducible machine
learning. We study the stochastic multi-armed bandit problem. In particular, we
consider a replicable algorithm that ensures, with high probability, that the
algorithm's sequence of actions is not affected by the randomness inherent in
the dataset. We observe that existing algorithms require $O(1/\rho^2)$ times
more regret than nonreplicable algorithms, where $\rho$ is the level of
nonreplication. However, we demonstrate that this additional cost is
unnecessary when the time horizon $T$ is sufficiently large for a given $\rho$,
provided that the magnitude of the confidence bounds is chosen carefully. We
introduce an explore-then-commit algorithm that draws arms uniformly before
committing to a single arm. Additionally, we examine a successive elimination
algorithm that eliminates suboptimal arms at the end of each phase. To ensure
the replicability of these algorithms, we incorporate randomness into their
decision-making processes. We extend the use of successive elimination to the
linear bandit problem as well. For the analysis of these algorithms, we propose
a principled approach to limiting the probability of nonreplication. This
approach elucidates the steps that existing research has implicitly followed.
Furthermore, we derive the first lower bound for the two-armed replicable
bandit problem, which implies the optimality of the proposed algorithms up to a
$\log\log T$ factor for the two-armed case.
</p>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07435" title="Abstract">arXiv:2402.07435</a> (cross-list from q-fin.ST) [<a href="/pdf/2402.07435" title="Download PDF">pdf</a>, <a href="/ps/2402.07435" title="Download PostScript">ps</a>, <a href="/format/2402.07435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Currency Fluctuations: A Comparative Study of GARCH, EWMA, and  IV Models for GBP/USD and EUR/GBP Pairs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Tondapu%2C+N">Narayan Tondapu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Finance (q-fin.ST)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this study, we examine the fluctuation in the value of the Great Britain
Pound (GBP). We focus particularly on its relationship with the United States
Dollar (USD) and the Euro (EUR) currency pairs. Utilizing data from June 15,
2018, to June 15, 2023, we apply various mathematical models to assess their
effectiveness in predicting the 20-day variation in the pairs' daily returns.
Our analysis involves the implementation of Exponentially Weighted Moving
Average (EWMA), Generalized Autoregressive Conditional Heteroskedasticity
(GARCH) models, and Implied Volatility (IV) models. To evaluate their
performance, we compare the accuracy of their predictions using Root Mean
Square Error (RMSE) and Mean Absolute Error (MAE) metrics. We delve into the
intricacies of GARCH models, examining their statistical characteristics when
applied to the provided dataset. Our findings suggest the existence of
asymmetric returns in the EUR/GBP pair, while such evidence is inconclusive for
the GBP/USD pair. Additionally, we observe that GARCH-type models better fit
the data when assuming residuals follow a standard t-distribution rather than a
standard normal distribution. Furthermore, we investigate the efficacy of
different forecasting techniques within GARCH-type models. Comparing rolling
window forecasts to expanding window forecasts, we find no definitive
superiority in either approach across the tested scenarios. Our experiments
reveal that for the GBP/USD pair, the most accurate volatility forecasts stem
from the utilization of GARCH models employing a rolling window methodology.
Conversely, for the EUR/GBP pair, optimal forecasts are derived from GARCH
models and Ordinary Least Squares (OLS) models incorporating the annualized
implied volatility of the exchange rate as an independent variable.
</p>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07436" title="Abstract">arXiv:2402.07436</a> (cross-list from math.AT) [<a href="/pdf/2402.07436" title="Download PDF">pdf</a>, <a href="/format/2402.07436" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Novel definition and quantitative analysis of branch structure with  topological data analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Oda%2C+H">Haruhisa Oda</a>, 
<a href="/search/math?searchtype=author&query=Kida%2C+M">Mayuko Kida</a>, 
<a href="/search/math?searchtype=author&query=Nakata%2C+Y">Yoichi Nakata</a>, 
<a href="/search/math?searchtype=author&query=Kurihara%2C+H">Hiroki Kurihara</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Topology (math.AT)</span>; Computational Geometry (cs.CG); Computer Vision and Pattern Recognition (cs.CV); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">While branching network structures abound in nature, their objective analysis
is more difficult than expected because existing quantitative methods often
rely on the subjective judgment of branch structures. This problem is
particularly pronounced when dealing with images comprising discrete particles.
Here we propose an objective framework for quantitative analysis of branching
networks by introducing the mathematical definitions for internal and external
structures based on topological data analysis, specifically, persistent
homology. We compare persistence diagrams constructed from images with and
without plots on the convex hull. The unchanged points in the two diagrams are
the internal structures and the difference between the two diagrams is the
external structures. We construct a mathematical theory for our method and show
that the internal structures have a monotonicity relationship with respect to
the plots on the convex hull, while the external structures do not. This is the
phenomenon related to the resolution of the image. Our method can be applied to
a wide range of branch structures in biology, enabling objective analysis of
numbers, spatial distributions, sizes, and more. Additionally, our method has
the potential to be combined with other tools in topological data analysis,
such as the generalized persistence landscape.
</p>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07445" title="Abstract">arXiv:2402.07445</a> (cross-list from stat.ML) [<a href="/pdf/2402.07445" title="Download PDF">pdf</a>, <a href="/format/2402.07445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Top-$K$ ranking with a monotone adversary
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Yang%2C+Y">Yuepeng Yang</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+A">Antares Chen</a>, 
<a href="/search/stat?searchtype=author&query=Orecchia%2C+L">Lorenzo Orecchia</a>, 
<a href="/search/stat?searchtype=author&query=Ma%2C+C">Cong Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Information Theory (cs.IT); Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
<p class="mathjax">In this paper, we address the top-$K$ ranking problem with a monotone
adversary. We consider the scenario where a comparison graph is randomly
generated and the adversary is allowed to add arbitrary edges. The
statistician's goal is then to accurately identify the top-$K$ preferred items
based on pairwise comparisons derived from this semi-random comparison graph.
The main contribution of this paper is to develop a weighted maximum likelihood
estimator (MLE) that achieves near-optimal sample complexity, up to a
$\log^2(n)$ factor, where n denotes the number of items under comparison. This
is made possible through a combination of analytical and algorithmic
innovations. On the analytical front, we provide a refined $\ell_\infty$ error
analysis of the weighted MLE that is more explicit and tighter than existing
analyses. It relates the $\ell_\infty$ error with the spectral properties of
the weighted comparison graph. Motivated by this, our algorithmic innovation
involves the development of an SDP-based approach to reweight the semi-random
graph and meet specified spectral properties. Additionally, we propose a
first-order method based on the Matrix Multiplicative Weight Update (MMWU)
framework. This method efficiently solves the resulting SDP in nearly-linear
time relative to the size of the semi-random comparison graph.
</p>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07463" title="Abstract">arXiv:2402.07463</a> (cross-list from stat.CO) [<a href="/pdf/2402.07463" title="Download PDF">pdf</a>, <a href="/format/2402.07463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PyDMD: A Python package for robust dynamic mode decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ichinaga%2C+S+M">Sara M. Ichinaga</a>, 
<a href="/search/stat?searchtype=author&query=Andreuzzi%2C+F">Francesco Andreuzzi</a>, 
<a href="/search/stat?searchtype=author&query=Demo%2C+N">Nicola Demo</a>, 
<a href="/search/stat?searchtype=author&query=Tezzele%2C+M">Marco Tezzele</a>, 
<a href="/search/stat?searchtype=author&query=Lapo%2C+K">Karl Lapo</a>, 
<a href="/search/stat?searchtype=author&query=Rozza%2C+G">Gianluigi Rozza</a>, 
<a href="/search/stat?searchtype=author&query=Brunton%2C+S+L">Steven L. Brunton</a>, 
<a href="/search/stat?searchtype=author&query=Kutz%2C+J+N">J. Nathan Kutz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Systems and Control (eess.SY); Dynamical Systems (math.DS); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">The dynamic mode decomposition (DMD) is a simple and powerful data-driven
modeling technique that is capable of revealing coherent spatiotemporal
patterns from data. The method's linear algebra-based formulation additionally
allows for a variety of optimizations and extensions that make the algorithm
practical and viable for real-world data analysis. As a result, DMD has grown
to become a leading method for dynamical system analysis across multiple
scientific disciplines. PyDMD is a Python package that implements DMD and
several of its major variants. In this work, we expand the PyDMD package to
include a number of cutting-edge DMD methods and tools specifically designed to
handle dynamics that are noisy, multiscale, parameterized, prohibitively
high-dimensional, or even strongly nonlinear. We provide a complete overview of
the features available in PyDMD as of version 1.0, along with a brief overview
of the theory behind the DMD algorithm, information for developers, tips
regarding practical DMD usage, and introductory coding examples. All code is
available at https://github.com/PyDMD/PyDMD .
</p>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07472" title="Abstract">arXiv:2402.07472</a> (cross-list from physics.comp-ph) [<a href="/pdf/2402.07472" title="Download PDF">pdf</a>, <a href="/format/2402.07472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cartesian atomic cluster expansion for machine learning interatomic  potentials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Cheng%2C+B">Bingqing Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Machine Learning (cs.LG); Chemical Physics (physics.chem-ph)

</div>
<p class="mathjax">Machine learning interatomic potentials are revolutionizing large-scale,
accurate atomistic modelling in material science and chemistry. These
potentials often use atomic cluster expansion or equivariant message passing
with spherical harmonics as basis functions. However, the dependence on
Clebsch-Gordan coefficients for maintaining rotational symmetry leads to
computational inefficiencies and redundancies. We propose an alternative: a
Cartesian-coordinates-based atomic density expansion. This approach provides a
complete description of atomic environments while maintaining interaction body
orders. Additionally, we integrate low-dimensional embeddings of various
chemical elements and inter-atomic message passing. The resulting potential,
named Cartesian Atomic Cluster Expansion (CACE), exhibits good accuracy,
stability, and generalizability. We validate its performance in diverse
systems, including bulk water, small molecules, and 25-element high-entropy
alloys.
</p>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07476" title="Abstract">arXiv:2402.07476</a> (cross-list from quant-ph) [<a href="/pdf/2402.07476" title="Download PDF">pdf</a>, <a href="/format/2402.07476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expansion of higher-dimensional cubical complexes with application to  quantum locally testable codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Dinur%2C+I">Irit Dinur</a>, 
<a href="/search/quant-ph?searchtype=author&query=Lin%2C+T">Ting-Chun Lin</a>, 
<a href="/search/quant-ph?searchtype=author&query=Vidick%2C+T">Thomas Vidick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC); Information Theory (cs.IT)

</div>
<p class="mathjax">We introduce a higher-dimensional "cubical" chain complex and apply it to the
design of quantum locally testable codes. Our cubical chain complex can be
constructed for any dimension $t$, and in a precise sense generalizes the
Sipser-Spielman construction of expander codes (case $t=1$) and the
constructions by Dinur et. al and Panteleev and Kalachev of a square complex
(case $t$=2), which have been applied to the design of classical locally
testable and quantum low-density parity check codes respectively. For $t=4$ our
construction gives a family of quantum locally testable codes conditional on a
conjecture about robustness of four-tuples of random linear maps. These codes
have linear dimension, inverse poly-logarithmic relative distance and
soundness, and polylogarithmic-size parity checks.
<br />Our complex can be built in a modular way from two ingredients. Firstly, the
geometry (edges, faces, cubes, etc.) is provided by a set $G$ of size $N$,
together with pairwise commuting sets of actions $A_1,\ldots,A_t$ on it.
Secondly, the chain complex itself is obtained by associating local coefficient
spaces based on codes, with each geometric object, and introducing local maps
on those coefficient spaces.
<br />We bound the cycle and co-cycle expansion of the chain complex. The
assumptions we need are two-fold: firstly, each Cayley graph $Cay(G,A_j)$ needs
to be a good (spectral) expander, and secondly, the families of codes and their
duals both need to satisfy a form of robustness (that generalizes the condition
of agreement testability for pairs of codes). While the first assumption is
easy to satisfy, it is currently not known if the second can be achieved.
</p>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07492" title="Abstract">arXiv:2402.07492</a> (cross-list from astro-ph.IM) [<a href="/pdf/2402.07492" title="Download PDF">pdf</a>, <a href="/format/2402.07492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convolutional Neural Networks for signal detection in real LIGO data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Zelenka%2C+O">Ond&#x159;ej Zelenka</a>, 
<a href="/search/astro-ph?searchtype=author&query=Br%C3%BCgmann%2C+B">Bernd Br&#xfc;gmann</a>, 
<a href="/search/astro-ph?searchtype=author&query=Ohme%2C+F">Frank Ohme</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 4 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Machine Learning (cs.LG); General Relativity and Quantum Cosmology (gr-qc)

</div>
<p class="mathjax">Searching the data of gravitational-wave detectors for signals from compact
binary mergers is a computationally demanding task. Recently, machine learning
algorithms have been proposed to address current and future challenges.
However, the results of these publications often differ greatly due to
differing choices in the evaluation procedure. The Machine Learning
Gravitational-Wave Search Challenge was organized to resolve these issues and
produce a unified framework for machine-learning search evaluation. Six teams
submitted contributions, four of which are based on machine learning methods
and two are state-of-the-art production analyses. This paper describes the
submission from the team TPI FSU Jena and its updated variant. We also apply
our algorithm to real O3b data and recover the relevant events of the GWTC-3
catalog.
</p>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07595" title="Abstract">arXiv:2402.07595</a> (cross-list from eess.IV) [<a href="/pdf/2402.07595" title="Download PDF">pdf</a>, <a href="/format/2402.07595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparative Analysis of ImageNet Pre-Trained Deep Learning Models and  DINOv2 in Medical Imaging Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Huang%2C+Y">Yuning Huang</a>, 
<a href="/search/eess?searchtype=author&query=Zou%2C+J">Jingchen Zou</a>, 
<a href="/search/eess?searchtype=author&query=Meng%2C+L">Lanxi Meng</a>, 
<a href="/search/eess?searchtype=author&query=Yue%2C+X">Xin Yue</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+Q">Qing Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Jianqiang Li</a>, 
<a href="/search/eess?searchtype=author&query=Song%2C+C">Changwei Song</a>, 
<a href="/search/eess?searchtype=author&query=Jimenez%2C+G">Gabriel Jimenez</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+S">Shaowu Li</a>, 
<a href="/search/eess?searchtype=author&query=Fu%2C+G">Guanghui Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Medical image analysis frequently encounters data scarcity challenges.
Transfer learning has been effective in addressing this issue while conserving
computational resources. The recent advent of foundational models like the
DINOv2, which uses the vision transformer architecture, has opened new
opportunities in the field and gathered significant interest. However, DINOv2's
performance on clinical data still needs to be verified. In this paper, we
performed a glioma grading task using three clinical modalities of brain MRI
data. We compared the performance of various pre-trained deep learning models,
including those based on ImageNet and DINOv2, in a transfer learning context.
Our focus was on understanding the impact of the freezing mechanism on
performance. We also validated our findings on three other types of public
datasets: chest radiography, fundus radiography, and dermoscopy. Our findings
indicate that in our clinical dataset, DINOv2's performance was not as strong
as ImageNet-based pre-trained models, whereas in public datasets, DINOv2
generally outperformed other models, especially when using the frozen
mechanism. Similar performance was observed with various sizes of DINOv2 models
across different tasks. In summary, DINOv2 is viable for medical image
classification tasks, particularly with data resembling natural images.
However, its effectiveness may vary with data that significantly differs from
natural images such as MRI. In addition, employing smaller versions of the
model can be adequate for medical task, offering resource-saving benefits. Our
codes are available at https://github.com/GuanghuiFU/medical_DINOv2_eval.
</p>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07597" title="Abstract">arXiv:2402.07597</a> (cross-list from eess.IV) [<a href="/pdf/2402.07597" title="Download PDF">pdf</a>, <a href="/format/2402.07597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trustworthy SR: Resolving Ambiguity in Image Super-resolution via  Diffusion Models and Human Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Korkmaz%2C+C">Cansu Korkmaz</a>, 
<a href="/search/eess?searchtype=author&query=Cirakman%2C+E">Ege Cirakman</a>, 
<a href="/search/eess?searchtype=author&query=Tekalp%2C+A+M">A. Murat Tekalp</a>, 
<a href="/search/eess?searchtype=author&query=Dogan%2C+Z">Zafer Dogan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> total of 7 pages with double column, 1 and a half for references, 6 figures and 2 tables, submitted to IEEE ICIP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Super-resolution (SR) is an ill-posed inverse problem with a large set of
feasible solutions that are consistent with a given low-resolution image.
Various deterministic algorithms aim to find a single solution that balances
fidelity and perceptual quality; however, this trade-off often causes visual
artifacts that bring ambiguity in information-centric applications. On the
other hand, diffusion models (DMs) excel in generating a diverse set of
feasible SR images that span the solution space. The challenge is then how to
determine the most likely solution among this set in a trustworthy manner. We
observe that quantitative measures, such as PSNR, LPIPS, DISTS, are not
reliable indicators to resolve ambiguous cases. To this effect, we propose
employing human feedback, where we ask human subjects to select a small number
of likely samples and we ensemble the averages of selected samples. This
strategy leverages the high-quality image generation capabilities of DMs, while
recognizing the importance of obtaining a single trustworthy solution,
especially in use cases, such as identification of specific digits or letters,
where generating multiple feasible solutions may not lead to a reliable
outcome. Experimental results demonstrate that our proposed strategy provides
more trustworthy solutions when compared to state-of-the art SR methods.
</p>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07599" title="Abstract">arXiv:2402.07599</a> (cross-list from eess.AS) [<a href="/pdf/2402.07599" title="Download PDF">pdf</a>, <a href="/format/2402.07599" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interactive singing melody extraction based on active adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Saxena%2C+K+R">Kavya Ranjan Saxena</a>, 
<a href="/search/eess?searchtype=author&query=Arora%2C+V">Vipul Arora</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Extraction of predominant pitch from polyphonic audio is one of the
fundamental tasks in the field of music information retrieval and computational
musicology. To accomplish this task using machine learning, a large amount of
labeled audio data is required to train the model. However, a classical model
pre-trained on data from one domain (source), e.g., songs of a particular
singer or genre, may not perform comparatively well in extracting melody from
other domains (target). The performance of such models can be boosted by
adapting the model using very little annotated data from the target domain. In
this work, we propose an efficient interactive melody adaptation method. Our
method selects the regions in the target audio that require human annotation
using a confidence criterion based on normalized true class probability. The
annotations are used by the model to adapt itself to the target domain using
meta-learning. Our method also provides a novel meta-learning approach that
handles class imbalance, i.e., a few representative samples from a few classes
are available for adaptation in the target domain. Experimental results show
that the proposed method outperforms other adaptive melody extraction
baselines. The proposed method is model-agnostic and hence can be applied to
other non-adaptive melody extraction models to boost their performance. Also,
we released a Hindustani Alankaar and Raga (HAR) dataset containing 523 audio
files of about 6.86 hours of duration intended for singing melody extraction
tasks.
</p>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07613" title="Abstract">arXiv:2402.07613</a> (cross-list from math.ST) [<a href="/pdf/2402.07613" title="Download PDF">pdf</a>, <a href="/format/2402.07613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Global optimality under amenable symmetry constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Orbanz%2C+P">Peter Orbanz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">We ask whether there exists a function or measure that (1) minimizes a given
convex functional or risk and (2) satisfies a symmetry property specified by an
amenable group of transformations. Examples of such symmetry properties are
invariance, equivariance, or quasi-invariance. Our results draw on old ideas of
Stein and Le Cam and on approximate group averages that appear in ergodic
theorems for amenable groups. A class of convex sets known as orbitopes in
convex analysis emerges as crucial, and we establish properties of such
orbitopes in nonparametric settings. We also show how a simple device called a
cocycle can be used to reduce different forms of symmetry to a single problem.
As applications, we obtain results on invariant kernel mean embeddings and a
Monge-Kantorovich theorem on optimality of transport plans under symmetry
constraints. We also explain connections to the Hunt-Stein theorem on invariant
tests.
</p>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07626" title="Abstract">arXiv:2402.07626</a> (cross-list from stat.ML) [<a href="/pdf/2402.07626" title="Download PDF">pdf</a>, <a href="/format/2402.07626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Gradient Flow Dynamics of Test Risk and its Exact Solution  for Weak Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Veiga%2C+R">Rodrigo Veiga</a>, 
<a href="/search/stat?searchtype=author&query=Remizova%2C+A">Anastasia Remizova</a>, 
<a href="/search/stat?searchtype=author&query=Macris%2C+N">Nicolas Macris</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG)

</div>
<p class="mathjax">We investigate the test risk of continuous-time stochastic gradient flow
dynamics in learning theory. Using a path integral formulation we provide, in
the regime of a small learning rate, a general formula for computing the
difference between test risk curves of pure gradient and stochastic gradient
flows. We apply the general theory to a simple model of weak features, which
displays the double descent phenomenon, and explicitly compute the corrections
brought about by the added stochastic term in the dynamics, as a function of
time and model parameters. The analytical results are compared to simulations
of discrete-time stochastic gradient descent and show good agreement.
</p>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07637" title="Abstract">arXiv:2402.07637</a> (cross-list from eess.SP) [<a href="/pdf/2402.07637" title="Download PDF">pdf</a>, <a href="/format/2402.07637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compressive Recovery of Signals Defined on Perturbed Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ghosh%2C+S">Sabyasachi Ghosh</a>, 
<a href="/search/eess?searchtype=author&query=Rajwade%2C+A">Ajit Rajwade</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Recovery of signals with elements defined on the nodes of a graph, from
compressive measurements is an important problem, which can arise in various
domains such as sensor networks, image reconstruction and group testing. In
some scenarios, the graph may not be accurately known, and there may exist a
few edge additions or deletions relative to a ground truth graph. Such
perturbations, even if small in number, significantly affect the Graph Fourier
Transform (GFT). This impedes recovery of signals which may have sparse
representations in the GFT bases of the ground truth graph. We present an
algorithm which simultaneously recovers the signal from the compressive
measurements and also corrects the graph perturbations. We analyze some
important theoretical properties of the algorithm. Our approach to correction
for graph perturbations is based on model selection techniques such as
cross-validation in compressed sensing. We validate our algorithm on signals
which have a sparse representation in the GFT bases of many commonly used
graphs in the network science literature. An application to compressive image
reconstruction is also presented, where graph perturbations are modeled as
undesirable graph edges linking pixels with significant intensity difference.
In all experiments, our algorithm clearly outperforms baseline techniques which
either ignore the perturbations or use first order approximations to the
perturbations in the GFT bases.
</p>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07684" title="Abstract">arXiv:2402.07684</a> (cross-list from q-bio.QM) [<a href="/pdf/2402.07684" title="Download PDF">pdf</a>, <a href="/format/2402.07684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Foundation Model for Brain Age Prediction using coVariance  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Sihag%2C+S">Saurabh Sihag</a>, 
<a href="/search/q-bio?searchtype=author&query=Mateos%2C+G">Gonzalo Mateos</a>, 
<a href="/search/q-bio?searchtype=author&query=Ribeiro%2C+A">Alejandro Ribeiro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preliminary work. Contact sihag.saurabh@gmail.com for the NeuroVNN model and code used for results reported in this manuscript
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG); Applications (stat.AP)

</div>
<p class="mathjax">Brain age is the estimate of biological age derived from neuroimaging
datasets using machine learning algorithms. Increasing brain age with respect
to chronological age can reflect increased vulnerability to neurodegeneration
and cognitive decline. In this paper, we study NeuroVNN, based on coVariance
neural networks, as a paradigm for foundation model for the brain age
prediction application. NeuroVNN is pre-trained as a regression model on
healthy population to predict chronological age using cortical thickness
features and fine-tuned to estimate brain age in different neurological
contexts. Importantly, NeuroVNN adds anatomical interpretability to brain age
and has a `scale-free' characteristic that allows its transference to datasets
curated according to any arbitrary brain atlas. Our results demonstrate that
NeuroVNN can extract biologically plausible brain age estimates in different
populations, as well as transfer successfully to datasets of dimensionalities
distinct from that for the dataset used to train NeuroVNN.
</p>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07717" title="Abstract">arXiv:2402.07717</a> (cross-list from math.ST) [<a href="/pdf/2402.07717" title="Download PDF">pdf</a>, <a href="/ps/2402.07717" title="Download PostScript">ps</a>, <a href="/format/2402.07717" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient reductions between some statistical models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lou%2C+M">Mengqi Lou</a>, 
<a href="/search/math?searchtype=author&query=Bresler%2C+G">Guy Bresler</a>, 
<a href="/search/math?searchtype=author&query=Pananjady%2C+A">Ashwin Pananjady</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Information Theory (cs.IT); Probability (math.PR); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the problem of approximately transforming a sample from a source
statistical model to a sample from a target statistical model without knowing
the parameters of the source model, and construct several computationally
efficient such reductions between statistical experiments. In particular, we
provide computationally efficient procedures that approximately reduce uniform,
Erlang, and Laplace location models to general target families. We illustrate
our methodology by establishing nonasymptotic reductions between some canonical
high-dimensional problems, spanning mixtures of experts, phase retrieval, and
signal denoising. Notably, the reductions are structure preserving and can
accommodate missing data. We also point to a possible application in
transforming one differentially private mechanism to another.
</p>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07723" title="Abstract">arXiv:2402.07723</a> (cross-list from stat.ML) [<a href="/pdf/2402.07723" title="Download PDF">pdf</a>, <a href="/format/2402.07723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalization Bounds for Heavy-Tailed SDEs through the Fractional  Fokker-Planck Equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Dupuis%2C+B">Benjamin Dupuis</a>, 
<a href="/search/stat?searchtype=author&query=%C5%9Eim%C5%9Fekli%2C+U">Umut &#x15e;im&#x15f;ekli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Understanding the generalization properties of heavy-tailed stochastic
optimization algorithms has attracted increasing attention over the past years.
While illuminating interesting aspects of stochastic optimizers by using
heavy-tailed stochastic differential equations as proxies, prior works either
provided expected generalization bounds, or introduced non-computable
information theoretic terms. Addressing these drawbacks, in this work, we prove
high-probability generalization bounds for heavy-tailed SDEs which do not
contain any nontrivial information theoretic terms. To achieve this goal, we
develop new proof techniques based on estimating the entropy flows associated
with the so-called fractional Fokker-Planck equation (a partial differential
equation that governs the evolution of the distribution of the corresponding
heavy-tailed SDE). In addition to obtaining high-probability bounds, we show
that our bounds have a better dependence on the dimension of parameters as
compared to prior art. Our results further identify a phase transition
phenomenon, which suggests that heavy tails can be either beneficial or harmful
depending on the problem structure. We support our theory with experiments
conducted in a variety of settings.
</p>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07729" title="Abstract">arXiv:2402.07729</a> (cross-list from eess.AS) [<a href="/pdf/2402.07729" title="Download PDF">pdf</a>, <a href="/format/2402.07729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AIR-Bench: Benchmarking Large Audio-Language Models via Generative  Comprehension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yang%2C+Q">Qian Yang</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+J">Jin Xu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+W">Wenrui Liu</a>, 
<a href="/search/eess?searchtype=author&query=Chu%2C+Y">Yunfei Chu</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+Z">Ziyue Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+X">Xiaohuan Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Leng%2C+Y">Yichong Leng</a>, 
<a href="/search/eess?searchtype=author&query=Lv%2C+Y">Yuanjun Lv</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+Z">Zhou Zhao</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+C">Chang Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+J">Jingren Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD)

</div>
<p class="mathjax">Recently, instruction-following audio-language models have received broad
attention for human-audio interaction. However, the absence of benchmarks
capable of evaluating audio-centric interaction capabilities has impeded
advancements in this field. Previous models primarily focus on assessing
different fundamental tasks, such as Automatic Speech Recognition (ASR), and
lack an assessment of the open-ended generative capabilities centered around
audio. Thus, it is challenging to track the progression in the Large
Audio-Language Models (LALMs) domain and to provide guidance for future
improvement. In this paper, we introduce AIR-Bench (\textbf{A}udio
\textbf{I}nst\textbf{R}uction \textbf{Bench}mark), the first benchmark designed
to evaluate the ability of LALMs to understand various types of audio signals
(including human speech, natural sounds, and music), and furthermore, to
interact with humans in the textual format. AIR-Bench encompasses two
dimensions: \textit{foundation} and \textit{chat} benchmarks. The former
consists of 19 tasks with approximately 19k single-choice questions, intending
to inspect the basic single-task ability of LALMs. The latter one contains 2k
instances of open-ended question-and-answer data, directly assessing the
comprehension of the model on complex audio and its capacity to follow
instructions. Both benchmarks require the model to generate hypotheses
directly. We design a unified framework that leverages advanced language
models, such as GPT-4, to evaluate the scores of generated hypotheses given the
meta-information of the audio. Experimental results demonstrate a high level of
consistency between GPT-4-based evaluation and human evaluation. By revealing
the limitations of existing LALMs through evaluation results, AIR-Bench can
provide insights into the direction of future research.
</p>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07735" title="Abstract">arXiv:2402.07735</a> (cross-list from stat.ML) [<a href="/pdf/2402.07735" title="Download PDF">pdf</a>, <a href="/format/2402.07735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Structure Inference with BAM: Introducing the Bilinear Attention  Mechanism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Froehlich%2C+P">Philipp Froehlich</a>, 
<a href="/search/stat?searchtype=author&query=Koeppl%2C+H">Heinz Koeppl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In statistics and machine learning, detecting dependencies in datasets is a
central challenge. We propose a novel neural network model for supervised graph
structure learning, i.e., the process of learning a mapping between
observational data and their underlying dependence structure. The model is
trained with variably shaped and coupled simulated input data and requires only
a single forward pass through the trained network for inference. By leveraging
structural equation models and employing randomly generated multivariate
Chebyshev polynomials for the simulation of training data, our method
demonstrates robust generalizability across both linear and various types of
non-linear dependencies. We introduce a novel bilinear attention mechanism
(BAM) for explicit processing of dependency information, which operates on the
level of covariance matrices of transformed data and respects the geometry of
the manifold of symmetric positive definite matrices. Empirical evaluation
demonstrates the robustness of our method in detecting a wide range of
dependencies, excelling in undirected graph estimation and proving competitive
in completed partially directed acyclic graph estimation through a novel
two-step approach.
</p>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07746" title="Abstract">arXiv:2402.07746</a> (cross-list from eess.IV) [<a href="/pdf/2402.07746" title="Download PDF">pdf</a>, <a href="/ps/2402.07746" title="Download PostScript">ps</a>, <a href="/format/2402.07746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimally Interactive Segmentation of Soft-Tissue Tumors on CT and MRI  using Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Spaanderman%2C+D+J">Douwe J. Spaanderman</a> (1), 
<a href="/search/eess?searchtype=author&query=Starmans%2C+M+P+A">Martijn P. A. Starmans</a> (1), 
<a href="/search/eess?searchtype=author&query=van+Erp%2C+G+C+M">Gonnie C. M. van Erp</a> (1), 
<a href="/search/eess?searchtype=author&query=Hanff%2C+D+F">David F. Hanff</a> (1), 
<a href="/search/eess?searchtype=author&query=Sluijter%2C+J+H">Judith H. Sluijter</a> (1), 
<a href="/search/eess?searchtype=author&query=Schut%2C+A+W">Anne-Rose W. Schut</a> (2 and 3), 
<a href="/search/eess?searchtype=author&query=van+Leenders%2C+G+J+L+H">Geert J. L. H. van Leenders</a> (4), 
<a href="/search/eess?searchtype=author&query=Verhoef%2C+C">Cornelis Verhoef</a> (2), 
<a href="/search/eess?searchtype=author&query=Grunhagen%2C+D+J">Dirk J. Grunhagen</a> (2), 
<a href="/search/eess?searchtype=author&query=Niessen%2C+W+J">Wiro J. Niessen</a> (5), 
<a href="/search/eess?searchtype=author&query=Visser%2C+J+J">Jacob J. Visser</a> (1), 
<a href="/search/eess?searchtype=author&query=Klein%2C+S">Stefan Klein</a> (1) ((1) Department of Radiology and Nuclear Medicine, Erasmus MC, Rotterdam, the Netherlands, (2) Department of Surgical Oncology, Erasmus MC Cancer Institute, Rotterdam, the Netherlands, (3) Department of Medical Oncology, Erasmus MC Cancer Institute, Rotterdam, the Netherlands, (4) Department of Pathology, Erasmus MC Cancer Institute, Rotterdam, the Netherlands, (5) Faculty of Medical Sciences, University of Groningen, Groningen, The Netherlands)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Segmentations are crucial in medical imaging to obtain morphological,
volumetric, and radiomics biomarkers. Manual segmentation is accurate but not
feasible in the radiologist's clinical workflow, while automatic segmentation
generally obtains sub-par performance. We therefore developed a minimally
interactive deep learning-based segmentation method for soft-tissue tumors
(STTs) on CT and MRI. The method requires the user to click six points near the
tumor's extreme boundaries. These six points are transformed into a distance
map and serve, with the image, as input for a Convolutional Neural Network. For
training and validation, a multicenter dataset containing 514 patients and nine
STT types in seven anatomical locations was used, resulting in a Dice
Similarity Coefficient (DSC) of 0.85$\pm$0.11 (mean $\pm$ standard deviation
(SD)) for CT and 0.84$\pm$0.12 for T1-weighted MRI, when compared to manual
segmentations made by expert radiologists. Next, the method was externally
validated on a dataset including five unseen STT phenotypes in extremities,
achieving 0.81$\pm$0.08 for CT, 0.84$\pm$0.09 for T1-weighted MRI, and
0.88\pm0.08 for previously unseen T2-weighted fat-saturated (FS) MRI. In
conclusion, our minimally interactive segmentation method effectively segments
different types of STTs on CT and MRI, with robust generalization to previously
unseen phenotypes and imaging modalities.
</p>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07759" title="Abstract">arXiv:2402.07759</a> (cross-list from physics.ao-ph) [<a href="/pdf/2402.07759" title="Download PDF">pdf</a>, <a href="/format/2402.07759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust and accurate simulations of flows over orography using  non-conforming meshes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Orlando%2C+G">Giuseppe Orlando</a>, 
<a href="/search/physics?searchtype=author&query=Benacchio%2C+T">Tommaso Benacchio</a>, 
<a href="/search/physics?searchtype=author&query=Bonaventura%2C+L">Luca Bonaventura</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Atmospheric and Oceanic Physics (physics.ao-ph)</span>; Numerical Analysis (math.NA); Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">We systematically validate the static local mesh refinement capabilities of a
recently proposed IMEX-DG scheme implemented in the framework of the deal.II
library. Non-conforming meshes are employed in atmospheric flow simulations to
increase the resolution around complex orography. A number of numerical
experiments based on classical benchmarks with idealized as well as real
orography profiles demonstrate that simulations with the refined mesh are
stable for long lead times and no spurious effects arise at the interfaces of
mesh regions with different resolutions. Moreover, correct values of the
momentum flux are retrieved and the correct large-scale orographic response is
established. Hence, large-scale orography-driven flow features can be simulated
without loss of accuracy using a much lower total amount of degrees of freedom.
In a context of spatial resolutions approaching the hectometric scale in
numerical weather prediction models, these results support the use of locally
refined, non-conforming meshes as a reliable and effective tool to greatly
reduce the dependence of atmospheric models on orographic wave drag
parametrizations.
</p>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07762" title="Abstract">arXiv:2402.07762</a> (cross-list from stat.ML) [<a href="/pdf/2402.07762" title="Download PDF">pdf</a>, <a href="/format/2402.07762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Structure Learning for Sparse Context-Specific Causal Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Rios%2C+F+L">Felix Leopoldo Rios</a>, 
<a href="/search/stat?searchtype=author&query=Markham%2C+A">Alex Markham</a>, 
<a href="/search/stat?searchtype=author&query=Solus%2C+L">Liam Solus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Combinatorics (math.CO)

</div>
<p class="mathjax">Several approaches to graphically representing context-specific relations
among jointly distributed categorical variables have been proposed, along with
structure learning algorithms. While existing optimization-based methods have
limited scalability due to the large number of context-specific models, the
constraint-based methods are more prone to error than even constraint-based DAG
learning algorithms since more relations must be tested. We present a hybrid
algorithm for learning context-specific models that scales to hundreds of
variables while testing no more constraints than standard DAG learning
algorithms. Scalable learning is achieved through a combination of an
order-based MCMC algorithm and sparsity assumptions analogous to those
typically invoked for DAG models. To implement the method, we solve a special
case of an open problem recently posed by Alon and Balogh. The method is shown
to perform well on synthetic data and real world examples, in terms of both
accuracy and scalability.
</p>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07763" title="Abstract">arXiv:2402.07763</a> (cross-list from math.OC) [<a href="/pdf/2402.07763" title="Download PDF">pdf</a>, <a href="/format/2402.07763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-level Optimal Control with Neural Surrogate Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kalise%2C+D">Dante Kalise</a>, 
<a href="/search/math?searchtype=author&query=Loayza-Romero%2C+E">Estefan&#xed;a Loayza-Romero</a>, 
<a href="/search/math?searchtype=author&query=Morris%2C+K+A">Kirsten A. Morris</a>, 
<a href="/search/math?searchtype=author&query=Zhong%2C+Z">Zhengang Zhong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA)

</div>
<p class="mathjax">Optimal actuator and control design is studied as a multi-level optimisation
problem, where the actuator design is evaluated based on the performance of the
associated optimal closed loop. The evaluation of the optimal closed loop for a
given actuator realisation is a computationally demanding task, for which the
use of a neural network surrogate is proposed. The use of neural network
surrogates to replace the lower level of the optimisation hierarchy enables the
use of fast gradient-based and gradient-free consensus-based optimisation
methods to determine the optimal actuator design. The effectiveness of the
proposed surrogate models and optimisation methods is assessed in a test
related to optimal actuator location for heat control.
</p>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07793" title="Abstract">arXiv:2402.07793</a> (cross-list from math.OC) [<a href="/pdf/2402.07793" title="Download PDF">pdf</a>, <a href="/ps/2402.07793" title="Download PostScript">ps</a>, <a href="/format/2402.07793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tuning-Free Stochastic Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Khaled%2C+A">Ahmed Khaled</a>, 
<a href="/search/math?searchtype=author&query=Jin%2C+C">Chi Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Large-scale machine learning problems make the cost of hyperparameter tuning
ever more prohibitive. This creates a need for algorithms that can tune
themselves on-the-fly. We formalize the notion of "tuning-free" algorithms that
can match the performance of optimally-tuned optimization algorithms up to
polylogarithmic factors given only loose hints on the relevant problem
parameters. We consider in particular algorithms that can match optimally-tuned
Stochastic Gradient Descent (SGD). When the domain of optimization is bounded,
we show tuning-free matching of SGD is possible and achieved by several
existing algorithms. We prove that for the task of minimizing a convex and
smooth or Lipschitz function over an unbounded domain, tuning-free optimization
is impossible. We discuss conditions under which tuning-free optimization is
possible even over unbounded domains. In particular, we show that the recently
proposed DoG and DoWG algorithms are tuning-free when the noise distribution is
sufficiently well-behaved. For the task of finding a stationary point of a
smooth and potentially nonconvex function, we give a variant of SGD that
matches the best-known high-probability convergence rate for tuned SGD at only
an additional polylogarithmic cost. However, we also give an impossibility
result that shows no algorithm can hope to match the optimal expected
convergence rate for tuned SGD with high probability.
</p>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07795" title="Abstract">arXiv:2402.07795</a> (cross-list from math.CA) [<a href="/pdf/2402.07795" title="Download PDF">pdf</a>, <a href="/ps/2402.07795" title="Download PostScript">ps</a>, <a href="/format/2402.07795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite and infinite order differential properties of the reduced  Mittag--Leffler polynomials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Rajkovi%C4%87%2C+P+M">Predrag M. Rajkovi&#x107;</a>, 
<a href="/search/math?searchtype=author&query=Marinkovi%C4%87%2C+S+D">Sladjana D. Marinkovi&#x107;</a>, 
<a href="/search/math?searchtype=author&query=Stankovi%C4%87%2C+M+S">Miomir S. Stankovi&#x107;</a>, 
<a href="/search/math?searchtype=author&query=Petkovi%C4%87%2C+M+D">Marko D. Petkovi&#x107;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Classical Analysis and ODEs (math.CA)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">This paper deals with the Mittag-Leffler polynomials (MLP) by extracting
their essence which consists of real polynomials with fine properties. They are
orthogonal on the real line instead of the imaginary axes for MLP. Beside
recurrence relations and zeros, we will point to the closed form of its Fourier
transform. The most important contribution consists of the new differential
properties, especially the finite and infinite differential equation.
</p>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07802" title="Abstract">arXiv:2402.07802</a> (cross-list from stat.ML) [<a href="/pdf/2402.07802" title="Download PDF">pdf</a>, <a href="/ps/2402.07802" title="Download PostScript">ps</a>, <a href="/format/2402.07802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a mathematical theory for consistency training in diffusion  models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+G">Gen Li</a>, 
<a href="/search/stat?searchtype=author&query=Huang%2C+Z">Zhihan Huang</a>, 
<a href="/search/stat?searchtype=author&query=Wei%2C+Y">Yuting Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Information Theory (cs.IT); Machine Learning (cs.LG); Statistics Theory (math.ST)

</div>
<p class="mathjax">Consistency models, which were proposed to mitigate the high computational
overhead during the sampling phase of diffusion models, facilitate single-step
sampling while attaining state-of-the-art empirical performance. When
integrated into the training phase, consistency models attempt to train a
sequence of consistency functions capable of mapping any point at any time step
of the diffusion process to its starting point. Despite the empirical success,
a comprehensive theoretical understanding of consistency training remains
elusive. This paper takes a first step towards establishing theoretical
underpinnings for consistency models. We demonstrate that, in order to generate
samples within $\varepsilon$ proximity to the target in distribution (measured
by some Wasserstein metric), it suffices for the number of steps in consistency
learning to exceed the order of $d^{5/2}/\varepsilon$, with $d$ the data
dimension. Our theory offers rigorous insights into the validity and efficacy
of consistency models, illuminating their utility in downstream inference
tasks.
</p>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07809" title="Abstract">arXiv:2402.07809</a> (cross-list from quant-ph) [<a href="/pdf/2402.07809" title="Download PDF">pdf</a>, <a href="/format/2402.07809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum walks, the discrete wave equation and Chebyshev polynomials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Apers%2C+S">Simon Apers</a>, 
<a href="/search/quant-ph?searchtype=author&query=Miclo%2C+L">Laurent Miclo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Data Structures and Algorithms (cs.DS); Probability (math.PR)

</div>
<p class="mathjax">A quantum walk is the quantum analogue of a random walk. While it is
relatively well understood how quantum walks can speed up random walk hitting
times, it is a long-standing open question to what extent quantum walks can
speed up the spreading or mixing rate of random walks on graphs. In this
expository paper, inspired by a blog post by Terence Tao, we describe a
particular perspective on this question that derives quantum walks from the
discrete wave equation on graphs. This yields a description of the quantum walk
dynamics as simply applying a Chebyshev polynomial to the random walk
transition matrix. This perspective decouples the problem from its quantum
origin, and highlights connections to earlier (non-quantum) work and the use of
Chebyshev polynomials in random walk theory as in the Varopoulos-Carne bound.
We illustrate the approach by proving a weak limit of the quantum walk dynamics
on the lattice. This gives a different proof of the quadratically improved
spreading behavior of quantum walks on lattices.
</p>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07870" title="Abstract">arXiv:2402.07870</a> (cross-list from math.CO) [<a href="/pdf/2402.07870" title="Download PDF">pdf</a>, <a href="/ps/2402.07870" title="Download PostScript">ps</a>, <a href="/format/2402.07870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perfect stable regularity lemma and slice-wise stable hypergraphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chernikov%2C+A">Artem Chernikov</a>, 
<a href="/search/math?searchtype=author&query=Towsner%2C+H">Henry Towsner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 67 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Logic (math.LO)

</div>
<p class="mathjax">We investigate various forms of (model-theoretic) stability for hypergraphs
and their corresponding strengthenings of the hypergraph regularity lemma with
respect to partitions of vertices. On the one hand, we provide a complete
classification of the various possibilities in the ternary case. On the other
hand, we provide an example of a family of slice-wise stable 3-hypergraphs so
that for no partition of the vertices, any triple of parts has density close to
0 or 1. In particular, this addresses some questions and conjectures of Terry
and Wolf. We work in the general measure theoretic context of graded
probability spaces, so all our results apply both to measures in ultraproducts
of finite graphs, leading to the aforementioned combinatorial applications, and
to commuting definable Keisler measures, leading to applications in model
theory.
</p>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07874" title="Abstract">arXiv:2402.07874</a> (cross-list from math.RA) [<a href="/pdf/2402.07874" title="Download PDF">pdf</a>, <a href="/format/2402.07874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Factorizating the Brauer monoid in polynomial time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Marchei%2C+D">Daniele Marchei</a>, 
<a href="/search/math?searchtype=author&query=Merelli%2C+E">Emanuela Merelli</a>, 
<a href="/search/math?searchtype=author&query=Francis%2C+A">Andrew Francis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Rings and Algebras (math.RA)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Finding a minimal factorization for a generic semigroup can be done by using
the Froidure-Pin Algorithm, which is not feasible for semigroups of large
sizes. On the other hand, if we restrict our attention to just a particular
semigroup, we could leverage its structure to obtain a much faster algorithm.
In particular, $\mathcal{O}(N^2)$ algorithms are known for factorizing the
Symmetric group $S_N$ and the Temperley-Lieb monoid $\mathcal{T}\mathcal{L}_N$,
but none for their superset the Brauer monoid $\mathcal{B}_{N}$. In this paper
we hence propose a $\mathcal{O}(N^4)$ factorization algorithm for
$\mathcal{B}_{N}$. At each iteration, the algorithm rewrites the input $X \in
\mathcal{B}_{N}$ as $X = X' \circ p_i$ such that $\ell(X') = \ell(X) - 1$,
where $p_i$ is a factor for $X$ and $\ell$ is a length function that returns
the minimal number of factors needed to generate $X$.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Tue, 13 Feb 24</h3>
<dl>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1711.01820" title="Abstract">arXiv:1711.01820</a> (replaced) [<a href="/pdf/1711.01820" title="Download PDF">pdf</a>, <a href="/format/1711.01820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Game Theoretic Semi-Distributed D2D Resource Allocation Underlaying an  LTE Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neogi%2C+A">Anushree Neogi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Computer Science and Game Theory (cs.GT); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1911.01951" title="Abstract">arXiv:1911.01951</a> (replaced) [<a href="/e-print/1911.01951" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Bron-Kerbosch Algorithm with Vertex Ordering is Output-Sensitive
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manoussakis%2C+G">George Manoussakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> result is false
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2003.08867" title="Abstract">arXiv:2003.08867</a> (replaced) [<a href="/pdf/2003.08867" title="Download PDF">pdf</a>, <a href="/format/2003.08867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of a fully discrete approximation for the classical  Keller--Segel model: lower and a priori bounds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Guti%C3%A9rrez-Santacreu%2C+J+V">Juan Vicente Guti&#xe9;rrez-Santacreu</a>, 
<a href="/search/math?searchtype=author&query=Rodr%C3%ADguez-Galv%C3%A1n%2C+J+R">Jos&#xe9; Rafael Rodr&#xed;guez-Galv&#xe1;n</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2010.13442" title="Abstract">arXiv:2010.13442</a> (replaced) [<a href="/pdf/2010.13442" title="Download PDF">pdf</a>, <a href="/ps/2010.13442" title="Download PostScript">ps</a>, <a href="/format/2010.13442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Purely Regular Approach to Non-Regular Core Spanners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schmid%2C+M+L">Markus L. Schmid</a>, 
<a href="/search/cs?searchtype=author&query=Schweikardt%2C+N">Nicole Schweikardt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>; Formal Languages and Automata Theory (cs.FL); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2011.04672" title="Abstract">arXiv:2011.04672</a> (replaced) [<a href="/pdf/2011.04672" title="Download PDF">pdf</a>, <a href="/format/2011.04672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Principles of Quantum Communication Theory: A Modern Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Khatri%2C+S">Sumeet Khatri</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wilde%2C+M+M">Mark M. Wilde</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2: 1240 pages, 60 figures. Comments welcome!
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Statistical Mechanics (cond-mat.stat-mech); Information Theory (cs.IT); High Energy Physics - Theory (hep-th); Mathematical Physics (math-ph)

</div>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2012.02282" title="Abstract">arXiv:2012.02282</a> (replaced) [<a href="/pdf/2012.02282" title="Download PDF">pdf</a>, <a href="/ps/2012.02282" title="Download PostScript">ps</a>, <a href="/format/2012.02282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Creativity of Deep Learning: Conceptualization and Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Basalla%2C+M">Marcus Basalla</a>, 
<a href="/search/cs?searchtype=author&query=Schneider%2C+J">Johannes Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Brocke%2C+J+v">Jan vom Brocke</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the International Conference on Agents and
  Artificial Intelligence (ICAART), 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.03527" title="Abstract">arXiv:2104.03527</a> (replaced) [<a href="/pdf/2104.03527" title="Download PDF">pdf</a>, <a href="/format/2104.03527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse NMF with Archetypal Regularization: Computational and Robustness  Properties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Behdin%2C+K">Kayhan Behdin</a>, 
<a href="/search/stat?searchtype=author&query=Mazumder%2C+R">Rahul Mazumder</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.01265" title="Abstract">arXiv:2105.01265</a> (replaced) [<a href="/pdf/2105.01265" title="Download PDF">pdf</a>, <a href="/ps/2105.01265" title="Download PostScript">ps</a>, <a href="/format/2105.01265" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding Triangles or Independent Sets; and Other Dual Pair  Approximations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dumitrescu%2C+A">Adrian Dumitrescu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, no figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2105.02569" title="Abstract">arXiv:2105.02569</a> (replaced) [<a href="/pdf/2105.02569" title="Download PDF">pdf</a>, <a href="/ps/2105.02569" title="Download PostScript">ps</a>, <a href="/format/2105.02569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Liu%2C+Q">Qingfeng Liu</a>, 
<a href="/search/stat?searchtype=author&query=Feng%2C+Y">Yang Feng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Econometrics (econ.EM)

</div>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.06463" title="Abstract">arXiv:2107.06463</a> (replaced) [<a href="/pdf/2107.06463" title="Download PDF">pdf</a>, <a href="/format/2107.06463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learned Image Compression with Gaussian-Laplacian-Logistic Mixture Model  and Concatenated Residual Modules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fu%2C+H">Haisheng Fu</a>, 
<a href="/search/eess?searchtype=author&query=Liang%2C+F">Feng Liang</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+J">Jianping Lin</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+B">Bing Li</a>, 
<a href="/search/eess?searchtype=author&query=Akbari%2C+M">Mohammad Akbari</a>, 
<a href="/search/eess?searchtype=author&query=Liang%2C+J">Jie Liang</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+G">Guohe Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+D">Dong Liu</a>, 
<a href="/search/eess?searchtype=author&query=Tu%2C+C">Chengjie Tu</a>, 
<a href="/search/eess?searchtype=author&query=Han%2C+J">Jingning Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Transactions On Image Processing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2108.02119" title="Abstract">arXiv:2108.02119</a> (replaced) [<a href="/pdf/2108.02119" title="Download PDF">pdf</a>, <a href="/ps/2108.02119" title="Download PostScript">ps</a>, <a href="/format/2108.02119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-complexity Scaling Methods for DCT-II Approximations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Coelho%2C+D+F+G">D. F. G. Coelho</a>, 
<a href="/search/eess?searchtype=author&query=Cintra%2C+R+J">R. J. Cintra</a>, 
<a href="/search/eess?searchtype=author&query=Madanayake%2C+A">A. Madanayake</a>, 
<a href="/search/eess?searchtype=author&query=Perera%2C+S">S. Perera</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Clarified definition of matrix $\mathbf{U}_N$. 20 pages, 3 figures, 14 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Hardware Architecture (cs.AR); Computational Complexity (cs.CC); Multimedia (cs.MM); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.07753" title="Abstract">arXiv:2109.07753</a> (replaced) [<a href="/pdf/2109.07753" title="Download PDF">pdf</a>, <a href="/ps/2109.07753" title="Download PostScript">ps</a>, <a href="/format/2109.07753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilevel-Langevin pathwise average for Gibbs approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Eg%C3%A9a%2C+M">Maxime Eg&#xe9;a</a> (LAREMA), 
<a href="/search/math?searchtype=author&query=Panloup%2C+F">Fabien Panloup</a> (LAREMA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.09264" title="Abstract">arXiv:2109.09264</a> (replaced) [<a href="/pdf/2109.09264" title="Download PDF">pdf</a>, <a href="/format/2109.09264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computationally Efficient High-Dimensional Bayesian Optimization via  Variable Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yihang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Kingsford%2C+C">Carl Kingsford</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has already been accepted in AutoML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2109.10484" title="Abstract">arXiv:2109.10484</a> (replaced) [<a href="/pdf/2109.10484" title="Download PDF">pdf</a>, <a href="/ps/2109.10484" title="Download PostScript">ps</a>, <a href="/format/2109.10484" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalized Fractional Repetition Codes for Binary Coded Computations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Charalambides%2C+N">Neophytos Charalambides</a>, 
<a href="/search/cs?searchtype=author&query=Mahdavifar%2C+H">Hessam Mahdavifar</a>, 
<a href="/search/cs?searchtype=author&query=Hero%2C+A+O">Alfred O. Hero III</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.02879" title="Abstract">arXiv:2110.02879</a> (replaced) [<a href="/pdf/2110.02879" title="Download PDF">pdf</a>, <a href="/format/2110.02879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compositional Q-learning for electrolyte repletion with imbalanced  patient sub-populations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mandyam%2C+A">Aishwarya Mandyam</a>, 
<a href="/search/cs?searchtype=author&query=Jones%2C+A">Andrew Jones</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiayu Yao</a>, 
<a href="/search/cs?searchtype=author&query=Laudanski%2C+K">Krzysztof Laudanski</a>, 
<a href="/search/cs?searchtype=author&query=Engelhardt%2C+B">Barbara Engelhardt</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 3rd Machine Learning for Health Symposium, PMLR
  225:323-339, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.03196" title="Abstract">arXiv:2110.03196</a> (replaced) [<a href="/pdf/2110.03196" title="Download PDF">pdf</a>, <a href="/format/2110.03196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explicitly Multi-Modal Benchmarks for Multi-Objective Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ota%2C+R">Ryosuke Ota</a>, 
<a href="/search/math?searchtype=author&query=Hagiwara%2C+R">Reiya Hagiwara</a>, 
<a href="/search/math?searchtype=author&query=Hamada%2C+N">Naoki Hamada</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+L">Likun Liu</a>, 
<a href="/search/math?searchtype=author&query=Yamamoto%2C+T">Takahiro Yamamoto</a>, 
<a href="/search/math?searchtype=author&query=Sakurai%2C+D">Daisuke Sakurai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.04596" title="Abstract">arXiv:2111.04596</a> (replaced) [<a href="/pdf/2111.04596" title="Download PDF">pdf</a>, <a href="/format/2111.04596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inertial Newton Algorithms Avoiding Strict Saddle Points
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Castera%2C+C">Camille Castera</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Optimization Theory and Applications (2023)
  199(12):881--903
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2111.11313" title="Abstract">arXiv:2111.11313</a> (replaced) [<a href="/pdf/2111.11313" title="Download PDF">pdf</a>, <a href="/format/2111.11313" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Homomorphism Tensors and Linear Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Grohe%2C+M">Martin Grohe</a>, 
<a href="/search/math?searchtype=author&query=Rattan%2C+G">Gaurav Rattan</a>, 
<a href="/search/math?searchtype=author&query=Seppelt%2C+T">Tim Seppelt</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 49th International Colloquium on Automata, Languages, and
  Programming (ICALP 2022)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2112.08958" title="Abstract">arXiv:2112.08958</a> (replaced) [<a href="/pdf/2112.08958" title="Download PDF">pdf</a>, <a href="/format/2112.08958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Utility maximizing load balancing policies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Goldsztajn%2C+D">Diego Goldsztajn</a>, 
<a href="/search/math?searchtype=author&query=Borst%2C+S+C">Sem C. Borst</a>, 
<a href="/search/math?searchtype=author&query=van+Leeuwaarden%2C+J+S+H">Johan S.H. van Leeuwaarden</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 73 pages, 6 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Stochastic systems, 13(2):211-246, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.01805" title="Abstract">arXiv:2201.01805</a> (replaced) [<a href="/pdf/2201.01805" title="Download PDF">pdf</a>, <a href="/format/2201.01805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Monoidal categories, representation gap and cryptography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Khovanov%2C+M">Mikhail Khovanov</a>, 
<a href="/search/math?searchtype=author&query=Sitaraman%2C+M">Maithreya Sitaraman</a>, 
<a href="/search/math?searchtype=author&query=Tubbenhauer%2C+D">Daniel Tubbenhauer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 52 pages, many figures, revised version, comments welcome
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Trans. Amer. Math. Soc. Ser. B 11 (2024), 329-395
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Representation Theory (math.RT)</span>; Cryptography and Security (cs.CR); Group Theory (math.GR); Quantum Algebra (math.QA)

</div>
</div>
</dd>
<dt><a name="item660">[660]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.13674" title="Abstract">arXiv:2203.13674</a> (replaced) [<a href="/pdf/2203.13674" title="Download PDF">pdf</a>, <a href="/format/2203.13674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dense Continuous-Time Optical Flow from Events and Frames
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gehrig%2C+M">Mathias Gehrig</a>, 
<a href="/search/cs?searchtype=author&query=Muglikar%2C+M">Manasi Muglikar</a>, 
<a href="/search/cs?searchtype=author&query=Scaramuzza%2C+D">Davide Scaramuzza</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Pattern Analysis and Machine Intelligence
  (TPAMI), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item661">[661]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.08038" title="Abstract">arXiv:2205.08038</a> (replaced) [<a href="/pdf/2205.08038" title="Download PDF">pdf</a>, <a href="/format/2205.08038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Newton and interior-point methods for (constrained) nonconvex-nonconcave  minmax optimization with stability and instability guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chinchilla%2C+R">Raphael Chinchilla</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+G">Guosong Yang</a>, 
<a href="/search/math?searchtype=author&query=Hespanha%2C+J+P">Joao P. Hespanha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at the Journal of the Mathematics of Control, Signals, and Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item662">[662]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.06944" title="Abstract">arXiv:2207.06944</a> (replaced) [<a href="/pdf/2207.06944" title="Download PDF">pdf</a>, <a href="/ps/2207.06944" title="Download PostScript">ps</a>, <a href="/format/2207.06944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differentially Private Graph Learning via Sensitivity-Bounded  Personalized PageRank
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Epasto%2C+A">Alessandro Epasto</a>, 
<a href="/search/cs?searchtype=author&query=Mirrokni%2C+V">Vahab Mirrokni</a>, 
<a href="/search/cs?searchtype=author&query=Perozzi%2C+B">Bryan Perozzi</a>, 
<a href="/search/cs?searchtype=author&query=Tsitsulin%2C+A">Anton Tsitsulin</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+P">Peilin Zhong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item663">[663]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.10611" title="Abstract">arXiv:2207.10611</a> (replaced) [<a href="/pdf/2207.10611" title="Download PDF">pdf</a>, <a href="/format/2207.10611" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incentive Designs for Stackelberg Games with a Large Number of Followers  and their Mean-Field Limits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sanjari%2C+S">Sina Sanjari</a>, 
<a href="/search/cs?searchtype=author&query=Bose%2C+S">Subhonmesh Bose</a>, 
<a href="/search/cs?searchtype=author&query=Ba%C5%9Far%2C+T">Tamer Ba&#x15f;ar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item664">[664]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.03367" title="Abstract">arXiv:2208.03367</a> (replaced) [<a href="/pdf/2208.03367" title="Download PDF">pdf</a>, <a href="/ps/2208.03367" title="Download PostScript">ps</a>, <a href="/format/2208.03367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sublinear Time Algorithm for Online Weighted Bipartite Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhao Song</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+R">Runzhou Tao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhaozhuo Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+J">Junze Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zhuo%2C+D">Danyang Zhuo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item665">[665]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.04448" title="Abstract">arXiv:2208.04448</a> (replaced) [<a href="/pdf/2208.04448" title="Download PDF">pdf</a>, <a href="/format/2208.04448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NeuralVDB: High-resolution Sparse Volume Representation using  Hierarchical Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Doyub Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Minjae Lee</a>, 
<a href="/search/cs?searchtype=author&query=Museth%2C+K">Ken Museth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item666">[666]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.05531" title="Abstract">arXiv:2208.05531</a> (replaced) [<a href="/pdf/2208.05531" title="Download PDF">pdf</a>, <a href="/ps/2208.05531" title="Download PostScript">ps</a>, <a href="/format/2208.05531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foundations of Monte Carlo methods and stochastic simulations -- From  Monte Carlo Lebesgue integration to weak approximation of SDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Przyby%C5%82owicz%2C+P">Pawe&#x142; Przyby&#x142;owicz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item667">[667]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.07998" title="Abstract">arXiv:2208.07998</a> (replaced) [<a href="/pdf/2208.07998" title="Download PDF">pdf</a>, <a href="/format/2208.07998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Artificial Neural Networks Can Tell Us About Human Language  Acquisition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Warstadt%2C+A">Alex Warstadt</a>, 
<a href="/search/cs?searchtype=author&query=Bowman%2C+S+R">Samuel R. Bowman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Please cite the published version with the following information: @incollection{warstadt2022artificial, title={What artificial neural networks can tell us about human language acquisition}, author={Warstadt, Alex and Bowman, Samuel R.}, booktitle={Algebraic Structures in Natural Language}, pages={17--60}, year={2022}, publisher={CRC Press} }
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item668">[668]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.03120" title="Abstract">arXiv:2210.03120</a> (replaced) [<a href="/pdf/2210.03120" title="Download PDF">pdf</a>, <a href="/format/2210.03120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GBSVM: Granular-ball Support Vector Machine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+S">Shuyin Xia</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+X">Xiaoyu Lian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoyin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xinbo Gao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiancu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+X">Xiaoli Peng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item669">[669]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.03728" title="Abstract">arXiv:2210.03728</a> (replaced) [<a href="/pdf/2210.03728" title="Download PDF">pdf</a>, <a href="/format/2210.03728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Latent Separation for Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tuan%2C+Y">Yi-Lin Tuan</a>, 
<a href="/search/cs?searchtype=author&query=Chiu%2C+Z">Zih-Yun Chiu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item670">[670]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.07085" title="Abstract">arXiv:2210.07085</a> (replaced) [<a href="/pdf/2210.07085" title="Download PDF">pdf</a>, <a href="/format/2210.07085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Uniform Certification in QBF
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chew%2C+L">Leroy Chew</a>, 
<a href="/search/cs?searchtype=author&query=Slivovsky%2C+F">Friedrich Slivovsky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item671">[671]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.07888" title="Abstract">arXiv:2210.07888</a> (replaced) [<a href="/pdf/2210.07888" title="Download PDF">pdf</a>, <a href="/format/2210.07888" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guessing Random Additive Noise Decoding of Network Coded Data  Transmitted over Burst Error Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chatzigeorgiou%2C+I">Ioannis Chatzigeorgiou</a>, 
<a href="/search/cs?searchtype=author&query=Savostyanov%2C+D">Dmitry Savostyanov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 double-column pages, 10 figures, 1 table. Extended version of paper presented at IEEE ISIT 2022. Submitted to IEEE Transactions on Vehicular Technology
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item672">[672]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.11640" title="Abstract">arXiv:2210.11640</a> (replaced) [<a href="/pdf/2210.11640" title="Download PDF">pdf</a>, <a href="/format/2210.11640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Not All Asians are the Same: A Disaggregated Approach to Identifying  Anti-Asian Racism in Social Media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lakhanpal%2C+S">Sanyam Lakhanpal</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qian Li</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kookjin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Doowon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Chae%2C+H">Heewon Chae</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+H+K">Hazel K. Kwon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at theWebConf 2024 (formerly, WWW)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item673">[673]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.14547" title="Abstract">arXiv:2210.14547</a> (replaced) [<a href="/pdf/2210.14547" title="Download PDF">pdf</a>, <a href="/format/2210.14547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tracking-based distributed equilibrium seeking for aggregative games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Carnevale%2C+G">Guido Carnevale</a>, 
<a href="/search/eess?searchtype=author&query=Fabiani%2C+F">Filippo Fabiani</a>, 
<a href="/search/eess?searchtype=author&query=Fele%2C+F">Filiberto Fele</a>, 
<a href="/search/eess?searchtype=author&query=Margellos%2C+K">Kostas Margellos</a>, 
<a href="/search/eess?searchtype=author&query=Notarstefano%2C+G">Giuseppe Notarstefano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Computer Science and Game Theory (cs.GT); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item674">[674]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.03846" title="Abstract">arXiv:2211.03846</a> (replaced) [<a href="/pdf/2211.03846" title="Download PDF">pdf</a>, <a href="/format/2211.03846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Causal Discovery From Interventions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abyaneh%2C+A">Amin Abyaneh</a>, 
<a href="/search/cs?searchtype=author&query=Scherrer%2C+N">Nino Scherrer</a>, 
<a href="/search/cs?searchtype=author&query=Schwab%2C+P">Patrick Schwab</a>, 
<a href="/search/cs?searchtype=author&query=Bauer%2C+S">Stefan Bauer</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>, 
<a href="/search/cs?searchtype=author&query=Mehrjou%2C+A">Arash Mehrjou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Multiagent Systems (cs.MA); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item675">[675]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.04659" title="Abstract">arXiv:2211.04659</a> (replaced) [<a href="/pdf/2211.04659" title="Download PDF">pdf</a>, <a href="/format/2211.04659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When is Momentum Extragradient Optimal? A Polynomial-Based Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J+L">Junhyung Lyle Kim</a>, 
<a href="/search/cs?searchtype=author&query=Gidel%2C+G">Gauthier Gidel</a>, 
<a href="/search/cs?searchtype=author&query=Kyrillidis%2C+A">Anastasios Kyrillidis</a>, 
<a href="/search/cs?searchtype=author&query=Pedregosa%2C+F">Fabian Pedregosa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item676">[676]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.10745" title="Abstract">arXiv:2211.10745</a> (replaced) [<a href="/pdf/2211.10745" title="Download PDF">pdf</a>, <a href="/ps/2211.10745" title="Download PostScript">ps</a>, <a href="/format/2211.10745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A discrete-ordinate weak Galerkin method for radiative transfer equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Singh%2C+M+K">Maneesh Kumar Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in Applied Numerical Mathematics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item677">[677]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.11137" title="Abstract">arXiv:2211.11137</a> (replaced) [<a href="/pdf/2211.11137" title="Download PDF">pdf</a>, <a href="/format/2211.11137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Long Range Constraints for Neural Texture Synthesis Using Sliced  Wasserstein Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+L">Liping Yin</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+A">Albert Chua</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Added extra ablation studies
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item678">[678]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.15281" title="Abstract">arXiv:2211.15281</a> (replaced) [<a href="/pdf/2211.15281" title="Download PDF">pdf</a>, <a href="/format/2211.15281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flow: Per-Instance Personalized Federated Learning Through Dynamic  Routing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Panchal%2C+K">Kunjal Panchal</a>, 
<a href="/search/cs?searchtype=author&query=Choudhary%2C+S">Sunav Choudhary</a>, 
<a href="/search/cs?searchtype=author&query=Parikh%2C+N">Nisarg Parikh</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lijun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+H">Hui Guan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Annual Conference on Neural Information Processing Systems (NeurIPS), 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item679">[679]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.05949" title="Abstract">arXiv:2212.05949</a> (replaced) [<a href="/pdf/2212.05949" title="Download PDF">pdf</a>, <a href="/ps/2212.05949" title="Download PostScript">ps</a>, <a href="/format/2212.05949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Corruption-Robust Algorithms with Uncertainty Weighting for Nonlinear  Contextual Bandits and Markov Decision Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ye%2C+C">Chenlu Ye</a>, 
<a href="/search/stat?searchtype=author&query=Xiong%2C+W">Wei Xiong</a>, 
<a href="/search/stat?searchtype=author&query=Gu%2C+Q">Quanquan Gu</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We study the corruption-robust MDPs and contextual bandits with general function approximation
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICML 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item680">[680]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.07699" title="Abstract">arXiv:2212.07699</a> (replaced) [<a href="/pdf/2212.07699" title="Download PDF">pdf</a>, <a href="/format/2212.07699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieval-based Disentangled Representation Learning with Natural  Language Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiawei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoguang Li</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+L">Lifeng Shang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lei Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item681">[681]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.00270" title="Abstract">arXiv:2301.00270</a> (replaced) [<a href="/pdf/2301.00270" title="Download PDF">pdf</a>, <a href="/format/2301.00270" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NetEffect: Discovery and Exploitation of Generalized Network Effects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Meng-Chieh Lee</a>, 
<a href="/search/cs?searchtype=author&query=Shekhar%2C+S">Shubhranshu Shekhar</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+J">Jaemin Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Faloutsos%2C+C">Christos Faloutsos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to PAKDD 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item682">[682]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.04391" title="Abstract">arXiv:2301.04391</a> (replaced) [<a href="/pdf/2301.04391" title="Download PDF">pdf</a>, <a href="/format/2301.04391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grassroots Systems: Concept, Examples, Implementation and Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shapiro%2C+E">Ehud Shapiro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2202.05619">arXiv:2202.05619</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Multiagent Systems (cs.MA); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item683">[683]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.04494" title="Abstract">arXiv:2301.04494</a> (replaced) [<a href="/pdf/2301.04494" title="Download PDF">pdf</a>, <a href="/format/2301.04494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-label Image Classification using Adaptive Graph Convolutional  Networks: from a Single Domain to Multiple Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+I+P">Indel Pal Singh</a>, 
<a href="/search/cs?searchtype=author&query=Ghorbel%2C+E">Enjie Ghorbel</a>, 
<a href="/search/cs?searchtype=author&query=Oyedotun%2C+O">Oyebade Oyedotun</a>, 
<a href="/search/cs?searchtype=author&query=Aouada%2C+D">Djamila Aouada</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item684">[684]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.04741" title="Abstract">arXiv:2301.04741</a> (replaced) [<a href="/pdf/2301.04741" title="Download PDF">pdf</a>, <a href="/format/2301.04741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Preference-Based Reinforcement Learning Using Learned Dynamics  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Datta%2C+G">Gaurav Datta</a>, 
<a href="/search/cs?searchtype=author&query=Novoseller%2C+E">Ellen Novoseller</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+D+S">Daniel S. Brown</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In proceedings of the 2023 IEEE International Conference on Robotics and Automation (ICRA 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item685">[685]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.09505" title="Abstract">arXiv:2301.09505</a> (replaced) [<a href="/pdf/2301.09505" title="Download PDF">pdf</a>, <a href="/format/2301.09505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking the Expressive Power of GNNs via Graph Biconnectivity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bohang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Shengjie Luo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+D">Di He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended from ICLR 2023 Outstanding Paper; 60 pages, 12 figures. Fix typos in the previous version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item686">[686]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00722" title="Abstract">arXiv:2302.00722</a> (replaced) [<a href="/pdf/2302.00722" title="Download PDF">pdf</a>, <a href="/format/2302.00722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Deep Learning: From Activations to Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schneider%2C+J">Johannes Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Vlachos%2C+M">Michalis Vlachos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Agents and Artificial Intelligence (ICAART), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item687">[687]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01672" title="Abstract">arXiv:2302.01672</a> (replaced) [<a href="/pdf/2302.01672" title="Download PDF">pdf</a>, <a href="/format/2302.01672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Role of Game Networking in the Fusion of Physical and Digital Worlds  through 6G Wireless
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bui%2C+V">Van-Phuc Bui</a>, 
<a href="/search/eess?searchtype=author&query=Pandey%2C+S+R">Shashi Raj Pandey</a>, 
<a href="/search/eess?searchtype=author&query=Casparsen%2C+A">Andreas Casparsen</a>, 
<a href="/search/eess?searchtype=author&query=Chiariotti%2C+F">Federico Chiariotti</a>, 
<a href="/search/eess?searchtype=author&query=Popovski%2C+P">Petar Popovski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item688">[688]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02108" title="Abstract">arXiv:2302.02108</a> (replaced) [<a href="/pdf/2302.02108" title="Download PDF">pdf</a>, <a href="/ps/2302.02108" title="Download PostScript">ps</a>, <a href="/format/2302.02108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Distillation in Vision Transformers: A Critical Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Habib%2C+G">Gousia Habib</a>, 
<a href="/search/cs?searchtype=author&query=Saleem%2C+T+J">Tausifa Jan Saleem</a>, 
<a href="/search/cs?searchtype=author&query=Lall%2C+B">Brejesh Lall</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item689">[689]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.02182" title="Abstract">arXiv:2302.02182</a> (replaced) [<a href="/pdf/2302.02182" title="Download PDF">pdf</a>, <a href="/format/2302.02182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Reinforcement Learning in Non-Stationary Context-Driven  Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hamadanian%2C+P">Pouya Hamadanian</a>, 
<a href="/search/cs?searchtype=author&query=Nasr-Esfahany%2C+A">Arash Nasr-Esfahany</a>, 
<a href="/search/cs?searchtype=author&query=Schwarzkopf%2C+M">Malte Schwarzkopf</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+S">Siddartha Sen</a>, 
<a href="/search/cs?searchtype=author&query=Alizadeh%2C+M">Mohammad Alizadeh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages + 6 pages in the appendix, 10 Figures and 8 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item690">[690]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03106" title="Abstract">arXiv:2302.03106</a> (replaced) [<a href="/pdf/2302.03106" title="Download PDF">pdf</a>, <a href="/ps/2302.03106" title="Download PostScript">ps</a>, <a href="/format/2302.03106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient and Flexible Topic Modeling using Pretrained Embeddings and  Bag of Sentences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schneider%2C+J">Johannes Schneider</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Agents and Artificial Intelligence (ICAART), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item691">[691]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.05428" title="Abstract">arXiv:2302.05428</a> (replaced) [<a href="/pdf/2302.05428" title="Download PDF">pdf</a>, <a href="/format/2302.05428" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STERLING: Synergistic Representation Learning on Bipartite Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jing%2C+B">Baoyu Jing</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yuchen Yan</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+K">Kaize Ding</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+C">Chanyoung Park</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yada Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+H">Hanghang Tong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI'2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item692">[692]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.11628" title="Abstract">arXiv:2302.11628</a> (replaced) [<a href="/pdf/2302.11628" title="Download PDF">pdf</a>, <a href="/format/2302.11628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provable Robustness Against a Union of $\ell_0$ Adversarial Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hammoudeh%2C+Z">Zayd Hammoudeh</a>, 
<a href="/search/cs?searchtype=author&query=Lowd%2C+D">Daniel Lowd</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAAI 2024 -- Extended version including the supplementary material
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item693">[693]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.11694" title="Abstract">arXiv:2302.11694</a> (replaced) [<a href="/pdf/2302.11694" title="Download PDF">pdf</a>, <a href="/format/2302.11694" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trustworthy Reinforcement Learning for Quadrotor UAV Tracking Control  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Boyle%2C+D">David Boyle</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 9 figures. arXiv admin note: substantial text overlap with <a href="/abs/2205.07150">arXiv:2205.07150</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item694">[694]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.13053" title="Abstract">arXiv:2302.13053</a> (replaced) [<a href="/pdf/2302.13053" title="Download PDF">pdf</a>, <a href="/format/2302.13053" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Neural Network Training over Distributed Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kolluri%2C+A">Aashish Kolluri</a>, 
<a href="/search/cs?searchtype=author&query=Choudhary%2C+S">Sarthak Choudhary</a>, 
<a href="/search/cs?searchtype=author&query=Hooi%2C+B">Bryan Hooi</a>, 
<a href="/search/cs?searchtype=author&query=Saxena%2C+P">Prateek Saxena</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item695">[695]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02307" title="Abstract">arXiv:2303.02307</a> (replaced) [<a href="/pdf/2303.02307" title="Download PDF">pdf</a>, <a href="/format/2303.02307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Steganography via Coherent and Fock State Encoding in an Optical  Medium
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Avritzer%2C+B">Bruno Avritzer</a>, 
<a href="/search/quant-ph?searchtype=author&query=Brun%2C+T">Todd Brun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item696">[696]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02744" title="Abstract">arXiv:2303.02744</a> (replaced) [<a href="/pdf/2303.02744" title="Download PDF">pdf</a>, <a href="/ps/2303.02744" title="Download PostScript">ps</a>, <a href="/format/2303.02744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scientometric Rules as a Guide to Transform Science Systems in the  Middle East &amp; North Africa
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=El-Ouahi%2C+J">Jamal El-Ouahi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
</div>
</dd>
<dt><a name="item697">[697]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02812" title="Abstract">arXiv:2303.02812</a> (replaced) [<a href="/pdf/2303.02812" title="Download PDF">pdf</a>, <a href="/ps/2303.02812" title="Download PostScript">ps</a>, <a href="/format/2303.02812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Frames for signal processing on Cayley graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beck%2C+K">Kathryn Beck</a>, 
<a href="/search/cs?searchtype=author&query=Ghandehari%2C+M">Mahya Ghandehari</a>, 
<a href="/search/cs?searchtype=author&query=Hudson%2C+S">Skyler Hudson</a>, 
<a href="/search/cs?searchtype=author&query=Paltenstein%2C+J">Jenna Paltenstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item698">[698]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03673" title="Abstract">arXiv:2303.03673</a> (replaced) [<a href="/pdf/2303.03673" title="Download PDF">pdf</a>, <a href="/ps/2303.03673" title="Download PostScript">ps</a>, <a href="/format/2303.03673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multilevel Monte Carlo methods for stochastic convection-diffusion  eigenvalue problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cui%2C+T">Tiangang Cui</a>, 
<a href="/search/math?searchtype=author&query=De+Sterck%2C+H">Hans De Sterck</a>, 
<a href="/search/math?searchtype=author&query=Gilbert%2C+A+D">Alexander D. Gilbert</a>, 
<a href="/search/math?searchtype=author&query=Polishchuk%2C+S">Stanislav Polishchuk</a>, 
<a href="/search/math?searchtype=author&query=Scheichl%2C+R">Robert Scheichl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item699">[699]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08253" title="Abstract">arXiv:2303.08253</a> (replaced) [<a href="/pdf/2303.08253" title="Download PDF">pdf</a>, <a href="/format/2303.08253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> R2 Loss: Range Restriction Loss for Model Compression and Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kundu%2C+A">Arnav Kundu</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+C">Chungkuk Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Srijan Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+M">Minsik Cho</a>, 
<a href="/search/cs?searchtype=author&query=Adya%2C+S">Saurabh Adya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Performance (cs.PF); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item700">[700]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.14877" title="Abstract">arXiv:2303.14877</a> (replaced) [<a href="/pdf/2303.14877" title="Download PDF">pdf</a>, <a href="/format/2303.14877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Error-mitigated Quantum Approximate Optimization via Learning-based  Adaptive Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Cheng%2C+L">Lixue Cheng</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chen%2C+Y">Yu-Qin Chen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zhang%2C+S">Shi-Xin Zhang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zhang%2C+S">Shengyu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Main text: 11 pages, 4 figures, SI: 5 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item701">[701]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15792" title="Abstract">arXiv:2303.15792</a> (replaced) [<a href="/pdf/2303.15792" title="Download PDF">pdf</a>, <a href="/format/2303.15792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SDAT: Sub-Dataset Alternation Training for Improved Image Demosaicing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Becker%2C+Y">Yuval Becker</a>, 
<a href="/search/eess?searchtype=author&query=Nossek%2C+R+Z">Raz Z. Nossek</a>, 
<a href="/search/eess?searchtype=author&query=Peleg%2C+T">Tomer Peleg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item702">[702]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.17304" title="Abstract">arXiv:2303.17304</a> (replaced) [<a href="/pdf/2303.17304" title="Download PDF">pdf</a>, <a href="/format/2303.17304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust offset-free constrained Model Predictive Control with Long  Short-Term Memory Networks -- Extended version
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Schimperna%2C+I">Irene Schimperna</a>, 
<a href="/search/eess?searchtype=author&query=Magni%2C+L">Lalo Magni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item703">[703]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03894" title="Abstract">arXiv:2304.03894</a> (replaced) [<a href="/pdf/2304.03894" title="Download PDF">pdf</a>, <a href="/format/2304.03894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A multifidelity approach to continual learning for physical systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Howard%2C+A">Amanda Howard</a>, 
<a href="/search/math?searchtype=author&query=Fu%2C+Y">Yucheng Fu</a>, 
<a href="/search/math?searchtype=author&query=Stinis%2C+P">Panos Stinis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item704">[704]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06646" title="Abstract">arXiv:2304.06646</a> (replaced) [<a href="/pdf/2304.06646" title="Download PDF">pdf</a>, <a href="/ps/2304.06646" title="Download PostScript">ps</a>, <a href="/format/2304.06646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterising Modal Formulas with Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cate%2C+B+t">Balder ten Cate</a>, 
<a href="/search/cs?searchtype=author&query=Koudijs%2C+R">Raoul Koudijs</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Expanded version of material from Raoul Koudijs's MSc thesis (2022). To appear in ACM Transactions on Computational Logic
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item705">[705]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.06935" title="Abstract">arXiv:2304.06935</a> (replaced) [<a href="/pdf/2304.06935" title="Download PDF">pdf</a>, <a href="/ps/2304.06935" title="Download PostScript">ps</a>, <a href="/format/2304.06935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Groebner.jl: A package for Gr&#xf6;bner bases computations in Julia
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Demin%2C+A">Alexander Demin</a>, 
<a href="/search/cs?searchtype=author&query=Gowda%2C+S">Shashi Gowda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Software (cs.MS)</span>; Symbolic Computation (cs.SC); Commutative Algebra (math.AC)

</div>
</div>
</dd>
<dt><a name="item706">[706]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07428" title="Abstract">arXiv:2304.07428</a> (replaced) [<a href="/pdf/2304.07428" title="Download PDF">pdf</a>, <a href="/format/2304.07428" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Formal Synthesis of Unknown Systems via Robust Simulation  Relations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sch%C3%B6n%2C+O">Oliver Sch&#xf6;n</a>, 
<a href="/search/eess?searchtype=author&query=van+Huijgevoort%2C+B">Birgit van Huijgevoort</a>, 
<a href="/search/eess?searchtype=author&query=Haesaert%2C+S">Sofie Haesaert</a>, 
<a href="/search/eess?searchtype=author&query=Soudjani%2C+S">Sadegh Soudjani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2210.08269">arXiv:2210.08269</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item707">[707]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.08410" title="Abstract">arXiv:2304.08410</a> (replaced) [<a href="/pdf/2304.08410" title="Download PDF">pdf</a>, <a href="/format/2304.08410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> About the Expressive Power and Complexity of Order-Invariance with Two  Variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bednarczyk%2C+B">Bartosz Bednarczyk</a>, 
<a href="/search/cs?searchtype=author&query=Grange%2C+J">Julien Grange</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2207.04986">arXiv:2207.04986</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item708">[708]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.09986" title="Abstract">arXiv:2304.09986</a> (replaced) [<a href="/pdf/2304.09986" title="Download PDF">pdf</a>, <a href="/ps/2304.09986" title="Download PostScript">ps</a>, <a href="/format/2304.09986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A note on Stone-&#x10c;ech compactification in ZFA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Przyby%C5%82ek%2C+M+R">Micha&#x142; R. Przyby&#x142;ek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
</div>
</dd>
<dt><a name="item709">[709]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.00521" title="Abstract">arXiv:2305.00521</a> (replaced) [<a href="/pdf/2305.00521" title="Download PDF">pdf</a>, <a href="/format/2305.00521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StyleLipSync: Style-based Personalized Lip-sync Video Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ki%2C+T">Taekyung Ki</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+D">Dongchan Min</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Computer Vision (ICCV) 2023. Project page: <a href="https://stylelipsync.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item710">[710]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01080" title="Abstract">arXiv:2305.01080</a> (replaced) [<a href="/pdf/2305.01080" title="Download PDF">pdf</a>, <a href="/format/2305.01080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporal Betweenness Centrality on Shortest Walks Variants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Naima%2C+M">Mehdi Naima</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item711">[711]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06974" title="Abstract">arXiv:2305.06974</a> (replaced) [<a href="/pdf/2305.06974" title="Download PDF">pdf</a>, <a href="/format/2305.06974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hypergraph dualization with FPT-delay parameterized by the degeneracy  and dimension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bartier%2C+V">Valentin Bartier</a>, 
<a href="/search/cs?searchtype=author&query=Defrain%2C+O">Oscar Defrain</a>, 
<a href="/search/cs?searchtype=author&query=Inerney%2C+F+M">Fionn Mc Inerney</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item712">[712]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.06989" title="Abstract">arXiv:2305.06989</a> (replaced) [<a href="/pdf/2305.06989" title="Download PDF">pdf</a>, <a href="/format/2305.06989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Wave Functions for Superfluids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Lou%2C+W+T">Wan Tong Lou</a>, 
<a href="/search/cond-mat?searchtype=author&query=Sutterud%2C+H">Halvard Sutterud</a>, 
<a href="/search/cond-mat?searchtype=author&query=Cassella%2C+G">Gino Cassella</a>, 
<a href="/search/cond-mat?searchtype=author&query=Foulkes%2C+W+M+C">W.M.C. Foulkes</a>, 
<a href="/search/cond-mat?searchtype=author&query=Knolle%2C+J">Johannes Knolle</a>, 
<a href="/search/cond-mat?searchtype=author&query=Pfau%2C+D">David Pfau</a>, 
<a href="/search/cond-mat?searchtype=author&query=Spencer%2C+J+S">James S. Spencer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 5 figures. Talk presented at the 2023 APS March Meeting, March 5-10, 2023, Las Vegas, Nevada, United States
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Gases (cond-mat.quant-gas)</span>; Superconductivity (cond-mat.supr-con); Machine Learning (cs.LG); Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item713">[713]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08757" title="Abstract">arXiv:2305.08757</a> (replaced) [<a href="/pdf/2305.08757" title="Download PDF">pdf</a>, <a href="/format/2305.08757" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics Informed Token Transformer for Solving Partial Differential  Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lorsung%2C+C">Cooper Lorsung</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zijie Li</a>, 
<a href="/search/cs?searchtype=author&query=Farimani%2C+A+B">Amir Barati Farimani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item714">[714]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09492" title="Abstract">arXiv:2305.09492</a> (replaced) [<a href="/pdf/2305.09492" title="Download PDF">pdf</a>, <a href="/format/2305.09492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solar Active Region Magnetogram Image Dataset for Studies of Space  Weather
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Boucheron%2C+L+E">Laura E. Boucheron</a>, 
<a href="/search/astro-ph?searchtype=author&query=Vincent%2C+T">Ty Vincent</a>, 
<a href="/search/astro-ph?searchtype=author&query=Grajeda%2C+J+A">Jeremy A. Grajeda</a>, 
<a href="/search/astro-ph?searchtype=author&query=Wuest%2C+E">Ellery Wuest</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Solar and Stellar Astrophysics (astro-ph.SR)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item715">[715]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09824" title="Abstract">arXiv:2305.09824</a> (replaced) [<a href="/pdf/2305.09824" title="Download PDF">pdf</a>, <a href="/format/2305.09824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Costs and Benefits of Adopting Lifelong Learning for Software  Analytics -- Empirical Study on Brown Build and Risk Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Olewicki%2C+D">Doriane Olewicki</a>, 
<a href="/search/cs?searchtype=author&query=Habchi%2C+S">Sarra Habchi</a>, 
<a href="/search/cs?searchtype=author&query=Nayrolles%2C+M">Mathieu Nayrolles</a>, 
<a href="/search/cs?searchtype=author&query=Faramarzi%2C+M">Mojtaba Faramarzi</a>, 
<a href="/search/cs?searchtype=author&query=Chandar%2C+S">Sarath Chandar</a>, 
<a href="/search/cs?searchtype=author&query=Adams%2C+B">Bram Adams</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item716">[716]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.09859" title="Abstract">arXiv:2305.09859</a> (replaced) [<a href="/pdf/2305.09859" title="Download PDF">pdf</a>, <a href="/format/2305.09859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smaller Language Models are Better Black-box Machine-Generated Text  Detectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mireshghallah%2C+N">Niloofar Mireshghallah</a>, 
<a href="/search/cs?searchtype=author&query=Mattern%2C+J">Justus Mattern</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Sicun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Shokri%2C+R">Reza Shokri</a>, 
<a href="/search/cs?searchtype=author&query=Berg-Kirkpatrick%2C+T">Taylor Berg-Kirkpatrick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item717">[717]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10181" title="Abstract">arXiv:2305.10181</a> (replaced) [<a href="/pdf/2305.10181" title="Download PDF">pdf</a>, <a href="/format/2305.10181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the cloud of feature interaction scores in a Rashomon set
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sichao Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Q">Quanling Deng</a>, 
<a href="/search/cs?searchtype=author&query=Barnard%2C+A">Amanda Barnard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item718">[718]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10818" title="Abstract">arXiv:2305.10818</a> (replaced) [<a href="/pdf/2305.10818" title="Download PDF">pdf</a>, <a href="/format/2305.10818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Language Models Generation Can Be Halted Early
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vaina%2C+S+M+L+C">Sofia Maria Lo Cicero Vaina</a>, 
<a href="/search/cs?searchtype=author&query=Balagansky%2C+N">Nikita Balagansky</a>, 
<a href="/search/cs?searchtype=author&query=Gavrilov%2C+D">Daniil Gavrilov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item719">[719]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12975" title="Abstract">arXiv:2305.12975</a> (replaced) [<a href="/pdf/2305.12975" title="Download PDF">pdf</a>, <a href="/ps/2305.12975" title="Download PostScript">ps</a>, <a href="/format/2305.12975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graphical Proof Theory I: Sequent Systems on Undirected Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Acclavio%2C+M">Matteo Acclavio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item720">[720]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13068" title="Abstract">arXiv:2305.13068</a> (replaced) [<a href="/pdf/2305.13068" title="Download PDF">pdf</a>, <a href="/format/2305.13068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making Language Models Better Tool Learners with Execution Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiao%2C+S">Shuofei Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+H">Honghao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+C">Chengfei Lv</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Q">Qianghuai Jia</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item721">[721]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13915" title="Abstract">arXiv:2305.13915</a> (replaced) [<a href="/pdf/2305.13915" title="Download PDF">pdf</a>, <a href="/format/2305.13915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DAPR: A Benchmark on Document-Aware Passage Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kexin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Reimers%2C+N">Nils Reimers</a>, 
<a href="/search/cs?searchtype=author&query=Gurevych%2C+I">Iryna Gurevych</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item722">[722]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14051" title="Abstract">arXiv:2305.14051</a> (replaced) [<a href="/pdf/2305.14051" title="Download PDF">pdf</a>, <a href="/format/2305.14051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Downlink Clustering-Based Scheduling of IRS-Assisted Communications With  Reconfiguration Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rech%2C+A">Alberto Rech</a>, 
<a href="/search/eess?searchtype=author&query=Pagin%2C+M">Matteo Pagin</a>, 
<a href="/search/eess?searchtype=author&query=Badia%2C+L">Leonardo Badia</a>, 
<a href="/search/eess?searchtype=author&query=Tomasin%2C+S">Stefano Tomasin</a>, 
<a href="/search/eess?searchtype=author&query=Giordani%2C+M">Marco Giordani</a>, 
<a href="/search/eess?searchtype=author&query=Gambini%2C+J">Jonathan Gambini</a>, 
<a href="/search/eess?searchtype=author&query=Zorzi%2C+M">Michele Zorzi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 12 figures, journal paper. arXiv admin note: text overlap with <a href="/abs/2301.10738">arXiv:2301.10738</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item723">[723]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15096" title="Abstract">arXiv:2305.15096</a> (replaced) [<a href="/pdf/2305.15096" title="Download PDF">pdf</a>, <a href="/format/2305.15096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Masking Rate Schedules for MLM Pretraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ankner%2C+Z">Zachary Ankner</a>, 
<a href="/search/cs?searchtype=author&query=Saphra%2C+N">Naomi Saphra</a>, 
<a href="/search/cs?searchtype=author&query=Blalock%2C+D">Davis Blalock</a>, 
<a href="/search/cs?searchtype=author&query=Frankle%2C+J">Jonathan Frankle</a>, 
<a href="/search/cs?searchtype=author&query=Leavitt%2C+M+L">Matthew L. Leavitt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item724">[724]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15596" title="Abstract">arXiv:2305.15596</a> (replaced) [<a href="/pdf/2305.15596" title="Download PDF">pdf</a>, <a href="/format/2305.15596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Online Rollout for Multivehicle Routing in Unmapped  Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weber%2C+J+W">Jamison W. Weber</a>, 
<a href="/search/cs?searchtype=author&query=Giriyan%2C+D+R">Dhanush R. Giriyan</a>, 
<a href="/search/cs?searchtype=author&query=Parkar%2C+D+R">Devendra R. Parkar</a>, 
<a href="/search/cs?searchtype=author&query=Richa%2C+A+W">Andr&#xe9;a W. Richa</a>, 
<a href="/search/cs?searchtype=author&query=Bertsekas%2C+D+P">Dimitri P. Bertsekas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item725">[725]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16179" title="Abstract">arXiv:2305.16179</a> (replaced) [<a href="/pdf/2305.16179" title="Download PDF">pdf</a>, <a href="/ps/2305.16179" title="Download PostScript">ps</a>, <a href="/format/2305.16179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dropout Drops Double Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tian-Le Yang</a>, 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+J">Joe Suzuki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST)

</div>
</div>
</dd>
<dt><a name="item726">[726]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16860" title="Abstract">arXiv:2305.16860</a> (replaced) [<a href="/pdf/2305.16860" title="Download PDF">pdf</a>, <a href="/ps/2305.16860" title="Download PostScript">ps</a>, <a href="/format/2305.16860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Error Bounds for Flow Matching Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Benton%2C+J">Joe Benton</a>, 
<a href="/search/stat?searchtype=author&query=Deligiannidis%2C+G">George Deligiannidis</a>, 
<a href="/search/stat?searchtype=author&query=Doucet%2C+A">Arnaud Doucet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item727">[727]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18377" title="Abstract">arXiv:2305.18377</a> (replaced) [<a href="/pdf/2305.18377" title="Download PDF">pdf</a>, <a href="/format/2305.18377" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BadLabel: A Robust Perspective on Evaluating and Enhancing Label-noise  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingfeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+B">Bo Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haohan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bo Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sugiyama%2C+M">Masashi Sugiyama</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE T-PAMI 2024 Accept
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item728">[728]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19259" title="Abstract">arXiv:2305.19259</a> (replaced) [<a href="/pdf/2305.19259" title="Download PDF">pdf</a>, <a href="/format/2305.19259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Convergence of Incremental Gradient for Non-Convex Smooth Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koloskova%2C+A">Anastasia Koloskova</a>, 
<a href="/search/cs?searchtype=author&query=Doikov%2C+N">Nikita Doikov</a>, 
<a href="/search/cs?searchtype=author&query=Stich%2C+S+U">Sebastian U. Stich</a>, 
<a href="/search/cs?searchtype=author&query=Jaggi%2C+M">Martin Jaggi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item729">[729]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00353" title="Abstract">arXiv:2306.00353</a> (replaced) [<a href="/pdf/2306.00353" title="Download PDF">pdf</a>, <a href="/format/2306.00353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constructing Semantics-Aware Adversarial Examples with Probabilistic  Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhang%2C+A">Andi Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+M">Mingtian Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Wischik%2C+D">Damon Wischik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item730">[730]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00809" title="Abstract">arXiv:2306.00809</a> (replaced) [<a href="/pdf/2306.00809" title="Download PDF">pdf</a>, <a href="/format/2306.00809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Initial Guessing Bias: How Untrained Networks Favor Some Classes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Francazi%2C+E">Emanuele Francazi</a>, 
<a href="/search/cs?searchtype=author&query=Lucchi%2C+A">Aurelien Lucchi</a>, 
<a href="/search/cs?searchtype=author&query=Baity-Jesi%2C+M">Marco Baity-Jesi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item731">[731]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01315" title="Abstract">arXiv:2306.01315</a> (replaced) [<a href="/pdf/2306.01315" title="Download PDF">pdf</a>, <a href="/ps/2306.01315" title="Download PostScript">ps</a>, <a href="/format/2306.01315" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Short rank-metric codes and scattered subspaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lia%2C+S">Stefano Lia</a>, 
<a href="/search/cs?searchtype=author&query=Longobardi%2C+G">Giovanni Longobardi</a>, 
<a href="/search/cs?searchtype=author&query=Marino%2C+G">Giuseppe Marino</a>, 
<a href="/search/cs?searchtype=author&query=Trombetti%2C+R">Rocco Trombetti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item732">[732]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01745" title="Abstract">arXiv:2306.01745</a> (replaced) [<a href="/pdf/2306.01745" title="Download PDF">pdf</a>, <a href="/format/2306.01745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Biomarker Discovery with Quantum Neural Networks: A Case-study in  CTLA4-Activation Pathways
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Nguyen%2C+N">Nam Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item733">[733]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01798" title="Abstract">arXiv:2306.01798</a> (replaced) [<a href="/pdf/2306.01798" title="Download PDF">pdf</a>, <a href="/ps/2306.01798" title="Download PostScript">ps</a>, <a href="/format/2306.01798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring EFL students&#x27; prompt engineering in human-AI story writing: an  Activity Theory perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Woo%2C+D+J">David James Woo</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+K">Kai Guo</a>, 
<a href="/search/cs?searchtype=author&query=Susanto%2C+H">Hengky Susanto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item734">[734]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04120" title="Abstract">arXiv:2306.04120</a> (replaced) [<a href="/pdf/2306.04120" title="Download PDF">pdf</a>, <a href="/format/2306.04120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MESSY Estimation: Maximum-Entropy based Stochastic and Symbolic densitY  Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tohme%2C+T">Tony Tohme</a>, 
<a href="/search/cs?searchtype=author&query=Sadr%2C+M">Mohsen Sadr</a>, 
<a href="/search/cs?searchtype=author&query=Youcef-Toumi%2C+K">Kamal Youcef-Toumi</a>, 
<a href="/search/cs?searchtype=author&query=Hadjiconstantinou%2C+N+G">Nicolas G. Hadjiconstantinou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Transactions on Machine Learning Research (TMLR)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item735">[735]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04288" title="Abstract">arXiv:2306.04288</a> (replaced) [<a href="/pdf/2306.04288" title="Download PDF">pdf</a>, <a href="/format/2306.04288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revising deep learning methods in parking lot occupancy detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Martynova%2C+A">Anastasia Martynova</a>, 
<a href="/search/cs?searchtype=author&query=Kuznetsov%2C+M">Mikhail Kuznetsov</a>, 
<a href="/search/cs?searchtype=author&query=Porvatov%2C+V">Vadim Porvatov</a>, 
<a href="/search/cs?searchtype=author&query=Tishin%2C+V">Vladislav Tishin</a>, 
<a href="/search/cs?searchtype=author&query=Kuznetsov%2C+A">Andrey Kuznetsov</a>, 
<a href="/search/cs?searchtype=author&query=Semenova%2C+N">Natalia Semenova</a>, 
<a href="/search/cs?searchtype=author&query=Kuznetsova%2C+K">Ksenia Kuznetsova</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item736">[736]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04959" title="Abstract">arXiv:2306.04959</a> (replaced) [<a href="/pdf/2306.04959" title="Download PDF">pdf</a>, <a href="/format/2306.04959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedMLSecurity: A Benchmark for Attacks and Defenses in Federated  Learning and Federated LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Shanshan Han</a>, 
<a href="/search/cs?searchtype=author&query=Buyukates%2C+B">Baturalp Buyukates</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zijian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+H">Han Jin</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Weizhao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lichao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+W">Wenxuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+C">Chulin Xie</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuhang Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuhui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Joe-Wong%2C+C">Carlee Joe-Wong</a>, 
<a href="/search/cs?searchtype=author&query=Avestimehr%2C+S">Salman Avestimehr</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+C">Chaoyang He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item737">[737]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05253" title="Abstract">arXiv:2306.05253</a> (replaced) [<a href="/pdf/2306.05253" title="Download PDF">pdf</a>, <a href="/format/2306.05253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum computing algorithms for inverse problems on graphs and an  NP-complete inverse problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ilmavirta%2C+J">Joonas Ilmavirta</a>, 
<a href="/search/math?searchtype=author&query=Lassas%2C+M">Matti Lassas</a>, 
<a href="/search/math?searchtype=author&query=Lu%2C+J">Jinpeng Lu</a>, 
<a href="/search/math?searchtype=author&query=Oksanen%2C+L">Lauri Oksanen</a>, 
<a href="/search/math?searchtype=author&query=Ylinen%2C+L">Lauri Ylinen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 42 pages, 3 figures; added numerical examples (appendix A)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Complexity (cs.CC); Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item738">[738]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05859" title="Abstract">arXiv:2306.05859</a> (replaced) [<a href="/pdf/2306.05859" title="Download PDF">pdf</a>, <a href="/format/2306.05859" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bring Your Own (Non-Robust) Algorithm to Solve Robust MDPs by Estimating  The Worst Kernel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaixin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gadot%2C+U">Uri Gadot</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+N">Navdeep Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Levy%2C+K">Kfir Levy</a>, 
<a href="/search/cs?searchtype=author&query=Mannor%2C+S">Shie Mannor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item739">[739]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07458" title="Abstract">arXiv:2306.07458</a> (replaced) [<a href="/pdf/2306.07458" title="Download PDF">pdf</a>, <a href="/format/2306.07458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accuracy-Time Tradeoffs in AI-Assisted Decision Making under Time  Pressure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Swaroop%2C+S">Siddharth Swaroop</a>, 
<a href="/search/cs?searchtype=author&query=Bu%C3%A7inca%2C+Z">Zana Bu&#xe7;inca</a>, 
<a href="/search/cs?searchtype=author&query=Gajos%2C+K+Z">Krzysztof Z. Gajos</a>, 
<a href="/search/cs?searchtype=author&query=Doshi-Velez%2C+F">Finale Doshi-Velez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item740">[740]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07521" title="Abstract">arXiv:2306.07521</a> (replaced) [<a href="/pdf/2306.07521" title="Download PDF">pdf</a>, <a href="/format/2306.07521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Bias and Noise Induced by the U.S. Census Bureau&#x27;s Privacy  Protection Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kenny%2C+C+T">Christopher T. Kenny</a>, 
<a href="/search/cs?searchtype=author&query=McCartan%2C+C">Cory McCartan</a>, 
<a href="/search/cs?searchtype=author&query=Kuriwaki%2C+S">Shiro Kuriwaki</a>, 
<a href="/search/cs?searchtype=author&query=Simko%2C+T">Tyler Simko</a>, 
<a href="/search/cs?searchtype=author&query=Imai%2C+K">Kosuke Imai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 6 figures, 2 tables, plus appendices
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item741">[741]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.07850" title="Abstract">arXiv:2306.07850</a> (replaced) [<a href="/pdf/2306.07850" title="Download PDF">pdf</a>, <a href="/format/2306.07850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact Mean Square Linear Stability Analysis for SGD
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mulayoff%2C+R">Rotem Mulayoff</a>, 
<a href="/search/cs?searchtype=author&query=Michaeli%2C+T">Tomer Michaeli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, revised
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item742">[742]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08125" title="Abstract">arXiv:2306.08125</a> (replaced) [<a href="/pdf/2306.08125" title="Download PDF">pdf</a>, <a href="/format/2306.08125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Implicit Compressibility of Overparametrized Neural Networks Trained  with Heavy-Tailed SGD
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Wan%2C+Y">Yijun Wan</a>, 
<a href="/search/stat?searchtype=author&query=Barsbey%2C+M">Melih Barsbey</a>, 
<a href="/search/stat?searchtype=author&query=Zaidi%2C+A">Abdellatif Zaidi</a>, 
<a href="/search/stat?searchtype=author&query=Simsekli%2C+U">Umut Simsekli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 31 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item743">[743]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08266" title="Abstract">arXiv:2306.08266</a> (replaced) [<a href="/pdf/2306.08266" title="Download PDF">pdf</a>, <a href="/format/2306.08266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing Robustness of Angluin&#x27;s L$^*$ Algorithm in Presence of Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+L">Lina Ye</a>, 
<a href="/search/cs?searchtype=author&query=Khmelnitsky%2C+I">Igor Khmelnitsky</a>, 
<a href="/search/cs?searchtype=author&query=Haddad%2C+S">Serge Haddad</a>, 
<a href="/search/cs?searchtype=author&query=Barbot%2C+B">Beno&#xee;t Barbot</a>, 
<a href="/search/cs?searchtype=author&query=Bollig%2C+B">Benedikt Bollig</a>, 
<a href="/search/cs?searchtype=author&query=Leucker%2C+M">Martin Leucker</a>, 
<a href="/search/cs?searchtype=author&query=Neider%2C+D">Daniel Neider</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+R">Rajarshi Roy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages. arXiv admin note: substantial text overlap with <a href="/abs/2209.10315">arXiv:2209.10315</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Formal Languages and Automata Theory (cs.FL)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item744">[744]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09152" title="Abstract">arXiv:2306.09152</a> (replaced) [<a href="/pdf/2306.09152" title="Download PDF">pdf</a>, <a href="/ps/2306.09152" title="Download PostScript">ps</a>, <a href="/format/2306.09152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Schematic Unification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cerna%2C+D+M">David M. Cerna</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted for review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item745">[745]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11336" title="Abstract">arXiv:2306.11336</a> (replaced) [<a href="/pdf/2306.11336" title="Download PDF">pdf</a>, <a href="/format/2306.11336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cooperative Multi-Agent Learning for Navigation via Structured State  Abstraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdelaziz%2C+M+K">Mohamed K. Abdelaziz</a>, 
<a href="/search/cs?searchtype=author&query=Elbamby%2C+M+S">Mohammed S. Elbamby</a>, 
<a href="/search/cs?searchtype=author&query=Samarakoon%2C+S">Sumudu Samarakoon</a>, 
<a href="/search/cs?searchtype=author&query=Bennis%2C+M">Mehdi Bennis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Double columns, 10 Pages, 12 Figures, Accepted for publication in IEEE TCOM
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item746">[746]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12465" title="Abstract">arXiv:2306.12465</a> (replaced) [<a href="/pdf/2306.12465" title="Download PDF">pdf</a>, <a href="/format/2306.12465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Deep Spiking Multi-Layer Perceptrons with Multiplication-Free  Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Boyan Li</a>, 
<a href="/search/cs?searchtype=author&query=Leng%2C+L">Luziwei Leng</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+R">Ran Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Shuaijie Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaixuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianguo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+J">Jianxing Liao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item747">[747]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13092" title="Abstract">arXiv:2306.13092</a> (replaced) [<a href="/pdf/2306.13092" title="Download PDF">pdf</a>, <a href="/format/2306.13092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale  From A New Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zeyuan Yin</a>, 
<a href="/search/cs?searchtype=author&query=Xing%2C+E">Eric Xing</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zhiqiang Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 spotlight. Code at <a href="https://github.com/VILA-Lab/SRe2L">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item748">[748]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13461" title="Abstract">arXiv:2306.13461</a> (replaced) [<a href="/pdf/2306.13461" title="Download PDF">pdf</a>, <a href="/format/2306.13461" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding quantum machine learning also requires rethinking  generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Gil-Fuster%2C+E">Elies Gil-Fuster</a>, 
<a href="/search/quant-ph?searchtype=author&query=Eisert%2C+J">Jens Eisert</a>, 
<a href="/search/quant-ph?searchtype=author&query=Bravo-Prieto%2C+C">Carlos Bravo-Prieto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14+4 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Quantum Gases (cond-mat.quant-gas); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item749">[749]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14009" title="Abstract">arXiv:2306.14009</a> (replaced) [<a href="/pdf/2306.14009" title="Download PDF">pdf</a>, <a href="/format/2306.14009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Multitask Learning on Graphs through Higher-Order Task  Affinities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongyue Li</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+H">Haotian Ju</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Aneesh Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H+R">Hongyang R. Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages. Appeared in KDD 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item750">[750]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14043" title="Abstract">arXiv:2306.14043</a> (replaced) [<a href="/pdf/2306.14043" title="Download PDF">pdf</a>, <a href="/format/2306.14043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning needs Better Randomness Standards: Randomised Smoothing  and PRNG-based attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dahiya%2C+P">Pranav Dahiya</a>, 
<a href="/search/cs?searchtype=author&query=Shumailov%2C+I">Ilia Shumailov</a>, 
<a href="/search/cs?searchtype=author&query=Anderson%2C+R">Ross Anderson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> USENIX Security 2024 (<a href="https://www.usenix.org/conference/usenixsecurity24/presentation/dahiya">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item751">[751]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.14291" title="Abstract">arXiv:2306.14291</a> (replaced) [<a href="/pdf/2306.14291" title="Download PDF">pdf</a>, <a href="/format/2306.14291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hyp-OW: Exploiting Hierarchical Structure Learning with Hyperbolic  Distance Enhances Open World Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Doan%2C+T">Thang Doan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xin Li</a>, 
<a href="/search/cs?searchtype=author&query=Behpour%2C+S">Sima Behpour</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+W">Wenbin He</a>, 
<a href="/search/cs?searchtype=author&query=Gou%2C+L">Liang Gou</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+L">Liu Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAAI 2024 || keywords: Open World Object Detection, Hyperbolic Distance, Unknown Detection, Deformable Transformers, Hierarchical Representation Learning
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item752">[752]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.00933" title="Abstract">arXiv:2307.00933</a> (replaced) [<a href="/pdf/2307.00933" title="Download PDF">pdf</a>, <a href="/format/2307.00933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Driven Information Extraction and Enrichment of Molecular Profiling  Data for Cancer Cell Lines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Smith%2C+E">Ellery Smith</a>, 
<a href="/search/cs?searchtype=author&query=Paloots%2C+R">Rahel Paloots</a>, 
<a href="/search/cs?searchtype=author&query=Giagkos%2C+D">Dimitris Giagkos</a>, 
<a href="/search/cs?searchtype=author&query=Baudis%2C+M">Michael Baudis</a>, 
<a href="/search/cs?searchtype=author&query=Stockinger%2C+K">Kurt Stockinger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computational Engineering, Finance, and Science (cs.CE); Databases (cs.DB)

</div>
</div>
</dd>
<dt><a name="item753">[753]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01107" title="Abstract">arXiv:2307.01107</a> (replaced) [<a href="/pdf/2307.01107" title="Download PDF">pdf</a>, <a href="/format/2307.01107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sampling the lattice Nambu-Goto string using Continuous Normalizing  Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-lat?searchtype=author&query=Caselle%2C+M">Michele Caselle</a>, 
<a href="/search/hep-lat?searchtype=author&query=Cellini%2C+E">Elia Cellini</a>, 
<a href="/search/hep-lat?searchtype=author&query=Nada%2C+A">Alessandro Nada</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 1+28 pages, 11 figures; v2 1+29 pages, 12 figures, added discussion on the implications of this approach for EST modeling, added results on the comparison between CNF and HMC, matches published version
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> JHEP 02 (2024) 048
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Lattice (hep-lat)</span>; Machine Learning (cs.LG); High Energy Physics - Theory (hep-th)

</div>
</div>
</dd>
<dt><a name="item754">[754]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01878" title="Abstract">arXiv:2307.01878</a> (replaced) [<a href="/pdf/2307.01878" title="Download PDF">pdf</a>, <a href="/format/2307.01878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KDSTM: Neural Semi-supervised Topic Modeling with Knowledge Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weijie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xiaoyu Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Desai%2C+J">Jay Desai</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bin Han</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+F">Fuqin Yan</a>, 
<a href="/search/cs?searchtype=author&query=Iannacci%2C+F">Francis Iannacci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 4 figures, ICLR 2022 Workshop
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICLR 2022 Workshop PML4DC
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item755">[755]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02245" title="Abstract">arXiv:2307.02245</a> (replaced) [<a href="/pdf/2307.02245" title="Download PDF">pdf</a>, <a href="/format/2307.02245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Set Learning for Accurate and Calibrated Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Muttenthaler%2C+L">Lukas Muttenthaler</a>, 
<a href="/search/cs?searchtype=author&query=Vandermeulen%2C+R+A">Robert A. Vandermeulen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qiuyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Unterthiner%2C+T">Thomas Unterthiner</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+K">Klaus-Robert M&#xfc;ller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item756">[756]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02349" title="Abstract">arXiv:2307.02349</a> (replaced) [<a href="/pdf/2307.02349" title="Download PDF">pdf</a>, <a href="/format/2307.02349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Error Approximation and Bias Correction in Dynamic Problems using a  Recurrent Neural Network/Finite Element Hybrid Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=von+Tresckow%2C+M">Moritz von Tresckow</a>, 
<a href="/search/cs?searchtype=author&query=De+Gersem%2C+H">Herbert De Gersem</a>, 
<a href="/search/cs?searchtype=author&query=Loukrezis%2C+D">Dimitrios Loukrezis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
</div>
</dd>
<dt><a name="item757">[757]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02573" title="Abstract">arXiv:2307.02573</a> (replaced) [<a href="/pdf/2307.02573" title="Download PDF">pdf</a>, <a href="/format/2307.02573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of a Programmable Quantum Annealer as a Random Number Generator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Pelofske%2C+E">Elijah Pelofske</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR); Emerging Technologies (cs.ET)

</div>
</div>
</dd>
<dt><a name="item758">[758]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03355" title="Abstract">arXiv:2307.03355</a> (replaced) [<a href="/pdf/2307.03355" title="Download PDF">pdf</a>, <a href="/ps/2307.03355" title="Download PostScript">ps</a>, <a href="/format/2307.03355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimized Path Planning for USVs under Ocean Currents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akbari%2C+B">Behzad Akbari</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Ya-Jun Pan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianye Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages and 8 figures, submitted for IEEE Transactions on Man, systems ,and Cybernetics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item759">[759]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05329" title="Abstract">arXiv:2307.05329</a> (replaced) [<a href="/pdf/2307.05329" title="Download PDF">pdf</a>, <a href="/format/2307.05329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding the Popularity of TV Series: A Network Analysis Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+M">Melody Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computation and Language (cs.CL); Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item760">[760]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07084" title="Abstract">arXiv:2307.07084</a> (replaced) [<a href="/pdf/2307.07084" title="Download PDF">pdf</a>, <a href="/format/2307.07084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safe Reinforcement Learning as Wasserstein Variational Inference: Formal  Methods for Interpretability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Q">Qiuchen Qian</a>, 
<a href="/search/cs?searchtype=author&query=Boyle%2C+D">David Boyle</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 7 figures, containing Appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item761">[761]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07572" title="Abstract">arXiv:2307.07572</a> (replaced) [<a href="/pdf/2307.07572" title="Download PDF">pdf</a>, <a href="/format/2307.07572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Harpa: High-Rate Phase Association with Travel Time Neural Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Shi%2C+C">Cheng Shi</a>, 
<a href="/search/physics?searchtype=author&query=de+Hoop%2C+M+V">Maarten V. de Hoop</a>, 
<a href="/search/physics?searchtype=author&query=Dokmani%C4%87%2C+I">Ivan Dokmani&#x107;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Geophysics (physics.geo-ph)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item762">[762]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08303" title="Abstract">arXiv:2307.08303</a> (replaced) [<a href="/pdf/2307.08303" title="Download PDF">pdf</a>, <a href="/format/2307.08303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+Z">Zhiyuan Peng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xuyang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yi Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Reivew
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item763">[763]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08930" title="Abstract">arXiv:2307.08930</a> (replaced) [<a href="/pdf/2307.08930" title="Download PDF">pdf</a>, <a href="/format/2307.08930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised Deep Graph Matching Based on Cycle Consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tourani%2C+S">Siddharth Tourani</a>, 
<a href="/search/cs?searchtype=author&query=Rother%2C+C">Carsten Rother</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+M+H">Muhammad Haris Khan</a>, 
<a href="/search/cs?searchtype=author&query=Savchynskyy%2C+B">Bogdan Savchynskyy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures, 3 papers
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item764">[764]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09253" title="Abstract">arXiv:2307.09253</a> (replaced) [<a href="/pdf/2307.09253" title="Download PDF">pdf</a>, <a href="/ps/2307.09253" title="Download PostScript">ps</a>, <a href="/format/2307.09253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Higher Catoids, Higher Quantales and their Correspondences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Calk%2C+C">Cameron Calk</a>, 
<a href="/search/cs?searchtype=author&query=Malbos%2C+P">Philippe Malbos</a>, 
<a href="/search/cs?searchtype=author&query=Pous%2C+D">Damien Pous</a>, 
<a href="/search/cs?searchtype=author&query=Struth%2C+G">Georg Struth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item765">[765]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11278" title="Abstract">arXiv:2307.11278</a> (replaced) [<a href="/pdf/2307.11278" title="Download PDF">pdf</a>, <a href="/format/2307.11278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Novel Dual-Generator Framework for Open-Domain Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdallah%2C+A">Abdelrahman Abdallah</a>, 
<a href="/search/cs?searchtype=author&query=Jatowt%2C+A">Adam Jatowt</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> J Big Data 10, 127 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item766">[766]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11280" title="Abstract">arXiv:2307.11280</a> (replaced) [<a href="/pdf/2307.11280" title="Download PDF">pdf</a>, <a href="/format/2307.11280" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Epsilon*: Privacy Metric for Machine Learning Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Negoescu%2C+D+M">Diana M. Negoescu</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+H">Humberto Gonzalez</a>, 
<a href="/search/cs?searchtype=author&query=Orjany%2C+S+E+A">Saad Eddin Al Orjany</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jilei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lut%2C+Y">Yuliia Lut</a>, 
<a href="/search/cs?searchtype=author&query=Tandra%2C+R">Rahul Tandra</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaowen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xinyi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Douglas%2C+Z">Zach Douglas</a>, 
<a href="/search/cs?searchtype=author&query=Nolkha%2C+V">Vidita Nolkha</a>, 
<a href="/search/cs?searchtype=author&query=Ahammad%2C+P">Parvez Ahammad</a>, 
<a href="/search/cs?searchtype=author&query=Samorodnitsky%2C+G">Gennady Samorodnitsky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item767">[767]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12659" title="Abstract">arXiv:2307.12659</a> (replaced) [<a href="/pdf/2307.12659" title="Download PDF">pdf</a>, <a href="/format/2307.12659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Model for Every User and Budget: Label-Free and Personalized  Mixed-Precision Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fish%2C+E">Edward Fish</a>, 
<a href="/search/cs?searchtype=author&query=Michieli%2C+U">Umberto Michieli</a>, 
<a href="/search/cs?searchtype=author&query=Ozay%2C+M">Mete Ozay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> INTERSPEECH 2023. Code is available at <a href="https://github.com/SamsungLabs/myQASR">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item768">[768]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13360" title="Abstract">arXiv:2307.13360</a> (replaced) [<a href="/pdf/2307.13360" title="Download PDF">pdf</a>, <a href="/ps/2307.13360" title="Download PostScript">ps</a>, <a href="/format/2307.13360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Axiomatic Theory for Reversible Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lanese%2C+I">Ivan Lanese</a> (Olas Team, University of Bologna/INRIA, Italy), 
<a href="/search/cs?searchtype=author&query=Phillips%2C+I">Iain Phillips</a> (Imperial College London, England), 
<a href="/search/cs?searchtype=author&query=Ulidowski%2C+I">Irek Ulidowski</a> (University of Leicester, England and AGH University of Science and Technology, Krakow, Poland)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 13 figures, 1 table; minor revisions (results unaffected), references added
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item769">[769]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.13771" title="Abstract">arXiv:2307.13771</a> (replaced) [<a href="/pdf/2307.13771" title="Download PDF">pdf</a>, <a href="/ps/2307.13771" title="Download PostScript">ps</a>, <a href="/format/2307.13771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accuracy Improvement in Differentially Private Logistic Regression: A  Pre-training Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoseinpour%2C+M">Mohammad Hoseinpour</a>, 
<a href="/search/cs?searchtype=author&query=Hoseinpour%2C+M">Milad Hoseinpour</a>, 
<a href="/search/cs?searchtype=author&query=Aghagolzadeh%2C+A">Ali Aghagolzadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item770">[770]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.00170" title="Abstract">arXiv:2308.00170</a> (replaced) [<a href="/pdf/2308.00170" title="Download PDF">pdf</a>, <a href="/format/2308.00170" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boundedness for proper conflict-free and odd colorings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Jim%C3%A9nez%2C+A">Andrea Jim&#xe9;nez</a>, 
<a href="/search/math?searchtype=author&query=Knauer%2C+K">Kolja Knauer</a>, 
<a href="/search/math?searchtype=author&query=Lintzmayer%2C+C+N">Carla Negri Lintzmayer</a>, 
<a href="/search/math?searchtype=author&query=Matamala%2C+M">Mart&#xed;n Matamala</a>, 
<a href="/search/math?searchtype=author&query=Pe%C3%B1a%2C+J+P">Juan Pablo Pe&#xf1;a</a>, 
<a href="/search/math?searchtype=author&query=Quiroz%2C+D+A">Daniel A. Quiroz</a>, 
<a href="/search/math?searchtype=author&query=Sambinelli%2C+M">Maycon Sambinelli</a>, 
<a href="/search/math?searchtype=author&query=Wakabayashi%2C+Y">Yoshiko Wakabayashi</a>, 
<a href="/search/math?searchtype=author&query=Yu%2C+W">Weiqiang Yu</a>, 
<a href="/search/math?searchtype=author&query=Zamora%2C+J">Jos&#xe9; Zamora</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 1 figure. Slight changes in introduction. References added
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item771">[771]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01531" title="Abstract">arXiv:2308.01531</a> (replaced) [<a href="/pdf/2308.01531" title="Download PDF">pdf</a>, <a href="/ps/2308.01531" title="Download PostScript">ps</a>, <a href="/format/2308.01531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing multi-user indoor sound communications with acoustic  reconfigurable metasurfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongkuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qiyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fink%2C+M">Mathias Fink</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+G">Guancong Ma</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Nature Communications (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS); Applied Physics (physics.app-ph)

</div>
</div>
</dd>
<dt><a name="item772">[772]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02261" title="Abstract">arXiv:2308.02261</a> (replaced) [<a href="/pdf/2308.02261" title="Download PDF">pdf</a>, <a href="/format/2308.02261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Proximal Gradient Method for Convex Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Malitsky%2C+Y">Yura Malitsky</a>, 
<a href="/search/math?searchtype=author&query=Mishchenko%2C+K">Konstantin Mishchenko</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item773">[773]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02397" title="Abstract">arXiv:2308.02397</a> (replaced) [<a href="/pdf/2308.02397" title="Download PDF">pdf</a>, <a href="/format/2308.02397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design Space Exploration on Efficient and Accurate Human Pose Estimation  from Sparse IMU-Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=F%C3%BCrst-Walter%2C+I">Iris F&#xfc;rst-Walter</a>, 
<a href="/search/eess?searchtype=author&query=Nappi%2C+A">Antonio Nappi</a>, 
<a href="/search/eess?searchtype=author&query=Harbaum%2C+T">Tanja Harbaum</a>, 
<a href="/search/eess?searchtype=author&query=Becker%2C+J">J&#xfc;rgen Becker</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item774">[774]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04970" title="Abstract">arXiv:2308.04970</a> (replaced) [<a href="/e-print/2308.04970" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fitting Concentric Elliptical Shapes Under General Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Al-Sharadqah%2C+A">Ali Al-Sharadqah</a>, 
<a href="/search/stat?searchtype=author&query=Piga%2C+G">Giulano Piga</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> It has many serous mistakes highlighted by reviewers so we would like to withdraw it and fixing them. This could take several months
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item775">[775]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05737" title="Abstract">arXiv:2308.05737</a> (replaced) [<a href="/pdf/2308.05737" title="Download PDF">pdf</a>, <a href="/format/2308.05737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Follow Anything: Open-set detection, tracking, and following in  real-time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maalouf%2C+A">Alaa Maalouf</a>, 
<a href="/search/cs?searchtype=author&query=Jadhav%2C+N">Ninad Jadhav</a>, 
<a href="/search/cs?searchtype=author&query=Jatavallabhula%2C+K+M">Krishna Murthy Jatavallabhula</a>, 
<a href="/search/cs?searchtype=author&query=Chahine%2C+M">Makram Chahine</a>, 
<a href="/search/cs?searchtype=author&query=Vogt%2C+D+M">Daniel M.Vogt</a>, 
<a href="/search/cs?searchtype=author&query=Wood%2C+R+J">Robert J. Wood</a>, 
<a href="/search/cs?searchtype=author&query=Torralba%2C+A">Antonio Torralba</a>, 
<a href="/search/cs?searchtype=author&query=Rus%2C+D">Daniela Rus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project webpage: <a href="https://github.com/alaamaalouf/FollowAnything">this https URL</a> Explainer video: <a href="https://www.youtube.com/watch?v=6Mgt3EPytrw">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item776">[776]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.06394" title="Abstract">arXiv:2308.06394</a> (replaced) [<a href="/pdf/2308.06394" title="Download PDF">pdf</a>, <a href="/format/2308.06394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting and Preventing Hallucinations in Large Vision Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gunjal%2C+A">Anisha Gunjal</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+J">Jihan Yin</a>, 
<a href="/search/cs?searchtype=author&query=Bas%2C+E">Erhan Bas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item777">[777]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07130" title="Abstract">arXiv:2308.07130</a> (replaced) [<a href="/pdf/2308.07130" title="Download PDF">pdf</a>, <a href="/ps/2308.07130" title="Download PostScript">ps</a>, <a href="/format/2308.07130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forward completeness does not imply bounded reachability sets and global  asymptotic stability is not necessarily uniform for time-delay systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mancilla-Aguilar%2C+J+L">Jose L. Mancilla-Aguilar</a>, 
<a href="/search/eess?searchtype=author&query=Haimovich%2C+H">Hernan Haimovich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to Automatica. Print ISSN: 0005-1098, Online ISSN: 1873-2836
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item778">[778]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08925" title="Abstract">arXiv:2308.08925</a> (replaced) [<a href="/pdf/2308.08925" title="Download PDF">pdf</a>, <a href="/format/2308.08925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A White-Box False Positive Adversarial Attack Method on Contrastive Loss  Based Offline Handwritten Signature Verification Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhongliang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weiye Li</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yifei Qian</a>, 
<a href="/search/cs?searchtype=author&query=Arandjelovi%C4%87%2C+O">Ognjen Arandjelovi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+L">Lei Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 3 figures, 2 tables, accepted by the Proceedings of the 27th International Conference on Artificial Intelligence and Statistics (AISTATS) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item779">[779]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.09895" title="Abstract">arXiv:2308.09895</a> (replaced) [<a href="/pdf/2308.09895" title="Download PDF">pdf</a>, <a href="/format/2308.09895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Transfer from High-Resource to Low-Resource Programming  Languages for Code LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cassano%2C+F">Federico Cassano</a>, 
<a href="/search/cs?searchtype=author&query=Gouwar%2C+J">John Gouwar</a>, 
<a href="/search/cs?searchtype=author&query=Lucchetti%2C+F">Francesca Lucchetti</a>, 
<a href="/search/cs?searchtype=author&query=Schlesinger%2C+C">Claire Schlesinger</a>, 
<a href="/search/cs?searchtype=author&query=Freeman%2C+A">Anders Freeman</a>, 
<a href="/search/cs?searchtype=author&query=Anderson%2C+C+J">Carolyn Jane Anderson</a>, 
<a href="/search/cs?searchtype=author&query=Feldman%2C+M+Q">Molly Q Feldman</a>, 
<a href="/search/cs?searchtype=author&query=Greenberg%2C+M">Michael Greenberg</a>, 
<a href="/search/cs?searchtype=author&query=Jangda%2C+A">Abhinav Jangda</a>, 
<a href="/search/cs?searchtype=author&query=Guha%2C+A">Arjun Guha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item780">[780]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10214" title="Abstract">arXiv:2308.10214</a> (replaced) [<a href="/pdf/2308.10214" title="Download PDF">pdf</a>, <a href="/format/2308.10214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computational complexity of counting coincidences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chan%2C+S+H">Swee Hong Chan</a>, 
<a href="/search/math?searchtype=author&query=Pak%2C+I">Igor Pak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 6 figures. New references added. To appear in Theoret. Comput. Sci
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Complexity (cs.CC); Computational Geometry (cs.CG); Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item781">[781]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10743" title="Abstract">arXiv:2308.10743</a> (replaced) [<a href="/pdf/2308.10743" title="Download PDF">pdf</a>, <a href="/format/2308.10743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Adversarial Attacks: The Similar Target Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziruo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zikai Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huanran Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item782">[782]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11146" title="Abstract">arXiv:2308.11146</a> (replaced) [<a href="/pdf/2308.11146" title="Download PDF">pdf</a>, <a href="/format/2308.11146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding Small Complete Subgraphs Efficiently
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dumitrescu%2C+A">Adrian Dumitrescu</a>, 
<a href="/search/cs?searchtype=author&query=Lingas%2C+A">Andrzej Lingas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 1 figure. arXiv admin note: substantial text overlap with <a href="/abs/2105.01265">arXiv:2105.01265</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item783">[783]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14081" title="Abstract">arXiv:2308.14081</a> (replaced) [<a href="/e-print/2308.14081" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> U-SEANNet: A Simple, Efficient and Applied U-Shaped Network for  Diagnosis of Nasal Diseases on Nasal Endoscopic Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yue%2C+Y">Yubiao Yue</a>, 
<a href="/search/eess?searchtype=author&query=Xue%2C+J">Jun Xue</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+C">Chao Wang</a>, 
<a href="/search/eess?searchtype=author&query=Liang%2C+H">Haihua Liang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Z">Zhenzhang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> There are some descriptive errors in the manuscript
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item784">[784]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15515" title="Abstract">arXiv:2308.15515</a> (replaced) [<a href="/pdf/2308.15515" title="Download PDF">pdf</a>, <a href="/format/2308.15515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Algorithms for 2-Packing Sets on Arbitrary Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Borowitz%2C+J">Jannick Borowitz</a>, 
<a href="/search/cs?searchtype=author&query=Gro%C3%9Fmann%2C+E">Ernestine Gro&#xdf;mann</a>, 
<a href="/search/cs?searchtype=author&query=Schulz%2C+C">Christian Schulz</a>, 
<a href="/search/cs?searchtype=author&query=Schweisgut%2C+D">Dominik Schweisgut</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item785">[785]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16546" title="Abstract">arXiv:2308.16546</a> (replaced) [<a href="/pdf/2308.16546" title="Download PDF">pdf</a>, <a href="/format/2308.16546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inference of dynamic hypergraph representations in temporal interaction  data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kirkley%2C+A">Alec Kirkley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item786">[786]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16848" title="Abstract">arXiv:2308.16848</a> (replaced) [<a href="/pdf/2308.16848" title="Download PDF">pdf</a>, <a href="/format/2308.16848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Natural Quantum Monte Carlo Computation of Excited States
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Pfau%2C+D">David Pfau</a>, 
<a href="/search/physics?searchtype=author&query=Axelrod%2C+S">Simon Axelrod</a>, 
<a href="/search/physics?searchtype=author&query=Sutterud%2C+H">Halvard Sutterud</a>, 
<a href="/search/physics?searchtype=author&query=von+Glehn%2C+I">Ingrid von Glehn</a>, 
<a href="/search/physics?searchtype=author&query=Spencer%2C+J+S">James S. Spencer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Added funding acknowledgment
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Machine Learning (cs.LG); Chemical Physics (physics.chem-ph); Quantum Physics (quant-ph)

</div>
</div>
</dd>
<dt><a name="item787">[787]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00438" title="Abstract">arXiv:2309.00438</a> (replaced) [<a href="/pdf/2309.00438" title="Download PDF">pdf</a>, <a href="/format/2309.00438" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A shape-based heuristic for the detection of urban block artifacts in  street networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fleischmann%2C+M">Martin Fleischmann</a>, 
<a href="/search/cs?searchtype=author&query=Vybornova%2C+A">Anastassia Vybornova</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted version. Zenodo: <a href="https://doi.org/10.5281/zenodo.8300730">this https URL</a> ; GitHub: <a href="https://github.com/martinfleis/urban-block-artifacts">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item788">[788]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00612" title="Abstract">arXiv:2309.00612</a> (replaced) [<a href="/pdf/2309.00612" title="Download PDF">pdf</a>, <a href="/format/2309.00612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian deep learning for cosmic volumes with modified gravity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Garc%C3%ADa-Farieta%2C+J+E">Jorge Enrique Garc&#xed;a-Farieta</a>, 
<a href="/search/astro-ph?searchtype=author&query=Hort%C3%BAa%2C+H+J">H&#xe9;ctor J Hort&#xfa;a</a>, 
<a href="/search/astro-ph?searchtype=author&query=Kitaura%2C+F">Francisco-Shu Kitaura</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures and 7 tables. Minor changes to match the accepted version in A&amp;A
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cosmology and Nongalactic Astrophysics (astro-ph.CO)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item789">[789]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01107" title="Abstract">arXiv:2309.01107</a> (replaced) [<a href="/pdf/2309.01107" title="Download PDF">pdf</a>, <a href="/format/2309.01107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving Non-Rectangular Reward-Robust MDPs via Frequency Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gadot%2C+U">Uri Gadot</a>, 
<a href="/search/cs?searchtype=author&query=Derman%2C+E">Esther Derman</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+N">Navdeep Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Elfatihi%2C+M+M">Maxence Mohamed Elfatihi</a>, 
<a href="/search/cs?searchtype=author&query=Levy%2C+K">Kfir Levy</a>, 
<a href="/search/cs?searchtype=author&query=Mannor%2C+S">Shie Mannor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted in AAAI2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item790">[790]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01108" title="Abstract">arXiv:2309.01108</a> (replaced) [<a href="/pdf/2309.01108" title="Download PDF">pdf</a>, <a href="/format/2309.01108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Acoustic-to-articulatory inversion for dysarthric speech: Are  pre-trained self-supervised representations favorable?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Maharana%2C+S+K">Sarthak Kumar Maharana</a>, 
<a href="/search/eess?searchtype=author&query=Adidam%2C+K+K">Krishna Kamal Adidam</a>, 
<a href="/search/eess?searchtype=author&query=Nandi%2C+S">Shoumik Nandi</a>, 
<a href="/search/eess?searchtype=author&query=Srivastava%2C+A">Ajitesh Srivastava</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE ICASSP Workshops 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item791">[791]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.01753" title="Abstract">arXiv:2309.01753</a> (replaced) [<a href="/pdf/2309.01753" title="Download PDF">pdf</a>, <a href="/format/2309.01753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Penalty Methods for Nonconvex Bilevel Optimization and First-Order  Stochastic Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kwon%2C+J">Jeongyeol Kwon</a>, 
<a href="/search/math?searchtype=author&query=Kwon%2C+D">Dohyun Kwon</a>, 
<a href="/search/math?searchtype=author&query=Wright%2C+S">Stephen Wright</a>, 
<a href="/search/math?searchtype=author&query=Nowak%2C+R">Robert Nowak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item792">[792]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02440" title="Abstract">arXiv:2309.02440</a> (replaced) [<a href="/pdf/2309.02440" title="Download PDF">pdf</a>, <a href="/format/2309.02440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct inversion of the Longitudinal Ray Transform for 2D residual  elastic strain fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wensrich%2C+C+M">C.M. Wensrich</a>, 
<a href="/search/math?searchtype=author&query=Holman%2C+S">S. Holman</a>, 
<a href="/search/math?searchtype=author&query=Courdurier%2C+M">M. Courdurier</a>, 
<a href="/search/math?searchtype=author&query=Lionheart%2C+W+R+B">W.R.B. Lionheart</a>, 
<a href="/search/math?searchtype=author&query=Polyalova%2C+A">A. Polyalova</a>, 
<a href="/search/math?searchtype=author&query=Svetov%2C+I">I. Svetov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item793">[793]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02705" title="Abstract">arXiv:2309.02705</a> (replaced) [<a href="/pdf/2309.02705" title="Download PDF">pdf</a>, <a href="/format/2309.02705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Certifying LLM Safety against Adversarial Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Aounon Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+C">Chirag Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Srinivas%2C+S">Suraj Srinivas</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+A+J">Aaron Jiaxun Li</a>, 
<a href="/search/cs?searchtype=author&query=Feizi%2C+S">Soheil Feizi</a>, 
<a href="/search/cs?searchtype=author&query=Lakkaraju%2C+H">Himabindu Lakkaraju</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item794">[794]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03160" title="Abstract">arXiv:2309.03160</a> (replaced) [<a href="/pdf/2309.03160" title="Download PDF">pdf</a>, <a href="/format/2309.03160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ResFields: Residual Neural Fields for Spatiotemporal Signals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mihajlovic%2C+M">Marko Mihajlovic</a>, 
<a href="/search/cs?searchtype=author&query=Prokudin%2C+S">Sergey Prokudin</a>, 
<a href="/search/cs?searchtype=author&query=Pollefeys%2C+M">Marc Pollefeys</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Siyu Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> [ICLR 2024 Spotlight] Project and code at: <a href="https://markomih.github.io/ResFields/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item795">[795]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03202" title="Abstract">arXiv:2309.03202</a> (replaced) [<a href="/e-print/2309.03202" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluation of Reinforcement Learning Techniques for Trading on a Diverse  Portfolio
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Khare%2C+I+S">Ishan S. Khare</a>, 
<a href="/search/q-fin?searchtype=author&query=Martheswaran%2C+T+K">Tarun K. Martheswaran</a>, 
<a href="/search/q-fin?searchtype=author&query=Dassanaike-Perera%2C+A">Akshana Dassanaike-Perera</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Course project not to be posted online
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Trading and Market Microstructure (q-fin.TR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item796">[796]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.03320" title="Abstract">arXiv:2309.03320</a> (replaced) [<a href="/pdf/2309.03320" title="Download PDF">pdf</a>, <a href="/format/2309.03320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoNeS: Conditional neural fields with shift modulation for  multi-sequence MRI translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+Y">Yunjie Chen</a>, 
<a href="/search/eess?searchtype=author&query=Staring%2C+M">Marius Staring</a>, 
<a href="/search/eess?searchtype=author&query=Neve%2C+O+M">Olaf M. Neve</a>, 
<a href="/search/eess?searchtype=author&query=Romeijn%2C+S+R">Stephan R. Romeijn</a>, 
<a href="/search/eess?searchtype=author&query=Hensen%2C+E+F">Erik F. Hensen</a>, 
<a href="/search/eess?searchtype=author&query=Verbist%2C+B+M">Berit M. Verbist</a>, 
<a href="/search/eess?searchtype=author&query=Wolterink%2C+J+M">Jelmer M. Wolterink</a>, 
<a href="/search/eess?searchtype=author&query=Tao%2C+Q">Qian Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at the Journal of Machine Learning for Biomedical Imaging (MELBA) <a href="https://melba-journal.org/2024:004">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Machine.Learning.for.Biomedical.Imaging. 2 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item797">[797]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04344" title="Abstract">arXiv:2309.04344</a> (replaced) [<a href="/pdf/2309.04344" title="Download PDF">pdf</a>, <a href="/format/2309.04344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot Robustification of Zero-Shot Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adila%2C+D">Dyah Adila</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+C">Changho Shin</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+L">Linrong Cai</a>, 
<a href="/search/cs?searchtype=author&query=Sala%2C+F">Frederic Sala</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Learning Representations (ICLR), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item798">[798]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07708" title="Abstract">arXiv:2309.07708</a> (replaced) [<a href="/pdf/2309.07708" title="Download PDF">pdf</a>, <a href="/format/2309.07708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Market-GAN: Adding Control to Financial Market Data Generation with  Semantic Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+H">Haochong Xia</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Shuo Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinrun Wang</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+B">Bo An</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to the 38th Annual AAAI Conference on Artificial Intelligence (AAAI24), Vancouver, British Columbia, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Trading and Market Microstructure (q-fin.TR)

</div>
</div>
</dd>
<dt><a name="item799">[799]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07891" title="Abstract">arXiv:2309.07891</a> (replaced) [<a href="/pdf/2309.07891" title="Download PDF">pdf</a>, <a href="/format/2309.07891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HandNeRF: Learning to Reconstruct Hand-Object Interaction Scene from a  Single RGB Image
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+H">Hongsuk Choi</a>, 
<a href="/search/cs?searchtype=author&query=Chavan-Dafle%2C+N">Nikhil Chavan-Dafle</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jiacheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Isler%2C+V">Volkan Isler</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+H">Hyunsoo Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In ICRA 2024; 13 pages including the supplementary material, 8 tables, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item800">[800]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08325" title="Abstract">arXiv:2309.08325</a> (replaced) [<a href="/pdf/2309.08325" title="Download PDF">pdf</a>, <a href="/format/2309.08325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributional Inclusion Hypothesis and Quantifications: Probing for  Hypernymy in Functional Distributional Semantics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lo%2C+C+H">Chun Hei Lo</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+W">Wai Lam</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hong Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Emerson%2C+G">Guy Emerson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item801">[801]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08630" title="Abstract">arXiv:2309.08630</a> (replaced) [<a href="/pdf/2309.08630" title="Download PDF">pdf</a>, <a href="/format/2309.08630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PCN: A Deep Learning Approach to Jet Tagging Utilizing Novel Graph  Construction Methods and Chebyshev Graph Convolutions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-ph?searchtype=author&query=Semlani%2C+Y">Yash Semlani</a>, 
<a href="/search/hep-ph?searchtype=author&query=Relan%2C+M">Mihir Relan</a>, 
<a href="/search/hep-ph?searchtype=author&query=Ramesh%2C+K">Krithik Ramesh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 2 figures, and 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Phenomenology (hep-ph)</span>; Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex)

</div>
</div>
</dd>
<dt><a name="item802">[802]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09214" title="Abstract">arXiv:2309.09214</a> (replaced) [<a href="/pdf/2309.09214" title="Download PDF">pdf</a>, <a href="/format/2309.09214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Logic of Awareness in Agent&#x27;s Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kubono%2C+Y">Yudai Kubono</a>, 
<a href="/search/cs?searchtype=author&query=Racharak%2C+T">Teeradaj Racharak</a>, 
<a href="/search/cs?searchtype=author&query=Tojo%2C+S">Satoshi Tojo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A version reflecting the corrections of errors. The results are the same as originally stated. The erratum to <a href="/abs/2309.09214">arXiv:2309.09214v2</a> is attached at the end
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 15th International Conference on Agents and
  Artificial Intelligence - Volume 1: ICAART, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
</div>
</dd>
<dt><a name="item803">[803]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09800" title="Abstract">arXiv:2309.09800</a> (replaced) [<a href="/pdf/2309.09800" title="Download PDF">pdf</a>, <a href="/format/2309.09800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CrossLingR: A Comprehensive Multilingual Receipt Dataset for  Cross-Language Information Extraction and Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abdallah%2C+A">Abdelrahman Abdallah</a>, 
<a href="/search/cs?searchtype=author&query=Abdalla%2C+M">Mahmoud Abdalla</a>, 
<a href="/search/cs?searchtype=author&query=Elkasaby%2C+M">Mohamed Elkasaby</a>, 
<a href="/search/cs?searchtype=author&query=Elbendary%2C+Y">Yasser Elbendary</a>, 
<a href="/search/cs?searchtype=author&query=Jatowt%2C+A">Adam Jatowt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item804">[804]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10489" title="Abstract">arXiv:2309.10489</a> (replaced) [<a href="/pdf/2309.10489" title="Download PDF">pdf</a>, <a href="/format/2309.10489" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Parameterized Complexity of Learning Monadic Second-Order Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Bergerem%2C+S">Steffen van Bergerem</a>, 
<a href="/search/cs?searchtype=author&query=Grohe%2C+M">Martin Grohe</a>, 
<a href="/search/cs?searchtype=author&query=Runde%2C+N">Nina Runde</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item805">[805]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13713" title="Abstract">arXiv:2309.13713</a> (replaced) [<a href="/pdf/2309.13713" title="Download PDF">pdf</a>, <a href="/format/2309.13713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beginner&#x27;s guide to Aggregation-Diffusion Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=G%C3%B3mez-Castro%2C+D">David G&#xf3;mez-Castro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item806">[806]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14441" title="Abstract">arXiv:2309.14441</a> (replaced) [<a href="/pdf/2309.14441" title="Download PDF">pdf</a>, <a href="/format/2309.14441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Tree Isomorphism: An Algorithmic Bric-&#xe0;-Brac
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ingels%2C+F">Florian Ingels</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item807">[807]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15048" title="Abstract">arXiv:2309.15048</a> (replaced) [<a href="/pdf/2309.15048" title="Download PDF">pdf</a>, <a href="/format/2309.15048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Class Incremental Learning via Likelihood Ratio Based Task Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Haowei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y">Yijia Shao</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+W">Weinan Qian</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+N">Ningxin Pan</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yiduo Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bing Liu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item808">[808]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16002" title="Abstract">arXiv:2309.16002</a> (replaced) [<a href="/pdf/2309.16002" title="Download PDF">pdf</a>, <a href="/format/2309.16002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Blockwise Random Pivoting: Fast and Accurate Adaptive  Interpolative Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dong%2C+Y">Yijun Dong</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+C">Chao Chen</a>, 
<a href="/search/math?searchtype=author&query=Martinsson%2C+P">Per-Gunnar Martinsson</a>, 
<a href="/search/math?searchtype=author&query=Pearce%2C+K">Katherine Pearce</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item809">[809]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16457" title="Abstract">arXiv:2309.16457</a> (replaced) [<a href="/pdf/2309.16457" title="Download PDF">pdf</a>, <a href="/format/2309.16457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Sleep Decoder: Aligning awake and sleep neural representation  across subjects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhongtao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Hui Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haiteng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jianyang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Lin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yunzhe Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item810">[810]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17348" title="Abstract">arXiv:2309.17348</a> (replaced) [<a href="/pdf/2309.17348" title="Download PDF">pdf</a>, <a href="/format/2309.17348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intrinsic Biologically Plausible Adversarial Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Farinha%2C+M+T">Matilde Tristany Farinha</a>, 
<a href="/search/cs?searchtype=author&query=Ortner%2C+T">Thomas Ortner</a>, 
<a href="/search/cs?searchtype=author&query=Dellaferrera%2C+G">Giorgia Dellaferrera</a>, 
<a href="/search/cs?searchtype=author&query=Grewe%2C+B">Benjamin Grewe</a>, 
<a href="/search/cs?searchtype=author&query=Pantazi%2C+A">Angeliki Pantazi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item811">[811]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00290" title="Abstract">arXiv:2310.00290</a> (replaced) [<a href="/pdf/2310.00290" title="Download PDF">pdf</a>, <a href="/ps/2310.00290" title="Download PostScript">ps</a>, <a href="/format/2310.00290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universality of almost periodicity in bounded discrete time series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoneda%2C+T">Tsuyoshi Yoneda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Analysis of PDEs (math.AP); Dynamical Systems (math.DS); Functional Analysis (math.FA)

</div>
</div>
</dd>
<dt><a name="item812">[812]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01045" title="Abstract">arXiv:2310.01045</a> (replaced) [<a href="/pdf/2310.01045" title="Download PDF">pdf</a>, <a href="/format/2310.01045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tool-Augmented Reward Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+Y">Yekun Chai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuohuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yu Sun</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+H">Hao Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hua Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 Spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item813">[813]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01845" title="Abstract">arXiv:2310.01845</a> (replaced) [<a href="/pdf/2310.01845" title="Download PDF">pdf</a>, <a href="/format/2310.01845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot Refinement of Buildings&#x27; Segmentation Models using SAM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mayladan%2C+A">Ali Mayladan</a>, 
<a href="/search/cs?searchtype=author&query=Nasrallah%2C+H">Hasan Nasrallah</a>, 
<a href="/search/cs?searchtype=author&query=Moughnieh%2C+H">Hasan Moughnieh</a>, 
<a href="/search/cs?searchtype=author&query=Shukor%2C+M">Mustafa Shukor</a>, 
<a href="/search/cs?searchtype=author&query=Ghandour%2C+A+J">Ali J. Ghandour</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item814">[814]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02975" title="Abstract">arXiv:2310.02975</a> (replaced) [<a href="/pdf/2310.02975" title="Download PDF">pdf</a>, <a href="/ps/2310.02975" title="Download PostScript">ps</a>, <a href="/format/2310.02975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $(&#x3b5;, u)$-Adaptive Regret Minimization in Heavy-Tailed Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Genalti%2C+G">Gianmarco Genalti</a>, 
<a href="/search/cs?searchtype=author&query=Marsigli%2C+L">Lupo Marsigli</a>, 
<a href="/search/cs?searchtype=author&query=Gatti%2C+N">Nicola Gatti</a>, 
<a href="/search/cs?searchtype=author&query=Metelli%2C+A+M">Alberto Maria Metelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item815">[815]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03946" title="Abstract">arXiv:2310.03946</a> (replaced) [<a href="/pdf/2310.03946" title="Download PDF">pdf</a>, <a href="/format/2310.03946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved prediction of ligand-protein binding affinities by  meta-modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Ho-Joon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Emani%2C+P+S">Prashant S. Emani</a>, 
<a href="/search/cs?searchtype=author&query=Gerstein%2C+M+B">Mark B. Gerstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 62 pages, 4 main tables, 6 main figures, 7 supplementary figures, and supporting information. For 9 supplementary tables and code, see <a href="https://github.com/Lee1701/Lee2023a">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item816">[816]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04017" title="Abstract">arXiv:2310.04017</a> (replaced) [<a href="/pdf/2310.04017" title="Download PDF">pdf</a>, <a href="/format/2310.04017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PGraphDTA: Improving Drug Target Interaction Prediction using Protein  Language Models and Contact Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bal%2C+R">Rakesh Bal</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yijia Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AI for Science Workshop, NeurIPS 2023. 11 pages, 5 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item817">[817]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04286" title="Abstract">arXiv:2310.04286</a> (replaced) [<a href="/pdf/2310.04286" title="Download PDF">pdf</a>, <a href="/format/2310.04286" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-constrained symbolic model discovery for polyconvex  incompressible hyperelastic materials
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bahmani%2C+B">Bahador Bahmani</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">WaiChing Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>; Soft Condensed Matter (cond-mat.soft)

</div>
</div>
</dd>
<dt><a name="item818">[818]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04470" title="Abstract">arXiv:2310.04470</a> (replaced) [<a href="/pdf/2310.04470" title="Download PDF">pdf</a>, <a href="/format/2310.04470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Multi-Marginal Optimal Transport for Network Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Zhichen Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+B">Boxin Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Si Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yinglong Xia</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhining Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+H">Hanghang Tong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item819">[819]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04910" title="Abstract">arXiv:2310.04910</a> (replaced) [<a href="/pdf/2310.04910" title="Download PDF">pdf</a>, <a href="/format/2310.04910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faithful Knowledge Graph Explanations for Commonsense Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhai%2C+W">Weihe Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Zubiaga%2C+A">Arkaitz Zubiaga</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bingquan Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item820">[820]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05116" title="Abstract">arXiv:2310.05116</a> (replaced) [<a href="/pdf/2310.05116" title="Download PDF">pdf</a>, <a href="/format/2310.05116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Utilizing Contextual Clues and Role Correlations for Enhancing  Document-level Event Argument Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wanlong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+D">Dingyi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Li Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Malu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Shaohuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+W">Weishan Kong</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yichen Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hongyang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenyu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item821">[821]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05842" title="Abstract">arXiv:2310.05842</a> (replaced) [<a href="/pdf/2310.05842" title="Download PDF">pdf</a>, <a href="/format/2310.05842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Angular Synchronization via Directed Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yixuan He</a>, 
<a href="/search/cs?searchtype=author&query=Reinert%2C+G">Gesine Reinert</a>, 
<a href="/search/cs?searchtype=author&query=Wipf%2C+D">David Wipf</a>, 
<a href="/search/cs?searchtype=author&query=Cucuringu%2C+M">Mihai Cucuringu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages for main text, ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item822">[822]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06056" title="Abstract">arXiv:2310.06056</a> (replaced) [<a href="/pdf/2310.06056" title="Download PDF">pdf</a>, <a href="/ps/2310.06056" title="Download PostScript">ps</a>, <a href="/format/2310.06056" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Automated Tool to Detect Suicidal Susceptibility from Social Media  Posts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dus%2C+Y">Yasin Dus</a>, 
<a href="/search/cs?searchtype=author&query=Nefedov%2C+G">Georgiy Nefedov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 13 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item823">[823]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06085" title="Abstract">arXiv:2310.06085</a> (replaced) [<a href="/pdf/2310.06085" title="Download PDF">pdf</a>, <a href="/format/2310.06085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantile-based Maximum Likelihood Training for Outlier Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taghikhah%2C+M">Masoud Taghikhah</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+N">Nishant Kumar</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0egvi%C4%87%2C+S">Sini&#x161;a &#x160;egvi&#x107;</a>, 
<a href="/search/cs?searchtype=author&query=Eslami%2C+A">Abouzar Eslami</a>, 
<a href="/search/cs?searchtype=author&query=Gumhold%2C+S">Stefan Gumhold</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Camera Ready Version AAAI 2024. Code available at <a href="https://github.com/taghikhah/QuantOD">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item824">[824]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06121" title="Abstract">arXiv:2310.06121</a> (replaced) [<a href="/pdf/2310.06121" title="Download PDF">pdf</a>, <a href="/ps/2310.06121" title="Download PostScript">ps</a>, <a href="/format/2310.06121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Innermost to Full Almost-Sure Termination of Probabilistic Term  Rewriting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kassing%2C+J">Jan-Christoph Kassing</a>, 
<a href="/search/cs?searchtype=author&query=Frohn%2C+F">Florian Frohn</a>, 
<a href="/search/cs?searchtype=author&query=Giesl%2C+J">J&#xfc;rgen Giesl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item825">[825]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07779" title="Abstract">arXiv:2310.07779</a> (replaced) [<a href="/pdf/2310.07779" title="Download PDF">pdf</a>, <a href="/format/2310.07779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social Approval and Network Homophily as Motivators of Online Toxicity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Julie Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Luceri%2C+L">Luca Luceri</a>, 
<a href="/search/cs?searchtype=author&query=Walther%2C+J+B">Joseph B. Walther</a>, 
<a href="/search/cs?searchtype=author&query=Ferrara%2C+E">Emilio Ferrara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 9 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
</div>
</dd>
<dt><a name="item826">[826]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07879" title="Abstract">arXiv:2310.07879</a> (replaced) [<a href="/pdf/2310.07879" title="Download PDF">pdf</a>, <a href="/format/2310.07879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deepfakes, Phrenology, Surveillance, and More! A Taxonomy of AI Privacy  Risks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hao-Ping Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yu-Ju Yang</a>, 
<a href="/search/cs?searchtype=author&query=von+Davier%2C+T+S">Thomas Serban von Davier</a>, 
<a href="/search/cs?searchtype=author&query=Forlizzi%2C+J">Jodi Forlizzi</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Sauvik Das</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item827">[827]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08091" title="Abstract">arXiv:2310.08091</a> (replaced) [<a href="/pdf/2310.08091" title="Download PDF">pdf</a>, <a href="/format/2310.08091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discerning Temporal Difference Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jianfei Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item828">[828]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08352" title="Abstract">arXiv:2310.08352</a> (replaced) [<a href="/pdf/2310.08352" title="Download PDF">pdf</a>, <a href="/format/2310.08352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral deferred correction methods for second-order problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Akramov%2C+I">Ikrom Akramov</a>, 
<a href="/search/math?searchtype=author&query=G%C3%B6tschel%2C+S">Sebastian G&#xf6;tschel</a>, 
<a href="/search/math?searchtype=author&query=Minion%2C+M">Michael Minion</a>, 
<a href="/search/math?searchtype=author&query=Ruprecht%2C+D">Daniel Ruprecht</a>, 
<a href="/search/math?searchtype=author&query=Speck%2C+R">Robert Speck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item829">[829]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08560" title="Abstract">arXiv:2310.08560</a> (replaced) [<a href="/pdf/2310.08560" title="Download PDF">pdf</a>, <a href="/format/2310.08560" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MemGPT: Towards LLMs as Operating Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Packer%2C+C">Charles Packer</a>, 
<a href="/search/cs?searchtype=author&query=Wooders%2C+S">Sarah Wooders</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+K">Kevin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+V">Vivian Fang</a>, 
<a href="/search/cs?searchtype=author&query=Patil%2C+S+G">Shishir G. Patil</a>, 
<a href="/search/cs?searchtype=author&query=Stoica%2C+I">Ion Stoica</a>, 
<a href="/search/cs?searchtype=author&query=Gonzalez%2C+J+E">Joseph E. Gonzalez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and data available at <a href="https://research.memgpt.ai">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item830">[830]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08814" title="Abstract">arXiv:2310.08814</a> (replaced) [<a href="/pdf/2310.08814" title="Download PDF">pdf</a>, <a href="/format/2310.08814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PT-symmetry enabled stable modes in multi-core fiber
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Gratcheva%2C+T">Tamara Gratcheva</a>, 
<a href="/search/physics?searchtype=author&query=Joglekar%2C+Y+N">Yogesh N. Joglekar</a>, 
<a href="/search/physics?searchtype=author&query=Gopalakrishnan%2C+J">Jay Gopalakrishnan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optics (physics.optics)</span>; Numerical Analysis (math.NA); Spectral Theory (math.SP)

</div>
</div>
</dd>
<dt><a name="item831">[831]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09298" title="Abstract">arXiv:2310.09298</a> (replaced) [<a href="/pdf/2310.09298" title="Download PDF">pdf</a>, <a href="/format/2310.09298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ByteStack-ID: Integrated Stacked Model Leveraging Payload Byte Frequency  for Grayscale Image-based Network Intrusion Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+I">Irfan Khan</a>, 
<a href="/search/cs?searchtype=author&query=Farrukh%2C+Y+A">Yasir Ali Farrukh</a>, 
<a href="/search/cs?searchtype=author&query=Wali%2C+S">Syed Wali</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item832">[832]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09401" title="Abstract">arXiv:2310.09401</a> (replaced) [<a href="/pdf/2310.09401" title="Download PDF">pdf</a>, <a href="/format/2310.09401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CROWN: A Novel Approach to Comprehending Users&#x27; Preferences for Accurate  Personalized News Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ko%2C+Y">Yunyong Ko</a>, 
<a href="/search/cs?searchtype=author&query=Ryu%2C+S">Seongeun Ryu</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sang-Wook Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 6 figures, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item833">[833]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09656" title="Abstract">arXiv:2310.09656</a> (replaced) [<a href="/pdf/2310.09656" title="Download PDF">pdf</a>, <a href="/format/2310.09656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent  Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hengrui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiani Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+B">Balasubramaniam Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zhengyuan Shen</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+X">Xiao Qin</a>, 
<a href="/search/cs?searchtype=author&query=Faloutsos%2C+C">Christos Faloutsos</a>, 
<a href="/search/cs?searchtype=author&query=Rangwala%2C+H">Huzefa Rangwala</a>, 
<a href="/search/cs?searchtype=author&query=Karypis%2C+G">George Karypis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR 2024 (Oral Presentation)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item834">[834]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09669" title="Abstract">arXiv:2310.09669</a> (replaced) [<a href="/pdf/2310.09669" title="Download PDF">pdf</a>, <a href="/format/2310.09669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Framework For Automated Dissection Along Tissue Boundary
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oh%2C+K">Ki-Hwan Oh</a>, 
<a href="/search/cs?searchtype=author&query=Borgioli%2C+L">Leonardo Borgioli</a>, 
<a href="/search/cs?searchtype=author&query=Zefran%2C+M">Milos Zefran</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liaohai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Giulianotti%2C+P+C">Pier Cristoforo Giulianotti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 7 figures, 7 tables, submitted to 2024 International Conference on Biomedical Robotics and Biomechatronics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item835">[835]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09877" title="Abstract">arXiv:2310.09877</a> (replaced) [<a href="/pdf/2310.09877" title="Download PDF">pdf</a>, <a href="/format/2310.09877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical inference using machine learning and classical techniques  based on accumulated local effects (ALE)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Okoli%2C+C">Chitu Okoli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 1. P-values have been added to the analysis. 2. Neural network demonstration replaced with random forest
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item836">[836]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10531" title="Abstract">arXiv:2310.10531</a> (replaced) [<a href="/pdf/2310.10531" title="Download PDF">pdf</a>, <a href="/format/2310.10531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning optimal integration of spatial and temporal information in  noisy chemotaxis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alonso%2C+A">Albert Alonso</a>, 
<a href="/search/cs?searchtype=author&query=Kirkegaard%2C+J+B">Julius B. Kirkegaard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG); Biological Physics (physics.bio-ph)

</div>
</div>
</dd>
<dt><a name="item837">[837]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12609" title="Abstract">arXiv:2310.12609</a> (replaced) [<a href="/pdf/2310.12609" title="Download PDF">pdf</a>, <a href="/ps/2310.12609" title="Download PostScript">ps</a>, <a href="/format/2310.12609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising Heat-inspired Diffusion with Insulators for Collision Free  Motion Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+J">Junwoo Chang</a>, 
<a href="/search/cs?searchtype=author&query=Ryu%2C+H">Hyunwoo Ryu</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jiwoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+S">Soochul Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jongeun Choi</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+J">Joohwan Seo</a>, 
<a href="/search/cs?searchtype=author&query=Prakash%2C+N">Nikhil Prakash</a>, 
<a href="/search/cs?searchtype=author&query=Horowitz%2C+R">Roberto Horowitz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 6 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeurIPS 2023 Workshop on Diffusion Models
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item838">[838]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13283" title="Abstract">arXiv:2310.13283</a> (replaced) [<a href="/pdf/2310.13283" title="Download PDF">pdf</a>, <a href="/format/2310.13283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> pFedLoRA: Model-Heterogeneous Personalized Federated Learning with LoRA  Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+L">Liping Yi</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Han Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Gang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoguang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoxiao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item839">[839]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14174" title="Abstract">arXiv:2310.14174</a> (replaced) [<a href="/pdf/2310.14174" title="Download PDF">pdf</a>, <a href="/format/2310.14174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An In-Context Schema Understanding Method for Knowledge Base Question  Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yantao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zixuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xiaolong Jin</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yucan Guo</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+L">Long Bai</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+S">Saiping Guan</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jiafeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item840">[840]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14338" title="Abstract">arXiv:2310.14338</a> (replaced) [<a href="/pdf/2310.14338" title="Download PDF">pdf</a>, <a href="/format/2310.14338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Chaos to Clarity: Claim Normalization to Empower Fact-Checking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sundriyal%2C+M">Megha Sundriyal</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+T">Tanmoy Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Nakov%2C+P">Preslav Nakov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Findings EMNLP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item841">[841]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14807" title="Abstract">arXiv:2310.14807</a> (replaced) [<a href="/pdf/2310.14807" title="Download PDF">pdf</a>, <a href="/ps/2310.14807" title="Download PostScript">ps</a>, <a href="/format/2310.14807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Chaitin&#x27;s Heuristic Principle and Halting Probability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Salehi%2C+S">Saeed Salehi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages (consisting of two parts)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Information Theory (cs.IT); Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item842">[842]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15080" title="Abstract">arXiv:2310.15080</a> (replaced) [<a href="/pdf/2310.15080" title="Download PDF">pdf</a>, <a href="/format/2310.15080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Learning of Large Language Models with Parameter-Efficient  Prompt Tuning and Adaptive Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Che%2C+T">Tianshi Che</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Ji Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jiaxiang Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiwen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+V+S">Victor S. Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+H">Huaiyu Dai</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+D">Dejing Dou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, accepted by EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item843">[843]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16033" title="Abstract">arXiv:2310.16033</a> (replaced) [<a href="/pdf/2310.16033" title="Download PDF">pdf</a>, <a href="/format/2310.16033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Perceiving Small Visual Details in Zero-shot Visual Question  Answering with Multimodal LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiarui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Khayatkhoei%2C+M">Mahyar Khayatkhoei</a>, 
<a href="/search/cs?searchtype=author&query=Chhikara%2C+P">Prateek Chhikara</a>, 
<a href="/search/cs?searchtype=author&query=Ilievski%2C+F">Filip Ilievski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 12 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item844">[844]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16363" title="Abstract">arXiv:2310.16363</a> (replaced) [<a href="/pdf/2310.16363" title="Download PDF">pdf</a>, <a href="/format/2310.16363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite Time Analysis of Constrained Actor Critic and Constrained Natural  Actor Critic Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Panda%2C+P">Prashansa Panda</a>, 
<a href="/search/cs?searchtype=author&query=Bhatnagar%2C+S">Shalabh Bhatnagar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item845">[845]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18376" title="Abstract">arXiv:2310.18376</a> (replaced) [<a href="/pdf/2310.18376" title="Download PDF">pdf</a>, <a href="/format/2310.18376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SQLformer: Deep Auto-Regressive Query Graph Generation for Text-to-SQL  Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bazaga%2C+A">Adri&#xe1;n Bazaga</a>, 
<a href="/search/cs?searchtype=author&query=Li%C3%B2%2C+P">Pietro Li&#xf2;</a>, 
<a href="/search/cs?searchtype=author&query=Micklem%2C+G">Gos Micklem</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item846">[846]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18463" title="Abstract">arXiv:2310.18463</a> (replaced) [<a href="/pdf/2310.18463" title="Download PDF">pdf</a>, <a href="/format/2310.18463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PeTailor: Improving Large Language Model by Tailored Chunk Scorer in  Biomedical Triple Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingchen Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">M. Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Huixue Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Kilicoglu%2C+H">Halil Kilicoglu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> this is the second preprint version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item847">[847]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19158" title="Abstract">arXiv:2310.19158</a> (replaced) [<a href="/pdf/2310.19158" title="Download PDF">pdf</a>, <a href="/format/2310.19158" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perspectives from India: Challenges and Opportunities for Computational  Tools to Enhance Confidence in Published Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chakravorti%2C+T">Tatiana Chakravorti</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chuhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Koneru%2C+S">Sai Koneru</a>, 
<a href="/search/cs?searchtype=author&query=Rajtmajer%2C+S">Sarah Rajtmajer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item848">[848]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19545" title="Abstract">arXiv:2310.19545</a> (replaced) [<a href="/pdf/2310.19545" title="Download PDF">pdf</a>, <a href="/format/2310.19545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MENTOR: Human Perception-Guided Pretraining for Increased Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Crum%2C+C+R">Colton R. Crum</a>, 
<a href="/search/cs?searchtype=author&query=Czajka%2C+A">Adam Czajka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item849">[849]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00164" title="Abstract">arXiv:2311.00164</a> (replaced) [<a href="/pdf/2311.00164" title="Download PDF">pdf</a>, <a href="/format/2311.00164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Neural Networks for Road Safety Modeling: Datasets and Evaluations  for Accident Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nippani%2C+A">Abhinav Nippani</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongyue Li</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+H">Haotian Ju</a>, 
<a href="/search/cs?searchtype=author&query=Koutsopoulos%2C+H+N">Haris N. Koutsopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H+R">Hongyang R. Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages. Appeared in NeurIPS 2023 Datasets Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item850">[850]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00344" title="Abstract">arXiv:2311.00344</a> (replaced) [<a href="/pdf/2311.00344" title="Download PDF">pdf</a>, <a href="/format/2311.00344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Definition of Open-Ended Learning Problems for Goal-Conditioned Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sigaud%2C+O">Olivier Sigaud</a>, 
<a href="/search/cs?searchtype=author&query=Baldassarre%2C+G">Gianluca Baldassarre</a>, 
<a href="/search/cs?searchtype=author&query=Colas%2C+C">Cedric Colas</a>, 
<a href="/search/cs?searchtype=author&query=Doncieux%2C+S">Stephane Doncieux</a>, 
<a href="/search/cs?searchtype=author&query=Duro%2C+R">Richard Duro</a>, 
<a href="/search/cs?searchtype=author&query=Perrin-Gilbert%2C+N">Nicolas Perrin-Gilbert</a>, 
<a href="/search/cs?searchtype=author&query=Santucci%2C+V+G">Vieri Giuliano Santucci</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item851">[851]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00440" title="Abstract">arXiv:2311.00440</a> (replaced) [<a href="/pdf/2311.00440" title="Download PDF">pdf</a>, <a href="/ps/2311.00440" title="Download PostScript">ps</a>, <a href="/format/2311.00440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maximum $k$- vs. $\ell$-colourings of graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nakajima%2C+T">Tamio-Vesa Nakajima</a>, 
<a href="/search/cs?searchtype=author&query=%C5%BDivn%C3%BD%2C+S">Stanislav &#x17d;ivn&#xfd;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC); Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item852">[852]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00664" title="Abstract">arXiv:2311.00664</a> (replaced) [<a href="/pdf/2311.00664" title="Download PDF">pdf</a>, <a href="/format/2311.00664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Latent Space Translation via Semantic Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maiorca%2C+V">Valentino Maiorca</a>, 
<a href="/search/cs?searchtype=author&query=Moschella%2C+L">Luca Moschella</a>, 
<a href="/search/cs?searchtype=author&query=Norelli%2C+A">Antonio Norelli</a>, 
<a href="/search/cs?searchtype=author&query=Fumero%2C+M">Marco Fumero</a>, 
<a href="/search/cs?searchtype=author&query=Locatello%2C+F">Francesco Locatello</a>, 
<a href="/search/cs?searchtype=author&query=Rodol%C3%A0%2C+E">Emanuele Rodol&#xe0;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at NeurIPS 2023. 21 pages, 13 figures, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item853">[853]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00886" title="Abstract">arXiv:2311.00886</a> (replaced) [<a href="/pdf/2311.00886" title="Download PDF">pdf</a>, <a href="/format/2311.00886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> COSTAR: Improved Temporal Counterfactual Estimation with Self-Supervised  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meng%2C+C">Chuizheng Meng</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yihe Dong</a>, 
<a href="/search/cs?searchtype=author&query=Ar%C4%B1k%2C+S+%C3%96">Sercan &#xd6;. Ar&#x131;k</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pfister%2C+T">Tomas Pfister</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item854">[854]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01047" title="Abstract">arXiv:2311.01047</a> (replaced) [<a href="/pdf/2311.01047" title="Download PDF">pdf</a>, <a href="/format/2311.01047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Robustness via Tilted Exponential Layer: A  Communication-Theoretic Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Puranik%2C+B">Bhagyashree Puranik</a>, 
<a href="/search/cs?searchtype=author&query=Beirami%2C+A">Ahmad Beirami</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+Y">Yao Qin</a>, 
<a href="/search/cs?searchtype=author&query=Madhow%2C+U">Upamanyu Madhow</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This manuscript has been accepted for publication at the 27th International Conference on Artificial Intelligence and Statistics (AISTATS), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item855">[855]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01598" title="Abstract">arXiv:2311.01598</a> (replaced) [<a href="/pdf/2311.01598" title="Download PDF">pdf</a>, <a href="/format/2311.01598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CiFlow: Dataflow Analysis and Optimization of Key Switching for  Homomorphic Encryption
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neda%2C+N">Negar Neda</a>, 
<a href="/search/cs?searchtype=author&query=Ebel%2C+A">Austin Ebel</a>, 
<a href="/search/cs?searchtype=author&query=Reynwar%2C+B">Benedict Reynwar</a>, 
<a href="/search/cs?searchtype=author&query=Reagen%2C+B">Brandon Reagen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR); Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item856">[856]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01723" title="Abstract">arXiv:2311.01723</a> (replaced) [<a href="/pdf/2311.01723" title="Download PDF">pdf</a>, <a href="/format/2311.01723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Calibrated Robust Fine-Tuning of Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oh%2C+C">Changdae Oh</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+H">Hyesu Lim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Mijoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choo%2C+J">Jaegul Choo</a>, 
<a href="/search/cs?searchtype=author&query=Hauptmann%2C+A">Alexander Hauptmann</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zhi-Qi Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+K">Kyungwoo Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented at the NeurIPS 2023 Workshop on Distribution Shifts (DistShift)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item857">[857]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02017" title="Abstract">arXiv:2311.02017</a> (replaced) [<a href="/pdf/2311.02017" title="Download PDF">pdf</a>, <a href="/format/2311.02017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeliverAI: Reinforcement Learning Based Distributed Path-Sharing Network  for Food Deliveries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mehra%2C+A">Ashman Mehra</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+S">Snehanshu Saha</a>, 
<a href="/search/cs?searchtype=author&query=Raychoudhury%2C+V">Vaskar Raychoudhury</a>, 
<a href="/search/cs?searchtype=author&query=Mathur%2C+A">Archana Mathur</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item858">[858]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.04153" title="Abstract">arXiv:2311.04153</a> (replaced) [<a href="/pdf/2311.04153" title="Download PDF">pdf</a>, <a href="/format/2311.04153" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kernel-, mean- and noise-marginalised Gaussian processes for exoplanet  transits and $H_0$ inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Kroupa%2C+N">Namu Kroupa</a>, 
<a href="/search/astro-ph?searchtype=author&query=Yallup%2C+D">David Yallup</a>, 
<a href="/search/astro-ph?searchtype=author&query=Handley%2C+W">Will Handley</a>, 
<a href="/search/astro-ph?searchtype=author&query=Hobson%2C+M">Michael Hobson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 11 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Monthly Notices of the Royal Astronomical Society, Volume 528,
  Issue 2, February 2024, Pages 1232-1248
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cosmology and Nongalactic Astrophysics (astro-ph.CO)</span>; Earth and Planetary Astrophysics (astro-ph.EP); Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item859">[859]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06233" title="Abstract">arXiv:2311.06233</a> (replaced) [<a href="/pdf/2311.06233" title="Download PDF">pdf</a>, <a href="/ps/2311.06233" title="Download PostScript">ps</a>, <a href="/format/2311.06233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Contamination Quiz: A Tool to Detect and Estimate Contamination in  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Golchin%2C+S">Shahriar Golchin</a>, 
<a href="/search/cs?searchtype=author&query=Surdeanu%2C+M">Mihai Surdeanu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v2.1 preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item860">[860]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06922" title="Abstract">arXiv:2311.06922</a> (replaced) [<a href="/pdf/2311.06922" title="Download PDF">pdf</a>, <a href="/format/2311.06922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Socially Aware Robot Navigation: Taxonomy and Future  Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singamaneni%2C+P+T">Phani Teja Singamaneni</a>, 
<a href="/search/cs?searchtype=author&query=Bachiller-Burgos%2C+P">Pilar Bachiller-Burgos</a>, 
<a href="/search/cs?searchtype=author&query=Manso%2C+L+J">Luis J. Manso</a>, 
<a href="/search/cs?searchtype=author&query=Garrell%2C+A">Ana&#xed;s Garrell</a>, 
<a href="/search/cs?searchtype=author&query=Sanfeliu%2C+A">Alberto Sanfeliu</a>, 
<a href="/search/cs?searchtype=author&query=Spalanzani%2C+A">Anne Spalanzani</a>, 
<a href="/search/cs?searchtype=author&query=Alami%2C+R">Rachid Alami</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item861">[861]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07113" title="Abstract">arXiv:2311.07113</a> (replaced) [<a href="/pdf/2311.07113" title="Download PDF">pdf</a>, <a href="/format/2311.07113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpectralGPT: Spectral Remote Sensing Foundation Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+D">Danfeng Hong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jing Yao</a>, 
<a href="/search/cs?searchtype=author&query=Yokoya%2C+N">Naoto Yokoya</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/cs?searchtype=author&query=Ghamisi%2C+P">Pedram Ghamisi</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xiuping Jia</a>, 
<a href="/search/cs?searchtype=author&query=Plaza%2C+A">Antonio Plaza</a>, 
<a href="/search/cs?searchtype=author&query=Gamba%2C+P">Paolo Gamba</a>, 
<a href="/search/cs?searchtype=author&query=Benediktsson%2C+J+A">Jon Atli Benediktsson</a>, 
<a href="/search/cs?searchtype=author&query=Chanussot%2C+J">Jocelyn Chanussot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE TPAMI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item862">[862]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07466" title="Abstract">arXiv:2311.07466</a> (replaced) [<a href="/pdf/2311.07466" title="Download PDF">pdf</a>, <a href="/format/2311.07466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Measuring Faithfulness or Self-consistency of Natural Language  Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parcalabescu%2C+L">Letitia Parcalabescu</a>, 
<a href="/search/cs?searchtype=author&query=Frank%2C+A">Anette Frank</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 main paper pages, 29 appendix pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item863">[863]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08019" title="Abstract">arXiv:2311.08019</a> (replaced) [<a href="/pdf/2311.08019" title="Download PDF">pdf</a>, <a href="/format/2311.08019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Servoing NMPC Applied to UAVs for Photovoltaic Array Inspection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Velasco-S%C3%A1nchez%2C+E+P">Edison P. Velasco-S&#xe1;nchez</a>, 
<a href="/search/cs?searchtype=author&query=Recalde%2C+L+F">Luis F. Recalde</a>, 
<a href="/search/cs?searchtype=author&query=Guevara%2C+B+S">Bryan S. Guevara</a>, 
<a href="/search/cs?searchtype=author&query=Varela-Ald%C3%A1s%2C+J">Jos&#xe9; Varela-Ald&#xe1;s</a>, 
<a href="/search/cs?searchtype=author&query=Candelas%2C+F+A">Francisco A. Candelas</a>, 
<a href="/search/cs?searchtype=author&query=Puente%2C+S+T">Santiago T. Puente</a>, 
<a href="/search/cs?searchtype=author&query=Gandolfo%2C+D+C">Daniel C. Gandolfo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for publication in IEEE Robotics and Automation Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item864">[864]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08711" title="Abstract">arXiv:2311.08711</a> (replaced) [<a href="/pdf/2311.08711" title="Download PDF">pdf</a>, <a href="/format/2311.08711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhihan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dong-Ho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yuwei Fang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenhao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+M">Mengzhao Jia</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Meng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Barbieri%2C+F">Francesco Barbieri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item865">[865]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09393" title="Abstract">arXiv:2311.09393</a> (replaced) [<a href="/pdf/2311.09393" title="Download PDF">pdf</a>, <a href="/format/2311.09393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Taypsi: Static Enforcement of Privacy Policies for Policy-Agnostic  Oblivious Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+Q">Qianchuan Ye</a>, 
<a href="/search/cs?searchtype=author&query=Delaware%2C+B">Benjamin Delaware</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item866">[866]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09975" title="Abstract">arXiv:2311.09975</a> (replaced) [<a href="/pdf/2311.09975" title="Download PDF">pdf</a>, <a href="/ps/2311.09975" title="Download PostScript">ps</a>, <a href="/format/2311.09975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Version Age of Information Minimization over Fading Broadcast Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karevvanavar%2C+G">Gangadhar Karevvanavar</a>, 
<a href="/search/cs?searchtype=author&query=Pable%2C+H">Hrishikesh Pable</a>, 
<a href="/search/cs?searchtype=author&query=Patil%2C+O">Om Patil</a>, 
<a href="/search/cs?searchtype=author&query=Bhat%2C+R+V">Rajshekhar V Bhat</a>, 
<a href="/search/cs?searchtype=author&query=Pappas%2C+N">Nikolaos Pappas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item867">[867]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12244" title="Abstract">arXiv:2311.12244</a> (replaced) [<a href="/pdf/2311.12244" title="Download PDF">pdf</a>, <a href="/format/2311.12244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Reinforcement Learning from Partial Observability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+T">Tongzheng Ren</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chenjun Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Schuurmans%2C+D">Dale Schuurmans</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+B">Bo Dai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contribute equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item868">[868]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12304" title="Abstract">arXiv:2311.12304</a> (replaced) [<a href="/pdf/2311.12304" title="Download PDF">pdf</a>, <a href="/format/2311.12304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering Effective Policies for Land-Use Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miikkulainen%2C+R">Risto Miikkulainen</a>, 
<a href="/search/cs?searchtype=author&query=Francon%2C+O">Olivier Francon</a>, 
<a href="/search/cs?searchtype=author&query=Young%2C+D">Daniel Young</a>, 
<a href="/search/cs?searchtype=author&query=Meyerson%2C+E">Elliot Meyerson</a>, 
<a href="/search/cs?searchtype=author&query=Schwingshackl%2C+C">Clemens Schwingshackl</a>, 
<a href="/search/cs?searchtype=author&query=Bieker%2C+J">Jacob Bieker</a>, 
<a href="/search/cs?searchtype=author&query=Cunha%2C+H">Hugo Cunha</a>, 
<a href="/search/cs?searchtype=author&query=Hodjat%2C+B">Babak Hodjat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item869">[869]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12481" title="Abstract">arXiv:2311.12481</a> (replaced) [<a href="/pdf/2311.12481" title="Download PDF">pdf</a>, <a href="/ps/2311.12481" title="Download PostScript">ps</a>, <a href="/format/2311.12481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretability is in the eye of the beholder: Human versus artificial  classification of image segments generated by humans versus XAI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+R">Romy M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Tho%C3%9F%2C+M">Marius Tho&#xdf;</a>, 
<a href="/search/cs?searchtype=author&query=Ullrich%2C+J">Julian Ullrich</a>, 
<a href="/search/cs?searchtype=author&query=Seitz%2C+S">Steffen Seitz</a>, 
<a href="/search/cs?searchtype=author&query=Knoll%2C+C">Carsten Knoll</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item870">[870]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12965" title="Abstract">arXiv:2311.12965</a> (replaced) [<a href="/pdf/2311.12965" title="Download PDF">pdf</a>, <a href="/format/2311.12965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Terrestrial-Satellite Spectrum Sharing in the Upper Mid-Band with  Interference Nulling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kang%2C+S">Seongjoon Kang</a>, 
<a href="/search/eess?searchtype=author&query=Geraci%2C+G">Giovanni Geraci</a>, 
<a href="/search/eess?searchtype=author&query=Mezzavilla%2C+M">Marco Mezzavilla</a>, 
<a href="/search/eess?searchtype=author&query=Rangan%2C+S">Sundeep Rangan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item871">[871]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.13088" title="Abstract">arXiv:2311.13088</a> (replaced) [<a href="/e-print/2311.13088" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Validation of Consumer-grade Digital Camera-based Human Activity  Evaluation for Upper Limb Exercises and Development of a Therapist-guided,  Automated Telerehabilitation Framework and Platform for Stroke Rehabilitation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yeung%2C+E+H+L">Elton H.L. Yeung</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yingxian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Fok%2C+W+W+T">Wilton W.T. Fok</a>, 
<a href="/search/cs?searchtype=author&query=Lau%2C+G+K+K">Gary K.K. Lau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> it's not a ready paper to be uploaded
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item872">[872]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.13541" title="Abstract">arXiv:2311.13541</a> (replaced) [<a href="/pdf/2311.13541" title="Download PDF">pdf</a>, <a href="/format/2311.13541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear Log-Normal Attention with Unbiased Concentration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nahshan%2C+Y">Yury Nahshan</a>, 
<a href="/search/cs?searchtype=author&query=Kampeas%2C+J">Joseph Kampeas</a>, 
<a href="/search/cs?searchtype=author&query=Haleva%2C+E">Emir Haleva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 20 figures, 5 tables, submitted to ICLR2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item873">[873]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.13917" title="Abstract">arXiv:2311.13917</a> (replaced) [<a href="/pdf/2311.13917" title="Download PDF">pdf</a>, <a href="/ps/2311.13917" title="Download PostScript">ps</a>, <a href="/format/2311.13917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the impact of social stress on the adaptive dynamics of  COVID-19: Typing the behavior of na&#xef;ve populations faced with epidemics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Kastalskiy%2C+I">Innokentiy Kastalskiy</a>, 
<a href="/search/physics?searchtype=author&query=Zinovyev%2C+A">Andrei Zinovyev</a>, 
<a href="/search/physics?searchtype=author&query=Mirkes%2C+E">Evgeny Mirkes</a>, 
<a href="/search/physics?searchtype=author&query=Kazantsev%2C+V">Victor Kazantsev</a>, 
<a href="/search/physics?searchtype=author&query=Gorban%2C+A+N">Alexander N. Gorban</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 16 figures, 1 table, 2 appendices
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item874">[874]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.14361" title="Abstract">arXiv:2311.14361</a> (replaced) [<a href="/pdf/2311.14361" title="Download PDF">pdf</a>, <a href="/ps/2311.14361" title="Download PostScript">ps</a>, <a href="/format/2311.14361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deciphering and integrating invariants for neural operator learning with  various physical mechanisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Q">Qi Meng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhi-Ming Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA); Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item875">[875]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.14455" title="Abstract">arXiv:2311.14455</a> (replaced) [<a href="/pdf/2311.14455" title="Download PDF">pdf</a>, <a href="/format/2311.14455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Jailbreak Backdoors from Poisoned Human Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rando%2C+J">Javier Rando</a>, 
<a href="/search/cs?searchtype=author&query=Tram%C3%A8r%2C+F">Florian Tram&#xe8;r</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as conference paper in ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item876">[876]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.14968" title="Abstract">arXiv:2311.14968</a> (replaced) [<a href="/pdf/2311.14968" title="Download PDF">pdf</a>, <a href="/format/2311.14968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hide Your Model: A Parameter Transmission-free Federated Recommender  System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+W">Wei Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chaoqun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+L">Liang Qu</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+Q+V+H">Quoc Viet Hung Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianxin Li</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hongzhi Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICDE2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item877">[877]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.15054" title="Abstract">arXiv:2311.15054</a> (replaced) [<a href="/pdf/2311.15054" title="Download PDF">pdf</a>, <a href="/ps/2311.15054" title="Download PostScript">ps</a>, <a href="/format/2311.15054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detection of developmental language disorder in Cypriot Greek children  using a neural network algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Georgiou%2C+G+P">Georgios P. Georgiou</a>, 
<a href="/search/cs?searchtype=author&query=Theodorou%2C+E">Elena Theodorou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 3 figures, journal article
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item878">[878]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.15108" title="Abstract">arXiv:2311.15108</a> (replaced) [<a href="/pdf/2311.15108" title="Download PDF">pdf</a>, <a href="/format/2311.15108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Diffusion Perturbations for Measuring Fairness in Computer  Vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lui%2C+N">Nicholas Lui</a>, 
<a href="/search/cs?searchtype=author&query=Chia%2C+B">Bryan Chia</a>, 
<a href="/search/cs?searchtype=author&query=Berrios%2C+W">William Berrios</a>, 
<a href="/search/cs?searchtype=author&query=Ross%2C+C">Candace Ross</a>, 
<a href="/search/cs?searchtype=author&query=Kiela%2C+D">Douwe Kiela</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The Appendix can be found at <a href="https://bit.ly/dp-appendix">this https URL</a>; Added link to code and fixed formatting (Feb 10 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item879">[879]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.15738" title="Abstract">arXiv:2311.15738</a> (replaced) [<a href="/pdf/2311.15738" title="Download PDF">pdf</a>, <a href="/format/2311.15738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On full linear convergence and optimal complexity of adaptive FEM with  inexact solver
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bringmann%2C+P">Philipp Bringmann</a>, 
<a href="/search/math?searchtype=author&query=Feischl%2C+M">Michael Feischl</a>, 
<a href="/search/math?searchtype=author&query=Miraci%2C+A">Ani Miraci</a>, 
<a href="/search/math?searchtype=author&query=Praetorius%2C+D">Dirk Praetorius</a>, 
<a href="/search/math?searchtype=author&query=Streitberger%2C+J">Julian Streitberger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item880">[880]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.15885" title="Abstract">arXiv:2311.15885</a> (replaced) [<a href="/pdf/2311.15885" title="Download PDF">pdf</a>, <a href="/format/2311.15885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Quantifier Depth to Quantifier Number: Separating Structures with k  Variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vinall-Smeeth%2C+H">Harry Vinall-Smeeth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 53 pages, 8 figures; added new result on the relative succinctness of finite variable logic
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item881">[881]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.16466" title="Abstract">arXiv:2311.16466</a> (replaced) [<a href="/pdf/2311.16466" title="Download PDF">pdf</a>, <a href="/format/2311.16466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large language models can enhance persuasion through linguistic feature  alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shin%2C+M">Minkyu Shin</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jin Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item882">[882]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.16526" title="Abstract">arXiv:2311.16526</a> (replaced) [<a href="/pdf/2311.16526" title="Download PDF">pdf</a>, <a href="/format/2311.16526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On robust overfitting: adversarial training induced distribution matters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+R">Runzhi Tian</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yongyi Mao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item883">[883]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.16594" title="Abstract">arXiv:2311.16594</a> (replaced) [<a href="/e-print/2311.16594" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Monitor Placement for Fault Localization in Deep Neural Network  Accelerators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei-Kai Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical fallacies appear in this paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item884">[884]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.18575" title="Abstract">arXiv:2311.18575</a> (replaced) [<a href="/pdf/2311.18575" title="Download PDF">pdf</a>, <a href="/format/2311.18575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Class Distribution Shifts in Zero-Shot Learning: Learning Robust  Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Slavutsky%2C+Y">Yuli Slavutsky</a>, 
<a href="/search/cs?searchtype=author&query=Benjamini%2C+Y">Yuval Benjamini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item885">[885]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.00054" title="Abstract">arXiv:2312.00054</a> (replaced) [<a href="/pdf/2312.00054" title="Download PDF">pdf</a>, <a href="/format/2312.00054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Inverse Reinforcement Learning Harder than Standard Reinforcement  Learning? A Theoretical Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhao%2C+L">Lei Zhao</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+M">Mengdi Wang</a>, 
<a href="/search/stat?searchtype=author&query=Bai%2C+Y">Yu Bai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item886">[886]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.01163" title="Abstract">arXiv:2312.01163</a> (replaced) [<a href="/pdf/2312.01163" title="Download PDF">pdf</a>, <a href="/format/2312.01163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A New Learning Paradigm for Foundation Model-based Remote Sensing Change  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kaiyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xiangyong Cao</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+D">Deyu Meng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item887">[887]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.01169" title="Abstract">arXiv:2312.01169</a> (replaced) [<a href="/pdf/2312.01169" title="Download PDF">pdf</a>, <a href="/format/2312.01169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Virtual Category Learning: A Semi-Supervised Learning Method for Dense  Prediction with Extremely Limited Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Changrui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jungong Han</a>, 
<a href="/search/cs?searchtype=author&query=Debattista%2C+K">Kurt Debattista</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> T-PAMI accepted. arXiv admin note: text overlap with <a href="/abs/2207.03433">arXiv:2207.03433</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item888">[888]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.01920" title="Abstract">arXiv:2312.01920</a> (replaced) [<a href="/pdf/2312.01920" title="Download PDF">pdf</a>, <a href="/ps/2312.01920" title="Download PostScript">ps</a>, <a href="/format/2312.01920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> New method for SISO strong stabilization with advantages over existing  methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Faruqi%2C+A+H">Abdul Hannan Faruqi</a>, 
<a href="/search/eess?searchtype=author&query=Chatterjee%2C+A">Anindya Chatterjee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item889">[889]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.02093" title="Abstract">arXiv:2312.02093</a> (replaced) [<a href="/pdf/2312.02093" title="Download PDF">pdf</a>, <a href="/ps/2312.02093" title="Download PostScript">ps</a>, <a href="/format/2312.02093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cultural Differences in Students&#x27; Privacy Concerns in Learning Analytics  across Germany, South Korea, Spain, Sweden, and the United States
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Viberg%2C+O">Olga Viberg</a>, 
<a href="/search/cs?searchtype=author&query=Kizilcec%2C+R+F">Ren&#xe9; F. Kizilcec</a>, 
<a href="/search/cs?searchtype=author&query=Jivet%2C+I">Ioana Jivet</a>, 
<a href="/search/cs?searchtype=author&query=Mon%C3%A9s%2C+A+M">Alejandra Mart&#xed;nez Mon&#xe9;s</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+A">Alice Oh</a>, 
<a href="/search/cs?searchtype=author&query=Mutimukwe%2C+C">Chantal Mutimukwe</a>, 
<a href="/search/cs?searchtype=author&query=Hrastinski%2C+S">Stefan Hrastinski</a>, 
<a href="/search/cs?searchtype=author&query=Scheffel%2C+M">Maren Scheffel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item890">[890]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.03096" title="Abstract">arXiv:2312.03096</a> (replaced) [<a href="/pdf/2312.03096" title="Download PDF">pdf</a>, <a href="/format/2312.03096" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Causes Polysemanticity? An Alternative Origin Story of Mixed  Selectivity from Incidental Causes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lecomte%2C+V">Victor Lecomte</a>, 
<a href="/search/cs?searchtype=author&query=Thaman%2C+K">Kushal Thaman</a>, 
<a href="/search/cs?searchtype=author&query=Chow%2C+T">Trevor Chow</a>, 
<a href="/search/cs?searchtype=author&query=Schaeffer%2C+R">Rylan Schaeffer</a>, 
<a href="/search/cs?searchtype=author&query=Koyejo%2C+S">Sanmi Koyejo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item891">[891]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.03731" title="Abstract">arXiv:2312.03731</a> (replaced) [<a href="/pdf/2312.03731" title="Download PDF">pdf</a>, <a href="/format/2312.03731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xingtong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yuan Fang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinming Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item892">[892]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.04455" title="Abstract">arXiv:2312.04455</a> (replaced) [<a href="/pdf/2312.04455" title="Download PDF">pdf</a>, <a href="/format/2312.04455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fortify the Shortest Stave in Attention: Enhancing Context Awareness of  Large Language Models for Effective Tool Use
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuhan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+A">Ang Lv</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Ting-En Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Changyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuchuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongbin Li</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+R">Rui Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item893">[893]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.04579" title="Abstract">arXiv:2312.04579</a> (replaced) [<a href="/pdf/2312.04579" title="Download PDF">pdf</a>, <a href="/format/2312.04579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> zkDFL: An efficient and privacy-preserving decentralized federated  learning with zero-knowledge proof
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahmadi%2C+M">Mojtaba Ahmadi</a>, 
<a href="/search/cs?searchtype=author&query=Nourmohammadi%2C+R">Reza Nourmohammadi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item894">[894]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.05974" title="Abstract">arXiv:2312.05974</a> (replaced) [<a href="/pdf/2312.05974" title="Download PDF">pdf</a>, <a href="/format/2312.05974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning the Causal Structure of Networked Dynamical Systems under  Latent Nodes and Structured Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Santos%2C+A">Augusto Santos</a>, 
<a href="/search/cs?searchtype=author&query=Rente%2C+D">Diogo Rente</a>, 
<a href="/search/cs?searchtype=author&query=Seabra%2C+R">Rui Seabra</a>, 
<a href="/search/cs?searchtype=author&query=Moura%2C+J+M+F">Jos&#xe9; M. F. Moura</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at The 38th AAAI Conference on Artificial Intelligence (Main Track). Final Camera-Ready Version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item895">[895]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.06231" title="Abstract">arXiv:2312.06231</a> (replaced) [<a href="/pdf/2312.06231" title="Download PDF">pdf</a>, <a href="/format/2312.06231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncovering communities of pipelines in the task-fMRI analytical space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Germani%2C+E">Elodie Germani</a> (EMPENN), 
<a href="/search/cs?searchtype=author&query=Fromont%2C+E">Elisa Fromont</a> (LACODAM), 
<a href="/search/cs?searchtype=author&query=Maumet%2C+C">Camille Maumet</a> (EMPENN)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item896">[896]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.06908" title="Abstract">arXiv:2312.06908</a> (replaced) [<a href="/pdf/2312.06908" title="Download PDF">pdf</a>, <a href="/format/2312.06908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;I Want It That Way&quot;: Enabling Interactive Decision Support Using Large  Language Models and Constraint Programming
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lawless%2C+C">Connor Lawless</a>, 
<a href="/search/cs?searchtype=author&query=Schoeffer%2C+J">Jakob Schoeffer</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+L">Lindy Le</a>, 
<a href="/search/cs?searchtype=author&query=Rowan%2C+K">Kael Rowan</a>, 
<a href="/search/cs?searchtype=author&query=Sen%2C+S">Shilad Sen</a>, 
<a href="/search/cs?searchtype=author&query=Hill%2C+C+S">Cristina St. Hill</a>, 
<a href="/search/cs?searchtype=author&query=Suh%2C+J">Jina Suh</a>, 
<a href="/search/cs?searchtype=author&query=Sarrafzadeh%2C+B">Bahareh Sarrafzadeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item897">[897]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07130" title="Abstract">arXiv:2312.07130</a> (replaced) [<a href="/pdf/2312.07130" title="Download PDF">pdf</a>, <a href="/format/2312.07130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Divide-and-Conquer Attack: Harnessing the Power of LLM to Bypass Safety  Filters of Text-to-Image Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yimo Deng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huangxun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 11 figures, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item898">[898]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07364" title="Abstract">arXiv:2312.07364</a> (replaced) [<a href="/pdf/2312.07364" title="Download PDF">pdf</a>, <a href="/format/2312.07364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collapse-Aware Triplet Decoupling for Adversarially Robust Image  Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+Q">Qiwei Tian</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenhao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhengyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qian Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chao Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item899">[899]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08234" title="Abstract">arXiv:2312.08234</a> (replaced) [<a href="/pdf/2312.08234" title="Download PDF">pdf</a>, <a href="/format/2312.08234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond the Label Itself: Latent Labels Enhance Semi-supervised Point  Cloud Panoptic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yujun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+X">Xin Tan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhizhong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+Y">Yanyun Qu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yuan Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 8 figures, 11 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> CVPR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item900">[900]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08823" title="Abstract">arXiv:2312.08823</a> (replaced) [<a href="/pdf/2312.08823" title="Download PDF">pdf</a>, <a href="/format/2312.08823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast sampling from constrained spaces using the Metropolis-adjusted  Mirror Langevin algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Srinivasan%2C+V">Vishwak Srinivasan</a>, 
<a href="/search/stat?searchtype=author&query=Wibisono%2C+A">Andre Wibisono</a>, 
<a href="/search/stat?searchtype=author&query=Wilson%2C+A">Ashia Wilson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 48 pages, 6 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item901">[901]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.09006" title="Abstract">arXiv:2312.09006</a> (replaced) [<a href="/pdf/2312.09006" title="Download PDF">pdf</a>, <a href="/format/2312.09006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedSSA: Semantic Similarity-based Aggregation for Efficient  Model-Heterogeneous Personalized Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+L">Liping Yi</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Han Yu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zhuan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Gang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoguang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+L">Lizhen Cui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoxiao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item902">[902]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.09870" title="Abstract">arXiv:2312.09870</a> (replaced) [<a href="/pdf/2312.09870" title="Download PDF">pdf</a>, <a href="/format/2312.09870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CABBA: Compatible Authenticated Bandwidth-efficient Broadcast protocol  for ADS-B
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ngambo%C3%A9%2C+M">Mika&#xeb;la Ngambo&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+X">Xiao Niu</a>, 
<a href="/search/cs?searchtype=author&query=Joly%2C+B">Benoit Joly</a>, 
<a href="/search/cs?searchtype=author&query=Biegler%2C+S+P">Steven P Biegler</a>, 
<a href="/search/cs?searchtype=author&query=Berthier%2C+P">Paul Berthier</a>, 
<a href="/search/cs?searchtype=author&query=Benito%2C+R">R&#xe9;mi Benito</a>, 
<a href="/search/cs?searchtype=author&query=Rice%2C+G">Greg Rice</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez%2C+J+M">Jos&#xe9; M Fernandez</a>, 
<a href="/search/cs?searchtype=author&query=Nicolescu%2C+G">Gabriela Nicolescu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper has been submitted to IEEE Transactions on Aerospace and Electronic Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item903">[903]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.09963" title="Abstract">arXiv:2312.09963</a> (replaced) [<a href="/pdf/2312.09963" title="Download PDF">pdf</a>, <a href="/format/2312.09963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symbolic Numeric Planning with Patterns
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cardellini%2C+M">Matteo Cardellini</a>, 
<a href="/search/cs?searchtype=author&query=Giunchiglia%2C+E">Enrico Giunchiglia</a>, 
<a href="/search/cs?searchtype=author&query=Maratea%2C+M">Marco Maratea</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAAI24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item904">[904]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11801" title="Abstract">arXiv:2312.11801</a> (replaced) [<a href="/pdf/2312.11801" title="Download PDF">pdf</a>, <a href="/format/2312.11801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast, Scalable, Warm-Start Semidefinite Programming with Spectral  Bundling and Sketching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Angell%2C+R">Rico Angell</a>, 
<a href="/search/math?searchtype=author&query=McCallum%2C+A">Andrew McCallum</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item905">[905]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12568" title="Abstract">arXiv:2312.12568</a> (replaced) [<a href="/pdf/2312.12568" title="Download PDF">pdf</a>, <a href="/format/2312.12568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaling Opponent Shaping to High Dimensional Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+A">Akbir Khan</a>, 
<a href="/search/cs?searchtype=author&query=Willi%2C+T">Timon Willi</a>, 
<a href="/search/cs?searchtype=author&query=Kwan%2C+N">Newton Kwan</a>, 
<a href="/search/cs?searchtype=author&query=Tacchetti%2C+A">Andrea Tacchetti</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chris Lu</a>, 
<a href="/search/cs?searchtype=author&query=Grefenstette%2C+E">Edward Grefenstette</a>, 
<a href="/search/cs?searchtype=author&query=Rockt%C3%A4schel%2C+T">Tim Rockt&#xe4;schel</a>, 
<a href="/search/cs?searchtype=author&query=Foerster%2C+J">Jakob Foerster</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item906">[906]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12869" title="Abstract">arXiv:2312.12869</a> (replaced) [<a href="/pdf/2312.12869" title="Download PDF">pdf</a>, <a href="/format/2312.12869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameterized Projected Bellman Operator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vincent%2C+T">Th&#xe9;o Vincent</a>, 
<a href="/search/cs?searchtype=author&query=Metelli%2C+A+M">Alberto Maria Metelli</a>, 
<a href="/search/cs?searchtype=author&query=Belousov%2C+B">Boris Belousov</a>, 
<a href="/search/cs?searchtype=author&query=Peters%2C+J">Jan Peters</a>, 
<a href="/search/cs?searchtype=author&query=Restelli%2C+M">Marcello Restelli</a>, 
<a href="/search/cs?searchtype=author&query=D%27Eramo%2C+C">Carlo D&#x27;Eramo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the National Conference on Artificial Intelligence (AAAI-24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item907">[907]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.13327" title="Abstract">arXiv:2312.13327</a> (replaced) [<a href="/pdf/2312.13327" title="Download PDF">pdf</a>, <a href="/format/2312.13327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Reinforcement Learning for Variable Action Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sinii%2C+V">Viacheslav Sinii</a>, 
<a href="/search/cs?searchtype=author&query=Nikulin%2C+A">Alexander Nikulin</a>, 
<a href="/search/cs?searchtype=author&query=Kurenkov%2C+V">Vladislav Kurenkov</a>, 
<a href="/search/cs?searchtype=author&query=Zisman%2C+I">Ilya Zisman</a>, 
<a href="/search/cs?searchtype=author&query=Kolesnikov%2C+S">Sergey Kolesnikov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, Under Review; code: <a href="https://github.com/corl-team/headless-ad">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item908">[908]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.14212" title="Abstract">arXiv:2312.14212</a> (replaced) [<a href="/pdf/2312.14212" title="Download PDF">pdf</a>, <a href="/format/2312.14212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond mirkwood: Enhancing SED Modeling with Conformal Predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Gilda%2C+S">Sankalp Gilda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages + 1 reference page. Accepted to the 3rd AI2ASE workshop at AAAI 2024 (Vancouver, BC, Canada)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Astronomy 2024, 3, 14-20
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Instrumentation and Methods for Astrophysics (astro-ph.IM)</span>; Astrophysics of Galaxies (astro-ph.GA); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item909">[909]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.14890" title="Abstract">arXiv:2312.14890</a> (replaced) [<a href="/pdf/2312.14890" title="Download PDF">pdf</a>, <a href="/format/2312.14890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language  Models via Complexity Classes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+L">Lizhou Fan</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+W">Wenyue Hua</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lingyao Li</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+H">Haoyang Ling</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongfeng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 7 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computational Complexity (cs.CC); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item910">[910]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15403" title="Abstract">arXiv:2312.15403</a> (replaced) [<a href="/pdf/2312.15403" title="Download PDF">pdf</a>, <a href="/format/2312.15403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SIRD: A Sender-Informed, Receiver-Driven Datacenter Transport Protocol
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prasopoulos%2C+K">Konstantinos Prasopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Bugnion%2C+E">Edouard Bugnion</a>, 
<a href="/search/cs?searchtype=author&query=Kogias%2C+M">Marios Kogias</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item911">[911]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15910" title="Abstract">arXiv:2312.15910</a> (replaced) [<a href="/pdf/2312.15910" title="Download PDF">pdf</a>, <a href="/format/2312.15910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Unlearning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+D">Dayong Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Tianqing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Congcong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Derui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zewei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Sheng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wanlei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+M">Minhui Xue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item912">[912]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.00547" title="Abstract">arXiv:2401.00547</a> (replaced) [<a href="/e-print/2401.00547" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Learning for Ambiguous Chance Constrained Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Madhusudanarao%2C+A+C">A Ch Madhusudanarao</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+R">Rahul Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We have "not considered the uniform bound" for violation probabilities corresponding to the set of distributions in the ambiguity set
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item913">[913]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.01404" title="Abstract">arXiv:2401.01404</a> (replaced) [<a href="/pdf/2401.01404" title="Download PDF">pdf</a>, <a href="/format/2401.01404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable network reconstruction in subquadratic time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peixoto%2C+T+P">Tiago P. Peixoto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG); Data Analysis, Statistics and Probability (physics.data-an); Computation (stat.CO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item914">[914]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.01479" title="Abstract">arXiv:2401.01479</a> (replaced) [<a href="/pdf/2401.01479" title="Download PDF">pdf</a>, <a href="/format/2401.01479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kernel-U-Net: Symmetric and Hierarchical Architecture for Multivariate  Time Series Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=You%2C+J">Jiang You</a>, 
<a href="/search/cs?searchtype=author&query=Natowicz%2C+R">Ren&#xe9; Natowicz</a>, 
<a href="/search/cs?searchtype=author&query=Cela%2C+A">Arben Cela</a>, 
<a href="/search/cs?searchtype=author&query=Ouanounou%2C+J">Jacob Ouanounou</a>, 
<a href="/search/cs?searchtype=author&query=Siarry%2C+P">Patrick Siarry</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item915">[915]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.02333" title="Abstract">arXiv:2401.02333</a> (replaced) [<a href="/pdf/2401.02333" title="Download PDF">pdf</a>, <a href="/format/2401.02333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Extraction: Contextualising Tabular Data for Efficient  Summarisation by Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Allu%2C+U">Uday Allu</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+B">Biddwan Ahmed</a>, 
<a href="/search/cs?searchtype=author&query=Tripathi%2C+V">Vishesh Tripathi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item916">[916]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.02494" title="Abstract">arXiv:2401.02494</a> (replaced) [<a href="/pdf/2401.02494" title="Download PDF">pdf</a>, <a href="/format/2401.02494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing GUI for Generative AI: Charting the Design Space of Human-AI  Interactions through Task Creativity and Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Z">Zijian Ding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item917">[917]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.02731" title="Abstract">arXiv:2401.02731</a> (replaced) [<a href="/pdf/2401.02731" title="Download PDF">pdf</a>, <a href="/format/2401.02731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts  for Instruction Tuning on General Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haoyuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Haisheng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhuolun He</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bei Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item918">[918]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.03092" title="Abstract">arXiv:2401.03092</a> (replaced) [<a href="/pdf/2401.03092" title="Download PDF">pdf</a>, <a href="/format/2401.03092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite Expression Method for Learning Dynamics on Complex Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zezheng Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chunmei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haizhao Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Symbolic Computation (cs.SC)</span>; Numerical Analysis (math.NA); Applied Physics (physics.app-ph)

</div>
</div>
</dd>
<dt><a name="item919">[919]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.03752" title="Abstract">arXiv:2401.03752</a> (replaced) [<a href="/pdf/2401.03752" title="Download PDF">pdf</a>, <a href="/format/2401.03752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Limited Information Enough? An Approximate Multi-agent Coverage  Control in Non-Convex Discrete Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iwase%2C+T">Tatsuya Iwase</a>, 
<a href="/search/cs?searchtype=author&query=Beynier%2C+A">Aur&#xe9;lie Beynier</a>, 
<a href="/search/cs?searchtype=author&query=Bredeche%2C+N">Nicolas Bredeche</a>, 
<a href="/search/cs?searchtype=author&query=Maudet%2C+N">Nicolas Maudet</a>, 
<a href="/search/cs?searchtype=author&query=Marden%2C+J+R">Jason R. Marden</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item920">[920]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05507" title="Abstract">arXiv:2401.05507</a> (replaced) [<a href="/pdf/2401.05507" title="Download PDF">pdf</a>, <a href="/format/2401.05507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xueyu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Ziyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+S">Shuang Wei</a>, 
<a href="/search/cs?searchtype=author&query=Chai%2C+Z">Ziwei Chai</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Q">Qianli Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoyin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xuwu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jing Su</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jingjing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Ming Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jianbo Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+K">Kun Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongxia Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+F">Fei Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 7 figures, work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item921">[921]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05657" title="Abstract">arXiv:2401.05657</a> (replaced) [<a href="/pdf/2401.05657" title="Download PDF">pdf</a>, <a href="/ps/2401.05657" title="Download PostScript">ps</a>, <a href="/format/2401.05657" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An impossibility theorem concerning positive involvement in voting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Holliday%2C+W+H">Wesley H. Holliday</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Forthcoming in Economics Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item922">[922]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05860" title="Abstract">arXiv:2401.05860</a> (replaced) [<a href="/pdf/2401.05860" title="Download PDF">pdf</a>, <a href="/format/2401.05860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Confidence-Based Curriculum Learning for Multi-Agent Path Finding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Phan%2C+T">Thomy Phan</a>, 
<a href="/search/cs?searchtype=author&query=Driscoll%2C+J">Joseph Driscoll</a>, 
<a href="/search/cs?searchtype=author&query=Romberg%2C+J">Justin Romberg</a>, 
<a href="/search/cs?searchtype=author&query=Koenig%2C+S">Sven Koenig</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AAMAS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
</div>
</dd>
<dt><a name="item923">[923]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06790" title="Abstract">arXiv:2401.06790</a> (replaced) [<a href="/pdf/2401.06790" title="Download PDF">pdf</a>, <a href="/format/2401.06790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Zero-shot Prompting in the Automatic Creation and Expansion of  Topic Taxonomies for Tagging Retail Banking Transactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+S.+Moraes%2C+D">Daniel de S. Moraes</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+P+T+C">Pedro T. C. Santos</a>, 
<a href="/search/cs?searchtype=author&query=da+Costa%2C+P+B">Polyana B. da Costa</a>, 
<a href="/search/cs?searchtype=author&query=Pinto%2C+M+A+S">Matheus A. S. Pinto</a>, 
<a href="/search/cs?searchtype=author&query=de+J.+P.+Pinto%2C+I">Ivan de J. P. Pinto</a>, 
<a href="/search/cs?searchtype=author&query=da+Veiga%2C+%C3%81+M+G">&#xc1;lvaro M. G. da Veiga</a>, 
<a href="/search/cs?searchtype=author&query=Colcher%2C+S">Sergio Colcher</a>, 
<a href="/search/cs?searchtype=author&query=Busson%2C+A+J+G">Antonio J. G. Busson</a>, 
<a href="/search/cs?searchtype=author&query=Rocha%2C+R+H">Rafael H. Rocha</a>, 
<a href="/search/cs?searchtype=author&query=Gaio%2C+R">Rennan Gaio</a>, 
<a href="/search/cs?searchtype=author&query=Miceli%2C+R">Rafael Miceli</a>, 
<a href="/search/cs?searchtype=author&query=Tourinho%2C+G">Gabriela Tourinho</a>, 
<a href="/search/cs?searchtype=author&query=Rabaioli%2C+M">Marcos Rabaioli</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+L">Leandro Santos</a>, 
<a href="/search/cs?searchtype=author&query=Marques%2C+F">Fellipe Marques</a>, 
<a href="/search/cs?searchtype=author&query=Favaro%2C+D">David Favaro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item924">[924]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07322" title="Abstract">arXiv:2401.07322</a> (replaced) [<a href="/pdf/2401.07322" title="Download PDF">pdf</a>, <a href="/format/2401.07322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RSUD20K: A Dataset for Road Scene Understanding In Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zunair%2C+H">Hasib Zunair</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+S">Shakib Khan</a>, 
<a href="/search/cs?searchtype=author&query=Hamza%2C+A+B">A. Ben Hamza</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item925">[925]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08468" title="Abstract">arXiv:2401.08468</a> (replaced) [<a href="/pdf/2401.08468" title="Download PDF">pdf</a>, <a href="/format/2401.08468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Keep or toss? A nonparametric score to evaluate solutions for noisy ICA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kumar%2C+S">Syamantak Kumar</a>, 
<a href="/search/math?searchtype=author&query=Sarkar%2C+P">Purnamrita Sarkar</a>, 
<a href="/search/math?searchtype=author&query=Bickel%2C+P">Peter Bickel</a>, 
<a href="/search/math?searchtype=author&query=Bean%2C+D">Derek Bean</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item926">[926]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08513" title="Abstract">arXiv:2401.08513</a> (replaced) [<a href="/pdf/2401.08513" title="Download PDF">pdf</a>, <a href="/format/2401.08513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> X Hacking: The Threat of Misguided AutoML
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Rahul Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Redyuk%2C+S">Sergey Redyuk</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+S">Sumantrak Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Sipka%2C+A">Andrea Sipka</a>, 
<a href="/search/cs?searchtype=author&query=Vollmer%2C+S">Sebastian Vollmer</a>, 
<a href="/search/cs?searchtype=author&query=Selby%2C+D">David Selby</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures, plus supplementary materials
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item927">[927]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.09003" title="Abstract">arXiv:2401.09003</a> (replaced) [<a href="/pdf/2401.09003" title="Download PDF">pdf</a>, <a href="/format/2401.09003" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Augmenting Math Word Problems via Iterative Question Composing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haoxiong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yifan Luo</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+A+C">Andrew Chi-Chih Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item928">[928]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.09192" title="Abstract">arXiv:2401.09192</a> (replaced) [<a href="/pdf/2401.09192" title="Download PDF">pdf</a>, <a href="/format/2401.09192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preparing Lessons for Progressive Training on Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+Y">Yu Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Ye Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yichun Yin</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jiaxin Shi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zenglin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Ming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+L">Lifeng Shang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qun Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item929">[929]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.09417" title="Abstract">arXiv:2401.09417</a> (replaced) [<a href="/pdf/2401.09417" title="Download PDF">pdf</a>, <a href="/format/2401.09417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision Mamba: Efficient Visual Representation Learning with  Bidirectional State Space Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lianghui Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+B">Bencheng Liao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinlong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinggang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress. Code is available at <a href="https://github.com/hustvl/Vim">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item930">[930]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.09615" title="Abstract">arXiv:2401.09615</a> (replaced) [<a href="/pdf/2401.09615" title="Download PDF">pdf</a>, <a href="/ps/2401.09615" title="Download PostScript">ps</a>, <a href="/format/2401.09615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Shortcuts: On the Misleading Promise of NLU in Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bihani%2C+G">Geetanjali Bihani</a>, 
<a href="/search/cs?searchtype=author&query=Rayz%2C+J+T">Julia Taylor Rayz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at HICSS-SDPS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item931">[931]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.09798" title="Abstract">arXiv:2401.09798</a> (replaced) [<a href="/pdf/2401.09798" title="Download PDF">pdf</a>, <a href="/ps/2401.09798" title="Download PostScript">ps</a>, <a href="/format/2401.09798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Takemoto%2C+K">Kazuhiro Takemoto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 4 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item932">[932]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.09834" title="Abstract">arXiv:2401.09834</a> (replaced) [<a href="/pdf/2401.09834" title="Download PDF">pdf</a>, <a href="/ps/2401.09834" title="Download PostScript">ps</a>, <a href="/format/2401.09834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence of a spatial semidiscretization for a three-dimensional  stochastic Allen-Cahn equation with multiplicative noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+B">Binjie Li</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+Q">Qin Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item933">[933]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10386" title="Abstract">arXiv:2401.10386</a> (replaced) [<a href="/pdf/2401.10386" title="Download PDF">pdf</a>, <a href="/format/2401.10386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Noninvasive Acute Compartment Syndrome Diagnosis Using Random Forest  Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hweij%2C+Z+A">Zaina Abu Hweij</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+F">Florence Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sophie Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP); Medical Physics (physics.med-ph)

</div>
</div>
</dd>
<dt><a name="item934">[934]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11356" title="Abstract">arXiv:2401.11356</a> (replaced) [<a href="/pdf/2401.11356" title="Download PDF">pdf</a>, <a href="/format/2401.11356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ProLex: A Benchmark for Language Proficiency-oriented Lexical  Substitution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuanming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zixun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhou Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item935">[935]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11491" title="Abstract">arXiv:2401.11491</a> (replaced) [<a href="/pdf/2401.11491" title="Download PDF">pdf</a>, <a href="/ps/2401.11491" title="Download PostScript">ps</a>, <a href="/format/2401.11491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BA-LINS: A Frame-to-Frame Bundle Adjustment for LiDAR-Inertial  Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Hailiang Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tisheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+M">Man Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+X">Xiaoji Niu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item936">[936]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11535" title="Abstract">arXiv:2401.11535</a> (replaced) [<a href="/pdf/2401.11535" title="Download PDF">pdf</a>, <a href="/format/2401.11535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EndoGS: Deformable Endoscopic Tissues Reconstruction with Gaussian  Splatting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lingting Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+J">Jiahao Cui</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhenchao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+G">Guying Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lequan Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item937">[937]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12012" title="Abstract">arXiv:2401.12012</a> (replaced) [<a href="/pdf/2401.12012" title="Download PDF">pdf</a>, <a href="/format/2401.12012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for  Lazy Clients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengdi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bodonhelyi%2C+A">Anna Bodonhelyi</a>, 
<a href="/search/cs?searchtype=author&query=Bozkir%2C+E">Efe Bozkir</a>, 
<a href="/search/cs?searchtype=author&query=Kasneci%2C+E">Enkelejda Kasneci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the AAAI Conference on Artificial Intelligence 2024 (AAAI'24)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the AAAI Conference on Artificial Intelligence 2024
  (AAAI'24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item938">[938]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13597" title="Abstract">arXiv:2401.13597</a> (replaced) [<a href="/pdf/2401.13597" title="Download PDF">pdf</a>, <a href="/ps/2401.13597" title="Download PostScript">ps</a>, <a href="/format/2401.13597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Base-extension Semantics for Modal Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Eckhardt%2C+T">Timo Eckhardt</a>, 
<a href="/search/math?searchtype=author&query=Pym%2C+D+J">David J. Pym</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to be published in the Logic Journal of the IGPL
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item939">[939]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13695" title="Abstract">arXiv:2401.13695</a> (replaced) [<a href="/pdf/2401.13695" title="Download PDF">pdf</a>, <a href="/format/2401.13695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inverse analysis of granular flows using differentiable graph neural  network simulator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Choi%2C+Y">Yongjin Choi</a>, 
<a href="/search/physics?searchtype=author&query=Kumar%2C+K">Krishna Kumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Geophysics (physics.geo-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item940">[940]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14215" title="Abstract">arXiv:2401.14215</a> (replaced) [<a href="/pdf/2401.14215" title="Download PDF">pdf</a>, <a href="/format/2401.14215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Commonsense-augmented Memory Construction and Management in Long-term  Conversations via Context-aware Persona Refinement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hana Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+K+T">Kai Tzu-iunn Ong</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seoyeon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dongha Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yeo%2C+J">Jinyoung Yeo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item941">[941]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14606" title="Abstract">arXiv:2401.14606</a> (replaced) [<a href="/pdf/2401.14606" title="Download PDF">pdf</a>, <a href="/format/2401.14606" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Challenging Low Homophily in Social Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xinyi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Guandong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hongzhi Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by The Web Conference (WWW) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item942">[942]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14876" title="Abstract">arXiv:2401.14876</a> (replaced) [<a href="/pdf/2401.14876" title="Download PDF">pdf</a>, <a href="/format/2401.14876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Space Adaptive Filter: Integrating Graph Topology and Node  Attributes for Alleviating the Over-smoothing Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+W">Wenqiang Lei</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+J">Jiancheng Lv</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to WWW 2024. V2: update the results on GCN-BC based on our rebuttal on OpenReview. Our code is available at <a href="https://github.com/huangzichun/Cross-Space-Adaptive-Filter">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item943">[943]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15042" title="Abstract">arXiv:2401.15042</a> (replaced) [<a href="/pdf/2401.15042" title="Download PDF">pdf</a>, <a href="/format/2401.15042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PROXYQA: An Alternative Framework for Evaluating Long-Form Text  Generation with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+H">Haochen Tan</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhijiang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zhan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhili Liu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yunlong Feng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoguang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yasheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+L">Lifeng Shang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linqi Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item944">[944]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15284" title="Abstract">arXiv:2401.15284</a> (replaced) [<a href="/pdf/2401.15284" title="Download PDF">pdf</a>, <a href="/ps/2401.15284" title="Download PostScript">ps</a>, <a href="/format/2401.15284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Five ethical principles for generative AI in scientific research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhicheng Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item945">[945]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15292" title="Abstract">arXiv:2401.15292</a> (replaced) [<a href="/pdf/2401.15292" title="Download PDF">pdf</a>, <a href="/format/2401.15292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Block Sparse Regularization under Arbitrary Linear Transform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Furuhashi%2C+T">Takanobu Furuhashi</a>, 
<a href="/search/cs?searchtype=author&query=Hontani%2C+H">Hidekata Hontani</a>, 
<a href="/search/cs?searchtype=author&query=Yokota%2C+T">Tatsuya Yokota</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item946">[946]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15741" title="Abstract">arXiv:2401.15741</a> (replaced) [<a href="/pdf/2401.15741" title="Download PDF">pdf</a>, <a href="/ps/2401.15741" title="Download PostScript">ps</a>, <a href="/format/2401.15741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SERNet-Former: Semantic Segmentation by Efficient Residual Network with  Attention-Boosting Gates and Attention-Fusion Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Erisen%2C+S">Serdar Erisen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item947">[947]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16113" title="Abstract">arXiv:2401.16113</a> (replaced) [<a href="/pdf/2401.16113" title="Download PDF">pdf</a>, <a href="/ps/2401.16113" title="Download PostScript">ps</a>, <a href="/format/2401.16113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A parallel preconditioner for the all-at-once linear system from  evolutionary PDEs with Crank-Nicolson discretization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhao%2C+Y">Yong-Liang Zhao</a>, 
<a href="/search/math?searchtype=author&query=Gu%2C+X">Xian-Ming Gu</a>, 
<a href="/search/math?searchtype=author&query=Oosterlee%2C+C+W">Cornelis W. Oosterlee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 5 figures and 4 tables (update some contexts)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item948">[948]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16184" title="Abstract">arXiv:2401.16184</a> (replaced) [<a href="/pdf/2401.16184" title="Download PDF">pdf</a>, <a href="/format/2401.16184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Semantics of LM Latent Space: A Vocabulary-defined Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jian Gu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chunyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Aleti%2C+A">Aldeida Aleti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under peer-review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item949">[949]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16327" title="Abstract">arXiv:2401.16327</a> (replaced) [<a href="/pdf/2401.16327" title="Download PDF">pdf</a>, <a href="/format/2401.16327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PICL: Physics Informed Contrastive Learning for Partial Differential  Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lorsung%2C+C">Cooper Lorsung</a>, 
<a href="/search/cs?searchtype=author&query=Farimani%2C+A+B">Amir Barati Farimani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA); Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item950">[950]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16450" title="Abstract">arXiv:2401.16450</a> (replaced) [<a href="/pdf/2401.16450" title="Download PDF">pdf</a>, <a href="/format/2401.16450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ACCESS: Prompt Engineering for Automated Web Accessibility Violation  Corrections
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Calista Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+A">Alyssa Ma</a>, 
<a href="/search/cs?searchtype=author&query=Vyasamudri%2C+S">Suchir Vyasamudri</a>, 
<a href="/search/cs?searchtype=author&query=Puype%2C+E">Eugenie Puype</a>, 
<a href="/search/cs?searchtype=author&query=Kamal%2C+S">Sayem Kamal</a>, 
<a href="/search/cs?searchtype=author&query=Garcia%2C+J+B">Juan Belza Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Cheema%2C+S">Salar Cheema</a>, 
<a href="/search/cs?searchtype=author&query=Lutz%2C+M">Michael Lutz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item951">[951]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16852" title="Abstract">arXiv:2401.16852</a> (replaced) [<a href="/pdf/2401.16852" title="Download PDF">pdf</a>, <a href="/format/2401.16852" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Checkmating One, by Using Many: Combining Mixture of Experts with MCTS  to Improve in Chess
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Helfenstein%2C+F">Felix Helfenstein</a>, 
<a href="/search/cs?searchtype=author&query=Bl%C3%BCml%2C+J">Jannis Bl&#xfc;ml</a>, 
<a href="/search/cs?searchtype=author&query=Czech%2C+J">Johannes Czech</a>, 
<a href="/search/cs?searchtype=author&query=Kersting%2C+K">Kristian Kersting</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code available under <a href="https://github.com/HelpstoneX/CrazyAra">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item952">[952]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17083" title="Abstract">arXiv:2401.17083</a> (replaced) [<a href="/pdf/2401.17083" title="Download PDF">pdf</a>, <a href="/format/2401.17083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Robot Navigation and Manipulation with Distilled Vision-Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kangcheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xinhu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chaoqun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hesheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Ming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+K">Kai Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICRA 2024 (Oral), Prof. Kangcheng Liu is the corresponding author
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item953">[953]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17345" title="Abstract">arXiv:2401.17345</a> (replaced) [<a href="/pdf/2401.17345" title="Download PDF">pdf</a>, <a href="/ps/2401.17345" title="Download PostScript">ps</a>, <a href="/format/2401.17345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reproducibility, energy efficiency and performance of pseudorandom  number generators in machine learning: a comparative study of python, numpy,  tensorflow, and pytorch implementations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Antunes%2C+B">Benjamin Antunes</a>, 
<a href="/search/cs?searchtype=author&query=Hill%2C+D+R+C">David R.C Hill</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 10 tables, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Software (cs.MS)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item954">[954]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17806" title="Abstract">arXiv:2401.17806</a> (replaced) [<a href="/pdf/2401.17806" title="Download PDF">pdf</a>, <a href="/format/2401.17806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A new class of efficient high order semi-Lagrangian IMEX discontinuous  Galerkin methods on staggered unstructured meshes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Tavelli%2C+M">M. Tavelli</a>, 
<a href="/search/math?searchtype=author&query=Boscheri%2C+W">W. Boscheri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item955">[955]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17823" title="Abstract">arXiv:2401.17823</a> (replaced) [<a href="/pdf/2401.17823" title="Download PDF">pdf</a>, <a href="/format/2401.17823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-preserving data release leveraging optimal transport and  particle gradient descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Donhauser%2C+K">Konstantin Donhauser</a>, 
<a href="/search/cs?searchtype=author&query=Abad%2C+J">Javier Abad</a>, 
<a href="/search/cs?searchtype=author&query=Hulkund%2C+N">Neha Hulkund</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fanny Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item956">[956]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.18050" title="Abstract">arXiv:2401.18050</a> (replaced) [<a href="/pdf/2401.18050" title="Download PDF">pdf</a>, <a href="/ps/2401.18050" title="Download PostScript">ps</a>, <a href="/format/2401.18050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hypermultiplexed Integrated Tensor Optical Processor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ou%2C+S">Shaoyuan Ou</a>, 
<a href="/search/cs?searchtype=author&query=Sludds%2C+A">Alexander Sludds</a>, 
<a href="/search/cs?searchtype=author&query=Hamerly%2C+R">Ryan Hamerly</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Ke Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+H">Hanke Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+E">Eric Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Englund%2C+D">Dirk Englund</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+M">Mengjie Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zaijun Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Optics (physics.optics)

</div>
</div>
</dd>
<dt><a name="item957">[957]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00326" title="Abstract">arXiv:2402.00326</a> (replaced) [<a href="/pdf/2402.00326" title="Download PDF">pdf</a>, <a href="/format/2402.00326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PirateNets: Physics-informed Deep Learning with Residual Adaptive  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bowen Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuhan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Perdikaris%2C+P">Paris Perdikaris</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30Pages, 15 Figures, 8 Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item958">[958]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00654" title="Abstract">arXiv:2402.00654</a> (replaced) [<a href="/pdf/2402.00654" title="Download PDF">pdf</a>, <a href="/ps/2402.00654" title="Download PostScript">ps</a>, <a href="/format/2402.00654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving the accuracy of freight mode choice models: A case study using  the 2017 CFS PUF data set and ensemble learning techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Diyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+H">Hyeonsup Lim</a>, 
<a href="/search/cs?searchtype=author&query=Uddin%2C+M">Majbah Uddin</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuandong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+L+D">Lee D. Han</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+H">Ho-ling Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Chin%2C+S">Shih-Miao Chin</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Expert Systems with Applications, 240, 122478 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item959">[959]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00675" title="Abstract">arXiv:2402.00675</a> (replaced) [<a href="/pdf/2402.00675" title="Download PDF">pdf</a>, <a href="/ps/2402.00675" title="Download PostScript">ps</a>, <a href="/format/2402.00675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Modular Algorithms and Butterfly Operations in Number Theoretic  Transform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yanze Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Y">Yiran Jia</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Guangwu Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item960">[960]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00746" title="Abstract">arXiv:2402.00746</a> (replaced) [<a href="/pdf/2402.00746" title="Download PDF">pdf</a>, <a href="/format/2402.00746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+M">Mingyu Jin</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qinkai Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+D">Dong Shu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Suiyuan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+M">Mengnan Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongfeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Y">Yanda Meng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item961">[961]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01254" title="Abstract">arXiv:2402.01254</a> (replaced) [<a href="/pdf/2402.01254" title="Download PDF">pdf</a>, <a href="/format/2402.01254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Trajectory Model: Implicit Neural Trajectory Representation for  Trajectories Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zihan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yuqing Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item962">[962]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01276" title="Abstract">arXiv:2402.01276</a> (replaced) [<a href="/pdf/2402.01276" title="Download PDF">pdf</a>, <a href="/format/2402.01276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Unlearning: a Perspective of Stability and Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+J">Jiaqi Shao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+T">Tao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xuanyu Cao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+B">Bing Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item963">[963]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01345" title="Abstract">arXiv:2402.01345</a> (replaced) [<a href="/pdf/2402.01345" title="Download PDF">pdf</a>, <a href="/format/2402.01345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Skip \n: A Simple Method to Reduce Hallucination in Large  Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zongbo Han</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Z">Zechen Bai</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+H">Haiyang Mei</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qianli Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Changqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shou%2C+M+Z">Mike Zheng Shou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item964">[964]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01350" title="Abstract">arXiv:2402.01350</a> (replaced) [<a href="/pdf/2402.01350" title="Download PDF">pdf</a>, <a href="/format/2402.01350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> pFedMoE: Data-Level Personalization with Mixture of Experts for  Model-Heterogeneous Personalized Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+L">Liping Yi</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Han Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+C">Chao Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Heng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Gang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoguang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoxiao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item965">[965]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01713" title="Abstract">arXiv:2402.01713</a> (replaced) [<a href="/pdf/2402.01713" title="Download PDF">pdf</a>, <a href="/format/2402.01713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting Large Language Models for Zero-Shot Clinical Prediction with  Structured Longitudinal Electronic Health Record Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yinghao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zixiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Junyi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+Y">Yuning Tong</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+J">Jingkun An</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+W">Weibin Liao</a>, 
<a href="/search/cs?searchtype=author&query=Harrison%2C+E+M">Ewen M. Harrison</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Liantao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+C">Chengwei Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item966">[966]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01735" title="Abstract">arXiv:2402.01735</a> (replaced) [<a href="/pdf/2402.01735" title="Download PDF">pdf</a>, <a href="/format/2402.01735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yilin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+R">Rong Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jing Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hillming Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item967">[967]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01885" title="Abstract">arXiv:2402.01885</a> (replaced) [<a href="/pdf/2402.01885" title="Download PDF">pdf</a>, <a href="/ps/2402.01885" title="Download PostScript">ps</a>, <a href="/format/2402.01885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Growth Mindset Practices in an Introductory Physical  Computing Classroom: High School Students&#x27; Engagement with Debugging by  Design Activities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morales-Navarro%2C+L">Luis Morales-Navarro</a>, 
<a href="/search/cs?searchtype=author&query=Fields%2C+D+A">Deborah A. Fields</a>, 
<a href="/search/cs?searchtype=author&query=Kafai%2C+Y+B">Yasmin B. Kafai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Physics Education (physics.ed-ph)

</div>
</div>
</dd>
<dt><a name="item968">[968]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02047" title="Abstract">arXiv:2402.02047</a> (replaced) [<a href="/pdf/2402.02047" title="Download PDF">pdf</a>, <a href="/format/2402.02047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quality and Trust in LLM-generated Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Spiess%2C+C">Claudio Spiess</a>, 
<a href="/search/cs?searchtype=author&query=Gros%2C+D">David Gros</a>, 
<a href="/search/cs?searchtype=author&query=Pai%2C+K+S">Kunal Suresh Pai</a>, 
<a href="/search/cs?searchtype=author&query=Pradel%2C+M">Michael Pradel</a>, 
<a href="/search/cs?searchtype=author&query=Rabin%2C+M+R+I">Md Rafiqul Islam Rabin</a>, 
<a href="/search/cs?searchtype=author&query=Alipour%2C+A">Amin Alipour</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+S">Susmit Jha</a>, 
<a href="/search/cs?searchtype=author&query=Devanbu%2C+P">Prem Devanbu</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+T">Toufique Ahmed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item969">[969]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02209" title="Abstract">arXiv:2402.02209</a> (replaced) [<a href="/pdf/2402.02209" title="Download PDF">pdf</a>, <a href="/format/2402.02209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Exploitation of DCT-Traces in the Generative-AI Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pontorno%2C+O">Orazio Pontorno</a> (1), 
<a href="/search/cs?searchtype=author&query=Guarnera%2C+L">Luca Guarnera</a> (1), 
<a href="/search/cs?searchtype=author&query=Battiato%2C+S">Sebastiano Battiato</a> (1) ((1) University of Catania)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item970">[970]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02303" title="Abstract">arXiv:2402.02303</a> (replaced) [<a href="/pdf/2402.02303" title="Download PDF">pdf</a>, <a href="/format/2402.02303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bootstrapping Fisher Market Equilibrium and First-Price Pacing  Equilibrium
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liao%2C+L">Luofeng Liao</a>, 
<a href="/search/math?searchtype=author&query=Kroer%2C+C">Christian Kroer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> fix author names
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Computer Science and Game Theory (cs.GT); Econometrics (econ.EM); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item971">[971]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02317" title="Abstract">arXiv:2402.02317</a> (replaced) [<a href="/pdf/2402.02317" title="Download PDF">pdf</a>, <a href="/format/2402.02317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> INViT: A Generalizable Routing Problem Solver with Invariant Nested View  Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+H">Han Fang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Z">Zhihao Song</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+P">Paul Weng</a>, 
<a href="/search/cs?searchtype=author&query=Ban%2C+Y">Yutong Ban</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item972">[972]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02342" title="Abstract">arXiv:2402.02342</a> (replaced) [<a href="/pdf/2402.02342" title="Download PDF">pdf</a>, <a href="/format/2402.02342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MetaOptimize: A Framework for Optimizing Step Sizes and Other  Meta-parameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharifnassab%2C+A">Arsalan Sharifnassab</a>, 
<a href="/search/cs?searchtype=author&query=Salehkaleybar%2C+S">Saber Salehkaleybar</a>, 
<a href="/search/cs?searchtype=author&query=Sutton%2C+R">Richard Sutton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item973">[973]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02460" title="Abstract">arXiv:2402.02460</a> (replaced) [<a href="/pdf/2402.02460" title="Download PDF">pdf</a>, <a href="/format/2402.02460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Review of multimodal machine learning approaches in healthcare
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krones%2C+F">Felix Krones</a>, 
<a href="/search/cs?searchtype=author&query=Marikkar%2C+U">Umar Marikkar</a>, 
<a href="/search/cs?searchtype=author&query=Parsons%2C+G">Guy Parsons</a>, 
<a href="/search/cs?searchtype=author&query=Szmul%2C+A">Adam Szmul</a>, 
<a href="/search/cs?searchtype=author&query=Mahdi%2C+A">Adam Mahdi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item974">[974]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02823" title="Abstract">arXiv:2402.02823</a> (replaced) [<a href="/pdf/2402.02823" title="Download PDF">pdf</a>, <a href="/format/2402.02823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evading Data Contamination Detection for Language Models is (too) Easy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dekoninck%2C+J">Jasper Dekoninck</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+M+N">Mark Niklas M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Baader%2C+M">Maximilian Baader</a>, 
<a href="/search/cs?searchtype=author&query=Fischer%2C+M">Marc Fischer</a>, 
<a href="/search/cs?searchtype=author&query=Vechev%2C+M">Martin Vechev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item975">[975]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02954" title="Abstract">arXiv:2402.02954</a> (replaced) [<a href="/pdf/2402.02954" title="Download PDF">pdf</a>, <a href="/format/2402.02954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving Hierarchical Information-Sharing Dec-POMDPs: An Extensive-Form  Game Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peralez%2C+J">Johan Peralez</a>, 
<a href="/search/cs?searchtype=author&query=Delage%2C+A">Aur&#xe9;lien Delage</a>, 
<a href="/search/cs?searchtype=author&query=Buffet%2C+O">Olivier Buffet</a>, 
<a href="/search/cs?searchtype=author&query=Dibangoye%2C+J+S">Jilles S. Dibangoye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item976">[976]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03216" title="Abstract">arXiv:2402.03216</a> (replaced) [<a href="/pdf/2402.03216" title="Download PDF">pdf</a>, <a href="/format/2402.03216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity  Text Embeddings Through Self-Knowledge Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianlv Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+S">Shitao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peitian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+K">Kun Luo</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+D">Defu Lian</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zheng Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item977">[977]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03279" title="Abstract">arXiv:2402.03279</a> (replaced) [<a href="/pdf/2402.03279" title="Download PDF">pdf</a>, <a href="/format/2402.03279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stepping into the Right Shoes: The Effects of User-Matched Avatar  Ethnicity and Gender on Sense of Embodiment in Virtual Reality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Do%2C+T+D">Tiffany D. Do</a>, 
<a href="/search/cs?searchtype=author&query=Protko%2C+C+I">Camille Isabella Protko</a>, 
<a href="/search/cs?searchtype=author&query=McMahan%2C+R+P">Ryan P. McMahan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in IEEE Transactions on Visualization and Computer Graphics
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item978">[978]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03312" title="Abstract">arXiv:2402.03312</a> (replaced) [<a href="/pdf/2402.03312" title="Download PDF">pdf</a>, <a href="/format/2402.03312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Test-Time Adaptation for Depth Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+H">Hyoungseob Park</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Anjali Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+A">Alex Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item979">[979]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03570" title="Abstract">arXiv:2402.03570</a> (replaced) [<a href="/pdf/2402.03570" title="Download PDF">pdf</a>, <a href="/format/2402.03570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion World Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Z">Zihan Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+A">Amy Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yuandong Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Q">Qinqing Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item980">[980]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03669" title="Abstract">arXiv:2402.03669</a> (replaced) [<a href="/pdf/2402.03669" title="Download PDF">pdf</a>, <a href="/format/2402.03669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Generalized Nash Equilibria Seeking Algorithms Involving  Synchronous and Asynchronous Schemes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huaqing Li</a>, 
<a href="/search/cs?searchtype=author&query=Ran%2C+L">Liang Ran</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Lifeng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhe Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jinhui Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tingwen Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item981">[981]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03681" title="Abstract">arXiv:2402.03681</a> (replaced) [<a href="/pdf/2402.03681" title="Download PDF">pdf</a>, <a href="/format/2402.03681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model  Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yufei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhanyi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jesse Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xian%2C+Z">Zhou Xian</a>, 
<a href="/search/cs?searchtype=author&query=Biyik%2C+E">Erdem Biyik</a>, 
<a href="/search/cs?searchtype=author&query=Held%2C+D">David Held</a>, 
<a href="/search/cs?searchtype=author&query=Erickson%2C+Z">Zackory Erickson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item982">[982]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03776" title="Abstract">arXiv:2402.03776</a> (replaced) [<a href="/pdf/2402.03776" title="Download PDF">pdf</a>, <a href="/ps/2402.03776" title="Download PostScript">ps</a>, <a href="/format/2402.03776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models As MOOCs Graders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Golchin%2C+S">Shahriar Golchin</a>, 
<a href="/search/cs?searchtype=author&query=Garuda%2C+N">Nikhil Garuda</a>, 
<a href="/search/cs?searchtype=author&query=Impey%2C+C">Christopher Impey</a>, 
<a href="/search/cs?searchtype=author&query=Wenger%2C+M">Matthew Wenger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> v1.1 preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item983">[983]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03861" title="Abstract">arXiv:2402.03861</a> (replaced) [<a href="/pdf/2402.03861" title="Download PDF">pdf</a>, <a href="/format/2402.03861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Bernoulli-barycentric rational matrix collocation method with  preconditioning for a class of evolutionary PDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Luo%2C+W">Wei-Hua Luo</a>, 
<a href="/search/math?searchtype=author&query=Gu%2C+X">Xian-Ming Gu</a>, 
<a href="/search/math?searchtype=author&query=Carpentieri%2C+B">Bruno Carpentieri</a>, 
<a href="/search/math?searchtype=author&query=Guo%2C+J">Jun Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 6 figures, 9 tables (update some contexts)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item984">[984]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04384" title="Abstract">arXiv:2402.04384</a> (replaced) [<a href="/pdf/2402.04384" title="Download PDF">pdf</a>, <a href="/format/2402.04384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising Diffusion Probabilistic Models in Six Simple Steps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Turner%2C+R+E">Richard E. Turner</a>, 
<a href="/search/cs?searchtype=author&query=Diaconu%2C+C">Cristiana-Diana Diaconu</a>, 
<a href="/search/cs?searchtype=author&query=Markou%2C+S">Stratis Markou</a>, 
<a href="/search/cs?searchtype=author&query=Shysheya%2C+A">Aliaksandra Shysheya</a>, 
<a href="/search/cs?searchtype=author&query=Foong%2C+A+Y+K">Andrew Y. K. Foong</a>, 
<a href="/search/cs?searchtype=author&query=Mlodozeniec%2C+B">Bruno Mlodozeniec</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item985">[985]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04554" title="Abstract">arXiv:2402.04554</a> (replaced) [<a href="/pdf/2402.04554" title="Download PDF">pdf</a>, <a href="/format/2402.04554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BirdNeRF: Fast Neural Reconstruction of Large-Scale Scenes From Aerial  Imagery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Huiqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Y">Yifei Xue</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+M">Ming Liao</a>, 
<a href="/search/cs?searchtype=author&query=Lao%2C+Y">Yizhen Lao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item986">[986]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04844" title="Abstract">arXiv:2402.04844</a> (replaced) [<a href="/pdf/2402.04844" title="Download PDF">pdf</a>, <a href="/format/2402.04844" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reconfigurable Intelligent Surface for Industrial Automation: mmWave  Propagation Measurement, Simulation, and Control Algorithm Requirements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Radpour%2C+H">Hamed Radpour</a>, 
<a href="/search/eess?searchtype=author&query=Hofer%2C+M">Markus Hofer</a>, 
<a href="/search/eess?searchtype=author&query=Loschenbrand%2C+D">David Loschenbrand</a>, 
<a href="/search/eess?searchtype=author&query=Mayer%2C+L+W">Lukas Walter Mayer</a>, 
<a href="/search/eess?searchtype=author&query=Hofmann%2C+A">Andreas Hofmann</a>, 
<a href="/search/eess?searchtype=author&query=Schiefer%2C+M">Martin Schiefer</a>, 
<a href="/search/eess?searchtype=author&query=Zemen%2C+T">Thomas Zemen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to IEEE International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC), Valencia, Spain, September 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item987">[987]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04881" title="Abstract">arXiv:2402.04881</a> (replaced) [<a href="/pdf/2402.04881" title="Download PDF">pdf</a>, <a href="/format/2402.04881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Epistral Network: Revolutionizing Media Curation and Consumption through  Decentralization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+D">Dipankar Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Upadhyay%2C+S">Shubham Upadhyay</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item988">[988]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05116" title="Abstract">arXiv:2402.05116</a> (replaced) [<a href="/pdf/2402.05116" title="Download PDF">pdf</a>, <a href="/format/2402.05116" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying Similarity: Text-Mining Approaches to Evaluate ChatGPT and  Google Bard Content in Relation to BioMedical Literature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Klimczak%2C+J">Jakub Klimczak</a>, 
<a href="/search/cs?searchtype=author&query=Hamed%2C+A+A">Ahmed Abdeen Hamed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 10 figures, 4 tables; and 1 algorithm
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Digital Libraries (cs.DL); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item989">[989]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05131" title="Abstract">arXiv:2402.05131</a> (replaced) [<a href="/pdf/2402.05131" title="Download PDF">pdf</a>, <a href="/ps/2402.05131" title="Download PostScript">ps</a>, <a href="/format/2402.05131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Financial Report Chunking for Effective Retrieval Augmented Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yepes%2C+A+J">Antonio Jimeno Yepes</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yao You</a>, 
<a href="/search/cs?searchtype=author&query=Milczek%2C+J">Jan Milczek</a>, 
<a href="/search/cs?searchtype=author&query=Laverde%2C+S">Sebastian Laverde</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Renyu Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item990">[990]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05147" title="Abstract">arXiv:2402.05147</a> (replaced) [<a href="/pdf/2402.05147" title="Download PDF">pdf</a>, <a href="/format/2402.05147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ApiQ: Finetuning of 2-Bit Quantized Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+B">Baohao Liao</a>, 
<a href="/search/cs?searchtype=author&query=Monz%2C+C">Christof Monz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> compared to v0: new histogram formats for better reading
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item991">[991]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05210" title="Abstract">arXiv:2402.05210</a> (replaced) [<a href="/pdf/2402.05210" title="Download PDF">pdf</a>, <a href="/format/2402.05210" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anatomically-Controllable Medical Image Generation with  Segmentation-Guided Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Konz%2C+N">Nicholas Konz</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Y">Yuwen Chen</a>, 
<a href="/search/eess?searchtype=author&query=Dong%2C+H">Haoyu Dong</a>, 
<a href="/search/eess?searchtype=author&query=Mazurowski%2C+M+A">Maciej A. Mazurowski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and synthetic dataset: <a href="https://github.com/mazurowski-lab/segmentation-guided-diffusion">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item992">[992]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05290" title="Abstract">arXiv:2402.05290</a> (replaced) [<a href="/pdf/2402.05290" title="Download PDF">pdf</a>, <a href="/format/2402.05290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Transformer World Models Give Better Policy Gradients?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+M">Michel Ma</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+T">Tianwei Ni</a>, 
<a href="/search/cs?searchtype=author&query=Gehring%2C+C">Clement Gehring</a>, 
<a href="/search/cs?searchtype=author&query=D%27Oro%2C+P">Pierluca D&#x27;Oro</a>, 
<a href="/search/cs?searchtype=author&query=Bacon%2C+P">Pierre-Luc Bacon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Michel Ma and Pierluca D'Oro contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item993">[993]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05301" title="Abstract">arXiv:2402.05301</a> (replaced) [<a href="/pdf/2402.05301" title="Download PDF">pdf</a>, <a href="/format/2402.05301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and  Parametric CAD Designs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Regenwetter%2C+L">Lyle Regenwetter</a>, 
<a href="/search/cs?searchtype=author&query=Obaideh%2C+Y+A">Yazan Abu Obaideh</a>, 
<a href="/search/cs?searchtype=author&query=Nobari%2C+A+H">Amin Heyrani Nobari</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+F">Faez Ahmed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item994">[994]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05403" title="Abstract">arXiv:2402.05403</a> (replaced) [<a href="/pdf/2402.05403" title="Download PDF">pdf</a>, <a href="/format/2402.05403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Principle Learning from Mistakes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianjun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Madaan%2C+A">Aman Madaan</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+L">Luyu Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Steven Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Swaroop Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yiming Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tandon%2C+N">Niket Tandon</a>, 
<a href="/search/cs?searchtype=author&query=Alon%2C+U">Uri Alon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item995">[995]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05406" title="Abstract">arXiv:2402.05406</a> (replaced) [<a href="/pdf/2402.05406" title="Download PDF">pdf</a>, <a href="/format/2402.05406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dery%2C+L">Lucio Dery</a>, 
<a href="/search/cs?searchtype=author&query=Kolawole%2C+S">Steven Kolawole</a>, 
<a href="/search/cs?searchtype=author&query=Kagy%2C+J">Jean-Fran&#xe7;ois Kagy</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+V">Virginia Smith</a>, 
<a href="/search/cs?searchtype=author&query=Neubig%2C+G">Graham Neubig</a>, 
<a href="/search/cs?searchtype=author&query=Talwalkar%2C+A">Ameet Talwalkar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 4 fiigures, 15 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item996">[996]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05564" title="Abstract">arXiv:2402.05564</a> (replaced) [<a href="/e-print/2402.05564" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Game-Theoretical Approach for Optimal Supervisory Control of Discrete  Event Systems under Energy Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lv%2C+P">Peng Lv</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+S">Shaoyuan Li</a>, 
<a href="/search/eess?searchtype=author&query=Yin%2C+X">Xiang Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We will add richer content
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item997">[997]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05589" title="Abstract">arXiv:2402.05589</a> (replaced) [<a href="/pdf/2402.05589" title="Download PDF">pdf</a>, <a href="/format/2402.05589" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RESMatch: Referring Expression Segmentation in a Semi-Supervised Manner
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zang%2C+Y">Ying Zang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+C">Chenglong Fu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+R">Runlong Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+D">Didi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+W">Wenjun Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Lanyun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianrun Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item998">[998]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05631" title="Abstract">arXiv:2402.05631</a> (replaced) [<a href="/pdf/2402.05631" title="Download PDF">pdf</a>, <a href="/format/2402.05631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ShiftDTW: adapting the DTW metric for cyclic time series clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Foulon%2C+L">Lucas Foulon</a> (DM), 
<a href="/search/cs?searchtype=author&query=Korichi%2C+I">Ilyes Korichi</a> (DM), 
<a href="/search/cs?searchtype=author&query=Millot%2C+X">Xavier Millot</a> (Oxtaam)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> in French language. GAST@EGC 2024 : Atelier Gestion et Analyse des donn{\'e}es Spatiales et Temporelles, Aur{\'e}lie Leborgne; Nida Meddouri; Lo{\"i}c Salmon; Cl{\'e}ment Iphar, Jan 2024, Dijon, France
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item999">[999]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05783" title="Abstract">arXiv:2402.05783</a> (replaced) [<a href="/pdf/2402.05783" title="Download PDF">pdf</a>, <a href="/format/2402.05783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text-to-Code Generation with Modality-relative Pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Christopoulou%2C+F">Fenia Christopoulou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guchun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lampouras%2C+G">Gerasimos Lampouras</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at EACL 2024. 15 pages, 5 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1000">[1000]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05817" title="Abstract">arXiv:2402.05817</a> (replaced) [<a href="/pdf/2402.05817" title="Download PDF">pdf</a>, <a href="/ps/2402.05817" title="Download PostScript">ps</a>, <a href="/format/2402.05817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using YOLO v7 to Detect Kidney in Magnetic Resonance Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Anari%2C+P+Y">Pouria Yazdian Anari</a>, 
<a href="/search/eess?searchtype=author&query=Obiezu%2C+F">Fiona Obiezu</a>, 
<a href="/search/eess?searchtype=author&query=Lay%2C+N">Nathan Lay</a>, 
<a href="/search/eess?searchtype=author&query=Firouzabadi%2C+F+D">Fatemeh Dehghani Firouzabadi</a>, 
<a href="/search/eess?searchtype=author&query=Chaurasia%2C+A">Aditi Chaurasia</a>, 
<a href="/search/eess?searchtype=author&query=Golagha%2C+M">Mahshid Golagha</a>, 
<a href="/search/eess?searchtype=author&query=Singh%2C+S">Shiva Singh</a>, 
<a href="/search/eess?searchtype=author&query=Homayounieh%2C+F">Fatemeh Homayounieh</a>, 
<a href="/search/eess?searchtype=author&query=Zahergivar%2C+A">Aryan Zahergivar</a>, 
<a href="/search/eess?searchtype=author&query=Harmon%2C+S">Stephanie Harmon</a>, 
<a href="/search/eess?searchtype=author&query=Turkbey%2C+E">Evrim Turkbey</a>, 
<a href="/search/eess?searchtype=author&query=Gautam%2C+R">Rabindra Gautam</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+K">Kevin Ma</a>, 
<a href="/search/eess?searchtype=author&query=Merino%2C+M">Maria Merino</a>, 
<a href="/search/eess?searchtype=author&query=Jones%2C+E+C">Elizabeth C. Jones</a>, 
<a href="/search/eess?searchtype=author&query=Ball%2C+M+W">Mark W. Ball</a>, 
<a href="/search/eess?searchtype=author&query=Linehan%2C+W+M">W. Marston Linehan</a>, 
<a href="/search/eess?searchtype=author&query=Turkbey%2C+B">Baris Turkbey</a>, 
<a href="/search/eess?searchtype=author&query=Malayeri%2C+A+A">Ashkan A. Malayeri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1001">[1001]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05868" title="Abstract">arXiv:2402.05868</a> (replaced) [<a href="/pdf/2402.05868" title="Download PDF">pdf</a>, <a href="/format/2402.05868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EmojiCrypt: Prompt Encryption for Secure Communication with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+G">Guo Lin</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+W">Wenyue Hua</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongfeng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 4 figures, 2 tables, comments and suggestions are welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1002">[1002]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05880" title="Abstract">arXiv:2402.05880</a> (replaced) [<a href="/pdf/2402.05880" title="Download PDF">pdf</a>, <a href="/format/2402.05880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Echo Chamber? Effects of LLM-Powered Search Systems on  Diverse Information Seeking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+N">Nikhil Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Q+V">Q. Vera Liao</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Ziang Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in CHI'24. Supplementary material will be available online with the official submission in CHI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item1003">[1003]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05902" title="Abstract">arXiv:2402.05902</a> (replaced) [<a href="/pdf/2402.05902" title="Download PDF">pdf</a>, <a href="/ps/2402.05902" title="Download PostScript">ps</a>, <a href="/format/2402.05902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ClickSAM: Fine-tuning Segment Anything Model using click prompts for  ultrasound image segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+A">Aimee Guo</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+G">Gace Fei</a>, 
<a href="/search/cs?searchtype=author&query=Pasupuletic%2C+H">Hemanth Pasupuletic</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jing Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures, SPIE Medical Imaging Conference 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Medical Physics (physics.med-ph)

</div>
</div>
</dd>
<dt><a name="item1004">[1004]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05950" title="Abstract">arXiv:2402.05950</a> (replaced) [<a href="/pdf/2402.05950" title="Download PDF">pdf</a>, <a href="/format/2402.05950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SQT -- std $Q$-target
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soffair%2C+N">Nitsan Soffair</a>, 
<a href="/search/cs?searchtype=author&query=Di-Castro%2C+D">Dotan Di-Castro</a>, 
<a href="/search/cs?searchtype=author&query=Avner%2C+O">Orly Avner</a>, 
<a href="/search/cs?searchtype=author&query=Mannor%2C+S">Shie Mannor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1005">[1005]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05951" title="Abstract">arXiv:2402.05951</a> (replaced) [<a href="/pdf/2402.05951" title="Download PDF">pdf</a>, <a href="/format/2402.05951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MinMaxMin $Q$-learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soffair%2C+N">Nitsan Soffair</a>, 
<a href="/search/cs?searchtype=author&query=Mannor%2C+S">Shie Mannor</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1006">[1006]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06033" title="Abstract">arXiv:2402.06033</a> (replaced) [<a href="/pdf/2402.06033" title="Download PDF">pdf</a>, <a href="/ps/2402.06033" title="Download PostScript">ps</a>, <a href="/format/2402.06033" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Inexact Halpern Iteration with Application to Distributionally Robust  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liang%2C+L">Ling Liang</a>, 
<a href="/search/math?searchtype=author&query=Toh%2C+K">Kim-Chuan Toh</a>, 
<a href="/search/math?searchtype=author&query=Zhu%2C+J">Jia-Jie Zhu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Correct a typo in the title and update authors' information
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1007">[1007]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06073" title="Abstract">arXiv:2402.06073</a> (replaced) [<a href="/pdf/2402.06073" title="Download PDF">pdf</a>, <a href="/ps/2402.06073" title="Download PostScript">ps</a>, <a href="/format/2402.06073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LightCAM: A Fast and Light Implementation of Context-Aware Masking based  D-TDNN for Speaker Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+D">Di Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xianchen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Junfeng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiakai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+Y">Yanjing Lei</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenpeng Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1008">[1008]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06187" title="Abstract">arXiv:2402.06187</a> (replaced) [<a href="/pdf/2402.06187" title="Download PDF">pdf</a>, <a href="/format/2402.06187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Premier-TACO is a Few-Shot Policy Learner: Pretraining Multitask  Representation via Temporal Action-Driven Contrastive Loss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Ruijie Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yongyuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shuang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Daum%C3%A9%2C+H">Hal Daum&#xe9; III</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Huazhe Xu</a>, 
<a href="/search/cs?searchtype=author&query=Langford%2C+J">John Langford</a>, 
<a href="/search/cs?searchtype=author&query=Palanisamy%2C+P">Praveen Palanisamy</a>, 
<a href="/search/cs?searchtype=author&query=Basu%2C+K+S">Kalyan Shankar Basu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1009">[1009]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06366" title="Abstract">arXiv:2402.06366</a> (replaced) [<a href="/pdf/2402.06366" title="Download PDF">pdf</a>, <a href="/ps/2402.06366" title="Download PostScript">ps</a>, <a href="/format/2402.06366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAT-based Learning of Computation Tree Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pommellet%2C+A">Adrien Pommellet</a>, 
<a href="/search/cs?searchtype=author&query=Stan%2C+D">Daniel Stan</a>, 
<a href="/search/cs?searchtype=author&query=Scatton%2C+S">Simon Scatton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IJCAR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item1010">[1010]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06576" title="Abstract">arXiv:2402.06576</a> (replaced) [<a href="/pdf/2402.06576" title="Download PDF">pdf</a>, <a href="/format/2402.06576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Value-based Resource Matching with Fairness Criteria: Application to  Agricultural Water Trading
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adiga%2C+A">Abhijin Adiga</a>, 
<a href="/search/cs?searchtype=author&query=Trabelsi%2C+Y">Yohai Trabelsi</a>, 
<a href="/search/cs?searchtype=author&query=Ferdousi%2C+T">Tanvir Ferdousi</a>, 
<a href="/search/cs?searchtype=author&query=Marathe%2C+M">Madhav Marathe</a>, 
<a href="/search/cs?searchtype=author&query=Ravi%2C+S+S">S. S. Ravi</a>, 
<a href="/search/cs?searchtype=author&query=Swarup%2C+S">Samarth Swarup</a>, 
<a href="/search/cs?searchtype=author&query=Vullikanti%2C+A+K">Anil Kumar Vullikanti</a>, 
<a href="/search/cs?searchtype=author&query=Wilson%2C+M+L">Mandy L. Wilson</a>, 
<a href="/search/cs?searchtype=author&query=Kraus%2C+S">Sarit Kraus</a>, 
<a href="/search/cs?searchtype=author&query=Basu%2C+R">Reetwika Basu</a>, 
<a href="/search/cs?searchtype=author&query=Savalkar%2C+S">Supriya Savalkar</a>, 
<a href="/search/cs?searchtype=author&query=Yourek%2C+M">Matthew Yourek</a>, 
<a href="/search/cs?searchtype=author&query=Brady%2C+M">Michael Brady</a>, 
<a href="/search/cs?searchtype=author&query=Rajagopalan%2C+K">Kirti Rajagopalan</a>, 
<a href="/search/cs?searchtype=author&query=Yoder%2C+J">Jonathan Yoder</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item559">Cross-lists</a></li>
<li><a href="#item640">Replacements</a></li>
</ul>
<small>[ total of 1010 entries:  <b>1-1010</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2402">2402</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
