<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Thu 15 Feb 24  to  Fri 16 Feb 24, announced Mon, 19 Feb 24</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item307">Cross-lists</a></li>
<li><a href="#item353">Replacements</a></li>
</ul>
<small>[ total of 604 entries:  <b>1-604</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Mon, 19 Feb 24</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10216" title="Abstract">arXiv:2402.10216</a> [<a href="/pdf/2402.10216" title="Download PDF">pdf</a>, <a href="/ps/2402.10216" title="Download PostScript">ps</a>, <a href="/format/2402.10216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Watertightization of Trimmed Surfaces at Intersection Boundary
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hua Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+R">Ruoxi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Z">Zushang Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+R">Rui Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages,6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">This paper introduces a watertight technique to deal with the boundary
representation of surface-surface intersection in CAD.
<br />Surfaces play an important role in today's geometric design. The mathematical
model of non-uniform rational B-spline surfaces (NURBS) is the mainstream and
ISO standard. In the situation of surface-surface intersection, things are a
little complicated, for some parts of surfaces may be cut-off, so called
trimmed surfaces occur, which is the central topic in the past decades in CAD
community of both academia and industry. The main problem is that the
parametric domain of the trimmed surface generally is not the standard square
or rectangle, and rather, typically, bounded by curves, based on point inverse
of the intersection points and interpolated. The existence of gaps or overlaps
at the intersection boundary makes hard the preprocessing of CAE and other
downstream applications. The NURBS are in this case hard to keep a closed form.
In common, a special data structure of intersection curves must be affiliated
to support downstream applications, while the data structure of the whole CAD
system is not unified, and the calculation is not efficient.
<br />In terms of Bezier surface, a special case of NURBS, this paper designs a
reparameterization or normalization to transform the trimmed surface into a
group of Bezier surface patches in standard parametric domain [0,1]X[0,1]. And
then the boundary curve of normalized Bezier surface patch can be replaced by
the intersection curve to realize watertight along the boundary. In this way,
the trimmed surface is wiped out, the "gap" between CAD and CAE is closed.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10217" title="Abstract">arXiv:2402.10217</a> [<a href="/pdf/2402.10217" title="Download PDF">pdf</a>, <a href="/ps/2402.10217" title="Download PostScript">ps</a>, <a href="/format/2402.10217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Penetration Testing and Legacy Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Smyth%2C+S">Sandra Smyth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">As per Adusumilli (2015),'70% of corporate business systems today are legacy
applications. Recent statistics prove that over 60% of IT budget is spent on
maintaining these Legacy systems, showing the rigidity and the fragile nature
of these systems.' Usually, testing is included during the software development
cycle, using testing techniques such as unit testing, integration testing, and
system testing before releasing the product. After the software product is
released to production, no additional testing is done; the testing process is
back to the table only when modifications are made. Techniques such as
regression testing are included to ensure the changes do not affect existing
functionality, but testing nonfunctional features that are rarely included in
such regression tests' scope. Schrader (2021) affirms that 'legacy systems are
often maintained only to ensure function,' and IT organizations may fail to
consider the cybersecurity perspective to remain secure. Legacy systems are a
high-risk component for the organization that must be carefully considered when
structuring a cyber security strategy. This paper aims to help the reader
understand some measures that can be taken to secure legacy systems, explaining
what penetration testing is and how this testing technique can help secure
legacy systems. Keywords: Testing, legacy, security, risks, prevention,
mitigation, pentesting.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10218" title="Abstract">arXiv:2402.10218</a> [<a href="/pdf/2402.10218" title="Download PDF">pdf</a>, <a href="/format/2402.10218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AntiDeepFake: AI for Deep Fake Speech Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Togootogtokh%2C+E">Enkhtogtokh Togootogtokh</a>, 
<a href="/search/cs?searchtype=author&query=Klasen%2C+C">Christian Klasen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2308.12734">arXiv:2308.12734</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">In this research study, we propose a modern artificial intelligence (AI)
approach to recognize deepfake voice, also known as generative AI cloned
synthetic voice. Our proposed AI technology, called AntiDeepFake, consists of
all main pipelines from data to evaluation in the whole picture. We provide
experimental results and scores for all our proposed methods. The main source
code for our approach is available in the provided link:
https://github.com/enkhtogtokh/antideepfake repository.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10220" title="Abstract">arXiv:2402.10220</a> [<a href="/pdf/2402.10220" title="Download PDF">pdf</a>, <a href="/ps/2402.10220" title="Download PostScript">ps</a>, <a href="/format/2402.10220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Semi-Autonomous Robotic Arm Manipulation Operator Intention  Detection from Forces Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alharthi%2C+A">Abdullah Alharthi</a>, 
<a href="/search/cs?searchtype=author&query=Tokatli%2C+O">Ozan Tokatli</a>, 
<a href="/search/cs?searchtype=author&query=Lopez%2C+E">Erwin Lopez</a>, 
<a href="/search/cs?searchtype=author&query=Herrmann%2C+G">Guido Herrmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In harsh environments such as those found in nuclear facilities, the use of
robotic systems is crucial for performing tasks that would otherwise require
human intervention. This is done to minimize the risk of human exposure to
dangerous levels of radiation, which can have severe consequences for health
and even be fatal. However, the telemanipulation systems employed in these
environments are becoming increasingly intricate, relying heavily on
sophisticated control methods and local master devices. Consequently, the
cognitive burden on operators during labor-intensive tasks is growing. To
tackle this challenge, operator intention detection based on task learning can
greatly enhance the performance of robotic tasks while reducing the reliance on
human effort in teleoperation, particularly in a glovebox environment. By
accurately predicting the operator's intentions, the robot can carry out tasks
more efficiently and effectively, with minimal input from the operator. In this
regard, we propose the utilization of Convolutional Neural Networks, a machine
learning approach, to learn and forecast the operator's intentions using raw
force feedback spatiotemporal data. Through our experimental study on glovebox
tasks for nuclear applications, such as radiation survey and object grasping,
we have achieved promising outcomes. Our approach holds the potential to
enhance the safety and efficiency of robotic systems in harsh environments,
thus diminishing the risk of human exposure to radiation while simultaneously
improving the precision and speed of robotic operations.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10222" title="Abstract">arXiv:2402.10222</a> [<a href="/pdf/2402.10222" title="Download PDF">pdf</a>, <a href="/format/2402.10222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autonomous Vehicle Patrolling Through Deep Reinforcement Learning:  Learning to Communicate and Cooperate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tong%2C+C">Chenhao Tong</a>, 
<a href="/search/cs?searchtype=author&query=Rodriguez%2C+M+A">Maria A. Rodriguez</a>, 
<a href="/search/cs?searchtype=author&query=Sinnott%2C+R+O">Richard O. Sinnott</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Autonomous vehicles are suited for continuous area patrolling problems.
Finding an optimal patrolling strategy can be challenging due to unknown
environmental factors, such as wind or landscape; or autonomous vehicles'
constraints, such as limited battery life or hardware failures. Importantly,
patrolling large areas often requires multiple agents to collectively
coordinate their actions. However, an optimal coordination strategy is often
non-trivial to be manually defined due to the complex nature of patrolling
environments. In this paper, we consider a patrolling problem with
environmental factors, agent limitations, and three typical cooperation
problems -- collision avoidance, congestion avoidance, and patrolling target
negotiation. We propose a multi-agent reinforcement learning solution based on
a reinforced inter-agent learning (RIAL) method. With this approach, agents are
trained to develop their own communication protocol to cooperate during
patrolling where faults can and do occur. The solution is validated through
simulation experiments and is compared with several state-of-the-art patrolling
solutions from different perspectives, including the overall patrol
performance, the collision avoidance performance, the efficiency of battery
recharging strategies, and the overall fault tolerance.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10223" title="Abstract">arXiv:2402.10223</a> [<a href="/pdf/2402.10223" title="Download PDF">pdf</a>, <a href="/format/2402.10223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integer Optimization of CT Trajectories using a Discrete Data  Completeness Formulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schneider%2C+L">Linda-Sophie Schneider</a>, 
<a href="/search/cs?searchtype=author&query=Herl%2C+G">Gabriel Herl</a>, 
<a href="/search/cs?searchtype=author&query=Maier%2C+A">Andreas Maier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV); Optimization and Control (math.OC)

</div>
<p class="mathjax">X-ray computed tomography (CT) plays a key role in digitizing
three-dimensional structures for a wide range of medical and industrial
applications. Traditional CT systems often rely on standard circular and
helical scan trajectories, which may not be optimal for challenging scenarios
involving large objects, complex structures, or resource constraints. In
response to these challenges, we are exploring the potential of twin robotic CT
systems, which offer the flexibility to acquire projections from arbitrary
views around the object of interest. Ensuring complete and mathematically sound
reconstructions becomes critical in such systems. In this work, we present an
integer programming-based CT trajectory optimization method. Utilizing discrete
data completeness conditions, we formulate an optimization problem to select an
optimized set of projections. This approach enforces data completeness and
considers absorption-based metrics for reliability evaluation. We compare our
method with an equidistant circular CT trajectory and a greedy approach. While
greedy already performs well in some cases, we provide a way to improve
greedy-based projection selection using an integer optimization approach. Our
approach improves CT trajectories and quantifies the optimality of the solution
in terms of an optimality gap.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10224" title="Abstract">arXiv:2402.10224</a> [<a href="/pdf/2402.10224" title="Download PDF">pdf</a>, <a href="/format/2402.10224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-Centric Goal Reasoning with Ripple-Down Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brameld%2C+K">Kenji Brameld</a>, 
<a href="/search/cs?searchtype=author&query=Castro%2C+G">Germ&#xe1;n Castro</a>, 
<a href="/search/cs?searchtype=author&query=Sammut%2C+C">Claude Sammut</a>, 
<a href="/search/cs?searchtype=author&query=Roberts%2C+M">Mark Roberts</a>, 
<a href="/search/cs?searchtype=author&query=Aha%2C+D+W">David W. Aha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the Ninth Goal Reasoning Workshop (Advances in Cognitive Systems, 2021)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">ActorSim is a goal reasoning framework developed at the Naval Research
Laboratory. Originally, all goal reasoning rules were hand-crafted. This work
extends ActorSim with the capability of learning by demonstration, that is,
when a human trainer disagrees with a decision made by the system, the trainer
can take over and show the system the correct decision. The learning component
uses Ripple-Down Rules (RDR) to build new decision rules to correctly handle
similar cases in the future. The system is demonstrated using the RoboCup
Rescue Agent Simulation, which simulates a city-wide disaster, requiring
emergency services, including fire, ambulance and police, to be dispatched to
different sites to evacuate civilians from dangerous situations. The RDRs are
implemented in a scripting language, FrameScript, which is used to mediate
between ActorSim and the agent simulator. Using Ripple-Down Rules, ActorSim can
scale to an order of magnitude more goals than the previous version.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10226" title="Abstract">arXiv:2402.10226</a> [<a href="/pdf/2402.10226" title="Download PDF">pdf</a>, <a href="/format/2402.10226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simulation-based Analysis of a Novel Loop-based Road Topology for  Autonomous Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramdhan%2C+S">Stefan Ramdhan</a>, 
<a href="/search/cs?searchtype=author&query=Trandinh%2C+W">Winnie Trandinh</a>, 
<a href="/search/cs?searchtype=author&query=Arulmohan%2C+S">Sathurshan Arulmohan</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiayong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Deevy%2C+S">Spencer Deevy</a>, 
<a href="/search/cs?searchtype=author&query=Bandur%2C+V">Victor Bandur</a>, 
<a href="/search/cs?searchtype=author&query=Pantelic%2C+V">Vera Pantelic</a>, 
<a href="/search/cs?searchtype=author&query=Lawford%2C+M">Mark Lawford</a>, 
<a href="/search/cs?searchtype=author&query=Wassyng%2C+A">Alan Wassyng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 10 figures, Submitted to IV2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">The challenges in implementing SAE Level 4/5 autonomous vehicles are
manifold, with intersection navigation being a pervasive one. We analyze a
novel road topology invented by a co-author of this paper, Xiayong Hu. The
topology eliminates the need for traditional traffic control and cross-traffic
at intersections, potentially improving the safety of autonomous driving
systems. The topology, herein called the Zonal Road Topology, consists of
unidirectional loops of road with traffic flowing either clockwise or
counter-clockwise. Adjacent loops are directionally aligned with one another,
allowing vehicles to transfer from one loop to another through a simple lane
change. To evaluate the Zonal Road Topology, a one km2 pilot-track near
Changshu, China is currently being set aside for testing. In parallel, traffic
simulations are being performed. To this end, we conduct a simulation-based
comparison between the Zonal Road Topology and a traditional road topology for
a generic Electric Vehicle (EV) using the Simulation for Urban MObility (SUMO)
platform and MATLAB/Simulink. We analyze the topologies in terms of their
travel efficiency, safety, energy usage, and capacity. Drive time, number of
halts, progress rate, and other metrics are analyzed across varied traffic
levels to investigate the advantages and disadvantages of the Zonal Road
Topology. Our results indicate that vehicles on the Zonal Road Topology have a
lower, more consistent drive time with greater traffic throughput, while using
less energy on average. These results become more prominent at higher traffic
densities.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10227" title="Abstract">arXiv:2402.10227</a> [<a href="/pdf/2402.10227" title="Download PDF">pdf</a>, <a href="/format/2402.10227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Correlational Lagrangian Schr&#xf6;dinger Bridge: Learning Dynamics with  Population-Level Regularization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=You%2C+Y">Yuning You</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+R">Ruida Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yang Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Accurate modeling of system dynamics holds intriguing potential in broad
scientific fields including cytodynamics and fluid mechanics. This task often
presents significant challenges when (i) observations are limited to
cross-sectional samples (where individual trajectories are inaccessible for
learning), and moreover, (ii) the behaviors of individual particles are
heterogeneous (especially in biological systems due to biodiversity). To
address them, we introduce a novel framework dubbed correlational Lagrangian
Schr\"odinger bridge (CLSB), aiming to seek for the evolution "bridging" among
cross-sectional observations, while regularized for the minimal population
"cost". In contrast to prior methods relying on \textit{individual}-level
regularizers for all particles \textit{homogeneously} (e.g. restraining
individual motions), CLSB operates at the population level admitting the
heterogeneity nature, resulting in a more generalizable modeling in practice.
To this end, our contributions include (1) a new class of population
regularizers capturing the temporal variations in multivariate relations, with
the tractable formulation derived, (2) three domain-informed instantiations
based on genetic co-expression stability, and (3) an integration of population
regularizers into data-driven generative models as constrained optimization,
and a numerical solution, with further extension to conditional generative
models. Empirically, we demonstrate the superiority of CLSB in single-cell
sequencing data analyses such as simulating cell development over time and
predicting cellular responses to drugs of varied doses.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10228" title="Abstract">arXiv:2402.10228</a> [<a href="/pdf/2402.10228" title="Download PDF">pdf</a>, <a href="/format/2402.10228" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement  Learning Framework for Complex Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yingru Li</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiawei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+L">Lei Han</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zhi-Quan Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Bridging the theory and practice!
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">To solve complex tasks under resource constraints, reinforcement learning
(RL) agents need to be simple, efficient, and scalable with (1) large state
space and (2) increasingly accumulated data of interactions. We propose the
HyperAgent, a RL framework with hypermodel, index sampling schemes and
incremental update mechanism, enabling computation-efficient sequential
posterior approximation and data-efficient action selection under general value
function approximation beyond conjugacy. The implementation of \HyperAgent is
simple as it only adds one module and one line of code additional to DDQN.
Practically, HyperAgent demonstrates its robust performance in large-scale deep
RL benchmarks with significant efficiency gain in terms of both data and
computation. Theoretically, among the practically scalable algorithms,
HyperAgent is the first method to achieve provably scalable per-step
computational complexity as well as sublinear regret under tabular RL. The core
of our theoretical analysis is the sequential posterior approximation argument,
made possible by the first analytical tool for sequential random projection, a
non-trivial martingale extension of the Johnson-Lindenstrauss lemma. This work
bridges the theoretical and practical realms of RL, establishing a new
benchmark for RL algorithm design.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10230" title="Abstract">arXiv:2402.10230</a> [<a href="/pdf/2402.10230" title="Download PDF">pdf</a>, <a href="/format/2402.10230" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporal Analysis of Drifting Hashtags in Textual Data Streams: A  Graph-Based Application
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garcia%2C+C+M">Cristiano M. Garcia</a>, 
<a href="/search/cs?searchtype=author&query=de+Souza+Britto%2C+A">Alceu de Souza Britto Jr</a>, 
<a href="/search/cs?searchtype=author&query=Barddal%2C+J+P">Jean Paul Barddal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Social media has played an important role since its emergence. People use the
internet to express opinions about anything, making social media platforms a
social sensor. Initially supported by Twitter, the hashtags are now in use on
several social media platforms. Hashtags are helpful to tag, track, and group
posts on similar topics. In this paper, we analyze hashtag drifts over time
using concepts from graph analysis and textual data streams using the
Girvan-Newman method to uncover hashtag communities in annual snapshots. More
specifically, we analyzed the #mybodymychoice hashtag between 2018 and 2022. In
addition, we offer insights about some hashtags found in the study.
Furthermore, our approach can be useful for monitoring changes over time in
opinions and sentiment patterns about an entity on social media. Even though
the hashtag #mybodymychoice was initially coupled with women's rights,
abortion, and bodily autonomy, we observe that it suffered drifts during the
studied period across topics such as drug legalization, vaccination, political
protests, war, and civil rights. The year 2021 was the most significant
drifting year, in which the communities detected suggest that #mybodymychoice
significantly drifted to vaccination and Covid-19-related topics.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10231" title="Abstract">arXiv:2402.10231</a> [<a href="/pdf/2402.10231" title="Download PDF">pdf</a>, <a href="/format/2402.10231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multi-faceted Semi-Synthetic Dataset for Automated Cyberbullying  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ejaz%2C+N">Naveed Ejaz</a>, 
<a href="/search/cs?searchtype=author&query=Kashif%2C+F">Fakhra Kashif</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+S">Salimur Choudhury</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In recent years, the rising use of social media has propelled automated
cyberbullying detection into a prominent research domain. However, challenges
persist due to the absence of a standardized definition and universally
accepted datasets. Many researchers now view cyberbullying as a facet of
cyberaggression, encompassing factors like repetition, peer relationships, and
harmful intent in addition to online aggression. Acquiring comprehensive data
reflective of all cyberbullying components from social media networks proves to
be a complex task. This paper provides a description of an extensive
semi-synthetic cyberbullying dataset that incorporates all of the essential
aspects of cyberbullying, including aggression, repetition, peer relationships,
and intent to harm. The method of creating the dataset is succinctly outlined,
and a detailed overview of the publicly accessible dataset is additionally
presented. This accompanying data article provides an in-depth look at the
dataset, increasing transparency and enabling replication. It also aids in a
deeper understanding of the data, supporting broader research use.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10233" title="Abstract">arXiv:2402.10233</a> [<a href="/pdf/2402.10233" title="Download PDF">pdf</a>, <a href="/ps/2402.10233" title="Download PostScript">ps</a>, <a href="/format/2402.10233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The small-world phenomenon: a model, explanations, characterizations and  examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Egghe%2C+L">Leo Egghe</a>, 
<a href="/search/cs?searchtype=author&query=Rousseau%2C+R">Ronald Rousseau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">We introduce and define three types of small worlds: small worlds based on
the diameter of the network (SWD), those based on the average geodesic distance
between nodes (SWA), and those based on the median geodesic distance (SWMd).
These types of networks are defined as limiting properties of sequences of
sets. We show the exact relation between these three types, namely that each
SWD network is also an SWA network and that each SWA network is also an SWMd
network. Yet, having the small-world property is rather evident, in the sense
that most networks are small-world networks in one of the three ways. We
introduce sequences of distance frequencies, so-called alpha-sequences, and
prove a relation between the majorization property between alpha-sequences and
small-world properties.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10236" title="Abstract">arXiv:2402.10236</a> [<a href="/pdf/2402.10236" title="Download PDF">pdf</a>, <a href="/format/2402.10236" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering Sensorimotor Agency in Cellular Automata using Diversity  Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hamon%2C+G">Gautier Hamon</a>, 
<a href="/search/cs?searchtype=author&query=Etcheverry%2C+M">Mayalen Etcheverry</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+B+W">Bert Wang-Chak Chan</a>, 
<a href="/search/cs?searchtype=author&query=Moulin-Frier%2C+C">Cl&#xe9;ment Moulin-Frier</a>, 
<a href="/search/cs?searchtype=author&query=Oudeyer%2C+P">Pierre-Yves Oudeyer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The research field of Artificial Life studies how life-like phenomena such as
autopoiesis, agency, or self-regulation can self-organize in computer
simulations. In cellular automata (CA), a key open-question has been whether it
it is possible to find environment rules that self-organize robust
"individuals" from an initial state with no prior existence of things like
"bodies", "brain", "perception" or "action". In this paper, we leverage recent
advances in machine learning, combining algorithms for diversity search,
curriculum learning and gradient descent, to automate the search of such
"individuals", i.e. localized structures that move around with the ability to
react in a coherent manner to external obstacles and maintain their integrity,
hence primitive forms of sensorimotor agency. We show that this approach
enables to find systematically environmental conditions in CA leading to
self-organization of such basic forms of agency. Through multiple experiments,
we show that the discovered agents have surprisingly robust capabilities to
move, maintain their body integrity and navigate among various obstacles. They
also show strong generalization abilities, with robustness to changes of scale,
random updates or perturbations from the environment not seen during training.
We discuss how this approach opens new perspectives in AI and synthetic
bioengineering.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10238" title="Abstract">arXiv:2402.10238</a> [<a href="/pdf/2402.10238" title="Download PDF">pdf</a>, <a href="/format/2402.10238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parametric Learning of Time-Advancement Operators for Unstable Flame  Evolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+R">Rixin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Hodzic%2C+E">Erdzan Hodzic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This study investigates the application of machine learning, specifically
Fourier Neural Operator (FNO) and Convolutional Neural Network (CNN), to learn
time-advancement operators for parametric partial differential equations
(PDEs). Our focus is on extending existing operator learning methods to handle
additional inputs representing PDE parameters. The goal is to create a unified
learning approach that accurately predicts short-term solutions and provides
robust long-term statistics under diverse parameter conditions, facilitating
computational cost savings and accelerating development in engineering
simulations. We develop and compare parametric learning methods based on FNO
and CNN, evaluating their effectiveness in learning parametric-dependent
solution time-advancement operators for one-dimensional PDEs and realistic
flame front evolution data obtained from direct numerical simulations of the
Navier-Stokes equations.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10240" title="Abstract">arXiv:2402.10240</a> [<a href="/pdf/2402.10240" title="Download PDF">pdf</a>, <a href="/format/2402.10240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Dynamical View of the Question of Why
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fatemi%2C+M">Mehdi Fatemi</a>, 
<a href="/search/cs?searchtype=author&query=Gowda%2C+S">Sindhu Gowda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the Twelfth International Conference on Learning Representations (ICLR'24). arXiv admin note: text overlap with <a href="/abs/2102.02311">arXiv:2102.02311</a>, <a href="/abs/2012.05123">arXiv:2012.05123</a>, <a href="/abs/2201.13169">arXiv:2201.13169</a>, <a href="/abs/2012.05603">arXiv:2012.05603</a>, <a href="/abs/1812.03789">arXiv:1812.03789</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
<p class="mathjax">We address causal reasoning in multivariate time series data generated by
stochastic processes. Existing approaches are largely restricted to static
settings, ignoring the continuity and emission of variations across time. In
contrast, we propose a learning paradigm that directly establishes causation
between events in the course of time. We present two key lemmas to compute
causal contributions and frame them as reinforcement learning problems. Our
approach offers formal and computational tools for uncovering and quantifying
causal relationships in diffusion processes, subsuming various important
settings such as discrete-time Markov decision processes. Finally, in fairly
intricate experiments and through sheer learning, our framework reveals and
quantifies causal links, which otherwise seem inexplicable.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10242" title="Abstract">arXiv:2402.10242</a> [<a href="/pdf/2402.10242" title="Download PDF">pdf</a>, <a href="/ps/2402.10242" title="Download PostScript">ps</a>, <a href="/format/2402.10242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Signed Diverse Multiplex Networks: Clustering and Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pensky%2C+M">Marianna Pensky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
<p class="mathjax">The paper introduces a Signed Generalized Random Dot Product Graph (SGRDPG)
model, which is a variant of the Generalized Random Dot Product Graph (GRDPG),
where, in addition, edges can be positive or negative. The setting is extended
to a multiplex version, where all layers have the same collection of nodes and
follow the SGRDPG. The only common feature of the layers of the network is that
they can be partitioned into groups with common subspace structures, while
otherwise all matrices of connection probabilities can be all different. The
setting above is extremely flexible and includes a variety of existing
multiplex network models as its particular cases. The paper fulfills two
objectives. First, it shows that keeping signs of the edges in the process of
network construction leads to a better precision of estimation and clustering
and, hence, is beneficial for tackling real world problems such as analysis of
brain networks. Second, by employing novel algorithms, our paper ensures
equivalent or superior accuracy than has been achieved in simpler multiplex
network models. In addition to theoretical guarantees, both of those features
are demonstrated using numerical simulations and a real data example.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10247" title="Abstract">arXiv:2402.10247</a> [<a href="/pdf/2402.10247" title="Download PDF">pdf</a>, <a href="/format/2402.10247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Engraving Oriented Joint Estimation of Pitch Spelling and Local and  Global Keys
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bouquillard%2C+A">Augustin Bouquillard</a>, 
<a href="/search/cs?searchtype=author&query=Jacquemard%2C+F">Florent Jacquemard</a> (CEDRIC - VERTIGO)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Technologies for Music Notation and Representation (TENOR), Apr 2024, Zurich (CH), Switzerland
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Information Retrieval (cs.IR); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">We revisit the problems of pitch spelling and tonality guessing with a new
algorithm for their joint estimation from a MIDI file including information
about the measure boundaries. Our algorithm does not only identify a global key
but also local ones all along the analyzed piece. It uses Dynamic Programming
techniques to search for an optimal spelling in term, roughly, of the number of
accidental symbols that would be displayed in the engraved score. The
evaluation of this number is coupled with an estimation of the global key and
some local keys, one for each measure. Each of the three informations is used
for the estimation of the other, in a multi-steps procedure. An evaluation
conducted on a monophonic and a piano dataset, comprising 216 464 notes in
total, shows a high degree of accuracy, both for pitch spelling (99.5% on
average on the Bach corpus and 98.2% on the whole dataset) and global key
signature estimation (93.0% on average, 95.58% on the piano dataset). Designed
originally as a backend tool in a music transcription framework, this method
should also be useful in other tasks related to music notation processing.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10248" title="Abstract">arXiv:2402.10248</a> [<a href="/pdf/2402.10248" title="Download PDF">pdf</a>, <a href="/format/2402.10248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Data-Driven Supervised Machine Learning Approach to Estimating Global  Ambient Air Pollution Concentrations With Associated Prediction Intervals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Berrisford%2C+L+J">Liam J Berrisford</a>, 
<a href="/search/cs?searchtype=author&query=Barbosa%2C+H">Hugo Barbosa</a>, 
<a href="/search/cs?searchtype=author&query=Menezes%2C+R">Ronaldo Menezes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Main Paper: 25 pages, 15 figures, 5 tables. Supplementary: 4 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Global ambient air pollution, a transboundary challenge, is typically
addressed through interventions relying on data from spatially sparse and
heterogeneously placed monitoring stations. These stations often encounter
temporal data gaps due to issues such as power outages. In response, we have
developed a scalable, data-driven, supervised machine learning framework. This
model is designed to impute missing temporal and spatial measurements, thereby
generating a comprehensive dataset for pollutants including NO$_2$, O$_3$,
PM$_{10}$, PM$_{2.5}$, and SO$_2$. The dataset, with a fine granularity of
0.25$^{\circ}$ at hourly intervals and accompanied by prediction intervals for
each estimate, caters to a wide range of stakeholders relying on outdoor air
pollution data for downstream assessments. This enables more detailed studies.
Additionally, the model's performance across various geographical locations is
examined, providing insights and recommendations for strategic placement of
future monitoring stations to further enhance the model's accuracy.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10250" title="Abstract">arXiv:2402.10250</a> [<a href="/pdf/2402.10250" title="Download PDF">pdf</a>, <a href="/ps/2402.10250" title="Download PostScript">ps</a>, <a href="/format/2402.10250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zastosowanie graf&#xf3;w i sieci w systemach rekomendacji
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Malinowski%2C+M">Micha&#x142; Malinowski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> in Polish language. Przedsi\k{e}biorstwo w nowej rzeczywisto\'sci gospodarczej. Relacje zmiany strategie; 2021
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>

</div>
<p class="mathjax">The chapter aims to explore the application of graph theory and networks in
the recommendation domain, encompassing the mathematical models that form the
foundation for the algorithms and recommendation systems developed based on
them. The initial section of the chapter provides a concise overview of the
recommendation field, with a particular focus on the types of recommendation
solutions and the mathematical description of the problem. Subsequently, the
chapter delves into the models and techniques for utilizing graphs and
networks, along with illustrative examples of algorithms constructed on their
basis.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10252" title="Abstract">arXiv:2402.10252</a> [<a href="/pdf/2402.10252" title="Download PDF">pdf</a>, <a href="/ps/2402.10252" title="Download PostScript">ps</a>, <a href="/format/2402.10252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Control of Linear Systems with Unbounded and Degenerate Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ito%2C+K">Kaito Ito</a>, 
<a href="/search/eess?searchtype=author&query=Tsuchiya%2C+T">Taira Tsuchiya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">This paper investigates the problem of controlling a linear system under
possibly unbounded and degenerate noise with unknown cost functions, known as
an online control problem. In contrast to the existing work, which assumes the
boundedness of noise, we reveal that for convex costs, an $
\widetilde{O}(\sqrt{T}) $ regret bound can be achieved even for unbounded
noise, where $ T $ denotes the time horizon. Moreover, when the costs are
strongly convex, we establish an $ O({\rm poly} (\log T)) $ regret bound
without the assumption that noise covariance is non-degenerate, which has been
required in the literature. The key ingredient in removing the rank assumption
on noise is a system transformation associated with the noise covariance. This
simultaneously enables the parameter reduction of an online control algorithm.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10254" title="Abstract">arXiv:2402.10254</a> [<a href="/pdf/2402.10254" title="Download PDF">pdf</a>, <a href="/format/2402.10254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalized Federated Learning for Statistical Heterogeneity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Firdaus%2C+M">Muhammad Firdaus</a>, 
<a href="/search/cs?searchtype=author&query=Rhee%2C+K">Kyung-Hyune Rhee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">The popularity of federated learning (FL) is on the rise, along with growing
concerns about data privacy in artificial intelligence applications. FL
facilitates collaborative multi-party model learning while simultaneously
ensuring the preservation of data confidentiality. Nevertheless, the problem of
statistical heterogeneity caused by the presence of diverse client data
distributions gives rise to certain challenges, such as inadequate
personalization and slow convergence. In order to address the above issues,
this paper offers a brief summary of the current research progress in the field
of personalized federated learning (PFL). It outlines the PFL concept, examines
related techniques, and highlights current endeavors. Furthermore, this paper
also discusses potential further research and obstacles associated with PFL.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10259" title="Abstract">arXiv:2402.10259</a> [<a href="/pdf/2402.10259" title="Download PDF">pdf</a>, <a href="/format/2402.10259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object  with Gaussian Splatting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sikuang Li</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+J">Jiemin Fang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+R">Ruofan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Lingxi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaopeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+W">Wei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Q">Qi Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://gaussianobject.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">Reconstructing and rendering 3D objects from highly sparse views is of
critical importance for promoting applications of 3D vision techniques and
improving user experience. However, images from sparse views only contain very
limited 3D information, leading to two significant challenges: 1) Difficulty in
building multi-view consistency as images for matching are too few; 2)
Partially omitted or highly compressed object information as view coverage is
insufficient. To tackle these challenges, we propose GaussianObject, a
framework to represent and render the 3D object with Gaussian splatting, that
achieves high rendering quality with only 4 input images. We first introduce
techniques of visual hull and floater elimination which explicitly inject
structure priors into the initial optimization process for helping build
multi-view consistency, yielding a coarse 3D Gaussian representation. Then we
construct a Gaussian repair model based on diffusion models to supplement the
omitted object information, where Gaussians are further refined. We design a
self-generating strategy to obtain image pairs for training the repair model.
Our GaussianObject is evaluated on several challenging datasets, including
MipNeRF360, OmniObject3D, and OpenIllumination, achieving strong reconstruction
results from only 4 views and significantly outperforming previous
state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10260" title="Abstract">arXiv:2402.10260</a> [<a href="/pdf/2402.10260" title="Download PDF">pdf</a>, <a href="/format/2402.10260" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A StrongREJECT for Empty Jailbreaks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Souly%2C+A">Alexandra Souly</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Q">Qingyuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Bowen%2C+D">Dillon Bowen</a>, 
<a href="/search/cs?searchtype=author&query=Trinh%2C+T">Tu Trinh</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+E">Elvis Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Pandey%2C+S">Sana Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Abbeel%2C+P">Pieter Abbeel</a>, 
<a href="/search/cs?searchtype=author&query=Svegliato%2C+J">Justin Svegliato</a>, 
<a href="/search/cs?searchtype=author&query=Emmons%2C+S">Scott Emmons</a>, 
<a href="/search/cs?searchtype=author&query=Watkins%2C+O">Olivia Watkins</a>, 
<a href="/search/cs?searchtype=author&query=Toyer%2C+S">Sam Toyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and data at <a href="https://github.com/alexandrasouly/strongreject">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The rise of large language models (LLMs) has drawn attention to the existence
of "jailbreaks" that allow the models to be used maliciously. However, there is
no standard benchmark for measuring the severity of a jailbreak, leaving
authors of jailbreak papers to create their own. We show that these benchmarks
often include vague or unanswerable questions and use grading criteria that are
biased towards overestimating the misuse potential of low-quality model
responses. Some jailbreak techniques make the problem worse by decreasing the
quality of model responses even on benign questions: we show that several
jailbreaking techniques substantially reduce the zero-shot performance of GPT-4
on MMLU. Jailbreaks can also make it harder to elicit harmful responses from an
"uncensored" open-source model. We present a new benchmark, StrongREJECT, which
better discriminates between effective and ineffective jailbreaks by using a
higher-quality question set and a more accurate response grading algorithm. We
show that our new grading scheme better accords with human judgment of response
quality and overall jailbreak effectiveness, especially on the sort of
low-quality responses that contribute the most to over-estimation of jailbreak
performance on existing benchmarks. We release our code and data at
https://github.com/alexandrasouly/strongreject.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10280" title="Abstract">arXiv:2402.10280</a> [<a href="/pdf/2402.10280" title="Download PDF">pdf</a>, <a href="/format/2402.10280" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SusFL: Energy-Aware Federated Learning-based Monitoring for Sustainable  Smart Farms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+P">Paul Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+I">Ing-Ray Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ha%2C+D+S">Dong Sam Ha</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+J">Jin-Hee Cho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We propose a novel energy-aware federated learning (FL)-based system, namely
SusFL, for sustainable smart farming to address the challenge of inconsistent
health monitoring due to fluctuating energy levels of solar sensors. This
system equips animals, such as cattle, with solar sensors with computational
capabilities, including Raspberry Pis, to train a local deep-learning model on
health data. These sensors periodically update Long Range (LoRa) gateways,
forming a wireless sensor network (WSN) to detect diseases like mastitis. Our
proposed SusFL system incorporates mechanism design, a game theory concept, for
intelligent client selection to optimize monitoring quality while minimizing
energy use. This strategy ensures the system's sustainability and resilience
against adversarial attacks, including data poisoning and privacy threats, that
could disrupt FL operations. Through extensive comparative analysis using
real-time datasets, we demonstrate that our FL-based monitoring system
significantly outperforms existing methods in prediction accuracy, operational
efficiency, system reliability (i.e., mean time between failures or MTBF), and
social welfare maximization by the mechanism designer. Our findings validate
the superiority of our system for effective and sustainable animal health
monitoring in smart farms. The experimental results show that SusFL
significantly improves system performance, including a $10\%$ reduction in
energy consumption, a $15\%$ increase in social welfare, and a $34\%$ rise in
Mean Time Between Failures (MTBF), alongside a marginal increase in the global
model's prediction accuracy.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10281" title="Abstract">arXiv:2402.10281</a> [<a href="/pdf/2402.10281" title="Download PDF">pdf</a>, <a href="/ps/2402.10281" title="Download PostScript">ps</a>, <a href="/format/2402.10281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparison of variational discretizations for a convection-diffusion  problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bacuta%2C+C">Constantin Bacuta</a>, 
<a href="/search/math?searchtype=author&query=Bacuta%2C+C">Cristina Bacuta</a>, 
<a href="/search/math?searchtype=author&query=Hayes%2C+D">Daniel Hayes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, no figures. arXiv admin note: text overlap with <a href="/abs/2402.03314">arXiv:2402.03314</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">For a model convection-diffusion problem, we obtain new error estimates for a
general upwinding finite element discretization based on bubble modification of
the test space. The key analysis tool is based on finding representations of
the optimal norms on the trial spaces at the continuous and discrete levels. We
analyze and compare the standard linear discretization, the saddle point least
square and upwinding Petrov-Galerkin methods. We conclude that the bubble
upwinding Petrov-Galerkin method is the most performant discretization for the
one dimensional model. Our results for the model convection-diffusion problem
can be extended for creating new and efficient discretizations for the
multidimensional cases.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10282" title="Abstract">arXiv:2402.10282</a> [<a href="/pdf/2402.10282" title="Download PDF">pdf</a>, <a href="/format/2402.10282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Information Capacity Regret Bounds for Bandits with Mediator Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eldowa%2C+K">Khaled Eldowa</a>, 
<a href="/search/cs?searchtype=author&query=Cesa-Bianchi%2C+N">Nicol&#xf2; Cesa-Bianchi</a>, 
<a href="/search/cs?searchtype=author&query=Metelli%2C+A+M">Alberto Maria Metelli</a>, 
<a href="/search/cs?searchtype=author&query=Restelli%2C+M">Marcello Restelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">This work addresses the mediator feedback problem, a bandit game where the
decision set consists of a number of policies, each associated with a
probability distribution over a common space of outcomes. Upon choosing a
policy, the learner observes an outcome sampled from its distribution and
incurs the loss assigned to this outcome in the present round. We introduce the
policy set capacity as an information-theoretic measure for the complexity of
the policy set. Adopting the classical EXP4 algorithm, we provide new regret
bounds depending on the policy set capacity in both the adversarial and the
stochastic settings. For a selection of policy set families, we prove
nearly-matching lower bounds, scaling similarly with the capacity. We also
consider the case when the policies' distributions can vary between rounds,
thus addressing the related bandits with expert advice problem, which we
improve upon its prior results. Additionally, we prove a lower bound showing
that exploiting the similarity between the policies is not possible in general
under linear bandit feedback. Finally, for a full-information variant, we
provide a regret bound scaling with the information radius of the policy set.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10283" title="Abstract">arXiv:2402.10283</a> [<a href="/pdf/2402.10283" title="Download PDF">pdf</a>, <a href="/format/2402.10283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Backdoor Attack against One-Class Sequential Anomaly Detection Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">He Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Shuhan Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work is accepted by the PAKDD 2024. 12 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Information Theory (cs.IT)

</div>
<p class="mathjax">Deep anomaly detection on sequential data has garnered significant attention
due to the wide application scenarios. However, deep learning-based models face
a critical security threat - their vulnerability to backdoor attacks. In this
paper, we explore compromising deep sequential anomaly detection models by
proposing a novel backdoor attack strategy. The attack approach comprises two
primary steps, trigger generation and backdoor injection. Trigger generation is
to derive imperceptible triggers by crafting perturbed samples from the benign
normal data, of which the perturbed samples are still normal. The backdoor
injection is to properly inject the backdoor triggers to comprise the model
only for the samples with triggers. The experimental results demonstrate the
effectiveness of our proposed attack strategy by injecting backdoors on two
well-established one-class anomaly detection models.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10290" title="Abstract">arXiv:2402.10290</a> [<a href="/pdf/2402.10290" title="Download PDF">pdf</a>, <a href="/format/2402.10290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Experiments with Encoding Structured Data for Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koujalgi%2C+S+N">Sujay Nagesh Koujalgi</a>, 
<a href="/search/cs?searchtype=author&query=Dodge%2C+J">Jonathan Dodge</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 8 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">The project's aim is to create an AI agent capable of selecting good actions
in a game-playing domain called Battlespace. Sequential domains like
Battlespace are important testbeds for planning problems, as such, the
Department of Defense uses such domains for wargaming exercises. The agents we
developed combine Monte Carlo Tree Search (MCTS) and Deep Q-Network (DQN)
techniques in an effort to navigate the game environment, avoid obstacles,
interact with adversaries, and capture the flag. This paper will focus on the
encoding techniques we explored to present complex structured data stored in a
Python class, a necessary precursor to an agent.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10291" title="Abstract">arXiv:2402.10291</a> [<a href="/pdf/2402.10291" title="Download PDF">pdf</a>, <a href="/format/2402.10291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Evaluation of Real-time Adaptive Sampling Change Point Detection  Algorithm using KCUSUM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saravanan%2C+V">Vijayalakshmi Saravanan</a>, 
<a href="/search/cs?searchtype=author&query=Siehien%2C+P">Perry Siehien</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+S">Shinjae Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Van+Dam%2C+H">Hubertus Van Dam</a>, 
<a href="/search/cs?searchtype=author&query=Flynn%2C+T">Thomas Flynn</a>, 
<a href="/search/cs?searchtype=author&query=Kelly%2C+C">Christopher Kelly</a>, 
<a href="/search/cs?searchtype=author&query=Ibrahim%2C+K+Z">Khaled Z Ibrahim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages. arXiv admin note: text overlap with <a href="/abs/1903.01661">arXiv:1903.01661</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Detecting abrupt changes in real-time data streams from scientific
simulations presents a challenging task, demanding the deployment of accurate
and efficient algorithms. Identifying change points in live data stream
involves continuous scrutiny of incoming observations for deviations in their
statistical characteristics, particularly in high-volume data scenarios.
Maintaining a balance between sudden change detection and minimizing false
alarms is vital. Many existing algorithms for this purpose rely on known
probability distributions, limiting their feasibility. In this study, we
introduce the Kernel-based Cumulative Sum (KCUSUM) algorithm, a non-parametric
extension of the traditional Cumulative Sum (CUSUM) method, which has gained
prominence for its efficacy in online change point detection under less
restrictive conditions. KCUSUM splits itself by comparing incoming samples
directly with reference samples and computes a statistic grounded in the
Maximum Mean Discrepancy (MMD) non-parametric framework. This approach extends
KCUSUM's pertinence to scenarios where only reference samples are available,
such as atomic trajectories of proteins in vacuum, facilitating the detection
of deviations from the reference sample without prior knowledge of the data's
underlying distribution. Furthermore, by harnessing MMD's inherent random-walk
structure, we can theoretically analyze KCUSUM's performance across various use
cases, including metrics like expected delay and mean runtime to false alarms.
Finally, we discuss real-world use cases from scientific simulations such as
NWChem CODAR and protein folding data, demonstrating KCUSUM's practical
effectiveness in online change point detection.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10293" title="Abstract">arXiv:2402.10293</a> [<a href="/pdf/2402.10293" title="Download PDF">pdf</a>, <a href="/format/2402.10293" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallel Play Saves Quantifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carmosino%2C+M">Marco Carmosino</a>, 
<a href="/search/cs?searchtype=author&query=Fagin%2C+R">Ronald Fagin</a>, 
<a href="/search/cs?searchtype=author&query=Immerman%2C+N">Neil Immerman</a>, 
<a href="/search/cs?searchtype=author&query=Kolaitis%2C+P">Phokion Kolaitis</a>, 
<a href="/search/cs?searchtype=author&query=Lenchner%2C+J">Jonathan Lenchner</a>, 
<a href="/search/cs?searchtype=author&query=Sengupta%2C+R">Rik Sengupta</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+R">Ryan Williams</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">The number of quantifiers needed to express first-order properties is
captured by two-player combinatorial games called multi-structural (MS) games.
We play these games on linear orders and strings, and introduce a technique we
call "parallel play", that dramatically reduces the number of quantifiers
needed in many cases. Linear orders and strings are the most basic
representatives of ordered structures -- a class of structures that has
historically been notoriously difficult to analyze. Yet, in this paper, we
provide upper bounds on the number of quantifiers needed to characterize
different-sized subsets of these structures, and prove that they are tight up
to constant factors, including, in some cases, up to a factor of
$1+\varepsilon$, for arbitrarily small $\varepsilon$.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10294" title="Abstract">arXiv:2402.10294</a> [<a href="/pdf/2402.10294" title="Download PDF">pdf</a>, <a href="/format/2402.10294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video  Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bryan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+Z">Zhaoyang Lv</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+H">Haijun Xia</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Sodhi%2C+R">Raj Sodhi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper accepted to the ACM Conference on Intelligent User Interfaces (ACM IUI) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multimedia (cs.MM)

</div>
<p class="mathjax">Video creation has become increasingly popular, yet the expertise and effort
required for editing often pose barriers to beginners. In this paper, we
explore the integration of large language models (LLMs) into the video editing
workflow to reduce these barriers. Our design vision is embodied in LAVE, a
novel system that provides LLM-powered agent assistance and language-augmented
editing features. LAVE automatically generates language descriptions for the
user's footage, serving as the foundation for enabling the LLM to process
videos and assist in editing tasks. When the user provides editing objectives,
the agent plans and executes relevant actions to fulfill them. Moreover, LAVE
allows users to edit videos through either the agent or direct UI manipulation,
providing flexibility and enabling manual refinement of agent actions. Our user
study, which included eight participants ranging from novices to proficient
editors, demonstrated LAVE's effectiveness. The results also shed light on user
perceptions of the proposed LLM-assisted editing paradigm and its impact on
users' creativity and sense of co-creation. Based on these findings, we propose
design implications to inform the future development of agent-assisted content
editing.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10298" title="Abstract">arXiv:2402.10298</a> [<a href="/pdf/2402.10298" title="Download PDF">pdf</a>, <a href="/ps/2402.10298" title="Download PostScript">ps</a>, <a href="/format/2402.10298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Streaming algorithm for balance gain and cost with cardinality  constraint on the integer lattice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+J">Jingjing Tan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">Team formation problem is a very important problem in the labor market, and
it is proved to be NP-hard. In this paper, we design an efficient bicriteria
streaming algorithms to construct a balance between gain and cost in a team
formation problem with cardinality constraint on the integer lattice. To solve
this problem, we establish a model for maximizing the difference between a
nonnegative normalized monotone submodule function and a nonnegative linear
function. Further, we discuss the case where the first function of the object
function is $\alpha$--weakly submodular. Combining the lattice binary search
with the threshold method, we present an online algorithm called bicriteria
streaming algorithms. Meanwhile, we give detailed analysis for both of these
models.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10302" title="Abstract">arXiv:2402.10302</a> [<a href="/pdf/2402.10302" title="Download PDF">pdf</a>, <a href="/format/2402.10302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to Discern Important Urgent News?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vasilyev%2C+O">Oleg Vasilyev</a>, 
<a href="/search/cs?searchtype=author&query=Bohannon%2C+J">John Bohannon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 12 figures, 12 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We found that a simple property of clusters in a clustered dataset of news
correlate strongly with importance and urgency of news (IUN) as assessed by
LLM. We verified our finding across different news datasets, dataset sizes,
clustering algorithms and embeddings. The found correlation should allow using
clustering (as an alternative to LLM) for identifying the most important urgent
news, or for filtering out unimportant articles.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10307" title="Abstract">arXiv:2402.10307</a> [<a href="/pdf/2402.10307" title="Download PDF">pdf</a>, <a href="/ps/2402.10307" title="Download PostScript">ps</a>, <a href="/format/2402.10307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A New Radio to Overcome Critical Link Budgets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+R+R">Ralf R. M&#xfc;ller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">We propose Multi-Antenna (MA) Towards Inband Shift Keying (TISK): a new
multi-carrier radio concept to cope with critical link budgets. In contrast to
common proposals that rely on analog beamforming at both transmitter and
receiver, MA-TISK does not require beam alignment. The transmitted signals have
all constant envelope in continuous time, which allows for efficient, low-cost
power amplification and up-conversion. The concept is compatible with any
linear PSK-modulation as well as pulse position modulation. Each sub-carrier is
sent over a separate antenna that is equipped with a voltage-controlled
oscillator. The phases of these oscillators are controlled by digital baseband.
Temporal signal combining makes up for the lack of beamforming gain at the
transmitter. A common message may be broadcast to many receivers,
simultaneously. Demodulation can be efficiently implemented by means of fast
Fourier transform.
<br />MA-TISK does not suffer from spectral re-growth issues plaguing other
constant envelope modulations like GMSK. Almost rectangular signal spectra
similar to those for linear modulation with root-raised-cosine pulse shaping
are possible.
<br />For the 100 MHz-wide spectral mask of 5G downlink, QPSK-modulation allows for
160 MBit/s with 5.74 MHz subcarrier spacing when using 16 transmit antennas.
The wide carrier spacing makes the signals insensitive to Doppler effects.
There is no loss in link budget gain compared to spatial beamforming at the
transmitter.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10309" title="Abstract">arXiv:2402.10309</a> [<a href="/pdf/2402.10309" title="Download PDF">pdf</a>, <a href="/format/2402.10309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discrete Probabilistic Inference as Control in Multi-path Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deleu%2C+T">Tristan Deleu</a>, 
<a href="/search/cs?searchtype=author&query=Nouri%2C+P">Padideh Nouri</a>, 
<a href="/search/cs?searchtype=author&query=Malkin%2C+N">Nikolay Malkin</a>, 
<a href="/search/cs?searchtype=author&query=Precup%2C+D">Doina Precup</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We consider the problem of sampling from a discrete and structured
distribution as a sequential decision problem, where the objective is to find a
stochastic policy such that objects are sampled at the end of this sequential
process proportionally to some predefined reward. While we could use maximum
entropy Reinforcement Learning (MaxEnt RL) to solve this problem for some
distributions, it has been shown that in general, the distribution over states
induced by the optimal policy may be biased in cases where there are multiple
ways to generate the same object. To address this issue, Generative Flow
Networks (GFlowNets) learn a stochastic policy that samples objects
proportionally to their reward by approximately enforcing a conservation of
flows across the whole Markov Decision Process (MDP). In this paper, we extend
recent methods correcting the reward in order to guarantee that the marginal
distribution induced by the optimal MaxEnt RL policy is proportional to the
original reward, regardless of the structure of the underlying MDP. We also
prove that some flow-matching objectives found in the GFlowNet literature are
in fact equivalent to well-established MaxEnt RL algorithms with a corrected
reward. Finally, we study empirically the performance of multiple MaxEnt RL and
GFlowNet algorithms on multiple problems involving sampling from discrete
distributions.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10310" title="Abstract">arXiv:2402.10310</a> [<a href="/pdf/2402.10310" title="Download PDF">pdf</a>, <a href="/format/2402.10310" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Generative Adversarial Imitation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Danyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Aasi%2C+E">Erfan Aasi</a>, 
<a href="/search/cs?searchtype=author&query=Tron%2C+R">Roberto Tron</a>, 
<a href="/search/cs?searchtype=author&query=Belta%2C+C">Calin Belta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to L4DC 2024 (under review)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Imitation learning methods have demonstrated considerable success in teaching
autonomous systems complex tasks through expert demonstrations. However, a
limitation of these methods is their lack of interpretability, particularly in
understanding the specific task the learning agent aims to accomplish. In this
paper, we propose a novel imitation learning method that combines Signal
Temporal Logic (STL) inference and control synthesis, enabling the explicit
representation of the task as an STL formula. This approach not only provides a
clear understanding of the task but also allows for the incorporation of human
knowledge and adaptation to new scenarios through manual adjustments of the STL
formulae. Additionally, we employ a Generative Adversarial Network
(GAN)-inspired training approach for both the inference and the control policy,
effectively narrowing the gap between the expert and learned policies. The
effectiveness of our algorithm is demonstrated through two case studies,
showcasing its practical applicability and adaptability.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10311" title="Abstract">arXiv:2402.10311</a> [<a href="/pdf/2402.10311" title="Download PDF">pdf</a>, <a href="/format/2402.10311" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The optimal placement of the head in the noun phrase. The case of  demonstrative, numeral, adjective and noun
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ferrer-i-Cancho%2C+R">Ramon Ferrer-i-Cancho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">The word order of a sentence is shaped by multiple principles. The principle
of syntactic dependency distance minimization is in conflict with the principle
of surprisal minimization (or predictability maximization) in single head
syntactic dependency structures: while the former predicts that the head should
be placed at the center of the linear arrangement, the latter predicts that the
head should be placed at one of the ends (either first or last). A critical
question is when surprisal minimization (or predictability maximization) should
surpass syntactic dependency distance minimization. In the context of single
head structures, it has been predicted that this is more likely to happen when
two conditions are met, i.e. (a) fewer words are involved and (b) words are
shorter. Here we test the prediction on the noun phrase when its composed of a
demonstrative, a numeral, an adjective and a noun. We find that, across
preferred orders in languages, the noun tends to be placed at one of the ends,
confirming the theoretical prediction. We also show evidence of anti locality
effects: syntactic dependency distances in preferred orders are longer than
expected by chance.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10312" title="Abstract">arXiv:2402.10312</a> [<a href="/pdf/2402.10312" title="Download PDF">pdf</a>, <a href="/format/2402.10312" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Tight Convex Relaxations for Contact-Rich Manipulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Graesdal%2C+B+P">Bernhard P. Graesdal</a>, 
<a href="/search/cs?searchtype=author&query=Chia%2C+S+Y+C">Shao Y. C. Chia</a>, 
<a href="/search/cs?searchtype=author&query=Marcucci%2C+T">Tobia Marcucci</a>, 
<a href="/search/cs?searchtype=author&query=Morozov%2C+S">Savva Morozov</a>, 
<a href="/search/cs?searchtype=author&query=Amice%2C+A">Alexandre Amice</a>, 
<a href="/search/cs?searchtype=author&query=Parrilo%2C+P+A">Pablo A. Parrilo</a>, 
<a href="/search/cs?searchtype=author&query=Tedrake%2C+R">Russ Tedrake</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We present a method for global motion planning of robotic systems that
interact with the environment through contacts. Our method directly handles the
hybrid nature of such tasks using tools from convex optimization. We formulate
the motion-planning problem as a shortest-path problem in a graph of convex
sets, where a path in the graph corresponds to a contact sequence and a convex
set models the quasi-static dynamics within a fixed contact mode. For each
contact mode, we use semidefinite programming to relax the nonconvex dynamics
that results from the simultaneous optimization of the object's pose, contact
locations, and contact forces. The result is a tight convex relaxation of the
overall planning problem, that can be efficiently solved and quickly rounded to
find a feasible contact-rich trajectory. As a first application of this
technique, we focus on the task of planar pushing. Exhaustive experiments show
that our convex-optimization method generates plans that are consistently
within a small percentage of the global optimum. We demonstrate the quality of
these plans on a real robotic system.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10318" title="Abstract">arXiv:2402.10318</a> [<a href="/pdf/2402.10318" title="Download PDF">pdf</a>, <a href="/format/2402.10318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Antenna Towards Inband Shift Keying
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+R+R">Ralf R. M&#xfc;ller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Multi-antenna towards inband shift keying is a new continuous phase frequency
shift keying that is particularly suited for multi-antenna communications when
the link budget is critical. It combines the constant envelope of frequency
modulation with low-rate repetition coding in order to make transmit
beamforming dispensable.
<br />Although it is a frequency modulation, its transmit signal shows close to
rectangular spectral shape. Similar to GSM's Gaussian minimum shift keying, it
can be well approximated by linear modulation, when combined with differential
precoding. This allows for easy coherent demodulation by means of a windowed
fast Fourier transform.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10321" title="Abstract">arXiv:2402.10321</a> [<a href="/pdf/2402.10321" title="Download PDF">pdf</a>, <a href="/format/2402.10321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LaserSAM: Zero-Shot Change Detection Using Visual Segmentation of  Spinning LiDAR
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krawciw%2C+A">Alexander Krawciw</a>, 
<a href="/search/cs?searchtype=author&query=Lilge%2C+S">Sven Lilge</a>, 
<a href="/search/cs?searchtype=author&query=Barfoot%2C+T+D">Timothy D. Barfoot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages (8 content, 1 references). 9 figures, submitted to the 2024 Conference on Robots and Vision (CRV)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper presents an approach for applying camera perception techniques to
spinning LiDAR data. To improve the robustness of long-term change detection
from a 3D LiDAR, range and intensity information are rendered into virtual
perspectives using a pinhole camera model. Hue-saturation-value image encoding
is used to colourize the images by range and near-IR intensity. The LiDAR's
active scene illumination makes it invariant to ambient brightness, which
enables night-to-day change detection without additional processing. Using the
colourized, perspective range image allows existing foundation models to detect
semantic regions. Specifically, the Segment Anything Model detects semantically
similar regions in both a previously acquired map and live view from a
path-repeating robot. By comparing the masks in both views, changes in the live
scan are detected. Results indicate that the Segment Anything Model is capable
of accurately capturing the shape of arbitrary changes introduced into scenes.
The system achieves an object recall of 82.6% and a precision of 47.0%. Changes
can be detected through day-to-night illumination variations reliably. After
pixel-level masks are generated, the one-to-one correspondence with 3D points
means that the 2D masks can be directly used to recover the 3D location of the
changes. Eventually, the detected 3D changes can be avoided by treating them as
obstacles in a local motion planner.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10323" title="Abstract">arXiv:2402.10323</a> [<a href="/pdf/2402.10323" title="Download PDF">pdf</a>, <a href="/format/2402.10323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Computationally Efficient Learning-Based Model Predictive Control for  Multirotors under Aerodynamic Disturbances
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akbari%2C+B">Babak Akbari</a>, 
<a href="/search/cs?searchtype=author&query=Greeff%2C+M">Melissa Greeff</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Neglecting complex aerodynamic effects hinders high-speed yet high-precision
multirotor autonomy. In this paper, we present a computationally efficient
learning-based model predictive controller that simultaneously optimizes a
trajectory that can be tracked within the physical limits (on thrust and
orientation) of the multirotor system despite unknown aerodynamic forces and
adapts the control input. To do this, we leverage the well-known differential
flatness property of multirotors, which allows us to transform their nonlinear
dynamics into a linear model. The main limitation of current flatness-based
planning and control approaches is that they often neglect dynamic feasibility.
This is because these constraints are nonlinear as a result of the mapping
between the input, i.e., multirotor thrust, and the flat state. In our
approach, we learn a novel representation of the drag forces by learning the
mapping from the flat state to the multirotor thrust vector (in a world frame)
as a Gaussian Process (GP). Our proposed approach leverages the properties of
GPs to develop a convex optimal controller that can be iteratively solved as a
second-order cone program (SOCP). In simulation experiments, our proposed
approach outperforms related model predictive controllers that do not account
for aerodynamic effects on trajectory feasibility, leading to a reduction of up
to 55% in absolute tracking error.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10324" title="Abstract">arXiv:2402.10324</a> [<a href="/pdf/2402.10324" title="Download PDF">pdf</a>, <a href="/format/2402.10324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hacktivism Goes Orbital: Investigating NB65&#x27;s Breach of ROSCOSMOS
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thummala%2C+R">Rajiv Thummala</a>, 
<a href="/search/cs?searchtype=author&query=Falco%2C+G">Gregory Falco</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">In March of 2022, Network battalion 65 (NB65), a hacktivist affiliate of
Anonymous, publicly asserted its successful breach of ROSCOSMOS's satellite
imaging capabilities in response to Russia's invasion of Ukraine. NB65
disseminated a series of primary sources as substantiation, proclaiming the
incapacitation of ROSCOSMOS's space-based vehicle monitoring system and doxing
of related proprietary documentation. Despite the profound implications of
hacktivist incursions into the space sector, the event has garnered limited
attention due to the obscurity of technical attack vectors and ROCOSMOS's
denial of NB65's allegations. Through analysis of NB65's released primary
sources of evidence, this paper uncovers the probable vulnerabilities and
exploits that enabled the alleged breach into ROSCOSMOS's ground and space
segment. Additionally, we highlight lessons learned and the consequences this
event has for the global aerospace community.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10329" title="Abstract">arXiv:2402.10329</a> [<a href="/pdf/2402.10329" title="Download PDF">pdf</a>, <a href="/format/2402.10329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Manipulation Interface: In-The-Wild Robot Teaching Without  In-The-Wild Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chi%2C+C">Cheng Chi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhenjia Xu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+C">Chuer Pan</a>, 
<a href="/search/cs?searchtype=author&query=Cousineau%2C+E">Eric Cousineau</a>, 
<a href="/search/cs?searchtype=author&query=Burchfiel%2C+B">Benjamin Burchfiel</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Siyuan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Tedrake%2C+R">Russ Tedrake</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shuran Song</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project website: <a href="https://umi-gripper.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">We present Universal Manipulation Interface (UMI) -- a data collection and
policy learning framework that allows direct skill transfer from in-the-wild
human demonstrations to deployable robot policies. UMI employs hand-held
grippers coupled with careful interface design to enable portable, low-cost,
and information-rich data collection for challenging bimanual and dynamic
manipulation demonstrations. To facilitate deployable policy learning, UMI
incorporates a carefully designed policy interface with inference-time latency
matching and a relative-trajectory action representation. The resulting learned
policies are hardware-agnostic and deployable across multiple robot platforms.
Equipped with these features, UMI framework unlocks new robot manipulation
capabilities, allowing zero-shot generalizable dynamic, bimanual, precise, and
long-horizon behaviors, by only changing the training data for each task. We
demonstrate UMI's versatility and efficacy with comprehensive real-world
experiments, where policies learned via UMI zero-shot generalize to novel
environments and objects when trained on diverse human demonstrations. UMI's
hardware and software system is open-sourced at https://umi-gripper.github.io.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10334" title="Abstract">arXiv:2402.10334</a> [<a href="/pdf/2402.10334" title="Download PDF">pdf</a>, <a href="/format/2402.10334" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HI-GAN: Hierarchical Inpainting GAN with Auxiliary Inputs for Combined  RGB and Depth Inpainting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dash%2C+A">Ankan Dash</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jingyi Gu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guiling Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Inpainting involves filling in missing pixels or areas in an image, a crucial
technique employed in Mixed Reality environments for various applications,
particularly in Diminished Reality (DR) where content is removed from a user's
visual environment. Existing methods rely on digital replacement techniques
which necessitate multiple cameras and incur high costs. AR devices and
smartphones use ToF depth sensors to capture scene depth maps aligned with RGB
images. Despite speed and affordability, ToF cameras create imperfect depth
maps with missing pixels. To address the above challenges, we propose
Hierarchical Inpainting GAN (HI-GAN), a novel approach comprising three GANs in
a hierarchical fashion for RGBD inpainting. EdgeGAN and LabelGAN inpaint masked
edge and segmentation label images respectively, while CombinedRGBD-GAN
combines their latent representation outputs and performs RGB and Depth
inpainting. Edge images and particularly segmentation label images as auxiliary
inputs significantly enhance inpainting performance by complementary context
and hierarchical optimization. We believe we make the first attempt to
incorporate label images into inpainting process.Unlike previous approaches
requiring multiple sequential models and separate outputs, our work operates in
an end-to-end manner, training all three models simultaneously and
hierarchically. Specifically, EdgeGAN and LabelGAN are first optimized
separately and further optimized inside CombinedRGBD-GAN to enhance inpainting
quality. Experiments demonstrate that HI-GAN works seamlessly and achieves
overall superior performance compared with existing approaches.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10335" title="Abstract">arXiv:2402.10335</a> [<a href="/pdf/2402.10335" title="Download PDF">pdf</a>, <a href="/format/2402.10335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Correlation Clustering with Vertex Splitting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bentert%2C+M">Matthias Bentert</a>, 
<a href="/search/cs?searchtype=author&query=Crane%2C+A">Alex Crane</a>, 
<a href="/search/cs?searchtype=author&query=Drange%2C+P+G">P&#xe5;l Gr&#xf8;n&#xe5;s Drange</a>, 
<a href="/search/cs?searchtype=author&query=Reidl%2C+F">Felix Reidl</a>, 
<a href="/search/cs?searchtype=author&query=Sullivan%2C+B+D">Blair D. Sullivan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">We explore Cluster Editing and its generalization Correlation Clustering with
a new operation called permissive vertex splitting which addresses finding
overlapping clusters in the face of uncertain information. We determine that
both problems are NP-hard, yet they exhibit significant differences in
parameterized complexity and approximability. For Cluster Editing with
Permissive Vertex Splitting, we show a polynomial kernel when parameterized by
the solution size and develop a polynomial-time algorithm with approximation
factor 7. In the case of Correlation Clustering, we establish para-NP-hardness
when parameterized by solution size and demonstrate that computing an
$n^{1-\epsilon}$-approximation is NP-hard for any constant $\epsilon &gt; 0$.
Additionally, we extend the established link between Correlation Clustering and
Multicut to the setting with permissive vertex splitting.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10339" title="Abstract">arXiv:2402.10339</a> [<a href="/pdf/2402.10339" title="Download PDF">pdf</a>, <a href="/format/2402.10339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What to Do When Your Discrete Optimization Is the Size of a Neural  Network?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Silva%2C+H">Hugo Silva</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+M">Martha White</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to JMLR
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Oftentimes, machine learning applications using neural networks involve
solving discrete optimization problems, such as in pruning,
parameter-isolation-based continual learning and training of binary networks.
Still, these discrete problems are combinatorial in nature and are also not
amenable to gradient-based optimization. Additionally, classical approaches
used in discrete settings do not scale well to large neural networks, forcing
scientists and empiricists to rely on alternative methods. Among these, two
main distinct sources of top-down information can be used to lead the model to
good solutions: (1) extrapolating gradient information from points outside of
the solution set (2) comparing evaluations between members of a subset of the
valid solutions. We take continuation path (CP) methods to represent using
purely the former and Monte Carlo (MC) methods to represent the latter, while
also noting that some hybrid methods combine the two. The main goal of this
work is to compare both approaches. For that purpose, we first overview the two
classes while also discussing some of their drawbacks analytically. Then, on
the experimental section, we compare their performance, starting with smaller
microworld experiments, which allow more fine-grained control of problem
variables, and gradually moving towards larger problems, including neural
network regression and neural network pruning for image classification, where
we additionally compare against magnitude-based pruning.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10340" title="Abstract">arXiv:2402.10340</a> [<a href="/pdf/2402.10340" title="Download PDF">pdf</a>, <a href="/format/2402.10340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting  the Risks and Vulnerabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiyang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xian%2C+R">Ruiqi Xian</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+T">Tianrui Guan</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jing Liang</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Souradip Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fuxiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sadler%2C+B">Brian Sadler</a>, 
<a href="/search/cs?searchtype=author&query=Manocha%2C+D">Dinesh Manocha</a>, 
<a href="/search/cs?searchtype=author&query=Bedi%2C+A+S">Amrit Singh Bedi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper, we highlight the critical issues of robustness and safety
associated with integrating large language models (LLMs) and vision-language
models (VLMs) into robotics applications. Recent works have focused on using
LLMs and VLMs to improve the performance of robotics tasks, such as
manipulation, navigation, etc. However, such integration can introduce
significant vulnerabilities, in terms of their susceptibility to adversarial
attacks due to the language models, potentially leading to catastrophic
consequences. By examining recent works at the interface of LLMs/VLMs and
robotics, we show that it is easy to manipulate or misguide the robot's
actions, leading to safety hazards. We define and provide examples of several
plausible adversarial attacks, and conduct experiments on three prominent robot
frameworks integrated with a language model, including KnowNo VIMA, and
Instruct2Act, to assess their susceptibility to these attacks. Our empirical
findings reveal a striking vulnerability of LLM/VLM-robot integrated systems:
simple adversarial attacks can significantly undermine the effectiveness of
LLM/VLM-robot integrated systems. Specifically, our data demonstrate an average
performance deterioration of 21.2% under prompt attacks and a more alarming
30.2% under perception attacks. These results underscore the critical need for
robust countermeasures to ensure the safe and reliable deployment of the
advanced LLM/VLM-based robotic systems.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10342" title="Abstract">arXiv:2402.10342</a> [<a href="/pdf/2402.10342" title="Download PDF">pdf</a>, <a href="/ps/2402.10342" title="Download PostScript">ps</a>, <a href="/format/2402.10342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploration-Driven Policy Optimization in RLHF: Theoretical Insights on  Efficient Data Utilization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yihan Du</a>, 
<a href="/search/cs?searchtype=author&query=Winnicki%2C+A">Anna Winnicki</a>, 
<a href="/search/cs?searchtype=author&query=Dalal%2C+G">Gal Dalal</a>, 
<a href="/search/cs?searchtype=author&query=Mannor%2C+S">Shie Mannor</a>, 
<a href="/search/cs?searchtype=author&query=Srikant%2C+R">R. Srikant</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Reinforcement Learning from Human Feedback (RLHF) has achieved impressive
empirical successes while relying on a small amount of human feedback. However,
there is limited theoretical justification for this phenomenon. Additionally,
most recent studies focus on value-based algorithms despite the recent
empirical successes of policy-based algorithms. In this work, we consider an
RLHF algorithm based on policy optimization (PO-RLHF). The algorithm is based
on the popular Policy Cover-Policy Gradient (PC-PG) algorithm, which assumes
knowledge of the reward function. In PO-RLHF, knowledge of the reward function
is not assumed and the algorithm relies on trajectory-based comparison feedback
to infer the reward function. We provide performance bounds for PO-RLHF with
low query complexity, which provides insight into why a small amount of human
feedback may be sufficient to get good performance with RLHF. A key novelty is
our trajectory-level elliptical potential analysis technique used to infer
reward function parameters when comparison queries rather than reward
observations are used. We provide and analyze algorithms in two settings:
linear and neural function approximation, PG-RLHF and NN-PG-RLHF, respectively.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10343" title="Abstract">arXiv:2402.10343</a> [<a href="/pdf/2402.10343" title="Download PDF">pdf</a>, <a href="/ps/2402.10343" title="Download PostScript">ps</a>, <a href="/format/2402.10343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-adaptive Bellman-Ford: Yen&#x27;s improvement is optimal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jialu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Kozma%2C+L">L&#xe1;szl&#xf3; Kozma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Combinatorics (math.CO)

</div>
<p class="mathjax">The Bellman-Ford algorithm for single-source shortest paths repeatedly
updates tentative distances in an operation called relaxing an edge. In several
important applications a non-adaptive (oblivious) implementation is preferred,
which means fixing the entire sequence of relaxations upfront, independent of
the edge-weights. In a dense graph on $n$ vertices, the algorithm in its
standard form performs $(1 + o(1))n^3$ relaxations. An improvement by Yen from
1970 reduces the number of relaxations by a factor of two. We show that no
further constant-factor improvements are possible, and every non-adaptive
deterministic algorithm based on relaxations must perform $(\frac{1}{2} -
o(1))n^3$ steps. This improves an earlier lower bound of Eppstein of
$(\frac{1}{6} - o(1))n^3$. Given that a non-adaptive randomized variant of
Bellman-Ford with at most $(\frac{1}{3} + o(1))n^3$ relaxations (with high
probability) is known, our result implies a strict separation between
deterministic and randomized strategies, answering an open question of
Eppstein.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10344" title="Abstract">arXiv:2402.10344</a> [<a href="/pdf/2402.10344" title="Download PDF">pdf</a>, <a href="/format/2402.10344" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating NeRFs for 3D Plant Geometry Reconstruction in Field  Conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arshad%2C+M+A">Muhammad Arbab Arshad</a>, 
<a href="/search/cs?searchtype=author&query=Jubery%2C+T">Talukder Jubery</a>, 
<a href="/search/cs?searchtype=author&query=Afful%2C+J">James Afful</a>, 
<a href="/search/cs?searchtype=author&query=Jignasu%2C+A">Anushrut Jignasu</a>, 
<a href="/search/cs?searchtype=author&query=Balu%2C+A">Aditya Balu</a>, 
<a href="/search/cs?searchtype=author&query=Ganapathysubramanian%2C+B">Baskar Ganapathysubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+S">Soumik Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+A">Adarsh Krishnamurthy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We evaluate different Neural Radiance Fields (NeRFs) techniques for
reconstructing (3D) plants in varied environments, from indoor settings to
outdoor fields. Traditional techniques often struggle to capture the complex
details of plants, which is crucial for botanical and agricultural
understanding. We evaluate three scenarios with increasing complexity and
compare the results with the point cloud obtained using LiDAR as ground truth
data. In the most realistic field scenario, the NeRF models achieve a 74.65% F1
score with 30 minutes of training on the GPU, highlighting the efficiency and
accuracy of NeRFs in challenging environments. These findings not only
demonstrate the potential of NeRF in detailed and realistic 3D plant modeling
but also suggest practical approaches for enhancing the speed and efficiency of
the 3D reconstruction process.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10346" title="Abstract">arXiv:2402.10346</a> [<a href="/pdf/2402.10346" title="Download PDF">pdf</a>, <a href="/format/2402.10346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wavelet compressed, modified Hilbert transform in the space-time  discretization of the heat equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Harbrecht%2C+H">Helmut Harbrecht</a>, 
<a href="/search/math?searchtype=author&query=Schwab%2C+C">Christoph Schwab</a>, 
<a href="/search/math?searchtype=author&query=Zank%2C+M">Marco Zank</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">On a finite time interval $(0,T)$, we consider the multiresolution Galerkin
discretization of a modified Hilbert transform $\mathcal H_T$ which arises in
the space-time Galerkin discretization of the linear diffusion equation. To
this end, we design spline-wavelet systems in $(0,T)$ consisting of piecewise
polynomials of degree $\geq 1$ with sufficiently many vanishing moments which
constitute Riesz bases in the Sobolev spaces $ H^{s}_{0,}(0,T)$ and $
H^{s}_{,0}(0,T)$. These bases provide multilevel splittings of the temporal
discretization spaces into "increment" or "detail" spaces of direct sum type.
Via algebraic tensor-products of these temporal multilevel discretizations with
standard, hierarchic finite element spaces in the spatial domain (with standard
Lagrangian FE bases), sparse space-time tensor-product spaces are obtained,
which afford a substantial reduction in the number of the degrees of freedom as
compared to time-marching discretizations. In addition, temporal spline-wavelet
bases allow to compress certain nonlocal integrodifferential operators which
appear in stable space-time variational formulations of initial-boundary value
problems, such as the heat equation and the acoustic wave equation. An
efficient preconditioner is proposed that affords linear complexity solves of
the linear system of equations which results from the sparse space-time
Galerkin discretization.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10350" title="Abstract">arXiv:2402.10350</a> [<a href="/pdf/2402.10350" title="Download PDF">pdf</a>, <a href="/format/2402.10350" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models for Forecasting and Anomaly Detection: A  Systematic Literature Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jing Su</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Chufeng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xin Jin</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yuxin Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+T">Tingsong Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Hongda Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+R">Rong Wei</a>, 
<a href="/search/cs?searchtype=author&query=Jing%2C+Z">Zhi Jing</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiajun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Junhong Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This systematic literature review comprehensively examines the application of
Large Language Models (LLMs) in forecasting and anomaly detection, highlighting
the current state of research, inherent challenges, and prospective future
directions. LLMs have demonstrated significant potential in parsing and
analyzing extensive datasets to identify patterns, predict future events, and
detect anomalous behavior across various domains. However, this review
identifies several critical challenges that impede their broader adoption and
effectiveness, including the reliance on vast historical datasets, issues with
generalizability across different contexts, the phenomenon of model
hallucinations, limitations within the models' knowledge boundaries, and the
substantial computational resources required. Through detailed analysis, this
review discusses potential solutions and strategies to overcome these
obstacles, such as integrating multimodal data, advancements in learning
methodologies, and emphasizing model explainability and computational
efficiency. Moreover, this review outlines critical trends that are likely to
shape the evolution of LLMs in these fields, including the push toward
real-time processing, the importance of sustainable modeling practices, and the
value of interdisciplinary collaboration. Conclusively, this review underscores
the transformative impact LLMs could have on forecasting and anomaly detection
while emphasizing the need for continuous innovation, ethical considerations,
and practical solutions to realize their full potential.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10351" title="Abstract">arXiv:2402.10351</a> [<a href="/pdf/2402.10351" title="Download PDF">pdf</a>, <a href="/ps/2402.10351" title="Download PostScript">ps</a>, <a href="/format/2402.10351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Automating $\mathbf{TC}^0$-Frege Is LWE-Hard
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arteche%2C+N">Noel Arteche</a>, 
<a href="/search/cs?searchtype=author&query=Carenini%2C+G">Gaia Carenini</a>, 
<a href="/search/cs?searchtype=author&query=Gray%2C+M">Matthew Gray</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
<p class="mathjax">We prove the first hardness results against efficient proof search by quantum
algorithms. We show that under Learning with Errors (LWE), the standard
lattice-based cryptographic assumption, no quantum algorithm can weakly
automate $\mathbf{TC}^0$-Frege. This extends the line of results of
Kraj\'i\v{c}ek and Pudl\'ak (Information and Computation, 1998), Bonet,
Pitassi, and Ray (FOCS, 1997), and Bonet et al. (Computational Complexity,
2004), who showed that Extended Frege, $\mathbf{TC}^0$-Frege and
$\mathbf{AC}^0$-Frege, respectively, cannot be weakly automated by classical
algorithms if either the RSA cryptosystem or the Diffie-Hellman key exchange
protocol are secure. To the best of our knowledge, this is the first
interaction between quantum computation and propositional proof search.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10353" title="Abstract">arXiv:2402.10353</a> [<a href="/pdf/2402.10353" title="Download PDF">pdf</a>, <a href="/format/2402.10353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt-Based Bias Calibration for Better Zero/Few-Shot Learning of  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+K">Kang He</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+Y">Yinghan Long</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+K">Kaushik Roy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Prompt learning is susceptible to intrinsic bias present in pre-trained
language models (LMs), resulting in sub-optimal performance of prompt-based
zero/few-shot learning. In this work, we propose a null-input prompting method
to calibrate intrinsic bias encoded in pre-trained LMs. Different from prior
efforts that address intrinsic bias primarily for social fairness and often
involve excessive computational cost, our objective is to explore enhancing
LMs' performance in downstream zero/few-shot learning while emphasizing the
efficiency of intrinsic bias calibration. Specifically, we leverage a diverse
set of auto-selected null-meaning inputs generated from GPT-4 to prompt
pre-trained LMs for intrinsic bias probing. Utilizing the bias-reflected
probability distribution, we formulate a distribution disparity loss for bias
calibration, where we exclusively update bias parameters ($0.1\%$ of total
parameters) of LMs towards equal probability distribution. Experimental results
show that the calibration promotes an equitable starting point for LMs while
preserving language modeling abilities. Across a wide range of datasets,
including sentiment analysis and topic classification, our method significantly
improves zero/few-shot learning performance of LMs for both in-context learning
and prompt-based fine-tuning (on average $9\%$ and $2\%$, respectively).
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10359" title="Abstract">arXiv:2402.10359</a> [<a href="/pdf/2402.10359" title="Download PDF">pdf</a>, <a href="/format/2402.10359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can we soft prompt LLMs for graph learning tasks?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zheyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiaoxin He</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yijun Tian</a>, 
<a href="/search/cs?searchtype=author&query=Chawla%2C+N+V">Nitesh V. Chawla</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages including the references
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Graph plays an important role in representing complex relationships in
real-world applications such as social networks, biological data and citation
networks. In recent years, Large Language Models (LLMs) have achieved
tremendous success in various domains, which makes applying LLMs to graphs
particularly appealing. However, directly applying LLMs to graph modalities
presents unique challenges due to the discrepancy and mismatch between the
graph and text modalities. Hence, to further investigate LLMs' potential for
comprehending graph information, we introduce GraphPrompter, a novel framework
designed to align graph information with LLMs via soft prompts. Specifically,
GraphPrompter consists of two main components: a graph neural network to encode
complex graph information and an LLM that effectively processes textual
information. Comprehensive experiments on various benchmark datasets under node
classification and link prediction tasks demonstrate the effectiveness of our
proposed method. The GraphPrompter framework unveils the substantial
capabilities of LLMs as predictors in graph-related tasks, enabling researchers
to utilize LLMs across a spectrum of real-world graph scenarios more
effectively.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10360" title="Abstract">arXiv:2402.10360</a> [<a href="/pdf/2402.10360" title="Download PDF">pdf</a>, <a href="/ps/2402.10360" title="Download PostScript">ps</a>, <a href="/format/2402.10360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learnability is a Compact Property
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asilis%2C+J">Julian Asilis</a>, 
<a href="/search/cs?searchtype=author&query=Devic%2C+S">Siddartha Devic</a>, 
<a href="/search/cs?searchtype=author&query=Dughmi%2C+S">Shaddin Dughmi</a>, 
<a href="/search/cs?searchtype=author&query=Sharan%2C+V">Vatsal Sharan</a>, 
<a href="/search/cs?searchtype=author&query=Teng%2C+S">Shang-Hua Teng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Complexity (cs.CC); Data Structures and Algorithms (cs.DS); Logic in Computer Science (cs.LO); Machine Learning (stat.ML)

</div>
<p class="mathjax">Recent work on learning has yielded a striking result: the learnability of
various problems can be undecidable, or independent of the standard ZFC axioms
of set theory. Furthermore, the learnability of such problems can fail to be a
property of finite character: informally, it cannot be detected by examining
finite projections of the problem.
<br />On the other hand, learning theory abounds with notions of dimension that
characterize learning and consider only finite restrictions of the problem,
i.e., are properties of finite character. How can these results be reconciled?
More precisely, which classes of learning problems are vulnerable to logical
undecidability, and which are within the grasp of finite characterizations?
<br />We demonstrate that the difficulty of supervised learning with metric losses
admits a tight finite characterization. In particular, we prove that the sample
complexity of learning a hypothesis class can be detected by examining its
finite projections. For realizable and agnostic learning with respect to a wide
class of proper loss functions, we demonstrate an exact compactness result: a
class is learnable with a given sample complexity precisely when the same is
true of all its finite projections. For realizable learning with improper loss
functions, we show that exact compactness of sample complexity can fail, and
provide matching upper and lower bounds of a factor of 2 on the extent to which
such sample complexities can differ. We conjecture that larger gaps are
possible for the agnostic case.
<br />At the heart of our technical work is a compactness result concerning
assignments of variables that maintain a class of functions below a target
value, which generalizes Hall's classic matching theorem and may be of
independent interest.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10365" title="Abstract">arXiv:2402.10365</a> [<a href="/pdf/2402.10365" title="Download PDF">pdf</a>, <a href="/format/2402.10365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Spectral Meshes: Multi-Frequency Facial Mesh Processing with Graph  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kosk%2C+R">Robert Kosk</a>, 
<a href="/search/cs?searchtype=author&query=Southern%2C+R">Richard Southern</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+L">Lihua You</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+S">Shaojun Bian</a>, 
<a href="/search/cs?searchtype=author&query=Kokke%2C+W">Willem Kokke</a>, 
<a href="/search/cs?searchtype=author&query=Maguire%2C+G">Greg Maguire</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 10 figures, journal article
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Electronics. 2024; 13(4):720
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computational Geometry (cs.CG); Graphics (cs.GR)

</div>
<p class="mathjax">With the rising popularity of virtual worlds, the importance of data-driven
parametric models of 3D meshes has grown rapidly. Numerous applications, such
as computer vision, procedural generation, and mesh editing, vastly rely on
these models. However, current approaches do not allow for independent editing
of deformations at different frequency levels. They also do not benefit from
representing deformations at different frequencies with dedicated
representations, which would better expose their properties and improve the
generated meshes' geometric and perceptual quality. In this work, spectral
meshes are introduced as a method to decompose mesh deformations into
low-frequency and high-frequency deformations. These features of low- and
high-frequency deformations are used for representation learning with graph
convolutional networks. A parametric model for 3D facial mesh synthesis is
built upon the proposed framework, exposing user parameters that control
disentangled high- and low-frequency deformations. Independent control of
deformations at different frequencies and generation of plausible synthetic
examples are mutually exclusive objectives. A Conditioning Factor is introduced
to leverage these objectives. Our model takes further advantage of spectral
partitioning by representing different frequency levels with disparate, more
suitable representations. Low frequencies are represented with standardised
Euclidean coordinates, and high frequencies with a normalised deformation
representation (DR). This paper investigates applications of our proposed
approach in mesh reconstruction, mesh interpolation, and multi-frequency
editing. It is demonstrated that our method improves the overall quality of
generated meshes on most datasets when considering both the $L_1$ norm and
perceptual Dihedral Angle Mesh Error (DAME) metrics.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10372" title="Abstract">arXiv:2402.10372</a> [<a href="/pdf/2402.10372" title="Download PDF">pdf</a>, <a href="/format/2402.10372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sequential Manipulation of Deformable Linear Object Networks with  Endpoint Pose Measurements using Adaptive Model Predictive Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Toner%2C+T">Tyler Toner</a>, 
<a href="/search/cs?searchtype=author&query=Molazadeh%2C+V">Vahidreza Molazadeh</a>, 
<a href="/search/cs?searchtype=author&query=Saez%2C+M">Miguel Saez</a>, 
<a href="/search/cs?searchtype=author&query=Tilbury%2C+D+M">Dawn M. Tilbury</a>, 
<a href="/search/cs?searchtype=author&query=Barton%2C+K">Kira Barton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE International Conference on Robotics and Automation - ICRA 2024. 7 pages. 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Robotic manipulation of deformable linear objects (DLOs) is an active area of
research, though emerging applications, like automotive wire harness
installation, introduce constraints that have not been considered in prior
work. Confined workspaces and limited visibility complicate prior assumptions
of multi-robot manipulation and direct measurement of DLO configuration
(state). This work focuses on single-arm manipulation of stiff DLOs (StDLOs)
connected to form a DLO network (DLON), for which the measurements (output) are
the endpoint poses of the DLON, which are subject to unknown dynamics during
manipulation. To demonstrate feasibility of output-based control without state
estimation, direct input-output dynamics are shown to exist by training neural
network models on simulated trajectories. Output dynamics are then approximated
with polynomials and found to contain well-known rigid body dynamics terms. A
composite model consisting of a rigid body model and an online data-driven
residual is developed, which predicts output dynamics more accurately than
either model alone, and without prior experience with the system. An adaptive
model predictive controller is developed with the composite model for DLON
manipulation, which completes DLON installation tasks, both in simulation and
with a physical automotive wire harness.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10373" title="Abstract">arXiv:2402.10373</a> [<a href="/pdf/2402.10373" title="Download PDF">pdf</a>, <a href="/format/2402.10373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BioMistral: A Collection of Open-Source Pretrained Large Language Models  for Medical Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Labrak%2C+Y">Yanis Labrak</a>, 
<a href="/search/cs?searchtype=author&query=Bazoge%2C+A">Adrien Bazoge</a>, 
<a href="/search/cs?searchtype=author&query=Morin%2C+E">Emmanuel Morin</a>, 
<a href="/search/cs?searchtype=author&query=Gourraud%2C+P">Pierre-Antoine Gourraud</a>, 
<a href="/search/cs?searchtype=author&query=Rouvier%2C+M">Mickael Rouvier</a>, 
<a href="/search/cs?searchtype=author&query=Dufour%2C+R">Richard Dufour</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated remarkable versatility in
recent years, offering potential applications across specialized domains such
as healthcare and medicine. Despite the availability of various open-source
LLMs tailored for health contexts, adapting general-purpose LLMs to the medical
domain presents significant challenges. In this paper, we introduce BioMistral,
an open-source LLM tailored for the biomedical domain, utilizing Mistral as its
foundation model and further pre-trained on PubMed Central. We conduct a
comprehensive evaluation of BioMistral on a benchmark comprising 10 established
medical question-answering (QA) tasks in English. We also explore lightweight
models obtained through quantization and model merging approaches. Our results
demonstrate BioMistral's superior performance compared to existing open-source
medical models and its competitive edge against proprietary counterparts.
Finally, to address the limited availability of data beyond English and to
assess the multilingual generalization of medical LLMs, we automatically
translated and evaluated this benchmark into 7 other languages. This marks the
first large-scale multilingual evaluation of LLMs in the medical domain.
Datasets, multilingual evaluation benchmarks, scripts, and all the models
obtained during our experiments are freely released.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10374" title="Abstract">arXiv:2402.10374</a> [<a href="/pdf/2402.10374" title="Download PDF">pdf</a>, <a href="/format/2402.10374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Experience Replayable Conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kobayashi%2C+T">Taisuke Kobayashi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Experience replay (ER) used in (deep) reinforcement learning is considered to
be applicable only to off-policy algorithms. However, there have been some
cases in which ER has been applied for on-policy algorithms, suggesting that
off-policyness might be a sufficient condition for applying ER. This paper
reconsiders more strict "experience replayable conditions" (ERC) and proposes
the way of modifying the existing algorithms to satisfy ERC. To this end,
instability of policy improvements is assumed to be a key in ERC. The
instability factors are revealed from the viewpoint of metric learning as i)
repulsive forces from negative samples and ii) replays of inappropriate
experiences. Accordingly, the corresponding stabilization tricks are derived.
As a result, it is confirmed through numerical simulations that the proposed
stabilization tricks make ER applicable to an advantage actor-critic, an
on-policy algorithm. In addition, its learning performance is comparable to
that of a soft actor-critic, a state-of-the-art off-policy algorithm.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10376" title="Abstract">arXiv:2402.10376</a> [<a href="/pdf/2402.10376" title="Download PDF">pdf</a>, <a href="/format/2402.10376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhalla%2C+U">Usha Bhalla</a>, 
<a href="/search/cs?searchtype=author&query=Oesterling%2C+A">Alex Oesterling</a>, 
<a href="/search/cs?searchtype=author&query=Srinivas%2C+S">Suraj Srinivas</a>, 
<a href="/search/cs?searchtype=author&query=Calmon%2C+F+P">Flavio P. Calmon</a>, 
<a href="/search/cs?searchtype=author&query=Lakkaraju%2C+H">Himabindu Lakkaraju</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 8 figures, Code is provided at <a href="https://github.com/AI4LIFE-GROUP/SpLiCE">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">CLIP embeddings have demonstrated remarkable performance across a wide range
of computer vision tasks. However, these high-dimensional, dense vector
representations are not easily interpretable, restricting their usefulness in
downstream applications that require transparency. In this work, we empirically
show that CLIP's latent space is highly structured, and consequently that CLIP
representations can be decomposed into their underlying semantic components. We
leverage this understanding to propose a novel method, Sparse Linear Concept
Embeddings (SpLiCE), for transforming CLIP representations into sparse linear
combinations of human-interpretable concepts. Distinct from previous work,
SpLiCE does not require concept labels and can be applied post hoc. Through
extensive experimentation with multiple real-world datasets, we validate that
the representations output by SpLiCE can explain and even replace traditional
dense CLIP representations, maintaining equivalent downstream performance while
significantly improving their interpretability. We also demonstrate several use
cases of SpLiCE representations including detecting spurious correlations,
model editing, and quantifying semantic shifts in datasets.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10379" title="Abstract">arXiv:2402.10379</a> [<a href="/pdf/2402.10379" title="Download PDF">pdf</a>, <a href="/format/2402.10379" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM  Workflows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Patel%2C+A">Ajay Patel</a>, 
<a href="/search/cs?searchtype=author&query=Raffel%2C+C">Colin Raffel</a>, 
<a href="/search/cs?searchtype=author&query=Callison-Burch%2C+C">Chris Callison-Burch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) have become a dominant and important tool for
NLP researchers in a wide range of tasks. Today, many researchers use LLMs in
synthetic data generation, task evaluation, fine-tuning, distillation, and
other model-in-the-loop research workflows. However, challenges arise when
using these models that stem from their scale, their closed source nature, and
the lack of standardized tooling for these new and emerging workflows. The
rapid rise to prominence of these models and these unique challenges has had
immediate adverse impacts on open science and on the reproducibility of work
that uses them. In this paper, we introduce DataDreamer, an open source Python
library that allows researchers to write simple code to implement powerful LLM
workflows. DataDreamer also helps researchers adhere to best practices that we
propose to encourage open science and reproducibility. The library and
documentation are available at https://github.com/datadreamer-dev/DataDreamer .
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10380" title="Abstract">arXiv:2402.10380</a> [<a href="/pdf/2402.10380" title="Download PDF">pdf</a>, <a href="/format/2402.10380" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subgraph-level Universal Prompt Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Junhyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wooseong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jaewoo Kang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the evolving landscape of machine learning, the adaptation of pre-trained
models through prompt tuning has become increasingly prominent. This trend is
particularly observable in the graph domain, where diverse pre-training
strategies present unique challenges in developing effective prompt-based
tuning methods for graph neural networks. Previous approaches have been
limited, focusing on specialized prompting functions tailored to models with
edge prediction pre-training tasks. These methods, however, suffer from a lack
of generalizability across different pre-training strategies. Recently, a
simple prompt tuning method has been designed for any pre-training strategy,
functioning within the input graph's feature space. This allows it to
theoretically emulate any type of prompting function, thereby significantly
increasing its versatility for a range of downstream applications.
Nevertheless, the capacity of such simple prompts to fully grasp the complex
contexts found in graphs remains an open question, necessitating further
investigation. Addressing this challenge, our work introduces the
Subgraph-level Universal Prompt Tuning (SUPT) approach, focusing on the
detailed context within subgraphs. In SUPT, prompt features are assigned at the
subgraph-level, preserving the method's universal capability. This requires
extremely fewer tuning parameters than fine-tuning-based methods, outperforming
them in 42 out of 45 full-shot scenario experiments with an average improvement
of over 2.5%. In few-shot scenarios, it excels in 41 out of 45 experiments,
achieving an average performance increase of more than 6.6%.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10381" title="Abstract">arXiv:2402.10381</a> [<a href="/pdf/2402.10381" title="Download PDF">pdf</a>, <a href="/format/2402.10381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UMAIR-FPS: User-aware Multi-modal Animation Illustration Recommendation  Fusion with Painting Style
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+Y">Yan Kang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Mingjian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Shin-Jye Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The rapid advancement of high-quality image generation models based on AI has
generated a deluge of anime illustrations. Recommending illustrations to users
within massive data has become a challenging and popular task. However,
existing anime recommendation systems have focused on text features but still
need to integrate image features. In addition, most multi-modal recommendation
research is constrained by tightly coupled datasets, limiting its applicability
to anime illustrations. We propose the User-aware Multi-modal Animation
Illustration Recommendation Fusion with Painting Style (UMAIR-FPS) to tackle
these gaps. In the feature extract phase, for image features, we are the first
to combine image painting style features with semantic features to construct a
dual-output image encoder for enhancing representation. For text features, we
obtain text embeddings based on fine-tuning Sentence-Transformers by
incorporating domain knowledge that composes a variety of domain text pairs
from multilingual mappings, entity relationships, and term explanation
perspectives, respectively. In the multi-modal fusion phase, we novelly propose
a user-aware multi-modal contribution measurement mechanism to weight
multi-modal features dynamically according to user features at the interaction
level and employ the DCN-V2 module to model bounded-degree multi-modal crosses
effectively. UMAIR-FPS surpasses the stat-of-the-art baselines on large
real-world datasets, demonstrating substantial performance enhancements.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10382" title="Abstract">arXiv:2402.10382</a> [<a href="/pdf/2402.10382" title="Download PDF">pdf</a>, <a href="/format/2402.10382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Making Short-Form Videos Accessible with Hierarchical Video Summaries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Van+Daele%2C+T">Tess Van Daele</a>, 
<a href="/search/cs?searchtype=author&query=Iyer%2C+A">Akhil Iyer</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuning Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Derry%2C+J+C">Jalyn C. Derry</a>, 
<a href="/search/cs?searchtype=author&query=Huh%2C+M">Mina Huh</a>, 
<a href="/search/cs?searchtype=author&query=Pavel%2C+A">Amy Pavel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at CHI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Short videos on platforms such as TikTok, Instagram Reels, and YouTube Shorts
(i.e. short-form videos) have become a primary source of information and
entertainment. Many short-form videos are inaccessible to blind and low vision
(BLV) viewers due to their rapid visual changes, on-screen text, and music or
meme-audio overlays. In our formative study, 7 BLV viewers who regularly
watched short-form videos reported frequently skipping such inaccessible
content. We present ShortScribe, a system that provides hierarchical visual
summaries of short-form videos at three levels of detail to support BLV viewers
in selecting and understanding short-form videos. ShortScribe allows BLV users
to navigate between video descriptions based on their level of interest. To
evaluate ShortScribe, we assessed description accuracy and conducted a user
study with 10 BLV participants comparing ShortScribe to a baseline interface.
When using ShortScribe, participants reported higher comprehension and provided
more accurate summaries of video content.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10385" title="Abstract">arXiv:2402.10385</a> [<a href="/pdf/2402.10385" title="Download PDF">pdf</a>, <a href="/ps/2402.10385" title="Download PostScript">ps</a>, <a href="/format/2402.10385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Middleware-based multi-agent development environment for building and  testing distributed intelligent systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aguayo-Canela%2C+F+J">Francisco Jos&#xe9; Aguayo-Canela</a>, 
<a href="/search/cs?searchtype=author&query=Alaiz-Moret%C3%B3n%2C+H">H&#xe9;ctor Alaiz-Moret&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Ord%C3%A1s%2C+M+T">Mar&#xed;a Teresa Garc&#xed;a-Ord&#xe1;s</a>, 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>, 
<a href="/search/cs?searchtype=author&query=Benavides%2C+C">Carmen Benavides</a>, 
<a href="/search/cs?searchtype=author&query=Novais%2C+P">Paulo Novais</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Rodr%C3%ADguez%2C+I">Isa&#xed;as Garc&#xed;a-Rodr&#xed;guez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2402.09499">arXiv:2402.09499</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Cluster Computing, 2021, Volume 24, pp 2313-2325
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">The spread of the Internet of Things (IoT) is demanding new, powerful
architectures for handling the huge amounts of data produced by the IoT
devices. In many scenarios, many existing isolated solutions applied to IoT
devices use a set of rules to detect, report and mitigate malware activities or
threats. This paper describes a development environment that allows the
programming and debugging of such rule-based multi-agent solutions. The
solution consists of the integration of a rule engine into the agent, the use
of a specialized, wrapping agent class with a graphical user interface for
programming and testing purposes, and a mechanism for the incremental
composition of behaviors. Finally, a set of examples and a comparative study
were accomplished to test the suitability and validity of the approach. The
JADE multi-agent middleware has been used for the practical implementation of
the approach.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10388" title="Abstract">arXiv:2402.10388</a> [<a href="/pdf/2402.10388" title="Download PDF">pdf</a>, <a href="/ps/2402.10388" title="Download PostScript">ps</a>, <a href="/format/2402.10388" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improvising Age Verification Technologies in Canada: Technical,  Regulatory and Social Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adib%2C+A">Azfar Adib</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wei-Ping Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+M+O">M. Omair Ahmad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented and accepted for publication in the 2023 IEEE International Humanitarian Technologies Conference (IEEE IHTC 2023), November 1 to 3, 2023, Cartagena, Colombia
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Age verification, which is a mandatory legal requirement for delivering
certain age-appropriate services or products, has recently been emphasized
around the globe to ensure online safety for children. The rapid advancement of
artificial intelligence has facilitated the recent development of some
cutting-edge age-verification technologies, particularly using biometrics.
However, successful deployment and mass acceptance of these technologies are
significantly dependent on the corresponding socio-economic and regulatory
context. This paper reviews such key dynamics for improvising age-verification
technologies in Canada. It is particularly essential for such technologies to
be inclusive, transparent, adaptable, privacy-preserving, and secure. Effective
collaboration between academia, government, and industry entities can help to
meet the growing demands for age-verification services in Canada while
maintaining a user-centric approach.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10389" title="Abstract">arXiv:2402.10389</a> [<a href="/pdf/2402.10389" title="Download PDF">pdf</a>, <a href="/ps/2402.10389" title="Download PostScript">ps</a>, <a href="/format/2402.10389" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling Zero Trust Security in IoMT Edge Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Allouzi%2C+M+A">Maha Ali Allouzi</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+J">Javed Khan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Internet of Medical Things (IoMT) deals with a patient-data-rich segment,
which makes security and privacy a severe concern for patients. Therefore,
access control is a significant aspect of ensuring trust in the IoMT. However,
deploying existing authentication and authorization solutions to the Internet
of Medical Things (IoMT) is not straightforward because of highly dynamic and
possibly unprotected environments and untrusted supply chain for the IoT
devices. In this article, we propose Soter, a Zero-Trust based authentication
system for the IoMT. Soter Incorporates trust negotiation mechanisms within the
Zero Trust framework to enable dynamic trust establishment. When a user or
device seeks access to a resource, initiate a trust negotiation process. During
this process, credentials, attributes, and contextual information are exchanged
between the requester and the resource owner. Soter defines access rules based
on various factors, including user identity, device health, and location.
Access is granted or denied based on these conditions.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10392" title="Abstract">arXiv:2402.10392</a> [<a href="/pdf/2402.10392" title="Download PDF">pdf</a>, <a href="/format/2402.10392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pretext Training Algorithms for Event Sequence Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yimu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">He Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+R">Ruizhi Deng</a>, 
<a href="/search/cs?searchtype=author&query=Tung%2C+F">Frederick Tung</a>, 
<a href="/search/cs?searchtype=author&query=Mori%2C+G">Greg Mori</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Pretext training followed by task-specific fine-tuning has been a successful
approach in vision and language domains. This paper proposes a self-supervised
pretext training framework tailored to event sequence data. We introduce a
novel alignment verification task that is specialized to event sequences,
building on good practices in masked reconstruction and contrastive learning.
Our pretext tasks unlock foundational representations that are generalizable
across different down-stream tasks, including next-event prediction for
temporal point process models, event sequence classification, and missing event
interpolation. Experiments on popular public benchmarks demonstrate the
potential of the proposed method across different tasks and data domains.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10393" title="Abstract">arXiv:2402.10393</a> [<a href="/pdf/2402.10393" title="Download PDF">pdf</a>, <a href="/ps/2402.10393" title="Download PostScript">ps</a>, <a href="/format/2402.10393" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Darwin Turing Dawkins: Building a General Theory of Evolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adleman%2C+L+M">Leonard M. Adleman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 247 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Literature (cs.GL)</span>; Artificial Intelligence (cs.AI); Populations and Evolution (q-bio.PE)

</div>
<p class="mathjax">Living things, computers, societies, and even books are part of a grand
evolutionary struggle to survive. That struggle shapes nature, nations,
religions, art, science, and you. What you think, feel, and do is determined by
it. Darwinian evolution does not apply solely to the genes that are stored in
DNA. Using the insights of Alan Turing and Richard Dawkins, we will see that it
also applies to the memes we store in our brains and the information we store
in our computers. The next time you run for president, fight a war, or just
deal with the ordinary problems humans are heir to, perhaps this book will be
of use. If you want to understand why and when you will die, or if you want to
achieve greatness this book may help. If you are concerned about where the
computer revolution is headed, this book may provide some answers.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10395" title="Abstract">arXiv:2402.10395</a> [<a href="/pdf/2402.10395" title="Download PDF">pdf</a>, <a href="/format/2402.10395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing the Performance of OpenTitan as Cryptographic Accelerator in  Secure Open-Hardware System-on-Chips
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parisi%2C+E">Emanuele Parisi</a>, 
<a href="/search/cs?searchtype=author&query=Musa%2C+A">Alberto Musa</a>, 
<a href="/search/cs?searchtype=author&query=Ciani%2C+M">Maicol Ciani</a>, 
<a href="/search/cs?searchtype=author&query=Barchi%2C+F">Francesco Barchi</a>, 
<a href="/search/cs?searchtype=author&query=Rossi%2C+D">Davide Rossi</a>, 
<a href="/search/cs?searchtype=author&query=Bartolini%2C+A">Andrea Bartolini</a>, 
<a href="/search/cs?searchtype=author&query=Acquaviva%2C+A">Andrea Acquaviva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 2 figures, accepted at CF'24 conference, pre camera-ready version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Performance (cs.PF)

</div>
<p class="mathjax">RISC-V open-source systems are emerging in deployment scenarios where safety
and security are critical. OpenTitan is an open-source silicon root-of-trust
designed to be deployed in a wide range of systems, from high-end to deeply
embedded secure environments. Despite the availability of various cryptographic
hardware accelerators that make OpenTitan suitable for offloading cryptographic
workloads from the main processor, there has been no accurate and quantitative
establishment of the benefits derived from using OpenTitan as a secure
accelerator. This paper addresses this gap by thoroughly analysing strengths
and inefficiencies when offloading cryptographic workloads to OpenTitan. The
focus is on three key IPs - HMAC, AES, and OpenTitan Big Number accelerator
(OTBN) - which can accelerate four security workloads: Secure Hash Functions,
Message Authentication Codes, Symmetric cryptography, and Asymmetric
cryptography. For every workload, we develop a bare-metal driver for the
OpenTitan accelerator and analyze its efficiency when computation is offloaded
from a RISC-V application core within a System-on-Chip designed for secure
Cyber-Physical Systems applications. Finally, we assess it against a software
implementation on the application core. The characterization was conducted on a
cycle-accurate RTL simulator of the System-on-Chip (SoC). Our study
demonstrates that OpenTitan significantly outperforms software implementations,
with speedups ranging from 4.3x to 12.5x. However, there is potential for even
greater gains as the current OpenTitan utilizes a fraction of the accelerator
bandwidths, which ranges from 16% to 61%, depending on the memory being
accessed and the accelerator used. Our results open the way to the optimization
of OpenTitan-based secure platforms, providing design guidelines to unlock the
full potential of its accelerators in secure applications.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10397" title="Abstract">arXiv:2402.10397</a> [<a href="/pdf/2402.10397" title="Download PDF">pdf</a>, <a href="/format/2402.10397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LogELECTRA: Self-supervised Anomaly Detection for Unstructured Logs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yamanaka%2C+Y">Yuuki Yamanaka</a>, 
<a href="/search/cs?searchtype=author&query=Takahashi%2C+T">Tomokatsu Takahashi</a>, 
<a href="/search/cs?searchtype=author&query=Minami%2C+T">Takuya Minami</a>, 
<a href="/search/cs?searchtype=author&query=Nakajima%2C+Y">Yoshiaki Nakajima</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">System logs are some of the most important information for the maintenance of
software systems, which have become larger and more complex in recent years.
The goal of log-based anomaly detection is to automatically detect system
anomalies by analyzing the large number of logs generated in a short period of
time, which is a critical challenge in the real world. Previous studies have
used a log parser to extract templates from unstructured log data and detect
anomalies on the basis of patterns of the template occurrences. These methods
have limitations for logs with unknown templates. Furthermore, since most log
anomalies are known to be point anomalies rather than contextual anomalies,
detection methods based on occurrence patterns can cause unnecessary delays in
detection. In this paper, we propose LogELECTRA, a new log anomaly detection
model that analyzes a single line of log messages more deeply on the basis of
self-supervised anomaly detection. LogELECTRA specializes in detecting log
anomalies as point anomalies by applying ELECTRA, a natural language processing
model, to analyze the semantics of a single line of log messages. LogELECTRA
outperformed existing state-of-the-art methods in experiments on the public
benchmark log datasets BGL, Sprit, and Thunderbird.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10398" title="Abstract">arXiv:2402.10398</a> [<a href="/pdf/2402.10398" title="Download PDF">pdf</a>, <a href="/ps/2402.10398" title="Download PostScript">ps</a>, <a href="/format/2402.10398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt Learning for Multi-Label Code Smell Detection: A Promising  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Haiyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Saikrishna%2C+V">Vidya Saikrishna</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Q">Quanquan Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+K">Kun Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Code smells indicate the potential problems of software quality so that
developers can identify refactoring opportunities by detecting code smells.
State-of-the-art approaches leverage heuristics, machine learning, and deep
learning to detect code smells. However, existing approaches have not fully
explored the potential of large language models (LLMs). In this paper, we
propose \textit{PromptSmell}, a novel approach based on prompt learning for
detecting multi-label code smell. Firstly, code snippets are acquired by
traversing abstract syntax trees. Combined code snippets with natural language
prompts and mask tokens, \textit{PromptSmell} constructs the input of LLMs.
Secondly, to detect multi-label code smell, we leverage a label combination
approach by converting a multi-label problem into a multi-classification
problem. A customized answer space is added to the word list of pre-trained
language models, and the probability distribution of intermediate answers is
obtained by predicting the words at the mask positions. Finally, the
intermediate answers are mapped to the target class labels by a verbalizer as
the final classification result. We evaluate the effectiveness of
\textit{PromptSmell} by answering six research questions. The experimental
results demonstrate that \textit{PromptSmell} obtains an improvement of 11.17\%
in $precision_{w}$ and 7.4\% in $F1_{w}$ compared to existing approaches.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10399" title="Abstract">arXiv:2402.10399</a> [<a href="/pdf/2402.10399" title="Download PDF">pdf</a>, <a href="/format/2402.10399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Model Predictive Control for Cooperative Multirotor Landing  on Uncrewed Surface Vessel in Waves
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stephenson%2C+J">Jess Stephenson</a>, 
<a href="/search/cs?searchtype=author&query=Duncan%2C+N+T">Nathan T. Duncan</a>, 
<a href="/search/cs?searchtype=author&query=Greeff%2C+M">Melissa Greeff</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">Heterogeneous autonomous robot teams consisting of multirotor and uncrewed
surface vessels (USVs) have the potential to enable various maritime
applications, including advanced search-and-rescue operations. A critical
requirement of these applications is the ability to land a multirotor on a USV
for tasks such as recharging. This paper addresses the challenge of safely
landing a multirotor on a cooperative USV in harsh open waters. To tackle this
problem, we propose a novel sequential distributed model predictive control
(MPC) scheme for cooperative multirotor-USV landing. Our approach combines
standard tracking MPCs for the multirotor and USV with additional artificial
intermediate goal locations. These artificial goals enable the robots to
coordinate their cooperation without prior guidance. Each vehicle solves an
individual optimization problem for both the artificial goal and an input that
tracks it but only communicates the former to the other vehicle. The artificial
goals are penalized by a suitable coupling cost. Furthermore, our proposed
distributed MPC scheme utilizes a spatial-temporal wave model to coordinate in
real-time a safer landing location and time the multirotor's landing to limit
severe tilt of the USV.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10400" title="Abstract">arXiv:2402.10400</a> [<a href="/pdf/2402.10400" title="Download PDF">pdf</a>, <a href="/format/2402.10400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chain of Logic: Rule-Based Reasoning with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Servantez%2C+S">Sergio Servantez</a>, 
<a href="/search/cs?searchtype=author&query=Barrow%2C+J">Joe Barrow</a>, 
<a href="/search/cs?searchtype=author&query=Hammond%2C+K">Kristian Hammond</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+R">Rajiv Jain</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Rule-based reasoning, a fundamental type of legal reasoning, enables us to
draw conclusions by accurately applying a rule to a set of facts. We explore
causal language models as rule-based reasoners, specifically with respect to
compositional rules - rules consisting of multiple elements which form a
complex logical expression. Reasoning about compositional rules is challenging
because it requires multiple reasoning steps, and attending to the logical
relationships between elements. We introduce a new prompting method, Chain of
Logic, which elicits rule-based reasoning through decomposition (solving
elements as independent threads of logic), and recomposition (recombining these
sub-answers to resolve the underlying logical expression). This method was
inspired by the IRAC (Issue, Rule, Application, Conclusion) framework, a
sequential reasoning approach used by lawyers. We evaluate chain of logic
across eight rule-based reasoning tasks involving three distinct compositional
rules from the LegalBench benchmark and demonstrate it consistently outperforms
other prompting methods, including chain of thought and self-ask, using
open-source and commercial language models.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10401" title="Abstract">arXiv:2402.10401</a> [<a href="/pdf/2402.10401" title="Download PDF">pdf</a>, <a href="/format/2402.10401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ManiFPT: Defining and Analyzing Fingerprints of Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+H+J">Hae Jin Song</a>, 
<a href="/search/cs?searchtype=author&query=Khayatkhoei%2C+M">Mahyar Khayatkhoei</a>, 
<a href="/search/cs?searchtype=author&query=AbdAlmageed%2C+W">Wael AbdAlmageed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review at CVPR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Recent works have shown that generative models leave traces of their
underlying generative process on the generated samples, broadly referred to as
fingerprints of a generative model, and have studied their utility in detecting
synthetic images from real ones. However, the extend to which these
fingerprints can distinguish between various types of synthetic image and help
identify the underlying generative process remain under-explored. In
particular, the very definition of a fingerprint remains unclear, to our
knowledge. To that end, in this work, we formalize the definition of artifact
and fingerprint in generative models, propose an algorithm for computing them
in practice, and finally study its effectiveness in distinguishing a large
array of different generative models. We find that using our proposed
definition can significantly improve the performance on the task of identifying
the underlying generative process from samples (model attribution) compared to
existing methods. Additionally, we study the structure of the fingerprints, and
observe that it is very predictive of the effect of different design choices on
the generative process.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10403" title="Abstract">arXiv:2402.10403</a> [<a href="/pdf/2402.10403" title="Download PDF">pdf</a>, <a href="/format/2402.10403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polyhedral Complex Derivation from Piecewise Trilinear Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jin-Hwa Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR)

</div>
<p class="mathjax">Recent advancements in visualizing deep neural networks provide insights into
their structures and mesh extraction from Continuous Piecewise Affine (CPWA)
functions. Meanwhile, developments in neural surface representation learning
incorporate non-linear positional encoding, addressing issues like spectral
bias; however, this poses challenges in applying mesh extraction techniques
based on CPWA functions. Focusing on trilinear interpolating methods as
positional encoding, we present theoretical insights and an analytical mesh
extraction, showing the transformation of hypersurfaces to flat planes within
the trilinear region under the eikonal constraint. Moreover, we introduce a
method for approximating intersecting points among three hypersurfaces
contributing to broader applications. We empirically validate correctness and
parsimony through chamfer distance and efficiency, and angular distance, while
examining the correlation between the eikonal loss and the planarity of the
hypersurfaces.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10404" title="Abstract">arXiv:2402.10404</a> [<a href="/pdf/2402.10404" title="Download PDF">pdf</a>, <a href="/format/2402.10404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explaining generative diffusion models via visual analysis for  interpretable decision-making process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Ji-Hoon Park</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+Y">Yeong-Joon Ju</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Seong-Whan Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, published in Expert Systems with Applications
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Expert Systems with Applications 248 (2024) 123231
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Diffusion models have demonstrated remarkable performance in generation
tasks. Nevertheless, explaining the diffusion process remains challenging due
to it being a sequence of denoising noisy images that are difficult for experts
to interpret. To address this issue, we propose the three research questions to
interpret the diffusion process from the perspective of the visual concepts
generated by the model and the region where the model attends in each time
step. We devise tools for visualizing the diffusion process and answering the
aforementioned research questions to render the diffusion process
human-understandable. We show how the output is progressively generated in the
diffusion process by explaining the level of denoising and highlighting
relationships to foundational visual concepts at each time step through the
results of experiments with various visual analyses using the tools. Throughout
the training of the diffusion model, the model learns diverse visual concepts
corresponding to each time-step, enabling the model to predict varying levels
of visual concepts at different stages. We substantiate our tools using Area
Under Cover (AUC) score, correlation quantification, and cross-attention
mapping. Our findings provide insights into the diffusion process and pave the
way for further research into explainable diffusion mechanisms.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10409" title="Abstract">arXiv:2402.10409</a> [<a href="/pdf/2402.10409" title="Download PDF">pdf</a>, <a href="/format/2402.10409" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Survey Paper Taxonomy about Large Language Models via  Graph Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+J">Jun Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Kennington%2C+C">Casey Kennington</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> TL;DR: We collected metadata about LLM surveys and developed a method for categorizing them into a taxonomy, indicating the superiority of graph representation learning over language models and revealing the efficacy of fine-tuning using weak labels
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">As new research on Large Language Models (LLMs) continues, it is difficult to
keep up with new research and models. To help researchers synthesize the new
research many have written survey papers, but even those have become numerous.
In this paper, we develop a method to automatically assign survey papers to a
taxonomy. We collect the metadata of 144 LLM survey papers and explore three
paradigms to classify papers within the taxonomy. Our work indicates that
leveraging graph structure information on co-category graphs can significantly
outperform the language models in two paradigms; pre-trained language models'
fine-tuning and zero-shot/few-shot classifications using LLMs. We find that our
model surpasses an average human recognition level and that fine-tuning LLMs
using weak labels generated by a smaller model, such as the GCN in this study,
can be more effective than using ground-truth labels, revealing the potential
of weak-to-strong generalization in the taxonomy classification task.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10412" title="Abstract">arXiv:2402.10412</a> [<a href="/pdf/2402.10412" title="Download PDF">pdf</a>, <a href="/format/2402.10412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring and Reducing LLM Hallucination without Gold-Standard Answers  via Expertise-Weighting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+J">Jiaheng Wei</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuanshun Yao</a>, 
<a href="/search/cs?searchtype=author&query=Ton%2C+J">Jean-Francois Ton</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Hongyi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Estornell%2C+A">Andrew Estornell</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">LLM hallucination, i.e. generating factually incorrect yet seemingly
convincing answers, is currently a major threat to the trustworthiness and
reliability of LLMs. The first step towards solving this complicated problem is
to measure it. However, existing hallucination metrics require to have a
benchmark dataset with gold-standard answers, i.e. "best" or "correct" answers
written by humans. Such requirement makes hallucination measurement costly and
prone to human errors. In this work, we propose Factualness Evaluations via
Weighting LLMs (FEWL), the first hallucination metric that is specifically
designed for the scenario when gold-standard answers are absent. FEWL leverages
the answers from off-the-shelf LLMs that serve as a proxy of gold-standard
answers. The key challenge is how to quantify the expertise of reference LLMs
resourcefully. We show FEWL has certain theoretical guarantees and demonstrate
empirically it gives more accurate hallucination measures than naively using
reference LLMs. We also show how to leverage FEWL to reduce hallucination
through both in-context learning and supervised finetuning. Last, we build a
large-scale benchmark dataset to facilitate LLM hallucination research.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10416" title="Abstract">arXiv:2402.10416</a> [<a href="/pdf/2402.10416" title="Download PDF">pdf</a>, <a href="/format/2402.10416" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grounding Language about Belief in a Bayesian Theory-of-Mind
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ying%2C+L">Lance Ying</a>, 
<a href="/search/cs?searchtype=author&query=Zhi-Xuan%2C+T">Tan Zhi-Xuan</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+L">Lionel Wong</a>, 
<a href="/search/cs?searchtype=author&query=Mansinghka%2C+V">Vikash Mansinghka</a>, 
<a href="/search/cs?searchtype=author&query=Tenenbaum%2C+J">Joshua Tenenbaum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review, 7 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Despite the fact that beliefs are mental states that cannot be directly
observed, humans talk about each others' beliefs on a regular basis, often
using rich compositional language to describe what others think and know. What
explains this capacity to interpret the hidden epistemic content of other
minds? In this paper, we take a step towards an answer by grounding the
semantics of belief statements in a Bayesian theory-of-mind: By modeling how
humans jointly infer coherent sets of goals, beliefs, and plans that explain an
agent's actions, then evaluating statements about the agent's beliefs against
these inferences via epistemic logic, our framework provides a conceptual role
semantics for belief, explaining the gradedness and compositionality of human
belief attributions, as well as their intimate connection with goals and plans.
We evaluate this framework by studying how humans attribute goals and beliefs
while watching an agent solve a doors-and-keys gridworld puzzle that requires
instrumental reasoning about hidden objects. In contrast to pure logical
deduction, non-mentalizing baselines, and mentalizing that ignores the role of
instrumental plans, our model provides a much better fit to human goal and
belief attributions, demonstrating the importance of theory-of-mind for a
semantics of belief.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10422" title="Abstract">arXiv:2402.10422</a> [<a href="/pdf/2402.10422" title="Download PDF">pdf</a>, <a href="/format/2402.10422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pushing the Limits of Zero-shot End-to-End Speech Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsiamas%2C+I">Ioannis Tsiamas</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%A1llego%2C+G+I">Gerard I. G&#xe1;llego</a>, 
<a href="/search/cs?searchtype=author&query=Fonollosa%2C+J+A+R">Jos&#xe9; A. R. Fonollosa</a>, 
<a href="/search/cs?searchtype=author&query=Costa-juss%C3%A0%2C+M+R">Marta R. Costa-juss&#xe0;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Data scarcity and the modality gap between the speech and text modalities are
two major obstacles of end-to-end Speech Translation (ST) systems, thus
hindering their performance. Prior work has attempted to mitigate these
challenges by leveraging external MT data and optimizing distance metrics that
bring closer the speech-text representations. However, achieving competitive
results typically requires some ST data. For this reason, we introduce
ZeroSwot, a method for zero-shot ST that bridges the modality gap without any
paired ST data. Leveraging a novel CTC compression and Optimal Transport, we
train a speech encoder using only ASR data, to align with the representation
space of a massively multilingual MT model. The speech encoder seamlessly
integrates with the MT model at inference, enabling direct translation from
speech to text, across all languages supported by the MT model. Our experiments
show that we can effectively close the modality gap without ST data, while our
results on MuST-C and CoVoST demonstrate our method's superiority over not only
previous zero-shot models, but also supervised ones, achieving state-of-the-art
results.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10423" title="Abstract">arXiv:2402.10423</a> [<a href="/pdf/2402.10423" title="Download PDF">pdf</a>, <a href="/ps/2402.10423" title="Download PostScript">ps</a>, <a href="/format/2402.10423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Connect the dots: Dataset Condensation, Differential Privacy, and  Adversarial Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Odoh%2C+K">Kenneth Odoh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Our work focuses on understanding the underpinning mechanism of dataset
condensation by drawing connections with ($\epsilon$, $\delta$)-differential
privacy where the optimal noise, $\epsilon$, is chosen by adversarial
uncertainty \cite{Grining2017}. We can answer the question about the inner
workings of the dataset condensation procedure. Previous work \cite{dong2022}
proved the link between dataset condensation (DC) and ($\epsilon$,
$\delta$)-differential privacy. However, it is unclear from existing works on
ablating DC to obtain a lower-bound estimate of $\epsilon$ that will suffice
for creating high-fidelity synthetic data. We suggest that adversarial
uncertainty is the most appropriate method to achieve an optimal noise level,
$\epsilon$. As part of the internal dynamics of dataset condensation, we adopt
a satisfactory scheme for noise estimation that guarantees high-fidelity data
while providing privacy.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10424" title="Abstract">arXiv:2402.10424</a> [<a href="/pdf/2402.10424" title="Download PDF">pdf</a>, <a href="/format/2402.10424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding In-Context Learning with a Pelican Soup Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chiang%2C+T">Ting-Rui Chiang</a>, 
<a href="/search/cs?searchtype=author&query=Yogatama%2C+D">Dani Yogatama</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Many existing theoretical analyses of in-context learning for natural
language processing are based on latent variable models that leaves gaps
between theory and practice. We aim to close these gaps by proposing a
theoretical framework, the Pelican Soup Framework. In this framework, we
introduce (1) the notion of a common sense knowledge base, (2) a general
formalism for natural language classification tasks, and the notion of (3)
meaning association. Under this framework, we can establish a
$\mathcal{O}(1/T)$ loss bound for in-context learning, where $T$ is the number
of example-label pairs in the demonstration. Compared with previous works, our
bound reflects the effect of the choice of verbalizers and the effect of
instruction tuning. An additional notion of \textit{atom concepts} makes our
framework possible to explain the generalization to tasks unseen in the
language model training data. Finally, we propose a toy setup, Calcutec, and a
digit addition task that mimics types of distribution shifts a model needs to
overcome to perform in-context learning. We also experiment with GPT2-Large on
real-world NLP tasks. Our empirical results demonstrate the efficacy of our
framework to explain in-context learning.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10426" title="Abstract">arXiv:2402.10426</a> [<a href="/pdf/2402.10426" title="Download PDF">pdf</a>, <a href="/format/2402.10426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DELL: Generating Reactions and Explanations for LLM-Based Misinformation  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+H">Herun Wan</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shangbin Feng</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhaoxuan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Heng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tsvetkov%2C+Y">Yulia Tsvetkov</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+M">Minnan Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models are limited by challenges in factuality and
hallucinations to be directly employed off-the-shelf for judging the veracity
of news articles, where factual accuracy is paramount. In this work, we propose
DELL that identifies three key stages in misinformation detection where LLMs
could be incorporated as part of the pipeline: 1) LLMs could \emph{generate
news reactions} to represent diverse perspectives and simulate user-news
interaction networks; 2) LLMs could \emph{generate explanations} for proxy
tasks (e.g., sentiment, stance) to enrich the contexts of news articles and
produce experts specializing in various aspects of news understanding; 3) LLMs
could \emph{merge task-specific experts} and provide an overall prediction by
incorporating the predictions and confidence scores of varying experts.
Extensive experiments on seven datasets with three LLMs demonstrate that DELL
outperforms state-of-the-art baselines by up to 16.8\% in macro f1-score.
Further analysis reveals that the generated reactions and explanations are
greatly helpful in misinformation detection, while our proposed LLM-guided
expert merging helps produce better-calibrated predictions.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10427" title="Abstract">arXiv:2402.10427</a> [<a href="/pdf/2402.10427" title="Download PDF">pdf</a>, <a href="/format/2402.10427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating and Improving Continual Learning in Spoken Language  Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Muqiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Cappellazzo%2C+U">Umberto Cappellazzo</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+S">Shinji Watanabe</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+B">Bhiksha Raj</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Continual learning has emerged as an increasingly important challenge across
various tasks, including Spoken Language Understanding (SLU). In SLU, its
objective is to effectively handle the emergence of new concepts and evolving
environments. The evaluation of continual learning algorithms typically
involves assessing the model's stability, plasticity, and generalizability as
fundamental aspects of standards. However, existing continual learning metrics
primarily focus on only one or two of the properties. They neglect the overall
performance across all tasks, and do not adequately disentangle the plasticity
versus stability/generalizability trade-offs within the model. In this work, we
propose an evaluation methodology that provides a unified evaluation on
stability, plasticity, and generalizability in continual learning. By employing
the proposed metric, we demonstrate how introducing various knowledge
distillations can improve different aspects of these three properties of the
SLU model. We further show that our proposed metric is more sensitive in
capturing the impact of task ordering in continual learning, making it better
suited for practical use-case scenarios.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10430" title="Abstract">arXiv:2402.10430</a> [<a href="/pdf/2402.10430" title="Download PDF">pdf</a>, <a href="/format/2402.10430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smaller Language Models are capable of selecting Instruction-Tuning  Training Data for Larger Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mekala%2C+D">Dheeraj Mekala</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+A">Alex Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+J">Jingbo Shang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Instruction-tuning language models has become a crucial step in aligning them
for general use. Typically, this process involves extensive training on large
datasets, incurring high training costs. In this paper, we introduce a novel
training data selection based on the learning percentage of the samples. We
assert that current language models possess the capability to autonomously
select high-quality training data, leading to comparable or improved
performance compared to training on the entire dataset. Our experiments span
different-sized models, revealing that this characteristic holds for models
ranging from 1B (small) to 13B (large) in size. Moreover, we demonstrate an
interesting finding that the data hardness transfers across model sizes, and a
smaller 350M model can effectively curate high-quality training data with hard
samples for a larger 13B model, resulting in an equally or superior
instruction-tuned model compared to training on the complete dataset. Utilizing
open-sourced OPT and Llama-2 models up to 13B in size, two publicly available
instruction-tuning training datasets and evaluated by both automatic metrics &amp;
humans, our paper introduces a novel approach to training data selection,
showcasing a more efficient alternative.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10434" title="Abstract">arXiv:2402.10434</a> [<a href="/pdf/2402.10434" title="Download PDF">pdf</a>, <a href="/format/2402.10434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parametric Augmentation for Time Series Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xu Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianchun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+A">Aitian Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sha%2C+M">Mo Sha</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+D">Dongsheng Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by International Conference on Learning Representations (ICLR 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Modern techniques like contrastive learning have been effectively used in
many areas, including computer vision, natural language processing, and
graph-structured data. Creating positive examples that assist the model in
learning robust and discriminative representations is a crucial stage in
contrastive learning approaches. Usually, preset human intuition directs the
selection of relevant data augmentations. Due to patterns that are easily
recognized by humans, this rule of thumb works well in the vision and language
domains. However, it is impractical to visually inspect the temporal structures
in time series. The diversity of time series augmentations at both the dataset
and instance levels makes it difficult to choose meaningful augmentations on
the fly. In this study, we address this gap by analyzing time series data
augmentation using information theory and summarizing the most commonly adopted
augmentations in a unified format. We then propose a contrastive learning
framework with parametric augmentation, AutoTCL, which can be adaptively
employed to support time series representation learning. The proposed approach
is encoder-agnostic, allowing it to be seamlessly integrated with different
backbone encoders. Experiments on univariate forecasting tasks demonstrate the
highly competitive results of our method, with an average 6.5\% reduction in
MSE and 4.7\% in MAE over the leading baselines. In classification tasks,
AutoTCL achieves a $1.2\%$ increase in average accuracy.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10435" title="Abstract">arXiv:2402.10435</a> [<a href="/pdf/2402.10435" title="Download PDF">pdf</a>, <a href="/format/2402.10435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Patch-aware Enrichment Transformer for Occluded Person  Re-Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+K">Keren Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Q">Qijun Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Person re-identification (re-ID) continues to pose a significant challenge,
particularly in scenarios involving occlusions. Prior approaches aimed at
tackling occlusions have predominantly focused on aligning physical body
features through the utilization of external semantic cues. However, these
methods tend to be intricate and susceptible to noise. To address the
aforementioned challenges, we present an innovative end-to-end solution known
as the Dynamic Patch-aware Enrichment Transformer (DPEFormer). This model
effectively distinguishes human body information from occlusions automatically
and dynamically, eliminating the need for external detectors or precise image
alignment. Specifically, we introduce a dynamic patch token selection module
(DPSM). DPSM utilizes a label-guided proxy token as an intermediary to identify
informative occlusion-free tokens. These tokens are then selected for deriving
subsequent local part features. To facilitate the seamless integration of
global classification features with the finely detailed local features selected
by DPSM, we introduce a novel feature blending module (FBM). FBM enhances
feature representation through the complementary nature of information and the
exploitation of part diversity. Furthermore, to ensure that DPSM and the entire
DPEFormer can effectively learn with only identity labels, we also propose a
Realistic Occlusion Augmentation (ROA) strategy. This strategy leverages the
recent advances in the Segment Anything Model (SAM). As a result, it generates
occlusion images that closely resemble real-world occlusions, greatly enhancing
the subsequent contrastive learning process. Experiments on occluded and
holistic re-ID benchmarks signify a substantial advancement of DPEFormer over
existing state-of-the-art approaches. The code will be made publicly available.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10436" title="Abstract">arXiv:2402.10436</a> [<a href="/pdf/2402.10436" title="Download PDF">pdf</a>, <a href="/format/2402.10436" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> I Am Not Them: Fluid Identities and Persistent Out-group Bias in Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+W">Wenchao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhunis%2C+A">Assem Zhunis</a>, 
<a href="/search/cs?searchtype=author&query=Chin%2C+H">Hyojin Chin</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiyoung Han</a>, 
<a href="/search/cs?searchtype=author&query=Cha%2C+M">Meeyoung Cha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We explored cultural biases-individualism vs. collectivism-in ChatGPT across
three Western languages (i.e., English, German, and French) and three Eastern
languages (i.e., Chinese, Japanese, and Korean). When ChatGPT adopted an
individualistic persona in Western languages, its collectivism scores (i.e.,
out-group values) exhibited a more negative trend, surpassing their positive
orientation towards individualism (i.e., in-group values). Conversely, when a
collectivistic persona was assigned to ChatGPT in Eastern languages, a similar
pattern emerged with more negative responses toward individualism (i.e.,
out-group values) as compared to collectivism (i.e., in-group values). The
results indicate that when imbued with a particular social identity, ChatGPT
discerns in-group and out-group, embracing in-group values while eschewing
out-group values. Notably, the negativity towards the out-group, from which
prejudices and discrimination arise, exceeded the positivity towards the
in-group. The experiment was replicated in the political domain, and the
results remained consistent. Furthermore, this replication unveiled an
intrinsic Democratic bias in Large Language Models (LLMs), aligning with
earlier findings and providing integral insights into mitigating such bias
through prompt engineering. Extensive robustness checks were performed using
varying hyperparameter and persona setup methods, with or without social
identity labels, across other popular language models.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10439" title="Abstract">arXiv:2402.10439</a> [<a href="/pdf/2402.10439" title="Download PDF">pdf</a>, <a href="/format/2402.10439" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Competitive Equilibrium for Chores: from Dual Eisenberg-Gale to a Fast,  Greedy, LP-based Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chaudhury%2C+B+R">Bhaskar Ray Chaudhury</a>, 
<a href="/search/cs?searchtype=author&query=Kroer%2C+C">Christian Kroer</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+R">Ruta Mehta</a>, 
<a href="/search/cs?searchtype=author&query=Nan%2C+T">Tianlong Nan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">We study the computation of competitive equilibrium for Fisher markets with
$n$ agents and $m$ divisible chores. Prior work showed that competitive
equilibria correspond to the nonzero KKT points of a non-convex analogue of the
Eisenberg-Gale convex program. We introduce an analogue of the Eisenberg-Gale
dual for chores: we show that all KKT points of this dual correspond to
competitive equilibria, and while it is not a dual of the non-convex primal
program in a formal sense, the objectives touch at all KKT points. Similar to
the primal, the dual has problems from an optimization perspective: there are
many feasible directions where the objective tends to positive infinity. We
then derive a new constraint for the dual, which restricts optimization to a
hyperplane that avoids all these directions. We show that restriction to this
hyperplane retains all KKT points, and surprisingly, does not introduce any new
ones. This allows, for the first time ever, application of iterative
optimization methods over a convex region for computing competitive equilibria
for chores.
<br />We next introduce a greedy Frank-Wolfe algorithm for optimization over our
program and show a state-of-the-art convergence rate to competitive
equilibrium. In the case of equal incomes, we show a $\mathcal{\tilde
O}(n/\epsilon^2)$ rate of convergence, which improves over the two prior
state-of-the-art rates of $\mathcal{\tilde O}(n^3/\epsilon^2)$ for an
exterior-point method and $\mathcal{\tilde O}(nm/\epsilon^2)$ for a
combinatorial method. Moreover, our method is significantly simpler: each
iteration of our method only requires solving a simple linear program. We show
through numerical experiments on simulated data and a paper review bidding
dataset that our method is extremely practical. This is the first highly
practical method for solving competitive equilibrium for Fisher markets with
chores.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10441" title="Abstract">arXiv:2402.10441</a> [<a href="/pdf/2402.10441" title="Download PDF">pdf</a>, <a href="/format/2402.10441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Barrier-Enhanced Homotopic Parallel Trajectory Optimization for  Safety-Critical Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+L">Lei Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Rui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M+Y">Michael Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jun Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Enforcing safety while preventing overly conservative behaviors is essential
for autonomous vehicles to achieve high task performance. In this paper, we
propose a barrier-enhanced homotopic parallel trajectory optimization (BHPTO)
approach with over-relaxed alternating direction method of multipliers (ADMM)
for real-time integrated decision-making and planning. To facilitate safety
interactions between the ego vehicle (EV) and surrounding vehicles, a
spatiotemporal safety module exhibiting bi-convexity is developed on the basis
of barrier function. Varying barrier coefficients are adopted for different
time steps in a planning horizon to account for the motion uncertainties of
surrounding HVs and mitigate conservative behaviors. Additionally, we exploit
the discrete characteristics of driving maneuvers to initialize nominal
behavior-oriented free-end homotopic trajectories based on reachability
analysis, and each trajectory is locally constrained to a specific driving
maneuver while sharing the same task objectives. By leveraging the bi-convexity
of the safety module and the kinematics of the EV, we formulate the BHPTO as a
bi-convex optimization problem. Then constraint transcription and over-relaxed
ADMM are employed to streamline the optimization process, such that multiple
trajectories are generated in real time with feasibility guarantees. Through a
series of experiments, the proposed development demonstrates improved task
accuracy, stability, and consistency in various traffic scenarios using
synthetic and real-world traffic datasets.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10445" title="Abstract">arXiv:2402.10445</a> [<a href="/pdf/2402.10445" title="Download PDF">pdf</a>, <a href="/format/2402.10445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collaborative Learning with Different Labeling Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yuyang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+M">Mingda Qiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study a variant of Collaborative PAC Learning, in which we aim to learn an
accurate classifier for each of the $n$ data distributions, while minimizing
the number of samples drawn from them in total. Unlike in the usual
collaborative learning setup, it is not assumed that there exists a single
classifier that is simultaneously accurate for all distributions.
<br />We show that, when the data distributions satisfy a weaker realizability
assumption, sample-efficient learning is still feasible. We give a learning
algorithm based on Empirical Risk Minimization (ERM) on a natural augmentation
of the hypothesis class, and the analysis relies on an upper bound on the VC
dimension of this augmented class.
<br />In terms of the computational efficiency, we show that ERM on the augmented
hypothesis class is NP-hard, which gives evidence against the existence of
computationally efficient learners in general. On the positive side, for two
special cases, we give learners that are both sample- and
computationally-efficient.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10447" title="Abstract">arXiv:2402.10447</a> [<a href="/pdf/2402.10447" title="Download PDF">pdf</a>, <a href="/format/2402.10447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incremental Sequence Labeling: A Tale of Two Shifts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+S">Shengjie Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Junhao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yicheng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Q">Qianli Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The incremental sequence labeling task involves continuously learning new
classes over time while retaining knowledge of the previous ones. Our
investigation identifies two significant semantic shifts: E2O (where the model
mislabels an old entity as a non-entity) and O2E (where the model labels a
non-entity or old entity as a new entity). Previous research has predominantly
focused on addressing the E2O problem, neglecting the O2E issue. This
negligence results in a model bias towards classifying new data samples as
belonging to the new class during the learning process. To address these
challenges, we propose a novel framework, Incremental Sequential Labeling
without Semantic Shifts (IS3). Motivated by the identified semantic shifts (E2O
and O2E), IS3 aims to mitigate catastrophic forgetting in models. As for the
E2O problem, we use knowledge distillation to maintain the model's
discriminative ability for old entities. Simultaneously, to tackle the O2E
problem, we alleviate the model's bias towards new entities through debiased
loss and optimization levels. Our experimental evaluation, conducted on three
datasets with various incremental settings, demonstrates the superior
performance of IS3 compared to the previous state-of-the-art method by a
significant margin.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10450" title="Abstract">arXiv:2402.10450</a> [<a href="/pdf/2402.10450" title="Download PDF">pdf</a>, <a href="/format/2402.10450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PRISE: Learning Temporal Action Abstractions as a Sequence Compression  Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Ruijie Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+C">Ching-An Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Daum%C3%A9%2C+H">Hal Daum&#xe9; III</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Kolobov%2C+A">Andrey Kolobov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Temporal action abstractions, along with belief state representations, are a
powerful knowledge sharing mechanism for sequential decision making. In this
work, we propose a novel view that treats inducing temporal action abstractions
as a sequence compression problem. To do so, we bring a subtle but critical
component of LLM training pipelines -- input tokenization via byte pair
encoding (BPE) -- to the seemingly distant task of learning skills of variable
time span in continuous control domains. We introduce an approach called
Primitive Sequence Encoding (PRISE) that combines continuous action
quantization with BPE to learn powerful action abstractions. We empirically
show that high-level skills discovered by PRISE from a multitask set of robotic
manipulation demonstrations significantly boost the performance of both
multitask imitation learning as well as few-shot imitation learning on unseen
tasks. Our code will be released at https://github.com/FrankZheng2022/PRISE.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10451" title="Abstract">arXiv:2402.10451</a> [<a href="/pdf/2402.10451" title="Download PDF">pdf</a>, <a href="/ps/2402.10451" title="Download PostScript">ps</a>, <a href="/format/2402.10451" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Composition Orderings for Linear Functions and Matrix Multiplication  Orderings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kubo%2C+S">Susumu Kubo</a>, 
<a href="/search/cs?searchtype=author&query=Makino%2C+K">Kazuhisa Makino</a>, 
<a href="/search/cs?searchtype=author&query=Sakamoto%2C+S">Souta Sakamoto</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We consider composition orderings for linear functions of one variable. Given
$n$ linear functions $f_1,\dots,f_n$ and a constant $c$, the objective is to
find a permutation $\sigma$ that minimizes/maximizes
$f_{\sigma(n)}\circ\dots\circ f_{\sigma(1)}(c)$. It was first studied in the
area of time-dependent scheduling, and known to be solvable in $O(n\log n)$
time if all functions are nondecreasing. In this paper, we present a complete
characterization of optimal composition orderings for this case, by regarding
linear functions as two-dimensional vectors. We also show several interesting
properties on optimal composition orderings such as the equivalence between
local and global optimality. Furthermore, by using the characterization above,
we provide a fixed-parameter tractable (FPT) algorithm for the composition
ordering problem for general linear functions, with respect to the number of
decreasing linear functions. We next deal with matrix multiplication orderings
as a generalization of composition of linear functions. Given $n$ matrices
$M_1,\dots,M_n\in\mathbb{R}^{m\times m}$ and two vectors $w,y\in\mathbb{R}^m$,
where $m$ denotes a positive integer, the objective is to find a permutation
$\sigma$ that minimizes/maximizes $w^\top M_{\sigma(n)}\dots M_{\sigma(1)} y$.
The problem is also viewed as a generalization of flow shop scheduling through
a limit. By this extension, we show that the multiplication ordering problem
for $2\times 2$ matrices is solvable in $O(n\log n)$ time if all the matrices
are simultaneously triangularizable and have nonnegative determinants, and FPT
with respect to the number of matrices with negative determinants, if all the
matrices are simultaneously triangularizable. As the negative side, we finally
prove that three possible natural generalizations are NP-hard: 1) when $m=2$,
2) when $m\geq 3$, and 3) the target version of the problem.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10453" title="Abstract">arXiv:2402.10453</a> [<a href="/pdf/2402.10453" title="Download PDF">pdf</a>, <a href="/format/2402.10453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Steering Conversational Large Language Models for Long Emotional Support  Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Madani%2C+N">Navid Madani</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+S">Sougata Saha</a>, 
<a href="/search/cs?searchtype=author&query=Srihari%2C+R">Rohini Srihari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this study, we address the challenge of consistently following emotional
support strategies in long conversations by large language models (LLMs). We
introduce the Strategy-Relevant Attention (SRA) metric, a model-agnostic
measure designed to evaluate the effectiveness of LLMs in adhering to strategic
prompts in emotional support contexts. By analyzing conversations within the
Emotional Support Conversations dataset (ESConv) using LLaMA models, we
demonstrate that SRA is significantly correlated with a model's ability to
sustain the outlined strategy throughout the interactions. Our findings reveal
that the application of SRA-informed prompts leads to enhanced strategic
adherence, resulting in conversations that more reliably exhibit the desired
emotional support strategies over longer conversations. Furthermore, we
contribute a comprehensive, multi-branch synthetic conversation dataset for
ESConv, featuring a variety of strategy continuations informed by our optimized
prompting method. The code and data are publicly available on our Github.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10454" title="Abstract">arXiv:2402.10454</a> [<a href="/pdf/2402.10454" title="Download PDF">pdf</a>, <a href="/format/2402.10454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Skin Lesion Classification via Multimodal Data and Auxiliary  Task Integration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khurshid%2C+M">Mahapara Khurshid</a>, 
<a href="/search/cs?searchtype=author&query=Vatsa%2C+M">Mayank Vatsa</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+R">Richa Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The rising global prevalence of skin conditions, some of which can escalate
to life-threatening stages if not timely diagnosed and treated, presents a
significant healthcare challenge. This issue is particularly acute in remote
areas where limited access to healthcare often results in delayed treatment,
allowing skin diseases to advance to more critical stages. One of the primary
challenges in diagnosing skin diseases is their low inter-class variations, as
many exhibit similar visual characteristics, making accurate classification
challenging. This research introduces a novel multimodal method for classifying
skin lesions, integrating smartphone-captured images with essential clinical
and demographic information. This approach mimics the diagnostic process
employed by medical professionals. A distinctive aspect of this method is the
integration of an auxiliary task focused on super-resolution image prediction.
This component plays a crucial role in refining visual details and enhancing
feature extraction, leading to improved differentiation between classes and,
consequently, elevating the overall effectiveness of the model. The
experimental evaluations have been conducted using the PAD-UFES20 dataset,
applying various deep-learning architectures. The results of these experiments
not only demonstrate the effectiveness of the proposed method but also its
potential applicability under-resourced healthcare environments.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10457" title="Abstract">arXiv:2402.10457</a> [<a href="/pdf/2402.10457" title="Download PDF">pdf</a>, <a href="/format/2402.10457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning-Augmented Skip Lists
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+C">Chunkai Fu</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+J+H">Jung Hoon Seo</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Samson Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We study the integration of machine learning advice into the design of skip
lists to improve upon traditional data structure design. Given access to a
possibly erroneous oracle that outputs estimated fractional frequencies for
search queries on a set of items, we construct a skip list that provably
provides the optimal expected search time, within nearly a factor of two. In
fact, our learning-augmented skip list is still optimal up to a constant
factor, even if the oracle is only accurate within a constant factor. We show
that if the search queries follow the ubiquitous Zipfian distribution, then the
expected search time for an item by our skip list is only a constant,
independent of the total number $n$ of items, i.e., $\mathcal{O}(1)$, whereas a
traditional skip list will have an expected search time of $\mathcal{O}(\log
n)$. We also demonstrate robustness by showing that our data structure achieves
an expected search time that is within a constant factor of an oblivious skip
list construction even when the predictions are arbitrarily incorrect. Finally,
we empirically show that our learning-augmented skip list outperforms
traditional skip lists on both synthetic and real-world datasets.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10460" title="Abstract">arXiv:2402.10460</a> [<a href="/pdf/2402.10460" title="Download PDF">pdf</a>, <a href="/format/2402.10460" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A survey of LSM-Tree based Indexes, Data Systems and KV-stores
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Supriya Mishra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">Modern databases typically makes use of the Log Structured Merge-Tree for
organizing data in indexes, which is a kind of disk-based data structure. It
was proposed to efficiently handle frequent update queries (also called update
intensive workloads) databases. In recent years, LSM-Tree has gained popularity
and has been adopted by a number of NoSql databases, and key-value stores.
Since LSM-Tree was first proposed, researchers and the database community
started efforts to improve different components of LSM-Tree. In recent years,
Non-volatile Memory, also called Persistent Memory, has also gained significant
popularity. This is a class of memory that is non-volatile and byte-addressable
at the same time, and hence also termed Storage Class Memory. Apart from that,
storage class memory exhibits the combination of the best characteristics of
both memory and storage. An overview of the current state of the art in
LSM-Tree-based indexes, data systems, and Key-Value stores is provided in this
paper.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10462" title="Abstract">arXiv:2402.10462</a> [<a href="/pdf/2402.10462" title="Download PDF">pdf</a>, <a href="/format/2402.10462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large  Language Model Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rajabzadeh%2C+H">Hossein Rajabzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Valipour%2C+M">Mojtaba Valipour</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Tianshu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Tahaei%2C+M">Marzieh Tahaei</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+H+J">Hyock Ju Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Ghodsi%2C+A">Ali Ghodsi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Boxing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Rezagholizadeh%2C+M">Mehdi Rezagholizadeh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Best Paper Award AAAI EIW Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Finetuning large language models requires huge GPU memory, restricting the
choice to acquire Larger models. While the quantized version of the Low-Rank
Adaptation technique, named QLoRA, significantly alleviates this issue, finding
the efficient LoRA rank is still challenging. Moreover, QLoRA is trained on a
pre-defined rank and, therefore, cannot be reconfigured for its lower ranks
without requiring further fine-tuning steps. This paper proposes QDyLoRA
-Quantized Dynamic Low-Rank Adaptation-, as an efficient quantization approach
for dynamic low-rank adaptation. Motivated by Dynamic LoRA, QDyLoRA is able to
efficiently finetune LLMs on a set of pre-defined LoRA ranks. QDyLoRA enables
fine-tuning Falcon-40b for ranks 1 to 64 on a single 32 GB V100-GPU through one
round of fine-tuning. Experimental results show that QDyLoRA is competitive to
QLoRA and outperforms when employing its optimal rank.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10464" title="Abstract">arXiv:2402.10464</a> [<a href="/pdf/2402.10464" title="Download PDF">pdf</a>, <a href="/format/2402.10464" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedKit: Enabling Cross-Platform Federated Learning for Android and iOS
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+S">Sichang He</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+B">Beilong Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Boyan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+J">Jiaoqi Shao</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+X">Xiaomin Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Nugraha%2C+D+N">Daniel Nata Nugraha</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+B">Bing Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been accepted for demonstration on IEEE International Conference on Computer Communications (INFOCOM) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">We present FedKit, a federated learning (FL) system tailored for
cross-platform FL research on Android and iOS devices. FedKit pipelines
cross-platform FL development by enabling model conversion,
hardware-accelerated training, and cross-platform model aggregation. Our FL
workflow supports flexible machine learning operations (MLOps) in production,
facilitating continuous model delivery and training. We have deployed FedKit in
a real-world use case for health data analysis on university campuses,
demonstrating its effectiveness. FedKit is open-source at
https://github.com/FedCampus/FedKit.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10465" title="Abstract">arXiv:2402.10465</a> [<a href="/pdf/2402.10465" title="Download PDF">pdf</a>, <a href="/ps/2402.10465" title="Download PostScript">ps</a>, <a href="/format/2402.10465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Subfield codes of $C_D$-codes over $\mathbb{F}_2[x]/\langle x^3-x  \rangle$ are really nice!
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhagat%2C+A+K">Anuj Kumar Bhagat</a>, 
<a href="/search/cs?searchtype=author&query=Sarma%2C+R">Ritumoni Sarma</a>, 
<a href="/search/cs?searchtype=author&query=Sagar%2C+V">Vidya Sagar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">A non-zero $\mathbb{F}$-linear map from a finite-dimensional commutative
$\mathbb{F}$-algebra to $\mathbb{F}$ is called an $\mathbb{F}$-valued trace if
its kernel does not contain any non-zero ideals. In this article, we utilize an
$\mathbb{F}_2$-valued trace of the $\mathbb{F}_2$-algebra
$\mathcal{R}_2:=\mathbb{F}_2[x]/\langle x^3-x\rangle$ to study binary subfield
code $\mathcal{C}_D^{(2)}$ of $\mathcal{C}_D:=\{\left(x\cdot d\right)_{d\in D}:
x\in \mathcal{R}_2^m\}$ for each defining set $D$ derived from a certain
simplicial complex. For $m\in \mathbb{N}$ and $X\subseteq \{1, 2, \dots, m\}$,
define $\Delta_X:=\{v\in \mathbb{F}_2^m: \Supp(v)\subseteq X\}$ and
$D:=(1+u^2)D_1+u^2D_2+(u+u^2)D_3,$ a subset of $\mathcal{R}_2^m,$ where
$u=x+\langle x^3-x\rangle, D_1\in \{\Delta_L, \Delta_L^c\},\, D_2\in
\{\Delta_M, \Delta_M^c\}$ and $ D_3\in \{\Delta_N, \Delta_N^c\}$, for $L, M,
N\subseteq \{1, 2, \dots, m\}.$ The parameters and the Hamming weight
distribution of the binary subfield code $\mathcal{C}_D^{(2)}$ of
$\mathcal{C}_D$ are determined for each $D.$ These binary subfield codes are
minimal under certain mild conditions on the cardinalities of $L, M$ and $N$.
Moreover, most of these codes are distance-optimal. Consequently, we obtain a
few infinite families of minimal, self-orthogonal and distance-optimal binary
linear codes that are either $2$-weight or $4$-weight. It is worth mentioning
that we have obtained several new distance-optimal binary linear codes.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10466" title="Abstract">arXiv:2402.10466</a> [<a href="/pdf/2402.10466" title="Download PDF">pdf</a>, <a href="/format/2402.10466" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models as Zero-shot Dialogue State Tracker through  Function Calling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zekun Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z+Z">Zhiyu Zoey Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ross%2C+M">Mike Ross</a>, 
<a href="/search/cs?searchtype=author&query=Huber%2C+P">Patrick Huber</a>, 
<a href="/search/cs?searchtype=author&query=Moon%2C+S">Seungwhan Moon</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhaojiang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X+L">Xin Luna Dong</a>, 
<a href="/search/cs?searchtype=author&query=Sagar%2C+A">Adithya Sagar</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xifeng Yan</a>, 
<a href="/search/cs?searchtype=author&query=Crook%2C+P+A">Paul A. Crook</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) are increasingly prevalent in conversational
systems due to their advanced understanding and generative capabilities in
general contexts. However, their effectiveness in task-oriented dialogues
(TOD), which requires not only response generation but also effective dialogue
state tracking (DST) within specific tasks and domains, remains less
satisfying. In this work, we propose a novel approach FnCTOD for solving DST
with LLMs through function calling. This method improves zero-shot DST,
allowing adaptation to diverse domains without extensive data collection or
model tuning. Our experimental results demonstrate that our approach achieves
exceptional performance with both modestly sized open-source and also
proprietary LLMs: with in-context prompting it enables various 7B or 13B
parameter models to surpass the previous state-of-the-art (SOTA) achieved by
ChatGPT, and improves ChatGPT's performance beating the SOTA by 5.6% Avg. JGA.
Individual model results for GPT-3.5 and GPT-4 are boosted by 4.8% and 14%,
respectively. We also show that by fine-tuning on a small collection of diverse
task-oriented dialogues, we can equip modestly sized models, specifically a 13B
parameter LLaMA2-Chat model, with function-calling capabilities and DST
performance comparable to ChatGPT while maintaining their chat capabilities. We
plan to open-source experimental code and model.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10468" title="Abstract">arXiv:2402.10468</a> [<a href="/pdf/2402.10468" title="Download PDF">pdf</a>, <a href="/format/2402.10468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Curriculum Graph Contrastive Learning with Pair-wise  Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xinjian Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Liang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+R">Ruocheng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xiangyu Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Graph contrastive learning (GCL) has emerged as a pivotal technique in the
domain of graph representation learning. A crucial aspect of effective GCL is
the caliber of generated positive and negative samples, which is intrinsically
dictated by their resemblance to the original data. Nevertheless, precise
control over similarity during sample generation presents a formidable
challenge, often impeding the effective discovery of representative graph
patterns. To address this challenge, we propose an innovative framework:
Adversarial Curriculum Graph Contrastive Learning (ACGCL), which capitalizes on
the merits of pair-wise augmentation to engender graph-level positive and
negative samples with controllable similarity, alongside subgraph contrastive
learning to discern effective graph patterns therein. Within the ACGCL
framework, we have devised a novel adversarial curriculum training methodology
that facilitates progressive learning by sequentially increasing the difficulty
of distinguishing the generated samples. Notably, this approach transcends the
prevalent sparsity issue inherent in conventional curriculum learning
strategies by adaptively concentrating on more challenging training data.
Finally, a comprehensive assessment of ACGCL is conducted through extensive
experiments on six well-known benchmark datasets, wherein ACGCL conspicuously
surpasses a set of state-of-the-art baselines.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10469" title="Abstract">arXiv:2402.10469</a> [<a href="/pdf/2402.10469" title="Download PDF">pdf</a>, <a href="/format/2402.10469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pressure-stabilized fixed-stress iterative solutions of compositional  poromechanics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Aronson%2C+R+M">Ryan M. Aronson</a>, 
<a href="/search/math?searchtype=author&query=Castelletto%2C+N">Nicola Castelletto</a>, 
<a href="/search/math?searchtype=author&query=Hamon%2C+F+P">Fran&#xe7;ois P. Hamon</a>, 
<a href="/search/math?searchtype=author&query=White%2C+J+A">J. A. White</a>, 
<a href="/search/math?searchtype=author&query=Tchelepi%2C+H+A">Hamdi A. Tchelepi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We consider the numerical behavior of the fixed-stress splitting method for
coupled poromechanics as undrained regimes are approached. We explain that
pressure stability is related to the splitting error of the scheme, not the
fact that the discrete saddle point matrix never appears in the fixed-stress
approach. This observation reconciles previous results regarding the pressure
stability of the splitting method. Using examples of compositional
poromechanics with application to geological CO$_2$ sequestration, we see that
solutions obtained using the fixed-stress scheme with a low order finite
element-finite volume discretization which is not inherently inf-sup stable can
exhibit the same pressure oscillations obtained with the corresponding fully
implicit scheme. Moreover, pressure jump stabilization can effectively remove
these spurious oscillations in the fixed-stress setting, while also improving
the efficiency of the scheme in terms of the number of iterations required at
every time step to reach convergence.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10470" title="Abstract">arXiv:2402.10470</a> [<a href="/pdf/2402.10470" title="Download PDF">pdf</a>, <a href="/format/2402.10470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Theoretical Understanding of Learning from Adversarial Perturbations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kumano%2C+S">Soichiro Kumano</a>, 
<a href="/search/cs?searchtype=author&query=Kera%2C+H">Hiroshi Kera</a>, 
<a href="/search/cs?searchtype=author&query=Yamasaki%2C+T">Toshihiko Yamasaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">It is not fully understood why adversarial examples can deceive neural
networks and transfer between different networks. To elucidate this, several
studies have hypothesized that adversarial perturbations, while appearing as
noises, contain class features. This is supported by empirical evidence showing
that networks trained on mislabeled adversarial examples can still generalize
well to correctly labeled test samples. However, a theoretical understanding of
how perturbations include class features and contribute to generalization is
limited. In this study, we provide a theoretical framework for understanding
learning from perturbations using a one-hidden-layer network trained on
mutually orthogonal samples. Our results highlight that various adversarial
perturbations, even perturbations of a few pixels, contain sufficient class
features for generalization. Moreover, we reveal that the decision boundary
when learning from perturbations matches that from standard samples except for
specific regions under mild conditions. The code is available at
https://github.com/s-kumano/learning-from-adversarial-perturbations.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10473" title="Abstract">arXiv:2402.10473</a> [<a href="/pdf/2402.10473" title="Download PDF">pdf</a>, <a href="/format/2402.10473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy for Fairness: Information Obfuscation for Fair Representation  Learning with Local Differential Privacy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Songjie Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Youlong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+M">Ming Ding</a>, 
<a href="/search/cs?searchtype=author&query=Letaief%2C+K+B">Khaled B. Letaief</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Information Theory (cs.IT)

</div>
<p class="mathjax">As machine learning (ML) becomes more prevalent in human-centric
applications, there is a growing emphasis on algorithmic fairness and privacy
protection. While previous research has explored these areas as separate
objectives, there is a growing recognition of the complex relationship between
privacy and fairness. However, previous works have primarily focused on
examining the interplay between privacy and fairness through empirical
investigations, with limited attention given to theoretical exploration. This
study aims to bridge this gap by introducing a theoretical framework that
enables a comprehensive examination of their interrelation. We shall develop
and analyze an information bottleneck (IB) based information obfuscation method
with local differential privacy (LDP) for fair representation learning. In
contrast to many empirical studies on fairness in ML, we show that the
incorporation of LDP randomizers during the encoding process can enhance the
fairness of the learned representation. Our analysis will demonstrate that the
disclosure of sensitive information is constrained by the privacy budget of the
LDP randomizer, thereby enabling the optimization process within the IB
framework to effectively suppress sensitive information while preserving the
desired utility through obfuscation. Based on the proposed method, we further
develop a variational representation encoding approach that simultaneously
achieves fairness and LDP. Our variational encoding approach offers practical
advantages. It is trained using a non-adversarial method and does not require
the introduction of any variational prior. Extensive experiments will be
presented to validate our theoretical results and demonstrate the ability of
our proposed approach to achieve both LDP and fairness while preserving
adequate utility.
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10474" title="Abstract">arXiv:2402.10474</a> [<a href="/pdf/2402.10474" title="Download PDF">pdf</a>, <a href="/format/2402.10474" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One-Bit Quantization and Sparsification for Multiclass Linear  Classification via Regularized Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghane%2C+R">Reza Ghane</a>, 
<a href="/search/cs?searchtype=author&query=Akhtiamov%2C+D">Danil Akhtiamov</a>, 
<a href="/search/cs?searchtype=author&query=Hassibi%2C+B">Babak Hassibi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the use of linear regression for multiclass classification in the
over-parametrized regime where some of the training data is mislabeled. In such
scenarios it is necessary to add an explicit regularization term, $\lambda
f(w)$, for some convex function $f(\cdot)$, to avoid overfitting the mislabeled
data. In our analysis, we assume that the data is sampled from a Gaussian
Mixture Model with equal class sizes, and that a proportion $c$ of the training
labels is corrupted for each class. Under these assumptions, we prove that the
best classification performance is achieved when $f(\cdot) = \|\cdot\|^2_2$ and
$\lambda \to \infty$. We then proceed to analyze the classification errors for
$f(\cdot) = \|\cdot\|_1$ and $f(\cdot) = \|\cdot\|_\infty$ in the large
$\lambda$ regime and notice that it is often possible to find sparse and
one-bit solutions, respectively, that perform almost as well as the one
corresponding to $f(\cdot) = \|\cdot\|_2^2$.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10476" title="Abstract">arXiv:2402.10476</a> [<a href="/pdf/2402.10476" title="Download PDF">pdf</a>, <a href="/format/2402.10476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spike-EVPR: Deep Spiking Residual Network with Cross-Representation  Aggregation for Event-Based Visual Place Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+C">Chenming Hu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zheng Fang</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+K">Kuanxu Hou</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+D">Delei Kong</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Junjie Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+H">Hao Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mingyuan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xinjie Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Event cameras have been successfully applied to visual place recognition
(VPR) tasks by using deep artificial neural networks (ANNs) in recent years.
However, previously proposed deep ANN architectures are often unable to harness
the abundant temporal information presented in event streams. In contrast, deep
spiking networks exhibit more intricate spatiotemporal dynamics and are
inherently well-suited to process sparse asynchronous event streams.
Unfortunately, directly inputting temporal-dense event volumes into the spiking
network introduces excessive time steps, resulting in prohibitively high
training costs for large-scale VPR tasks. To address the aforementioned issues,
we propose a novel deep spiking network architecture called Spike-EVPR for
event-based VPR tasks. First, we introduce two novel event representations
tailored for SNN to fully exploit the spatio-temporal information from the
event streams, and reduce the video memory occupation during training as much
as possible. Then, to exploit the full potential of these two representations,
we construct a Bifurcated Spike Residual Encoder (BSR-Encoder) with powerful
representational capabilities to better extract the high-level features from
the two event representations. Next, we introduce a Shared &amp; Specific
Descriptor Extractor (SSD-Extractor). This module is designed to extract
features shared between the two representations and features specific to each.
Finally, we propose a Cross-Descriptor Aggregation Module (CDA-Module) that
fuses the above three features to generate a refined, robust global descriptor
of the scene. Our experimental results indicate the superior performance of our
Spike-EVPR compared to several existing EVPR pipelines on Brisbane-Event-VPR
and DDD20 datasets, with the average Recall@1 increased by 7.61% on Brisbane
and 13.20% on DDD20.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10477" title="Abstract">arXiv:2402.10477</a> [<a href="/pdf/2402.10477" title="Download PDF">pdf</a>, <a href="/format/2402.10477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Likelihood of Normalizing Flow and Image Complexity  through the Lens of Out-of-Distribution Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Osada%2C+G">Genki Osada</a>, 
<a href="/search/cs?searchtype=author&query=Takahashi%2C+T">Tsubasa Takahashi</a>, 
<a href="/search/cs?searchtype=author&query=Nishide%2C+T">Takashi Nishide</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAAI-24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Out-of-distribution (OOD) detection is crucial to safety-critical machine
learning applications and has been extensively studied. While recent studies
have predominantly focused on classifier-based methods, research on deep
generative model (DGM)-based methods have lagged relatively. This disparity may
be attributed to a perplexing phenomenon: DGMs often assign higher likelihoods
to unknown OOD inputs than to their known training data. This paper focuses on
explaining the underlying mechanism of this phenomenon. We propose a hypothesis
that less complex images concentrate in high-density regions in the latent
space, resulting in a higher likelihood assignment in the Normalizing Flow
(NF). We experimentally demonstrate its validity for five NF architectures,
concluding that their likelihood is untrustworthy. Additionally, we show that
this problem can be alleviated by treating image complexity as an independent
variable. Finally, we provide evidence of the potential applicability of our
hypothesis in another DGM, PixelCNN++.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10478" title="Abstract">arXiv:2402.10478</a> [<a href="/pdf/2402.10478" title="Download PDF">pdf</a>, <a href="/format/2402.10478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodaMal: Contrastive Domain Adaptation for Malaria Detection in Low-Cost  Microscopes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dave%2C+I+R">Ishan Rajendrakumar Dave</a>, 
<a href="/search/cs?searchtype=author&query=de+Blegiers%2C+T">Tristan de Blegiers</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+M">Mubarak Shah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review. Project Page: <a href="https://daveishan.github.io/codamal-webpage/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Malaria is a major health issue worldwide, and its diagnosis requires
scalable solutions that can work effectively with low-cost microscopes (LCM).
Deep learning-based methods have shown success in computer-aided diagnosis from
microscopic images. However, these methods need annotated images that show
cells affected by malaria parasites and their life stages. Annotating images
from LCM significantly increases the burden on medical experts compared to
annotating images from high-cost microscopes (HCM). For this reason, a
practical solution would be trained on HCM images which should generalize well
on LCM images during testing. While earlier methods adopted a multi-stage
learning process, they did not offer an end-to-end approach. In this work, we
present an end-to-end learning framework, named CodaMal (Contrastive Domain
Adpation for Malaria). In order to bridge the gap between HCM (training) and
LCM (testing), we propose a domain adaptive contrastive loss. It reduces the
domain shift by promoting similarity between the representations of HCM and its
corresponding LCM image, without imposing an additional annotation burden. In
addition, the training objective includes object detection objectives with
carefully designed augmentations, ensuring the accurate detection of malaria
parasites. On the publicly available large-scale M5-dataset, our proposed
method shows a significant improvement of 16% over the state-of-the-art methods
in terms of the mean average precision metric (mAP), provides 21x speed up
during inference, and requires only half learnable parameters than the prior
methods. Our code is publicly available.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10482" title="Abstract">arXiv:2402.10482</a> [<a href="/pdf/2402.10482" title="Download PDF">pdf</a>, <a href="/format/2402.10482" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Self-Distillation and Partial Label Learning in  Multi-Class Classification with Label Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jeong%2C+H">Hyeonsu Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+H+W">Hye Won Chung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Self-distillation (SD) is the process of training a student model using the
outputs of a teacher model, with both models sharing the same architecture. Our
study theoretically examines SD in multi-class classification with
cross-entropy loss, exploring both multi-round SD and SD with refined teacher
outputs, inspired by partial label learning (PLL). By deriving a closed-form
solution for the student model's outputs, we discover that SD essentially
functions as label averaging among instances with high feature correlations.
Initially beneficial, this averaging helps the model focus on feature clusters
correlated with a given instance for predicting the label. However, it leads to
diminishing performance with increasing distillation rounds. Additionally, we
demonstrate SD's effectiveness in label noise scenarios and identify the label
corruption condition and minimum number of distillation rounds needed to
achieve 100% classification accuracy. Our study also reveals that one-step
distillation with refined teacher outputs surpasses the efficacy of multi-step
SD using the teacher's direct output in high noise rate regimes.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10483" title="Abstract">arXiv:2402.10483</a> [<a href="/pdf/2402.10483" title="Download PDF">pdf</a>, <a href="/format/2402.10483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GaussianHair: Hair Modeling and Rendering with Light-aware Gaussians
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haimin Luo</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+M">Min Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zijun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Suyi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Longwen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qixuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jingyi Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Hairstyle reflects culture and ethnicity at first glance. In the digital era,
various realistic human hairstyles are also critical to high-fidelity digital
human assets for beauty and inclusivity. Yet, realistic hair modeling and
real-time rendering for animation is a formidable challenge due to its sheer
number of strands, complicated structures of geometry, and sophisticated
interaction with light. This paper presents GaussianHair, a novel explicit hair
representation. It enables comprehensive modeling of hair geometry and
appearance from images, fostering innovative illumination effects and dynamic
animation capabilities. At the heart of GaussianHair is the novel concept of
representing each hair strand as a sequence of connected cylindrical 3D
Gaussian primitives. This approach not only retains the hair's geometric
structure and appearance but also allows for efficient rasterization onto a 2D
image plane, facilitating differentiable volumetric rendering. We further
enhance this model with the "GaussianHair Scattering Model", adept at
recreating the slender structure of hair strands and accurately capturing their
local diffuse color in uniform lighting. Through extensive experiments, we
substantiate that GaussianHair achieves breakthroughs in both geometric and
appearance fidelity, transcending the limitations encountered in
state-of-the-art methods for hair reconstruction. Beyond representation,
GaussianHair extends to support editing, relighting, and dynamic rendering of
hair, offering seamless integration with conventional CG pipeline workflows.
Complementing these advancements, we have compiled an extensive dataset of real
human hair, each with meticulously detailed strand geometry, to propel further
research in this field.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10487" title="Abstract">arXiv:2402.10487</a> [<a href="/pdf/2402.10487" title="Download PDF">pdf</a>, <a href="/format/2402.10487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Random Projection Layers for Multidimensional Time Sires Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yeh%2C+C+M">Chin-Chia Michael Yeh</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yujie Fan</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xin Dai</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+V">Vivian Lai</a>, 
<a href="/search/cs?searchtype=author&query=Aboagye%2C+P+O">Prince Osei Aboagye</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junpeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huiyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Z">Zhongfang Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">All-Multi-Layer Perceptron (all-MLP) mixer models have been shown to be
effective for time series forecasting problems. However, when such a model is
applied to high-dimensional time series (e.g., the time series in a
spatial-temporal dataset), its performance is likely to degrade due to
overfitting issues. In this paper, we propose an all-MLP time series
forecasting architecture, referred to as RPMixer. Our method leverages the
ensemble-like behavior of deep neural networks, where each individual block
within the network acts like a base learner in an ensemble model, especially
when identity mapping residual connections are incorporated. By integrating
random projection layers into our model, we increase the diversity among the
blocks' outputs, thereby enhancing the overall performance of RPMixer.
Extensive experiments conducted on large-scale spatial-temporal forecasting
benchmark datasets demonstrate that our proposed method outperforms alternative
methods, including both spatial-temporal graph models and general forecasting
models.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10488" title="Abstract">arXiv:2402.10488</a> [<a href="/pdf/2402.10488" title="Download PDF">pdf</a>, <a href="/ps/2402.10488" title="Download PostScript">ps</a>, <a href="/format/2402.10488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reduced Order Model Enhanced Source Iteration with Synthetic  Acceleration for Parametric Radiative Transfer Equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Peng%2C+Z">Zhichao Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Applications such as uncertainty quantification and optical tomography,
require solving the radiative transfer equation (RTE) many times for various
parameters. Efficient solvers for RTE are highly desired.
<br />Source Iteration with Synthetic Acceleration (SISA) is one of the most
popular and successful iterative solvers for RTE. Synthetic Acceleration (SA)
acts as a preconditioning step to accelerate the convergence of Source
Iteration (SI). After each source iteration, classical SA strategies introduce
a correction to the macroscopic particle density by solving a low order
approximation to a kinetic correction equation. For example, Diffusion
Synthetic Acceleration (DSA) uses the diffusion limit. However, these
strategies may become less effective when the underlying low order
approximations are not accurate enough. Furthermore, they do not exploit low
rank structures concerning the parameters of parametric problems.
<br />To address these issues, we propose enhancing SISA with data-driven ROMs for
the parametric problem and the corresponding kinetic correction equation.
First, the ROM for the parametric problem can be utilized to obtain an improved
initial guess. Second, the ROM for the kinetic correction equation can be
utilized to design a low rank approximation to it. Unlike the diffusion limit,
this ROM-based approximation builds on the kinetic description of the
correction equation and leverages low rank structures concerning the
parameters. We further introduce a novel SA strategy called ROMSAD. ROMSAD
initially adopts our ROM-based approximation to exploit its greater efficiency
in the early stage, and then automatically switches to DSA to leverage its
robustness in the later stage. Additionally, we propose an approach to
construct the ROM for the kinetic correction equation without directly solving
it.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10491" title="Abstract">arXiv:2402.10491</a> [<a href="/pdf/2402.10491" title="Download PDF">pdf</a>, <a href="/format/2402.10491" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Make a Cheap Scaling: A Self-Cascade Diffusion Model for  Higher-Resolution Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Lanqing Guo</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yingqing He</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haoxin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+M">Menghan Xia</a>, 
<a href="/search/cs?searchtype=author&query=Cun%2C+X">Xiaodong Cun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yufei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Siyu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xintao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+Y">Ying Shan</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+B">Bihan Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://guolanqing.github.io/Self-Cascade/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Diffusion models have proven to be highly effective in image and video
generation; however, they still face composition challenges when generating
images of varying sizes due to single-scale training data. Adapting large
pre-trained diffusion models for higher resolution demands substantial
computational and optimization resources, yet achieving a generation capability
comparable to low-resolution models remains elusive. This paper proposes a
novel self-cascade diffusion model that leverages the rich knowledge gained
from a well-trained low-resolution model for rapid adaptation to
higher-resolution image and video generation, employing either tuning-free or
cheap upsampler tuning paradigms. Integrating a sequence of multi-scale
upsampler modules, the self-cascade diffusion model can efficiently adapt to a
higher resolution, preserving the original composition and generation
capabilities. We further propose a pivot-guided noise re-schedule strategy to
speed up the inference process and improve local structural details. Compared
to full fine-tuning, our approach achieves a 5X training speed-up and requires
only an additional 0.002M tuning parameters. Extensive experiments demonstrate
that our approach can quickly adapt to higher resolution image and video
synthesis by fine-tuning for just 10k steps, with virtually no additional
inference time.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10492" title="Abstract">arXiv:2402.10492</a> [<a href="/pdf/2402.10492" title="Download PDF">pdf</a>, <a href="/ps/2402.10492" title="Download PostScript">ps</a>, <a href="/format/2402.10492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Developing an Optimal Model for Predicting the Severity of Wheat Stem  Rust (Case study of Arsi and Bale Zone)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Altaye%2C+T">Tewodrose Altaye</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This research utilized three types of artificial neural network (ANN)
methodologies, namely Backpropagation Neural Network (BPNN) with varied
training, transfer, divide, and learning functions; Radial Basis Function
Neural Network (RBFNN); and General Regression Neural Network (GRNN), to
forecast the severity of stem rust. It considered parameters such as mean
maximum temperature, mean minimum temperature, mean rainfall, mean average
temperature, mean relative humidity, and different wheat varieties. The
statistical analysis revealed that GRNN demonstrated effective predictive
capability and required less training time compared to the other models.
Additionally, the results indicated that total seasonal rainfall positively
influenced the development of wheat stem rust.
<br />Keywords: Wheat stem rust, Back propagation neural network, Radial Basis
Function Neural Network, General Regression Neural Network.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10494" title="Abstract">arXiv:2402.10494</a> [<a href="/pdf/2402.10494" title="Download PDF">pdf</a>, <a href="/format/2402.10494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mechanised uniform interpolation for modal logics K, GL and iSL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=F%C3%A9r%C3%A9e%2C+H">Hugo F&#xe9;r&#xe9;e</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Giessen%2C+I">Iris van der Giessen</a>, 
<a href="/search/cs?searchtype=author&query=van+Gool%2C+S">Sam van Gool</a>, 
<a href="/search/cs?searchtype=author&query=Shillito%2C+I">Ian Shillito</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, submitted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Logic (math.LO)

</div>
<p class="mathjax">The uniform interpolation property in a given logic can be understood as the
definability of propositional quantifiers. We mechanise the computation of
these quantifiers and prove correctness in the Coq proof assistant for three
modal logics, namely: (1) the modal logic K, for which a pen-and-paper proof
exists; (2) G\"odel-L\"ob logic GL, for which our formalisation clarifies an
important point in an existing, but incomplete, sequent-style proof; and (3)
intuitionistic strong L\"ob logic iSL, for which this is the first
proof-theoretic construction of uniform interpolants. Our work also yields
verified programs that allow one to compute the propositional quantifiers on
any formula in this logic.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10496" title="Abstract">arXiv:2402.10496</a> [<a href="/pdf/2402.10496" title="Download PDF">pdf</a>, <a href="/format/2402.10496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comparing Hallucination Detection Metrics for Multilingual Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+H">Haoqiang Kang</a>, 
<a href="/search/cs?searchtype=author&query=Blevins%2C+T">Terra Blevins</a>, 
<a href="/search/cs?searchtype=author&query=Zettlemoyer%2C+L">Luke Zettlemoyer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While many automatic hallucination detection techniques have been proposed
for English texts, their effectiveness in multilingual contexts remains
unexplored. This paper aims to bridge the gap in understanding how these
hallucination detection metrics perform on non-English languages. We evaluate
the efficacy of various detection metrics, including lexical metrics like ROUGE
and Named Entity Overlap and Natural Language Inference (NLI)-based metrics, at
detecting hallucinations in biographical summaries in many languages; we also
evaluate how correlated these different metrics are to gauge whether they
measure the same phenomena. Our empirical analysis reveals that while lexical
metrics show limited effectiveness, NLI-based metrics perform well in
high-resource languages at the sentence level. In contrast, NLI-based metrics
often fail to detect atomic fact hallucinations. Our findings highlight
existing gaps in multilingual hallucination detection and motivate future
research to develop more robust detection methods for LLM hallucination in
other languages.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10500" title="Abstract">arXiv:2402.10500</a> [<a href="/pdf/2402.10500" title="Download PDF">pdf</a>, <a href="/format/2402.10500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provably Sample Efficient RLHF via Active Preference Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+N">Nirjhar Das</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Souradip Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Pacchiano%2C+A">Aldo Pacchiano</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+S+R">Sayak Ray Chowdhury</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Reinforcement Learning from Human Feedback (RLHF) is pivotal in aligning
Large Language Models (LLMs) with human preferences. While these aligned
generative models have demonstrated impressive capabilities across various
tasks, the dependence on high-quality human preference data poses a costly
bottleneck in practical implementation of RLHF. Hence better and adaptive
strategies for data collection is needed. To this end, we frame RLHF as a
contextual preference bandit problem with prompts as contexts and show that the
naive way of collecting preference data by choosing prompts uniformly at random
leads to a policy that suffers an $\Omega(1)$ suboptimality gap in rewards.
Then we propose $\textit{Active Preference Optimization}$ ($\texttt{APO}$), an
algorithm that actively selects prompts to collect preference data. Under the
Bradley-Terry-Luce (BTL) preference model, \texttt{APO} achieves sample
efficiency without compromising on policy performance. We show that given a
sample budget of $T$, the suboptimality gap of a policy learned via
$\texttt{APO}$ scales as $O(1/\sqrt{T})$. Next, we propose a compute-efficient
batch version of $\texttt{APO}$ with minor modification and evaluate its
performance in practice. Experimental evaluations on a human preference dataset
validate \texttt{APO}'s efficacy as a sample-efficient and practical solution
to data collection for RLHF, facilitating alignment of LLMs with human
preferences in a cost-effective and scalable manner.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10505" title="Abstract">arXiv:2402.10505</a> [<a href="/pdf/2402.10505" title="Download PDF">pdf</a>, <a href="/format/2402.10505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of Resilient Coordination for Cyber-Physical Systems Against  Malicious Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Liao%2C+Z">Zirui Liao</a>, 
<a href="/search/eess?searchtype=author&query=Shi%2C+J">Jian Shi</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yuwei Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+S">Shaoping Wang</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+Z">Zhiyong Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 7 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Cyber-physical systems (CPSs) facilitate the integration of physical entities
and cyber infrastructures through the utilization of pervasive computational
resources and communication units, leading to improved efficiency, automation,
and practical viability in both academia and industry. Due to its openness and
distributed characteristics, a critical issue prevalent in CPSs is to guarantee
resilience in presence of malicious attacks. This paper conducts a
comprehensive survey of recent advances on resilient coordination for CPSs.
Different from existing survey papers, we focus on the node injection attack
and propose a novel taxonomy according to the multi-layered framework of CPS.
Furthermore, miscellaneous resilient coordination problems are discussed in
this survey. Specifically, some preliminaries and the fundamental problem
settings are given at the beginning. Subsequently, based on a multi-layered
framework of CPSs, promising results of resilient consensus are classified and
reviewed from three perspectives: physical structure, communication mechanism,
and network topology. Next, two typical application scenarios, i.e.,
multi-robot systems and smart grids are exemplified to extend resilient
consensus to other coordination tasks. Particularly, we examine resilient
containment and resilient distributed optimization problems, both of which
demonstrate the applicability of resilient coordination approaches. Finally,
potential avenues are highlighted for future research.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10510" title="Abstract">arXiv:2402.10510</a> [<a href="/pdf/2402.10510" title="Download PDF">pdf</a>, <a href="/format/2402.10510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human Goal Recognition as Bayesian Inference: Investigating the Impact  of Actions, Timing, and Goal Solvability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kemp%2C+C">Charles Kemp</a>, 
<a href="/search/cs?searchtype=author&query=Lipovetzky%2C+N">Nir Lipovetzky</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAMAS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Goal recognition is a fundamental cognitive process that enables individuals
to infer intentions based on available cues. Current goal recognition
algorithms often take only observed actions as input, but here we use a
Bayesian framework to explore the role of actions, timing, and goal solvability
in goal recognition. We analyze human responses to goal-recognition problems in
the Sokoban domain, and find that actions are assigned most importance, but
that timing and solvability also influence goal recognition in some cases,
especially when actions are uninformative. We leverage these findings to
develop a goal recognition model that matches human inferences more closely
than do existing algorithms. Our work provides new insight into human goal
recognition and takes a step towards more human-like AI models.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10511" title="Abstract">arXiv:2402.10511</a> [<a href="/pdf/2402.10511" title="Download PDF">pdf</a>, <a href="/format/2402.10511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Transformers Predict Vibrations?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuniyoshi%2C+F">Fusataka Kuniyoshi</a>, 
<a href="/search/cs?searchtype=author&query=Sawada%2C+Y">Yoshihide Sawada</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Highly accurate time-series vibration prediction is an important research
issue for electric vehicles (EVs). EVs often experience vibrations when driving
on rough terrains, known as torsional resonance. This resonance, caused by the
interaction between motor and tire vibrations, puts excessive loads on the
vehicle's drive shaft. However, current damping technologies only detect
resonance after the vibration amplitude of the drive shaft torque reaches a
certain threshold, leading to significant loads on the shaft at the time of
detection. In this study, we propose a novel approach to address this issue by
introducing Resoformer, a transformer-based model for predicting torsional
resonance. Resoformer utilizes time-series of the motor rotation speed as input
and predicts the amplitude of torsional vibration at a specified quantile
occurring in the shaft after the input series. By calculating the attention
between recursive and convolutional features extracted from the measured data
points, Resoformer improves the accuracy of vibration forecasting. To evaluate
the model, we use a vibration dataset called VIBES (Dataset for Forecasting
Vibration Transition in EVs), consisting of 2,600 simulator-generated vibration
sequences. Our experiments, conducted on strong baselines built on the VIBES
dataset, demonstrate that Resoformer achieves state-of-the-art results. In
conclusion, our study answers the question "Can Transformers Forecast
Vibrations?" While traditional transformer architectures show low performance
in forecasting torsional resonance waves, our findings indicate that combining
recurrent neural network and temporal convolutional network using the
transformer architecture improves the accuracy of long-term vibration
forecasting.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10512" title="Abstract">arXiv:2402.10512</a> [<a href="/pdf/2402.10512" title="Download PDF">pdf</a>, <a href="/format/2402.10512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Computing Paradigm for MobileNetV3 using Memristor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiale Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Longyu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Sham%2C+C">Chiu-Wing Sham</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+C">Chong Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">The advancement in the field of machine learning is inextricably linked with
the concurrent progress in domain-specific hardware accelerators such as GPUs
and TPUs. However, the rapidly growing computational demands necessitated by
larger models and increased data have become a primary bottleneck in further
advancing machine learning, especially in mobile and edge devices. Currently,
the neuromorphic computing paradigm based on memristors presents a promising
solution. In this study, we introduce a memristor-based MobileNetV3 neural
network computing paradigm and provide an end-to-end framework for validation.
The results demonstrate that this computing paradigm achieves over 90\%
accuracy on the CIFAR-10 dataset while saving inference time and reducing
energy consumption. With the successful development and verification of
MobileNetV3, the potential for realizing more memristor-based neural networks
using this computing paradigm and open-source framework has significantly
increased. This progress sets a groundbreaking pathway for future deployment
initiatives.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10513" title="Abstract">arXiv:2402.10513</a> [<a href="/pdf/2402.10513" title="Download PDF">pdf</a>, <a href="/format/2402.10513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Delays in AF\_XDP-based Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Perron%2C+K+C+d">Killian Castillon du Perron</a>, 
<a href="/search/cs?searchtype=author&query=Pacheco%2C+D+L">Dino Lopez Pacheco</a>, 
<a href="/search/cs?searchtype=author&query=Huet%2C+F">Fabrice Huet</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2024 IEEE International Conference on Communications (ICC): Next-Generation Networking and Internet Symposium (IEEE ICC'24 - NGNI Symposium), IEEE, Jun 2024, Denver, United States
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Packet processing on Linux can be slow due to its complex network stack. To
solve this problem, there are two main solutions: eXpress Data Path (XDP) and
Data Plane Development Kit (DPDK). XDP and the AF XDP socket offer full
interoperability with the legacy system and is being adopted by major internet
players like Open vSwitch or Facebook. While the performance evaluation of AF
XDP against the legacy protocol stack in the kernel or against DPDK has been
studied in the literature, the impact of the multiple socket parameters and the
system configuration on its latency has been left aside. To address this, we
conduct an experimental study to understand the XDP/AF XDP ecosystem and detect
microseconds delays to better architect future latency-sensitive applications.
Since the performance of AF XDP depends on multiple parameters found in
different layers, finding the configuration minimizing its latency is a
challenging task. We rely on a classification algorithm to group the
performance results, allowing us to easily identify parameters with the biggest
impact on performance at different loads. Last, but not least, we show that
some configurations can significantly decrease the benefits of AF XDP, leading
to undesirable behaviors, while other configurations are able to reduce such
round trip delays to an impressive value of 6.5 $\mu$s in the best case,
including the tracing overhead. In summary, AF XDP is a promising solution, and
careful selection of both application and socket parameters can significantly
improve performance.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10517" title="Abstract">arXiv:2402.10517</a> [<a href="/pdf/2402.10517" title="Download PDF">pdf</a>, <a href="/format/2402.10517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+Y">Yeonhong Park</a>, 
<a href="/search/cs?searchtype=author&query=Hyun%2C+J">Jake Hyun</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+S">SangLyul Cho</a>, 
<a href="/search/cs?searchtype=author&query=Sim%2C+B">Bonggeun Sim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+W">Jae W. Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recently, considerable efforts have been directed towards compressing Large
Language Models (LLMs), which showcase groundbreaking capabilities across
diverse applications but entail significant deployment costs due to their large
sizes. Meanwhile, much less attention has been given to mitigating the costs
associated with deploying multiple LLMs of varying sizes despite its practical
significance. Thus, this paper introduces \emph{any-precision LLM}, extending
the concept of any-precision DNN to LLMs. Addressing challenges in
any-precision LLM, we propose a lightweight method for any-precision
quantization of LLMs, leveraging a post-training quantization framework, and
develop a specialized software engine for its efficient serving. As a result,
our solution significantly reduces the high costs of deploying multiple,
different-sized LLMs by overlaying LLMs quantized to varying bit-widths, such
as 3, 4, ..., $n$ bits, into a memory footprint comparable to a single $n$-bit
LLM. All the supported LLMs with varying bit-widths demonstrate
state-of-the-art model quality and inference throughput, proving itself to be a
compelling option for deployment of multiple, different-sized LLMs. The source
code will be publicly available soon.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10520" title="Abstract">arXiv:2402.10520</a> [<a href="/pdf/2402.10520" title="Download PDF">pdf</a>, <a href="/format/2402.10520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-Time Model-Based Quantitative Ultrasound and Radar
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharon%2C+T">Tom Sharon</a>, 
<a href="/search/cs?searchtype=author&query=Eldar%2C+Y+C">Yonina C. Eldar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Ultrasound and radar signals are highly beneficial for medical imaging as
they are non-invasive and non-ionizing. Traditional imaging techniques have
limitations in terms of contrast and physical interpretation. Quantitative
medical imaging can display various physical properties such as speed of sound,
density, conductivity, and relative permittivity. This makes it useful for a
wider range of applications, including improving cancer detection, diagnosing
fatty liver, and fast stroke imaging. However, current quantitative imaging
techniques that estimate physical properties from received signals, such as
Full Waveform Inversion, are time-consuming and tend to converge to local
minima, making them unsuitable for medical imaging. To address these
challenges, we propose a neural network based on the physical model of wave
propagation, which defines the relationship between the received signals and
physical properties. Our network can reconstruct multiple physical properties
in less than one second for complex and realistic scenarios, using data from
only eight elements. We demonstrate the effectiveness of our approach for both
radar and ultrasound signals.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10524" title="Abstract">arXiv:2402.10524</a> [<a href="/pdf/2402.10524" title="Download PDF">pdf</a>, <a href="/format/2402.10524" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kahng%2C+M">Minsuk Kahng</a>, 
<a href="/search/cs?searchtype=author&query=Tenney%2C+I">Ian Tenney</a>, 
<a href="/search/cs?searchtype=author&query=Pushkarna%2C+M">Mahima Pushkarna</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M+X">Michael Xieyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wexler%2C+J">James Wexler</a>, 
<a href="/search/cs?searchtype=author&query=Reif%2C+E">Emily Reif</a>, 
<a href="/search/cs?searchtype=author&query=Kallarackal%2C+K">Krystal Kallarackal</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+M">Minsuk Chang</a>, 
<a href="/search/cs?searchtype=author&query=Terry%2C+M">Michael Terry</a>, 
<a href="/search/cs?searchtype=author&query=Dixon%2C+L">Lucas Dixon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Automatic side-by-side evaluation has emerged as a promising approach to
evaluating the quality of responses from large language models (LLMs). However,
analyzing the results from this evaluation approach raises scalability and
interpretability challenges. In this paper, we present LLM Comparator, a novel
visual analytics tool for interactively analyzing results from automatic
side-by-side evaluation. The tool supports interactive workflows for users to
understand when and why a model performs better or worse than a baseline model,
and how the responses from two models are qualitatively different. We
iteratively designed and developed the tool by closely working with researchers
and engineers at a large technology company. This paper details the user
challenges we identified, the design and development of the tool, and an
observational study with participants who regularly evaluate their models.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10525" title="Abstract">arXiv:2402.10525</a> [<a href="/pdf/2402.10525" title="Download PDF">pdf</a>, <a href="/format/2402.10525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How People Prompt to Create Interactive VR Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manesh%2C+S+A">Setareh Aghel Manesh</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Onishi%2C+Y">Yuki Onishi</a>, 
<a href="/search/cs?searchtype=author&query=Hara%2C+K">Kotaro Hara</a>, 
<a href="/search/cs?searchtype=author&query=Bateman%2C+S">Scott Bateman</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiannan Li</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+A">Anthony Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Generative AI tools can provide people with the ability to create virtual
environments and scenes with natural language prompts. Yet, how people will
formulate such prompts is unclear -- particularly when they inhabit the
environment that they are designing. For instance, it is likely that a person
might say, "Put a chair here", while pointing at a location. If such linguistic
features are common to people's prompts, we need to tune models to accommodate
them. In this work, we present a wizard-of-oz elicitation study with 22
participants, where we studied people's implicit expectations when verbally
prompting such programming agents to create interactive VR scenes. Our findings
show that people prompt with several implicit expectations: (1) that agents
have an embodied knowledge of the environment; (2) that agents understand
embodied prompts by users; (3) that the agents can recall previous states of
the scene and the conversation, and that (4) agents have a commonsense
understanding of objects in the scene. Further, we found that participants
prompt differently when they are prompting in situ (i.e. within the VR
environment) versus ex situ (i.e. viewing the VR environment from the outside).
To explore how our could be applied, we designed and built Oastaad, a
conversational programming agent that allows non-programmers to design
interactive VR experiences that they inhabit. Based on these explorations, we
outline new opportunities and challenges for conversational programming agents
that create VR environments.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10527" title="Abstract">arXiv:2402.10527</a> [<a href="/pdf/2402.10527" title="Download PDF">pdf</a>, <a href="/format/2402.10527" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-shot sampling of adversarial entities in biomedical question  answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xian%2C+R+P">R. Patrick Xian</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+A+J">Alex J. Lee</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+V">Vincent Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Q">Qiming Cui</a>, 
<a href="/search/cs?searchtype=author&query=Ro%2C+R">Russell Ro</a>, 
<a href="/search/cs?searchtype=author&query=Abbasi-Asl%2C+R">Reza Abbasi-Asl</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages incl. appendix, under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR); Applications (stat.AP)

</div>
<p class="mathjax">The increasing depth of parametric domain knowledge in large language models
(LLMs) is fueling their rapid deployment in real-world applications. In
high-stakes and knowledge-intensive tasks, understanding model vulnerabilities
is essential for quantifying the trustworthiness of model predictions and
regulating their use. The recent discovery of named entities as adversarial
examples in natural language processing tasks raises questions about their
potential guises in other settings. Here, we propose a powerscaled
distance-weighted sampling scheme in embedding space to discover diverse
adversarial entities as distractors. We demonstrate its advantage over random
sampling in adversarial question answering on biomedical topics. Our approach
enables the exploration of different regions on the attack surface, which
reveals two regimes of adversarial entities that markedly differ in their
characteristics. Moreover, we show that the attacks successfully manipulate
token-wise Shapley value explanations, which become deceptive in the
adversarial setting. Our investigations illustrate the brittleness of domain
knowledge in LLMs and reveal a shortcoming of standard evaluations for
high-capacity models.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10528" title="Abstract">arXiv:2402.10528</a> [<a href="/pdf/2402.10528" title="Download PDF">pdf</a>, <a href="/format/2402.10528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can We Verify Step by Step for Incorrect Answer Detection?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Diao%2C+S">Shizhe Diao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Can Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Chain-of-Thought (CoT) prompting has marked a significant advancement in
enhancing the reasoning capabilities of large language models (LLMs). Previous
studies have developed various extensions of CoT, which focus primarily on
enhancing end-task performance. In addition, there has been research on
assessing the quality of reasoning chains in CoT. This raises an intriguing
question: Is it possible to predict the accuracy of LLM outputs by scrutinizing
the reasoning chains they generate? To answer this research question, we
introduce a benchmark, R2PE, designed specifically to explore the relationship
between reasoning chains and performance in various reasoning tasks spanning
five different domains. This benchmark aims to measure the falsehood of the
final output of LLMs based on the reasoning steps. To make full use of
information in multiple reasoning chains, we propose the process discernibility
score (PDS) framework that beats the answer-checking baseline by a large
margin. Concretely, this resulted in an average of 5.1% increase in the F1
score across all 45 subsets within R2PE. We further demonstrate our PDS's
efficacy in advancing open-domain QA accuracy. Data and code are available at
https://github.com/XinXU-USTC/R2PE.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10529" title="Abstract">arXiv:2402.10529</a> [<a href="/pdf/2402.10529" title="Download PDF">pdf</a>, <a href="/format/2402.10529" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-aware Multi-UAV Coverage Mission Planning with Optimal Speed of  Flight
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Datsko%2C+D">Denys Datsko</a>, 
<a href="/search/cs?searchtype=author&query=Nekovar%2C+F">Frantisek Nekovar</a>, 
<a href="/search/cs?searchtype=author&query=Penicka%2C+R">Robert Penicka</a>, 
<a href="/search/cs?searchtype=author&query=Saska%2C+M">Martin Saska</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> in IEEE Robotics and Automation Letters
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Robotics and Automation Letters, vol. 9, no. 3, pp.
  2893-2900, March 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper tackles the problem of planning minimum-energy coverage paths for
multiple UAVs. The addressed Multi-UAV Coverage Path Planning (mCPP) is a
crucial problem for many UAV applications such as inspection and aerial survey.
However, the typical path-length objective of existing approaches does not
directly minimize the energy consumption, nor allows for constraining energy of
individual paths by the battery capacity. To this end, we propose a novel mCPP
method that uses the optimal flight speed for minimizing energy consumption per
traveled distance and a simple yet precise energy consumption estimation
algorithm that is utilized during the mCPP planning phase. The method
decomposes a given area with boustrophedon decomposition and represents the
mCPP as an instance of Multiple Set Traveling Salesman Problem with a minimum
energy objective and energy consumption constraint. The proposed method is
shown to outperform state-of-the-art methods in terms of computational time and
energy efficiency of produced paths. The experimental results show that the
accuracy of the energy consumption estimation is on average 97% compared to
real flight consumption. The feasibility of the proposed method was verified in
a real-world coverage experiment with two UAVs.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10532" title="Abstract">arXiv:2402.10532</a> [<a href="/pdf/2402.10532" title="Download PDF">pdf</a>, <a href="/format/2402.10532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Properties and Challenges of LLM-Generated Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kunz%2C+J">Jenny Kunz</a>, 
<a href="/search/cs?searchtype=author&query=Kuhlmann%2C+M">Marco Kuhlmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">The self-rationalising capabilities of large language models (LLMs) have been
explored in restricted settings, using task/specific data sets. However,
current LLMs do not (only) rely on specifically annotated data; nonetheless,
they frequently explain their outputs. The properties of the generated
explanations are influenced by the pre-training corpus and by the target data
used for instruction fine-tuning. As the pre-training corpus includes a large
amount of human-written explanations "in the wild", we hypothesise that LLMs
adopt common properties of human explanations. By analysing the outputs for a
multi-domain instruction fine-tuning data set, we find that generated
explanations show selectivity and contain illustrative elements, but less
frequently are subjective or misleading. We discuss reasons and consequences of
the properties' presence or absence. In particular, we outline positive and
negative implications depending on the goals and user groups of the
self-rationalising system.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10533" title="Abstract">arXiv:2402.10533</a> [<a href="/pdf/2402.10533" title="Download PDF">pdf</a>, <a href="/format/2402.10533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> APCodec: A Neural Audio Codec with Parallel Amplitude and Phase Spectrum  Encoding and Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ai%2C+Y">Yang Ai</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xiao-Hang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Ye-Xin Lu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+H">Hui-Peng Du</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+Z">Zhen-Hua Ling</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE/ACM Transactions on Audio, Speech, and Language Processing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This paper introduces a novel neural audio codec targeting high waveform
sampling rates and low bitrates named APCodec, which seamlessly integrates the
strengths of parametric codecs and waveform codecs. The APCodec revolutionizes
the process of audio encoding and decoding by concurrently handling the
amplitude and phase spectra as audio parametric characteristics like parametric
codecs. It is composed of an encoder and a decoder with the modified ConvNeXt
v2 network as the backbone, connected by a quantizer based on the residual
vector quantization (RVQ) mechanism. The encoder compresses the audio amplitude
and phase spectra in parallel, amalgamating them into a continuous latent code
at a reduced temporal resolution. This code is subsequently quantized by the
quantizer. Ultimately, the decoder reconstructs the audio amplitude and phase
spectra in parallel, and the decoded waveform is obtained by inverse short-time
Fourier transform. To ensure the fidelity of decoded audio like waveform
codecs, spectral-level loss, quantization loss, and generative adversarial
network (GAN) based loss are collectively employed for training the APCodec. To
support low-latency streamable inference, we employ feed-forward layers and
causal convolutional layers in APCodec, incorporating a knowledge distillation
training strategy to enhance the quality of decoded audio. Experimental results
confirm that our proposed APCodec can encode 48 kHz audio at bitrate of just 6
kbps, with no significant degradation in the quality of the decoded audio. At
the same bitrate, our proposed APCodec also demonstrates superior decoded audio
quality and faster generation speed compared to well-known codecs, such as
SoundStream, Encodec, HiFi-Codec and AudioDec.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10534" title="Abstract">arXiv:2402.10534</a> [<a href="/pdf/2402.10534" title="Download PDF">pdf</a>, <a href="/format/2402.10534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Left and Right Brains Together: Towards Vision and Language  Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cen%2C+J">Jun Cen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chenfei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+S">Shengming Yin</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+Y">Yixuan Pei</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jinglong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Qifeng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+N">Nan Duan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianguo Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) and Large Multi-modality Models (LMMs) have
demonstrated remarkable decision masking capabilities on a variety of tasks.
However, they inherently operate planning within the language space, lacking
the vision and spatial imagination ability. In contrast, humans utilize both
left and right hemispheres of the brain for language and visual planning during
the thinking process. Therefore, we introduce a novel vision-language planning
framework in this work to perform concurrent visual and language planning for
tasks with inputs of any form. Our framework incorporates visual planning to
capture intricate environmental details, while language planning enhances the
logical coherence of the overall system. We evaluate the effectiveness of our
framework across vision-language tasks, vision-only tasks, and language-only
tasks. The results demonstrate the superior performance of our approach,
indicating that the integration of visual and language planning yields better
contextually aware task execution.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10535" title="Abstract">arXiv:2402.10535</a> [<a href="/pdf/2402.10535" title="Download PDF">pdf</a>, <a href="/format/2402.10535" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying and combining uncertainty for improving the behavior of  Digital Twin Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Deantoni%2C+J">Julien Deantoni</a>, 
<a href="/search/eess?searchtype=author&query=Mu%C3%B1oz%2C+P">Paula Mu&#xf1;oz</a>, 
<a href="/search/eess?searchtype=author&query=Gomes%2C+C">Cl&#xe1;udio Gomes</a>, 
<a href="/search/eess?searchtype=author&query=Verbrugge%2C+C">Clark Verbrugge</a>, 
<a href="/search/eess?searchtype=author&query=Mittal%2C+R">Rakshit Mittal</a>, 
<a href="/search/eess?searchtype=author&query=Heinrich%2C+R">Robert Heinrich</a>, 
<a href="/search/eess?searchtype=author&query=Bellis%2C+S">Stijn Bellis</a>, 
<a href="/search/eess?searchtype=author&query=Vallecillo%2C+A">Antonio Vallecillo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Uncertainty is an inherent property of any complex system, especially those
that integrate physical parts or operate in real environments. In this paper,
we focus on the Digital Twins of adaptive systems, which are particularly
complex to design, verify, and optimize. One of the problems of having two
systems (the physical one and its digital replica) is that their behavior may
not always be consistent. In addition, both twins are normally subject to
different types of uncertainties, which complicates their comparison. In this
paper we propose the explicit representation and treatment of the uncertainty
of both twins, and show how this enables a more accurate comparison of their
behaviors. Furthermore, this allows us to reduce the overall system uncertainty
and improve its behavior by properly averaging the individual uncertainties of
the two twins. An exemplary incubator system is used to illustrate and validate
our proposal.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10538" title="Abstract">arXiv:2402.10538</a> [<a href="/pdf/2402.10538" title="Download PDF">pdf</a>, <a href="/ps/2402.10538" title="Download PostScript">ps</a>, <a href="/format/2402.10538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimal Constraint Violation Probability in Model Predictive Control for  Linear Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fink%2C+M">Michael Fink</a>, 
<a href="/search/eess?searchtype=author&query=Br%C3%BCdigam%2C+T">Tim Br&#xfc;digam</a>, 
<a href="/search/eess?searchtype=author&query=Wollherr%2C+D">Dirk Wollherr</a>, 
<a href="/search/eess?searchtype=author&query=Leibold%2C+M">Marion Leibold</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Handling uncertainty in model predictive control comes with various
challenges, especially when considering state constraints under uncertainty.
Most methods focus on either the conservative approach of robustly accounting
for uncertainty or allowing a small probability of constraint violation. In
this work, we propose a linear model predictive control approach that minimizes
the probability that linear state constraints are violated in the presence of
additive uncertainty. This is achieved by first determining a set of inputs
that minimize the probability of constraint violation. Then, this resulting set
is used to define admissible inputs for the optimal control problem. Recursive
feasibility is guaranteed and input-to-state stability is proved under
assumptions. Numerical results illustrate the benefits of the proposed model
predictive control approach.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10543" title="Abstract">arXiv:2402.10543</a> [<a href="/pdf/2402.10543" title="Download PDF">pdf</a>, <a href="/format/2402.10543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strong hallucinations from negation and how to fix them
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asher%2C+N">Nicholas Asher</a>, 
<a href="/search/cs?searchtype=author&query=Bhar%2C+S">Swarnadeep Bhar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Despite great performance on many tasks, language models (LMs) still struggle
with reasoning, sometimes providing responses that cannot possibly be true
because they stem from logical incoherence. We call such responses
\textit{strong hallucinations} and prove that they follow from an LM's
computation of its internal representations for logical operators and outputs
from those representations. Focusing on negation, we provide a novel solution
in which negation is treated not as another element of a latent representation,
but as \textit{an operation over an LM's latent representations that constrains
how they may evolve}. We show that our approach improves model performance in
cloze prompting and natural language inference tasks with negation without
requiring training on sparse negative data.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10547" title="Abstract">arXiv:2402.10547</a> [<a href="/pdf/2402.10547" title="Download PDF">pdf</a>, <a href="/format/2402.10547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Disentangled Audio Representations through Controlled Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brima%2C+Y">Yusuf Brima</a>, 
<a href="/search/cs?searchtype=author&query=Krumnack%2C+U">Ulf Krumnack</a>, 
<a href="/search/cs?searchtype=author&query=Pika%2C+S">Simone Pika</a>, 
<a href="/search/cs?searchtype=author&query=Heidemann%2C+G">Gunther Heidemann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 12 figures, accepted as a Tiny paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">This paper tackles the scarcity of benchmarking data in disentangled auditory
representation learning. We introduce SynTone, a synthetic dataset with
explicit ground truth explanatory factors for evaluating disentanglement
techniques. Benchmarking state-of-the-art methods on SynTone highlights its
utility for method evaluation. Our results underscore strengths and limitations
in audio disentanglement, motivating future research.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10548" title="Abstract">arXiv:2402.10548</a> [<a href="/pdf/2402.10548" title="Download PDF">pdf</a>, <a href="/format/2402.10548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cognitive Personalized Search Integrating Large Language Models with an  Efficient Memory Mechanism
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yujia Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qiannan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+J">Jiajie Jin</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Z">Zhicheng Dou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Traditional search engines usually provide identical search results for all
users, overlooking individual preferences. To counter this limitation,
personalized search has been developed to re-rank results based on user
preferences derived from query logs. Deep learning-based personalized search
methods have shown promise, but they rely heavily on abundant training data,
making them susceptible to data sparsity challenges. This paper proposes a
Cognitive Personalized Search (CoPS) model, which integrates Large Language
Models (LLMs) with a cognitive memory mechanism inspired by human cognition.
CoPS employs LLMs to enhance user modeling and user search experience. The
cognitive memory mechanism comprises sensory memory for quick sensory
responses, working memory for sophisticated cognitive responses, and long-term
memory for storing historical interactions. CoPS handles new queries using a
three-step approach: identifying re-finding behaviors, constructing user
profiles with relevant historical information, and ranking documents based on
personalized query intent. Experiments show that CoPS outperforms baseline
models in zero-shot scenarios.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10549" title="Abstract">arXiv:2402.10549</a> [<a href="/pdf/2402.10549" title="Download PDF">pdf</a>, <a href="/format/2402.10549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-order reliable numerical methods for epidemic models with  non-constant recruitment rate
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Tak%C3%A1cs%2C+B+M">B. M. Tak&#xe1;cs</a>, 
<a href="/search/math?searchtype=author&query=Sebesty%C3%A9n%2C+G+S">G. Svantnern&#xe9; Sebesty&#xe9;n</a>, 
<a href="/search/math?searchtype=author&query=Farag%C3%B3%2C+I">I. Farag&#xf3;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The mathematical modeling of the propagation of illnesses has an important
role from both mathematical and biological points of view. In this article, we
observe an SEIR-type model with a general incidence rate and a non-constant
recruitment rate function. First, we observe the qualitative properties of
different methods: first-order and higher-order strong stability preserving
Runge-Kutta methods \cite{shu}. We give different conditions under which the
numerical schemes behave as expected. Then, the theoretical results are
demonstrated by some numerical experiments. \keywords{positivity preservation,
general SEIR model, SSP Runge-Kutta methods}
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10551" title="Abstract">arXiv:2402.10551</a> [<a href="/pdf/2402.10551" title="Download PDF">pdf</a>, <a href="/format/2402.10551" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Personalised Drug Identifier for Cancer Treatment with Transformers  using Auxiliary Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jayagopal%2C+A">Aishwarya Jayagopal</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+H">Hansheng Xue</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Ziyang He</a>, 
<a href="/search/cs?searchtype=author&query=Walsh%2C+R+J">Robert J. Walsh</a>, 
<a href="/search/cs?searchtype=author&query=Hariprasannan%2C+K+K">Krishna Kumar Hariprasannan</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+D+S+P">David Shao Peng Tan</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+T+Z">Tuan Zea Tan</a>, 
<a href="/search/cs?searchtype=author&query=Pitt%2C+J+J">Jason J. Pitt</a>, 
<a href="/search/cs?searchtype=author&query=Jeyasekharan%2C+A+D">Anand D. Jeyasekharan</a>, 
<a href="/search/cs?searchtype=author&query=Rajan%2C+V">Vaibhav Rajan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Cancer remains a global challenge due to its growing clinical and economic
burden. Its uniquely personal manifestation, which makes treatment difficult,
has fuelled the quest for personalized treatment strategies. Thus, genomic
profiling is increasingly becoming part of clinical diagnostic panels.
Effective use of such panels requires accurate drug response prediction (DRP)
models, which are challenging to build due to limited labelled patient data.
Previous methods to address this problem have used various forms of transfer
learning. However, they do not explicitly model the variable length sequential
structure of the list of mutations in such diagnostic panels. Further, they do
not utilize auxiliary information (like patient survival) for model training.
We address these limitations through a novel transformer based method, which
surpasses the performance of state-of-the-art DRP models on benchmark data. We
also present the design of a treatment recommendation system (TRS), which is
currently deployed at the National University Hospital, Singapore and is being
evaluated in a clinical trial.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10552" title="Abstract">arXiv:2402.10552</a> [<a href="/pdf/2402.10552" title="Download PDF">pdf</a>, <a href="/format/2402.10552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conversational SimulMT: Efficient Simultaneous Translation with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Minghan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+T">Thuy-Trang Vu</a>, 
<a href="/search/cs?searchtype=author&query=Shareghi%2C+E">Ehsan Shareghi</a>, 
<a href="/search/cs?searchtype=author&query=Haffari%2C+G">Gholamreza Haffari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Simultaneous machine translation (SimulMT) presents a challenging trade-off
between translation quality and latency. Recent studies have shown that LLMs
can achieve good performance in SimulMT tasks. However, this often comes at the
expense of high inference cost and latency. In this paper, we propose a
conversational SimulMT framework to enhance the inference efficiency of
LLM-based SimulMT through multi-turn-dialogue-based decoding. Our experiments
with Llama2-7b-chat on two SimulMT benchmarks demonstrate the superiority of
LLM in translation quality while achieving comparable computational latency to
specialized SimulMT models.
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10553" title="Abstract">arXiv:2402.10553</a> [<a href="/pdf/2402.10553" title="Download PDF">pdf</a>, <a href="/format/2402.10553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A novel integrated industrial approach with cobots in the age of  industry 4.0 through conversational interaction and computer vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pazienza%2C+A">Andrea Pazienza</a>, 
<a href="/search/cs?searchtype=author&query=Macchiarulo%2C+N">Nicola Macchiarulo</a>, 
<a href="/search/cs?searchtype=author&query=Vitulano%2C+F">Felice Vitulano</a>, 
<a href="/search/cs?searchtype=author&query=Fiorentini%2C+A">Antonio Fiorentini</a>, 
<a href="/search/cs?searchtype=author&query=Cammisa%2C+M">Marco Cammisa</a>, 
<a href="/search/cs?searchtype=author&query=Rigutini%2C+L">Leonardo Rigutini</a>, 
<a href="/search/cs?searchtype=author&query=Di+Iorio%2C+E">Ernesto Di Iorio</a>, 
<a href="/search/cs?searchtype=author&query=Globo%2C+A">Achille Globo</a>, 
<a href="/search/cs?searchtype=author&query=Trevisi%2C+A">Antonio Trevisi</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 6th Italian Conference on Computational
  Linguistics (CLiC-it 2019)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">From robots that replace workers to robots that serve as helpful colleagues,
the field of robotic automation is experiencing a new trend that represents a
huge challenge for component manufacturers. The contribution starts from an
innovative vision that sees an ever closer collaboration between Cobot, able to
do a specific physical job with precision, the AI world, able to analyze
information and support the decision-making process, and the man able to have a
strategic vision of the future.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10554" title="Abstract">arXiv:2402.10554</a> [<a href="/pdf/2402.10554" title="Download PDF">pdf</a>, <a href="/format/2402.10554" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disordered-DABS: A Benchmark for Dynamic Aspect-Based Summarization in  Disordered Texts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xiaobo Guo</a>, 
<a href="/search/cs?searchtype=author&query=Vosoughi%2C+S">Soroush Vosoughi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Aspect-based summarization has seen significant advancements, especially in
structured text. Yet, summarizing disordered, large-scale texts, like those
found in social media and customer feedback, remains a significant challenge.
Current research largely targets predefined aspects within structured texts,
neglecting the complexities of dynamic and disordered environments. Addressing
this gap, we introduce Disordered-DABS, a novel benchmark for dynamic
aspect-based summarization tailored to unstructured text. Developed by adapting
existing datasets for cost-efficiency and scalability, our comprehensive
experiments and detailed human evaluations reveal that Disordered-DABS poses
unique challenges to contemporary summarization models, including
state-of-the-art language models such as GPT-3.5.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10555" title="Abstract">arXiv:2402.10555</a> [<a href="/pdf/2402.10555" title="Download PDF">pdf</a>, <a href="/format/2402.10555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPAR: Personalized Content-Based Recommendation via Long Engagement  Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chiyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yifei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+J">Jie Lei</a>, 
<a href="/search/cs?searchtype=author&query=Abdul-Mageed%2C+M">Muhammad Abdul-Mageed</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sinong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+R">Rong Jin</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Sem Park</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+N">Ning Yao</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+B">Bo Long</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Leveraging users' long engagement histories is essential for personalized
content recommendations. The success of pretrained language models (PLMs) in
NLP has led to their use in encoding user histories and candidate items,
framing content recommendations as textual semantic matching tasks. However,
existing works still struggle with processing very long user historical text
and insufficient user-item interaction. In this paper, we introduce a
content-based recommendation framework, SPAR, which effectively tackles the
challenges of holistic user interest extraction from the long user engagement
history. It achieves so by leveraging PLM, poly-attention layers and attention
sparsity mechanisms to encode user's history in a session-based manner. The
user and item side features are sufficiently fused for engagement prediction
while maintaining standalone representations for both sides, which is efficient
for practical model deployment. Moreover, we enhance user profiling by
exploiting large language model (LLM) to extract global interests from user
engagement history. Extensive experiments on two benchmark datasets demonstrate
that our framework outperforms existing state-of-the-art (SoTA) methods.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10558" title="Abstract">arXiv:2402.10558</a> [<a href="/pdf/2402.10558" title="Download PDF">pdf</a>, <a href="/ps/2402.10558" title="Download PostScript">ps</a>, <a href="/format/2402.10558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural paraphrasing by automatically crawled and aligned sentence pairs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Globo%2C+A">Achille Globo</a>, 
<a href="/search/cs?searchtype=author&query=Trevisi%2C+A">Antonio Trevisi</a>, 
<a href="/search/cs?searchtype=author&query=Zugarini%2C+A">Andrea Zugarini</a>, 
<a href="/search/cs?searchtype=author&query=Rigutini%2C+L">Leonardo Rigutini</a>, 
<a href="/search/cs?searchtype=author&query=Maggini%2C+M">Marco Maggini</a>, 
<a href="/search/cs?searchtype=author&query=Melacci%2C+S">Stefano Melacci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 6th International Conference on Social Networks Analysis, Management and Security (SNAMS 2019)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of The 6th International Conference on Social Networks
  Analysis, Management and Security (SNAMS 2019)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Paraphrasing is the task of re-writing an input text using other words,
without altering the meaning of the original content. Conversational systems
can exploit automatic paraphrasing to make the conversation more natural, e.g.,
talking about a certain topic using different paraphrases in different time
instants. Recently, the task of automatically generating paraphrases has been
approached in the context of Natural Language Generation (NLG). While many
existing systems simply consist in rule-based models, the recent success of the
Deep Neural Networks in several NLG tasks naturally suggests the possibility of
exploiting such networks for generating paraphrases. However, the main obstacle
toward neural-network-based paraphrasing is the lack of large datasets with
aligned pairs of sentences and paraphrases, that are needed to efficiently
train the neural models. In this paper we present a method for the automatic
generation of large aligned corpora, that is based on the assumption that news
and blog websites talk about the same events using different narrative styles.
We propose a similarity search procedure with linguistic constraints that,
given a reference sentence, is able to locate the most similar candidate
paraphrases out from millions of indexed sentences. The data generation process
is evaluated in the case of the Italian language, performing experiments using
pointer-based deep neural architectures.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10562" title="Abstract">arXiv:2402.10562</a> [<a href="/pdf/2402.10562" title="Download PDF">pdf</a>, <a href="/ps/2402.10562" title="Download PostScript">ps</a>, <a href="/format/2402.10562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Precise Hybrid-Actuation Robotic Fiber for Enhanced Cervical Disease  Treatment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jinshi Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Q">Qindong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Demircali%2C+A+A">Ali Anil Demircali</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xiaotong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Simon%2C+D">Daniel Simon</a>, 
<a href="/search/cs?searchtype=author&query=Paraskevaidi%2C+M">Maria Paraskevaidi</a>, 
<a href="/search/cs?searchtype=author&query=Linton%2C+N+W+F">Nick W F Linton</a>, 
<a href="/search/cs?searchtype=author&query=Takats%2C+Z">Zoltan Takats</a>, 
<a href="/search/cs?searchtype=author&query=Kyrgiou%2C+M">Maria Kyrgiou</a>, 
<a href="/search/cs?searchtype=author&query=Temelkuran%2C+B">Burak Temelkuran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Medical Physics (physics.med-ph)

</div>
<p class="mathjax">Treatment for high-grade precancerous cervical lesions and early-stage
cancers, mainly affecting women of reproductive age, often involves
fertility-sparing treatment methods. Commonly used local treatments for
cervical precancers have shown the risk of leaving a positive cancer margin and
engendering subsequent complications according to the precision and depth of
excision. An intra-operative device that allows the careful excision of the
disease while conserving healthy cervical tissue would potentially enhance such
treatment. In this study, we developed a polymer-based robotic fiber measuring
150 mm in length and 1.7 mm in diameter, fabricated using a highly scalable
fiber drawing technique. This robotic fiber utilizes a hybrid actuation
mechanism, combining electrothermal and tendon-driven actuation mechanisms,
thus enabling a maximum motion range of 46 mm from its origin with a sub-100
{\mu}m motion precision. We also developed control algorithms for the actuation
methods of this robotic fiber, including predefined path control and
telemanipulation, enabling coarse positioning of the fiber tip to the target
area followed by a precise scan. The combination of a surgical laser fiber with
the robotic fiber allows for high-precision surgical ablation. Additionally, we
conducted experiments using a cervical phantom that demonstrated the robotic
fiber's ability to access and perform high-precision scans, highlighting its
potential for cervical disease treatments and improvement of oncological
outcomes.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10567" title="Abstract">arXiv:2402.10567</a> [<a href="/pdf/2402.10567" title="Download PDF">pdf</a>, <a href="/format/2402.10567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs  ready for the Indian Legal Domain?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tripathi%2C+Y">Yogesh Tripathi</a>, 
<a href="/search/cs?searchtype=author&query=Donakanti%2C+R">Raghav Donakanti</a>, 
<a href="/search/cs?searchtype=author&query=Girhepuje%2C+S">Sahil Girhepuje</a>, 
<a href="/search/cs?searchtype=author&query=Kavathekar%2C+I">Ishan Kavathekar</a>, 
<a href="/search/cs?searchtype=author&query=Vedula%2C+B+H">Bhaskara Hanuma Vedula</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+G+S">Gokul S Krishnan</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+S">Shreya Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Goel%2C+A">Anmol Goel</a>, 
<a href="/search/cs?searchtype=author&query=Ravindran%2C+B">Balaraman Ravindran</a>, 
<a href="/search/cs?searchtype=author&query=Kumaraguru%2C+P">Ponnurangam Kumaraguru</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advancements in language technology and Artificial Intelligence have
resulted in numerous Language Models being proposed to perform various tasks in
the legal domain ranging from predicting judgments to generating summaries.
Despite their immense potential, these models have been proven to learn and
exhibit societal biases and make unfair predictions. In this study, we explore
the ability of Large Language Models (LLMs) to perform legal tasks in the
Indian landscape when social factors are involved. We present a novel metric,
$\beta$-weighted $\textit{Legal Safety Score ($LSS_{\beta}$)}$, which
encapsulates both the fairness and accuracy aspects of the LLM. We assess LLMs'
safety by considering its performance in the $\textit{Binary Statutory
Reasoning}$ task and its fairness exhibition with respect to various axes of
disparities in the Indian society. Task performance and fairness scores of
LLaMA and LLaMA--2 models indicate that the proposed $LSS_{\beta}$ metric can
effectively determine the readiness of a model for safe usage in the legal
sector. We also propose finetuning pipelines, utilising specialised legal
datasets, as a potential method to mitigate bias and improve model safety. The
finetuning procedures on LLaMA and LLaMA--2 models increase the $LSS_{\beta}$,
improving their usability in the Indian legal domain. Our code is publicly
released.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10569" title="Abstract">arXiv:2402.10569</a> [<a href="/pdf/2402.10569" title="Download PDF">pdf</a>, <a href="/format/2402.10569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymptotic spectral properties and preconditioning of an approximated  nonlocal Helmholtz equation with Caputo fractional Laplacian and variable  coefficient wave number $&#x3bc;$
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Adriani%2C+A">Andrea Adriani</a>, 
<a href="/search/math?searchtype=author&query=Sormani%2C+R+L">Rosita Luisa Sormani</a>, 
<a href="/search/math?searchtype=author&query=Tablino-Possio%2C+C">Cristina Tablino-Possio</a>, 
<a href="/search/math?searchtype=author&query=Krause%2C+R">Rolf Krause</a>, 
<a href="/search/math?searchtype=author&query=Serra-Capizzano%2C+S">Stefano Serra-Capizzano</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 10 figures. arXiv admin note: text overlap with <a href="/abs/2206.05171">arXiv:2206.05171</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The current study investigates the asymptotic spectral properties of a finite
difference approximation of nonlocal Helmholtz equations with a Caputo
fractional Laplacian and a variable coefficient wave number $\mu$, as it occurs
when considering a wave propagation in complex media, characterized by nonlocal
interactions and spatially varying wave speeds. More specifically, by using
tools from Toeplitz and generalized locally Toeplitz theory, the present
research delves into the spectral analysis of nonpreconditioned and
preconditioned matrix-sequences. We report numerical evidences supporting the
theoretical findings. Finally, open problems and potential extensions in
various directions are presented and briefly discussed.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10570" title="Abstract">arXiv:2402.10570</a> [<a href="/pdf/2402.10570" title="Download PDF">pdf</a>, <a href="/format/2402.10570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimisation--Based Coupling of Finite Element Model and Reduced Order  Model for Computational Fluid Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Prusak%2C+I">Ivan Prusak</a>, 
<a href="/search/math?searchtype=author&query=Torlo%2C+D">Davide Torlo</a>, 
<a href="/search/math?searchtype=author&query=Nonino%2C+M">Monica Nonino</a>, 
<a href="/search/math?searchtype=author&query=Rozza%2C+G">Gianluigi Rozza</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">With the increased interest in complex problems, such as multiphysics and
multiscale models, as well as real-time computations, there is a strong need
for domain-decomposition (DD) segregated solvers and reduced-order models
(ROMs). Segregated models decouple the subcomponents of the problems at hand
and use already existing state-of-the-art numerical codes in each component. In
this manuscript, starting with a DD algorithm on non-overlapping domains, we
aim at the comparison of couplings of different discretisation models, such as
Finite Element (FEM) and ROM for separate subcomponents. In particular, we
consider an optimisation-based DD model on two non-overlapping subdomains where
the coupling on the common interface is performed by introducing a control
variable representing a normal flux. Gradient-based optimisation algorithms are
used to construct an iterative procedure to fully decouple the subdomain state
solutions as well as to locally generate ROMs on each subdomain. Then, we
consider FEM or ROM discretisation models for each of the DD problem
components, namely, the triplet state1-state2-control. We perform numerical
tests on the backward-facing step Navier-Stokes problem to investigate the
efficacy of the presented couplings in terms of optimisation iterations and
relative errors.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10571" title="Abstract">arXiv:2402.10571</a> [<a href="/pdf/2402.10571" title="Download PDF">pdf</a>, <a href="/format/2402.10571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct Preference Optimization with an Offset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amini%2C+A">Afra Amini</a>, 
<a href="/search/cs?searchtype=author&query=Vieira%2C+T">Tim Vieira</a>, 
<a href="/search/cs?searchtype=author&query=Cotterell%2C+R">Ryan Cotterell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Direct preference optimization (DPO) is a successful fine-tuning strategy for
aligning large language models with human preferences without the need to train
a reward model or employ reinforcement learning. DPO, as originally formulated,
relies on binary preference data and fine-tunes a language model to increase
the likelihood of a preferred response over a dispreferred response. However,
not all preference pairs are equal: while in some cases the preferred response
is only slightly better than the dispreferred response, there can be a stronger
preference for one response when, for example, the other response includes
harmful or toxic content. In this paper, we propose a generalization of DPO,
termed DPO with an offset (ODPO), that does not treat every preference pair
equally during fine-tuning. Intuitively, ODPO requires the difference between
the likelihood of the preferred and dispreferred response to be greater than an
offset value. The offset is determined based on the extent to which one
response is preferred over another. Our experiments on various tasks suggest
that ODPO significantly outperforms DPO in aligning language models, especially
when the number of preference pairs is limited.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10573" title="Abstract">arXiv:2402.10573</a> [<a href="/pdf/2402.10573" title="Download PDF">pdf</a>, <a href="/format/2402.10573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LinkNER: Linking Local Named Entity Recognition Models to Large Language  Models using Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yuhua Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Hang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+M">Mengting Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WebConf (WWW'2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Named Entity Recognition (NER) serves as a fundamental task in natural
language understanding, bearing direct implications for web content analysis,
search engines, and information retrieval systems. Fine-tuned NER models
exhibit satisfactory performance on standard NER benchmarks. However, due to
limited fine-tuning data and lack of knowledge, it performs poorly on unseen
entity recognition. As a result, the usability and reliability of NER models in
web-related applications are compromised. Instead, Large Language Models (LLMs)
like GPT-4 possess extensive external knowledge, but research indicates that
they lack specialty for NER tasks. Furthermore, non-public and large-scale
weights make tuning LLMs difficult. To address these challenges, we propose a
framework that combines small fine-tuned models with LLMs (LinkNER) and an
uncertainty-based linking strategy called RDC that enables fine-tuned models to
complement black-box LLMs, achieving better performance. We experiment with
both standard NER test sets and noisy social media datasets. LinkNER enhances
NER task performance, notably surpassing SOTA models in robustness tests. We
also quantitatively analyze the influence of key components like uncertainty
estimation methods, LLMs, and in-context learning on diverse NER tasks,
offering specific web-related recommendations.
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10575" title="Abstract">arXiv:2402.10575</a> [<a href="/pdf/2402.10575" title="Download PDF">pdf</a>, <a href="/format/2402.10575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symbolic Autoencoding for Self-Supervised Sequence Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amani%2C+M+H">Mohammad Hossein Amani</a>, 
<a href="/search/cs?searchtype=author&query=Baldwin%2C+N+M">Nicolas Mario Baldwin</a>, 
<a href="/search/cs?searchtype=author&query=Mansouri%2C+A">Amin Mansouri</a>, 
<a href="/search/cs?searchtype=author&query=Josifoski%2C+M">Martin Josifoski</a>, 
<a href="/search/cs?searchtype=author&query=Peyrard%2C+M">Maxime Peyrard</a>, 
<a href="/search/cs?searchtype=author&query=West%2C+R">Robert West</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Traditional language models, adept at next-token prediction in text
sequences, often struggle with transduction tasks between distinct symbolic
systems, particularly when parallel data is scarce. Addressing this issue, we
introduce \textit{symbolic autoencoding} ($\Sigma$AE), a self-supervised
framework that harnesses the power of abundant unparallel data alongside
limited parallel data. $\Sigma$AE connects two generative models via a discrete
bottleneck layer and is optimized end-to-end by minimizing reconstruction loss
(simultaneously with supervised loss for the parallel data), such that the
sequence generated by the discrete bottleneck can be read out as the transduced
input sequence. We also develop gradient-based methods allowing for efficient
self-supervised sequence learning despite the discreteness of the bottleneck.
Our results demonstrate that $\Sigma$AE significantly enhances performance on
transduction tasks, even with minimal parallel data, offering a promising
solution for weakly supervised learning scenarios.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10576" title="Abstract">arXiv:2402.10576</a> [<a href="/pdf/2402.10576" title="Download PDF">pdf</a>, <a href="/ps/2402.10576" title="Download PostScript">ps</a>, <a href="/format/2402.10576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Post-Quantum Cryptography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pranjal">Pranjal</a>, 
<a href="/search/cs?searchtype=author&query=Chaturvedi%2C+A">Atul Chaturvedi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">In this survey we propose to cover the prose of post-quantum cryptography
over classical cryptography. We talk about the various cryptographic methods
that are being practiced to safeguard our information. The future of secure
communication is expected to be the implementation of quantum-safe
cryptographic systems, and that in the post-quantum era, the development of
post-quantum cryptography is essential for ensuring the security of sensitive
data.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10580" title="Abstract">arXiv:2402.10580</a> [<a href="/pdf/2402.10580" title="Download PDF">pdf</a>, <a href="/format/2402.10580" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Multi-task Uncertainties for Joint Semantic Segmentation and  Monocular Depth Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Landgraf%2C+S">Steven Landgraf</a>, 
<a href="/search/cs?searchtype=author&query=Hillemann%2C+M">Markus Hillemann</a>, 
<a href="/search/cs?searchtype=author&query=Kapler%2C+T">Theodor Kapler</a>, 
<a href="/search/cs?searchtype=author&query=Ulrich%2C+M">Markus Ulrich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 5 figures, 10 tables, submitted to peer-reviewed journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Quantifying the predictive uncertainty emerged as a possible solution to
common challenges like overconfidence or lack of explainability and robustness
of deep neural networks, albeit one that is often computationally expensive.
Many real-world applications are multi-modal in nature and hence benefit from
multi-task learning. In autonomous driving, for example, the joint solution of
semantic segmentation and monocular depth estimation has proven to be valuable.
In this work, we first combine different uncertainty quantification methods
with joint semantic segmentation and monocular depth estimation and evaluate
how they perform in comparison to each other. Additionally, we reveal the
benefits of multi-task learning with regard to the uncertainty quality compared
to solving both tasks separately. Based on these insights, we introduce
EMUFormer, a novel student-teacher distillation approach for joint semantic
segmentation and monocular depth estimation as well as efficient multi-task
uncertainty quantification. By implicitly leveraging the predictive
uncertainties of the teacher, EMUFormer achieves new state-of-the-art results
on Cityscapes and NYUv2 and additionally estimates high-quality predictive
uncertainties for both tasks that are comparable or superior to a Deep Ensemble
despite being an order of magnitude more efficient.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10582" title="Abstract">arXiv:2402.10582</a> [<a href="/pdf/2402.10582" title="Download PDF">pdf</a>, <a href="/ps/2402.10582" title="Download PostScript">ps</a>, <a href="/format/2402.10582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deterministic Leader Election for Stationary Programmable Matter with  Common Direction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chalopin%2C+J">J&#xe9;r&#xe9;mie Chalopin</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Shantanu Das</a>, 
<a href="/search/cs?searchtype=author&query=Kokkou%2C+M">Maria Kokkou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, Accepted by SIROCCO 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Leader Election is an important primitive for programmable matter, since it
is often an intermediate step for the solution of more complex problems.
Although the leader election problem itself is well studied even in the
specific context of programmable matter systems, research on fault tolerant
approaches is more limited. We consider the problem in the previously studied
Amoebot model on a triangular grid, when the configuration is connected but
contains nodes the particles cannot move to (e.g., obstacles). We assume that
particles agree on a common direction (i.e., the horizontal axis) but do not
have chirality (i.e., they do not agree on the other two directions of the
triangular grid). We begin by showing that an election algorithm with explicit
termination is not possible in this case, but we provide an implicitly
terminating algorithm that elects a unique leader without requiring any
movement. These results are in contrast to those in the more common model with
chirality but no agreement on directions, where explicit termination is always
possible but the number of elected leaders depends on the symmetry of the
initial configuration. Solving the problem under the assumption of one common
direction allows for a unique leader to be elected in a stationary and
deterministic way, which until now was only possible for simply connected
configurations under a sequential scheduler.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10586" title="Abstract">arXiv:2402.10586</a> [<a href="/pdf/2402.10586" title="Download PDF">pdf</a>, <a href="/format/2402.10586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse  Motifs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+Z+M">Zae Myung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K+H">Kwang Hee Lee</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+P">Preston Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Raheja%2C+V">Vipul Raheja</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dongyeop Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">With the advent of large language models (LLM), the line between
human-crafted and machine-generated texts has become increasingly blurred. This
paper delves into the inquiry of identifying discernible and unique linguistic
properties in texts that were written by humans, particularly uncovering the
underlying discourse structures of texts beyond their surface structures.
Introducing a novel methodology, we leverage hierarchical parse trees and
recursive hypergraphs to unveil distinctive discourse patterns in texts
produced by both LLMs and humans. Empirical findings demonstrate that, although
both LLMs and humans generate distinct discourse patterns influenced by
specific domains, human-written texts exhibit more structural variability,
reflecting the nuanced nature of human writing in different domains. Notably,
incorporating hierarchical discourse features enhances binary classifiers'
overall performance in distinguishing between human-written and
machine-generated texts, even on out-of-distribution and paraphrased samples.
This underscores the significance of incorporating hierarchical discourse
features in the analysis of text patterns. The code and dataset will be
available at [TBA].
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10588" title="Abstract">arXiv:2402.10588</a> [<a href="/pdf/2402.10588" title="Download PDF">pdf</a>, <a href="/format/2402.10588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Llamas Work in English? On the Latent Language of Multilingual  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wendler%2C+C">Chris Wendler</a>, 
<a href="/search/cs?searchtype=author&query=Veselovsky%2C+V">Veniamin Veselovsky</a>, 
<a href="/search/cs?searchtype=author&query=Monea%2C+G">Giovanni Monea</a>, 
<a href="/search/cs?searchtype=author&query=West%2C+R">Robert West</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages. 28 with appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">We ask whether multilingual language models trained on unbalanced,
English-dominated corpora use English as an internal pivot language -- a
question of key importance for understanding how language models function and
the origins of linguistic bias. Focusing on the Llama-2 family of transformer
models, our study uses carefully constructed non-English prompts with a unique
correct single-token continuation. From layer to layer, transformers gradually
map an input embedding of the final prompt token to an output embedding from
which next-token probabilities are computed. Tracking intermediate embeddings
through their high-dimensional space reveals three distinct phases, whereby
intermediate embeddings (1) start far away from output token embeddings; (2)
already allow for decoding a semantically correct next token in the middle
layers, but give higher probability to its version in English than in the input
language; (3) finally move into an input-language-specific region of the
embedding space. We cast these results into a conceptual model where the three
phases operate in "input space", "concept space", and "output space",
respectively. Crucially, our evidence suggests that the abstract "concept
space" lies closer to English than to other languages, which may have important
consequences regarding the biases held by multilingual language models.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10592" title="Abstract">arXiv:2402.10592</a> [<a href="/pdf/2402.10592" title="Download PDF">pdf</a>, <a href="/format/2402.10592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Adaptive Experiments: A Unified Approach to Regret  Minimization and Best-Arm Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qin%2C+C">Chao Qin</a>, 
<a href="/search/cs?searchtype=author&query=Russo%2C+D">Daniel Russo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Econometrics (econ.EM); Machine Learning (stat.ML)

</div>
<p class="mathjax">Practitioners conducting adaptive experiments often encounter two competing
priorities: reducing the cost of experimentation by effectively assigning
treatments during the experiment itself, and gathering information swiftly to
conclude the experiment and implement a treatment across the population.
Currently, the literature is divided, with studies on regret minimization
addressing the former priority in isolation, and research on best-arm
identification focusing solely on the latter. This paper proposes a unified
model that accounts for both within-experiment performance and post-experiment
outcomes. We then provide a sharp theory of optimal performance in large
populations that unifies canonical results in the literature. This unification
also uncovers novel insights. For example, the theory reveals that familiar
algorithms, like the recently proposed top-two Thompson sampling algorithm, can
be adapted to optimize a broad class of objectives by simply adjusting a single
scalar parameter. In addition, the theory reveals that enormous reductions in
experiment duration can sometimes be achieved with minimal impact on both
within-experiment and post-experiment regret.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10593" title="Abstract">arXiv:2402.10593</a> [<a href="/pdf/2402.10593" title="Download PDF">pdf</a>, <a href="/format/2402.10593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Learning for Double-RIS Aided ISAC Systems with Superimposed  Pilots and Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gan%2C+X">Xu Gan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chongwen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhaohui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+C">Caijun Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaoming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaoyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qinghua Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yuen%2C+C">Chau Yuen</a>, 
<a href="/search/cs?searchtype=author&query=Debbah%2C+M">Merouane Debbah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Reconfigurable intelligent surface (RIS) has great potential to improve the
performance of integrated sensing and communication (ISAC) systems, especially
in scenarios where line-of-sight paths between the base station and users are
blocked. However, the spectral efficiency (SE) of RIS-aided ISAC uplink
transmissions may be drastically reduced by the heavy burden of pilot overhead
for realizing sensing capabilities. In this paper, we tackle this bottleneck by
proposing a superimposed symbol scheme, which superimposes sensing pilots onto
data symbols over the same time-frequency resources. Specifically, we develop a
structure-aware sparse Bayesian learning framework, where decoded data symbols
serve as side information to enhance sensing performance and increase SE. To
meet the low-latency requirements of emerging ISAC applications, we further
propose a low-complexity simultaneous communication and localization algorithm
for multiple users. This algorithm employs the unitary approximate message
passing in the Bayesian learning framework for initial angle estimate, followed
by iterative refinements through reduced-dimension matrix calculations.
Moreover, the sparse code multiple access technology is incorporated into this
iterative framework for accurate data detection which also facilitates
localization. Numerical results show that the proposed superimposed
symbol-based scheme empowered by the developed algorithm can achieve
centimeter-level localization while attaining up to $96\%$ of the SE of
conventional communications without sensing capabilities. Moreover, compared to
other typical ISAC schemes, the proposed superimposed symbol scheme can provide
an effective throughput improvement over $133\%$.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10595" title="Abstract">arXiv:2402.10595</a> [<a href="/pdf/2402.10595" title="Download PDF">pdf</a>, <a href="/format/2402.10595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compact and De-biased Negative Instance Embedding for Multi-Instance  Learning on Whole-Slide Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Joohyung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Nam%2C+H">Heejeong Nam</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kwanhyung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Hahn%2C+S">Sangchul Hahn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICASSP 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Whole-slide image (WSI) classification is a challenging task because 1)
patches from WSI lack annotation, and 2) WSI possesses unnecessary variability,
e.g., stain protocol. Recently, Multiple-Instance Learning (MIL) has made
significant progress, allowing for classification based on slide-level, rather
than patch-level, annotations. However, existing MIL methods ignore that all
patches from normal slides are normal. Using this free annotation, we introduce
a semi-supervision signal to de-bias the inter-slide variability and to capture
the common factors of variation within normal patches. Because our method is
orthogonal to the MIL algorithm, we evaluate our method on top of the recently
proposed MIL algorithms and also compare the performance with other
semi-supervised approaches. We evaluate our method on two public WSI datasets
including Camelyon-16 and TCGA lung cancer and demonstrate that our approach
significantly improves the predictive performance of existing MIL algorithms
and outperforms other semi-supervised algorithms. We release our code at
https://github.com/AITRICS/pathology_mil.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10597" title="Abstract">arXiv:2402.10597</a> [<a href="/pdf/2402.10597" title="Download PDF">pdf</a>, <a href="/format/2402.10597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficiency at Scale: Investigating the Performance of Diminutive  Language Models in Clinical Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taylor%2C+N">Niall Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Ghose%2C+U">Upamanyu Ghose</a>, 
<a href="/search/cs?searchtype=author&query=Rohanian%2C+O">Omid Rohanian</a>, 
<a href="/search/cs?searchtype=author&query=Nouriborji%2C+M">Mohammadmahdi Nouriborji</a>, 
<a href="/search/cs?searchtype=author&query=Kormilitzin%2C+A">Andrey Kormilitzin</a>, 
<a href="/search/cs?searchtype=author&query=Clifton%2C+D">David Clifton</a>, 
<a href="/search/cs?searchtype=author&query=Nevado-Holgado%2C+A">Alejo Nevado-Holgado</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The entry of large language models (LLMs) into research and commercial spaces
has led to a trend of ever-larger models, with initial promises of
generalisability, followed by a widespread desire to downsize and create
specialised models without the need for complete fine-tuning, using Parameter
Efficient Fine-tuning (PEFT) methods. We present an investigation into the
suitability of different PEFT methods to clinical decision-making tasks, across
a range of model sizes, including extremely small models with as few as $25$
million parameters.
<br />Our analysis shows that the performance of most PEFT approaches varies
significantly from one task to another, with the exception of LoRA, which
maintains relatively high performance across all model sizes and tasks,
typically approaching or matching full fine-tuned performance. The
effectiveness of PEFT methods in the clinical domain is evident, particularly
for specialised models which can operate on low-cost, in-house computing
infrastructure. The advantages of these models, in terms of speed and reduced
training costs, dramatically outweighs any performance gain from large
foundation LLMs. Furthermore, we highlight how domain-specific pre-training
interacts with PEFT methods and model size, and discuss how these factors
interplay to provide the best efficiency-performance trade-off. Full code
available at: tbd.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10600" title="Abstract">arXiv:2402.10600</a> [<a href="/pdf/2402.10600" title="Download PDF">pdf</a>, <a href="/format/2402.10600" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Envisioning the Future Role of 3D Wireless Networks in Preventing and  Managing Disasters and Emergency Situations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alhammadi%2C+A">Ahmed Alhammadi</a>, 
<a href="/search/cs?searchtype=author&query=Abraham%2C+A">Anuj Abraham</a>, 
<a href="/search/cs?searchtype=author&query=Fakhreddine%2C+A">Aymen Fakhreddine</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yu Tian</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+J">Jun Du</a>, 
<a href="/search/cs?searchtype=author&query=Bader%2C+F">Faouzi Bader</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">In an era marked by unprecedented climatic upheavals and evolving urban
landscapes, the role of advanced communication networks in disaster prevention
and management is becoming increasingly critical. This paper explores the
transformative potential of 3D wireless networks, an innovative amalgamation of
terrestrial, aerial, and satellite technologies, in enhancing disaster response
mechanisms. We delve into a myriad of use cases, ranging from large facility
evacuations to wildfire management, underscoring the versatility of these
networks in ensuring timely communication, real-time situational awareness, and
efficient resource allocation during crises. We also present an overview of
cutting-edge prototypes, highlighting the practical feasibility and operational
efficacy of 3D wireless networks in real-world scenarios. Simultaneously, we
acknowledge the challenges posed by aspects such as cybersecurity, cross-border
coordination, and physical layer technological hurdles, and propose future
directions for research and development in this domain.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10601" title="Abstract">arXiv:2402.10601</a> [<a href="/pdf/2402.10601" title="Download PDF">pdf</a>, <a href="/format/2402.10601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Jailbreaking Proprietary Large Language Models using Word Substitution  Cipher
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Handa%2C+D">Divij Handa</a>, 
<a href="/search/cs?searchtype=author&query=Chirmule%2C+A">Advait Chirmule</a>, 
<a href="/search/cs?searchtype=author&query=Gajera%2C+B">Bimal Gajera</a>, 
<a href="/search/cs?searchtype=author&query=Baral%2C+C">Chitta Baral</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) are aligned to moral and ethical guidelines but
remain susceptible to creative prompts called Jailbreak that can bypass the
alignment process. However, most jailbreaking prompts contain harmful questions
in the natural language (mainly English), which can be detected by the LLM
themselves. In this paper, we present jailbreaking prompts encoded using
cryptographic techniques. We first present a pilot study on the
state-of-the-art LLM, GPT-4, in decoding several safe sentences that have been
encrypted using various cryptographic techniques and find that a
straightforward word substitution cipher can be decoded most effectively.
Motivated by this result, we use this encoding technique for writing
jailbreaking prompts. We present a mapping of unsafe words with safe words and
ask the unsafe question using these mapped words. Experimental results show an
attack success rate (up to 59.42%) of our proposed jailbreaking approach on
state-of-the-art proprietary models including ChatGPT, GPT-4, and Gemini-Pro.
Additionally, we discuss the over-defensiveness of these models. We believe
that our work will encourage further research in making these LLMs more robust
while maintaining their decoding capabilities.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10602" title="Abstract">arXiv:2402.10602</a> [<a href="/pdf/2402.10602" title="Download PDF">pdf</a>, <a href="/format/2402.10602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are ID Embeddings Necessary? Whitening Pre-trained Text Embeddings for  Effective Sequential Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lingzi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Zhiwei Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Z">Zhiqi Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Recent sequential recommendation models have combined pre-trained text
embeddings of items with item ID embeddings to achieve superior recommendation
performance. Despite their effectiveness, the expressive power of text features
in these models remains largely unexplored. While most existing models
emphasize the importance of ID embeddings in recommendations, our study takes a
step further by studying sequential recommendation models that only rely on
text features and do not necessitate ID embeddings. Upon examining pretrained
text embeddings experimentally, we discover that they reside in an anisotropic
semantic space, with an average cosine similarity of over 0.8 between items. We
also demonstrate that this anisotropic nature hinders recommendation models
from effectively differentiating between item representations and leads to
degenerated performance. To address this issue, we propose to employ a
pre-processing step known as whitening transformation, which transforms the
anisotropic text feature distribution into an isotropic Gaussian distribution.
Our experiments show that whitening pre-trained text embeddings in the
sequential model can significantly improve recommendation performance. However,
the full whitening operation might break the potential manifold of items with
similar text semantics. To preserve the original semantics while benefiting
from the isotropy of the whitened text features, we introduce WhitenRec+, an
ensemble approach that leverages both fully whitened and relaxed whitened item
representations for effective recommendations. We further discuss and analyze
the benefits of our design through experiments and proofs. Experimental results
on three public benchmark datasets demonstrate that WhitenRec+ outperforms
state-of-the-art methods for sequential recommendation.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10610" title="Abstract">arXiv:2402.10610</a> [<a href="/pdf/2402.10610" title="Download PDF">pdf</a>, <a href="/ps/2402.10610" title="Download PostScript">ps</a>, <a href="/format/2402.10610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spanning Matrices via Satisfiability Solving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eisenhofer%2C+C">Clemens Eisenhofer</a>, 
<a href="/search/cs?searchtype=author&query=Rawson%2C+M">Michael Rawson</a>, 
<a href="/search/cs?searchtype=author&query=Kov%C3%A1cs%2C+L">Laura Kov&#xe1;cs</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">We propose a new encoding of the first-order connection method as a Boolean
satisfiability problem. The encoding eschews tree-like presentations of the
connection method in favour of matrices, as we show that tree-like calculi have
a number of drawbacks in the context of satisfiability solving. The matrix
setting permits numerous global refinements of the basic connection calculus.
We also show that a suitably-refined calculus is a decision procedure for the
Bernays-Sch\"onfinkel class.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10612" title="Abstract">arXiv:2402.10612</a> [<a href="/pdf/2402.10612" title="Download PDF">pdf</a>, <a href="/format/2402.10612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieve Only When It Needs: Adaptive Retrieval Augmentation for  Hallucination Mitigation in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+H">Hanxing Ding</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+L">Liang Pang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zihao Wei</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Huawei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Hallucinations pose a significant challenge for the practical implementation
of large language models (LLMs). The utilization of parametric knowledge in
generating factual content is constrained by the limited knowledge of LLMs,
potentially resulting in internal hallucinations. While incorporating external
information can help fill knowledge gaps, it also introduces the risk of
irrelevant information, thereby increasing the likelihood of external
hallucinations. A careful and balanced integration of the parametric knowledge
within LLMs with external information is crucial to alleviate hallucinations.
In this study, we present Rowen, a novel approach that enhances LLMs with a
selective retrieval augmentation process tailored to address hallucinated
outputs. This process is governed by a multilingual semantic-aware detection
module, which evaluates the consistency of the perturbed responses across
various languages for the same queries. Upon detecting inconsistencies
indicative of hallucinations, Rowen activates the retrieval of external
information to rectify the model outputs. Rowen adeptly harmonizes the
intrinsic parameters in LLMs with external knowledge sources, effectively
mitigating hallucinations by ensuring a balanced integration of internal
reasoning and external evidence. Through a comprehensive empirical analysis, we
demonstrate that Rowen surpasses the current state-of-the-art in both detecting
and mitigating hallucinated content within the outputs of LLMs.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10614" title="Abstract">arXiv:2402.10614</a> [<a href="/pdf/2402.10614" title="Download PDF">pdf</a>, <a href="/format/2402.10614" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can LLMs Speak For Diverse People? Tuning LLMs via Debate to Generate  Controllable Controversial Statements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Ming Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiuhai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lichang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tianyi Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Making LLMs speak for different, especially minority groups of people, and
generate statements supporting their diverse or even controversial perspectives
is critical to creating an inclusive environment. However, existing LLMs lack
sufficient controllability to the stance of their generated content, which
often contains inconsistent, neutral, or biased statements. In this paper, we
improve the controllability of LLMs in generating statements supporting an
argument the user defined in the prompt. We find that multi-round debates
between two LLMs with opposite stances generate higher-quality and more salient
statements for each, which are important training data to improve the
controllability of LLMs. Motivated by this, we develop a novel debate &amp; tuning
("DEBATunE") pipeline finetuning LLMs to generate the statements obtained via
debate. To examine DEBATunE, we curate the largest dataset of debate topics so
far, which covers 710 controversial topics and corresponding arguments for each
topic. Evaluations by the GPT-4 judge with a novel controversy controllability
metric show that LLMs' capability of expressing diverse perspectives is
significantly improved by DEBATunE. Moreover, such controllability can be
generalized to unseen topics, generating high-quality statements supporting
controversial arguments. Our codes, models, and data will be released at
https://github.com/tianyi-lab/DEBATunE.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10615" title="Abstract">arXiv:2402.10615</a> [<a href="/pdf/2402.10615" title="Download PDF">pdf</a>, <a href="/format/2402.10615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A mortar method for the coupled Stokes-Darcy problem using the MAC  scheme for Stokes and mixed finite elements for Darcy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Boon%2C+W+M">Wietse M. Boon</a>, 
<a href="/search/math?searchtype=author&query=Gl%C3%A4ser%2C+D">Dennis Gl&#xe4;ser</a>, 
<a href="/search/math?searchtype=author&query=Helmig%2C+R">Rainer Helmig</a>, 
<a href="/search/math?searchtype=author&query=Weishaupt%2C+K">Kilian Weishaupt</a>, 
<a href="/search/math?searchtype=author&query=Yotov%2C+I">Ivan Yotov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A discretization method with non-matching grids is proposed for the coupled
Stokes-Darcy problem that uses a mortar variable at the interface to couple the
marker and cell (MAC) method in the Stokes domain with the Raviart-Thomas mixed
finite element pair in the Darcy domain. Due to this choice, the method
conserves linear momentum and mass locally in the Stokes domain and exhibits
local mass conservation in the Darcy domain. The MAC scheme is reformulated as
a mixed finite element method on a staggered grid, which allows for the
proposed scheme to be analyzed as a mortar mixed finite element method. We show
that the discrete system is well-posed and derive a priori error estimates that
indicate first order convergence in all variables. The system can be reduced to
an interface problem concerning only the mortar variables, leading to a
non-overlapping domain decomposition method. Numerical examples are presented
to illustrate the theoretical results and the applicability of the method.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10616" title="Abstract">arXiv:2402.10616</a> [<a href="/pdf/2402.10616" title="Download PDF">pdf</a>, <a href="/ps/2402.10616" title="Download PostScript">ps</a>, <a href="/format/2402.10616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Credential Control Balance: A Universal Blockchain Account Model  Abstract From Bank to Bitcoin, Ethereum External Owned Account and Account  Abstraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiao%2C+H">Huifeng Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Udomlertsakul%2C+D+N">Dr. Nathapon Udomlertsakul</a>, 
<a href="/search/cs?searchtype=author&query=Tamprasirt%2C+D+A">Dr. Anukul Tamprasirt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 15 figures, conference paper(Thailand International College Conference 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Blockchain market value peaked at $3 trillion, fell to $1 trillion, then
recovered to $1.5 trillion and is rising again. Blockchain accounts secure most
on-chain assets in this huge market (Web-12). This paper initiates a universal
blockchain account model from a comprehensive review of blockchain account
development, encompassing both academic and industry perspectives. This paper
uses a model analysis method to analysis the account progress and create high
level new account model. And it uses systematic literature review method to
search, filter, analysis and evaluate the papers about account models and
analyzes related technology trade-offs. Searching with key words: blockchain,
account, private key and security in WOS, Scopus and Bitcoin and Ethereum
community repositories, this research provides in-depth insights into the
design and evaluation of account models, from traditional bank accounts to
Bitcoin, EVM-adaptable, and abstraction accounts. Through data-driven
comparisons of account models (security, cost, adoption), this study also
explores future directions and provides an overview of cross-model account
theory, guiding further blockchain research. This paper leaves deeper dives
into model change drivers, application technology advancements.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10617" title="Abstract">arXiv:2402.10617</a> [<a href="/pdf/2402.10617" title="Download PDF">pdf</a>, <a href="/ps/2402.10617" title="Download PostScript">ps</a>, <a href="/format/2402.10617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multitask Kernel-based Learning with Logic Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Diligenti%2C+M">Michelangelo Diligenti</a>, 
<a href="/search/cs?searchtype=author&query=Gori%2C+M">Marco Gori</a>, 
<a href="/search/cs?searchtype=author&query=Maggini%2C+M">Marco Maggini</a>, 
<a href="/search/cs?searchtype=author&query=Rigutini%2C+L">Leonardo Rigutini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 19th European Conference on Artificial Intelligence (ECAI 2010)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 19th European Conference on Artificial
  Intelligence (ECAI 2010)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper presents a general framework to integrate prior knowledge in the
form of logic constraints among a set of task functions into kernel machines.
The logic propositions provide a partial representation of the environment, in
which the learner operates, that is exploited by the learning algorithm
together with the information available in the supervised examples. In
particular, we consider a multi-task learning scheme, where multiple unary
predicates on the feature space are to be learned by kernel machines and a
higher level abstract representation consists of logic clauses on these
predicates, known to hold for any input. A general approach is presented to
convert the logic clauses into a continuous implementation, that processes the
outputs computed by the kernel-based predicates. The learning task is
formulated as a primal optimization problem of a loss function that combines a
term measuring the fitting of the supervised examples, a regularization term,
and a penalty term that enforces the constraints on both supervised and
unsupervised examples. The proposed semi-supervised learning framework is
particularly suited for learning in high dimensionality feature spaces, where
the supervised training examples tend to be sparse and generalization
difficult. Unlike for standard kernel machines, the cost function to optimize
is not generally guaranteed to be convex. However, the experimental results
show that it is still possible to find good solutions using a two stage
learning schema, in which first the supervised examples are learned until
convergence and then the logic constraints are forced. Some promising
experimental results on artificial multi-task learning tasks are reported,
showing how the classification accuracy can be effectively improved by
exploiting the a priori rules and the unsupervised examples.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10618" title="Abstract">arXiv:2402.10618</a> [<a href="/pdf/2402.10618" title="Download PDF">pdf</a>, <a href="/format/2402.10618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Role-playing Systems through Aggressive Queries: Evaluation  and Improvement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Y">Yihong Tang</a>, 
<a href="/search/cs?searchtype=author&query=Ou%2C+J">Jiao Ou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Che Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fuzheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Di Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gai%2C+K">Kun Gai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The advent of Large Language Models (LLMs) has propelled dialogue generation
into new realms, particularly in the field of role-playing systems (RPSs).
While enhanced with ordinary role-relevant training dialogues, existing
LLM-based RPSs still struggle to align with roles when handling intricate and
trapped queries in boundary scenarios. In this paper, we design the Modular
ORchestrated Trap-setting Interaction SystEm (MORTISE) to benchmark and improve
the role-playing LLMs' performance. MORTISE can produce highly role-relevant
aggressive queries through the collaborative effort of multiple LLM-based
modules, and formulate corresponding responses to create an adversarial
training dataset via a consistent response generator. We select 190 Chinese and
English roles to construct aggressive queries to benchmark existing
role-playing LLMs. Through comprehensive evaluation, we find that existing
models exhibit a general deficiency in role alignment capabilities. We further
select 180 of the roles to collect an adversarial training dataset (named
RoleAD) and retain the other 10 roles for testing. Experiments on models
improved by RoleAD indicate that our adversarial dataset ameliorates this
deficiency, with the improvements demonstrating a degree of generalizability in
ordinary scenarios.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10626" title="Abstract">arXiv:2402.10626</a> [<a href="/pdf/2402.10626" title="Download PDF">pdf</a>, <a href="/format/2402.10626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Beamforming for RIS-aided Communications: Gradient-based Manifold  Meta Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+F">Fenghao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinquan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chongwen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhaohui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaoming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Alhammadi%2C+A">Ahmed Alhammadi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaoyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yuen%2C+C">Chau Yuen</a>, 
<a href="/search/cs?searchtype=author&query=Debbah%2C+M">M&#xe9;rouane Debbah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Reconfigurable intelligent surface (RIS) has become a promising technology to
realize the programmable wireless environment via steering the incident signal
in fully customizable ways. However, a major challenge in RIS-aided
communication systems is the simultaneous design of the precoding matrix at the
base station (BS) and the phase shifting matrix of the RIS elements. This is
mainly attributed to the highly non-convex optimization space of variables at
both the BS and the RIS, and the diversity of communication environments.
Generally, traditional optimization methods for this problem suffer from the
high complexity, while existing deep learning based methods are lack of
robustness in various scenarios. To address these issues, we introduce a
gradient-based manifold meta learning method (GMML), which works without
pre-training and has strong robustness for RIS-aided communications.
Specifically, the proposed method fuses meta learning and manifold learning to
improve the overall spectral efficiency, and reduce the overhead of the
high-dimensional signal process. Unlike traditional deep learning based methods
which directly take channel state information as input, GMML feeds the
gradients of the precoding matrix and phase shifting matrix into neural
networks. Coherently, we design a differential regulator to constrain the phase
shifting matrix of the RIS. Numerical results show that the proposed GMML can
improve the spectral efficiency by up to 7.31\%, and speed up the convergence
by 23 times faster compared to traditional approaches. Moreover, they also
demonstrate remarkable robustness and adaptability in dynamic settings.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10627" title="Abstract">arXiv:2402.10627</a> [<a href="/pdf/2402.10627" title="Download PDF">pdf</a>, <a href="/format/2402.10627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Alphabet Reduction for Reconfiguration Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ohsaka%2C+N">Naoto Ohsaka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We present a reconfiguration analogue of alphabet reduction \`a la Dinur (J.
ACM, 2007) and its applications. Given a binary constraint graph $G$ and its
two satisfying assignments $\psi^\mathsf{ini}$ and $\psi^\mathsf{tar}$, the
Maxmin Binary CSP Reconfiguration problem requests to transform
$\psi^\mathsf{ini}$ into $\psi^\mathsf{tar}$ by repeatedly changing the value
of a single vertex so that the minimum fraction of satisfied edges is
maximized. We demonstrate a polynomial-time reduction from Maxmin Binary CSP
Reconfiguration with arbitrarily large alphabet size $W \in \mathbb{N}$ to
itself with universal alphabet size $W_0 \in \mathbb{N}$ such that
<br />1. the perfect completeness is preserved, and
<br />2. if any reconfiguration for the former violates $\varepsilon$-fraction of
edges, then $\Omega(\varepsilon)$-fraction of edges must be unsatisfied during
any reconfiguration for the latter.
<br />The crux of its construction is the reconfigurability of Hadamard codes,
which enables to reconfigure between a pair of codewords, while avoiding
getting too close to the other codewords. Combining this alphabet reduction
with gap amplification due to Ohsaka (SODA 2024), we are able to amplify the
$1$ vs. $1-\varepsilon$ gap for arbitrarily small $\varepsilon \in (0,1)$ up to
the $1$ vs. $1-\varepsilon_0$ for some universal $\varepsilon_0 \in (0,1)$
without blowing up the alphabet size. In particular, a $1$ vs.
$1-\varepsilon_0$ gap version of Maxmin Binary CSP Reconfiguration with
alphabet size $W_0$ is PSPACE-hard only assuming the Reconfiguration
Inapproximability Hypothesis posed by Ohsaka (STACS 2023), whose gap parameter
can be arbitrarily small. This may not be achieved only by gap amplification of
Ohsaka, which makes the alphabet size gigantic depending on the gap value of
the hypothesis.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10628" title="Abstract">arXiv:2402.10628</a> [<a href="/pdf/2402.10628" title="Download PDF">pdf</a>, <a href="/format/2402.10628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FairSync: Ensuring Amortized Group Exposure in Distributed  Recommendation Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yiming Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Q">Qi Qi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in WWW'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">In pursuit of fairness and balanced development, recommender systems (RS)
often prioritize group fairness, ensuring that specific groups maintain a
minimum level of exposure over a given period. For example, RS platforms aim to
ensure adequate exposure for new providers or specific categories of items
according to their needs. Modern industry RS usually adopts a two-stage
pipeline: stage-1 (retrieval stage) retrieves hundreds of candidates from
millions of items distributed across various servers, and stage-2 (ranking
stage) focuses on presenting a small-size but accurate selection from items
chosen in stage-1. Existing efforts for ensuring amortized group exposures
focus on stage-2, however, stage-1 is also critical for the task. Without a
high-quality set of candidates, the stage-2 ranker cannot ensure the required
exposure of groups. Previous fairness-aware works designed for stage-2
typically require accessing and traversing all items. In stage-1, however,
millions of items are distributively stored in servers, making it infeasible to
traverse all of them. How to ensure group exposures in the distributed
retrieval process is a challenging question. To address this issue, we
introduce a model named FairSync, which transforms the problem into a
constrained distributed optimization problem. Specifically, FairSync resolves
the issue by moving it to the dual space, where a central node aggregates
historical fairness data into a vector and distributes it to all servers. To
trade off the efficiency and accuracy, the gradient descent technique is used
to periodically update the parameter of the dual vector. The experiment results
on two public recommender retrieval datasets showcased that FairSync
outperformed all the baselines, achieving the desired minimum level of
exposures while maintaining a high level of retrieval accuracy.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10631" title="Abstract">arXiv:2402.10631</a> [<a href="/pdf/2402.10631" title="Download PDF">pdf</a>, <a href="/format/2402.10631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via  Self-Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+D">Dayou Du</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yijia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+S">Shijie Cao</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jiaqi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+T">Ting Cao</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+X">Xiaowen Chu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+N">Ningyi Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The upscaling of Large Language Models (LLMs) has yielded impressive advances
in natural language processing, yet it also poses significant deployment
challenges. Weight quantization has emerged as a widely embraced solution to
reduce memory and computational demands. This paper introduces BitDistiller, a
framework that synergizes Quantization-Aware Training (QAT) with Knowledge
Distillation (KD) to boost the performance of LLMs at ultra-low precisions
(sub-4-bit). Specifically, BitDistiller first incorporates a tailored
asymmetric quantization and clipping technique to maximally preserve the
fidelity of quantized weights, and then proposes a novel Confidence-Aware
Kullback-Leibler Divergence (CAKLD) objective, which is employed in a
self-distillation manner to enable faster convergence and superior model
performance. Empirical evaluations demonstrate that BitDistiller significantly
surpasses existing methods in both 3-bit and 2-bit configurations on general
language understanding and complex reasoning benchmarks. Notably, BitDistiller
is shown to be more cost-effective, demanding fewer data and training
resources. The code is available at https://github.com/DD-DuDa/BitDistiller.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10634" title="Abstract">arXiv:2402.10634</a> [<a href="/pdf/2402.10634" title="Download PDF">pdf</a>, <a href="/format/2402.10634" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-based Forecasting with Missing Data through Spatiotemporal  Downsampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marisca%2C+I">Ivan Marisca</a>, 
<a href="/search/cs?searchtype=author&query=Alippi%2C+C">Cesare Alippi</a>, 
<a href="/search/cs?searchtype=author&query=Bianchi%2C+F+M">Filippo Maria Bianchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Given a set of synchronous time series, each associated with a sensor-point
in space and characterized by inter-series relationships, the problem of
spatiotemporal forecasting consists of predicting future observations for each
point. Spatiotemporal graph neural networks achieve striking results by
representing the relationships across time series as a graph. Nonetheless, most
existing methods rely on the often unrealistic assumption that inputs are
always available and fail to capture hidden spatiotemporal dynamics when part
of the data is missing. In this work, we tackle this problem through
hierarchical spatiotemporal downsampling. The input time series are
progressively coarsened over time and space, obtaining a pool of
representations that capture heterogeneous temporal and spatial dynamics.
Conditioned on observations and missing data patterns, such representations are
combined by an interpretable attention mechanism to generate the forecasts. Our
approach outperforms state-of-the-art methods on synthetic and real-world
benchmarks under different missing data distributions, particularly in the
presence of contiguous blocks of missing values.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10635" title="Abstract">arXiv:2402.10635</a> [<a href="/pdf/2402.10635" title="Download PDF">pdf</a>, <a href="/format/2402.10635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ContiFormer: Continuous-Time Transformer for Irregular Time Series  Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+K">Kan Ren</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yansen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yuchen Fang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Weiwei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongsheng Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Neurips 2023 Poster
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Modeling continuous-time dynamics on irregular time series is critical to
account for data evolution and correlations that occur continuously.
Traditional methods including recurrent neural networks or Transformer models
leverage inductive bias via powerful neural architectures to capture complex
patterns. However, due to their discrete characteristic, they have limitations
in generalizing to continuous-time data paradigms. Though neural ordinary
differential equations (Neural ODEs) and their variants have shown promising
results in dealing with irregular time series, they often fail to capture the
intricate correlations within these sequences. It is challenging yet demanding
to concurrently model the relationship between input data points and capture
the dynamic changes of the continuous-time system. To tackle this problem, we
propose ContiFormer that extends the relation modeling of vanilla Transformer
to the continuous-time domain, which explicitly incorporates the modeling
abilities of continuous dynamics of Neural ODEs with the attention mechanism of
Transformers. We mathematically characterize the expressive power of
ContiFormer and illustrate that, by curated designs of function hypothesis,
many Transformer variants specialized in irregular time series modeling can be
covered as a special case of ContiFormer. A wide range of experiments on both
synthetic and real-world datasets have illustrated the superior modeling
capacities and prediction performance of ContiFormer on irregular time series
data. The project link is https://seqml.github.io/contiformer/.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10636" title="Abstract">arXiv:2402.10636</a> [<a href="/pdf/2402.10636" title="Download PDF">pdf</a>, <a href="/format/2402.10636" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PEGASUS: Personalized Generative 3D Avatars with Composable Attributes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cha%2C+H">Hyunsoo Cha</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+B">Byungjun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Joo%2C+H">Hanbyul Joo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present, PEGASUS, a method for constructing personalized generative 3D
face avatars from monocular video sources. As a compositional generative model,
our model enables disentangled controls to selectively alter the facial
attributes (e.g., hair or nose) of the target individual, while preserving the
identity. We present two key approaches to achieve this goal. First, we present
a method to construct a person-specific generative 3D avatar by building a
synthetic video collection of the target identity with varying facial
attributes, where the videos are synthesized by borrowing parts from diverse
individuals from other monocular videos. Through several experiments, we
demonstrate the superior performance of our approach by generating unseen
attributes with high realism. Subsequently, we introduce a zero-shot approach
to achieve the same generative modeling more efficiently by leveraging a
previously constructed personalized generative model.
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10639" title="Abstract">arXiv:2402.10639</a> [<a href="/pdf/2402.10639" title="Download PDF">pdf</a>, <a href="/format/2402.10639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizability of Mixture of Domain-Specific Adapters from the Lens of  Signed Weight Directions and its Application to Effective Model Pruning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Tuc Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Thai Le</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Several parameter-efficient fine-tuning methods based on adapters have been
proposed as a streamlined approach to incorporate not only a single specialized
knowledge into existing Pre-Trained Language Models (PLMs) but also multiple of
them at once. Recent works such as AdapterSoup propose to mix not all but only
a selective sub-set of domain-specific adapters during inference via model
weight averaging to optimize performance on novel, unseen domains with
excellent computational efficiency. However, the essential generalizability of
this emerging weight-space adapter mixing mechanism on unseen, in-domain
examples remains unexplored. Thus, in this study, we conduct a comprehensive
analysis to elucidate the generalizability of domain-specific adapter mixtures
in in-domain evaluation. We also provide investigations into the inner workings
of the mixture of domain-specific adapters by analyzing their weight signs,
yielding critical analysis on the negative correlation between their fraction
of weight sign difference and their mixtures' generalizability. All source code
will be published.
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10641" title="Abstract">arXiv:2402.10641</a> [<a href="/pdf/2402.10641" title="Download PDF">pdf</a>, <a href="/format/2402.10641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Predictive Surrogate Model for Heat Transfer of an Impinging Jet on a  Concave Surface
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Salavatidezfouli%2C+S">Sajad Salavatidezfouli</a>, 
<a href="/search/math?searchtype=author&query=Rakhsha%2C+S">Saeid Rakhsha</a>, 
<a href="/search/math?searchtype=author&query=Sheidani%2C+A">Armin Sheidani</a>, 
<a href="/search/math?searchtype=author&query=Stabile%2C+G">Giovanni Stabile</a>, 
<a href="/search/math?searchtype=author&query=Rozza%2C+G">Gianluigi Rozza</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper aims to comprehensively investigate the efficacy of various Model
Order Reduction (MOR) and deep learning techniques in predicting heat transfer
in a pulsed jet impinging on a concave surface. Expanding on the previous
experimental and numerical research involving pulsed circular jets, this
investigation extends to evaluate Predictive Surrogate Models (PSM) for heat
transfer across various jet characteristics. To this end, this work introduces
two predictive approaches, one employing a Fast Fourier Transformation
augmented Artificial Neural Network (FFT-ANN) for predicting the average
Nusselt number under constant-frequency scenarios. Moreover, the investigation
introduces the Proper Orthogonal Decomposition and Long Short-Term Memory
(POD-LSTM) approach for random-frequency impingement jets. The POD-LSTM method
proves to be a robust solution for predicting the local heat transfer rate
under random-frequency impingement scenarios, capturing both the trend and
value of temporal modes. The comparison of these approaches highlights the
versatility and efficacy of advanced machine learning techniques in modelling
complex heat transfer phenomena.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10643" title="Abstract">arXiv:2402.10643</a> [<a href="/pdf/2402.10643" title="Download PDF">pdf</a>, <a href="/format/2402.10643" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> `Keep it Together&#x27;: Enforcing Cohesion in Extractive Summaries by  Simulating Human Memory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cardenas%2C+R">Ronald Cardenas</a>, 
<a href="/search/cs?searchtype=author&query=Galle%2C+M">Matthias Galle</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+S+B">Shay B. Cohen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Extractive summaries are usually presented as lists of sentences with no
expected cohesion between them. In this paper, we aim to enforce cohesion
whilst controlling for informativeness and redundancy in summaries, in cases
where the input exhibits high redundancy. The pipeline controls for redundancy
in long inputs as it is consumed, and balances informativeness and cohesion
during sentence selection. Our sentence selector simulates human memory to keep
track of topics --modeled as lexical chains--, enforcing cohesive ties between
noun phrases. Across a variety of domains, our experiments revealed that it is
possible to extract highly cohesive summaries that nevertheless read as
informative to humans as summaries extracted by only accounting for
informativeness or redundancy. The extracted summaries exhibit smooth topic
transitions between sentences as signaled by lexical chains, with chains
spanning adjacent or near-adjacent sentences.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10644" title="Abstract">arXiv:2402.10644</a> [<a href="/pdf/2402.10644" title="Download PDF">pdf</a>, <a href="/format/2402.10644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear Transformers with Learnable Kernel Functions are Better  In-Context Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aksenov%2C+Y">Yaroslav Aksenov</a>, 
<a href="/search/cs?searchtype=author&query=Balagansky%2C+N">Nikita Balagansky</a>, 
<a href="/search/cs?searchtype=author&query=Vaina%2C+S+M+L+C">Sofia Maria Lo Cicero Vaina</a>, 
<a href="/search/cs?searchtype=author&query=Shaposhnikov%2C+B">Boris Shaposhnikov</a>, 
<a href="/search/cs?searchtype=author&query=Gorbatovski%2C+A">Alexey Gorbatovski</a>, 
<a href="/search/cs?searchtype=author&query=Gavrilov%2C+D">Daniil Gavrilov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Advancing the frontier of subquadratic architectures for Language Models
(LMs) is crucial in the rapidly evolving field of natural language processing.
Current innovations, including State Space Models, were initially celebrated
for surpassing Transformer performance on language modeling tasks. However,
these models have revealed deficiencies in essential In-Context Learning
capabilities - a domain where the Transformer traditionally shines. The Based
model emerged as a hybrid solution, blending a Linear Transformer with a kernel
inspired by the Taylor expansion of exponential functions, augmented by
convolutional networks. Mirroring the Transformer's in-context adeptness, it
became a strong contender in the field. In our work, we present a singular,
elegant alteration to the Based kernel that amplifies its In-Context Learning
abilities evaluated with the Multi-Query Associative Recall task and overall
language modeling process, as demonstrated on the Pile dataset.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10645" title="Abstract">arXiv:2402.10645</a> [<a href="/pdf/2402.10645" title="Download PDF">pdf</a>, <a href="/format/2402.10645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Separators Improve Chain-of-Thought Prompting?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+Y">Yoonjeong Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+H">Hyunjin Kim</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+C">Chanyeol Choi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Junseong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Sohn%2C+J">Jy-yong Sohn</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Chain-of-thought (CoT) prompting is a simple and effective method for
improving the reasoning capabilities of Large language models (LLMs). The basic
idea of CoT is to let LLMs break down their thought processes step-by-step by
putting exemplars in the input prompt. However, the densely structured prompt
exemplars of CoT may cause the cognitive overload of LLMs. Inspired by human
cognition, we introduce CoT-Sep, a novel method that strategically employs
separators at the end of each exemplar in CoT prompting. These separators are
designed to help the LLMs understand their thought processes better while
reasoning. It turns out that CoT-Sep significantly improves the LLMs'
performances on complex reasoning tasks (e.g., GSM-8K, AQuA, CSQA), compared
with the vanilla CoT, which does not use separators. We also study the effects
of the type and the location of separators tested on multiple LLMs, including
GPT-3.5-Turbo, GPT-4, and LLaMA-2 7B. Interestingly, the type/location of
separators should be chosen appropriately to boost the reasoning capability of
CoT.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10646" title="Abstract">arXiv:2402.10646</a> [<a href="/pdf/2402.10646" title="Download PDF">pdf</a>, <a href="/format/2402.10646" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation  Tuning with Plausibility Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaowei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wei Fan</a>, 
<a href="/search/cs?searchtype=author&query=Zong%2C+Q">Qing Zong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+S">Sehyun Choi</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+T">Tianqing Fang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yangqiu Song</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+G+Y">Ginny Y. Wong</a>, 
<a href="/search/cs?searchtype=author&query=See%2C+S">Simon See</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Abstraction ability is crucial in human intelligence, which can also benefit
various tasks in NLP study. Existing work shows that LLMs are deficient in
abstract ability, and how to improve it remains unexplored. In this work, we
design the framework AbsInstruct to enhance LLMs' abstraction ability through
instruction tuning. The framework builds instructions with in-depth
explanations to assist LLMs in capturing the underlying rationale of
abstraction. Meanwhile, we introduce a plausibility estimator to select
instructions that are more consistent with the abstraction knowledge of LLMs to
be aligned. Then, our framework combines abstraction instructions with
general-purpose ones to build a hybrid dataset. Extensive experiments and
analyses demonstrate that our framework can considerably enhance LLMs'
abstraction ability with strong generalization performance while maintaining
their general instruction-following abilities.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10649" title="Abstract">arXiv:2402.10649</a> [<a href="/pdf/2402.10649" title="Download PDF">pdf</a>, <a href="/format/2402.10649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hermite Neural Network Simulation for Solving the 2D Schrodinger  Equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Parand%2C+K">Kourosh Parand</a>, 
<a href="/search/math?searchtype=author&query=Pakniyat%2C+A">Aida Pakniyat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The Schrodinger equation is a mathematical equation describing the wave
function's behavior in a quantum-mechanical system. It is a partial
differential equation that provides valuable insights into the fundamental
principles of quantum mechanics. In this paper, the aim was to solve the
Schrodinger equation with sufficient accuracy by using a mixture of neural
networks with the collocation method base Hermite functions. Initially, the
Hermite functions roots were employed as collocation points, enhancing the
efficiency of the solution. The Schrodinger equation is defined in an infinite
domain, the use of Hermite functions as activation functions resulted in
excellent precision. Finally, the proposed method was simulated using MATLAB's
Simulink tool. The results were then compared with those obtained using
Physics-informed neural networks and the presented method.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10654" title="Abstract">arXiv:2402.10654</a> [<a href="/pdf/2402.10654" title="Download PDF">pdf</a>, <a href="/format/2402.10654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Numerical Reasoning with the Guidance of Reliable Reasoning  Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dingzirui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+L">Longxu Dou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuanliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qingfu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Che%2C+W">Wanxiang Che</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Numerical reasoning is an essential ability for NLP systems to handle numeric
information. Recent research indicates that fine-tuning a small-scale model to
learn generating reasoning processes alongside answers can significantly
enhance performance. However, current methods have the limitation that most
methods generate reasoning processes with large language models (LLMs), which
are "unreliable" since such processes could contain information unrelated to
the answer. To address this limitation, we introduce Enhancing NumeriCal
reasOning with Reliable procEsses (Encore), which derives the reliable
reasoning process by decomposing the answer formula, ensuring which fully
supports the answer. Nevertheless, models could lack enough data to learn the
reasoning process generation adequately, since our method generates only one
single reasoning process for one formula. To overcome this difficulty, we
present a series of pre-training tasks to help models learn the reasoning
process generation with synthesized data. The experiments show that Encore
yields improvement on all five experimental datasets with an average of 1.8%,
proving the effectiveness of our method.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10655" title="Abstract">arXiv:2402.10655</a> [<a href="/pdf/2402.10655" title="Download PDF">pdf</a>, <a href="/format/2402.10655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An energy-based material model for the simulation of shape memory alloys  under complex boundary value problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Erdogan%2C+C">C. Erdogan</a>, 
<a href="/search/cs?searchtype=author&query=Bode%2C+T">T. Bode</a>, 
<a href="/search/cs?searchtype=author&query=Junker%2C+P">P. Junker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">Shape memory alloys are remarkable 'smart' materials used in a broad spectrum
of applications, ranging from aerospace to robotics, thanks to their unique
thermomechanical coupling capabilities. Given the complex properties of shape
memory alloys, which are largely influenced by thermal and mechanical loads, as
well as their loading history, predicting their behavior can be challenging.
Consequently, there exists a pronounced demand for an efficient material model
to simulate the behavior of these alloys. This paper introduces a material
model rooted in Hamilton's principle. The key advantages of the presented
material model encompass a more accurate depiction of the internal variable
evolution and heightened robustness. As such, the proposed material model
signifies an advancement in the realistic and efficient simulation of shape
memory alloys.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10659" title="Abstract">arXiv:2402.10659</a> [<a href="/pdf/2402.10659" title="Download PDF">pdf</a>, <a href="/format/2402.10659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Network Formation and Dynamics Among Multi-LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papachristou%2C+M">Marios Papachristou</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yuan Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Social networks influence behaviors, preferences, and relationships and play
a crucial role in the dissemination of information and norms within human
societies. As large language models (LLMs) increasingly integrate into social
and professional environments, understanding their behavior within the context
of social networks and interactions becomes essential. Our study analyzes the
behaviors of standard network structures and real-world networks to determine
whether the dynamics of multiple LLMs align with human social dynamics. We
explore various social network principles, including micro-level concepts such
as preferential attachment, triadic closure, and homophily, as well as
macro-level concepts like community structure and the small-world phenomenon.
Our findings suggest that LLMs demonstrate all these principles when they are
provided with network structures and asked about their preferences regarding
network formation. Furthermore, we investigate LLMs' decision-making based on
real-world networks to compare the strengths of these principles. Our results
reveal that triadic closure and homophily have a stronger influence than
preferential attachment and that LLMs substantially exceed random guessing in
the task of network formation predictions. Overall, our study contributes to
the development of socially aware LLMs by shedding light on LLMs' network
formation behaviors and exploring their impacts on social dynamics and norms.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10662" title="Abstract">arXiv:2402.10662</a> [<a href="/pdf/2402.10662" title="Download PDF">pdf</a>, <a href="/format/2402.10662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine Tuning Named Entity Extraction Models for the Fantasy Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sivaganeshan%2C+A">Aravinth Sivaganeshan</a>, 
<a href="/search/cs?searchtype=author&query=de+Silva%2C+N">Nisansa de Silva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Named Entity Recognition (NER) is a sequence classification Natural Language
Processing task where entities are identified in the text and classified into
predefined categories. It acts as a foundation for most information extraction
systems. Dungeons and Dragons (D&amp;D) is an open-ended tabletop fantasy game with
its own diverse lore. DnD entities are domain-specific and are thus
unrecognizable by even the state-of-the-art off-the-shelf NER systems as the
NER systems are trained on general data for pre-defined categories such as:
person (PERS), location (LOC), organization (ORG), and miscellaneous (MISC).
For meaningful extraction of information from fantasy text, the entities need
to be classified into domain-specific entity categories as well as the models
be fine-tuned on a domain-relevant corpus. This work uses available lore of
monsters in the D&amp;D domain to fine-tune Trankit, which is a prolific NER
framework that uses a pre-trained model for NER. Upon this training, the system
acquires the ability to extract monster names from relevant domain documents
under a novel NER tag. This work compares the accuracy of the monster name
identification against; the zero-shot Trankit model and two FLAIR models. The
fine-tuned Trankit model achieves an 87.86% F1 score surpassing all the other
considered models.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10663" title="Abstract">arXiv:2402.10663</a> [<a href="/pdf/2402.10663" title="Download PDF">pdf</a>, <a href="/format/2402.10663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Demonstration Diversity by Human-Free Fusing for Text-to-SQL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dingzirui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+L">Longxu Dou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuanliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qingfu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Che%2C+W">Wanxiang Che</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Currently, the in-context learning method based on large language models
(LLMs) has become the mainstream of text-to-SQL research. Previous works have
discussed how to select demonstrations related to the user question from a
human-labeled demonstration pool. However, human labeling suffers from the
limitations of insufficient diversity and high labeling overhead. Therefore, in
this paper, we discuss how to measure and improve the diversity of the
demonstrations for text-to-SQL. We present a metric to measure the diversity of
the demonstrations and analyze the insufficient of the existing labeled data by
experiments. Based on the above discovery, we propose fusing iteratively for
demonstrations (Fused) to build a high-diversity demonstration pool through
human-free multiple-iteration synthesis, improving diversity and lowering label
cost. Our method achieves an average improvement of 3.2% and 5.0% with and
without human labeling on several mainstream datasets, which proves the
effectiveness of Fused.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10664" title="Abstract">arXiv:2402.10664</a> [<a href="/pdf/2402.10664" title="Download PDF">pdf</a>, <a href="/format/2402.10664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative AI and Attentive User Interfaces: Five Strategies to Enhance  Take-Over Quality in Automated Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ebel%2C+P">Patrick Ebel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> MUM 2023 Workshop on Interruptions and Attention Management: Exploring the Potential of Generative AI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">As the automotive world moves toward higher levels of driving automation,
Level 3 automated driving represents a critical juncture. In Level 3 driving,
vehicles can drive alone under limited conditions, but drivers are expected to
be ready to take over when the system requests. Assisting the driver to
maintain an appropriate level of Situation Awareness (SA) in such contexts
becomes a critical task. This position paper explores the potential of
Attentive User Interfaces (AUIs) powered by generative Artificial Intelligence
(AI) to address this need. Rather than relying on overt notifications, we argue
that AUIs based on novel AI technologies such as large language models or
diffusion models can be used to improve SA in an unconscious and subtle way
without negative effects on drivers overall workload. Accordingly, we propose 5
strategies how generative AI s can be used to improve the quality of takeovers
and, ultimately, road safety.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10665" title="Abstract">arXiv:2402.10665</a> [<a href="/pdf/2402.10665" title="Download PDF">pdf</a>, <a href="/format/2402.10665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Selective Prediction for Semantic Segmentation using Post-Hoc Confidence  Estimation and Its Performance under Distribution Shift
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Borges%2C+B+L+C">Bruno Laboissiere Camargos Borges</a>, 
<a href="/search/cs?searchtype=author&query=Pacheco%2C+B+M">Bruno Machado Pacheco</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+D">Danilo Silva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Semantic segmentation plays a crucial role in various computer vision
applications, yet its efficacy is often hindered by the lack of high-quality
labeled data. To address this challenge, a common strategy is to leverage
models trained on data from different populations, such as publicly available
datasets. This approach, however, leads to the distribution shift problem,
presenting a reduced performance on the population of interest. In scenarios
where model errors can have significant consequences, selective prediction
methods offer a means to mitigate risks and reduce reliance on expert
supervision. This paper investigates selective prediction for semantic
segmentation in low-resource settings, thus focusing on post-hoc confidence
estimators applied to pre-trained models operating under distribution shift. We
propose a novel image-level confidence measure tailored for semantic
segmentation and demonstrate its effectiveness through experiments on three
medical imaging tasks. Our findings show that post-hoc confidence estimators
offer a cost-effective approach to reducing the impacts of distribution shift.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10666" title="Abstract">arXiv:2402.10666</a> [<a href="/pdf/2402.10666" title="Download PDF">pdf</a>, <a href="/format/2402.10666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Hop Table Retrieval for Open-Domain Text-to-SQL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuanliang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dingzirui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+L">Longxu Dou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qingfu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Che%2C+W">Wanxiang Che</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Open-domain text-to-SQL is an important task that retrieves question-relevant
tables from massive databases and then generates SQL. However, existing
retrieval methods that retrieve in a single hop do not pay attention to the
text-to-SQL challenge of schema linking, which is aligning the entities in the
question with table entities, reflected in two aspects: similar irrelevant
entity and domain mismatch entity. Therefore, we propose our method, the
multi-hop table retrieval with rewrite and beam search (Murre). To reduce the
effect of the similar irrelevant entity, our method focuses on unretrieved
entities at each hop and considers the low-ranked tables by beam search. To
alleviate the limitation of domain mismatch entity, Murre rewrites the question
based on retrieved tables in multiple hops, decreasing the domain gap with
relevant tables. We conduct experiments on SpiderUnion and BirdUnion+, reaching
new state-of-the-art results with an average improvement of 6.38%.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10667" title="Abstract">arXiv:2402.10667</a> [<a href="/pdf/2402.10667" title="Download PDF">pdf</a>, <a href="/ps/2402.10667" title="Download PostScript">ps</a>, <a href="/format/2402.10667" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Binary linear codes with a fixed point free permutation automorphism of  order three
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aksu%2C+F+A">Fatma Altunbulak Aksu</a>, 
<a href="/search/cs?searchtype=author&query=Hafezieh%2C+R">Roghayeh Hafezieh</a>, 
<a href="/search/cs?searchtype=author&query=Tuvay%2C+%C4%B0">&#x130;pek Tuvay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">We investigate the structural properties of binary linear codes whose
permutation automorphism group has a fixed point free automorphism of order
$3$. We prove that up to dimension or codimension $4$, there is no binary
linear code whose permutation automorphism group is generated by a fixed point
free permutation of order $3$. We also prove that there is no binary
$5$-dimensional linear code whose length is at least $30$ and whose permutation
automorphism group is generated by a fixed point free permutation of order $3$.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10668" title="Abstract">arXiv:2402.10668</a> [<a href="/pdf/2402.10668" title="Download PDF">pdf</a>, <a href="/format/2402.10668" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Driven Abstractions for Control Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Coppola%2C+R">Rudi Coppola</a>, 
<a href="/search/eess?searchtype=author&query=Peruffo%2C+A">Andrea Peruffo</a>, 
<a href="/search/eess?searchtype=author&query=Mazo%2C+M">Manuel Mazo Jr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">At the intersection of dynamical systems, control theory, and formal methods
lies the construction of symbolic abstractions: these typically represent
simpler, finite-state models whose behaviour mimics the one of an underlying
concrete system but are easier to analyse. Building an abstraction usually
requires an accurate knowledge of the underlying model: this knowledge may be
costly to gather, especially in real-life applications. We aim to bridge this
gap by building abstractions based on sampling finite length trajectories.
Adding the controller degrees of freedom, we newly define the notion of
probabilistic alternating simulation, and provide probably approximately
correct (PAC) guarantees that the constructed abstraction includes all
behaviours of the concrete system and that it is suitable for control design,
for arbitrarily long time horizons, leveraging the scenario theory. Our method
is then tested on several numerical benchmarks.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10669" title="Abstract">arXiv:2402.10669</a> [<a href="/pdf/2402.10669" title="Download PDF">pdf</a>, <a href="/format/2402.10669" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Humans or LLMs as the Judge? A Study on Judgement Biases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+G+H">Guiming Hardy Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shunian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziche Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+F">Feng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Benyou Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Adopting human and large language models (LLM) as judges (\textit{a.k.a}
human- and LLM-as-a-judge) for evaluating the performance of existing LLMs has
recently gained attention. Nonetheless, this approach concurrently introduces
potential biases from human and LLM judges, questioning the reliability of the
evaluation results. In this paper, we propose a novel framework for
investigating 5 types of biases for LLM and human judges. We curate a dataset
with 142 samples referring to the revised Bloom's Taxonomy and conduct
thousands of human and LLM evaluations. Results show that human and LLM judges
are vulnerable to perturbations to various degrees, and that even the most
cutting-edge judges possess considerable biases. We further exploit their
weakness and conduct attacks on LLM judges. We hope that our work can notify
the community of the vulnerability of human- and LLM-as-a-judge against
perturbations, as well as the urgency of developing robust evaluation systems.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10670" title="Abstract">arXiv:2402.10670</a> [<a href="/pdf/2402.10670" title="Download PDF">pdf</a>, <a href="/format/2402.10670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via  Vision-Language Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuang%2C+Y">Yuxuan Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Hai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Meng Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First submitted on Dec. 15, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Object navigation (ObjectNav) requires an agent to navigate through unseen
environments to find queried objects. Many previous methods attempted to solve
this task by relying on supervised or reinforcement learning, where they are
trained on limited household datasets with close-set objects. However, two key
challenges are unsolved: understanding free-form natural language instructions
that demand open-set objects, and generalizing to new environments in a
zero-shot manner. Aiming to solve the two challenges, in this paper, we propose
OpenFMNav, an Open-set Foundation Model based framework for zero-shot object
Navigation. We first unleash the reasoning abilities of large language models
(LLMs) to extract proposed objects from natural language instructions that meet
the user's demand. We then leverage the generalizability of large vision
language models (VLMs) to actively discover and detect candidate objects from
the scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting
common sense reasoning on VSSM, our method can perform effective
language-guided exploration and exploitation of the scene and finally reach the
goal. By leveraging the reasoning and generalizing abilities of foundation
models, our method can understand free-form human instructions and perform
effective open-set zero-shot navigation in diverse environments. Extensive
experiments on the HM3D ObjectNav benchmark show that our method surpasses all
the strong baselines on all metrics, proving our method's effectiveness.
Furthermore, we perform real robot demonstrations to validate our method's
open-set-ness and generalizability to real-world environments.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10671" title="Abstract">arXiv:2402.10671</a> [<a href="/pdf/2402.10671" title="Download PDF">pdf</a>, <a href="/format/2402.10671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decomposition for Enhancing Attention: Improving LLM-based Text-to-SQL  through Workflow Paradigm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yuanzhen Xie</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+X">Xinzhou Jin</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+T">Tao Xie</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+M">MingXiong Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chenyun Yu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Lei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhuo%2C+C">ChengXiang Zhuo</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+B">Bo Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In-context learning of large-language models (LLMs) has achieved remarkable
success in the field of natural language processing, while extensive case
studies reveal that the single-step chain-of-thought prompting approach faces
challenges such as attention diffusion and inadequate performance in complex
tasks like text-to-SQL. To improve the contextual learning capabilities of LLMs
in text-to-SQL, a workflow paradigm method is proposed, aiming to enhance the
attention and problem-solving scope of LLMs through decomposition.
Specifically, the information determination module for eliminating redundant
information and the brand-new prompt structure based on problem classification
greatly enhance the model's attention. Additionally, the inclusion of
self-correcting and active learning modules greatly expands the problem-solving
scope of LLMs, hence improving the upper limit of LLM-based approaches.
Extensive experiments conducted on three datasets demonstrate that our approach
outperforms other methods by a significant margin. About 2-3 percentage point
improvements compared to the existing baseline on the Spider Dev and
Spider-Realistic datasets and new SOTA results on the Spider Test dataset are
achieved. Our code is available on GitHub:
\url{https://github.com/FlyingFeather/DEA-SQL}.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10675" title="Abstract">arXiv:2402.10675</a> [<a href="/pdf/2402.10675" title="Download PDF">pdf</a>, <a href="/format/2402.10675" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> German Text Simplification: Finetuning Large Language Models with  Semi-Synthetic Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kl%C3%B6ser%2C+L">Lars Kl&#xf6;ser</a>, 
<a href="/search/cs?searchtype=author&query=Beele%2C+M">Mika Beele</a>, 
<a href="/search/cs?searchtype=author&query=Schagen%2C+J">Jan-Niklas Schagen</a>, 
<a href="/search/cs?searchtype=author&query=Kraft%2C+B">Bodo Kraft</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at Fourth Workshop on Language Technology for Equality, Diversity, Inclusion - EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This study pioneers the use of synthetically generated data for training
generative models in document-level text simplification of German texts. We
demonstrate the effectiveness of our approach with real-world online texts.
Addressing the challenge of data scarcity in language simplification, we
crawled professionally simplified German texts and synthesized a corpus using
GPT-4. We finetune Large Language Models with up to 13 billion parameters on
this data and evaluate their performance. This paper employs various
methodologies for evaluation and demonstrates the limitations of currently used
rule-based metrics. Both automatic and manual evaluations reveal that our
models can significantly simplify real-world online texts, indicating the
potential of synthetic data in improving text simplification.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10681" title="Abstract">arXiv:2402.10681</a> [<a href="/pdf/2402.10681" title="Download PDF">pdf</a>, <a href="/format/2402.10681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-informed MeshGraphNets (PI-MGNs): Neural finite element solvers  for non-stationary and nonlinear simulations on arbitrary meshes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=W%C3%BCrth%2C+T">Tobias W&#xfc;rth</a>, 
<a href="/search/cs?searchtype=author&query=Freymuth%2C+N">Niklas Freymuth</a>, 
<a href="/search/cs?searchtype=author&query=Zimmerling%2C+C">Clemens Zimmerling</a>, 
<a href="/search/cs?searchtype=author&query=Neumann%2C+G">Gerhard Neumann</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%A4rger%2C+L">Luise K&#xe4;rger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to CMAME
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">Engineering components must meet increasing technological demands in ever
shorter development cycles. To face these challenges, a holistic approach is
essential that allows for the concurrent development of part design, material
system and manufacturing process. Current approaches employ numerical
simulations, which however quickly becomes computation-intensive, especially
for iterative optimization. Data-driven machine learning methods can be used to
replace time- and resource-intensive numerical simulations. In particular,
MeshGraphNets (MGNs) have shown promising results. They enable fast and
accurate predictions on unseen mesh geometries while being fully differentiable
for optimization. However, these models rely on large amounts of expensive
training data, such as numerical simulations. Physics-informed neural networks
(PINNs) offer an opportunity to train neural networks with partial differential
equations instead of labeled data, but have not been extended yet to handle
time-dependent simulations of arbitrary meshes. This work introduces PI-MGNs, a
hybrid approach that combines PINNs and MGNs to quickly and accurately solve
non-stationary and nonlinear partial differential equations (PDEs) on arbitrary
meshes. The method is exemplified for thermal process simulations of unseen
parts with inhomogeneous material distribution. Further results show that the
model scales well to large and complex meshes, although it is trained on small
generic meshes only.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10684" title="Abstract">arXiv:2402.10684</a> [<a href="/pdf/2402.10684" title="Download PDF">pdf</a>, <a href="/format/2402.10684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language-Driven Engineering An Interdisciplinary Software Development  Paradigm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Steffen%2C+B">Bernhard Steffen</a>, 
<a href="/search/cs?searchtype=author&query=Margaria%2C+T">Tiziana Margaria</a>, 
<a href="/search/cs?searchtype=author&query=Bainczyk%2C+A">Alexander Bainczyk</a>, 
<a href="/search/cs?searchtype=author&query=Bo%C3%9Felmann%2C+S">Steve Bo&#xdf;elmann</a>, 
<a href="/search/cs?searchtype=author&query=Busch%2C+D">Daniel Busch</a>, 
<a href="/search/cs?searchtype=author&query=Driessen%2C+M">Marc Driessen</a>, 
<a href="/search/cs?searchtype=author&query=Frohme%2C+M">Markus Frohme</a>, 
<a href="/search/cs?searchtype=author&query=Howar%2C+F">Falk Howar</a>, 
<a href="/search/cs?searchtype=author&query=J%C3%B6rges%2C+S">Sven J&#xf6;rges</a>, 
<a href="/search/cs?searchtype=author&query=Krause%2C+M">Marvin Krause</a>, 
<a href="/search/cs?searchtype=author&query=Krumrey%2C+M">Marco Krumrey</a>, 
<a href="/search/cs?searchtype=author&query=Lamprecht%2C+A">Anna-Lena Lamprecht</a>, 
<a href="/search/cs?searchtype=author&query=Lybecait%2C+M">Michael Lybecait</a>, 
<a href="/search/cs?searchtype=author&query=Murtovi%2C+A">Alnis Murtovi</a>, 
<a href="/search/cs?searchtype=author&query=Naujokat%2C+S">Stefan Naujokat</a>, 
<a href="/search/cs?searchtype=author&query=Neubauer%2C+J">Johannes Neubauer</a>, 
<a href="/search/cs?searchtype=author&query=Schieweck%2C+A">Alexander Schieweck</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%BCrmann%2C+J">Jonas Sch&#xfc;rmann</a>, 
<a href="/search/cs?searchtype=author&query=Smyth%2C+S">Steven Smyth</a>, 
<a href="/search/cs?searchtype=author&query=Steffen%2C+B">Barbara Steffen</a>, 
<a href="/search/cs?searchtype=author&query=Storek%2C+F">Fabian Storek</a>, 
<a href="/search/cs?searchtype=author&query=Tegeler%2C+T">Tim Tegeler</a>, 
<a href="/search/cs?searchtype=author&query=Teumert%2C+S">Sebastian Teumert</a>, 
<a href="/search/cs?searchtype=author&query=Wirkner%2C+D">Dominic Wirkner</a>, 
<a href="/search/cs?searchtype=author&query=Zweihoff%2C+P">Philip Zweihoff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages, 30 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Programming Languages (cs.PL)

</div>
<p class="mathjax">We illustrate how purpose-specific, graphical modeling enables application
experts with different levels of expertise to collaboratively design and then
produce complex applications using their individual, purpose-specific modeling
language. Our illustration includes seven graphical Integrated Modeling
Environments (IMEs) that support full code generation, as well as four
browser-based applications that were modeled and then fully automatically
generated and produced using DIME, our most complex graphical IME. While the
seven IMEs were chosen to illustrate the types of languages we support with our
Language-Driven Engineering (LDE) approach, the four DIME products were chosen
to give an impression of the power of our LDE-generated IMEs. In fact,
Equinocs, Springer Nature's future editorial system for proceedings, is also
being fully automatically generated and then deployed at their Dordrecht site
using a deployment pipeline generated with Rig, one of the IMEs presented. Our
technology is open source and the products presented are currently in use.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10685" title="Abstract">arXiv:2402.10685</a> [<a href="/pdf/2402.10685" title="Download PDF">pdf</a>, <a href="/format/2402.10685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LongHeads: Multi-Head Attention is Secretly a Long Context Processor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yi Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+W">Wei He</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+T">Tao Ji</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) have achieved impressive performance in numerous
domains but often struggle to process lengthy inputs effectively and
efficiently due to limited length generalization and attention's quadratic
computational demands. Many sought to mitigate this by restricting the
attention window within the pre-trained length. However, these methods
introduce new issues such as ignoring the middle context and requiring
additional training. To address these problems, we propose LongHeads, a
training-free framework that enhances LLM's long context ability by unlocking
multi-head attention's untapped potential. Instead of allowing each head to
attend to the full sentence, which struggles with generalizing to longer
sequences due to out-of-distribution (OOD) issues, we allow each head to
process in-distribution length by selecting and attending to important context
chunks. To this end, we propose a chunk selection strategy that relies on the
inherent correlation between the query and the key representations, efficiently
distributing context chunks to different heads. In this way, each head ensures
it can effectively process attended tokens within the trained length, while
different heads in different layers can collectively process longer contexts.
LongHeads works efficiently in linear time, fits seamlessly with many LLMs that
use relative positional encoding. Our extensive empirical analyses verify
LongHeads's efficacy in extending the usable context window for existing
models, showcasing its promise for enhancing long text understanding.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10686" title="Abstract">arXiv:2402.10686</a> [<a href="/pdf/2402.10686" title="Download PDF">pdf</a>, <a href="/format/2402.10686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty, Calibration, and Membership Inference Attacks: An  Information-Theoretic Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Meiyi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+C">Caili Guo</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Chunyan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Simeone%2C+O">Osvaldo Simeone</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">In a membership inference attack (MIA), an attacker exploits the
overconfidence exhibited by typical machine learning models to determine
whether a specific data point was used to train a target model. In this paper,
we analyze the performance of the state-of-the-art likelihood ratio attack
(LiRA) within an information-theoretical framework that allows the
investigation of the impact of the aleatoric uncertainty in the true data
generation process, of the epistemic uncertainty caused by a limited training
data set, and of the calibration level of the target model. We compare three
different settings, in which the attacker receives decreasingly informative
feedback from the target model: confidence vector (CV) disclosure, in which the
output probability vector is released; true label confidence (TLC) disclosure,
in which only the probability assigned to the true label is made available by
the model; and decision set (DS) disclosure, in which an adaptive prediction
set is produced as in conformal prediction. We derive bounds on the advantage
of an MIA adversary with the aim of offering insights into the impact of
uncertainty and calibration on the effectiveness of MIAs. Simulation results
demonstrate that the derived analytical bounds predict well the effectiveness
of MIAs.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10688" title="Abstract">arXiv:2402.10688</a> [<a href="/pdf/2402.10688" title="Download PDF">pdf</a>, <a href="/format/2402.10688" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Opening the Black Box of Large Language Models: Two Views on Holistic  Interpretability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haiyan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lakkaraju%2C+H">Himabindu Lakkaraju</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+M">Mengnan Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">As large language models (LLMs) grow more powerful, concerns around potential
harms like toxicity, unfairness, and hallucination threaten user trust.
Ensuring beneficial alignment of LLMs with human values through model alignment
is thus critical yet challenging, requiring a deeper understanding of LLM
behaviors and mechanisms. We propose opening the black box of LLMs through a
framework of holistic interpretability encompassing complementary bottom-up and
top-down perspectives. The bottom-up view, enabled by mechanistic
interpretability, focuses on component functionalities and training dynamics.
The top-down view utilizes representation engineering to analyze behaviors
through hidden representations. In this paper, we review the landscape around
mechanistic interpretability and representation engineering, summarizing
approaches, discussing limitations and applications, and outlining future
challenges in using these techniques to achieve ethical, honest, and reliable
reasoning aligned with human values.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10689" title="Abstract">arXiv:2402.10689</a> [<a href="/pdf/2402.10689" title="Download PDF">pdf</a>, <a href="/format/2402.10689" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Cultural Commonsense Knowledge Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Tuan-Phong Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Razniewski%2C+S">Simon Razniewski</a>, 
<a href="/search/cs?searchtype=author&query=Weikum%2C+G">Gerhard Weikum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 5 figures, 13 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Despite recent progress, large language models (LLMs) still face the
challenge of appropriately reacting to the intricacies of social and cultural
conventions. This paper presents MANGO, a methodology for distilling
high-accuracy, high-recall assertions of cultural knowledge. We judiciously and
iteratively prompt LLMs for this purpose from two entry points, concepts and
cultures. Outputs are consolidated via clustering and generative summarization.
Running the MANGO method with GPT-3.5 as underlying LLM yields 167K
high-accuracy assertions for 30K concepts and 11K cultures, surpassing prior
resources by a large margin. For extrinsic evaluation, we explore augmenting
dialogue systems with cultural knowledge assertions. We find that adding
knowledge from MANGO improves the overall quality, specificity, and cultural
sensitivity of dialogue responses, as judged by human annotators. Data and code
are available for download.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10691" title="Abstract">arXiv:2402.10691</a> [<a href="/pdf/2402.10691" title="Download PDF">pdf</a>, <a href="/format/2402.10691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MultiPoT: Multilingual Program of Thoughts Harnesses Multiple  Programming Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xianzhen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qingfu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+L">Libo Qin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qing Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Dongliang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Che%2C+W">Wanxiang Che</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Program of Thoughts (PoT) is an approach characterized by its executable
intermediate steps, which ensure the accuracy of the numerical calculations in
the reasoning process. Currently, PoT primarily uses Python. However, relying
solely on a single language may result in suboptimal solutions and overlook the
potential benefits of other programming languages. In this paper, we conduct
comprehensive experiments on the programming languages used in PoT and find
that no single language consistently delivers optimal performance across all
tasks and models. The effectiveness of each language varies depending on the
specific scenarios. Inspired by this, we propose a task and model agnostic
approach called MultiPoT, which harnesses strength and diversity from various
languages. Experimental results reveal that it significantly outperforms Python
Self-Consistency. Furthermore, it achieves comparable or superior performance
compared to the best monolingual PoT in almost all tasks across all models. In
particular, MultiPoT achieves more than 4.6\% improvement on average on both
Starcoder and ChatGPT (gpt-3.5-turbo).
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10693" title="Abstract">arXiv:2402.10693</a> [<a href="/pdf/2402.10693" title="Download PDF">pdf</a>, <a href="/format/2402.10693" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Precision and Recall to assess the quality and diversity of  LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Florian%2C+L+B">Le Bronnec Florian</a>, 
<a href="/search/cs?searchtype=author&query=Alexandre%2C+V">Verine Alexandre</a>, 
<a href="/search/cs?searchtype=author&query=Benjamin%2C+N">Negrevergne Benjamin</a>, 
<a href="/search/cs?searchtype=author&query=Yann%2C+C">Chevaleyre Yann</a>, 
<a href="/search/cs?searchtype=author&query=Alexandre%2C+A">Allauzen Alexandre</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 15 figures, Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper introduces a novel evaluation framework for Large Language Models
(LLMs) such as Llama-2 and Mistral, focusing on the adaptation of Precision and
Recall metrics from image generation to text generation. This approach allows
for a nuanced assessment of the quality and diversity of generated text without
the need for aligned corpora. By conducting a comprehensive evaluation of
state-of-the-art language models, the study reveals significant insights into
their performance on open-ended generation tasks, which are not adequately
captured by traditional benchmarks. The findings highlight a trade-off between
the quality and diversity of generated samples, particularly when models are
fine-tuned with human feedback. This work extends the toolkit for
distribution-based NLP evaluation, offering insights into the practical
capabilities and challenges faced by current LLMs in generating diverse and
high-quality text.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10695" title="Abstract">arXiv:2402.10695</a> [<a href="/pdf/2402.10695" title="Download PDF">pdf</a>, <a href="/format/2402.10695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlink to Unlearn: Simplifying Edge Unlearning in GNNs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+J">Jiajun Tan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+F">Fei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+R">Ruichen Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+D">Du Su</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Huawei Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">As concerns over data privacy intensify, unlearning in Graph Neural Networks
(GNNs) has emerged as a prominent research frontier in academia. This concept
is pivotal in enforcing the right to be forgotten, which entails the selective
removal of specific data from trained GNNs upon user request. Our research
focuses on edge unlearning, a process of particular relevance to real-world
applications, owing to its widespread applicability. Current state-of-the-art
approaches like GNNDelete can eliminate the influence of specific edges, yet
our research has revealed a critical limitation in these approaches, termed
over-forgetting. It occurs when the unlearning process inadvertently removes
excessive information beyond specific data, leading to a significant decline in
prediction accuracy for the remaining edges. To address this issue, we have
identified the loss functions of GNNDelete as the primary source of the
over-forgetting phenomenon. Furthermore, our analysis also suggests that loss
functions may not be essential for effective edge unlearning. Building on these
insights, we have simplified GNNDelete to develop Unlink-to-Unlearn (UtU), a
novel method that facilitates unlearning exclusively through unlinking the
forget edges from graph structure. Our extensive experiments demonstrate that
UtU delivers privacy protection on par with that of a retrained model while
preserving high accuracy in downstream tasks. Specifically, UtU upholds over
97.3% of the retrained model's privacy protection capabilities and 99.8% of its
link prediction accuracy. Meanwhile, UtU requires only constant computational
demands, underscoring its advantage as a highly lightweight and practical edge
unlearning solution.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10698" title="Abstract">arXiv:2402.10698</a> [<a href="/pdf/2402.10698" title="Download PDF">pdf</a>, <a href="/format/2402.10698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Question-Instructed Visual Descriptions for Zero-Shot Video Question  Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Romero%2C+D">David Romero</a>, 
<a href="/search/cs?searchtype=author&query=Solorio%2C+T">Thamar Solorio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present Q-ViD, a simple approach for video question answering (video QA),
that unlike prior methods, which are based on complex architectures,
computationally expensive pipelines or use closed models like GPTs, Q-ViD
relies on a single instruction-aware open vision-language model (InstructBLIP)
to tackle videoQA using frame descriptions. Specifically, we create captioning
instruction prompts that rely on the target questions about the videos and
leverage InstructBLIP to obtain video frame captions that are useful to the
task at hand. Subsequently, we form descriptions of the whole video using the
question-dependent frame captions, and feed that information, along with a
question-answering prompt, to a large language model (LLM). The LLM is our
reasoning module, and performs the final step of multiple-choice QA. Our simple
Q-ViD framework achieves competitive or even higher performances than current
state of the art models on a diverse range of videoQA benchmarks, including
NExT-QA, STAR, How2QA, TVQA and IntentQA.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10699" title="Abstract">arXiv:2402.10699</a> [<a href="/pdf/2402.10699" title="Download PDF">pdf</a>, <a href="/format/2402.10699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Human-like Translation Strategy: Integrating Drift-Diffusion  Model with Large Language Models for Machine Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Na%2C+H">Hongbin Na</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zimu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Maimaiti%2C+M">Mieradilijiang Maimaiti</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+T">Tao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Ling Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated promising potential in various
downstream tasks, including machine translation. However, prior work on
LLM-based machine translation has mainly focused on better utilizing training
data, demonstrations, or pre-defined and universal knowledge to improve
performance, with a lack of consideration of decision-making like human
translators. In this paper, we incorporate Thinker with the Drift-Diffusion
Model (Thinker-DDM) to address this issue. We then redefine the Drift-Diffusion
process to emulate human translators' dynamic decision-making under constrained
resources. We conduct extensive experiments under the high-resource,
low-resource, and commonsense translation settings using the WMT22 and CommonMT
datasets, in which Thinker-DDM outperforms baselines in the first two
scenarios. We also perform additional analysis and evaluation on commonsense
translation to illustrate the high effectiveness and efficacy of the proposed
method.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10701" title="Abstract">arXiv:2402.10701</a> [<a href="/pdf/2402.10701" title="Download PDF">pdf</a>, <a href="/format/2402.10701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Does Twinning Vehicular Networks Enhance Their Performance in Dense  Areas?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Al-Shareeda%2C+S">Sarah Al-Shareeda</a>, 
<a href="/search/cs?searchtype=author&query=Oktug%2C+S+F">Sema F. Oktug</a>, 
<a href="/search/cs?searchtype=author&query=Yaslan%2C+Y">Yusuf Yaslan</a>, 
<a href="/search/cs?searchtype=author&query=Yurdakul%2C+G">Gokhan Yurdakul</a>, 
<a href="/search/cs?searchtype=author&query=Canberk%2C+B">Berk Canberk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 8 figures, 2tables, conference paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper investigates the potential of Digital Twins (DTs) to enhance
network performance in densely populated urban areas, specifically focusing on
vehicular networks. The study comprises two phases. In Phase I, we utilize
traffic data and AI clustering to identify critical locations, particularly in
crowded urban areas with high accident rates. In Phase II, we evaluate the
advantages of twinning vehicular networks through three deployment scenarios:
edge-based twin, cloud-based twin, and hybrid-based twin. Our analysis
demonstrates that twinning significantly reduces network delays, with virtual
twins outperforming physical networks. Virtual twins maintain low delays even
with increased vehicle density, such as 15.05 seconds for 300 vehicles.
Moreover, they exhibit faster computational speeds, with cloud-based twins
being 1.7 times faster than edge twins in certain scenarios. These findings
provide insights for efficient vehicular communication and underscore the
potential of virtual twins in enhancing vehicular networks in crowded areas
while emphasizing the importance of considering real-world factors when making
deployment decisions.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10705" title="Abstract">arXiv:2402.10705</a> [<a href="/pdf/2402.10705" title="Download PDF">pdf</a>, <a href="/format/2402.10705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoSAT: Automatically Optimize SAT Solvers via Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yiwen Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xianyin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shiyu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+S">Shaowei Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bing-Zhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+K">Ke Wei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Heuristics are crucial in SAT solvers, while no heuristic rules are suitable
for all problem instances. Therefore, it typically requires to refine specific
solvers for specific problem instances. In this context, we present AutoSAT, a
novel framework for automatically optimizing heuristics in SAT solvers. AutoSAT
is based on Large Large Models (LLMs) which is able to autonomously generate
code, conduct evaluation, then utilize the feedback to further optimize
heuristics, thereby reducing human intervention and enhancing solver
capabilities. AutoSAT operates on a plug-and-play basis, eliminating the need
for extensive preliminary setup and model training, and fosters a Chain of
Thought collaborative process with fault-tolerance, ensuring robust heuristic
optimization. Extensive experiments on a Conflict-Driven Clause Learning (CDCL)
solver demonstrates the overall superior performance of AutoSAT, especially in
solving some specific SAT problem instances.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10711" title="Abstract">arXiv:2402.10711</a> [<a href="/pdf/2402.10711" title="Download PDF">pdf</a>, <a href="/format/2402.10711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StableLego: Stability Analysis of Block Stacking Assembly
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Ruixuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+K">Kangle Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Changliu Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Recent advancements in robotics enable robots to accomplish complex assembly
tasks. However, designing an assembly requires a non-trivial effort since a
slight variation in the design could significantly affect the task feasibility.
It is critical to ensure the physical feasibility of the assembly design so
that the assembly task can be successfully executed. To address the challenge,
this paper studies the physical stability of assembly structures, in
particular, block stacking assembly, where people use cubic blocks to build 3D
structures (e.g., Lego constructions). The paper proposes a new optimization
formulation, which optimizes over force balancing equations, for inferring the
structural stability of 3D block-stacking structures. The proposed stability
analysis is tested and verified on hand-crafted Lego examples. The experiment
results demonstrate that the proposed stability analysis can correctly predict
whether the structure is stable. In addition, it outperforms the existing
methods since it can locate the weakest parts in the design, and more
importantly, solve any given assembly structure. To further validate the
proposed analysis formulation, we provide StableLego: a comprehensive dataset
including more than 50k 3D objects with their Lego layouts. We test the
proposed stability analysis and include the stability inference for each
corresponding object in StableLego. Our code and the dataset are available at
https://github.com/intelligent-control-lab/StableLego.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10712" title="Abstract">arXiv:2402.10712</a> [<a href="/pdf/2402.10712" title="Download PDF">pdf</a>, <a href="/format/2402.10712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient  Generative LLM Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yamaguchi%2C+A">Atsuki Yamaguchi</a>, 
<a href="/search/cs?searchtype=author&query=Villavicencio%2C+A">Aline Villavicencio</a>, 
<a href="/search/cs?searchtype=author&query=Aletras%2C+N">Nikolaos Aletras</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The development of state-of-the-art generative large language models (LLMs)
disproportionately relies on English-centric tokenizers, vocabulary and
pre-training data. Despite the fact that some LLMs have multilingual
capabilities, recent studies have shown that their inference efficiency
deteriorates when generating text in languages other than English. This results
in increased inference time and costs. Cross-lingual vocabulary adaptation
methods have been proposed for adapting models to a target language aiming to
improve downstream performance. However, the effectiveness of these methods on
increasing inference efficiency of generative LLMs has yet to be explored. In
this paper, we perform an empirical study of various cross-lingual vocabulary
adaptation methods on five generative LLMs (including monolingual and
multilingual models) across four typologically-diverse languages and four
natural language understanding tasks. We find that cross-lingual vocabulary
adaptation substantially contributes to LLM inference speedups of up to 271.5%.
We also show that adapting LLMs that have been pre-trained on more balanced
multilingual data results in downstream performance comparable to the original
models.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10717" title="Abstract">arXiv:2402.10717</a> [<a href="/pdf/2402.10717" title="Download PDF">pdf</a>, <a href="/format/2402.10717" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BioFusionNet: Deep Learning-Based Survival Risk Stratification in ER+  Breast Cancer Through Multifeature and Multimodal Data Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mondol%2C+R+K">Raktim Kumar Mondol</a>, 
<a href="/search/cs?searchtype=author&query=Millar%2C+E+K+A">Ewan K.A. Millar</a>, 
<a href="/search/cs?searchtype=author&query=Sowmya%2C+A">Arcot Sowmya</a>, 
<a href="/search/cs?searchtype=author&query=Meijering%2C+E">Erik Meijering</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Keywords: Multimodal Fusion, Breast Cancer, Whole Slide Images, Deep Neural Network, Survival Prediction
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Breast cancer is a significant health concern affecting millions of women
worldwide. Accurate survival risk stratification plays a crucial role in
guiding personalised treatment decisions and improving patient outcomes. Here
we present BioFusionNet, a deep learning framework that fuses image-derived
features with genetic and clinical data to achieve a holistic patient profile
and perform survival risk stratification of ER+ breast cancer patients. We
employ multiple self-supervised feature extractors, namely DINO and MoCoV3,
pretrained on histopathology patches to capture detailed histopathological
image features. We then utilise a variational autoencoder (VAE) to fuse these
features, and harness the latent space of the VAE to feed into a self-attention
network, generating patient-level features. Next, we develop a
co-dual-cross-attention mechanism to combine the histopathological features
with genetic data, enabling the model to capture the interplay between them.
Additionally, clinical data is incorporated using a feed-forward network (FFN),
further enhancing predictive performance and achieving comprehensive multimodal
feature integration. Furthermore, we introduce a weighted Cox loss function,
specifically designed to handle imbalanced survival data, which is a common
challenge in the field. The proposed model achieves a mean concordance index
(C-index) of 0.77 and a time-dependent area under the curve (AUC) of 0.84,
outperforming state-of-the-art methods. It predicts risk (high versus low) with
prognostic significance for overall survival (OS) in univariate analysis
(HR=2.99, 95% CI: 1.88--4.78, p&lt;0.005), and maintains independent significance
in multivariate analysis incorporating standard clinicopathological variables
(HR=2.91, 95% CI: 1.80--4.68, p&lt;0.005). The proposed method not only improves
model performance but also addresses a critical gap in handling imbalanced
data.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10724" title="Abstract">arXiv:2402.10724</a> [<a href="/pdf/2402.10724" title="Download PDF">pdf</a>, <a href="/format/2402.10724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine Learning based Prediction of Ditching Loads
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schwarz%2C+H">Henning Schwarz</a>, 
<a href="/search/cs?searchtype=author&query=%C3%9Cberr%C3%BCck%2C+M">Micha &#xdc;berr&#xfc;ck</a>, 
<a href="/search/cs?searchtype=author&query=Zemke%2C+J+M">Jens-Peter M. Zemke</a>, 
<a href="/search/cs?searchtype=author&query=Rung%2C+T">Thomas Rung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">We present approaches to predict dynamic ditching loads on aircraft fuselages
using machine learning. The employed learning procedure is structured into two
parts, the reconstruction of the spatial loads using a convolutional
autoencoder (CAE) and the transient evolution of these loads in a subsequent
part. Different CAE strategies are assessed and combined with either long
short-term memory (LSTM) networks or Koopman-operator based methods to predict
the transient behaviour. The training data is compiled by an extension of the
momentum method of von-Karman and Wagner and the rationale of the training
approach is briefly summarised. The application included refers to a full-scale
fuselage of a DLR-D150 aircraft for a range of horizontal and vertical approach
velocities at 6{\deg} incidence. Results indicate a satisfactory level of
predictive agreement for all four investigated surrogate models examined, with
the combination of an LSTM and a deep decoder CAE showing the best performance.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10725" title="Abstract">arXiv:2402.10725</a> [<a href="/pdf/2402.10725" title="Download PDF">pdf</a>, <a href="/format/2402.10725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cloud Kitchen: Using Planning-based Composite AI to Optimize Food  Delivery Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=%C5%A0vanc%C3%A1r%2C+S">Slavom&#xed;r &#x160;vanc&#xe1;r</a>, 
<a href="/search/cs?searchtype=author&query=Chrpa%2C+L">Luk&#xe1;&#x161; Chrpa</a>, 
<a href="/search/cs?searchtype=author&query=Dvo%C5%99%C3%A1k%2C+F">Filip Dvo&#x159;&#xe1;k</a>, 
<a href="/search/cs?searchtype=author&query=Balyo%2C+T">Tom&#xe1;&#x161; Balyo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">The global food delivery market provides many opportunities for AI-based
services that can improve the efficiency of feeding the world. This paper
presents the Cloud Kitchen platform as a decision-making tool for restaurants
with food delivery and a simulator to evaluate the impact of the decisions. The
platform consists of a Technology-Specific Bridge (TSB) that provides an
interface for communicating with restaurants or the simulator. TSB uses a PDDL
model to represent decisions embedded in the Unified Planning Framework (UPF).
Decision-making, which concerns allocating customers' orders to vehicles and
deciding in which order the customers will be served (for each vehicle), is
done via a Vehicle Routing Problem with Time Windows (VRPTW), an efficient tool
for this problem. We show that decisions made by our platform can improve
customer satisfaction by reducing the number of delayed deliveries using a
real-world historical dataset.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10726" title="Abstract">arXiv:2402.10726</a> [<a href="/pdf/2402.10726" title="Download PDF">pdf</a>, <a href="/ps/2402.10726" title="Download PostScript">ps</a>, <a href="/format/2402.10726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Planning Action Models from State Traces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balyo%2C+T">Tom&#xe1;&#x161; Balyo</a>, 
<a href="/search/cs?searchtype=author&query=Suda%2C+M">Martin Suda</a>, 
<a href="/search/cs?searchtype=author&query=Chrpa%2C+L">Luk&#xe1;&#x161; Chrpa</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0afr%C3%A1nek%2C+D">Dominik &#x160;afr&#xe1;nek</a>, 
<a href="/search/cs?searchtype=author&query=Dvo%C5%99%C3%A1k%2C+F">Filip Dvo&#x159;&#xe1;k</a>, 
<a href="/search/cs?searchtype=author&query=Bart%C3%A1k%2C+R">Roman Bart&#xe1;k</a>, 
<a href="/search/cs?searchtype=author&query=Youngblood%2C+G+M">G. Michael Youngblood</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Previous STRIPS domain model acquisition approaches that learn from state
traces start with the names and parameters of the actions to be learned.
Therefore their only task is to deduce the preconditions and effects of the
given actions. In this work, we explore learning in situations when the
parameters of learned actions are not provided. We define two levels of trace
quality based on which information is provided and present an algorithm for
each. In one level (L1), the states in the traces are labeled with action
names, so we can deduce the number and names of the actions, but we still need
to work out the number and types of parameters. In the other level (L2), the
states are additionally labeled with objects that constitute the parameters of
the corresponding grounded actions. Here we still need to deduce the types of
the parameters in the learned actions. We experimentally evaluate the proposed
algorithms and compare them with the state-of-the-art learning tool FAMA on a
large collection of IPC benchmarks. The evaluation shows that our new
algorithms are faster, can handle larger inputs and provide better results in
terms of learning action models more similar to reference models.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10729" title="Abstract">arXiv:2402.10729</a> [<a href="/pdf/2402.10729" title="Download PDF">pdf</a>, <a href="/format/2402.10729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A CBF-Adaptive Control Architecture for Visual Navigation for UAV in the  Presence of Uncertainties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sankaranarayanan%2C+V+N">Viswa Narayanan Sankaranarayanan</a>, 
<a href="/search/cs?searchtype=author&query=Saradagi%2C+A">Akshit Saradagi</a>, 
<a href="/search/cs?searchtype=author&query=Satpute%2C+S">Sumeet Satpute</a>, 
<a href="/search/cs?searchtype=author&query=Nikolakopoulos%2C+G">George Nikolakopoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 7 figures, accepted to be published in IEEE International Conference on Robotics and Automation (ICRA 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">In this article, we propose a control solution for the safe transfer of a
quadrotor UAV between two surface robots positioning itself only using the
visual features on the surface robots, which enforces safety constraints for
precise landing and visual locking, in the presence of modeling uncertainties
and external disturbances. The controller handles the ascending and descending
phases of the navigation using a visual locking control barrier function (VCBF)
and a parametrizable switching descending CBF (DCBF) respectively, eliminating
the need for an external planner. The control scheme has a backstepping
approach for the position controller with the CBF filter acting on the position
kinematics to produce a filtered virtual velocity control input, which is
tracked by an adaptive controller to overcome modeling uncertainties and
external disturbances. The experimental validation is carried out with a UAV
that navigates from the base to the target using an RGB camera.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10735" title="Abstract">arXiv:2402.10735</a> [<a href="/pdf/2402.10735" title="Download PDF">pdf</a>, <a href="/format/2402.10735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing the Reasoning Abilities of ChatGPT in the Context of Claim  Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dougrez-Lewis%2C+J">John Dougrez-Lewis</a>, 
<a href="/search/cs?searchtype=author&query=Akhter%2C+M+E">Mahmud Elahi Akhter</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yulan He</a>, 
<a href="/search/cs?searchtype=author&query=Liakata%2C+M">Maria Liakata</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The reasoning capabilities of LLMs are currently hotly debated. We examine
the issue from the perspective of claim/rumour verification. We propose the
first logical reasoning framework designed to break down any claim or rumor
paired with evidence into the atomic reasoning steps necessary for
verification. Based on our framework, we curate two annotated collections of
such claim/evidence pairs: a synthetic dataset from Wikipedia and a real-world
set stemming from rumours circulating on Twitter. We use them to evaluate the
reasoning capabilities of GPT-3.5-Turbo and GPT-4 (hereinafter referred to as
ChatGPT) within the context of our framework, providing a thorough analysis.
Our results show that ChatGPT struggles in abductive reasoning, although this
can be somewhat mitigated by using manual Chain of Thought (CoT) as opposed to
Zero Shot (ZS) and ZS CoT approaches. Our study contributes to the growing body
of research suggesting that ChatGPT's reasoning processes are unlikely to
mirror human-like reasoning, and that LLMs need to be more rigorously evaluated
in order to distinguish between hype and actual capabilities, especially in
high stake real-world tasks such as claim verification.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10738" title="Abstract">arXiv:2402.10738</a> [<a href="/pdf/2402.10738" title="Download PDF">pdf</a>, <a href="/format/2402.10738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Let&#x27;s Learn Step by Step: Enhancing In-Context Learning Ability with  Curriculum Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yinpeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiawei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xiang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Q">Qikai Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+W">Wei Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Demonstration ordering, which is an important strategy for in-context
learning (ICL), can significantly affects the performance of large language
models (LLMs). However, most of the current approaches of ordering require
additional knowledge and similarity calculation. We advocate the few-shot
in-context curriculum learning (ICCL), a simple but effective demonstration
ordering method for ICL, which implies gradually increasing the complexity of
prompt demonstrations during the inference process. Then we design three
experiments to discuss the effectiveness of ICCL, the formation mechanism of
LLM's ICCL capability, and the impact of ordering subjects. Experimental
results demonstrate that ICCL, developed during the instruction-tuning stage,
is effective for open-source LLMs. Moreover, LLMs exhibit a weaker capacity
compared to humans in discerning the difficulty levels of demonstrations. We
release our code at https://github.com/61peng/curri_learning.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10739" title="Abstract">arXiv:2402.10739</a> [<a href="/pdf/2402.10739" title="Download PDF">pdf</a>, <a href="/format/2402.10739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PointMamba: A Simple State Space Model for Point Cloud Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+D">Dingkang Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xingkui Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Z">Zhikang Zou</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+X">Xiaoqing Ye</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+X">Xiang Bai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The code is available at <a href="https://github.com/deepinact/PointMamba">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Transformers have become one of the foundational architectures in point cloud
analysis tasks due to their excellent global modeling ability. However, the
attention mechanism has quadratic complexity and is difficult to extend to long
sequence modeling due to limited computational resources and so on. Recently,
state space models (SSM), a new family of deep sequence models, have presented
great potential for sequence modeling in NLP tasks. In this paper, taking
inspiration from the success of SSM in NLP, we propose PointMamba, a framework
with global modeling and linear complexity. Specifically, by taking embedded
point patches as input, we proposed a reordering strategy to enhance SSM's
global modeling ability by providing a more logical geometric scanning order.
The reordered point tokens are then sent to a series of Mamba blocks to
causally capture the point cloud structure. Experimental results show our
proposed PointMamba outperforms the transformer-based counterparts on different
point cloud analysis datasets, while significantly saving about 44.3%
parameters and 25% FLOPs, demonstrating the potential option for constructing
foundational 3D vision models. We hope our PointMamba can provide a new
perspective for point cloud analysis. The code is available at
https://github.com/LMD0311/PointMamba.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10741" title="Abstract">arXiv:2402.10741</a> [<a href="/pdf/2402.10741" title="Download PDF">pdf</a>, <a href="/format/2402.10741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying heterogeneous micromechanical properties of biological  tissues via physics-informed neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wu%2C+W">Wensi Wu</a>, 
<a href="/search/math?searchtype=author&query=Daneker%2C+M">Mitchell Daneker</a>, 
<a href="/search/math?searchtype=author&query=Turner%2C+K+T">Kevin T. Turner</a>, 
<a href="/search/math?searchtype=author&query=Jolley%2C+M+A">Matthew A. Jolley</a>, 
<a href="/search/math?searchtype=author&query=Lu%2C+L">Lu Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The heterogeneous micromechanical properties of biological tissues have
profound implications across diverse medical and engineering domains. However,
identifying the full-field heterogeneous elastic properties of soft materials
using traditional computational and engineering approaches is fundamentally
challenging due to difficulties in estimating local stress fields. Recently,
there has been a growing interest in using data-driven models to learn
full-field mechanical responses such as displacement and strain from
experimental or synthetic data. However, research studies on inferring the
full-field elastic properties of materials, a more challenging problem, are
scarce, particularly for large deformation, hyperelastic materials. Here, we
propose a novel approach to identify the elastic modulus distribution in
nonlinear, large deformation hyperelastic materials utilizing physics-informed
neural networks (PINNs). We evaluate the prediction accuracies and
computational efficiency of PINNs, informed by mechanic features and
principles, across three synthetic materials with structural complexity that
closely resemble real tissue patterns, such as brain tissue and tricuspid valve
tissue. Our improved PINN architecture accurately estimates the full-field
elastic properties, with relative errors of less than 5% across all examples.
This research has significant potential for advancing our understanding of
micromechanical behaviors in biological materials, impacting future innovations
in engineering and medicine.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10743" title="Abstract">arXiv:2402.10743</a> [<a href="/pdf/2402.10743" title="Download PDF">pdf</a>, <a href="/ps/2402.10743" title="Download PostScript">ps</a>, <a href="/format/2402.10743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Construction of a Syntactic Analysis Map for Yi Shui School through Text  Mining and Natural Language Processing Research
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hanqing Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuehan Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Entity and relationship extraction is a crucial component in natural language
processing tasks such as knowledge graph construction, question answering
system design, and semantic analysis. Most of the information of the Yishui
school of traditional Chinese Medicine (TCM) is stored in the form of
unstructured classical Chinese text. The key information extraction of TCM
texts plays an important role in mining and studying the academic schools of
TCM. In order to solve these problems efficiently using artificial intelligence
methods, this study constructs a word segmentation and entity relationship
extraction model based on conditional random fields under the framework of
natural language processing technology to identify and extract the entity
relationship of traditional Chinese medicine texts, and uses the common
weighting technology of TF-IDF information retrieval and data mining to extract
important key entity information in different ancient books. The dependency
syntactic parser based on neural network is used to analyze the grammatical
relationship between entities in each ancient book article, and it is
represented as a tree structure visualization, which lays the foundation for
the next construction of the knowledge graph of Yishui school and the use of
artificial intelligence methods to carry out the research of TCM academic
schools.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10744" title="Abstract">arXiv:2402.10744</a> [<a href="/pdf/2402.10744" title="Download PDF">pdf</a>, <a href="/format/2402.10744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenRES: Rethinking Evaluation for Generative Relation Extraction in the  Era of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+P">Pengcheng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jiacheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zifeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Jimeng Sun</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiawei Han</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The field of relation extraction (RE) is experiencing a notable shift towards
generative relation extraction (GRE), leveraging the capabilities of large
language models (LLMs). However, we discovered that traditional relation
extraction (RE) metrics like precision and recall fall short in evaluating GRE
methods. This shortfall arises because these metrics rely on exact matching
with human-annotated reference relations, while GRE methods often produce
diverse and semantically accurate relations that differ from the references. To
fill this gap, we introduce GenRES for a multi-dimensional assessment in terms
of the topic similarity, uniqueness, granularity, factualness, and completeness
of the GRE results. With GenRES, we empirically identified that (1)
precision/recall fails to justify the performance of GRE methods; (2)
human-annotated referential relations can be incomplete; (3) prompting LLMs
with a fixed set of relations or entities can cause hallucinations. Next, we
conducted a human evaluation of GRE methods that shows GenRES is consistent
with human preferences for RE quality. Last, we made a comprehensive evaluation
of fourteen leading LLMs using GenRES across document, bag, and sentence level
RE datasets, respectively, to set the benchmark for future research in GRE
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10747" title="Abstract">arXiv:2402.10747</a> [<a href="/pdf/2402.10747" title="Download PDF">pdf</a>, <a href="/format/2402.10747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fully Differentiable Lagrangian Convolutional Neural Network for  Continuity-Consistent Physics-Informed Precipitation Nowcasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pavl%C3%ADk%2C+P">Peter Pavl&#xed;k</a>, 
<a href="/search/cs?searchtype=author&query=V%C3%BDboh%2C+M">Martin V&#xfd;boh</a>, 
<a href="/search/cs?searchtype=author&query=Ezzeddine%2C+A+B">Anna Bou Ezzeddine</a>, 
<a href="/search/cs?searchtype=author&query=Rozinajov%C3%A1%2C+V">Viera Rozinajov&#xe1;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICML 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">This paper presents a convolutional neural network model for precipitation
nowcasting that combines data-driven learning with physics-informed domain
knowledge. We propose LUPIN, a Lagrangian Double U-Net for Physics-Informed
Nowcasting, that draws from existing extrapolation-based nowcasting methods and
implements the Lagrangian coordinate system transformation of the data in a
fully differentiable and GPU-accelerated manner to allow for real-time
end-to-end training and inference. Based on our evaluation, LUPIN matches and
exceeds the performance of the chosen benchmark, opening the door for other
Lagrangian machine learning models.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10750" title="Abstract">arXiv:2402.10750</a> [<a href="/pdf/2402.10750" title="Download PDF">pdf</a>, <a href="/format/2402.10750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards benchmarking of Solidity verification tools
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bartoletti%2C+M">Massimo Bartoletti</a>, 
<a href="/search/cs?searchtype=author&query=Fioravanti%2C+F">Fabio Fioravanti</a>, 
<a href="/search/cs?searchtype=author&query=Matricardi%2C+G">Giulia Matricardi</a>, 
<a href="/search/cs?searchtype=author&query=Pettinau%2C+R">Roberto Pettinau</a>, 
<a href="/search/cs?searchtype=author&query=Sainas%2C+F">Franco Sainas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Formal verification of smart contracts has become a hot topic in academic and
industrial research, given the growing value of assets managed by decentralized
applications and the consequent incentive for adversaries to tamper with them.
Most of the current research on the verification of contracts revolves around
Solidity, the main high-level language supported by Ethereum and other leading
blockchains. Although bug detection tools for Solidity have been proliferating
almost since the inception of Ethereum, only in the last few years we have seen
verification tools capable of proving that a contract respects some desirable
properties. An open issue is how to evaluate and compare the effectiveness of
these tools: indeed, the existing benchmarks for general-purpose programming
languages cannot be adapted to Solidity, given substantial differences in the
programming model and in the desirable properties. We address this problem by
proposing an open benchmark for Solidity verification tools. By exploiting our
benchmark, we compare two leading tools, SolCMC and Certora, discussing their
completeness, soundness and expressiveness limitations.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10751" title="Abstract">arXiv:2402.10751</a> [<a href="/pdf/2402.10751" title="Download PDF">pdf</a>, <a href="/format/2402.10751" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Another Body in the World: Flusserian Freedom in Mixed Reality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A+L">Aven Le Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xi%2C+L">Lei Xi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">In Flusserian view of media history, humans often misperceive the world
projected by media to be the world itself, leading to a loss of freedom. This
paper examines Flusserian Freedom in the context of Mixed Reality (MR) and
explores how humans can recognize the obscuration of the world within the media
(i.e., MR) and understand their relationship. The authors investigate the
concept of playing against apparatus and deliberately alienating the perception
of the projected world through an artwork titled "Surrealism Me." This artwork
enables the user to have another body within MR through interactive and
immersive experiences based on the definition of Sense of Embodiment. The
purpose of this work is to raise awareness of the domination of media and to
approach Flusserian freedom within contemporary technical arrangements.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10752" title="Abstract">arXiv:2402.10752</a> [<a href="/pdf/2402.10752" title="Download PDF">pdf</a>, <a href="/format/2402.10752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STF: Spatio-Temporal Fusion Module for Improving Video Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anwar%2C+N">Noreen Anwar</a>, 
<a href="/search/cs?searchtype=author&query=Bilodeau%2C+G">Guillaume-Alexandre Bilodeau</a>, 
<a href="/search/cs?searchtype=author&query=Bouachir%2C+W">Wassim Bouachir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages,3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Consecutive frames in a video contain redundancy, but they may also contain
relevant complementary information for the detection task. The objective of our
work is to leverage this complementary information to improve detection.
Therefore, we propose a spatio-temporal fusion framework (STF). We first
introduce multi-frame and single-frame attention modules that allow a neural
network to share feature maps between nearby frames to obtain more robust
object representations. Second, we introduce a dual-frame fusion module that
merges feature maps in a learnable manner to improve them. Our evaluation is
conducted on three different benchmarks including video sequences of moving
road users. The performed experiments demonstrate that the proposed
spatio-temporal fusion module leads to improved detection performance compared
to baseline object detectors. Code is available at
https://github.com/noreenanwar/STF-module
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10753" title="Abstract">arXiv:2402.10753</a> [<a href="/pdf/2402.10753" title="Download PDF">pdf</a>, <a href="/format/2402.10753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ToolSword: Unveiling Safety Issues of Large Language Models in Tool  Learning Across Three Stages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Junjie Ye</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sixian Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guanyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Caishuang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+S">Songyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yilong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Tool learning is widely acknowledged as a foundational approach or deploying
large language models (LLMs) in real-world scenarios. While current research
primarily emphasizes leveraging tools to augment LLMs, it frequently neglects
emerging safety considerations tied to their application. To fill this gap, we
present $ToolSword$, a comprehensive framework dedicated to meticulously
investigating safety issues linked to LLMs in tool learning. Specifically,
ToolSword delineates six safety scenarios for LLMs in tool learning,
encompassing $malicious$ $queries$ and $jailbreak$ $attacks$ in the input
stage, $noisy$ $misdirection$ and $risky$ $cues$ in the execution stage, and
$harmful$ $feedback$ and $error$ $conflicts$ in the output stage. Experiments
conducted on 11 open-source and closed-source LLMs reveal enduring safety
challenges in tool learning, such as handling harmful queries, employing risky
tools, and delivering detrimental feedback, which even GPT-4 is susceptible to.
Moreover, we conduct further studies with the aim of fostering research on tool
learning safety. The data is released in
https://github.com/Junjie-Ye/ToolSword.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10754" title="Abstract">arXiv:2402.10754</a> [<a href="/pdf/2402.10754" title="Download PDF">pdf</a>, <a href="/format/2402.10754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Dataflow Analysis Meets Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengpeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wuqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Z">Zian Su</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiangzhe Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xiaoheng Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiangyu Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 16 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Machine Learning (cs.LG); Software Engineering (cs.SE)

</div>
<p class="mathjax">Dataflow analysis is a powerful code analysis technique that reasons
dependencies between program values, offering support for code optimization,
program comprehension, and bug detection. Existing approaches require the
successful compilation of the subject program and customizations for downstream
applications. This paper introduces LLMDFA, an LLM-powered dataflow analysis
framework that analyzes arbitrary code snippets without requiring a compilation
infrastructure and automatically synthesizes downstream applications. Inspired
by summary-based dataflow analysis, LLMDFA decomposes the problem into three
sub-problems, which are effectively resolved by several essential strategies,
including few-shot chain-of-thought prompting and tool synthesis. Our
evaluation has shown that the design can mitigate the hallucination and improve
the reasoning ability, obtaining high precision and recall in detecting
dataflow-related bugs upon benchmark programs, outperforming state-of-the-art
(classic) tools, including a very recent industrial analyzer.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10756" title="Abstract">arXiv:2402.10756</a> [<a href="/pdf/2402.10756" title="Download PDF">pdf</a>, <a href="/format/2402.10756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Cohesion-Fairness Harmony: Contrastive Regularization in  Individual Fair Graph Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghodsi%2C+S">Siamak Ghodsi</a>, 
<a href="/search/cs?searchtype=author&query=Seyedi%2C+S+A">Seyed Amjad Seyedi</a>, 
<a href="/search/cs?searchtype=author&query=Ntoutsi%2C+E">Eirini Ntoutsi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in "The 28th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD 2024)"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Conventional fair graph clustering methods face two primary challenges: i)
They prioritize balanced clusters at the expense of cluster cohesion by
imposing rigid constraints, ii) Existing methods of both individual and
group-level fairness in graph partitioning mostly rely on eigen decompositions
and thus, generally lack interpretability. To address these issues, we propose
iFairNMTF, an individual Fairness Nonnegative Matrix Tri-Factorization model
with contrastive fairness regularization that achieves balanced and cohesive
clusters. By introducing fairness regularization, our model allows for
customizable accuracy-fairness trade-offs, thereby enhancing user autonomy
without compromising the interpretability provided by nonnegative matrix
tri-factorization. Experimental evaluations on real and synthetic datasets
demonstrate the superior flexibility of iFairNMTF in achieving fairness and
clustering performance.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10757" title="Abstract">arXiv:2402.10757</a> [<a href="/pdf/2402.10757" title="Download PDF">pdf</a>, <a href="/format/2402.10757" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fitness-based Linkage Learning and Maximum-Clique Conditional Linkage  Modelling for Gray-box Optimization with RV-GOMEA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Andreadis%2C+G">Georgios Andreadis</a>, 
<a href="/search/cs?searchtype=author&query=Alderliesten%2C+T">Tanja Alderliesten</a>, 
<a href="/search/cs?searchtype=author&query=Bosman%2C+P+A+N">Peter A.N. Bosman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">For many real-world optimization problems it is possible to perform partial
evaluations, meaning that the impact of changing a few variables on a
solution's fitness can be computed very efficiently. It has been shown that
such partial evaluations can be excellently leveraged by the Real-Valued GOMEA
(RV-GOMEA) that uses a linkage model to capture dependencies between problem
variables. Recently, conditional linkage models were introduced for RV-GOMEA,
expanding its state-of-the-art performance even to problems with overlapping
dependencies. However, that work assumed that the dependency structure is known
a priori. Fitness-based linkage learning techniques have previously been used
to detect dependencies during optimization, but only for non-conditional
linkage models. In this work, we combine fitness-based linkage learning and
conditional linkage modelling in RV-GOMEA. In addition, we propose a new way to
model overlapping dependencies in conditional linkage models to maximize the
joint sampling of fully interdependent groups of variables. We compare the
resulting novel variant of RV-GOMEA to other variants of RV-GOMEA and VkD-CMA
on 12 problems with varying degree of overlapping dependencies. We find that
the new RV-GOMEA not only performs best on most problems, also the overhead of
learning the conditional linkage models during optimization is often
negligible.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10761" title="Abstract">arXiv:2402.10761</a> [<a href="/pdf/2402.10761" title="Download PDF">pdf</a>, <a href="/ps/2402.10761" title="Download PostScript">ps</a>, <a href="/format/2402.10761" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autonomous Emergency Braking With Driver-In-The-Loop: Torque Vectoring  for Active Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Sullivan%2C+B">Benjamin Sullivan</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+J">Jingjing Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Mavros%2C+G">Georgios Mavros</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+W">Wen-Hua Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Autonomous Emergency Braking (AEB) potentially brings significant
improvements in automotive safety due to its ability to autonomously prevent
collisions in situations where the driver may not be able to do so. Driven by
the poor performance of the state of the art in recent testing, this work
provides an online solution to identify critical parameters such as the current
and maximum friction coefficients. The method introduced here, namely Torque
Vectoring for Active Learning (TVAL), can perform state and parameter
estimation whilst following the driver's input. Importantly with less power
requirements than normal driving. Our method is designed with a crucial focus
on ensuring minimal disruption to the driver, allowing them to maintain full
control of the vehicle. Additionally, we exploit a rain/light sensor to drive
the observer resampling to maintain estimation certainty across prolonged
operation. Then a scheme to modulate TVAL is introduced that considers
powertrain efficiency, safety, and availability in an online fashion. Using a
high-fidelity vehicle model and drive cycle we demonstrate the functionality of
TVAL controller across changing road surfaces where we successfully identify
the road surface whenever possible.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10762" title="Abstract">arXiv:2402.10762</a> [<a href="/pdf/2402.10762" title="Download PDF">pdf</a>, <a href="/format/2402.10762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Explaining Unfairness: An Overview
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fragkathoulas%2C+C">Christos Fragkathoulas</a>, 
<a href="/search/cs?searchtype=author&query=Papanikou%2C+V">Vasiliki Papanikou</a>, 
<a href="/search/cs?searchtype=author&query=Karidi%2C+D+P">Danae Pla Karidi</a>, 
<a href="/search/cs?searchtype=author&query=Pitoura%2C+E">Evaggelia Pitoura</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Algorithmic fairness and explainability are foundational elements for
achieving responsible AI. In this paper, we focus on their interplay, a
research area that is recently receiving increasing attention. To this end, we
first present two comprehensive taxonomies, each representing one of the two
complementary fields of study: fairness and explanations. Then, we categorize
explanations for fairness into three types: (a) Explanations to enhance
fairness metrics, (b) Explanations to help us understand the causes of
(un)fairness, and (c) Explanations to assist us in designing methods for
mitigating unfairness. Finally, based on our fairness and explanation
taxonomies, we present undiscovered literature paths revealing gaps that can
serve as valuable insights for future research.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10765" title="Abstract">arXiv:2402.10765</a> [<a href="/pdf/2402.10765" title="Download PDF">pdf</a>, <a href="/format/2402.10765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Policy Learning for Off-Dynamics RL with Deficient Support
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Van%2C+L+L+P">Linh Le Pham Van</a>, 
<a href="/search/cs?searchtype=author&query=Tran%2C+H+T">Hung The Tran</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+S">Sunil Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAMAS 2024 as a full paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Reinforcement Learning (RL) can effectively learn complex policies. However,
learning these policies often demands extensive trial-and-error interactions
with the environment. In many real-world scenarios, this approach is not
practical due to the high costs of data collection and safety concerns. As a
result, a common strategy is to transfer a policy trained in a low-cost, rapid
source simulator to a real-world target environment. However, this process
poses challenges. Simulators, no matter how advanced, cannot perfectly
replicate the intricacies of the real world, leading to dynamics discrepancies
between the source and target environments. Past research posited that the
source domain must encompass all possible target transitions, a condition we
term full support. However, expecting full support is often unrealistic,
especially in scenarios where significant dynamics discrepancies arise. In this
paper, our emphasis shifts to addressing large dynamics mismatch adaptation. We
move away from the stringent full support condition of earlier research,
focusing instead on crafting an effective policy for the target domain. Our
proposed approach is simple but effective. It is anchored in the central
concepts of the skewing and extension of source support towards target support
to mitigate support deficiencies. Through comprehensive testing on a varied set
of benchmarks, our method's efficacy stands out, showcasing notable
improvements over previous techniques.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10767" title="Abstract">arXiv:2402.10767</a> [<a href="/pdf/2402.10767" title="Download PDF">pdf</a>, <a href="/format/2402.10767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inference to the Best Explanation in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dalal%2C+D">Dhairya Dalal</a>, 
<a href="/search/cs?searchtype=author&query=Valentino%2C+M">Marco Valentino</a>, 
<a href="/search/cs?searchtype=author&query=Freitas%2C+A">Andr&#xe9; Freitas</a>, 
<a href="/search/cs?searchtype=author&query=Buitelaar%2C+P">Paul Buitelaar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While Large Language Models (LLMs) have found success in real-world
applications, their underlying explanatory process is still poorly understood.
This paper proposes IBE-Eval, a framework inspired by philosophical accounts on
Inference to the Best Explanation (IBE) to advance the interpretation and
evaluation of LLMs' explanations. IBE-Eval estimates the plausibility of
natural language explanations through a combination of explicit logical and
linguistic features including: consistency, parsimony, coherence, and
uncertainty. Extensive experiments are conducted on Causal Question Answering
(CQA), where \textit{IBE-Eval} is tasked to select the most plausible causal
explanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama
2). The experiments reveal that IBE-Eval can successfully identify the best
explanation with up to 77\% accuracy ($\approx 27\%$ above random), improving
upon a GPT 3.5-as-a-Judge baseline ($\approx+17\%$) while being intrinsically
more efficient and interpretable. Additional analyses suggest that, despite
model-specific variances, LLM-generated explanations tend to conform to IBE
criteria and that IBE-Eval is significantly correlated with human judgment,
opening up opportunities for future development of automated explanation
verification tools.
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10769" title="Abstract">arXiv:2402.10769</a> [<a href="/pdf/2402.10769" title="Download PDF">pdf</a>, <a href="/format/2402.10769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distillation Enhanced Generative Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+L">Liqiang Nie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">Generative retrieval is a promising new paradigm in text retrieval that
generates identifier strings of relevant passages as the retrieval target. This
paradigm leverages powerful generative language models, distinct from
traditional sparse or dense retrieval methods. In this work, we identify a
viable direction to further enhance generative retrieval via distillation and
propose a feasible framework, named DGR. DGR utilizes sophisticated ranking
models, such as the cross-encoder, in a teacher role to supply a passage rank
list, which captures the varying relevance degrees of passages instead of
binary hard labels; subsequently, DGR employs a specially designed distilled
RankNet loss to optimize the generative retrieval model, considering the
passage rank order provided by the teacher model as labels. This framework only
requires an additional distillation step to enhance current generative
retrieval systems and does not add any burden to the inference stage. We
conduct experiments on four public datasets, and the results indicate that DGR
achieves state-of-the-art performance among the generative retrieval methods.
Additionally, DGR demonstrates exceptional robustness and generalizability with
various teacher models and distillation losses.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10770" title="Abstract">arXiv:2402.10770</a> [<a href="/pdf/2402.10770" title="Download PDF">pdf</a>, <a href="/format/2402.10770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Reliable Are Automatic Evaluation Methods for Instruction-Tuned  LLMs?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Doostmohammadi%2C+E">Ehsan Doostmohammadi</a>, 
<a href="/search/cs?searchtype=author&query=Holmstr%C3%B6m%2C+O">Oskar Holmstr&#xf6;m</a>, 
<a href="/search/cs?searchtype=author&query=Kuhlmann%2C+M">Marco Kuhlmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Work on instruction-tuned Large Language Models (LLMs) has used automatic
methods based on text overlap and LLM judgments as cost-effective alternatives
to human evaluation. In this paper, we study the reliability of such methods
across a broad range of tasks and in a cross-lingual setting. In contrast to
previous findings, we observe considerable variability in correlations between
automatic methods and human evaluators when scores are differentiated by task
type. Specifically, the widely-used ROUGE-L metric strongly correlates with
human judgments for short-answer English tasks but is unreliable in free-form
generation tasks and cross-lingual transfer. The effectiveness of GPT-4 as an
evaluator depends on including reference answers when prompting for
assessments, which can lead to overly strict evaluations in free-form
generation tasks. In summary, we find that, while automatic evaluation methods
can approximate human judgements under specific conditions, their reliability
is highly context-dependent. Our findings enhance the understanding of how
automatic methods should be applied and interpreted when developing and
evaluating instruction-tuned LLMs.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10772" title="Abstract">arXiv:2402.10772</a> [<a href="/pdf/2402.10772" title="Download PDF">pdf</a>, <a href="/format/2402.10772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing ESG Impact Type Identification through Early Fusion and  Multilingual Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Veeramani%2C+H">Hariram Veeramani</a>, 
<a href="/search/cs?searchtype=author&query=Thapa%2C+S">Surendrabikram Thapa</a>, 
<a href="/search/cs?searchtype=author&query=Naseem%2C+U">Usman Naseem</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to FinNLP workshop at IJCNLP-ACL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In the evolving landscape of Environmental, Social, and Corporate Governance
(ESG) impact assessment, the ML-ESG-2 shared task proposes identifying ESG
impact types. To address this challenge, we present a comprehensive system
leveraging ensemble learning techniques, capitalizing on early and late fusion
approaches. Our approach employs four distinct models: mBERT, FlauBERT-base,
ALBERT-base-v2, and a Multi-Layer Perceptron (MLP) incorporating Latent
Semantic Analysis (LSA) and Term Frequency-Inverse Document Frequency (TF-IDF)
features. Through extensive experimentation, we find that our early fusion
ensemble approach, featuring the integration of LSA, TF-IDF, mBERT,
FlauBERT-base, and ALBERT-base-v2, delivers the best performance. Our system
offers a comprehensive ESG impact type identification solution, contributing to
the responsible and sustainable decision-making processes vital in today's
financial and corporate governance landscape.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10773" title="Abstract">arXiv:2402.10773</a> [<a href="/pdf/2402.10773" title="Download PDF">pdf</a>, <a href="/format/2402.10773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AIM: Automated Input Set Minimization for Metamorphic Security Testing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chaleshtari%2C+N+B">Nazanin Bayati Chaleshtari</a>, 
<a href="/search/cs?searchtype=author&query=Marquer%2C+Y">Yoann Marquer</a>, 
<a href="/search/cs?searchtype=author&query=Pastore%2C+F">Fabrizio Pastore</a>, 
<a href="/search/cs?searchtype=author&query=Briand%2C+L+C">Lionel C. Briand</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">For Web systems, which are accessible to any machine connected to internet,
security is a critical concern. Although security testing can be automated by
generating crafted inputs as an attacker would do, solutions to automate the
test oracle, i.e., distinguishing correct from incorrect outputs for a given
input, remain preliminary. Specifically, previous work has demonstrated the
potential of metamorphic testing; indeed, security failures can be determined
by metamorphic relations that turn valid inputs into malicious inputs and
compare their outputs. However, without further guidance, metamorphic relations
should be executed on a very large set of valid inputs, which is time consuming
and makes metamorphic testing impractical. Hence, in this study, we propose
AIM, an approach that automatically selects inputs to reduce testing costs
while preserving vulnerability detection capabilities. AIM includes a
clustering-based black box approach, identifying similar inputs based on their
security properties. It also presents a novel genetic algorithm able to
efficiently select diverse inputs while minimizing their total cost. Further,
it contains a problem reduction component to reduce the search space and speed
up the minimization process. We evaluated the effectiveness of AIM on two
well-known web systems, Jenkins and Joomla. We compared AIM's results with four
baselines in security testing. Overall, AIM reduced MRs execution time by 84
percent for Jenkins and 82 percent for Joomla while preserving full
vulnerability detection. Furthermore, AIM outperformed all the considered
baselines regarding vulnerability coverage. Although it has been tuned to work
with Web system inputs, AIM could be applied to minimize metamorphic testing
cost in other contexts.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10774" title="Abstract">arXiv:2402.10774</a> [<a href="/pdf/2402.10774" title="Download PDF">pdf</a>, <a href="/format/2402.10774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness  Constants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Richt%C3%A1rik%2C+P">Peter Richt&#xe1;rik</a>, 
<a href="/search/cs?searchtype=author&query=Gasanov%2C+E">Elnur Gasanov</a>, 
<a href="/search/cs?searchtype=author&query=Burlachenko%2C+K">Konstantin Burlachenko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 70 pages, 14 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">Error Feedback (EF) is a highly popular and immensely effective mechanism for
fixing convergence issues which arise in distributed training methods (such as
distributed GD or SGD) when these are enhanced with greedy communication
compression techniques such as TopK. While EF was proposed almost a decade ago
(Seide et al., 2014), and despite concentrated effort by the community to
advance the theoretical understanding of this mechanism, there is still a lot
to explore. In this work we study a modern form of error feedback called EF21
(Richtarik et al., 2021) which offers the currently best-known theoretical
guarantees, under the weakest assumptions, and also works well in practice. In
particular, while the theoretical communication complexity of EF21 depends on
the quadratic mean of certain smoothness parameters, we improve this dependence
to their arithmetic mean, which is always smaller, and can be substantially
smaller, especially in heterogeneous data regimes. We take the reader on a
journey of our discovery process. Starting with the idea of applying EF21 to an
equivalent reformulation of the underlying problem which (unfortunately)
requires (often impractical) machine cloning, we continue to the discovery of a
new weighted version of EF21 which can (fortunately) be executed without any
cloning, and finally circle back to an improved analysis of the original EF21
method. While this development applies to the simplest form of EF21, our
approach naturally extends to more elaborate variants involving stochastic
gradients and partial participation. Further, our technique improves the
best-known theory of EF21 in the rare features regime (Richtarik et al., 2023).
Finally, we validate our theoretical findings with suitable experiments.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10777" title="Abstract">arXiv:2402.10777</a> [<a href="/pdf/2402.10777" title="Download PDF">pdf</a>, <a href="/format/2402.10777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MultiDimEr: a multi-dimensional bug analyzEr
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Silva%2C+L">Lakmal Silva</a>, 
<a href="/search/cs?searchtype=author&query=Unterkalmsteiner%2C+M">Michael Unterkalmsteiner</a>, 
<a href="/search/cs?searchtype=author&query=Wnuk%2C+K">Krzysztof Wnuk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> TechDebt@ICSE 2022: 66-70
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Background: Bugs and bug management consumes a significant amount of time and
effort from software development organizations. A reduction in bugs can
significantly improve the capacity for new feature development. Aims: We
categorize and visualize dimensions of bug reports to identify accruing
technical debt. This evidence can serve practitioners and decision makers not
only as an argumentative basis for steering improvement efforts, but also as a
starting point for root cause analysis, reducing overall bug inflow. Method: We
implemented a tool, MultiDimEr, that analyzes and visualizes bug reports. The
tool was implemented and evaluated at Ericsson. Results: We present our
preliminary findings using the MultiDimEr for bug analysis, where we
successfully identified components generating most of the bugs and bug trends
within certain components. Conclusions: By analyzing the dimensions provided by
MultiDimEr, we show that classifying and visualizing bug reports in different
dimensions can stimulate discussions around bug hot spots as well as validating
the accuracy of manually entered bug report attributes used in technical debt
measurements such as fault slip through.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10778" title="Abstract">arXiv:2402.10778</a> [<a href="/pdf/2402.10778" title="Download PDF">pdf</a>, <a href="/format/2402.10778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoGPT+P: Affordance-based Task Planning with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Birr%2C+T">Timo Birr</a>, 
<a href="/search/cs?searchtype=author&query=Pohl%2C+C">Christoph Pohl</a>, 
<a href="/search/cs?searchtype=author&query=Younes%2C+A">Abdelrahman Younes</a>, 
<a href="/search/cs?searchtype=author&query=Asfour%2C+T">Tamim Asfour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 16 pages including references and appendix, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advances in task planning leverage Large Language Models (LLMs) to
improve generalizability by combining such models with classical planning
algorithms to address their inherent limitations in reasoning capabilities.
However, these approaches face the challenge of dynamically capturing the
initial state of the task planning problem. To alleviate this issue, we propose
AutoGPT+P, a system that combines an affordance-based scene representation with
a planning system. Affordances encompass the action possibilities of an agent
on the environment and objects present in it. Thus, deriving the planning
domain from an affordance-based scene representation allows symbolic planning
with arbitrary objects. AutoGPT+P leverages this representation to derive and
execute a plan for a task specified by the user in natural language. In
addition to solving planning tasks under a closed-world assumption, AutoGPT+P
can also handle planning with incomplete information, e. g., tasks with missing
objects by exploring the scene, suggesting alternatives, or providing a partial
plan. The affordance-based scene representation combines object detection with
an automatically generated object-affordance-mapping using ChatGPT. The core
planning tool extends existing work by automatically correcting semantic and
syntactic errors. Our approach achieves a success rate of 98%, surpassing the
current 81% success rate of the current state-of-the-art LLM-based planning
method SayCan on the SayCan instruction set. Furthermore, we evaluated our
approach on our newly created dataset with 150 scenarios covering a wide range
of complex tasks with missing objects, achieving a success rate of 79% on our
dataset. The dataset and the code are publicly available at
https://git.h2t.iar.kit.edu/birr/autogpt-p-standalone.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10779" title="Abstract">arXiv:2402.10779</a> [<a href="/pdf/2402.10779" title="Download PDF">pdf</a>, <a href="/format/2402.10779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Condensed Transition Graph Framework for Zero-shot Link Prediction  with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingchen Li</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+C">Chen Ling</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Zero-shot link prediction (ZSLP) on knowledge graphs aims at automatically
identifying relations between given entities. Existing methods primarily employ
auxiliary information to predict tail entity given head entity and its
relation, yet face challenges due to the occasional unavailability of such
detailed information and the inherent simplicity of predicting tail entities
based on semantic similarities. Even though Large Language Models (LLMs) offer
a promising solution to predict unobserved relations between the head and tail
entity in a zero-shot manner, their performance is still restricted due to the
inability to leverage all the (exponentially many) paths' information between
two entities, which are critical in collectively indicating their relation
types. To address this, in this work, we introduce a Condensed Transition Graph
Framework for Zero-Shot Link Prediction (CTLP), which encodes all the paths'
information in linear time complexity to predict unseen relations between
entities, attaining both efficiency and information preservation. Specifically,
we design a condensed transition graph encoder with theoretical guarantees on
its coverage, expressiveness, and efficiency. It is learned by a transition
graph contrastive learning strategy. Subsequently, we design a soft instruction
tuning to learn and map the all-path embedding to the input of LLMs.
Experimental results show that our proposed CTLP method achieves
state-of-the-art performance on three standard ZSLP datasets
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10781" title="Abstract">arXiv:2402.10781</a> [<a href="/pdf/2402.10781" title="Download PDF">pdf</a>, <a href="/format/2402.10781" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards 6G Evolution: Three Enhancements, Three Innovations, and Three  Major Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+R">Rohit Singh</a>, 
<a href="/search/cs?searchtype=author&query=Kaushik%2C+A">Aryan Kaushik</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+W">Wonjae Shin</a>, 
<a href="/search/cs?searchtype=author&query=Di+Renzo%2C+M">Marco Di Renzo</a>, 
<a href="/search/cs?searchtype=author&query=Sciancalepore%2C+V">Vincenzo Sciancalepore</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Doohwan Lee</a>, 
<a href="/search/cs?searchtype=author&query=Sasaki%2C+H">Hirofumi Sasaki</a>, 
<a href="/search/cs?searchtype=author&query=Shojaeifard%2C+A">Arman Shojaeifard</a>, 
<a href="/search/cs?searchtype=author&query=Dobre%2C+O+A">Octavia A. Dobre</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">Over the past few decades, wireless communication has witnessed remarkable
growth, experiencing several transformative changes. This article aims to
provide a comprehensive overview of wireless communication technologies, from
the foundations to the recent wireless advances. Specifically, we take a
neutral look at the state-of-the-art technologies for 5G and the ongoing
evolutions towards 6G, reviewing the recommendations of the International
Mobile Communication vision for 2030 (IMT-2030). We first highlight specific
features of IMT 2030, including three IMT-2020 extensions (URLLC+, eMBB+, and
mMTC+) and three new innovations (Ubiquitous connectivity and integrating the
new capabilities of sensing &amp; AI with communication functionality). Then, we
delve into three major challenges in implementing 6G, along with global
standardization efforts. Besides, a proof of concept is provided by
demonstrating terahertz (THz) signal transmission using Orbital Angular
Momentum (OAM) multiplexing, which is one of the potential candidates for 6G
and beyond. To inspire further potential research, we conclude by identifying
research opportunities and future visions on IMT-2030 recommendations.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10783" title="Abstract">arXiv:2402.10783</a> [<a href="/pdf/2402.10783" title="Download PDF">pdf</a>, <a href="/format/2402.10783" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Permutation Selectors and their Applications in Ad-Hoc Radio Networks  Protocols
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuschner%2C+J">Jordan Kuschner</a>, 
<a href="/search/cs?searchtype=author&query=Shashwat%2C+Y">Yugarshi Shashwat</a>, 
<a href="/search/cs?searchtype=author&query=Yadav%2C+S">Sarthak Yadav</a>, 
<a href="/search/cs?searchtype=author&query=Chrobak%2C+M">Marek Chrobak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">Selective families of sets, or selectors, are combinatorial tools used to
"isolate" individual members of sets from some set family. Given a set $X$ and
an element $x\in X$, to isolate $x$ from $X$, at least one of the sets in the
selector must intersect $X$ on exactly $x$. We study (k,N)-permutation
selectors which have the property that they can isolate each element of each
$k$-element subset of $\{0,1,...,N-1\}$ in each possible order. These selectors
can be used in protocols for ad-hoc radio networks to more efficiently
disseminate information along multiple hops. In 2004, Gasieniec, Radzik and Xin
gave a construction of a (k,N)-permutation selector of size $O(k^2\log^3 N)$.
This paper improves this by providing a probabilistic construction of a
(k,N)-permutation selector of size $O(k^2\log N)$. Remarkably, this matches the
asymptotic bound for standard strong (k,N)-selectors, that isolate each element
of each set of size $k$, but with no restriction on the order. We then show
that the use of our (k,N)-permutation selector improves the best running time
for gossiping in ad-hoc radio networks by a poly-logarithmic factor.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10787" title="Abstract">arXiv:2402.10787</a> [<a href="/pdf/2402.10787" title="Download PDF">pdf</a>, <a href="/format/2402.10787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for  the Acceleration of Lightweight LLMs on the Edge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xuan Shen</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+Z">Zhenglun Kong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Changdi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zhaoyang Han</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+L">Lei Lu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+P">Peiyan Dong</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+C">Cheng Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chih-hsiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xuehang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+Z">Zhihao Shu</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+W">Wei Niu</a>, 
<a href="/search/cs?searchtype=author&query=Leeser%2C+M">Miriam Leeser</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Pu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanzhi Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Despite the remarkable strides of Large Language Models (LLMs) in various
fields, the wide applications of LLMs on edge devices are limited due to their
massive parameters and computations. To address this, quantization is commonly
adopted to generate lightweight LLMs with efficient computations and fast
inference. However, Post-Training Quantization (PTQ) methods dramatically
degrade in quality when quantizing weights, activations, and KV cache together
to below 8 bits. Besides, many Quantization-Aware Training (QAT) works quantize
model weights, leaving the activations untouched, which do not fully exploit
the potential of quantization for inference acceleration on the edge. In this
paper, we propose EdgeQAT, the Entropy and Distribution Guided QAT for the
optimization of lightweight LLMs to achieve inference acceleration on Edge
devices. We first identify that the performance drop of quantization primarily
stems from the information distortion in quantized attention maps, demonstrated
by the different distributions in quantized query and key of the self-attention
mechanism. Then, the entropy and distribution guided QAT is proposed to
mitigate the information distortion. Moreover, we design a token
importance-aware adaptive method to dynamically quantize the tokens with
different bit widths for further optimization and acceleration. Our extensive
experiments verify the substantial improvements with our framework across
various datasets. Furthermore, we achieve an on-device speedup of up to 2.37x
compared with its FP16 counterparts across multiple edge devices, signaling a
groundbreaking advancement.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10789" title="Abstract">arXiv:2402.10789</a> [<a href="/pdf/2402.10789" title="Download PDF">pdf</a>, <a href="/ps/2402.10789" title="Download PostScript">ps</a>, <a href="/format/2402.10789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Insights into mobile health application market via a content analysis of  marketplace data with machine learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aydin%2C+G">Gokhan Aydin</a>, 
<a href="/search/cs?searchtype=author&query=Silahtaroglu%2C+G">Gokhan Silahtaroglu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> PLoS ONE 2021 16(1): e0244302
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Background Despite the benefits offered by an abundance of health
applications promoted on app marketplaces (e.g., Google Play Store), the wide
adoption of mobile health and e-health apps is yet to come. Objective This
study aims to investigate the current landscape of smartphone apps that focus
on improving and sustaining health and wellbeing. Understanding the categories
that popular apps focus on and the relevant features provided to users, which
lead to higher user scores and downloads will offer insights to enable higher
adoption in the general populace. This study on 1,000 mobile health
applications aims to shed light on the reasons why particular apps are liked
and adopted while many are not. Methods User-generated data (i.e. review
scores) and company-generated data (i.e. app descriptions) were collected from
app marketplaces and manually coded and categorized by two researchers. For
analysis, Artificial Neural Networks, Random Forest and Na\"ive Bayes
Artificial Intelligence algorithms were used. Results The analysis led to
features that attracted more download behavior and higher user scores. The
findings suggest that apps that mention a privacy policy or provide videos in
description lead to higher user scores, whereas free apps with in-app purchase
possibilities, social networking and sharing features and feedback mechanisms
lead to higher number of downloads. Moreover, differences in user scores and
the total number of downloads are detected in distinct subcategories of mobile
health apps. Conclusion This study contributes to the current knowledge of
m-health application use by reviewing mobile health applications using content
analysis and machine learning algorithms. The content analysis adds significant
value by providing classification, keywords and factors that influence download
behavior and user scores in a m-health context.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10790" title="Abstract">arXiv:2402.10790</a> [<a href="/pdf/2402.10790" title="Download PDF">pdf</a>, <a href="/format/2402.10790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs  Miss
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuratov%2C+Y">Yuri Kuratov</a>, 
<a href="/search/cs?searchtype=author&query=Bulatov%2C+A">Aydar Bulatov</a>, 
<a href="/search/cs?searchtype=author&query=Anokhin%2C+P">Petr Anokhin</a>, 
<a href="/search/cs?searchtype=author&query=Sorokin%2C+D">Dmitry Sorokin</a>, 
<a href="/search/cs?searchtype=author&query=Sorokin%2C+A">Artyom Sorokin</a>, 
<a href="/search/cs?searchtype=author&query=Burtsev%2C+M">Mikhail Burtsev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper addresses the challenge of processing long documents using
generative transformer models. To evaluate different approaches, we introduce
BABILong, a new benchmark designed to assess model capabilities in extracting
and processing distributed facts within extensive texts. Our evaluation, which
includes benchmarks for GPT-4 and RAG, reveals that common methods are
effective only for sequences up to $10^4$ elements. In contrast, fine-tuning
GPT-2 with recurrent memory augmentations enables it to handle tasks involving
up to $10^7$ elements. This achievement marks a substantial leap, as it is by
far the longest input processed by any open neural network model to date,
demonstrating a significant improvement in the processing capabilities for long
sequences.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10793" title="Abstract">arXiv:2402.10793</a> [<a href="/pdf/2402.10793" title="Download PDF">pdf</a>, <a href="/format/2402.10793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Masked Attention is All You Need for Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Buterez%2C+D">David Buterez</a>, 
<a href="/search/cs?searchtype=author&query=Janet%2C+J+P">Jon Paul Janet</a>, 
<a href="/search/cs?searchtype=author&query=Oglic%2C+D">Dino Oglic</a>, 
<a href="/search/cs?searchtype=author&query=Lio%2C+P">Pietro Lio</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Graph neural networks (GNNs) and variations of the message passing algorithm
are the predominant means for learning on graphs, largely due to their
flexibility, speed, and satisfactory performance. The design of powerful and
general purpose GNNs, however, requires significant research efforts and often
relies on handcrafted, carefully-chosen message passing operators. Motivated by
this, we propose a remarkably simple alternative for learning on graphs that
relies exclusively on attention. Graphs are represented as node or edge sets
and their connectivity is enforced by masking the attention weight matrix,
effectively creating custom attention patterns for each graph. Despite its
simplicity, masked attention for graphs (MAG) has state-of-the-art performance
on long-range tasks and outperforms strong message passing baselines and much
more involved attention-based methods on over 55 node and graph-level tasks. We
also show significantly better transfer learning capabilities compared to GNNs
and comparable or better time and memory scaling. MAG has sub-linear memory
scaling in the number of nodes or edges, enabling learning on dense graphs and
future-proofing the approach.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10795" title="Abstract">arXiv:2402.10795</a> [<a href="/pdf/2402.10795" title="Download PDF">pdf</a>, <a href="/format/2402.10795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diversified Ensembling: An Experiment in Crowdsourced Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Globus-Harris%2C+I">Ira Globus-Harris</a>, 
<a href="/search/cs?searchtype=author&query=Harrison%2C+D">Declan Harrison</a>, 
<a href="/search/cs?searchtype=author&query=Kearns%2C+M">Michael Kearns</a>, 
<a href="/search/cs?searchtype=author&query=Perona%2C+P">Pietro Perona</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+A">Aaron Roth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Crowdsourced machine learning on competition platforms such as Kaggle is a
popular and often effective method for generating accurate models. Typically,
teams vie for the most accurate model, as measured by overall error on a
holdout set, and it is common towards the end of such competitions for teams at
the top of the leaderboard to ensemble or average their models outside the
platform mechanism to get the final, best global model. In <a href="/abs/2201.10408">arXiv:2201.10408</a>,
the authors developed an alternative crowdsourcing framework in the context of
fair machine learning, in order to integrate community feedback into models
when subgroup unfairness is present and identifiable. There, unlike in
classical crowdsourced ML, participants deliberately specialize their efforts
by working on subproblems, such as demographic subgroups in the service of
fairness. Here, we take a broader perspective on this work: we note that within
this framework, participants may both specialize in the service of fairness and
simply to cater to their particular expertise (e.g., focusing on identifying
bird species in an image classification task). Unlike traditional
crowdsourcing, this allows for the diversification of participants' efforts and
may provide a participation mechanism to a larger range of individuals (e.g. a
machine learning novice who has insight into a specific fairness concern). We
present the first medium-scale experimental evaluation of this framework, with
46 participating teams attempting to generate models to predict income from
American Community Survey data. We provide an empirical analysis of teams'
approaches, and discuss the novel system architecture we developed. From here,
we give concrete guidance for how best to deploy such a framework.
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10797" title="Abstract">arXiv:2402.10797</a> [<a href="/pdf/2402.10797" title="Download PDF">pdf</a>, <a href="/ps/2402.10797" title="Download PostScript">ps</a>, <a href="/format/2402.10797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BlackJAX: Composable Bayesian inference in JAX
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cabezas%2C+A">Alberto Cabezas</a>, 
<a href="/search/cs?searchtype=author&query=Corenflos%2C+A">Adrien Corenflos</a>, 
<a href="/search/cs?searchtype=author&query=Lao%2C+J">Junpeng Lao</a>, 
<a href="/search/cs?searchtype=author&query=Louf%2C+R">R&#xe9;mi Louf</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Companion paper for the library <a href="https://github.com/blackjax-devs/blackjax">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Mathematical Software (cs.MS)</span>; Machine Learning (cs.LG); Computation (stat.CO); Machine Learning (stat.ML)

</div>
<p class="mathjax">BlackJAX is a library implementing sampling and variational inference
algorithms commonly used in Bayesian computation. It is designed for ease of
use, speed, and modularity by taking a functional approach to the algorithms'
implementation. BlackJAX is written in Python, using JAX to compile and run
NumpPy-like samplers and variational methods on CPUs, GPUs, and TPUs. The
library integrates well with probabilistic programming languages by working
directly with the (un-normalized) target log density function. BlackJAX is
intended as a collection of low-level, composable implementations of basic
statistical 'atoms' that can be combined to perform well-defined Bayesian
inference, but also provides high-level routines for ease of use. It is
designed for users who need cutting-edge methods, researchers who want to
create complex sampling methods, and people who want to learn how these work.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10798" title="Abstract">arXiv:2402.10798</a> [<a href="/pdf/2402.10798" title="Download PDF">pdf</a>, <a href="/format/2402.10798" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VATr++: Choose Your Words Wisely for Handwritten Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vanherle%2C+B">Bram Vanherle</a>, 
<a href="/search/cs?searchtype=author&query=Pippi%2C+V">Vittorio Pippi</a>, 
<a href="/search/cs?searchtype=author&query=Cascianelli%2C+S">Silvia Cascianelli</a>, 
<a href="/search/cs?searchtype=author&query=Michiels%2C+N">Nick Michiels</a>, 
<a href="/search/cs?searchtype=author&query=Van+Reeth%2C+F">Frank Van Reeth</a>, 
<a href="/search/cs?searchtype=author&query=Cucchiara%2C+R">Rita Cucchiara</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Styled Handwritten Text Generation (HTG) has received significant attention
in recent years, propelled by the success of learning-based solutions employing
GANs, Transformers, and, preliminarily, Diffusion Models. Despite this surge in
interest, there remains a critical yet understudied aspect - the impact of the
input, both visual and textual, on the HTG model training and its subsequent
influence on performance. This study delves deeper into a cutting-edge
Styled-HTG approach, proposing strategies for input preparation and training
regularization that allow the model to achieve better performance and
generalize better. These aspects are validated through extensive analysis on
several different settings and datasets. Moreover, in this work, we go beyond
performance optimization and address a significant hurdle in HTG research - the
lack of a standardized evaluation protocol. In particular, we propose a
standardization of the evaluation protocol for HTG and conduct a comprehensive
benchmarking of existing approaches. By doing so, we aim to establish a
foundation for fair and meaningful comparisons between HTG strategies,
fostering progress in the field.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10800" title="Abstract">arXiv:2402.10800</a> [<a href="/pdf/2402.10800" title="Download PDF">pdf</a>, <a href="/format/2402.10800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Second Look at the Impact of Passive Voice Requirements on Domain  Modeling: Bayesian Reanalysis of an Experiment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Frattini%2C+J">Julian Frattini</a>, 
<a href="/search/cs?searchtype=author&query=Fucci%2C+D">Davide Fucci</a>, 
<a href="/search/cs?searchtype=author&query=Torkar%2C+R">Richard Torkar</a>, 
<a href="/search/cs?searchtype=author&query=Mendez%2C+D">Daniel Mendez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at the first International Workshop on Methodological Issues with Empirical Studies in Software Engineering (WSESE '24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">The quality of requirements specifications may impact subsequent, dependent
software engineering (SE) activities. However, empirical evidence of this
impact remains scarce and too often superficial as studies abstract from the
phenomena under investigation too much. Two of these abstractions are caused by
the lack of frameworks for causal inference and frequentist methods which
reduce complex data to binary results. In this study, we aim to demonstrate (1)
the use of a causal framework and (2) contrast frequentist methods with more
sophisticated Bayesian statistics for causal inference. To this end, we
reanalyze the only known controlled experiment investigating the impact of
passive voice on the subsequent activity of domain modeling. We follow a
framework for statistical causal inference and employ Bayesian data analysis
methods to re-investigate the hypotheses of the original study. Our results
reveal that the effects observed by the original authors turned out to be much
less significant than previously assumed. This study supports the recent call
to action in SE research to adopt Bayesian data analysis, including causal
frameworks and Bayesian statistics, for more sophisticated causal inference.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10802" title="Abstract">arXiv:2402.10802</a> [<a href="/pdf/2402.10802" title="Download PDF">pdf</a>, <a href="/format/2402.10802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TimeSeriesBench: An Industrial-Grade Benchmark for Time Series Anomaly  Detection Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Si%2C+H">Haotian Si</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+C">Changhua Pei</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+H">Hang Cui</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jingwen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yongqian Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shenglin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jingjing Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haiming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jing Han</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+D">Dan Pei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianhui Li</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+G">Gaogang Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Driven by the proliferation of real-world application scenarios and scales,
time series anomaly detection (TSAD) has attracted considerable scholarly and
industrial interest. However, existing algorithms exhibit a gap in terms of
training paradigm, online detection paradigm, and evaluation criteria when
compared to the actual needs of real-world industrial systems. Firstly, current
algorithms typically train a specific model for each individual time series. In
a large-scale online system with tens of thousands of curves, maintaining such
a multitude of models is impractical. The performance of using merely one
single unified model to detect anomalies remains unknown. Secondly, most TSAD
models are trained on the historical part of a time series and are tested on
its future segment. In distributed systems, however, there are frequent system
deployments and upgrades, with new, previously unseen time series emerging
daily. The performance of testing newly incoming unseen time series on current
TSAD algorithms remains unknown. Lastly, although some papers have conducted
detailed surveys, the absence of an online evaluation platform prevents
answering questions like "Who is the best at anomaly detection at the current
stage?" In this paper, we propose TimeSeriesBench, an industrial-grade
benchmark that we continuously maintain as a leaderboard. On this leaderboard,
we assess the performance of existing algorithms across more than 168
evaluation settings combining different training and testing paradigms,
evaluation metrics and datasets. Through our comprehensive analysis of the
results, we provide recommendations for the future design of anomaly detection
algorithms. To address known issues with existing public datasets, we release
an industrial dataset to the public together with TimeSeriesBench. All code,
data, and the online leaderboard have been made publicly available.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10805" title="Abstract">arXiv:2402.10805</a> [<a href="/pdf/2402.10805" title="Download PDF">pdf</a>, <a href="/format/2402.10805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Cross-Modal Retrieval: Memorizing Images in Multimodal  Language Models for Retrieval and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yongqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+L">Leigang Qu</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+L">Liqiang Nie</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenjie Li</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR)

</div>
<p class="mathjax">The recent advancements in generative language models have demonstrated their
ability to memorize knowledge from documents and recall knowledge to respond to
user queries effectively. Building upon this capability, we propose to enable
multimodal large language models (MLLMs) to memorize and recall images within
their parameters. Given a user query for visual content, the MLLM is
anticipated to "recall" the relevant image from its parameters as the response.
Achieving this target presents notable challenges, including inbuilt visual
memory and visual recall schemes within MLLMs. To address these challenges, we
introduce a generative cross-modal retrieval framework, which assigns unique
identifier strings to represent images and involves two training steps:
learning to memorize and learning to retrieve. The first step focuses on
training the MLLM to memorize the association between images and their
respective identifiers. The latter step teaches the MLLM to generate the
corresponding identifier of the target image, given the textual query input. By
memorizing images in MLLMs, we introduce a new paradigm to cross-modal
retrieval, distinct from previous discriminative approaches. The experiments
demonstrate that the generative paradigm performs effectively and efficiently
even with large-scale image candidate sets.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10806" title="Abstract">arXiv:2402.10806</a> [<a href="/pdf/2402.10806" title="Download PDF">pdf</a>, <a href="/format/2402.10806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Streaming Algorithms for Connectivity Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+C">Ce Jin</a>, 
<a href="/search/cs?searchtype=author&query=Kapralov%2C+M">Michael Kapralov</a>, 
<a href="/search/cs?searchtype=author&query=Mahabadi%2C+S">Sepideh Mahabadi</a>, 
<a href="/search/cs?searchtype=author&query=Vakilian%2C+A">Ali Vakilian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We study the $k$-connectivity augmentation problem ($k$-CAP) in the
single-pass streaming model. Given a $(k-1)$-edge connected graph $G=(V,E)$
that is stored in memory, and a stream of weighted edges $L$ with weights in
$\{0,1,\dots,W\}$, the goal is to choose a minimum weight subset $L'\subseteq
L$ such that $G'=(V,E\cup L')$ is $k$-edge connected. We give a
$(2+\epsilon)$-approximation algorithm for this problem which requires to store
$O(\epsilon^{-1} n\log n)$ words. Moreover, we show our result is tight: Any
algorithm with better than $2$-approximation for the problem requires
$\Omega(n^2)$ bits of space even when $k=2$. This establishes a gap between the
optimal approximation factor one can obtain in the streaming vs the offline
setting for $k$-CAP.
<br />We further consider a natural generalization to the fully streaming model
where both $E$ and $L$ arrive in the stream in an arbitrary order. We show that
this problem has a space lower bound that matches the best possible size of a
spanner of the same approximation ratio. Following this, we give improved
results for spanners on weighted graphs: We show a streaming algorithm that
finds a $(2t-1+\epsilon)$-approximate weighted spanner of size at most
$O(\epsilon^{-1} n^{1+1/t}\log n)$ for integer $t$, whereas the best prior
streaming algorithm for spanner on weighted graphs had size depending on $\log
W$. Using our spanner result, we provide an optimal $O(t)$-approximation for
$k$-CAP in the fully streaming model with $O(nk + n^{1+1/t})$ words of space.
<br />Finally we apply our results to network design problems such as Steiner tree
augmentation problem (STAP), $k$-edge connected spanning subgraph ($k$-ECSS),
and the general Survivable Network Design problem (SNDP). In particular, we
show a single-pass $O(t\log k)$-approximation for SNDP using $O(kn^{1+1/t})$
words of space, where $k$ is the maximum connectivity requirement.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10809" title="Abstract">arXiv:2402.10809</a> [<a href="/pdf/2402.10809" title="Download PDF">pdf</a>, <a href="/format/2402.10809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A lattice Boltzmann method for non-Newtonian blood flow in coiled  intracranial aneurysms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Horvat%2C+M">Medeea Horvat</a>, 
<a href="/search/math?searchtype=author&query=Lunowa%2C+S+B">Stephan B. Lunowa</a>, 
<a href="/search/math?searchtype=author&query=Sytnyk%2C+D">Dmytro Sytnyk</a>, 
<a href="/search/math?searchtype=author&query=Wohlmuth%2C+B">Barbara Wohlmuth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">Intracranial aneurysms are the leading cause of stroke. One of the
established treatment approaches is the embolization induced by coil insertion.
However, the prediction of treatment and subsequent changed flow
characteristics in the aneurysm, is still an open problem. In this work, we
present an approach based on patient specific geometry and parameters including
a coil representation as inhomogeneous porous medium. The model consists of the
volume-averaged Navier-Stokes equations including the non-Newtonian blood
rheology. We solve these equations using a problem-adapted lattice Boltzmann
method and present a comparison between fully-resolved and volume-averaged
simulations. The results indicate the validity of the model. Overall, this
workflow allows for patient specific assessment of the flow due to potential
treatment.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10810" title="Abstract">arXiv:2402.10810</a> [<a href="/pdf/2402.10810" title="Download PDF">pdf</a>, <a href="/ps/2402.10810" title="Download PostScript">ps</a>, <a href="/format/2402.10810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Double Duality: Variational Primal-Dual Policy Optimization for  Constrained Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zihao Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Boyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhuoran Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhaoran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengdi Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the Constrained Convex Markov Decision Process (MDP), where the goal
is to minimize a convex functional of the visitation measure, subject to a
convex constraint. Designing algorithms for a constrained convex MDP faces
several challenges, including (1) handling the large state space, (2) managing
the exploration/exploitation tradeoff, and (3) solving the constrained
optimization where the objective and the constraint are both nonlinear
functions of the visitation measure. In this work, we present a model-based
algorithm, Variational Primal-Dual Policy Optimization (VPDPO), in which
Lagrangian and Fenchel duality are implemented to reformulate the original
constrained problem into an unconstrained primal-dual optimization. Moreover,
the primal variables are updated by model-based value iteration following the
principle of Optimism in the Face of Uncertainty (OFU), while the dual
variables are updated by gradient ascent. Moreover, by embedding the visitation
measure into a finite-dimensional space, we can handle large state spaces by
incorporating function approximation. Two notable examples are (1) Kernelized
Nonlinear Regulators and (2) Low-rank MDPs. We prove that with an optimistic
planning oracle, our algorithm achieves sublinear regret and constraint
violation in both cases and can attain the globally optimal policy of the
original constrained problem.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10811" title="Abstract">arXiv:2402.10811</a> [<a href="/pdf/2402.10811" title="Download PDF">pdf</a>, <a href="/format/2402.10811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantifying the Persona Effect in LLM Simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+T">Tiancheng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Collier%2C+N">Nigel Collier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Large language models (LLMs) have shown remarkable promise in simulating
human language use and behavior. In this study, we delve into the intersection
of persona variables and the capability of LLMs to simulate different
perspectives. We find that persona variables can explain &lt;10\% variance in
annotations in existing subjective NLP datasets. Nonetheless, incorporating
them via prompting in LLMs provides modest improvement. Persona prompting is
most effective on data samples where disagreements among annotators are
frequent yet confined to a limited range. A linear correlation exists: the more
persona variables influence human annotations, the better LLMs predictions are
using persona prompting. However, when the utility of persona variables is low
(i.e., explaining &lt;10\% of human annotations), persona prompting has little
effect. Most subjective NLP datasets fall into this category, casting doubt on
simulating diverse perspectives in the current NLP landscape.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10812" title="Abstract">arXiv:2402.10812</a> [<a href="/pdf/2402.10812" title="Download PDF">pdf</a>, <a href="/format/2402.10812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Hybrid Question Answering via Program-based Prompting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Q">Qi Shi</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+H">Han Cui</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haofeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qingfu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Che%2C+W">Wanxiang Che</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Ting Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Question answering over heterogeneous data requires reasoning over diverse
sources of data, which is challenging due to the large scale of information and
organic coupling of heterogeneous data. Various approaches have been proposed
to address these challenges. One approach involves training specialized
retrievers to select relevant information, thereby reducing the input length.
Another approach is to transform diverse modalities of data into a single
modality, simplifying the task difficulty and enabling more straightforward
processing. In this paper, we propose HProPro, a novel program-based prompting
framework for the hybrid question answering task. HProPro follows the code
generation and execution paradigm. In addition, HProPro integrates various
functions to tackle the hybrid reasoning scenario. Specifically, HProPro
contains function declaration and function implementation to perform hybrid
information-seeking over data from various sources and modalities, which
enables reasoning over such data without training specialized retrievers or
performing modal transformations. Experimental results on two typical hybrid
question answering benchmarks HybridQA and MultiModalQA demonstrate the
effectiveness of HProPro: it surpasses all baseline systems and achieves the
best performances in the few-shot settings on both datasets.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10814" title="Abstract">arXiv:2402.10814</a> [<a href="/pdf/2402.10814" title="Download PDF">pdf</a>, <a href="/format/2402.10814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Associative Memories in the Feature Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Salvatori%2C+T">Tommaso Salvatori</a>, 
<a href="/search/cs?searchtype=author&query=Millidge%2C+B">Beren Millidge</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yuhang Song</a>, 
<a href="/search/cs?searchtype=author&query=Bogacz%2C+R">Rafal Bogacz</a>, 
<a href="/search/cs?searchtype=author&query=Lukasiewicz%2C+T">Thomas Lukasiewicz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 Pages, 4 Figures, accepted for publication at ECAI 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">An autoassociative memory model is a function that, given a set of data
points, takes as input an arbitrary vector and outputs the most similar data
point from the memorized set. However, popular memory models fail to retrieve
images even when the corruption is mild and easy to detect for a human
evaluator. This is because similarities are evaluated in the raw pixel space,
which does not contain any semantic information about the images. This problem
can be easily solved by computing \emph{similarities} in an embedding space
instead of the pixel space. We show that an effective way of computing such
embeddings is via a network pretrained with a contrastive loss. As the
dimension of embedding spaces is often significantly smaller than the pixel
space, we also have a faster computation of similarity scores. We test this
method on complex datasets such as CIFAR10 and STL10. An additional drawback of
current models is the need of storing the whole dataset in the pixel space,
which is often extremely large. We relax this condition and propose a class of
memory models that only stores low-dimensional semantic embeddings, and uses
them to retrieve similar, but not identical, memories. We demonstrate a proof
of concept of this method on a simple task on the MNIST dataset.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10815" title="Abstract">arXiv:2402.10815</a> [<a href="/pdf/2402.10815" title="Download PDF">pdf</a>, <a href="/format/2402.10815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Core Stability in Additively Separable Hedonic Games of Low Treewidth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hanaka%2C+T">Tesshu Hanaka</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6hler%2C+N">Noleen K&#xf6;hler</a>, 
<a href="/search/cs?searchtype=author&query=Lampis%2C+M">Michael Lampis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC); Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Additively Separable Hedonic Game (ASHG) are coalition-formation games where
we are given a graph whose vertices represent $n$ selfish agents and the weight
of each edge $uv$ denotes how much agent $u$ gains (or loses) when she is
placed in the same coalition as agent $v$. We revisit the computational
complexity of the well-known notion of core stability of ASHGs, where the goal
is to construct a partition of the agents into coalitions such that no group of
agents would prefer to diverge from the given partition and form a new
(blocking) coalition. Since both finding a core stable partition and verifying
that a given partition is core stable are intractable problems
($\Sigma_2^p$-complete and coNP-complete respectively) we study their
complexity from the point of view of structural parameterized complexity, using
standard graph-theoretic parameters, such as treewidth.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10816" title="Abstract">arXiv:2402.10816</a> [<a href="/pdf/2402.10816" title="Download PDF">pdf</a>, <a href="/format/2402.10816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TernaryVote: Differentially Private, Communication Efficient, and  Byzantine Resilient Distributed Optimization on Heterogeneous Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+R">Richeng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yujie Gu</a>, 
<a href="/search/cs?searchtype=author&query=Yue%2C+K">Kai Yue</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xiaofan He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaoyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+H">Huaiyu Dai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC); Signal Processing (eess.SP)

</div>
<p class="mathjax">Distributed training of deep neural networks faces three critical challenges:
privacy preservation, communication efficiency, and robustness to fault and
adversarial behaviors. Although significant research efforts have been devoted
to addressing these challenges independently, their synthesis remains less
explored. In this paper, we propose TernaryVote, which combines a ternary
compressor and the majority vote mechanism to realize differential privacy,
gradient compression, and Byzantine resilience simultaneously. We theoretically
quantify the privacy guarantee through the lens of the emerging f-differential
privacy (DP) and the Byzantine resilience of the proposed algorithm.
Particularly, in terms of privacy guarantees, compared to the existing
sign-based approach StoSign, the proposed method improves the dimension
dependence on the gradient size and enjoys privacy amplification by mini-batch
sampling while ensuring a comparable convergence rate. We also prove that
TernaryVote is robust when less than 50% of workers are blind attackers, which
matches that of SIGNSGD with majority vote. Extensive experimental results
validate the effectiveness of the proposed algorithm.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10818" title="Abstract">arXiv:2402.10818</a> [<a href="/pdf/2402.10818" title="Download PDF">pdf</a>, <a href="/format/2402.10818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trading off Consistency and Dimensionality of Convex Surrogates for the  Mode
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nueve%2C+E">Enrique Nueve</a>, 
<a href="/search/cs?searchtype=author&query=Waggoner%2C+B">Bo Waggoner</a>, 
<a href="/search/cs?searchtype=author&query=Kimpara%2C+D">Dhamma Kimpara</a>, 
<a href="/search/cs?searchtype=author&query=Finocchiaro%2C+J">Jessie Finocchiaro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">In multiclass classification over $n$ outcomes, the outcomes must be embedded
into the reals with dimension at least $n-1$ in order to design a consistent
surrogate loss that leads to the "correct" classification, regardless of the
data distribution. For large $n$, such as in information retrieval and
structured prediction tasks, optimizing a surrogate in $n-1$ dimensions is
often intractable. We investigate ways to trade off surrogate loss dimension,
the number of problem instances, and restricting the region of consistency in
the simplex for multiclass classification. Following past work, we examine an
intuitive embedding procedure that maps outcomes into the vertices of convex
polytopes in a low-dimensional surrogate space. We show that full-dimensional
subsets of the simplex exist around each point mass distribution for which
consistency holds, but also, with less than $n-1$ dimensions, there exist
distributions for which a phenomenon called hallucination occurs, which is when
the optimal report under the surrogate loss is an outcome with zero
probability. Looking towards application, we derive a result to check if
consistency holds under a given polytope embedding and low-noise assumption,
providing insight into when to use a particular embedding. We provide examples
of embedding $n = 2^{d}$ outcomes into the $d$-dimensional unit cube and $n =
d!$ outcomes into the $d$-dimensional permutahedron under low-noise
assumptions. Finally, we demonstrate that with multiple problem instances, we
can learn the mode with $\frac{n}{2}$ dimensions over the whole simplex.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10820" title="Abstract">arXiv:2402.10820</a> [<a href="/pdf/2402.10820" title="Download PDF">pdf</a>, <a href="/format/2402.10820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Goal-Conditioned Offline Reinforcement Learning via Metric Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reichlin%2C+A">Alfredo Reichlin</a>, 
<a href="/search/cs?searchtype=author&query=Vasco%2C+M">Miguel Vasco</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Hang Yin</a>, 
<a href="/search/cs?searchtype=author&query=Kragic%2C+D">Danica Kragic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In this work, we address the problem of learning optimal behavior from
sub-optimal datasets in the context of goal-conditioned offline reinforcement
learning. To do so, we propose a novel way of approximating the optimal value
function for goal-conditioned offline RL problems under sparse rewards,
symmetric and deterministic actions. We study a property for representations to
recover optimality and propose a new optimization objective that leads to such
property. We use the learned value function to guide the learning of a policy
in an actor-critic fashion, a method we name MetricRL. Experimentally, we show
how our method consistently outperforms other offline RL baselines in learning
from sub-optimal offline datasets. Moreover, we show the effectiveness of our
method in dealing with high-dimensional observations and in multi-goal tasks.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10821" title="Abstract">arXiv:2402.10821</a> [<a href="/pdf/2402.10821" title="Download PDF">pdf</a>, <a href="/format/2402.10821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Class-Imbalanced Diffusion Model Via Overlap Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+D">Divin Yan</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+L">Lu Qi</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+V+T">Vincent Tao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Ming-Hsuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+M">Meng Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technique Report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Diffusion models have made significant advances recently in high-quality
image synthesis and related tasks. However, diffusion models trained on
real-world datasets, which often follow long-tailed distributions, yield
inferior fidelity for tail classes. Deep generative models, including diffusion
models, are biased towards classes with abundant training images. To address
the observed appearance overlap between synthesized images of rare classes and
tail classes, we propose a method based on contrastive learning to minimize the
overlap between distributions of synthetic images for different classes. We
show variants of our probabilistic contrastive learning method can be applied
to any class conditional diffusion model. We show significant improvement in
image synthesis using our loss for multiple datasets with long-tailed
distribution. Extensive experimental results demonstrate that the proposed
method can effectively handle imbalanced data for diffusion-based generation
and classification models. Our code and datasets will be publicly available at
https://github.com/yanliang3612/DiffROP.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10822" title="Abstract">arXiv:2402.10822</a> [<a href="/pdf/2402.10822" title="Download PDF">pdf</a>, <a href="/format/2402.10822" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QKDNetSim+: Improvement of the Quantum Network Simulator for NS-3
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soler%2C+D">David Soler</a> (1), 
<a href="/search/cs?searchtype=author&query=Cillero%2C+I">Iv&#xe1;n Cillero</a> (1), 
<a href="/search/cs?searchtype=author&query=Dafonte%2C+C">Carlos Dafonte</a> (1), 
<a href="/search/cs?searchtype=author&query=Fern%C3%A1ndez-Veiga%2C+M">Manuel Fern&#xe1;ndez-Veiga</a> (2), 
<a href="/search/cs?searchtype=author&query=Fern%C3%A1ndez-Vilas%2C+A">Ana Fern&#xe1;ndez-Vilas</a> (2), 
<a href="/search/cs?searchtype=author&query=N%C3%B3voa%2C+F+J">Francisco J. N&#xf3;voa</a> (1) ((1) CITIC, Universidade da Coru&#x148;a, A Coru&#x148;a, Spain, (2) atlanTTic, Universidade de Vigo, Vigo, Spain)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 5 figures, preprint submitted to SoftwareX
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">The first Quantum Key Distribution (QKD) networks are currently being
deployed, but the implementation cost is still prohibitive for most
researchers. As such, there is a need for realistic QKD network simulators. The
\textit{QKDNetSim} module for the network simulator NS-3 focuses on the
representation of packets and the management of key material in a QKD network
at the application layer. Although QKDNetSim's representation of a QKD network
is insightful, some its components lack the depth that would allow the
simulator to faithfully represent the behaviour of a real quantum network. In
this work, we analyse QKDNetSim's architecture to identify its limitations, and
we present an enhanced version of QKDNetSim in which some of its components
have been modified to provide a more realistic simulation environment.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10825" title="Abstract">arXiv:2402.10825</a> [<a href="/pdf/2402.10825" title="Download PDF">pdf</a>, <a href="/format/2402.10825" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nash Equilibrium and Learning Dynamics in Three-Player Matching  $m$-Action Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fujimoto%2C+Y">Yuma Fujimoto</a>, 
<a href="/search/cs?searchtype=author&query=Ariu%2C+K">Kaito Ariu</a>, 
<a href="/search/cs?searchtype=author&query=Abe%2C+K">Kenshi Abe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 4 figures (main), 9 pages, 1 figure (appendix)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Multiagent Systems (cs.MA); Optimization and Control (math.OC); Chaotic Dynamics (nlin.CD)

</div>
<p class="mathjax">Learning in games discusses the processes where multiple players learn their
optimal strategies through the repetition of game plays. The dynamics of
learning between two players in zero-sum games, such as matching pennies, where
their benefits are competitive, have already been well analyzed. However, it is
still unexplored and challenging to analyze the dynamics of learning among
three players. In this study, we formulate a minimalistic game where three
players compete to match their actions with one another. Although interaction
among three players diversifies and complicates the Nash equilibria, we fully
analyze the equilibria. We also discuss the dynamics of learning based on some
famous algorithms categorized into Follow the Regularized Leader. From both
theoretical and experimental aspects, we characterize the dynamics by
categorizing three-player interactions into three forces to synchronize their
actions, switch their actions rotationally, and seek competition.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10828" title="Abstract">arXiv:2402.10828</a> [<a href="/pdf/2402.10828" title="Download PDF">pdf</a>, <a href="/format/2402.10828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented  In-Context Learning in Multi-Modal Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jianhao Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Shuyang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Omeiza%2C+D">Daniel Omeiza</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B">Bo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Newman%2C+P">Paul Newman</a>, 
<a href="/search/cs?searchtype=author&query=Kunze%2C+L">Lars Kunze</a>, 
<a href="/search/cs?searchtype=author&query=Gadd%2C+M">Matthew Gadd</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Robots powered by 'blackbox' models need to provide human-understandable
explanations which we can trust. Hence, explainability plays a critical role in
trustworthy autonomous decision-making to foster transparency and acceptance
among end users, especially in complex autonomous driving. Recent advancements
in Multi-Modal Large Language models (MLLMs) have shown promising potential in
enhancing the explainability as a driving agent by producing control
predictions along with natural language explanations. However, severe data
scarcity due to expensive annotation costs and significant domain gaps between
different datasets makes the development of a robust and generalisable system
an extremely challenging task. Moreover, the prohibitively expensive training
requirements of MLLM and the unsolved problem of catastrophic forgetting
further limit their generalisability post-deployment. To address these
challenges, we present RAG-Driver, a novel retrieval-augmented multi-modal
large language model that leverages in-context learning for high-performance,
explainable, and generalisable autonomous driving. By grounding in retrieved
expert demonstration, we empirically validate that RAG-Driver achieves
state-of-the-art performance in producing driving action explanations,
justifications, and control signal prediction. More importantly, it exhibits
exceptional zero-shot generalisation capabilities to unseen environments
without further training endeavours.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10830" title="Abstract">arXiv:2402.10830</a> [<a href="/pdf/2402.10830" title="Download PDF">pdf</a>, <a href="/format/2402.10830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Designing a Taxonomy for Smart Tourism Tools
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Galv%C3%A3o%2C+A">Ant&#xf3;nio Galv&#xe3;o</a>, 
<a href="/search/cs?searchtype=author&query=Abreu%2C+F+B+e">Fernando Brito e Abreu</a>, 
<a href="/search/cs?searchtype=author&query=de+Melo%2C+J+J">Jo&#xe3;o Joanaz de Melo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Smart tourism (ST) stems from the concepts of e-tourism - focused on the
digitalization of processes within the tourism industry, and digital tourism -
also considering the digitalization within the tourist experience. The earlier
ST references found regard ST Destinations and emerge from the development of
Smart Cities.
<br />Our initial literature review on the ST concept and Smart Tourism Tools (STT)
revealed significant research uncertainties: ST is poorly defined and
frequently linked to the concept of Smart Cities; different authors have
different, sometimes contradictory, views on the goals of ST; STT claims are
often only based on technological aspects, and their "smartness" is difficult
to evaluate; often the term "Smart" describes developments fueled by
cutting-edge technologies, which lose that status after a few years.
<br />This chapter is part of the ongoing initiative to build an online observatory
that provides a comprehensive view of STTs' offerings in Europe, known as the
European STT Observatory. To achieve this, the observatory requires
methodologies and tools to evaluate "smartness" based on a sound definition of
ST and STT, while also being able to adapt to technological advancements. In
this chapter, we present the results of a participatory approach where we
invited ST experts from around the world to help us achieve this level of
soundness. Our goal is to make a valuable contribution to the discussion on the
definition of ST and STT.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10835" title="Abstract">arXiv:2402.10835</a> [<a href="/pdf/2402.10835" title="Download PDF">pdf</a>, <a href="/format/2402.10835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time Series Forecasting with LLMs: Understanding and Enhancing Model  Capabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+M">Mingyu Jin</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Hua Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qinkai Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chengzhi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Suiyuan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongfeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+M">Mengnan Du</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have been applied in many fields with rapid
development in recent years. As a classic machine learning task, time series
forecasting has recently received a boost from LLMs. However, there is a
research gap in the LLMs' preferences in this field. In this paper, by
comparing LLMs with traditional models, many properties of LLMs in time series
prediction are found. For example, our study shows that LLMs excel in
predicting time series with clear patterns and trends but face challenges with
datasets lacking periodicity. We explain our findings through designing prompts
to require LLMs to tell the period of the datasets. In addition, the input
strategy is investigated, and it is found that incorporating external knowledge
and adopting natural language paraphrases positively affects the predictive
performance of LLMs for time series. Overall, this study contributes to insight
into the advantages and limitations of LLMs in time series forecasting under
different conditions.
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10837" title="Abstract">arXiv:2402.10837</a> [<a href="/pdf/2402.10837" title="Download PDF">pdf</a>, <a href="/format/2402.10837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pedipulate: Enabling Manipulation Skills using a Quadruped Robot&#x27;s Leg
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arm%2C+P">Philip Arm</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+M">Mayank Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Kolvenbach%2C+H">Hendrik Kolvenbach</a>, 
<a href="/search/cs?searchtype=author&query=Hutter%2C+M">Marco Hutter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project website: <a href="https://sites.google.com/leggedrobotics.com/pedipulate">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
<p class="mathjax">Legged robots have the potential to become vital in maintenance, home
support, and exploration scenarios. In order to interact with and manipulate
their environments, most legged robots are equipped with a dedicated robot arm,
which means additional mass and mechanical complexity compared to standard
legged robots. In this work, we explore pedipulation - using the legs of a
legged robot for manipulation. By training a reinforcement learning policy that
tracks position targets for one foot, we enable a dedicated pedipulation
controller that is robust to disturbances, has a large workspace through
whole-body behaviors, and can reach far-away targets with gait emergence,
enabling loco-pedipulation. By deploying our controller on a quadrupedal robot
using teleoperation, we demonstrate various real-world tasks such as door
opening, sample collection, and pushing obstacles. We demonstrate load carrying
of more than 2.0 kg at the foot. Additionally, the controller is robust to
interaction forces at the foot, disturbances at the base, and slippery contact
surfaces. Videos of the experiments are available at
https://sites.google.com/leggedrobotics.com/pedipulate.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10846" title="Abstract">arXiv:2402.10846</a> [<a href="/pdf/2402.10846" title="Download PDF">pdf</a>, <a href="/format/2402.10846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedD2S: Personalized Data-Free Federated Knowledge Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Atapour%2C+K">Kawa Atapour</a>, 
<a href="/search/cs?searchtype=author&query=Seyedmohammadi%2C+S+J">S. Jamal Seyedmohammadi</a>, 
<a href="/search/cs?searchtype=author&query=Abouei%2C+J">Jamshid Abouei</a>, 
<a href="/search/cs?searchtype=author&query=Mohammadi%2C+A">Arash Mohammadi</a>, 
<a href="/search/cs?searchtype=author&query=Plataniotis%2C+K+N">Konstantinos N. Plataniotis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">This paper addresses the challenge of mitigating data heterogeneity among
clients within a Federated Learning (FL) framework. The model-drift issue,
arising from the noniid nature of client data, often results in suboptimal
personalization of a global model compared to locally trained models for each
client. To tackle this challenge, we propose a novel approach named FedD2S for
Personalized Federated Learning (pFL), leveraging knowledge distillation.
FedD2S incorporates a deep-to-shallow layer-dropping mechanism in the data-free
knowledge distillation process to enhance local model personalization. Through
extensive simulations on diverse image datasets-FEMNIST, CIFAR10, CINIC0, and
CIFAR100-we compare FedD2S with state-of-the-art FL baselines. The proposed
approach demonstrates superior performance, characterized by accelerated
convergence and improved fairness among clients. The introduced layer-dropping
technique effectively captures personalized knowledge, resulting in enhanced
performance compared to alternative FL models. Moreover, we investigate the
impact of key hyperparameters, such as the participation ratio and
layer-dropping rate, providing valuable insights into the optimal configuration
for FedD2S. The findings demonstrate the efficacy of adaptive layer-dropping in
the knowledge distillation process to achieve enhanced personalization and
performance across diverse datasets and tasks.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10847" title="Abstract">arXiv:2402.10847</a> [<a href="/pdf/2402.10847" title="Download PDF">pdf</a>, <a href="/format/2402.10847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancement-Driven Pretraining for Robust Fingerprint Representation  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gavas%2C+E">Ekta Gavas</a>, 
<a href="/search/cs?searchtype=author&query=Olpadkar%2C+K">Kaustubh Olpadkar</a>, 
<a href="/search/cs?searchtype=author&query=Namboodiri%2C+A">Anoop Namboodiri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures, Accepted at 19th VISIGRAPP 2024: VISAPP conference
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 19th International Joint Conference on Computer
  Vision, Imaging and Computer Graphics Theory and Applications (VISIGRAPP
  2024) - Volume 2: VISAPP, ISBN 978-989-758-679-8, ISSN 2184-4321, pages
  821-828
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Fingerprint recognition stands as a pivotal component of biometric
technology, with diverse applications from identity verification to advanced
search tools. In this paper, we propose a unique method for deriving robust
fingerprint representations by leveraging enhancement-based pre-training.
Building on the achievements of U-Net-based fingerprint enhancement, our method
employs a specialized encoder to derive representations from fingerprint images
in a self-supervised manner. We further refine these representations, aiming to
enhance the verification capabilities. Our experimental results, tested on
publicly available fingerprint datasets, reveal a marked improvement in
verification performance against established self-supervised training
techniques. Our findings not only highlight the effectiveness of our method but
also pave the way for potential advancements. Crucially, our research indicates
that it is feasible to extract meaningful fingerprint representations from
degraded images without relying on enhanced samples.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10850" title="Abstract">arXiv:2402.10850</a> [<a href="/pdf/2402.10850" title="Download PDF">pdf</a>, <a href="/format/2402.10850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Error Checking for Sparse Systolic Tensor Arrays
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peltekis%2C+C">Christodoulos Peltekis</a>, 
<a href="/search/cs?searchtype=author&query=Filippas%2C+D">Dionysios Filippas</a>, 
<a href="/search/cs?searchtype=author&query=Dimitrakopoulos%2C+G">Giorgos Dimitrakopoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AICAS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Structured sparsity is an efficient way to prune the complexity of modern
Machine Learning (ML) applications and to simplify the handling of sparse data
in hardware. In such cases, the acceleration of structured-sparse ML models is
handled by sparse systolic tensor arrays. The increasing prevalence of ML in
safety-critical systems requires enhancing the sparse tensor arrays with online
error detection for managing random hardware failures. Algorithm-based fault
tolerance has been proposed as a low-cost mechanism to check online the result
of computations against random hardware failures. In this work, we address a
key architectural challenge with structured-sparse tensor arrays: how to
provide online error checking for a range of structured sparsity levels while
maintaining high utilization of the hardware. Experimental results highlight
the minimum hardware overhead incurred by the proposed checking logic and its
error detection properties after injecting random hardware faults on sparse
tensor arrays that execute layers of ResNet50 CNN.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10853" title="Abstract">arXiv:2402.10853</a> [<a href="/pdf/2402.10853" title="Download PDF">pdf</a>, <a href="/format/2402.10853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering and exploring cases of educational source code plagiarism  with Dolos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maertens%2C+R">Rien Maertens</a>, 
<a href="/search/cs?searchtype=author&query=Van+Neyghem%2C+M">Maarten Van Neyghem</a>, 
<a href="/search/cs?searchtype=author&query=Geldhof%2C+M">Maxiem Geldhof</a>, 
<a href="/search/cs?searchtype=author&query=Van+Petegem%2C+C">Charlotte Van Petegem</a>, 
<a href="/search/cs?searchtype=author&query=Strijbol%2C+N">Niko Strijbol</a>, 
<a href="/search/cs?searchtype=author&query=Dawyndt%2C+P">Peter Dawyndt</a>, 
<a href="/search/cs?searchtype=author&query=Mesuere%2C+B">Bart Mesuere</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Source code plagiarism is a significant issue in educational practice, and
educators need user-friendly tools to cope with such academic dishonesty. This
article introduces the latest version of Dolos, a state-of-the-art ecosystem of
tools for detecting and preventing plagiarism in educational source code. In
this new version, the primary focus has been on enhancing the user experience.
Educators can now run the entire plagiarism detection pipeline from a new web
app in their browser, eliminating the need for any installation or
configuration. Completely redesigned analytics dashboards provide an instant
assessment of whether a collection of source files contains suspected cases of
plagiarism and how widespread plagiarism is within the collection. The
dashboards support hierarchically structured navigation to facilitate zooming
in and out of suspect cases. Clusters are an essential new component of the
dashboard design, reflecting the observation that plagiarism can occur among
larger groups of students. To meet various user needs, the Dolos software stack
for source code plagiarism detections now includes a web interface, a JSON
application programming interface (API), a command line interface (CLI), a
JavaScript library and a preconfigured Docker container. Clear documentation
and a free-to-use instance of the web app can be found at
https://dolos.ugent.be. The source code is also available on GitHub.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10855" title="Abstract">arXiv:2402.10855</a> [<a href="/pdf/2402.10855" title="Download PDF">pdf</a>, <a href="/format/2402.10855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Control Color: Multimodal Diffusion-based Interactive Image Colorization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+Z">Zhexin Liang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhaochen Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Shangchen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chongyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Loy%2C+C+C">Chen Change Loy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://zhexinliang.github.io/Control_Color/">this https URL</a>; Demo Video: <a href="https://youtu.be/tSCwA-srl8Q">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Despite the existence of numerous colorization methods, several limitations
still exist, such as lack of user interaction, inflexibility in local
colorization, unnatural color rendering, insufficient color variation, and
color overflow. To solve these issues, we introduce Control Color (CtrlColor),
a multi-modal colorization method that leverages the pre-trained Stable
Diffusion (SD) model, offering promising capabilities in highly controllable
interactive image colorization. While several diffusion-based methods have been
proposed, supporting colorization in multiple modalities remains non-trivial.
In this study, we aim to tackle both unconditional and conditional image
colorization (text prompts, strokes, exemplars) and address color overflow and
incorrect color within a unified framework. Specifically, we present an
effective way to encode user strokes to enable precise local color manipulation
and employ a practical way to constrain the color distribution similar to
exemplars. Apart from accepting text prompts as conditions, these designs add
versatility to our approach. We also introduce a novel module based on
self-attention and a content-guided deformable autoencoder to address the
long-standing issues of color overflow and inaccurate coloring. Extensive
comparisons show that our model outperforms state-of-the-art image colorization
methods both qualitatively and quantitatively.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10857" title="Abstract">arXiv:2402.10857</a> [<a href="/pdf/2402.10857" title="Download PDF">pdf</a>, <a href="/ps/2402.10857" title="Download PostScript">ps</a>, <a href="/format/2402.10857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JetTrain: IDE-Native Machine Learning Experiments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Trofimov%2C+A">Artem Trofimov</a>, 
<a href="/search/cs?searchtype=author&query=Kostyukov%2C+M">Mikhail Kostyukov</a>, 
<a href="/search/cs?searchtype=author&query=Ugdyzhekov%2C+S">Sergei Ugdyzhekov</a>, 
<a href="/search/cs?searchtype=author&query=Ponomareva%2C+N">Natalia Ponomareva</a>, 
<a href="/search/cs?searchtype=author&query=Naumov%2C+I">Igor Naumov</a>, 
<a href="/search/cs?searchtype=author&query=Melekhovets%2C+M">Maksim Melekhovets</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IDE workshop @ ICSE 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Integrated development environments (IDEs) are prevalent code-writing and
debugging tools. However, they have yet to be widely adopted for launching
machine learning (ML) experiments. This work aims to fill this gap by
introducing JetTrain, an IDE-integrated tool that delegates specific tasks from
an IDE to remote computational resources. A user can write and debug code
locally and then seamlessly run it remotely using on-demand hardware. We argue
that this approach can lower the entry barrier for ML training problems and
increase experiment throughput.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10861" title="Abstract">arXiv:2402.10861</a> [<a href="/pdf/2402.10861" title="Download PDF">pdf</a>, <a href="/format/2402.10861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hypergraph Connectivity Augmentation in Strongly Polynomial Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=B%C3%A9rczi%2C+K">Krist&#xf3;f B&#xe9;rczi</a>, 
<a href="/search/cs?searchtype=author&query=Chandrasekaran%2C+K">Karthekeyan Chandrasekaran</a>, 
<a href="/search/cs?searchtype=author&query=Kir%C3%A1ly%2C+T">Tam&#xe1;s Kir&#xe1;ly</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+S">Shubhang Kulkarni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2307.08555">arXiv:2307.08555</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">We consider hypergraph network design problems where the goal is to construct
a hypergraph that satisfies certain connectivity requirements. For graph
network design problems where the goal is to construct a graph that satisfies
certain connectivity requirements, the number of edges in every feasible
solution is at most quadratic in the number of vertices. In contrast, for
hypergraph network design problems, we might have feasible solutions in which
the number of hyperedges is exponential in the number of vertices. This
presents an additional technical challenge in hypergraph network design
problems compared to graph network design problems: in order to solve the
problem in polynomial time, we first need to show that there exists a feasible
solution in which the number of hyperedges is polynomial in the input size.
<br />The central theme of this work is to show that certain hypergraph network
design problems admit solutions in which the number of hyperedges is polynomial
in the number of vertices and moreover, can be solved in strongly polynomial
time. Our work improves on the previous fastest pseudo-polynomial run-time for
these problems. In addition, we develop strongly polynomial time algorithms
that return near-uniform hypergraphs as solutions (i.e., every pair of
hyperedges differ in size by at most one). As applications of our results, we
derive the first strongly polynomial time algorithms for (i) degree-specified
hypergraph connectivity augmentation using hyperedges, (ii) degree-specified
hypergraph node-to-area connectivity augmentation using hyperedges, and (iii)
degree-constrained mixed-hypergraph connectivity augmentation using hyperedges.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10862" title="Abstract">arXiv:2402.10862</a> [<a href="/pdf/2402.10862" title="Download PDF">pdf</a>, <a href="/format/2402.10862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differential Private Federated Transfer Learning for Mental Health  Monitoring in Everyday Settings: A Case Study on Stress Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhongqi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Azimi%2C+I">Iman Azimi</a>, 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+A+M">Amir M. Rahmani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Mental health conditions, prevalent across various demographics, necessitate
efficient monitoring to mitigate their adverse impacts on life quality. The
surge in data-driven methodologies for mental health monitoring has underscored
the importance of privacy-preserving techniques in handling sensitive health
data. Despite strides in federated learning for mental health monitoring,
existing approaches struggle with vulnerabilities to certain cyber-attacks and
data insufficiency in real-world applications. In this paper, we introduce a
differential private federated transfer learning framework for mental health
monitoring to enhance data privacy and enrich data sufficiency. To accomplish
this, we integrate federated learning with two pivotal elements: (1)
differential privacy, achieved by introducing noise into the updates, and (2)
transfer learning, employing a pre-trained universal model to adeptly address
issues of data imbalance and insufficiency. We evaluate the framework by a case
study on stress detection, employing a dataset of physiological and contextual
data from a longitudinal study. Our finding show that the proposed approach can
attain a 10% boost in accuracy and a 21% enhancement in recall, while ensuring
privacy protection.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10865" title="Abstract">arXiv:2402.10865</a> [<a href="/pdf/2402.10865" title="Download PDF">pdf</a>, <a href="/format/2402.10865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Model 3D Registration: Finding Multiple Moving Objects in  Cluttered Point Clouds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+D">David Jin</a>, 
<a href="/search/cs?searchtype=author&query=Karmalkar%2C+S">Sushrut Karmalkar</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Harry Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Carlone%2C+L">Luca Carlone</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, Accepted by ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We investigate a variation of the 3D registration problem, named multi-model
3D registration. In the multi-model registration problem, we are given two
point clouds picturing a set of objects at different poses (and possibly
including points belonging to the background) and we want to simultaneously
reconstruct how all objects moved between the two point clouds. This setup
generalizes standard 3D registration where one wants to reconstruct a single
pose, e.g., the motion of the sensor picturing a static scene. Moreover, it
provides a mathematically grounded formulation for relevant robotics
applications, e.g., where a depth sensor onboard a robot perceives a dynamic
scene and has the goal of estimating its own motion (from the static portion of
the scene) while simultaneously recovering the motion of all dynamic objects.
We assume a correspondence-based setup where we have putative matches between
the two point clouds and consider the practical case where these
correspondences are plagued with outliers. We then propose a simple approach
based on Expectation-Maximization (EM) and establish theoretical conditions
under which the EM approach converges to the ground truth. We evaluate the
approach in simulated and real datasets ranging from table-top scenes to
self-driving scenarios and demonstrate its effectiveness when combined with
state-of-the-art scene flow methods to establish dense correspondences.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10866" title="Abstract">arXiv:2402.10866</a> [<a href="/pdf/2402.10866" title="Download PDF">pdf</a>, <a href="/format/2402.10866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EcoRank: Budget-Constrained Text Re-ranking Using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rashid%2C+M+S">Muhammad Shihab Rashid</a>, 
<a href="/search/cs?searchtype=author&query=Meem%2C+J+A">Jannat Ara Meem</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yue Dong</a>, 
<a href="/search/cs?searchtype=author&query=Hristidis%2C+V">Vagelis Hristidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have achieved state-of-the-art performance in
text re-ranking. This process includes queries and candidate passages in the
prompts, utilizing pointwise, listwise, and pairwise prompting strategies. A
limitation of these ranking strategies with LLMs is their cost: the process can
become expensive due to API charges, which are based on the number of input and
output tokens. We study how to maximize the re-ranking performance given a
budget, by navigating the vast search spaces of prompt choices, LLM APIs, and
budget splits. We propose a suite of budget-constrained methods to perform text
re-ranking using a set of LLM APIs. Our most efficient method, called EcoRank,
is a two-layered pipeline that jointly optimizes decisions regarding budget
allocation across prompt strategies and LLM APIs. Our experimental results on
four popular QA and passage reranking datasets show that EcoRank outperforms
other budget-aware supervised and unsupervised baselines.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10870" title="Abstract">arXiv:2402.10870</a> [<a href="/pdf/2402.10870" title="Download PDF">pdf</a>, <a href="/format/2402.10870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Best of Three Worlds: Adaptive Experimentation for Digital Marketing in  Practice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fiez%2C+T">Tanner Fiez</a>, 
<a href="/search/cs?searchtype=author&query=Nassif%2C+H">Houssam Nassif</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+A">Arick Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gamez%2C+S">Sergio Gamez</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+L">Lalit Jain</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
<p class="mathjax">Adaptive experimental design (AED) methods are increasingly being used in
industry as a tool to boost testing throughput or reduce experimentation cost
relative to traditional A/B/N testing methods. However, the behavior and
guarantees of such methods are not well-understood beyond idealized stationary
settings. This paper shares lessons learned regarding the challenges of naively
using AED systems in industrial settings where non-stationarity is prevalent,
while also providing perspectives on the proper objectives and system
specifications in such settings. We developed an AED framework for
counterfactual inference based on these experiences, and tested it in a
commercial environment.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10871" title="Abstract">arXiv:2402.10871</a> [<a href="/pdf/2402.10871" title="Download PDF">pdf</a>, <a href="/ps/2402.10871" title="Download PostScript">ps</a>, <a href="/format/2402.10871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lightweight ciphers based on chaotic Map -- LFSR architectures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garcia-Bosque%2C+M">M. Garcia-Bosque</a>, 
<a href="/search/cs?searchtype=author&query=S%C3%A1nchez-Azqueta%2C+C">C. S&#xe1;nchez-Azqueta</a>, 
<a href="/search/cs?searchtype=author&query=Royo%2C+G">G. Royo</a>, 
<a href="/search/cs?searchtype=author&query=Celma%2C+S">S. Celma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of 2016 12th Conference on Ph.D. Research in Microelectronics and Electronics (PRIME)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this paper, we propose and analyze two different stream ciphers based on a
Skew Tent Map and a Modified Logistic Map respectively. In order to improve the
randomness of these systems, a single method for increasing the period length
of the generated sequences has been applied. The results prove that the
randomness of these systems can be severally increased by using this method,
making these systems suitable for secure communications.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10873" title="Abstract">arXiv:2402.10873</a> [<a href="/pdf/2402.10873" title="Download PDF">pdf</a>, <a href="/ps/2402.10873" title="Download PostScript">ps</a>, <a href="/format/2402.10873" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic On-Demand Charging Scheduling for ISAC-Assisted WRSNs with  Multiple Mobile Charging Vehicles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qaisar%2C+M+U+F">Muhammad Umar Farooq Qaisar</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+W">Weijie Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Bellavista%2C+P">Paolo Bellavista</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+G">Guangjie Han</a>, 
<a href="/search/cs?searchtype=author&query=Zakariyya%2C+R+S">Rabiu Sale Zakariyya</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+A">Adeel Ahmed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at the IEEE Global Communications Conference (GLOBECOM) 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">The internet of things (IoT) based wireless sensor networks (WSNs) face an
energy shortage challenge that could be overcome by the novel wireless power
transfer (WPT) technology. The combination of WSNs and WPT is known as wireless
rechargeable sensor networks (WRSNs), with the charging efficiency and charging
scheduling being the primary concerns. Therefore, this paper proposes a
probabilistic on-demand charging scheduling for integrated sensing and
communication (ISAC)-assisted WRSNs with multiple mobile charging vehicles
(MCVs) that addresses three parts. First, it considers the four attributes with
their probability distributions to balance the charging load on each MCV. The
distributions are residual energy of charging node, distance from MCV to
charging node, degree of charging node, and charging node betweenness
centrality. Second, it considers the efficient charging factor strategy to
partially charge network nodes. Finally, it employs the ISAC concept to
efficiently utilize the wireless resources to reduce the traveling cost of each
MCV and to avoid the charging conflicts between them. The simulation results
show that the proposed protocol outperforms cutting-edge protocols in terms of
energy usage efficiency, charging delay, survival rate, and travel distance.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10876" title="Abstract">arXiv:2402.10876</a> [<a href="/pdf/2402.10876" title="Download PDF">pdf</a>, <a href="/format/2402.10876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Sparse DNNs Based on Tiled GEMM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+C">Cong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+F">Fengchen Xue</a>, 
<a href="/search/cs?searchtype=author&query=Leng%2C+J">Jingwen Leng</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Y">Yuxian Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+Y">Yue Guan</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+W">Weihao Cui</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Q">Quan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+M">Minyi Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transactions on Computers. arXiv admin note: substantial text overlap with <a href="/abs/2008.13006">arXiv:2008.13006</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Network pruning can reduce the computation cost of deep neural network (DNN)
models. However, sparse models often produce randomly-distributed weights to
maintain accuracy, leading to irregular computations. Consequently,
unstructured sparse models cannot achieve meaningful speedup on commodity
hardware built for dense matrix computations. Accelerators are usually modified
or designed with structured sparsity-optimized architectures for exploiting
sparsity. For example, the Ampere architecture introduces a sparse tensor core,
which adopts the 2:4 sparsity pattern.
<br />We propose a pruning method that builds upon the insight that matrix
multiplication generally breaks the large matrix into multiple smaller tiles
for parallel execution. We present the tile-wise sparsity pattern, which
maintains a structured sparsity pattern at the tile level for efficient
execution but allows for irregular pruning at the global scale to maintain high
accuracy. In addition, the tile-wise sparsity is implemented at the global
memory level, and the 2:4 sparsity executes at the register level inside the
sparse tensor core. We can combine these two patterns into a tile-vector-wise
(TVW) sparsity pattern to explore more fine-grained sparsity and further
accelerate the sparse DNN models. We evaluate the TVW on the GPU, achieving
averages of $1.85\times$, $2.75\times$, and $22.18\times$ speedups over the
dense model, block sparsity, and unstructured sparsity.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10877" title="Abstract">arXiv:2402.10877</a> [<a href="/pdf/2402.10877" title="Download PDF">pdf</a>, <a href="/format/2402.10877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust agents learn causal world models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Richens%2C+J">Jonathan Richens</a>, 
<a href="/search/cs?searchtype=author&query=Everitt%2C+T">Tom Everitt</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 (oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">It has long been hypothesised that causal reasoning plays a fundamental role
in robust and general intelligence. However, it is not known if agents must
learn causal models in order to generalise to new domains, or if other
inductive biases are sufficient. We answer this question, showing that any
agent capable of satisfying a regret bound under a large set of distributional
shifts must have learned an approximate causal model of the data generating
process, which converges to the true causal model for optimal agents. We
discuss the implications of this result for several research areas including
transfer learning and causal inference.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10882" title="Abstract">arXiv:2402.10882</a> [<a href="/pdf/2402.10882" title="Download PDF">pdf</a>, <a href="/format/2402.10882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Prompt Optimizer for Safe Text-to-Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zongyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Hongcheng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yueze Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Suhang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Text-to-Image (T2I) models have shown great performance in generating images
based on textual prompts. However, these models are vulnerable to unsafe input
to generate unsafe content like sexual, harassment and illegal-activity images.
Existing studies based on image checker, model fine-tuning and embedding
blocking are impractical in real-world applications. Hence, \textit{we propose
the first universal prompt optimizer for safe T2I generation in black-box
scenario}. We first construct a dataset consisting of toxic-clean prompt pairs
by GPT-3.5 Turbo. To guide the optimizer to have the ability of converting
toxic prompt to clean prompt while preserving semantic information, we design a
novel reward function measuring toxicity and text alignment of generated images
and train the optimizer through Proximal Policy Optimization. Experiments show
that our approach can effectively reduce the likelihood of various T2I models
in generating inappropriate images, with no significant impact on text
alignment. It is also flexible to be combined with methods to achieve better
performance.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10883" title="Abstract">arXiv:2402.10883</a> [<a href="/pdf/2402.10883" title="Download PDF">pdf</a>, <a href="/ps/2402.10883" title="Download PostScript">ps</a>, <a href="/format/2402.10883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Electronic Conductivity Measurements in Solid Electrolytes Using an Ion  Blocking Microelectrode: Noise Rejection Based on a Median Filter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gunes%2C+V">Veyis Gunes</a>, 
<a href="/search/eess?searchtype=author&query=Botquelen%2C+J">Jean-Yves Botquelen</a>, 
<a href="/search/eess?searchtype=author&query=Bohnke%2C+O">Odile Bohnke</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Journal of Computational Materials Science and
  Engineering. Vol.2, No.2, Imperial College Press, October 2013
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Materials Science (cond-mat.mtrl-sci)

</div>
<p class="mathjax">A method of electronic conductivity measurement is presented. It combines two
well known methods of electrochemistry: cyclic voltammetry and
chronoamperometry. This DC technique uses the Hebb/Wagner approach to block
ionic conduction when steady state conditions are reached and allows electronic
conduction of solid electrolytes to be determined. In order to get short
diffusion times, a micro contact is used as an ion blocking electrode. However,
as the electronic conduction in electrolytes is and should be very low, the
current is also very low, typically some tens of nanoamps. Thus, the heating
system inevitably generates noise problems that are solved using a median
filter. Our system allows the determination of the conductivities without any
preliminary smoothing or fitting of the curves. Some results with oxygen ion
conductors are also give
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10884" title="Abstract">arXiv:2402.10884</a> [<a href="/pdf/2402.10884" title="Download PDF">pdf</a>, <a href="/format/2402.10884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-modal preference alignment remedies regression of visual  instruction tuning on language model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shengzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+R">Rongyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+S">Shichao Pei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">In production, multi-modal large language models (MLLMs) are expected to
support multi-turn queries of interchanging image and text modalities. However,
the current MLLMs trained with visual-question-answering (VQA) datasets could
suffer from degradation, as VQA datasets lack the diversity and complexity of
the original text instruction datasets which the underlying language model had
been trained with. To address this challenging degradation, we first collect a
lightweight (6k entries) VQA preference dataset where answers were annotated by
Gemini for 5 quality metrics in a granular fashion, and investigate standard
Supervised Fine-tuning, rejection sampling, Direct Preference Optimization
(DPO), and SteerLM. Our findings indicate that the with DPO we are able to
surpass instruction-following capabilities of the language model, achieving a
6.73 score on MT-Bench, compared to Vicuna's 6.57 and LLaVA's 5.99 despite
small data scale. This enhancement in textual instruction proficiency
correlates with boosted visual instruction performance (+4.9\% on MM-Vet, +6\%
on LLaVA-Bench), with minimal alignment tax on visual knowledge benchmarks
compared to previous RLHF approach. In conclusion, we propose a
distillation-based multi-modal alignment model with fine-grained annotations on
a small dataset that reconciles the textual and visual performance of MLLMs,
restoring and boosting language capability after visual instruction tuning.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10885" title="Abstract">arXiv:2402.10885</a> [<a href="/pdf/2402.10885" title="Download PDF">pdf</a>, <a href="/format/2402.10885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D Diffuser Actor: Policy Diffusion with 3D Scene Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ke%2C+T">Tsung-Wei Ke</a>, 
<a href="/search/cs?searchtype=author&query=Gkanatsios%2C+N">Nikolaos Gkanatsios</a>, 
<a href="/search/cs?searchtype=author&query=Fragkiadaki%2C+K">Katerina Fragkiadaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">We marry diffusion policies and 3D scene representations for robot
manipulation. Diffusion policies learn the action distribution conditioned on
the robot and environment state using conditional diffusion models. They have
recently shown to outperform both deterministic and alternative
state-conditioned action distribution learning methods. 3D robot policies use
3D scene feature representations aggregated from a single or multiple camera
views using sensed depth. They have shown to generalize better than their 2D
counterparts across camera viewpoints. We unify these two lines of work and
present 3D Diffuser Actor, a neural policy architecture that, given a language
instruction, builds a 3D representation of the visual scene and conditions on
it to iteratively denoise 3D rotations and translations for the robot's
end-effector. At each denoising iteration, our model represents end-effector
pose estimates as 3D scene tokens and predicts the 3D translation and rotation
error for each of them, by featurizing them using 3D relative attention to
other 3D visual and language tokens. 3D Diffuser Actor sets a new
state-of-the-art on RLBench with an absolute performance gain of 16.3% over the
current SOTA on a multi-view setup and an absolute gain of 13.1% on a
single-view setup. On the CALVIN benchmark, it outperforms the current SOTA in
the setting of zero-shot unseen scene generalization by being able to
successfully run 0.2 more tasks, a 7% relative increase. It also works in the
real world from a handful of demonstrations. We ablate our model's
architectural design choices, such as 3D scene featurization and 3D relative
attentions, and show they all help generalization. Our results suggest that 3D
scene representations and powerful generative modeling are keys to efficient
robot learning from demonstrations.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10886" title="Abstract">arXiv:2402.10886</a> [<a href="/pdf/2402.10886" title="Download PDF">pdf</a>, <a href="/format/2402.10886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reviewer2: Optimizing Review Generation Through Prompt Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+Z">Zhaolin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Brantley%2C+K">Kiant&#xe9; Brantley</a>, 
<a href="/search/cs?searchtype=author&query=Joachims%2C+T">Thorsten Joachims</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent developments in LLMs offer new opportunities for assisting authors in
improving their work. In this paper, we envision a use case where authors can
receive LLM-generated reviews that uncover weak points in the current draft.
While initial methods for automated review generation already exist, these
methods tend to produce reviews that lack detail, and they do not cover the
range of opinions that human reviewers produce. To address this shortcoming, we
propose an efficient two-stage review generation framework called Reviewer2.
Unlike prior work, this approach explicitly models the distribution of possible
aspects that the review may address. We show that this leads to more detailed
reviews that better cover the range of aspects that human reviewers identify in
the draft. As part of the research, we generate a large-scale review dataset of
27k papers and 99k reviews that we annotate with aspect prompts, which we make
available as a resource for future research.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10888" title="Abstract">arXiv:2402.10888</a> [<a href="/pdf/2402.10888" title="Download PDF">pdf</a>, <a href="/format/2402.10888" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainability for Machine Learning Models: From Data Adaptability to  User Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Delaunay%2C+j">julien Delaunay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PhD Thesis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">This thesis explores the generation of local explanations for already
deployed machine learning models, aiming to identify optimal conditions for
producing meaningful explanations considering both data and user requirements.
The primary goal is to develop methods for generating explanations for any
model while ensuring that these explanations remain faithful to the underlying
model and comprehensible to the users.
<br />The thesis is divided into two parts. The first enhances a widely used
rule-based explanation method. It then introduces a novel approach for
evaluating the suitability of linear explanations to approximate a model.
Additionally, it conducts a comparative experiment between two families of
counterfactual explanation methods to analyze the advantages of one over the
other. The second part focuses on user experiments to assess the impact of
three explanation methods and two distinct representations. These experiments
measure how users perceive their interaction with the model in terms of
understanding and trust, depending on the explanations and representations.
This research contributes to a better explanation generation, with potential
implications for enhancing the transparency, trustworthiness, and usability of
deployed AI systems.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10889" title="Abstract">arXiv:2402.10889</a> [<a href="/pdf/2402.10889" title="Download PDF">pdf</a>, <a href="/format/2402.10889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluation of EAP Usage for Authenticating Eduroam Users in 5G Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Oliveira%2C+L+A">Leonardo Azalim de Oliveira</a> (1), 
<a href="/search/cs?searchtype=author&query=Silva%2C+E+F">Edelberto Franco Silva</a> (1) ((1) Federal University of Juiz de Fora)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">The fifth generation of the telecommunication networks (5G) established the
service-oriented paradigm on the mobile networks. In this new context, the 5G
Core component has become extremely flexible so, in addition to serving mobile
networks, it can also be used to connect devices from the so-called non-3GPP
networks, which contains technologies such as WiFi. The implementation of this
connectivity requires specific protocols to ensure authentication and
reliability. Given these characteristics and the possibility of convergence, it
is necessary to carefully choose the encryption algorithms and authentication
methods used by non-3GPP user equipment. In light of the above, this paper
highlights key findings resulting from an analysis on the subject conducted
through a test environment which could be used in the context of the Eduroam
federation.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10890" title="Abstract">arXiv:2402.10890</a> [<a href="/pdf/2402.10890" title="Download PDF">pdf</a>, <a href="/format/2402.10890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When is Tree Search Useful for LLM Planning? It Depends on the  Discriminator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ziru Chen</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+M">Michael White</a>, 
<a href="/search/cs?searchtype=author&query=Mooney%2C+R">Raymond Mooney</a>, 
<a href="/search/cs?searchtype=author&query=Payani%2C+A">Ali Payani</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yu Su</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Huan Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we examine how large language models (LLMs) solve multi-step
problems under a language agent framework with three components: a generator, a
discriminator, and a planning method. We investigate the practical utility of
two advanced planning methods, iterative correction and tree search. We present
a comprehensive analysis of how discrimination accuracy affects the overall
performance of agents when using these two methods or a simpler method,
re-ranking. Experiments on two tasks, text-to-SQL parsing and mathematical
reasoning, show that: (1) advanced planning methods demand discriminators with
at least 90% accuracy to achieve significant improvements over re-ranking; (2)
current LLMs' discrimination abilities have not met the needs of advanced
planning methods to achieve such improvements; (3) with LLM-based
discriminators, advanced planning methods may not adequately balance accuracy
and efficiency. For example, compared to the other two methods, tree search is
at least 10--20 times slower but leads to negligible performance gains, which
hinders its real-world applications. Code and data will be released at
https://github.com/OSU-NLP-Group/llm-planning-eval.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10891" title="Abstract">arXiv:2402.10891</a> [<a href="/pdf/2402.10891" title="Download PDF">pdf</a>, <a href="/format/2402.10891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instruction Diversity Drives Generalization To Unseen Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dylan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Justin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Charton%2C+F">Francois Charton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Instruction tuning -- fine-tuning a large language model (LLM) on pairs of
instructions and desired outcomes -- is an approach that enables pre-trained
language models to perform real-world tasks and follow human instructions. Its
practical success depends on the model learning a broader set of instructions
than those it was trained on. Yet the factors that determine model
generalization to such \emph{unseen tasks} are not well understood. %To
understand the driving factors of generalization, In this paper, we experiment
with string rewrites, a symbolic task that serves as a building block for
Turing complete Markov algorithms while allowing experimental control of
"inputs" and "instructions". We investigate the trade-off between the number of
instructions the model is trained on and the number of training samples
provided for each instruction and observe that the diversity of the instruction
set determines generalization. Generalization emerges once a diverse enough set
of tasks is provided, even though very few examples are provided for each task.
Instruction diversity also ensures robustness with respect to non-uniform
distributions of instructions in the training set.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10892" title="Abstract">arXiv:2402.10892</a> [<a href="/pdf/2402.10892" title="Download PDF">pdf</a>, <a href="/format/2402.10892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Proving membership in LLM pretraining data via data watermarks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+J+T">Johnny Tian-Zheng Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R+Y">Ryan Yixiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+R">Robin Jia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Detecting whether copyright holders' works were used in LLM pretraining is
poised to be an important problem. This work proposes using data watermarks to
enable principled detection with only black-box model access, provided that the
rightholder contributed multiple training documents and watermarked them before
public release. By applying a randomly sampled data watermark, detection can be
framed as hypothesis testing, which provides guarantees on the false detection
rate. We study two watermarks: one that inserts random sequences, and another
that randomly substitutes characters with Unicode lookalikes. We first show how
three aspects of watermark design -- watermark length, number of duplications,
and interference -- affect the power of the hypothesis test. Next, we study how
a watermark's detection strength changes under model and dataset scaling: while
increasing the dataset size decreases the strength of the watermark, watermarks
remain strong if the model size also increases. Finally, we view SHA hashes as
natural watermarks and show that we can robustly detect hashes from
BLOOM-176B's training data, as long as they occurred at least 90 times.
Together, our results point towards a promising future for data watermarks in
real world use.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10893" title="Abstract">arXiv:2402.10893</a> [<a href="/pdf/2402.10893" title="Download PDF">pdf</a>, <a href="/format/2402.10893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RLVF: Learning from Verbal Feedback without Overgeneralization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stephan%2C+M">Moritz Stephan</a>, 
<a href="/search/cs?searchtype=author&query=Khazatsky%2C+A">Alexander Khazatsky</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+E">Eric Mitchell</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+A+S">Annie S Chen</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+S">Sheryl Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Archit Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Finn%2C+C">Chelsea Finn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">The diversity of contexts in which large language models (LLMs) are deployed
requires the ability to modify or customize default model behaviors to
incorporate nuanced requirements and preferences. A convenient interface to
specify such model adjustments is high-level verbal feedback, such as "Don't
use emojis when drafting emails to my boss." However, while writing high-level
feedback is far simpler than collecting annotations for reinforcement learning
from human feedback (RLHF), we find that simply prompting a model with such
feedback leads to overgeneralization of the feedback to contexts where it is
not relevant. We study the problem of incorporating verbal feedback without
such overgeneralization, inspiring a new method Contextualized Critiques with
Constrained Preference Optimization (C3PO). C3PO uses a piece of high-level
feedback to generate a small synthetic preference dataset specifying how the
feedback should (and should not) be applied. It then fine-tunes the model in
accordance with the synthetic preference data while minimizing the divergence
from the original model for prompts where the feedback does not apply. Our
experimental results indicate that our approach effectively applies verbal
feedback to relevant scenarios while preserving existing behaviors for other
contexts. For both human- and GPT-4-generated high-level feedback, C3PO
effectively adheres to the given feedback comparably to in-context baselines
while reducing overgeneralization by 30%.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10894" title="Abstract">arXiv:2402.10894</a> [<a href="/pdf/2402.10894" title="Download PDF">pdf</a>, <a href="/format/2402.10894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fusion of Diffusion Weighted MRI and Clinical Data for Predicting  Functional Outcome after Acute Ischemic Stroke with Deep Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsai%2C+C">Chia-Ling Tsai</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hui-Yun Su</a>, 
<a href="/search/cs?searchtype=author&query=Sung%2C+S">Shen-Feng Sung</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+W">Wei-Yang Lin</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Ying-Ying Su</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+T">Tzu-Hsien Yang</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+M">Man-Lin Mai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Stroke is a common disabling neurological condition that affects about
one-quarter of the adult population over age 25; more than half of patients
still have poor outcomes, such as permanent functional dependence or even
death, after the onset of acute stroke. The aim of this study is to investigate
the efficacy of diffusion-weighted MRI modalities combining with structured
health profile on predicting the functional outcome to facilitate early
intervention. A deep fusion learning network is proposed with two-stage
training: the first stage focuses on cross-modality representation learning and
the second stage on classification. Supervised contrastive learning is
exploited to learn discriminative features that separate the two classes of
patients from embeddings of individual modalities and from the fused multimodal
embedding. The network takes as the input DWI and ADC images, and structured
health profile data. The outcome is the prediction of the patient needing
long-term care at 3 months after the onset of stroke. Trained and evaluated
with a dataset of 3297 patients, our proposed fusion model achieves 0.87, 0.80
and 80.45% for AUC, F1-score and accuracy, respectively, outperforming existing
models that consolidate both imaging and structured data in the medical domain.
If trained with comprehensive clinical variables, including NIHSS and
comorbidities, the gain from images on making accurate prediction is not
considered substantial, but significant. However, diffusion-weighted MRI can
replace NIHSS to achieve comparable level of accuracy combining with other
readily available clinical variables for better generalization.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10896" title="Abstract">arXiv:2402.10896</a> [<a href="/pdf/2402.10896" title="Download PDF">pdf</a>, <a href="/format/2402.10896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong  Vision-language Adapter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Junfei Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yuille%2C+A">Alan Yuille</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shen Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Boyu Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report, 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper demonstrates that a progressively aligned language model can
effectively bridge frozen vision encoders and large language models (LLMs).
While the fundamental architecture and pre-training methods of vision encoders
and LLMs have been extensively studied, the architecture and training strategy
of vision-language adapters vary significantly across recent works. Our
research undertakes a thorough exploration of the state-of-the-art perceiver
resampler architecture and builds a strong baseline. However, we observe that
the vision-language alignment with perceiver resampler exhibits slow
convergence and limited scalability with a lack of direct supervision. To
address this issue, we propose PaLM2-VAdapter, employing a progressively
aligned language model as the vision-language adapter. Compared to the strong
baseline with perceiver resampler, our method empirically shows faster
convergence, higher performance, and stronger scalability. Extensive
experiments across various Visual Question Answering (VQA) and captioning tasks
on both images and videos demonstrate that our model exhibits state-of-the-art
visual understanding and multi-modal reasoning capabilities. Notably, our
method achieves these advancements with 30~70% fewer parameters than the
state-of-the-art large vision-language models, marking a significant efficiency
improvement.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Mon, 19 Feb 24</h3>
<dl>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09430" title="Abstract">arXiv:2402.09430</a> (cross-list from eess.SP) [<a href="/pdf/2402.09430" title="Download PDF">pdf</a>, <a href="/format/2402.09430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WiMANS: A Benchmark Dataset for WiFi-based Multi-user Activity Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Huang%2C+S">Shuokang Huang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+K">Kaihan Li</a>, 
<a href="/search/eess?searchtype=author&query=You%2C+D">Di You</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Y">Yichong Chen</a>, 
<a href="/search/eess?searchtype=author&query=Lin%2C+A">Arvin Lin</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+S">Siying Liu</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+X">Xiaohui Li</a>, 
<a href="/search/eess?searchtype=author&query=McCann%2C+J+A">Julie A. McCann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We present WiMANS, to our knowledge, the first dataset for multi-user activity sensing based on WiFi
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)

</div>
<p class="mathjax">WiFi-based human sensing has exhibited remarkable potential to analyze user
behaviors in a non-intrusive and device-free manner, benefiting applications as
diverse as smart homes and healthcare. However, most previous works focus on
single-user sensing, which has limited practicability in scenarios involving
multiple users. Although recent studies have begun to investigate WiFi-based
multi-user activity sensing, there remains a lack of benchmark datasets to
facilitate reproducible and comparable research. To bridge this gap, we present
WiMANS, to our knowledge, the first dataset for multi-user activity sensing
based on WiFi. WiMANS contains over 9.4 hours of WiFi Channel State Information
(CSI), monitoring simultaneous activities performed by multiple users in
various environments. Compared to existing datasets, WiMANS not only collects
the CSI of dual WiFi bands but also includes synchronized videos. We exploit
WiMANS to benchmark the performance of state-of-the-art WiFi-based human
sensing models and video-based models, posing new challenges and opportunities
for WiFi-based multi-user identification, localization, and activity
recognition. We believe that WiMANS can push the boundaries of current
WiFi-based human sensing and catalyze the research on multi-user activity
analysis.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10213" title="Abstract">arXiv:2402.10213</a> (cross-list from q-bio.NC) [<a href="/pdf/2402.10213" title="Download PDF">pdf</a>, <a href="/format/2402.10213" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Clustering Inductive Biases with Unrolled Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Huml%2C+J">Jonathan Huml</a>, 
<a href="/search/q-bio?searchtype=author&query=Tasissa%2C+A">Abiy Tasissa</a>, 
<a href="/search/q-bio?searchtype=author&query=Ba%2C+D">Demba Ba</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The classical sparse coding (SC) model represents visual stimuli as a linear
combination of a handful of learned basis functions that are Gabor-like when
trained on natural image data. However, the Gabor-like filters learned by
classical sparse coding far overpredict well-tuned simple cell receptive field
profiles observed empirically. While neurons fire sparsely, neuronal
populations are also organized in physical space by their sensitivity to
certain features. In V1, this organization is a smooth progression of
orientations along the cortical sheet. A number of subsequent models have
either discarded the sparse dictionary learning framework entirely or whose
updates have yet to take advantage of the surge in unrolled, neural dictionary
learning architectures. A key missing theme of these updates is a stronger
notion of \emph{structured sparsity}. We propose an autoencoder architecture
(WLSC) whose latent representations are implicitly, locally organized for
spectral clustering through a Laplacian quadratic form of a bipartite graph,
which generates a diverse set of artificial receptive fields that match primate
data in V1 as faithfully as recent contrastive frameworks like Local Low
Dimensionality, or LLD \citep{lld} that discard sparse dictionary learning. By
unifying sparse and smooth coding in models of the early visual cortex through
our autoencoder, we also show that our regularization can be interpreted as
early-stage specialization of receptive fields to certain classes of stimuli;
that is, we induce a weak clustering bias for later stages of cortex where
functional and spatial segregation (i.e. topography) are known to occur. The
results show an imperative for \emph{spatial regularization} of both the
receptive fields and firing rates to begin to describe feature disentanglement
in V1 and beyond.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10229" title="Abstract">arXiv:2402.10229</a> (cross-list from stat.CO) [<a href="/pdf/2402.10229" title="Download PDF">pdf</a>, <a href="/format/2402.10229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixture-Models: a one-stop Python Library for Model-based Clustering  using various Mixture Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kasa%2C+S+R">Siva Rajesh Kasa</a>, 
<a href="/search/stat?searchtype=author&query=Yijie%2C+H">Hu Yijie</a>, 
<a href="/search/stat?searchtype=author&query=Kasa%2C+S+K">Santhosh Kumar Kasa</a>, 
<a href="/search/stat?searchtype=author&query=Rajan%2C+V">Vaibhav Rajan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">\texttt{Mixture-Models} is an open-source Python library for fitting Gaussian
Mixture Models (GMM) and their variants, such as Parsimonious GMMs, Mixture of
Factor Analyzers, MClust models, Mixture of Student's t distributions, etc. It
streamlines the implementation and analysis of these models using various
first/second order optimization routines such as Gradient Descent and Newton-CG
through automatic differentiation (AD) tools. This helps in extending these
models to high-dimensional data, which is first of its kind among Python
libraries. The library provides user-friendly model evaluation tools, such as
BIC, AIC, and log-likelihood estimation. The source-code is licensed under MIT
license and can be accessed at \url{https://github.com/kasakh/Mixture-Models}.
The package is highly extensible, allowing users to incorporate new
distributions and optimization techniques with ease. We conduct a large scale
simulation to compare the performance of various gradient based approaches
against Expectation Maximization on a wide range of settings and identify the
corresponding best suited approach.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10232" title="Abstract">arXiv:2402.10232</a> (cross-list from stat.ML) [<a href="/pdf/2402.10232" title="Download PDF">pdf</a>, <a href="/ps/2402.10232" title="Download PostScript">ps</a>, <a href="/format/2402.10232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple, unified analysis of Johnson-Lindenstrauss with applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+Y">Yingru Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to COLT2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG); Probability (math.PR)

</div>
<p class="mathjax">In this work, we present a simple and unified analysis of the
Johnson-Lindenstrauss (JL) lemma, a cornerstone in the field of dimensionality
reduction critical for managing high-dimensional data. Our approach not only
simplifies the understanding but also unifies various constructions under the
JL framework, including spherical, Gaussian, binary coin, and sub-Gaussian
models. This simplification and unification make significant strides in
preserving the intrinsic geometry of data, essential across diverse
applications from streaming algorithms to reinforcement learning. Notably, we
deliver the first rigorous proof of the spherical construction's effectiveness
within this simplified framework. At the heart of our contribution is an
innovative extension of the Hanson-Wright inequality to high dimensions,
complete with explicit constants, marking a substantial leap in the literature.
By employing simple yet powerful probabilistic tools and analytical techniques,
such as an enhanced diagonalization process, our analysis not only solidifies
the JL lemma's theoretical foundation but also extends its practical reach,
showcasing its adaptability and importance in contemporary computational
algorithms.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10239" title="Abstract">arXiv:2402.10239</a> (cross-list from hep-ph) [<a href="/pdf/2402.10239" title="Download PDF">pdf</a>, <a href="/format/2402.10239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Language Model for Particle Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-ph?searchtype=author&query=Huang%2C+A">Andris Huang</a>, 
<a href="/search/hep-ph?searchtype=author&query=Melkani%2C+Y">Yash Melkani</a>, 
<a href="/search/hep-ph?searchtype=author&query=Calafiura%2C+P">Paolo Calafiura</a>, 
<a href="/search/hep-ph?searchtype=author&query=Lazar%2C+A">Alina Lazar</a>, 
<a href="/search/hep-ph?searchtype=author&query=Murnane%2C+D+T">Daniel Thomas Murnane</a>, 
<a href="/search/hep-ph?searchtype=author&query=Pham%2C+M">Minh-Tuan Pham</a>, 
<a href="/search/hep-ph?searchtype=author&query=Ju%2C+X">Xiangyang Ju</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures, A Proceeding of the Connecting the Dots Workshop (CTD 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Phenomenology (hep-ph)</span>; Machine Learning (cs.LG); High Energy Physics - Experiment (hep-ex)

</div>
<p class="mathjax">Particle tracking is crucial for almost all physics analysis programs at the
Large Hadron Collider. Deep learning models are pervasively used in particle
tracking related tasks. However, the current practice is to design and train
one deep learning model for one task with supervised learning techniques. The
trained models work well for tasks they are trained on but show no or little
generalization capabilities. We propose to unify these models with a language
model. In this paper, we present a tokenized detector representation that
allows us to train a BERT model for particle tracking. The trained BERT model,
namely TrackingBERT, offers latent detector module embedding that can be used
for other tasks. This work represents the first step towards developing a
foundational model for particle detector understanding.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10243" title="Abstract">arXiv:2402.10243</a> (cross-list from physics.soc-ph) [<a href="/pdf/2402.10243" title="Download PDF">pdf</a>, <a href="/format/2402.10243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding team collapse via probabilistic graphical models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Nikolaou%2C+I">Iasonas Nikolaou</a>, 
<a href="/search/physics?searchtype=author&query=Pelechrinis%2C+K">Konstantinos Pelechrinis</a>, 
<a href="/search/physics?searchtype=author&query=Terzi%2C+E">Evimaria Terzi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Machine Learning (cs.LG); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">In this work, we develop a graphical model to capture team dynamics. We
analyze the model and show how to learn its parameters from data. Using our
model we study the phenomenon of team collapse from a computational
perspective. We use simulations and real-world experiments to find the main
causes of team collapse. We also provide the principles of building resilient
teams, i.e., teams that avoid collapsing. Finally, we use our model to analyze
the structure of NBA teams and dive deeper into games of interest.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10251" title="Abstract">arXiv:2402.10251</a> (cross-list from q-bio.NC) [<a href="/pdf/2402.10251" title="Download PDF">pdf</a>, <a href="/format/2402.10251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Brant-2: Foundation Model for Brain Signals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Yuan%2C+Z">Zhizhang Yuan</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+D">Daoze Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Chen%2C+J">Junru Chen</a>, 
<a href="/search/q-bio?searchtype=author&query=Gu%2C+G">Geifei Gu</a>, 
<a href="/search/q-bio?searchtype=author&query=Yang%2C+Y">Yang Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">Foundational models benefit from pre-training on large amounts of unlabeled
data and enable strong performance in a wide variety of applications with a
small amount of labeled data. Such models can be particularly effective in
analyzing brain signals, as this field encompasses numerous application
scenarios, and it is costly to perform large-scale annotation. In this work, we
present the largest foundation model in brain signals, Brant-2. Compared to
Brant, a foundation model designed for intracranial neural signals, Brant-2 not
only exhibits robustness towards data variations and modeling scales but also
can be applied to a broader range of brain neural data. By experimenting on an
extensive range of tasks, we demonstrate that Brant-2 is adaptive to various
application scenarios in brain signals. Further analyses reveal the scalability
of the Brant-2, validate each component's effectiveness, and showcase our
model's ability to maintain performance in scenarios with scarce labels. The
source code and pre-trained weights are available at:
https://anonymous.4open.science/r/Brant-2-5843.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10255" title="Abstract">arXiv:2402.10255</a> (cross-list from quant-ph) [<a href="/pdf/2402.10255" title="Download PDF">pdf</a>, <a href="/format/2402.10255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking the Operation of Quantum Heuristics and Ising Machines:  Scoring Parameter Setting Strategies on Optimization Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Neira%2C+D+E+B">David E. Bernal Neira</a>, 
<a href="/search/quant-ph?searchtype=author&query=Brown%2C+R">Robin Brown</a>, 
<a href="/search/quant-ph?searchtype=author&query=Sathe%2C+P">Pratik Sathe</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wudarski%2C+F">Filip Wudarski</a>, 
<a href="/search/quant-ph?searchtype=author&query=Pavone%2C+M">Marco Pavone</a>, 
<a href="/search/quant-ph?searchtype=author&query=Rieffel%2C+E+G">Eleanor G. Rieffel</a>, 
<a href="/search/quant-ph?searchtype=author&query=Venturelli%2C+D">Davide Venturelli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET); Computation (stat.CO); Methodology (stat.ME)

</div>
<p class="mathjax">We discuss guidelines for evaluating the performance of parameterized
stochastic solvers for optimization problems, with particular attention to
systems that employ novel hardware, such as digital quantum processors running
variational algorithms, analog processors performing quantum annealing, or
coherent Ising Machines. We illustrate through an example a benchmarking
procedure grounded in the statistical analysis of the expectation of a given
performance metric measured in a test environment. In particular, we discuss
the necessity and cost of setting parameters that affect the algorithm's
performance. The optimal value of these parameters could vary significantly
between instances of the same target problem. We present an open-source
software package that facilitates the design, evaluation, and visualization of
practical parameter tuning strategies for complex use of the heterogeneous
components of the solver. We examine in detail an example using parallel
tempering and a simulator of a photonic Coherent Ising Machine computing and
display the scoring of an illustrative baseline family of parameter-setting
strategies that feature an exploration-exploitation trade-off.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10289" title="Abstract">arXiv:2402.10289</a> (cross-list from stat.ML) [<a href="/pdf/2402.10289" title="Download PDF">pdf</a>, <a href="/format/2402.10289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thompson Sampling in Partially Observable Contextual Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Park%2C+H">Hongju Park</a>, 
<a href="/search/stat?searchtype=author&query=Faradonbeh%2C+M+K+S">Mohamad Kazem Shirani Faradonbeh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 43 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Contextual bandits constitute a classical framework for decision-making under
uncertainty. In this setting, the goal is to learn the arms of highest reward
subject to contextual information, while the unknown reward parameters of each
arm need to be learned by experimenting that specific arm. Accordingly, a
fundamental problem is that of balancing exploration (i.e., pulling different
arms to learn their parameters), versus exploitation (i.e., pulling the best
arms to gain reward). To study this problem, the existing literature mostly
considers perfectly observed contexts. However, the setting of partial context
observations remains unexplored to date, despite being theoretically more
general and practically more versatile. We study bandit policies for learning
to select optimal arms based on the data of observations, which are noisy
linear functions of the unobserved context vectors. Our theoretical analysis
shows that the Thompson sampling policy successfully balances exploration and
exploitation. Specifically, we establish the followings: (i) regret bounds that
grow poly-logarithmically with time, (ii) square-root consistency of parameter
estimation, and (iii) scaling of the regret with other quantities including
dimensions and number of arms. Extensive numerical experiments with both real
and synthetic data are presented as well, corroborating the efficacy of
Thompson sampling. To establish the results, we introduce novel martingale
techniques and concentration inequalities to address partially observed
dependent random variables generated from unspecified distributions, and also
leverage problem-dependent information to sharpen probabilistic bounds for
time-varying suboptimality gaps. These techniques pave the road towards
studying other decision-making problems with contextual information as well as
partial observations.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10305" title="Abstract">arXiv:2402.10305</a> (cross-list from math.NT) [<a href="/pdf/2402.10305" title="Download PDF">pdf</a>, <a href="/ps/2402.10305" title="Download PostScript">ps</a>, <a href="/format/2402.10305" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective module lattices and their shortest vectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gargava%2C+N">Nihar Gargava</a>, 
<a href="/search/math?searchtype=author&query=Serban%2C+V">Vlad Serban</a>, 
<a href="/search/math?searchtype=author&query=Viazovska%2C+M">Maryna Viazovska</a>, 
<a href="/search/math?searchtype=author&query=Viglino%2C+I">Ilaria Viglino</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Number Theory (math.NT)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">We prove tight probabilistic bounds for the shortest vectors in module
lattices over number fields using the results of <a href="/abs/2308.15275">arXiv:2308.15275</a>. Moreover,
establishing asymptotic formulae for counts of fixed rank matrices with
algebraic integer entries and bounded Euclidean length, we prove an approximate
Rogers integral formula for discrete sets of module lattices obtained from
lifts of algebraic codes. This in turn implies that the moment estimates of
<a href="/abs/2308.15275">arXiv:2308.15275</a> as well as the aforementioned bounds on the shortest vector
also carry through for large enough discrete sets of module lattices.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10319" title="Abstract">arXiv:2402.10319</a> (cross-list from physics.med-ph) [<a href="/pdf/2402.10319" title="Download PDF">pdf</a>, <a href="/ps/2402.10319" title="Download PostScript">ps</a>, <a href="/format/2402.10319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling Blood Alcohol Concentration Using Fractional Differential  Equations Based on the $&#x3c8;$-Caputo Derivative
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Wanassi%2C+O+K">Om Kalthoum Wanassi</a>, 
<a href="/search/physics?searchtype=author&query=Torres%2C+D+F+M">Delfim F. M. Torres</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is a preprint of a paper whose final and definite form is published Open Access in 'Math. Meth. Appl. Sci.' at [<a href="http://doi.org/10.1002/mma.10002">this http URL</a>]
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Medical Physics (physics.med-ph)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We propose a novel dynamical model for blood alcohol concentration that
incorporates $\psi$-Caputo fractional derivatives. Using the generalized
Laplace transform technique, we successfully derive an analytic solution for
both the alcohol concentration in the stomach and the alcohol concentration in
the blood of an individual. These analytical formulas provide us a
straightforward numerical scheme, which demonstrates the efficacy of the
$\psi$-Caputo derivative operator in achieving a better fit to real
experimental data on blood alcohol levels available in the literature. In
comparison to existing classical and fractional models found in the literature,
our model outperforms them significantly. Indeed, by employing a simple yet
non-standard kernel function $\psi(t)$, we are able to reduce the error by more
than half, resulting in an impressive gain improvement of 59 percent.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10326" title="Abstract">arXiv:2402.10326</a> (cross-list from math.OC) [<a href="/pdf/2402.10326" title="Download PDF">pdf</a>, <a href="/format/2402.10326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mathematical Opportunities in Digital Twins (MATH-DT)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Antil%2C+H">Harbir Antil</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA); Machine Learning (stat.ML)

</div>
<p class="mathjax">The report describes the discussions from the Workshop on Mathematical
Opportunities in Digital Twins (MATH-DT) from December 11-13, 2023, George
Mason University.
<br />It illustrates that foundational Mathematical advances are required for
Digital Twins (DTs) that are different from traditional approaches. A
traditional model, in biology, physics, engineering or medicine, starts with a
generic physical law (e.g., equations) and is often a simplification of
reality. A DT starts with a specific ecosystem, object or person (e.g.,
personalized care) representing reality, requiring multi -scale, -physics
modeling and coupling. Thus, these processes begin at opposite ends of the
simulation and modeling pipeline, requiring different reliability criteria and
uncertainty assessments. Additionally, unlike existing approaches, a DT assists
humans to make decisions for the physical system, which (via sensors) in turn
feeds data into the DT, and operates for the life of the physical system.
<br />While some of the foundational mathematical research can be done without a
specific application context, one must also keep specific applications in mind
for DTs. E.g., modeling a bridge or a biological system (a patient), or a
socio-technical system (a city) is very different. The models range from
differential equations (deterministic/uncertain) in engineering, to stochastic
in biology, including agent-based. These are multi-scale hybrid models or large
scale (multi-objective) optimization problems under uncertainty. There are no
universal models or approaches. For e.g., Kalman filters for forecasting might
work in engineering, but can fail in biomedical domain. Ad hoc studies, with
limited systematic work, have shown that AI/ML methods can fail for simple
engineering systems and can work well for biomedical problems.
<br />A list of `Mathematical Opportunities and Challenges' concludes the report.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10357" title="Abstract">arXiv:2402.10357</a> (cross-list from math.ST) [<a href="/pdf/2402.10357" title="Download PDF">pdf</a>, <a href="/format/2402.10357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Sampling on Riemannian Manifolds via Langevin MCMC
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cheng%2C+X">Xiang Cheng</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+J">Jingzhao Zhang</a>, 
<a href="/search/math?searchtype=author&query=Sra%2C+S">Suvrit Sra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is an old paper from NeurIPS 2022. arXiv admin note: text overlap with <a href="/abs/2204.13665">arXiv:2204.13665</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Probability (math.PR); Computation (stat.CO); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the task of efficiently sampling from a Gibbs distribution $d \pi^*
= e^{-h} d {vol}_g$ over a Riemannian manifold $M$ via (geometric) Langevin
MCMC; this algorithm involves computing exponential maps in random Gaussian
directions and is efficiently implementable in practice. The key to our
analysis of Langevin MCMC is a bound on the discretization error of the
geometric Euler-Murayama scheme, assuming $\nabla h$ is Lipschitz and $M$ has
bounded sectional curvature. Our error bound matches the error of Euclidean
Euler-Murayama in terms of its stepsize dependence. Combined with a contraction
guarantee for the geometric Langevin Diffusion under Kendall-Cranston coupling,
we prove that the Langevin MCMC iterates lie within $\epsilon$-Wasserstein
distance of $\pi^*$ after $\tilde{O}(\epsilon^{-2})$ steps, which matches the
iteration complexity for Euclidean Langevin MCMC. Our results apply in general
settings where $h$ can be nonconvex and $M$ can have negative Ricci curvature.
Under additional assumptions that the Riemannian curvature tensor has bounded
derivatives, and that $\pi^*$ satisfies a $CD(\cdot,\infty)$ condition, we
analyze the stochastic gradient version of Langevin MCMC, and bound its
iteration complexity by $\tilde{O}(\epsilon^{-2})$ as well.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10387" title="Abstract">arXiv:2402.10387</a> (cross-list from q-bio.BM) [<a href="/pdf/2402.10387" title="Download PDF">pdf</a>, <a href="/format/2402.10387" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MFBind: a Multi-Fidelity Approach for Evaluating Drug Compounds in  Practical Generative Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Eckmann%2C+P">Peter Eckmann</a>, 
<a href="/search/q-bio?searchtype=author&query=Wu%2C+D">Dongxia Wu</a>, 
<a href="/search/q-bio?searchtype=author&query=Heinzelmann%2C+G">Germano Heinzelmann</a>, 
<a href="/search/q-bio?searchtype=author&query=Gilson%2C+M+K">Michael K Gilson</a>, 
<a href="/search/q-bio?searchtype=author&query=Yu%2C+R">Rose Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Current generative models for drug discovery primarily use molecular docking
to evaluate the quality of generated compounds. However, such models are often
not useful in practice because even compounds with high docking scores do not
consistently show experimental activity. More accurate methods for activity
prediction exist, such as molecular dynamics based binding free energy
calculations, but they are too computationally expensive to use in a generative
model. We propose a multi-fidelity approach, Multi-Fidelity Bind (MFBind), to
achieve the optimal trade-off between accuracy and computational cost. MFBind
integrates docking and binding free energy simulators to train a multi-fidelity
deep surrogate model with active learning. Our deep surrogate model utilizes a
pretraining technique and linear prediction heads to efficiently fit small
amounts of high-fidelity data. We perform extensive experiments and show that
MFBind (1) outperforms other state-of-the-art single and multi-fidelity
baselines in surrogate modeling, and (2) boosts the performance of generative
models with markedly higher quality compounds.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10402" title="Abstract">arXiv:2402.10402</a> (cross-list from math.OC) [<a href="/pdf/2402.10402" title="Download PDF">pdf</a>, <a href="/ps/2402.10402" title="Download PostScript">ps</a>, <a href="/format/2402.10402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-convex optimization problems for maximum hands-off control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ikeda%2C+T">Takuya Ikeda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">The maximum hands-off control is the optimal solution to the L0 optimal
control problem. It has the minimum support length among all feasible control
inputs. To avoid computational difficulties arising from its combinatorial
nature, the convex approximation method that replaces the L0 norm by the L1
norm in the cost function has been employed on standard. However, this
approximation method does not necessarily obtain the maximum hands-off control.
In response to this limitation, this paper newly introduces a non-convex
approximation method and formulates a class of non-convex optimal control
problems that are always equivalent to the maximum hands-off control problem.
Based on the results, this paper describes the computation method that quotes
algorithms designed for the difference of convex functions optimization.
Finally, this paper confirms the effectiveness of the non-convex approximation
method with a numerical example.
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10425" title="Abstract">arXiv:2402.10425</a> (cross-list from eess.IV) [<a href="/pdf/2402.10425" title="Download PDF">pdf</a>, <a href="/ps/2402.10425" title="Download PostScript">ps</a>, <a href="/format/2402.10425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DABS-LS: Deep Atlas-Based Segmentation Using Regional Level Set  Self-Supervision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mason%2C+H+G">Hannah G. Mason</a>, 
<a href="/search/eess?searchtype=author&query=Noble%2C+J+H">Jack H. Noble</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Cochlear implants (CIs) are neural prosthetics used to treat patients with
severe-to-profound hearing loss. Patient-specific modeling of CI stimulation of
the auditory nerve fiber (ANFs) can help audiologists improve the CI
programming. These models require localization of the ANFs relative to
surrounding anatomy and the CI. Localization is challenging because the ANFs
are so small they are not directly visible in clinical imaging. In this work,
we hypothesize the position of the ANFs can be accurately inferred from the
location of the internal auditory canal (IAC), which has high contrast in CT,
since the ANFs pass through this canal between the cochlea and the brain.
Inspired by VoxelMorph, in this paper we propose a deep atlas-based IAC
segmentation network. We create a single atlas in which the IAC and ANFs are
pre-localized. Our network is trained to produce deformation fields (DFs)
mapping coordinates from the atlas to new target volumes and that accurately
segment the IAC. We hypothesize that DFs that accurately segment the IAC in
target images will also facilitate accurate atlas-based localization of the
ANFs. As opposed to VoxelMorph, which aims to produce DFs that accurately
register the entire volume, our novel contribution is an entirely
self-supervised training scheme that aims to produce DFs that accurately
segment the target structure. This self-supervision is facilitated using a
regional level set (LS) inspired loss function. We call our method Deep Atlas
Based Segmentation using Level Sets (DABS-LS). Results show that DABS-LS
outperforms VoxelMorph for IAC segmentation. Tests with publicly available
datasets for trachea and kidney segmentation also show significant improvement
in segmentation accuracy, demonstrating the generalizability of the method.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10429" title="Abstract">arXiv:2402.10429</a> (cross-list from stat.ML) [<a href="/pdf/2402.10429" title="Download PDF">pdf</a>, <a href="/ps/2402.10429" title="Download PostScript">ps</a>, <a href="/format/2402.10429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fixed Confidence Best Arm Identification in the Bayesian Setting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Jang%2C+K">Kyoungseok Jang</a>, 
<a href="/search/stat?searchtype=author&query=Komiyama%2C+J">Junpei Komiyama</a>, 
<a href="/search/stat?searchtype=author&query=Yamazaki%2C+K">Kazutoshi Yamazaki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We consider the fixed-confidence best arm identification (FC-BAI) problem in
the Bayesian Setting. This problem aims to find the arm of the largest mean
with a fixed confidence level when the bandit model has been sampled from the
known prior. Most studies on the FC-BAI problem have been conducted in the
frequentist setting, where the bandit model is predetermined before the game
starts. We show that the traditional FC-BAI algorithms studied in the
frequentist setting, such as track-and-stop and top-two algorithms, result in
arbitrary suboptimal performances in the Bayesian setting. We also prove a
lower bound of the expected number of samples in the Bayesian setting and
introduce a variant of successive elimination that has a matching performance
with the lower bound up to a logarithmic factor. Simulations verify the
theoretical results.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10433" title="Abstract">arXiv:2402.10433</a> (cross-list from q-bio.BM) [<a href="/pdf/2402.10433" title="Download PDF">pdf</a>, <a href="/format/2402.10433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fusing Neural and Physical: Augment Protein Conformation Sampling with  Tractable Simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Lu%2C+J">Jiarui Lu</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+Z">Zuobai Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhong%2C+B">Bozitao Zhong</a>, 
<a href="/search/q-bio?searchtype=author&query=Shi%2C+C">Chence Shi</a>, 
<a href="/search/q-bio?searchtype=author&query=Tang%2C+J">Jian Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">The protein dynamics are common and important for their biological functions
and properties, the study of which usually involves time-consuming molecular
dynamics (MD) simulations in silico. Recently, generative models has been
leveraged as a surrogate sampler to obtain conformation ensembles with orders
of magnitude faster and without requiring any simulation data (a "zero-shot"
inference). However, being agnostic of the underlying energy landscape, the
accuracy of such generative model may still be limited. In this work, we
explore the few-shot setting of such pre-trained generative sampler which
incorporates MD simulations in a tractable manner. Specifically, given a target
protein of interest, we first acquire some seeding conformations from the
pre-trained sampler followed by a number of physical simulations in parallel
starting from these seeding samples. Then we fine-tuned the generative model
using the simulation trajectories above to become a target-specific sampler.
Experimental results demonstrated the superior performance of such few-shot
conformation sampler at a tractable computational cost.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10456" title="Abstract">arXiv:2402.10456</a> (cross-list from stat.ML) [<a href="/pdf/2402.10456" title="Download PDF">pdf</a>, <a href="/format/2402.10456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Modeling for Tabular Data via Penalized Optimal Transport  Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lu%2C+W+S">Wenhui Sophia Lu</a>, 
<a href="/search/stat?searchtype=author&query=Zhong%2C+C">Chenyang Zhong</a>, 
<a href="/search/stat?searchtype=author&query=Wong%2C+W+H">Wing Hung Wong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 23 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Applications (stat.AP); Methodology (stat.ME)

</div>
<p class="mathjax">The task of precisely learning the probability distribution of rows within
tabular data and producing authentic synthetic samples is both crucial and
non-trivial. Wasserstein generative adversarial network (WGAN) marks a notable
improvement in generative modeling, addressing the challenges faced by its
predecessor, generative adversarial network. However, due to the mixed data
types and multimodalities prevalent in tabular data, the delicate equilibrium
between the generator and discriminator, as well as the inherent instability of
Wasserstein distance in high dimensions, WGAN often fails to produce
high-fidelity samples. To this end, we propose POTNet (Penalized Optimal
Transport Network), a generative deep neural network based on a novel, robust,
and interpretable marginally-penalized Wasserstein (MPW) loss. POTNet can
effectively model tabular data containing both categorical and continuous
features. Moreover, it offers the flexibility to condition on a subset of
features. We provide theoretical justifications for the motivation behind the
MPW loss. We also empirically demonstrate the effectiveness of our proposed
method on four different benchmarks across a variety of real-world and
simulated datasets. Our proposed model achieves orders of magnitude speedup
during the sampling stage compared to state-of-the-art generative models for
tabular data, thereby enabling efficient large-scale synthetic data generation.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10475" title="Abstract">arXiv:2402.10475</a> (cross-list from math.OC) [<a href="/pdf/2402.10475" title="Download PDF">pdf</a>, <a href="/format/2402.10475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fundamental Benefit of Alternating Updates in Minimax Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Lee%2C+J">Jaewook Lee</a>, 
<a href="/search/math?searchtype=author&query=Cho%2C+H">Hanseul Cho</a>, 
<a href="/search/math?searchtype=author&query=Yun%2C+C">Chulhee Yun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 77 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The Gradient Descent-Ascent (GDA) algorithm, designed to solve minimax
optimization problems, takes the descent and ascent steps either simultaneously
(Sim-GDA) or alternately (Alt-GDA). While Alt-GDA is commonly observed to
converge faster, the performance gap between the two is not yet well understood
theoretically, especially in terms of global convergence rates. To address this
theory-practice gap, we present fine-grained convergence analyses of both
algorithms for strongly-convex-strongly-concave and Lipschitz-gradient
objectives. Our new iteration complexity upper bound of Alt-GDA is strictly
smaller than the lower bound of Sim-GDA; i.e., Alt-GDA is provably faster.
Moreover, we propose Alternating-Extrapolation GDA (Alex-GDA), a general
algorithmic framework that subsumes Sim-GDA and Alt-GDA, for which the main
idea is to alternately take gradients from extrapolations of the iterates. We
show that Alex-GDA satisfies a smaller iteration complexity bound, identical to
that of the Extra-gradient method, while requiring less gradient computations.
We also prove that Alex-GDA enjoys linear convergence for bilinear problems,
for which both Sim-GDA and Alt-GDA fail to converge at all.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10481" title="Abstract">arXiv:2402.10481</a> (cross-list from q-fin.CP) [<a href="/pdf/2402.10481" title="Download PDF">pdf</a>, <a href="/ps/2402.10481" title="Download PostScript">ps</a>, <a href="/format/2402.10481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emoji Driven Crypto Assets Market Reactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Zuo%2C+X">Xiaorui Zuo</a>, 
<a href="/search/q-fin?searchtype=author&query=Chen%2C+Y">Yao-Tsung Chen</a>, 
<a href="/search/q-fin?searchtype=author&query=H%C3%A4rdle%2C+W+K">Wolfgang Karl H&#xe4;rdle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Finance (q-fin.CP)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG); Statistical Finance (q-fin.ST)

</div>
<p class="mathjax">In the burgeoning realm of cryptocurrency, social media platforms like
Twitter have become pivotal in influencing market trends and investor
sentiments. In our study, we leverage GPT-4 and a fine-tuned transformer-based
BERT model for a multimodal sentiment analysis, focusing on the impact of emoji
sentiment on cryptocurrency markets. By translating emojis into quantifiable
sentiment data, we correlate these insights with key market indicators like BTC
Price and the VCRIX index. This approach may be fed into the development of
trading strategies aimed at utilizing social media elements to identify and
forecast market trends. Crucially, our findings suggest that strategies based
on emoji sentiment can facilitate the avoidance of significant market downturns
and contribute to the stabilization of returns. This research underscores the
practical benefits of integrating advanced AI-driven analyses into financial
strategies, offering a nuanced perspective on the interplay between digital
communication and market dynamics in an academic context.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10502" title="Abstract">arXiv:2402.10502</a> (cross-list from astro-ph.CO) [<a href="/pdf/2402.10502" title="Download PDF">pdf</a>, <a href="/format/2402.10502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Late-time transition of $M_B$ inferred via neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Mukherjee%2C+P">Purba Mukherjee</a>, 
<a href="/search/astro-ph?searchtype=author&query=Dialektopoulos%2C+K+F">Konstantinos F. Dialektopoulos</a>, 
<a href="/search/astro-ph?searchtype=author&query=Said%2C+J+L">Jackson Levi Said</a>, 
<a href="/search/astro-ph?searchtype=author&query=Mifsud%2C+J">Jurgen Mifsud</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 sets of figures, 2 tables. Comments are welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cosmology and Nongalactic Astrophysics (astro-ph.CO)</span>; Machine Learning (cs.LG); General Relativity and Quantum Cosmology (gr-qc)

</div>
<p class="mathjax">The strengthening of tensions in the cosmological parameters has led to a
reconsideration of fundamental aspects of standard cosmology. The tension in
the Hubble constant can also be viewed as a tension between local and early
Universe constraints on the absolute magnitude $M_B$ of Type Ia supernova. In
this work, we reconsider the possibility of a variation of this parameter in a
model-independent way. We employ neural networks to agnostically constrain the
value of the absolute magnitude as well as assess the impact and statistical
significance of a variation in $M_B$ with redshift from the Pantheon+
compilation, together with a thorough analysis of the neural network
architecture. We find an indication for a transition redshift at the $z\approx
1$ region.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10504" title="Abstract">arXiv:2402.10504</a> (cross-list from math.PR) [<a href="/pdf/2402.10504" title="Download PDF">pdf</a>, <a href="/format/2402.10504" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resilience of the quadratic Littlewood-Offord problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Aigner-Horev%2C+E">Elad Aigner-Horev</a>, 
<a href="/search/math?searchtype=author&query=Rozenberg%2C+D">Daniel Rozenberg</a>, 
<a href="/search/math?searchtype=author&query=Weiss%2C+R">Roi Weiss</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Information Theory (cs.IT); Machine Learning (cs.LG); Combinatorics (math.CO); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study the statistical resilience of high-dimensional data. Our results
provide estimates as to the effects of adversarial noise over the
anti-concentration properties of the quadratic Radamecher chaos
$\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi}$, where $M$ is a fixed
(high-dimensional) matrix and $\boldsymbol{\xi}$ is a conformal Rademacher
vector. Specifically, we pursue the question of how many adversarial sign-flips
can $\boldsymbol{\xi}$ sustain without "inflating" $\sup_{x\in \mathbb{R}}
\mathbb{P} \left\{\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi} = x\right\}$
and thus "de-smooth" the original distribution resulting in a more "grainy" and
adversarially biased distribution. Our results provide lower bound estimations
for the statistical resilience of the quadratic and bilinear Rademacher chaos;
these are shown to be asymptotically tight across key regimes.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10515" title="Abstract">arXiv:2402.10515</a> (cross-list from eess.SP) [<a href="/pdf/2402.10515" title="Download PDF">pdf</a>, <a href="/format/2402.10515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Power-Efficient Indoor Localization Using Adaptive Channel-aware  Ultra-wideband DL-TDOA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bhattacharya%2C+S">Sagnik Bhattacharya</a>, 
<a href="/search/eess?searchtype=author&query=Choi%2C+J">Junyoung Choi</a>, 
<a href="/search/eess?searchtype=author&query=Lee%2C+J">Joohyun Lee</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE GLOBECOM 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Among the various Ultra-wideband (UWB) ranging methods, the absence of uplink
communication or centralized computation makes downlink
time-difference-of-arrival (DL-TDOA) localization the most suitable for
large-scale industrial deployments. However, temporary or permanent obstacles
in the deployment region often lead to non-line-of-sight (NLOS) channel path
and signal outage effects, which result in localization errors. Prior research
has addressed this problem by increasing the ranging frequency, which leads to
a heavy increase in the user device power consumption. It also does not
contribute to any increase in localization accuracy under line-of-sight (LOS)
conditions. In this paper, we propose and implement a novel low-power
channel-aware dynamic frequency DL-TDOA ranging algorithm. It comprises NLOS
probability predictor based on a convolutional neural network (CNN), a dynamic
ranging frequency control module, and an IMU sensor-based ranging filter. Based
on the conducted experiments, we show that the proposed algorithm achieves 50%
higher accuracy in NLOS conditions while having 46% lower power consumption in
LOS conditions compared to baseline methods from prior research.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10516" title="Abstract">arXiv:2402.10516</a> (cross-list from q-bio.BM) [<a href="/pdf/2402.10516" title="Download PDF">pdf</a>, <a href="/format/2402.10516" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative AI for Controllable Protein Sequence Design: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Zhu%2C+Y">Yiheng Zhu</a>, 
<a href="/search/q-bio?searchtype=author&query=Kong%2C+Z">Zitai Kong</a>, 
<a href="/search/q-bio?searchtype=author&query=Wu%2C+J">Jialu Wu</a>, 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+W">Weize Liu</a>, 
<a href="/search/q-bio?searchtype=author&query=Han%2C+Y">Yuqiang Han</a>, 
<a href="/search/q-bio?searchtype=author&query=Yin%2C+M">Mingze Yin</a>, 
<a href="/search/q-bio?searchtype=author&query=Xu%2C+H">Hongxia Xu</a>, 
<a href="/search/q-bio?searchtype=author&query=Hsieh%2C+C">Chang-Yu Hsieh</a>, 
<a href="/search/q-bio?searchtype=author&query=Hou%2C+T">Tingjun Hou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The design of novel protein sequences with targeted functionalities underpins
a central theme in protein engineering, impacting diverse fields such as drug
discovery and enzymatic engineering. However, navigating this vast
combinatorial search space remains a severe challenge due to time and financial
constraints. This scenario is rapidly evolving as the transformative
advancements in AI, particularly in the realm of generative models and
optimization algorithms, have been propelling the protein design field towards
an unprecedented revolution. In this survey, we systematically review recent
advances in generative AI for controllable protein sequence design. To set the
stage, we first outline the foundational tasks in protein sequence design in
terms of the constraints involved and present key generative models and
optimization algorithms. We then offer in-depth reviews of each design task and
discuss the pertinent applications. Finally, we identify the unresolved
challenges and highlight research opportunities that merit deeper exploration.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10609" title="Abstract">arXiv:2402.10609</a> (cross-list from eess.IV) [<a href="/pdf/2402.10609" title="Download PDF">pdf</a>, <a href="/format/2402.10609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> U$^2$MRPD: Unsupervised undersampled MRI reconstruction by prompting a  large latent diffusion model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gao%2C+Z">Ziqi Gao</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+S+K">S. Kevin Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 6 figures, 5 tables, 2 pseudocodes
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Implicit visual knowledge in a large latent diffusion model (LLDM)
pre-trained on natural images is rich and hypothetically universal to natural
and medical images. To test this hypothesis, we introduce a novel framework for
Unsupervised Undersampled MRI Reconstruction by Prompting a pre-trained large
latent Diffusion model ( U$^2$MRPD). Existing data-driven, supervised
undersampled MRI reconstruction networks are typically of limited
generalizability and adaptability toward diverse data acquisition scenarios;
yet U$^2$MRPD supports image-specific MRI reconstruction by prompting an LLDM
with an MRSampler tailored for complex-valued MRI images. With any
single-source or diverse-source MRI dataset, U$^2$MRPD's performance is further
boosted by an MRAdapter while keeping the generative image priors intact.
Experiments on multiple datasets show that U$^2$MRPD achieves comparable or
better performance than supervised and MRI diffusion methods on in-domain
datasets while demonstrating the best generalizability on out-of-domain
datasets. To the best of our knowledge, U$^2$MRPD is the {\bf first}
unsupervised method that demonstrates the universal prowess of a LLDM, %trained
on magnitude-only natural images in medical imaging, attaining the best
adaptability for both MRI database-free and database-available scenarios and
generalizability towards out-of-domain data.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10642" title="Abstract">arXiv:2402.10642</a> (cross-list from eess.AS) [<a href="/pdf/2402.10642" title="Download PDF">pdf</a>, <a href="/format/2402.10642" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speaking in Wavelet Domain: A Simple and Efficient Approach to Speed up  Speech Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X">Xiangyu Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+D">Daijiao Liu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+H">Hexin Liu</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Q">Qiquan Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Meng%2C+H">Hanyu Meng</a>, 
<a href="/search/eess?searchtype=author&query=Garcia%2C+L+P">Leibny Paola Garcia</a>, 
<a href="/search/eess?searchtype=author&query=Chng%2C+E+S">Eng Siong Chng</a>, 
<a href="/search/eess?searchtype=author&query=Yao%2C+L">Lina Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recently, Denoising Diffusion Probabilistic Models (DDPMs) have attained
leading performances across a diverse range of generative tasks. However, in
the field of speech synthesis, although DDPMs exhibit impressive performance,
their long training duration and substantial inference costs hinder practical
deployment. Existing approaches primarily focus on enhancing inference speed,
while approaches to accelerate training a key factor in the costs associated
with adding or customizing voices often necessitate complex modifications to
the model, compromising their universal applicability. To address the
aforementioned challenges, we propose an inquiry: is it possible to enhance the
training/inference speed and performance of DDPMs by modifying the speech
signal itself? In this paper, we double the training and inference speed of
Speech DDPMs by simply redirecting the generative target to the wavelet domain.
This method not only achieves comparable or superior performance to the
original model in speech synthesis tasks but also demonstrates its versatility.
By investigating and utilizing different wavelet bases, our approach proves
effective not just in speech synthesis, but also in speech enhancement.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10674" title="Abstract">arXiv:2402.10674</a> (cross-list from math.AG) [<a href="/pdf/2402.10674" title="Download PDF">pdf</a>, <a href="/format/2402.10674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Border subrank via a generalised Hilbert-Mumford criterion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Biaggi%2C+B">Benjamin Biaggi</a>, 
<a href="/search/math?searchtype=author&query=Chang%2C+C">Chia-Yu Chang</a>, 
<a href="/search/math?searchtype=author&query=Draisma%2C+J">Jan Draisma</a>, 
<a href="/search/math?searchtype=author&query=Rupniewski%2C+F">Filip Rupniewski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Geometry (math.AG)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">We show that the border subrank of a sufficiently general tensor in
$(\mathbb{C}^n)^{\otimes d}$ is $\mathcal{O}(n^{1/(d-1)})$ for $n \to \infty$.
Since this matches the growth rate $\Theta(n^{1/(d-1)})$ for the generic
(non-border) subrank recently established by Derksen-Makam-Zuiddam, we find
that the generic border subrank has the same growth rate. In our proof, we use
a generalisation of the Hilbert-Mumford criterion that we believe will be of
independent interest.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10677" title="Abstract">arXiv:2402.10677</a> (cross-list from stat.ML) [<a href="/pdf/2402.10677" title="Download PDF">pdf</a>, <a href="/format/2402.10677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance Gaps in Multi-view Clustering under the Nested Matrix-Tensor  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lebeau%2C+H">Hugo Lebeau</a>, 
<a href="/search/stat?searchtype=author&query=Seddik%2C+M+E+A">Mohamed El Amine Seddik</a>, 
<a href="/search/stat?searchtype=author&query=de+Morais+Goulart%2C+J+H">Jos&#xe9; Henrique de Morais Goulart</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Probability (math.PR)

</div>
<p class="mathjax">We study the estimation of a planted signal hidden in a recently introduced
nested matrix-tensor model, which is an extension of the classical spiked
rank-one tensor model, motivated by multi-view clustering. Prior work has
theoretically examined the performance of a tensor-based approach, which relies
on finding a best rank-one approximation, a problem known to be computationally
hard. A tractable alternative approach consists in computing instead the best
rank-one (matrix) approximation of an unfolding of the observed tensor data,
but its performance was hitherto unknown. We quantify here the performance gap
between these two approaches, in particular by deriving the precise algorithmic
threshold of the unfolding approach and demonstrating that it exhibits a
BBP-type transition behavior. This work is therefore in line with recent
contributions which deepen our understanding of why tensor-based methods
surpass matrix-based methods in handling structured tensor data.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10678" title="Abstract">arXiv:2402.10678</a> (cross-list from quant-ph) [<a href="/pdf/2402.10678" title="Download PDF">pdf</a>, <a href="/format/2402.10678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Covering a Graph with Minimal Local Sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Claudet%2C+N">Nathan Claudet</a>, 
<a href="/search/quant-ph?searchtype=author&query=Perdrix%2C+S">Simon Perdrix</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">Local sets, a graph structure invariant under local complementation, have
been originally introduced in the context of quantum computing for the study of
quantum entanglement within the so-called graph state formalism. A local set in
a graph is made of a non-empty set of vertices together with its odd
neighborhood. We show that any graph can be covered by minimal local sets, i.e.
that every vertex is contained in at least one local set that is minimal by
inclusion. More precisely, we introduce an algorithm for finding a minimal
local set cover in polynomial time. This result is proved by exploring the link
between local sets and cut-rank. We prove some additional results on minimal
local sets: we give tight bounds on their size, and we show that there can be
exponentially many of them in a graph. Finally, we provide an extension of our
definitions and our main result to $q$-multigraphs, the graphical counterpart
of quantum qudit graph states.
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10687" title="Abstract">arXiv:2402.10687</a> (cross-list from eess.SP) [<a href="/pdf/2402.10687" title="Download PDF">pdf</a>, <a href="/format/2402.10687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beamforming Optimization for Active RIS-Aided Multiuser Communications  With Hardware Impairments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Peng%2C+Z">Zhangjie Peng</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Z">Zhibo Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Pan%2C+C">Cunhua Pan</a>, 
<a href="/search/eess?searchtype=author&query=Di+Renzo%2C+M">Marco Di Renzo</a>, 
<a href="/search/eess?searchtype=author&query=Dobre%2C+O+A">Octavia A. Dobre</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jiangzhou Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 8 figures, accepted by IEEE Transactions on Wireless Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">In this paper, we consider an active reconfigurable intelligent surface (RIS)
to assist the multiuser downlink transmission in the presence of practical
hardware impairments (HWIs), including the HWIs at the transceivers and the
phase noise at the active RIS. The active RIS is deployed to amplify the
incident signals to alleviate the multiplicative fading effect, which is a
limitation in the conventional passive RIS-aided wireless systems. We aim to
maximize the sum rate through jointly designing the transmit beamforming at the
base station (BS), the amplification factors and the phase shifts at the active
RIS. To tackle this challenging optimization problem effectively, we decouple
it into two tractable subproblems. Subsequently, each subproblem is transformed
into a second order cone programming problem. The block coordinate descent
framework is applied to tackle them, where the transmit beamforming and the
reflection coefficients are alternately designed. In addition, another
efficient algorithm is presented to reduce the computational complexity.
Specifically, by exploiting the majorization-minimization approach, each
subproblem is reformulated into a tractable surrogate problem, whose
closed-form solutions are obtained by Lagrange dual decomposition approach and
element-wise alternating sequential optimization method. Simulation results
validate the effectiveness of our developed algorithms, and reveal that the
HWIs significantly limit the system performance of active RIS-empowered
wireless communications. Furthermore, the active RIS noticeably boosts the sum
rate under the same total power budget, compared with the passive RIS.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10708" title="Abstract">arXiv:2402.10708</a> (cross-list from math.OC) [<a href="/pdf/2402.10708" title="Download PDF">pdf</a>, <a href="/format/2402.10708" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Application of an adaptive model hierarchy to parametrized optimal  control problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kleikamp%2C+H">Hendrik Kleikamp</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">In this contribution we apply an adaptive model hierarchy, consisting of a
full-order model, a reduced basis reduced order model, and a machine learning
surrogate, to parametrized linear-quadratic optimal control problems. The
involved reduced order models are constructed adaptively and are called in such
a way that the model hierarchy returns an approximate solution of given
accuracy for every parameter value. At the same time, the fastest model of the
hierarchy is evaluated whenever possible and slower models are only queried if
the faster ones are not sufficiently accurate. The performance of the model
hierarchy is studied for a parametrized heat equation example with boundary
value control.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10723" title="Abstract">arXiv:2402.10723</a> (cross-list from stat.ML) [<a href="/pdf/2402.10723" title="Download PDF">pdf</a>, <a href="/format/2402.10723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conformalized Credal Set Predictors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Javanmardi%2C+A">Alireza Javanmardi</a>, 
<a href="/search/stat?searchtype=author&query=Stutz%2C+D">David Stutz</a>, 
<a href="/search/stat?searchtype=author&query=H%C3%BCllermeier%2C+E">Eyke H&#xfc;llermeier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Credal sets are sets of probability distributions that are considered as
candidates for an imprecisely known ground-truth distribution. In machine
learning, they have recently attracted attention as an appealing formalism for
uncertainty representation, in particular due to their ability to represent
both the aleatoric and epistemic uncertainty in a prediction. However, the
design of methods for learning credal set predictors remains a challenging
problem. In this paper, we make use of conformal prediction for this purpose.
More specifically, we propose a method for predicting credal sets in the
classification task, given training data labeled by probability distributions.
Since our method inherits the coverage guarantees of conformal prediction, our
conformal credal sets are guaranteed to be valid with high probability (without
any assumptions on model or distribution). We demonstrate the applicability of
our method to natural language inference, a highly ambiguous natural language
task where it is common to obtain multiple annotations per example.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10727" title="Abstract">arXiv:2402.10727</a> (cross-list from stat.ML) [<a href="/pdf/2402.10727" title="Download PDF">pdf</a>, <a href="/format/2402.10727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predictive Uncertainty Quantification via Risk Decompositions for  Strictly Proper Scoring Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kotelevskii%2C+N">Nikita Kotelevskii</a>, 
<a href="/search/stat?searchtype=author&query=Panov%2C+M">Maxim Panov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Distinguishing sources of predictive uncertainty is of crucial importance in
the application of forecasting models across various domains. Despite the
presence of a great variety of proposed uncertainty measures, there are no
strict definitions to disentangle them. Furthermore, the relationship between
different measures of uncertainty quantification remains somewhat unclear. In
this work, we introduce a general framework, rooted in statistical reasoning,
which not only allows the creation of new uncertainty measures but also
clarifies their interrelations. Our approach leverages statistical risk to
distinguish aleatoric and epistemic uncertainty components and utilizes proper
scoring rules to quantify them. To make it practically tractable, we propose an
idea to incorporate Bayesian reasoning into this framework and discuss the
properties of the proposed approximation.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10728" title="Abstract">arXiv:2402.10728</a> (cross-list from eess.IV) [<a href="/pdf/2402.10728" title="Download PDF">pdf</a>, <a href="/format/2402.10728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-weakly-supervised neural network training for medical image  registration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+Y">Yiwen Li</a>, 
<a href="/search/eess?searchtype=author&query=Fu%2C+Y">Yunguan Fu</a>, 
<a href="/search/eess?searchtype=author&query=Gayo%2C+I+J+M+B">Iani J.M.B. Gayo</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+Q">Qianye Yang</a>, 
<a href="/search/eess?searchtype=author&query=Min%2C+Z">Zhe Min</a>, 
<a href="/search/eess?searchtype=author&query=Saeed%2C+S+U">Shaheer U. Saeed</a>, 
<a href="/search/eess?searchtype=author&query=Yan%2C+W">Wen Yan</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yipei Wang</a>, 
<a href="/search/eess?searchtype=author&query=Noble%2C+J+A">J. Alison Noble</a>, 
<a href="/search/eess?searchtype=author&query=Emberton%2C+M">Mark Emberton</a>, 
<a href="/search/eess?searchtype=author&query=Clarkson%2C+M+J">Matthew J. Clarkson</a>, 
<a href="/search/eess?searchtype=author&query=Barratt%2C+D+C">Dean C. Barratt</a>, 
<a href="/search/eess?searchtype=author&query=Prisacariu%2C+V+A">Victor A. Prisacariu</a>, 
<a href="/search/eess?searchtype=author&query=Hu%2C+Y">Yipeng Hu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">For training registration networks, weak supervision from segmented
corresponding regions-of-interest (ROIs) have been proven effective for (a)
supplementing unsupervised methods, and (b) being used independently in
registration tasks in which unsupervised losses are unavailable or ineffective.
This correspondence-informing supervision entails cost in annotation that
requires significant specialised effort. This paper describes a
semi-weakly-supervised registration pipeline that improves the model
performance, when only a small corresponding-ROI-labelled dataset is available,
by exploiting unlabelled image pairs. We examine two types of augmentation
methods by perturbation on network weights and image resampling, such that
consistency-based unsupervised losses can be applied on unlabelled data. The
novel WarpDDF and RegCut approaches are proposed to allow commutative
perturbation between an image pair and the predicted spatial transformation
(i.e. respective input and output of registration networks), distinct from
existing perturbation methods for classification or segmentation. Experiments
using 589 male pelvic MR images, labelled with eight anatomical ROIs, show the
improvement in registration performance and the ablated contributions from the
individual strategies. Furthermore, this study attempts to construct one of the
first computational atlases for pelvic structures, enabled by registering
inter-subject MRs, and quantifies the significant differences due to the
proposed semi-weak supervision with a discussion on the potential clinical use
of example atlas-derived statistics.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10748" title="Abstract">arXiv:2402.10748</a> (cross-list from eess.SP) [<a href="/pdf/2402.10748" title="Download PDF">pdf</a>, <a href="/format/2402.10748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Noisy Beat is Worth 16 Words: a Tiny Transformer for Low-Power  Arrhythmia Classification on Microcontrollers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Busia%2C+P">Paola Busia</a>, 
<a href="/search/eess?searchtype=author&query=Scrugli%2C+M+A">Matteo Antonio Scrugli</a>, 
<a href="/search/eess?searchtype=author&query=Jung%2C+V+J">Victor Jean-Baptiste Jung</a>, 
<a href="/search/eess?searchtype=author&query=Benini%2C+L">Luca Benini</a>, 
<a href="/search/eess?searchtype=author&query=Meloni%2C+P">Paolo Meloni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Wearable systems for the long-term monitoring of cardiovascular diseases are
becoming widespread and valuable assets in diagnosis and therapy. A promising
approach for real-time analysis of the electrocardiographic (ECG) signal and
the detection of heart conditions, such as arrhythmia, is represented by the
transformer machine learning model. Transformers are powerful models for the
classification of time series, although efficient implementation in the
wearable domain raises significant design challenges, to combine adequate
accuracy and a suitable complexity. In this work, we present a tiny transformer
model for the analysis of the ECG signal, requiring only 6k parameters and
reaching 98.97% accuracy in the recognition of the 5 most common arrhythmia
classes from the MIT-BIH Arrhythmia database, assessed considering 8-bit
integer inference as required for efficient execution on low-power
microcontroller-based devices. We explored an augmentation-based training
approach for improving the robustness against electrode motion artifacts noise,
resulting in a worst-case post-deployment performance assessment of 98.36%
accuracy. Suitability for wearable monitoring solutions is finally demonstrated
through efficient deployment on the parallel ultra-low-power GAP9 processor,
where inference execution requires 4.28ms and 0.09mJ.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10758" title="Abstract">arXiv:2402.10758</a> (cross-list from stat.ML) [<a href="/pdf/2402.10758" title="Download PDF">pdf</a>, <a href="/format/2402.10758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Localization via Iterative Posterior Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Grenioux%2C+L">Louis Grenioux</a>, 
<a href="/search/stat?searchtype=author&query=Noble%2C+M">Maxence Noble</a>, 
<a href="/search/stat?searchtype=author&query=Gabri%C3%A9%2C+M">Marylou Gabri&#xe9;</a>, 
<a href="/search/stat?searchtype=author&query=Durmus%2C+A+O">Alain Oliviero Durmus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Computation (stat.CO)

</div>
<p class="mathjax">Building upon score-based learning, new interest in stochastic localization
techniques has recently emerged. In these models, one seeks to noise a sample
from the data distribution through a stochastic process, called observation
process, and progressively learns a denoiser associated to this dynamics. Apart
from specific applications, the use of stochastic localization for the problem
of sampling from an unnormalized target density has not been explored
extensively. This work contributes to fill this gap. We consider a general
stochastic localization framework and introduce an explicit class of
observation processes, associated with flexible denoising schedules. We provide
a complete methodology, $\textit{Stochastic Localization via Iterative
Posterior Sampling}$ (SLIPS), to obtain approximate samples of this dynamics,
and as a by-product, samples from the target distribution. Our scheme is based
on a Markov chain Monte Carlo estimation of the denoiser and comes with
detailed practical guidelines. We illustrate the benefits and applicability of
SLIPS on several benchmarks, including Gaussian mixtures in increasing
dimensions, Bayesian logistic regression and a high-dimensional field system
from statistical-mechanics.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10760" title="Abstract">arXiv:2402.10760</a> (cross-list from q-fin.ST) [<a href="/pdf/2402.10760" title="Download PDF">pdf</a>, <a href="/format/2402.10760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RAGIC: Risk-Aware Generative Adversarial Model for Stock Interval  Construction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Gu%2C+J">Jingyi Gu</a>, 
<a href="/search/q-fin?searchtype=author&query=Du%2C+W">Wenlu Du</a>, 
<a href="/search/q-fin?searchtype=author&query=Wang%2C+G">Guiling Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistical Finance (q-fin.ST)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Efforts to predict stock market outcomes have yielded limited success due to
the inherently stochastic nature of the market, influenced by numerous
unpredictable factors. Many existing prediction approaches focus on
single-point predictions, lacking the depth needed for effective
decision-making and often overlooking market risk. To bridge this gap, we
propose a novel model, RAGIC, which introduces sequence generation for stock
interval prediction to quantify uncertainty more effectively. Our approach
leverages a Generative Adversarial Network (GAN) to produce future price
sequences infused with randomness inherent in financial markets. RAGIC's
generator includes a risk module, capturing the risk perception of informed
investors, and a temporal module, accounting for historical price trends and
seasonality. This multi-faceted generator informs the creation of
risk-sensitive intervals through statistical inference, incorporating
horizon-wise insights. The interval's width is carefully adjusted to reflect
market volatility. Importantly, our approach relies solely on publicly
available data and incurs only low computational overhead. RAGIC's evaluation
across globally recognized broad-based indices demonstrates its balanced
performance, offering both accuracy and informativeness. Achieving a consistent
95% coverage, RAGIC maintains a narrow interval width. This promising outcome
suggests that our approach effectively addresses the challenges of stock market
prediction while incorporating vital risk considerations.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10776" title="Abstract">arXiv:2402.10776</a> (cross-list from eess.IV) [<a href="/pdf/2402.10776" title="Download PDF">pdf</a>, <a href="/ps/2402.10776" title="Download PostScript">ps</a>, <a href="/format/2402.10776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Vivo Hyperspectral Human Brain Image Database for Brain Cancer  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Fabelo%2C+H">H. Fabelo</a>, 
<a href="/search/eess?searchtype=author&query=Ortega%2C+S">S. Ortega</a>, 
<a href="/search/eess?searchtype=author&query=Szolna%2C+A">A. Szolna</a>, 
<a href="/search/eess?searchtype=author&query=Bulters%2C+D">D. Bulters</a>, 
<a href="/search/eess?searchtype=author&query=Pineiro%2C+J+F">J.F. Pineiro</a>, 
<a href="/search/eess?searchtype=author&query=Kabwama%2C+S">S. Kabwama</a>, 
<a href="/search/eess?searchtype=author&query=Shanahan%2C+A">A. Shanahan</a>, 
<a href="/search/eess?searchtype=author&query=Bulstrode%2C+H">H. Bulstrode</a>, 
<a href="/search/eess?searchtype=author&query=Bisshopp%2C+S">S. Bisshopp</a>, 
<a href="/search/eess?searchtype=author&query=Kiran%2C+B+R">B.R. Kiran</a>, 
<a href="/search/eess?searchtype=author&query=Ravi%2C+D">D. Ravi</a>, 
<a href="/search/eess?searchtype=author&query=Lazcano%2C+R">R. Lazcano</a>, 
<a href="/search/eess?searchtype=author&query=Madronal%2C+D">D. Madronal</a>, 
<a href="/search/eess?searchtype=author&query=Sosa%2C+C">C. Sosa</a>, 
<a href="/search/eess?searchtype=author&query=Espino%2C+C">C. Espino</a>, 
<a href="/search/eess?searchtype=author&query=Marquez%2C+M">M. Marquez</a>, 
<a href="/search/eess?searchtype=author&query=De+la+Luz+Plaza%2C+M">M. De la Luz Plaza</a>, 
<a href="/search/eess?searchtype=author&query=Camacho%2C+R">R. Camacho</a>, 
<a href="/search/eess?searchtype=author&query=Carrera%2C+D">D. Carrera</a>, 
<a href="/search/eess?searchtype=author&query=Hernandez%2C+M">M. Hernandez</a>, 
<a href="/search/eess?searchtype=author&query=Callico%2C+G+M">G.M. Callico</a>, 
<a href="/search/eess?searchtype=author&query=Morera%2C+J">J. Morera</a>, 
<a href="/search/eess?searchtype=author&query=Stanciulescu%2C+B">B. Stanciulescu</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+G+Z">G.Z. Yang</a>, 
<a href="/search/eess?searchtype=author&query=Salvador%2C+R">R. Salvador</a>, 
<a href="/search/eess?searchtype=author&query=Juarez%2C+E">E. Juarez</a>, 
<a href="/search/eess?searchtype=author&query=Sanz%2C+C">C. Sanz</a>, 
<a href="/search/eess?searchtype=author&query=Sarmiento%2C+R">R. Sarmiento</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 12 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Access, 2019, 7, pp. 39098 39116
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The use of hyperspectral imaging for medical applications is becoming more
common in recent years. One of the main obstacles that researchers find when
developing hyperspectral algorithms for medical applications is the lack of
specific, publicly available, and hyperspectral medical data. The work
described in this paper was developed within the framework of the European
project HELICoiD (HypErspectraL Imaging Cancer Detection), which had as a main
goal the application of hyperspectral imaging to the delineation of brain
tumors in real-time during neurosurgical operations. In this paper, the
methodology followed to generate the first hyperspectral database of in-vivo
human brain tissues is presented. Data was acquired employing a customized
hyperspectral acquisition system capable of capturing information in the Visual
and Near InfraRed (VNIR) range from 400 to 1000 nm. Repeatability was assessed
for the cases where two images of the same scene were captured consecutively.
The analysis reveals that the system works more efficiently in the spectral
range between 450 and 900 nm. A total of 36 hyperspectral images from 22
different patients were obtained. From these data, more than 300 000 spectral
signatures were labeled employing a semi-automatic methodology based on the
spectral angle mapper algorithm. Four different classes were defined: normal
tissue, tumor tissue, blood vessel, and background elements. All the
hyperspectral data has been made available in a public repository.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10803" title="Abstract">arXiv:2402.10803</a> (cross-list from q-fin.CP) [<a href="/pdf/2402.10803" title="Download PDF">pdf</a>, <a href="/format/2402.10803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modelling crypto markets by multi-agent reinforcement learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Lussange%2C+J">Johann Lussange</a>, 
<a href="/search/q-fin?searchtype=author&query=Vrizzi%2C+S">Stefano Vrizzi</a>, 
<a href="/search/q-fin?searchtype=author&query=Palminteri%2C+S">Stefano Palminteri</a>, 
<a href="/search/q-fin?searchtype=author&query=Gutkin%2C+B">Boris Gutkin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Finance (q-fin.CP)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Building on a previous foundation work (Lussange et al. 2020), this study
introduces a multi-agent reinforcement learning (MARL) model simulating crypto
markets, which is calibrated to the Binance's daily closing prices of $153$
cryptocurrencies that were continuously traded between 2018 and 2022. Unlike
previous agent-based models (ABM) or multi-agent systems (MAS) which relied on
zero-intelligence agents or single autonomous agent methodologies, our approach
relies on endowing agents with reinforcement learning (RL) techniques in order
to model crypto markets. This integration is designed to emulate, with a
bottom-up approach to complexity inference, both individual and collective
agents, ensuring robustness in the recent volatile conditions of such markets
and during the COVID-19 era. A key feature of our model also lies in the fact
that its autonomous agents perform asset price valuation based on two sources
of information: the market prices themselves, and the approximation of the
crypto assets fundamental values beyond what those market prices are. Our MAS
calibration against real market data allows for an accurate emulation of crypto
markets microstructure and probing key market behaviors, in both the bearish
and bullish regimes of that particular time period.
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10831" title="Abstract">arXiv:2402.10831</a> (cross-list from eess.IV) [<a href="/pdf/2402.10831" title="Download PDF">pdf</a>, <a href="/format/2402.10831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GAN-driven Electromagnetic Imaging of 2-D Dielectric Scatterers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Naseer%2C+E">Ehtasham Naseer</a>, 
<a href="/search/eess?searchtype=author&query=Sandhu%2C+A+I">Ali Imran Sandhu</a>, 
<a href="/search/eess?searchtype=author&query=Siddique%2C+M+A">Muhammad Adnan Siddique</a>, 
<a href="/search/eess?searchtype=author&query=Ahmed%2C+W+W">Waqas W. Ahmed</a>, 
<a href="/search/eess?searchtype=author&query=Farhat%2C+M">Mohamed Farhat</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+Y">Ying Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">Inverse scattering problems are inherently challenging, given the fact they
are ill-posed and nonlinear. This paper presents a powerful deep learning-based
approach that relies on generative adversarial networks to accurately and
efficiently reconstruct randomly-shaped two-dimensional dielectric objects from
amplitudes of multi-frequency scattered electric fields. An adversarial
autoencoder (AAE) is trained to learn to generate the scatterer's geometry from
a lower-dimensional latent representation constrained to adhere to the Gaussian
distribution. A cohesive inverse neural network (INN) framework is set up
comprising a sequence of appropriately designed dense layers, the
already-trained generator as well as a separately trained forward neural
network. The images reconstructed at the output of the inverse network are
validated through comparison with outputs from the forward neural network,
addressing the non-uniqueness challenge inherent to electromagnetic (EM)
imaging problems. The trained INN demonstrates an enhanced robustness,
evidenced by a mean binary cross-entropy (BCE) loss of $0.13$ and a structure
similarity index (SSI) of $0.90$. The study not only demonstrates a significant
reduction in computational load, but also marks a substantial improvement over
traditional objective-function-based methods. It contributes both to the fields
of machine learning and EM imaging by offering a real-time quantitative imaging
approach. The results obtained with the simulated data, for both training and
testing, yield promising results and may open new avenues for radio-frequency
inverse imaging.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10834" title="Abstract">arXiv:2402.10834</a> (cross-list from stat.AP) [<a href="/pdf/2402.10834" title="Download PDF">pdf</a>, <a href="/format/2402.10834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Agent-based Simulation Evaluation of CBD Tolling: A Case Study from New  York City
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Liang%2C+Q">Qingnan Liang</a>, 
<a href="/search/stat?searchtype=author&query=Yao%2C+R">Ruili Yao</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+R">Ruixuan Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+Z">Zhibin Chen</a>, 
<a href="/search/stat?searchtype=author&query=Wu%2C+G">Guoyuan Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by 2024 IEEE Forum on Integrated and Sustainable Transportation Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Congestion tollings have been widely developed and adopted as an effective
tool to mitigate urban traffic congestion and enhance transportation system
sustainability. Nevertheless, these tolling schemes are often tailored on a
city-by-city or even area-by-area basis, and the cost of conducting field
experiments often makes the design and evaluation process challenging. In this
work, we leverage MATSim, a simulation platform that provides microscopic
behaviors at the agent level, to evaluate performance on tolling schemes.
Specifically, we conduct a case study of the Manhattan Central Business
District (CBD) in New York City (NYC) using a fine-granularity traffic network
model in the large-scale agent behavior setting. The flexibility of MATSim
enables the implementation of a customized tolling policy proposed yet not
deployed by the NYC agency while providing detailed interpretations. The
quantitative and qualitative results indicate that the tested tolling program
can regulate the personal vehicle volume in the CBD area and encourage the
usage of public transportation, which proves to be a practical move towards
sustainable transportation systems. More importantly, our work demonstrates
that agent-based simulation helps better understand the travel pattern change
subject to tollings in dense and complex urban environments, and it has the
potential to facilitate efficient decision-making for the devotion to
sustainable traffic management.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10851" title="Abstract">arXiv:2402.10851</a> (cross-list from eess.IV) [<a href="/pdf/2402.10851" title="Download PDF">pdf</a>, <a href="/format/2402.10851" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HistoSegCap: Capsules for Weakly-Supervised Semantic Segmentation of  Histological Tissue Type in Whole Slide Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Mansoori%2C+M">Mobina Mansoori</a>, 
<a href="/search/eess?searchtype=author&query=Shahabodini%2C+S">Sajjad Shahabodini</a>, 
<a href="/search/eess?searchtype=author&query=Abouei%2C+J">Jamshid Abouei</a>, 
<a href="/search/eess?searchtype=author&query=Mohammadi%2C+A">Arash Mohammadi</a>, 
<a href="/search/eess?searchtype=author&query=Plataniotis%2C+K+N">Konstantinos N. Plataniotis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Digital pathology involves converting physical tissue slides into
high-resolution Whole Slide Images (WSIs), which pathologists analyze for
disease-affected tissues. However, large histology slides with numerous
microscopic fields pose challenges for visual search. To aid pathologists,
Computer Aided Diagnosis (CAD) systems offer visual assistance in efficiently
examining WSIs and identifying diagnostically relevant regions. This paper
presents a novel histopathological image analysis method employing Weakly
Supervised Semantic Segmentation (WSSS) based on Capsule Networks, the first
such application. The proposed model is evaluated using the Atlas of Digital
Pathology (ADP) dataset and its performance is compared with other
histopathological semantic segmentation methodologies. The findings underscore
the potential of Capsule Networks in enhancing the precision and efficiency of
histopathological image analysis. Experimental results show that the proposed
model outperforms traditional methods in terms of accuracy and the mean
Intersection-over-Union (mIoU) metric.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10874" title="Abstract">arXiv:2402.10874</a> (cross-list from cond-mat.mtrl-sci) [<a href="/pdf/2402.10874" title="Download PDF">pdf</a>, <a href="/format/2402.10874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design of 2D Skyrmionic Metamaterial Through Controlled Assembly
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Xu%2C+Q">Qichen Xu</a>, 
<a href="/search/cond-mat?searchtype=author&query=Shen%2C+Z">Zhuanglin Shen</a>, 
<a href="/search/cond-mat?searchtype=author&query=Edstr%C3%B6m%2C+A">Alexander Edstr&#xf6;m</a>, 
<a href="/search/cond-mat?searchtype=author&query=Miranda%2C+I+P">I. P. Miranda</a>, 
<a href="/search/cond-mat?searchtype=author&query=Lu%2C+Z">Zhiwei Lu</a>, 
<a href="/search/cond-mat?searchtype=author&query=Bergman%2C+A">Anders Bergman</a>, 
<a href="/search/cond-mat?searchtype=author&query=Thonig%2C+D">Danny Thonig</a>, 
<a href="/search/cond-mat?searchtype=author&query=Yin%2C+W">Wanjian Yin</a>, 
<a href="/search/cond-mat?searchtype=author&query=Eriksson%2C+O">Olle Eriksson</a>, 
<a href="/search/cond-mat?searchtype=author&query=Delin%2C+A">Anna Delin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Machine Learning (cs.LG); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">Despite extensive research on magnetic skyrmions and antiskyrmions, a
significant challenge remains in crafting nontrivial high-order skyrmionic
textures with varying, or even tailor-made, topologies. We address this
challenge, by focusing on a construction pathway of skyrmionics metamaterial
within a monolayer thin film and suggest several promising lattice-like,
flakes-like, and cell-like skyrmionic metamaterials that are surprisingly
stable. Central to our approach is the concept of 'simulated controlled
assembly', in short, a protocol inspired by 'click chemistry' that allows for
positioning topological magnetic structures where one likes, and then allowing
for energy minimization to elucidate the stability. Utilizing high-throughput
atomistic-spin-dynamic (ASD) simulations alongside state-of-the-art AI-driven
tools, we have isolated skyrmions (topological charge Q=1), antiskyrmions
(Q=-1), and skyrmionium (Q=0). These entities serve as foundational 'skyrmionic
building blocks' to forming reported intricate textures. In this work, two key
contributions are introduced to the field of skyrmionic systems. First, we
present a novel method for integrating control assembly protocols for the
stabilization and investigation of topological magnets, which marks a
significant advancement in the ability to explore new skyrmionic textures.
Second, we report on the discovery of skyrmionic metamaterials, which shows a
plethora of complex topologies that are possible to investigate theoretically
and experimentally.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10887" title="Abstract">arXiv:2402.10887</a> (cross-list from eess.IV) [<a href="/pdf/2402.10887" title="Download PDF">pdf</a>, <a href="/format/2402.10887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weak-Mamba-UNet: Visual Mamba Makes CNN and ViT Work Better for  Scribble-based Medical Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Ziyang Wang</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+C">Chao Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Medical image segmentation is increasingly reliant on deep learning
techniques, yet the promising performance often come with high annotation
costs. This paper introduces Weak-Mamba-UNet, an innovative weakly-supervised
learning (WSL) framework that leverages the capabilities of Convolutional
Neural Network (CNN), Vision Transformer (ViT), and the cutting-edge Visual
Mamba (VMamba) architecture for medical image segmentation, especially when
dealing with scribble-based annotations. The proposed WSL strategy incorporates
three distinct architecture but same symmetrical encoder-decoder networks: a
CNN-based UNet for detailed local feature extraction, a Swin Transformer-based
SwinUNet for comprehensive global context understanding, and a VMamba-based
Mamba-UNet for efficient long-range dependency modeling. The key concept of
this framework is a collaborative and cross-supervisory mechanism that employs
pseudo labels to facilitate iterative learning and refinement across the
networks. The effectiveness of Weak-Mamba-UNet is validated on a publicly
available MRI cardiac segmentation dataset with processed scribble annotations,
where it surpasses the performance of a similar WSL framework utilizing only
UNet or SwinUNet. This highlights its potential in scenarios with sparse or
imprecise annotations. The source code is made publicly accessible.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10898" title="Abstract">arXiv:2402.10898</a> (cross-list from math.OC) [<a href="/pdf/2402.10898" title="Download PDF">pdf</a>, <a href="/format/2402.10898" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Price of Adaptivity in Stochastic Convex Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Carmon%2C+Y">Yair Carmon</a>, 
<a href="/search/math?searchtype=author&query=Hinder%2C+O">Oliver Hinder</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">We prove impossibility results for adaptivity in non-smooth stochastic convex
optimization. Given a set of problem parameters we wish to adapt to, we define
a "price of adaptivity" (PoA) that, roughly speaking, measures the
multiplicative increase in suboptimality due to uncertainty in these
parameters. When the initial distance to the optimum is unknown but a gradient
norm bound is known, we show that the PoA is at least logarithmic for expected
suboptimality, and double-logarithmic for median suboptimality. When there is
uncertainty in both distance and gradient norm, we show that the PoA must be
polynomial in the level of uncertainty. Our lower bounds nearly match existing
upper bounds, and establish that there is no parameter-free lunch.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Mon, 19 Feb 24</h3>
<dl>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1906.01741" title="Abstract">arXiv:1906.01741</a> (replaced) [<a href="/pdf/1906.01741" title="Download PDF">pdf</a>, <a href="/format/1906.01741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fr&#xe9;chet random forests for metric space valued regression with non  euclidean predictors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Capitaine%2C+L">Louis Capitaine</a>, 
<a href="/search/stat?searchtype=author&query=Bigot%2C+J">J&#xe9;r&#xe9;mie Bigot</a>, 
<a href="/search/stat?searchtype=author&query=Thi%C3%A9baut%2C+R">Rodolphe Thi&#xe9;baut</a>, 
<a href="/search/stat?searchtype=author&query=Genuer%2C+R">Robin Genuer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.09254" title="Abstract">arXiv:2201.09254</a> (replaced) [<a href="/pdf/2201.09254" title="Download PDF">pdf</a>, <a href="/format/2201.09254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How networks shape diversity for better or worse
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Musso%2C+A">Andrea Musso</a>, 
<a href="/search/physics?searchtype=author&query=Helbing%2C+D">Dirk Helbing</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.06095" title="Abstract">arXiv:2202.06095</a> (replaced) [<a href="/pdf/2202.06095" title="Download PDF">pdf</a>, <a href="/format/2202.06095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Review of Deep Learning-based Approaches for Deepfake Content  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Passos%2C+L+A">Leandro A. Passos</a>, 
<a href="/search/cs?searchtype=author&query=Jodas%2C+D">Danilo Jodas</a>, 
<a href="/search/cs?searchtype=author&query=da+Costa%2C+K+A+P">Kelton A. P. da Costa</a>, 
<a href="/search/cs?searchtype=author&query=J%C3%BAnior%2C+L+A+S">Luis A. Souza J&#xfa;nior</a>, 
<a href="/search/cs?searchtype=author&query=Rodrigues%2C+D">Douglas Rodrigues</a>, 
<a href="/search/cs?searchtype=author&query=Del+Ser%2C+J">Javier Del Ser</a>, 
<a href="/search/cs?searchtype=author&query=Camacho%2C+D">David Camacho</a>, 
<a href="/search/cs?searchtype=author&query=Papa%2C+J+P">Jo&#xe3;o Paulo Papa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.05093" title="Abstract">arXiv:2203.05093</a> (replaced) [<a href="/pdf/2203.05093" title="Download PDF">pdf</a>, <a href="/ps/2203.05093" title="Download PostScript">ps</a>, <a href="/format/2203.05093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sampling from the Sherrington-Kirkpatrick Gibbs measure via algorithmic  stochastic localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Alaoui%2C+A+E">Ahmed El Alaoui</a>, 
<a href="/search/math?searchtype=author&query=Montanari%2C+A">Andrea Montanari</a>, 
<a href="/search/math?searchtype=author&query=Sellke%2C+M">Mark Sellke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Probability (math.PR)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.11242" title="Abstract">arXiv:2203.11242</a> (replaced) [<a href="/pdf/2203.11242" title="Download PDF">pdf</a>, <a href="/format/2203.11242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A survey on GANs for computer vision: Recent research, analysis and  taxonomy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iglesias%2C+G">Guillermo Iglesias</a>, 
<a href="/search/cs?searchtype=author&query=Talavera%2C+E">Edgar Talavera</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%ADaz-%C3%81lvarez%2C+A">Alberto D&#xed;az-&#xc1;lvarez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 77 pages, 11 figures, 4 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computer Science Review, 48, 100553 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.13423" title="Abstract">arXiv:2203.13423</a> (replaced) [<a href="/pdf/2203.13423" title="Download PDF">pdf</a>, <a href="/ps/2203.13423" title="Download PostScript">ps</a>, <a href="/format/2203.13423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling Attrition in Recommender Systems with Departing Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ben-Porat%2C+O">Omer Ben-Porat</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+L">Lee Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Leqi%2C+L">Liu Leqi</a>, 
<a href="/search/cs?searchtype=author&query=Lipton%2C+Z+C">Zachary C. Lipton</a>, 
<a href="/search/cs?searchtype=author&query=Mansour%2C+Y">Yishay Mansour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAAI 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.08331" title="Abstract">arXiv:2204.08331</a> (replaced) [<a href="/pdf/2204.08331" title="Download PDF">pdf</a>, <a href="/format/2204.08331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Practical KMP/BM Style Pattern-Matching on Indeterminate Strings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dehghani%2C+H">Hossein Dehghani</a>, 
<a href="/search/cs?searchtype=author&query=Mhaskar%2C+N">Neerja Mhaskar</a>, 
<a href="/search/cs?searchtype=author&query=Smyth%2C+W+F">W. F. Smyth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.09795" title="Abstract">arXiv:2205.09795</a> (replaced) [<a href="/pdf/2205.09795" title="Download PDF">pdf</a>, <a href="/format/2205.09795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SARI: Shared Autonomy across Repeated Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jonnavittula%2C+A">Ananth Jonnavittula</a>, 
<a href="/search/cs?searchtype=author&query=Mehta%2C+S+A">Shaunak A. Mehta</a>, 
<a href="/search/cs?searchtype=author&query=Losey%2C+D+P">Dylan P. Losey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 20 figures. arXiv admin note: substantial text overlap with <a href="/abs/2107.09650">arXiv:2107.09650</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.13055" title="Abstract">arXiv:2205.13055</a> (replaced) [<a href="/pdf/2205.13055" title="Download PDF">pdf</a>, <a href="/format/2205.13055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complexity-optimal and Parameter-free First-order Methods for Finding  Stationary Points of Composite Optimization Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kong%2C+W">Weiwei Kong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Computational Complexity (cs.CC); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.14881" title="Abstract">arXiv:2205.14881</a> (replaced) [<a href="/pdf/2205.14881" title="Download PDF">pdf</a>, <a href="/ps/2205.14881" title="Download PostScript">ps</a>, <a href="/format/2205.14881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Byzantine Fault-Tolerant Min-Max Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Vaidya%2C+N">Nitin Vaidya</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages; Update with necessary citations and fixings in the proofs in Section 6
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.00089" title="Abstract">arXiv:2206.00089</a> (replaced) [<a href="/pdf/2206.00089" title="Download PDF">pdf</a>, <a href="/format/2206.00089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Defining Quantum Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Piispanen%2C+L">Laura Piispanen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Pfaffhauser%2C+M">Marcel Pfaffhauser</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wootton%2C+J+R">James R. Wootton</a>, 
<a href="/search/quant-ph?searchtype=author&query=Togelius%2C+J">Julian Togelius</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kultima%2C+A">Annakaisa Kultima</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages + references, 34 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.00594" title="Abstract">arXiv:2206.00594</a> (replaced) [<a href="/pdf/2206.00594" title="Download PDF">pdf</a>, <a href="/format/2206.00594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse graphs with bounded induced cycle packing number have logarithmic  treewidth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bonamy%2C+M">Marthe Bonamy</a>, 
<a href="/search/math?searchtype=author&query=Bonnet%2C+%C3%89">&#xc9;douard Bonnet</a>, 
<a href="/search/math?searchtype=author&query=D%C3%A9pr%C3%A9s%2C+H">Hugues D&#xe9;pr&#xe9;s</a>, 
<a href="/search/math?searchtype=author&query=Esperet%2C+L">Louis Esperet</a>, 
<a href="/search/math?searchtype=author&query=Geniet%2C+C">Colin Geniet</a>, 
<a href="/search/math?searchtype=author&query=Hilaire%2C+C">Claire Hilaire</a>, 
<a href="/search/math?searchtype=author&query=Thomass%C3%A9%2C+S">St&#xe9;phan Thomass&#xe9;</a>, 
<a href="/search/math?searchtype=author&query=Wesolek%2C+A">Alexandra Wesolek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 6 figures. v5: revised version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.08810" title="Abstract">arXiv:2206.08810</a> (replaced) [<a href="/pdf/2206.08810" title="Download PDF">pdf</a>, <a href="/ps/2206.08810" title="Download PostScript">ps</a>, <a href="/format/2206.08810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interior point methods are not worse than Simplex
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Allamigeon%2C+X">Xavier Allamigeon</a>, 
<a href="/search/math?searchtype=author&query=Dadush%2C+D">Daniel Dadush</a>, 
<a href="/search/math?searchtype=author&query=Loho%2C+G">Georg Loho</a>, 
<a href="/search/math?searchtype=author&query=Natura%2C+B">Bento Natura</a>, 
<a href="/search/math?searchtype=author&query=V%C3%A9gh%2C+L+A">L&#xe1;szl&#xf3; A. V&#xe9;gh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.12532" title="Abstract">arXiv:2206.12532</a> (replaced) [<a href="/pdf/2206.12532" title="Download PDF">pdf</a>, <a href="/format/2206.12532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Scoring: A Framework for Effect Estimation, Effect Ordering, and  Effect Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Fern%C3%A1ndez-Lor%C3%ADa%2C+C">Carlos Fern&#xe1;ndez-Lor&#xed;a</a>, 
<a href="/search/stat?searchtype=author&query=Lor%C3%ADa%2C+J">Jorge Lor&#xed;a</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.13508" title="Abstract">arXiv:2206.13508</a> (replaced) [<a href="/pdf/2206.13508" title="Download PDF">pdf</a>, <a href="/format/2206.13508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Augmentation techniques in time series domain: A survey and  taxonomy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iglesias%2C+G">Guillermo Iglesias</a>, 
<a href="/search/cs?searchtype=author&query=Talavera%2C+E">Edgar Talavera</a>, 
<a href="/search/cs?searchtype=author&query=Gonz%C3%A1lez-Prieto%2C+%C3%81">&#xc1;ngel Gonz&#xe1;lez-Prieto</a>, 
<a href="/search/cs?searchtype=author&query=Mozo%2C+A">Alberto Mozo</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B3mez-Canaval%2C+S">Sandra G&#xf3;mez-Canaval</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 9 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Neural Computing and Applications, 35(14), 10123-10145 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.04771" title="Abstract">arXiv:2207.04771</a> (replaced) [<a href="/pdf/2207.04771" title="Download PDF">pdf</a>, <a href="/format/2207.04771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Functional Generalized Empirical Likelihood Estimation for Conditional  Moment Restrictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kremer%2C+H">Heiner Kremer</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jia-Jie Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Muandet%2C+K">Krikamol Muandet</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.09031" title="Abstract">arXiv:2207.09031</a> (replaced) [<a href="/pdf/2207.09031" title="Download PDF">pdf</a>, <a href="/format/2207.09031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decorrelative Network Architecture for Robust Electrocardiogram  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wiedeman%2C+C">Christopher Wiedeman</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Ge Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.13358" title="Abstract">arXiv:2207.13358</a> (replaced) [<a href="/pdf/2207.13358" title="Download PDF">pdf</a>, <a href="/format/2207.13358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Managing DRAM: A Low-Cost Framework for Enabling Autonomous and  Efficient in-DRAM Operations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hassan%2C+H">Hasan Hassan</a>, 
<a href="/search/cs?searchtype=author&query=Olgun%2C+A">Ataberk Olgun</a>, 
<a href="/search/cs?searchtype=author&query=Yaglikci%2C+A+G">A. Giray Yaglikci</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haocong Luo</a>, 
<a href="/search/cs?searchtype=author&query=Mutlu%2C+O">Onur Mutlu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.05853" title="Abstract">arXiv:2208.05853</a> (replaced) [<a href="/pdf/2208.05853" title="Download PDF">pdf</a>, <a href="/format/2208.05853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MultiMatch: Multi-task Learning for Semi-supervised Domain  Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+L">Lei Qi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Hongpeng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yinghuan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+X">Xin Geng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM TOMM
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.03065" title="Abstract">arXiv:2211.03065</a> (replaced) [<a href="/pdf/2211.03065" title="Download PDF">pdf</a>, <a href="/format/2211.03065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling Deep Learning-based Physical-layer Secret Key Generation for  FDD-OFDM Systems in Multi-Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guyue Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junqing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+L">Linning Peng</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+A">Aiqun Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xianbin Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE TVT
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.03295" title="Abstract">arXiv:2211.03295</a> (replaced) [<a href="/pdf/2211.03295" title="Download PDF">pdf</a>, <a href="/format/2211.03295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MogaNet: Multi-order Gated Aggregation Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zedong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zicheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">Cheng Tan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Haitao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Di Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhiyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jiangbin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S+Z">Stan Z. Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 Camera Ready. Preprint V3 (19 pages + 16 pages). Implementations refer to <a href="https://github.com/Westlake-AI/MogaNet">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.07132" title="Abstract">arXiv:2211.07132</a> (replaced) [<a href="/pdf/2211.07132" title="Download PDF">pdf</a>, <a href="/ps/2211.07132" title="Download PostScript">ps</a>, <a href="/format/2211.07132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The $\ell_p$-Subspace Sketch Problem in Small Dimensions with  Applications to Support Vector Machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yi Li</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+H">Honghao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Woodruff%2C+D+P">David P. Woodruff</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Corrected the citation for Lemma 3.3 and adjusted the constants in the proof accordingly
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.09593" title="Abstract">arXiv:2211.09593</a> (replaced) [<a href="/pdf/2211.09593" title="Download PDF">pdf</a>, <a href="/format/2211.09593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NorMatch: Matching Normalizing Flows with Discriminative Classifiers for  Semi-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+Z">Zhongying Deng</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+R">Rihuan Ke</a>, 
<a href="/search/cs?searchtype=author&query=Schonlieb%2C+C">Carola-Bibiane Schonlieb</a>, 
<a href="/search/cs?searchtype=author&query=Aviles-Rivero%2C+A+I">Angelica I Aviles-Rivero</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Transactions on Machine Learning Research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.11278" title="Abstract">arXiv:2211.11278</a> (replaced) [<a href="/pdf/2211.11278" title="Download PDF">pdf</a>, <a href="/ps/2211.11278" title="Download PostScript">ps</a>, <a href="/format/2211.11278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Extended Neighbourhood Rule $k$ Nearest Neighbours Ensemble
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ali%2C+A">Amjad Ali</a>, 
<a href="/search/stat?searchtype=author&query=Khan%2C+Z">Zardad Khan</a>, 
<a href="/search/stat?searchtype=author&query=Khan%2C+D+M">Dost Muhammad Khan</a>, 
<a href="/search/stat?searchtype=author&query=Aldahmani%2C+S">Saeed Aldahmani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This manuscript has been submitted for publication in the esteemed journal Pattern Recognition Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.00322" title="Abstract">arXiv:2212.00322</a> (replaced) [<a href="/pdf/2212.00322" title="Download PDF">pdf</a>, <a href="/format/2212.00322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hijack Vertical Federated Learning Models As One Party
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+P">Pengyu Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuhong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+S">Shouling Ji</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Changjiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+Y">Yuwen Pu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xing Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Ting Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://doi.ieeecomputersociety.org/10.1109/TDSC.2024.3358081">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.03733" title="Abstract">arXiv:2212.03733</a> (replaced) [<a href="/pdf/2212.03733" title="Download PDF">pdf</a>, <a href="/format/2212.03733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tiered Reward Functions: Specifying and Fast Learning of Desired  Behavior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhiyuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Raman%2C+S+S">Shreyas Sundara Raman</a>, 
<a href="/search/cs?searchtype=author&query=Sowerby%2C+H">Henry Sowerby</a>, 
<a href="/search/cs?searchtype=author&query=Littman%2C+M+L">Michael L. Littman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> For code, see <a href="https://github.com/zhouzypaul/tiered-reward">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.14424" title="Abstract">arXiv:2212.14424</a> (replaced) [<a href="/pdf/2212.14424" title="Download PDF">pdf</a>, <a href="/format/2212.14424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Normalizing flow neural networks by JKO scheme
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Xu%2C+C">Chen Xu</a>, 
<a href="/search/stat?searchtype=author&query=Cheng%2C+X">Xiuyuan Cheng</a>, 
<a href="/search/stat?searchtype=author&query=Xie%2C+Y">Yao Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.03701" title="Abstract">arXiv:2301.03701</a> (replaced) [<a href="/pdf/2301.03701" title="Download PDF">pdf</a>, <a href="/format/2301.03701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Artificial Intelligence Model for Tumoral Clinical Decision Support  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Iglesias%2C+G">Guillermo Iglesias</a>, 
<a href="/search/eess?searchtype=author&query=Talavera%2C+E">Edgar Talavera</a>, 
<a href="/search/eess?searchtype=author&query=Garc%C3%ACa%2C+J+T">Jes&#xfa;s Troya Garc&#xec;a</a>, 
<a href="/search/eess?searchtype=author&query=D%C3%ADaz-%C3%81lvarez%2C+A">Alberto D&#xed;az-&#xc1;lvarez</a>, 
<a href="/search/eess?searchtype=author&query=Grac%C3%ADa-Remesal%2C+M">Miguel Grac&#xed;a-Remesal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 7 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.04195" title="Abstract">arXiv:2301.04195</a> (replaced) [<a href="/pdf/2301.04195" title="Download PDF">pdf</a>, <a href="/format/2301.04195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Orbit: A Unified Simulation Framework for Interactive Robot Learning  Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mittal%2C+M">Mayank Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Calvin Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qinxi Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jingzhou Liu</a>, 
<a href="/search/cs?searchtype=author&query=Rudin%2C+N">Nikita Rudin</a>, 
<a href="/search/cs?searchtype=author&query=Hoeller%2C+D">David Hoeller</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J+L">Jia Lin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+R">Ritvik Singh</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yunrong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Mazhar%2C+H">Hammad Mazhar</a>, 
<a href="/search/cs?searchtype=author&query=Mandlekar%2C+A">Ajay Mandlekar</a>, 
<a href="/search/cs?searchtype=author&query=Babich%2C+B">Buck Babich</a>, 
<a href="/search/cs?searchtype=author&query=State%2C+G">Gavriel State</a>, 
<a href="/search/cs?searchtype=author&query=Hutter%2C+M">Marco Hutter</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+A">Animesh Garg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project website: <a href="https://isaac-orbit.github.io/">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Robotics and Automation Letters (Volume: 8, Issue: 6, June
  2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.05352" title="Abstract">arXiv:2301.05352</a> (replaced) [<a href="/pdf/2301.05352" title="Download PDF">pdf</a>, <a href="/format/2301.05352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Concentration in Gossip Opinion Dynamics over Random Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xing%2C+Y">Yu Xing</a>, 
<a href="/search/eess?searchtype=author&query=Johansson%2C+K+H">Karl Henrik Johansson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11118" title="Abstract">arXiv:2301.11118</a> (replaced) [<a href="/pdf/2301.11118" title="Download PDF">pdf</a>, <a href="/format/2301.11118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dual Box Embeddings for the Description Logic EL++
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jackermeier%2C+M">Mathias Jackermeier</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiaoyan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Horrocks%2C+I">Ian Horrocks</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated version accepted to WWW '24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Logic in Computer Science (cs.LO)

</div>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00488" title="Abstract">arXiv:2302.00488</a> (replaced) [<a href="/pdf/2302.00488" title="Download PDF">pdf</a>, <a href="/ps/2302.00488" title="Download PostScript">ps</a>, <a href="/format/2302.00488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extending the Known Region of Nonlocal Boxes that Collapse Communication  Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Botteron%2C+P">Pierre Botteron</a>, 
<a href="/search/quant-ph?searchtype=author&query=Broadbent%2C+A">Anne Broadbent</a>, 
<a href="/search/quant-ph?searchtype=author&query=Proulx%2C+M">Marc-Olivier Proulx</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Phys. Rev. Lett. 132, 070201 (Feb. 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT); Mathematical Physics (math-ph)

</div>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03038" title="Abstract">arXiv:2302.03038</a> (replaced) [<a href="/pdf/2302.03038" title="Download PDF">pdf</a>, <a href="/format/2302.03038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Single Cells Are Spatial Tokens: Transformers for Spatial Transcriptomic  Data Imputation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Wen%2C+H">Hongzhi Wen</a>, 
<a href="/search/q-bio?searchtype=author&query=Tang%2C+W">Wenzhuo Tang</a>, 
<a href="/search/q-bio?searchtype=author&query=Jin%2C+W">Wei Jin</a>, 
<a href="/search/q-bio?searchtype=author&query=Ding%2C+J">Jiayuan Ding</a>, 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+R">Renming Liu</a>, 
<a href="/search/q-bio?searchtype=author&query=Dai%2C+X">Xinnan Dai</a>, 
<a href="/search/q-bio?searchtype=author&query=Shi%2C+F">Feng Shi</a>, 
<a href="/search/q-bio?searchtype=author&query=Shang%2C+L">Lulu Shang</a>, 
<a href="/search/q-bio?searchtype=author&query=Liu%2C+H">Hui Liu</a>, 
<a href="/search/q-bio?searchtype=author&query=Xie%2C+Y">Yuying Xie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.03651" title="Abstract">arXiv:2302.03651</a> (replaced) [<a href="/pdf/2302.03651" title="Download PDF">pdf</a>, <a href="/format/2302.03651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quality Engineering for Agile and DevOps on the Cloud and Edge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Farchi%2C+E">Eitan Farchi</a>, 
<a href="/search/cs?searchtype=author&query=Route%2C+S">Saritha Route</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07930" title="Abstract">arXiv:2302.07930</a> (replaced) [<a href="/pdf/2302.07930" title="Download PDF">pdf</a>, <a href="/format/2302.07930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Deep Learning Methods for Multiview Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hengkang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Han Lu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+J">Ju Sun</a>, 
<a href="/search/cs?searchtype=author&query=Safo%2C+S+E">Sandra E Safo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in BMC Bioinformatics (<a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-024-05679-9">this https URL</a>)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> BMC Bioinformatics 25, 69 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.03723" title="Abstract">arXiv:2303.03723</a> (replaced) [<a href="/pdf/2303.03723" title="Download PDF">pdf</a>, <a href="/format/2303.03723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chance-Aware Lane Change with High-Level Model Predictive Control  Through Curriculum Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yubin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yulin Li</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Z">Zengqi Peng</a>, 
<a href="/search/cs?searchtype=author&query=Ghazzai%2C+H">Hakim Ghazzai</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jun Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper has been accepted by ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.07154" title="Abstract">arXiv:2303.07154</a> (replaced) [<a href="/pdf/2303.07154" title="Download PDF">pdf</a>, <a href="/format/2303.07154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Differential Good Arm Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tsai%2C+Y">Yun-Da Tsai</a>, 
<a href="/search/cs?searchtype=author&query=Tsai%2C+T">Tzu-Hsien Tsai</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+S">Shou-De Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08431" title="Abstract">arXiv:2303.08431</a> (replaced) [<a href="/pdf/2303.08431" title="Download PDF">pdf</a>, <a href="/format/2303.08431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Policy Gradient Converges to the Globally Optimal Policy for Nearly  Linear-Quadratic Regulators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+Y">Yinbin Han</a>, 
<a href="/search/cs?searchtype=author&query=Razaviyayn%2C+M">Meisam Razaviyayn</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Renyuan Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.12023" title="Abstract">arXiv:2303.12023</a> (replaced) [<a href="/pdf/2303.12023" title="Download PDF">pdf</a>, <a href="/format/2303.12023" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Logical Reasoning over Natural Language as Knowledge Representation: A  Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zonglin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+X">Xinya Du</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+R">Rui Mao</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+J">Jinjie Ni</a>, 
<a href="/search/cs?searchtype=author&query=Cambria%2C+E">Erik Cambria</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15103" title="Abstract">arXiv:2303.15103</a> (replaced) [<a href="/pdf/2303.15103" title="Download PDF">pdf</a>, <a href="/format/2303.15103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Learning Is Spectral Clustering On Similarity Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhiquan Tan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jingqin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yang Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024; We express our gratitude to the anonymous reviewers for their valuable feedback
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00216" title="Abstract">arXiv:2304.00216</a> (replaced) [<a href="/pdf/2304.00216" title="Download PDF">pdf</a>, <a href="/format/2304.00216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-scale Multi-instance Learning for Pathological Image Diagnosis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Deng%2C+R">Ruining Deng</a>, 
<a href="/search/eess?searchtype=author&query=Cui%2C+C">Can Cui</a>, 
<a href="/search/eess?searchtype=author&query=Remedios%2C+L+W">Lucas W. Remedios</a>, 
<a href="/search/eess?searchtype=author&query=Bao%2C+S">Shunxing Bao</a>, 
<a href="/search/eess?searchtype=author&query=Womick%2C+R+M">R. Michael Womick</a>, 
<a href="/search/eess?searchtype=author&query=Chiron%2C+S">Sophie Chiron</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+J">Jia Li</a>, 
<a href="/search/eess?searchtype=author&query=Roland%2C+J+T">Joseph T. Roland</a>, 
<a href="/search/eess?searchtype=author&query=Lau%2C+K+S">Ken S. Lau</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Q">Qi Liu</a>, 
<a href="/search/eess?searchtype=author&query=Wilson%2C+K+T">Keith T. Wilson</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yaohong Wang</a>, 
<a href="/search/eess?searchtype=author&query=Coburn%2C+L+A">Lori A. Coburn</a>, 
<a href="/search/eess?searchtype=author&query=Landman%2C+B+A">Bennett A. Landman</a>, 
<a href="/search/eess?searchtype=author&query=Huo%2C+Y">Yuankai Huo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00685" title="Abstract">arXiv:2304.00685</a> (replaced) [<a href="/pdf/2304.00685" title="Download PDF">pdf</a>, <a href="/format/2304.00685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision-Language Models for Vision Tasks: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jiaxing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Sheng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shijian Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.01722" title="Abstract">arXiv:2304.01722</a> (replaced) [<a href="/pdf/2304.01722" title="Download PDF">pdf</a>, <a href="/format/2304.01722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning quantities of interest from parametric PDEs: An efficient  neural-weighted Minimal Residual approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Brevis%2C+I">Ignacio Brevis</a>, 
<a href="/search/math?searchtype=author&query=Muga%2C+I">Ignacio Muga</a>, 
<a href="/search/math?searchtype=author&query=Pardo%2C+D">David Pardo</a>, 
<a href="/search/math?searchtype=author&query=Rodriguez%2C+O">Oscar Rodriguez</a>, 
<a href="/search/math?searchtype=author&query=van+der+Zee%2C+K+G">Kristoffer G. van der Zee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.05099" title="Abstract">arXiv:2304.05099</a> (replaced) [<a href="/pdf/2304.05099" title="Download PDF">pdf</a>, <a href="/format/2304.05099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feudal Graph Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marzi%2C+T">Tommaso Marzi</a>, 
<a href="/search/cs?searchtype=author&query=Khehra%2C+A">Arshjot Khehra</a>, 
<a href="/search/cs?searchtype=author&query=Cini%2C+A">Andrea Cini</a>, 
<a href="/search/cs?searchtype=author&query=Alippi%2C+C">Cesare Alippi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.07724" title="Abstract">arXiv:2304.07724</a> (replaced) [<a href="/pdf/2304.07724" title="Download PDF">pdf</a>, <a href="/format/2304.07724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MS-LSTM: Exploring Spatiotemporal Multiscale Representations in Video  Prediction Domain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhifeng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jie Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2206.03010">arXiv:2206.03010</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10640" title="Abstract">arXiv:2304.10640</a> (replaced) [<a href="/pdf/2304.10640" title="Download PDF">pdf</a>, <a href="/format/2304.10640" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Effects of Data Heterogeneity on the Convergence Rates of  Distributed Linear System Solvers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Velasevic%2C+B">Boris Velasevic</a>, 
<a href="/search/cs?searchtype=author&query=Parasnis%2C+R">Rohit Parasnis</a>, 
<a href="/search/cs?searchtype=author&query=Brinton%2C+C+G">Christopher G. Brinton</a>, 
<a href="/search/cs?searchtype=author&query=Azizan%2C+N">Navid Azizan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10813" title="Abstract">arXiv:2304.10813</a> (replaced) [<a href="/pdf/2304.10813" title="Download PDF">pdf</a>, <a href="/format/2304.10813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tokenization Preference for Human and Machine Learning Model: An  Annotation Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hiraoka%2C+T">Tatsuya Hiraoka</a>, 
<a href="/search/cs?searchtype=author&query=Iwakura%2C+T">Tomoya Iwakura</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.13806" title="Abstract">arXiv:2304.13806</a> (replaced) [<a href="/pdf/2304.13806" title="Download PDF">pdf</a>, <a href="/format/2304.13806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Hellan-Herrmann-Johnson and TDNNS method for linear and nonlinear  shells
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Neunteufel%2C+M">Michael Neunteufel</a>, 
<a href="/search/math?searchtype=author&query=Sch%C3%B6berl%2C+J">Joachim Sch&#xf6;berl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.00875" title="Abstract">arXiv:2305.00875</a> (replaced) [<a href="/pdf/2305.00875" title="Download PDF">pdf</a>, <a href="/format/2305.00875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Redundancy and Concept Analysis for Code-trained Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Arushi Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zefu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Quinn%2C+C">Christopher Quinn</a>, 
<a href="/search/cs?searchtype=author&query=Jannesari%2C+A">Ali Jannesari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.01713" title="Abstract">arXiv:2305.01713</a> (replaced) [<a href="/pdf/2305.01713" title="Download PDF">pdf</a>, <a href="/format/2305.01713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Disentangled Semantic Spaces of Explanations via Invertible  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yingji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Carvalho%2C+D+S">Danilo S. Carvalho</a>, 
<a href="/search/cs?searchtype=author&query=Freitas%2C+A">Andr&#xe9; Freitas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03515" title="Abstract">arXiv:2305.03515</a> (replaced) [<a href="/pdf/2305.03515" title="Download PDF">pdf</a>, <a href="/format/2305.03515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GradTree: Learning Axis-Aligned Decision Trees with Gradient Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marton%2C+S">Sascha Marton</a>, 
<a href="/search/cs?searchtype=author&query=L%C3%BCdtke%2C+S">Stefan L&#xfc;dtke</a>, 
<a href="/search/cs?searchtype=author&query=Bartelt%2C+C">Christian Bartelt</a>, 
<a href="/search/cs?searchtype=author&query=Stuckenschmidt%2C+H">Heiner Stuckenschmidt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07303" title="Abstract">arXiv:2305.07303</a> (replaced) [<a href="/pdf/2305.07303" title="Download PDF">pdf</a>, <a href="/format/2305.07303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Relational Hyperbolic Word Embeddings from Natural Language  Definitions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Valentino%2C+M">Marco Valentino</a>, 
<a href="/search/cs?searchtype=author&query=Carvalho%2C+D+S">Danilo S. Carvalho</a>, 
<a href="/search/cs?searchtype=author&query=Freitas%2C+A">Andr&#xe9; Freitas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 18th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2024), camera-ready
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.07358" title="Abstract">arXiv:2305.07358</a> (replaced) [<a href="/pdf/2305.07358" title="Download PDF">pdf</a>, <a href="/format/2305.07358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Versatile and Efficient Visual Knowledge Integration into  Pre-trained Language Models with Cross-Modal Adapters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+H">Haochen Tan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Han Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bei Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10544" title="Abstract">arXiv:2305.10544</a> (replaced) [<a href="/pdf/2305.10544" title="Download PDF">pdf</a>, <a href="/format/2305.10544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tractable Probabilistic Graph Representation Learning with Graph-Induced  Sum-Product Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Errica%2C+F">Federico Errica</a>, 
<a href="/search/cs?searchtype=author&query=Niepert%2C+M">Mathias Niepert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 12th International Conference on Learning Representations (ICLR 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11738" title="Abstract">arXiv:2305.11738</a> (replaced) [<a href="/pdf/2305.11738" title="Download PDF">pdf</a>, <a href="/format/2305.11738" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CRITIC: Large Language Models Can Self-Correct with Tool-Interactive  Critiquing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gou%2C+Z">Zhibin Gou</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Z">Zhihong Shao</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yeyun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yelong Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yujiu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+N">Nan Duan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weizhu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12095" title="Abstract">arXiv:2305.12095</a> (replaced) [<a href="/pdf/2305.12095" title="Download PDF">pdf</a>, <a href="/format/2305.12095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CARD: Channel Aligned Robust Blend Transformer for Time Series  Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xue%2C+W">Wang Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tian Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Q">Qingsong Wen</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jinyang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+B">Bolin Ding</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+R">Rong Jin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13330" title="Abstract">arXiv:2305.13330</a> (replaced) [<a href="/pdf/2305.13330" title="Download PDF">pdf</a>, <a href="/format/2305.13330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised ASR via Cross-Lingual Pseudo-Labeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Likhomanenko%2C+T">Tatiana Likhomanenko</a>, 
<a href="/search/eess?searchtype=author&query=Lugosch%2C+L">Loren Lugosch</a>, 
<a href="/search/eess?searchtype=author&query=Collobert%2C+R">Ronan Collobert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13503" title="Abstract">arXiv:2305.13503</a> (replaced) [<a href="/pdf/2305.13503" title="Download PDF">pdf</a>, <a href="/format/2305.13503" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asynchronous Multi-Model Dynamic Federated Learning over Wireless  Networks: Theory, Modeling, and Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+Z">Zhan-Lun Chang</a>, 
<a href="/search/cs?searchtype=author&query=Hosseinalipour%2C+S">Seyyedali Hosseinalipour</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+M">Mung Chiang</a>, 
<a href="/search/cs?searchtype=author&query=Brinton%2C+C+G">Christopher G. Brinton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Completed the major revision for IEEE Transactions on Cognitive Communications and Networking
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13619" title="Abstract">arXiv:2305.13619</a> (replaced) [<a href="/pdf/2305.13619" title="Download PDF">pdf</a>, <a href="/format/2305.13619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Memory Asymmetry Creates Heteroclinic Orbits to Nash Equilibrium in  Learning in Zero-Sum Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fujimoto%2C+Y">Yuma Fujimoto</a>, 
<a href="/search/cs?searchtype=author&query=Ariu%2C+K">Kaito Ariu</a>, 
<a href="/search/cs?searchtype=author&query=Abe%2C+K">Kenshi Abe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages &amp; 5 figures (main), 5 pages &amp; 2 figures (appendix)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Multiagent Systems (cs.MA); Optimization and Control (math.OC); Chaotic Dynamics (nlin.CD)

</div>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14448" title="Abstract">arXiv:2305.14448</a> (replaced) [<a href="/pdf/2305.14448" title="Download PDF">pdf</a>, <a href="/format/2305.14448" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust non-computability and stability of dynamical systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gra%C3%A7a%2C+D+S">Daniel S. Gra&#xe7;a</a>, 
<a href="/search/math?searchtype=author&query=Zhong%2C+N">Ning Zhong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2109.15080">arXiv:2109.15080</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic (math.LO)</span>; Logic in Computer Science (cs.LO); Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15949" title="Abstract">arXiv:2305.15949</a> (replaced) [<a href="/pdf/2305.15949" title="Download PDF">pdf</a>, <a href="/format/2305.15949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complexity analysis of quasi continuous level Monte Carlo
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Beschle%2C+C+A">Cedric Aaron Beschle</a>, 
<a href="/search/math?searchtype=author&query=Barth%2C+A">Andrea Barth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.17342" title="Abstract">arXiv:2305.17342</a> (replaced) [<a href="/pdf/2305.17342" title="Download PDF">pdf</a>, <a href="/format/2305.17342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Adversarial Policies: A Generalized Attack Formulation and  Provable Defense in RL
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiangyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Souradip Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yanchao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Furong Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Learning Representations (ICLR) 2024, spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00262" title="Abstract">arXiv:2306.00262</a> (replaced) [<a href="/pdf/2306.00262" title="Download PDF">pdf</a>, <a href="/format/2306.00262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maximal Domain Independent Representations Improve Transfer Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+A+S">Adrian Shuai Li</a>, 
<a href="/search/cs?searchtype=author&query=Bertino%2C+E">Elisa Bertino</a>, 
<a href="/search/cs?searchtype=author&query=Dang%2C+X">Xuan-Hong Dang</a>, 
<a href="/search/cs?searchtype=author&query=Singla%2C+A">Ankush Singla</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Y">Yuhai Tu</a>, 
<a href="/search/cs?searchtype=author&query=Wegman%2C+M+N">Mark N Wegman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01794" title="Abstract">arXiv:2306.01794</a> (replaced) [<a href="/pdf/2306.01794" title="Download PDF">pdf</a>, <a href="/format/2306.01794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffPack: A Torsional Diffusion Model for Autoregressive Protein  Side-Chain Packing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+Y">Yangtian Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+Z">Zuobai Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhong%2C+B">Bozitao Zhong</a>, 
<a href="/search/q-bio?searchtype=author&query=Misra%2C+S">Sanchit Misra</a>, 
<a href="/search/q-bio?searchtype=author&query=Tang%2C+J">Jian Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.03364" title="Abstract">arXiv:2306.03364</a> (replaced) [<a href="/pdf/2306.03364" title="Download PDF">pdf</a>, <a href="/format/2306.03364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Representations on the Unit Sphere: Investigating Angular  Gaussian and von Mises-Fisher Distributions for Online Continual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Michel%2C+N">Nicolas Michel</a>, 
<a href="/search/cs?searchtype=author&query=Chierchia%2C+G">Giovanni Chierchia</a>, 
<a href="/search/cs?searchtype=author&query=Negrel%2C+R">Romain Negrel</a>, 
<a href="/search/cs?searchtype=author&query=Bercher%2C+J">Jean-Fran&#xe7;ois Bercher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Fix some typo. Accepted to AAAI24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12893" title="Abstract">arXiv:2306.12893</a> (replaced) [<a href="/pdf/2306.12893" title="Download PDF">pdf</a>, <a href="/format/2306.12893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FlowBot++: Learning Generalized Articulated Objects Manipulation via  Articulation Projection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Harry Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Eisner%2C+B">Ben Eisner</a>, 
<a href="/search/cs?searchtype=author&query=Held%2C+D">David Held</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2205.04382">arXiv:2205.04382</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.15585" title="Abstract">arXiv:2306.15585</a> (replaced) [<a href="/pdf/2306.15585" title="Download PDF">pdf</a>, <a href="/ps/2306.15585" title="Download PostScript">ps</a>, <a href="/format/2306.15585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Credit Limit Adjustments Under Adversarial Goals Using  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Alfonso-S%C3%A1nchez%2C+S">Sherly Alfonso-S&#xe1;nchez</a>, 
<a href="/search/q-fin?searchtype=author&query=Solano%2C+J">Jes&#xfa;s Solano</a>, 
<a href="/search/q-fin?searchtype=author&query=Correa-Bahnsen%2C+A">Alejandro Correa-Bahnsen</a>, 
<a href="/search/q-fin?searchtype=author&query=Sendova%2C+K+P">Kristina P. Sendova</a>, 
<a href="/search/q-fin?searchtype=author&query=Bravo%2C+C">Cristi&#xe1;n Bravo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 16 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Alfonso-Sanchez, S., Solano, J., Correa-Bahnsen, A., Sendova, K.
  P., &amp; Bravo, C. (2024). Optimizing credit limit adjustments under adversarial
  goals using reinforcement learning. European Journal of Operational Research
  315(2): 802-817
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Finance (q-fin.GN)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16588" title="Abstract">arXiv:2306.16588</a> (replaced) [<a href="/pdf/2306.16588" title="Download PDF">pdf</a>, <a href="/format/2306.16588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Losing Control of your Network? Try Resilience Theory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bouvier%2C+J">Jean-Baptiste Bouvier</a>, 
<a href="/search/eess?searchtype=author&query=Nandanoori%2C+S+P">Sai Pushpak Nandanoori</a>, 
<a href="/search/eess?searchtype=author&query=Ornik%2C+M">Melkior Ornik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16881" title="Abstract">arXiv:2306.16881</a> (replaced) [<a href="/pdf/2306.16881" title="Download PDF">pdf</a>, <a href="/ps/2306.16881" title="Download PostScript">ps</a>, <a href="/format/2306.16881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Complexity results for modal logic with recursion via translations and  tableaux
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aceto%2C+L">Luca Aceto</a>, 
<a href="/search/cs?searchtype=author&query=Achilleos%2C+A">Antonis Achilleos</a>, 
<a href="/search/cs?searchtype=author&query=Anastasiadi%2C+E">Elli Anastasiadi</a>, 
<a href="/search/cs?searchtype=author&query=Francalanza%2C+A">Adrian Francalanza</a>, 
<a href="/search/cs?searchtype=author&query=Ing%C3%B3lfsd%C3%B3ttir%2C+A">Anna Ing&#xf3;lfsd&#xf3;ttir</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 58 pages. arXiv admin note: substantial text overlap with <a href="/abs/2209.10377">arXiv:2209.10377</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.17820" title="Abstract">arXiv:2306.17820</a> (replaced) [<a href="/pdf/2306.17820" title="Download PDF">pdf</a>, <a href="/format/2306.17820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta-Reasoning: Semantics-Symbol Deconstruction for Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuosheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Pei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+B">Baosong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02973" title="Abstract">arXiv:2307.02973</a> (replaced) [<a href="/pdf/2307.02973" title="Download PDF">pdf</a>, <a href="/format/2307.02973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pruning vs Quantization: Which is Better?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuzmin%2C+A">Andrey Kuzmin</a>, 
<a href="/search/cs?searchtype=author&query=Nagel%2C+M">Markus Nagel</a>, 
<a href="/search/cs?searchtype=author&query=van+Baalen%2C+M">Mart van Baalen</a>, 
<a href="/search/cs?searchtype=author&query=Behboodi%2C+A">Arash Behboodi</a>, 
<a href="/search/cs?searchtype=author&query=Blankevoort%2C+T">Tijmen Blankevoort</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07297" title="Abstract">arXiv:2307.07297</a> (replaced) [<a href="/pdf/2307.07297" title="Download PDF">pdf</a>, <a href="/format/2307.07297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Game Dynamics and Equilibrium Computation in the Population Protocol  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alistarh%2C+D">Dan Alistarh</a>, 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+K">Krishnendu Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Karrabi%2C+M">Mehrdad Karrabi</a>, 
<a href="/search/cs?searchtype=author&query=Lazarsfeld%2C+J">John Lazarsfeld</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07881" title="Abstract">arXiv:2307.07881</a> (replaced) [<a href="/pdf/2307.07881" title="Download PDF">pdf</a>, <a href="/ps/2307.07881" title="Download PostScript">ps</a>, <a href="/format/2307.07881" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Embedded Intuitionistic Fuzzy Random Vector Functional Link Neural  Network for Class Imbalance Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ganaie%2C+M+A">M.A. Ganaie</a>, 
<a href="/search/cs?searchtype=author&query=Sajid%2C+M">M. Sajid</a>, 
<a href="/search/cs?searchtype=author&query=Malik%2C+A+K">A.K. Malik</a>, 
<a href="/search/cs?searchtype=author&query=Tanveer%2C+M">M. Tanveer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE Transactions on Neural Networks and Learning Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08555" title="Abstract">arXiv:2307.08555</a> (replaced) [<a href="/pdf/2307.08555" title="Download PDF">pdf</a>, <a href="/format/2307.08555" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Splitting-off in Hypergraphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=B%C3%A9rczi%2C+K">Krist&#xf3;f B&#xe9;rczi</a>, 
<a href="/search/cs?searchtype=author&query=Chandrasekaran%2C+K">Karthekeyan Chandrasekaran</a>, 
<a href="/search/cs?searchtype=author&query=Kir%C3%A1ly%2C+T">Tam&#xe1;s Kir&#xe1;ly</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+S">Shubhang Kulkarni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10700" title="Abstract">arXiv:2307.10700</a> (replaced) [<a href="/pdf/2307.10700" title="Download PDF">pdf</a>, <a href="/format/2307.10700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topics, Authors, and Institutions in Large Language Model Research:  Trends from 17K arXiv Papers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Movva%2C+R">Rajiv Movva</a>, 
<a href="/search/cs?searchtype=author&query=Balachandar%2C+S">Sidhika Balachandar</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+K">Kenny Peng</a>, 
<a href="/search/cs?searchtype=author&query=Agostini%2C+G">Gabriel Agostini</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+N">Nikhil Garg</a>, 
<a href="/search/cs?searchtype=author&query=Pierson%2C+E">Emma Pierson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Data &amp; code available at <a href="https://github.com/rmovva/LLM-publication-patterns-public">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Computation and Language (cs.CL); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10928" title="Abstract">arXiv:2307.10928</a> (replaced) [<a href="/pdf/2307.10928" title="Download PDF">pdf</a>, <a href="/format/2307.10928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLASK: Fine-grained Language Model Evaluation based on Alignment Skill  Sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+S">Seonghyeon Ye</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Doyoung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungdong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+H">Hyeonbin Hwang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seungone Kim</a>, 
<a href="/search/cs?searchtype=author&query=Jo%2C+Y">Yongrae Jo</a>, 
<a href="/search/cs?searchtype=author&query=Thorne%2C+J">James Thorne</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Juho Kim</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+M">Minjoon Seo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 Spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11655" title="Abstract">arXiv:2307.11655</a> (replaced) [<a href="/pdf/2307.11655" title="Download PDF">pdf</a>, <a href="/format/2307.11655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preferences Evolve And So Should Your Bandits: Bandits with Evolving  States for Online Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khosravi%2C+K">Khashayar Khosravi</a>, 
<a href="/search/cs?searchtype=author&query=Leme%2C+R+P">Renato Paes Leme</a>, 
<a href="/search/cs?searchtype=author&query=Podimata%2C+C">Chara Podimata</a>, 
<a href="/search/cs?searchtype=author&query=Tsorvantzis%2C+A">Apostolis Tsorvantzis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12776" title="Abstract">arXiv:2307.12776</a> (replaced) [<a href="/pdf/2307.12776" title="Download PDF">pdf</a>, <a href="/ps/2307.12776" title="Download PostScript">ps</a>, <a href="/format/2307.12776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing Large Language Models&#x27; ability to predict how humans balance  self-interest and the interest of others
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Capraro%2C+V">Valerio Capraro</a>, 
<a href="/search/econ?searchtype=author&query=Di+Paolo%2C+R">Roberto Di Paolo</a>, 
<a href="/search/econ?searchtype=author&query=Pizziol%2C+V">Veronica Pizziol</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Economics (econ.GN)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12916" title="Abstract">arXiv:2307.12916</a> (replaced) [<a href="/pdf/2307.12916" title="Download PDF">pdf</a>, <a href="/ps/2307.12916" title="Download PostScript">ps</a>, <a href="/format/2307.12916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Approximation Guarantees for Maximin Share
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akrami%2C+H">Hannaneh Akrami</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+J">Jugal Garg</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+E">Eklavya Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Taki%2C+S">Setareh Taki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.14754" title="Abstract">arXiv:2307.14754</a> (replaced) [<a href="/pdf/2307.14754" title="Download PDF">pdf</a>, <a href="/format/2307.14754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair Machine Unlearning: Data Removal while Mitigating Disparities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oesterling%2C+A">Alex Oesterling</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jiaqi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Calmon%2C+F+P">Flavio P. Calmon</a>, 
<a href="/search/cs?searchtype=author&query=Lakkaraju%2C+H">Hima Lakkaraju</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 3 figures, accepted to AISTATS 2024. Code is provided at <a href="https://github.com/AI4LIFE-GROUP/fair-unlearning">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15007" title="Abstract">arXiv:2307.15007</a> (replaced) [<a href="/pdf/2307.15007" title="Download PDF">pdf</a>, <a href="/format/2307.15007" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discriminative Feature Attributions: Bridging Post Hoc Explainability  and Inherent Interpretability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhalla%2C+U">Usha Bhalla</a>, 
<a href="/search/cs?searchtype=author&query=Srinivas%2C+S">Suraj Srinivas</a>, 
<a href="/search/cs?searchtype=author&query=Lakkaraju%2C+H">Himabindu Lakkaraju</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> NeurIPS 2023 (Thirty-seventh Conference on Neural Information
  Processing Systems)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05381" title="Abstract">arXiv:2308.05381</a> (replaced) [<a href="/pdf/2308.05381" title="Download PDF">pdf</a>, <a href="/format/2308.05381" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Exploratory Study of V-Model in Building ML-Enabled Software: A  Systems Engineering Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J+J">Jie JW Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 2 figures, 2 tables. Accepted at CAIN 2024 (3rd International Conference on AI Engineering - Software Engineering for AI)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2024 IEEE/ACM 3rd International Conference on AI Engineering -
  Software Engineering for AI (CAIN)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07876" title="Abstract">arXiv:2308.07876</a> (replaced) [<a href="/pdf/2308.07876" title="Download PDF">pdf</a>, <a href="/format/2308.07876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthesizing Political Zero-Shot Relation Classification via Codebook  Knowledge, NLI, and ChatGPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yibo Hu</a>, 
<a href="/search/cs?searchtype=author&query=Parolin%2C+E+S">Erick Skorupa Parolin</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+L">Latifur Khan</a>, 
<a href="/search/cs?searchtype=author&query=Brandt%2C+P+T">Patrick T. Brandt</a>, 
<a href="/search/cs?searchtype=author&query=Osorio%2C+J">Javier Osorio</a>, 
<a href="/search/cs?searchtype=author&query=D%27Orazio%2C+V+J">Vito J. D&#x27;Orazio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08295" title="Abstract">arXiv:2308.08295</a> (replaced) [<a href="/pdf/2308.08295" title="Download PDF">pdf</a>, <a href="/format/2308.08295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CMD: a framework for Context-aware Model self-Detoxification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zecheng Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Keyan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juntao Li</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yuyang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pinzheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+B">Bowen Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.08874" title="Abstract">arXiv:2308.08874</a> (replaced) [<a href="/pdf/2308.08874" title="Download PDF">pdf</a>, <a href="/format/2308.08874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distribution-Free Proofs of Proximity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aaronson%2C+H">Hugo Aaronson</a>, 
<a href="/search/cs?searchtype=author&query=Gur%2C+T">Tom Gur</a>, 
<a href="/search/cs?searchtype=author&query=Rajgopal%2C+N">Ninad Rajgopal</a>, 
<a href="/search/cs?searchtype=author&query=Rothblum%2C+R+D">Ron D. Rothblum</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.09124" title="Abstract">arXiv:2308.09124</a> (replaced) [<a href="/pdf/2308.09124" title="Download PDF">pdf</a>, <a href="/format/2308.09124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linearity of Relation Decoding in Transformer Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hernandez%2C+E">Evan Hernandez</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A+S">Arnab Sen Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Haklay%2C+T">Tal Haklay</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+K">Kevin Meng</a>, 
<a href="/search/cs?searchtype=author&query=Wattenberg%2C+M">Martin Wattenberg</a>, 
<a href="/search/cs?searchtype=author&query=Andreas%2C+J">Jacob Andreas</a>, 
<a href="/search/cs?searchtype=author&query=Belinkov%2C+Y">Yonatan Belinkov</a>, 
<a href="/search/cs?searchtype=author&query=Bau%2C+D">David Bau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.12563" title="Abstract">arXiv:2308.12563</a> (replaced) [<a href="/pdf/2308.12563" title="Download PDF">pdf</a>, <a href="/format/2308.12563" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multivariate Time-Series Anomaly Detection with Contaminated Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ho%2C+T+K+K">Thi Kieu Khanh Ho</a>, 
<a href="/search/cs?searchtype=author&query=Armanfard%2C+N">Narges Armanfard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 tables, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15191" title="Abstract">arXiv:2308.15191</a> (replaced) [<a href="/pdf/2308.15191" title="Download PDF">pdf</a>, <a href="/ps/2308.15191" title="Download PostScript">ps</a>, <a href="/format/2308.15191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> State of the Art Report: Verified Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Woodcock%2C+J">Jim Woodcock</a>, 
<a href="/search/cs?searchtype=author&query=Andersen%2C+M+S">Mikkel Schmidt Andersen</a>, 
<a href="/search/cs?searchtype=author&query=Aranha%2C+D+F">Diego F. Aranha</a>, 
<a href="/search/cs?searchtype=author&query=Hallerstede%2C+S">Stefan Hallerstede</a>, 
<a href="/search/cs?searchtype=author&query=Hansen%2C+S+T">Simon Thrane Hansen</a>, 
<a href="/search/cs?searchtype=author&query=Jakobsen%2C+N+K">Nikolaj Kuhne Jakobsen</a>, 
<a href="/search/cs?searchtype=author&query=Kulik%2C+T">Tomas Kulik</a>, 
<a href="/search/cs?searchtype=author&query=Larsen%2C+P+G">Peter Gorm Larsen</a>, 
<a href="/search/cs?searchtype=author&query=Macedo%2C+H+D">Hugo Daniel Macedo</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+C+I+I">Carlos Ignacio Isasa Martin</a>, 
<a href="/search/cs?searchtype=author&query=Norrild%2C+V+A+M">Victor Alexander Mtsimbe Norrild</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 54 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15275" title="Abstract">arXiv:2308.15275</a> (replaced) [<a href="/pdf/2308.15275" title="Download PDF">pdf</a>, <a href="/ps/2308.15275" title="Download PostScript">ps</a>, <a href="/format/2308.15275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Moments of the number of points in a bounded set for number field  lattices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gargava%2C+N">Nihar Gargava</a>, 
<a href="/search/math?searchtype=author&query=Serban%2C+V">Vlad Serban</a>, 
<a href="/search/math?searchtype=author&query=Viazovska%2C+M">Maryna Viazovska</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 pages, 1 figure, incomplete tangential result in Section 2 was removed and treated in more detail in a separate paper, Appendix C was added
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Number Theory (math.NT)</span>; Information Theory (cs.IT); Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15997" title="Abstract">arXiv:2308.15997</a> (replaced) [<a href="/pdf/2308.15997" title="Download PDF">pdf</a>, <a href="/format/2308.15997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the entropy and information of Gaussian mixtures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eskenazis%2C+A">Alexandros Eskenazis</a>, 
<a href="/search/cs?searchtype=author&query=Gavalakis%2C+L">Lampros Gavalakis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, no figures. Reviewer's comments have been incorporated. To appear in Mathematika
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00378" title="Abstract">arXiv:2309.00378</a> (replaced) [<a href="/pdf/2309.00378" title="Download PDF">pdf</a>, <a href="/format/2309.00378" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Long-Term Ad Memorability: Understanding and Generating Memorable Ads
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=I%2C+H+S">Harini S I</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Somesh Singh</a>, 
<a href="/search/cs?searchtype=author&query=Singla%2C+Y+K">Yaman K Singla</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharyya%2C+A">Aanisha Bhattacharyya</a>, 
<a href="/search/cs?searchtype=author&query=Baths%2C+V">Veeky Baths</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Changyou Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+R+R">Rajiv Ratn Shah</a>, 
<a href="/search/cs?searchtype=author&query=Krishnamurthy%2C+B">Balaji Krishnamurthy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.02726" title="Abstract">arXiv:2309.02726</a> (replaced) [<a href="/pdf/2309.02726" title="Download PDF">pdf</a>, <a href="/format/2309.02726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models for Automated Open-domain Scientific Hypotheses  Discovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zonglin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+X">Xinya Du</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junxian Li</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jie Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Poria%2C+S">Soujanya Poria</a>, 
<a href="/search/cs?searchtype=author&query=Cambria%2C+E">Erik Cambria</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.04015" title="Abstract">arXiv:2309.04015</a> (replaced) [<a href="/pdf/2309.04015" title="Download PDF">pdf</a>, <a href="/format/2309.04015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Transport with Tempered Exponential Measures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amid%2C+E">Ehsan Amid</a>, 
<a href="/search/cs?searchtype=author&query=Nielsen%2C+F">Frank Nielsen</a>, 
<a href="/search/cs?searchtype=author&query=Nock%2C+R">Richard Nock</a>, 
<a href="/search/cs?searchtype=author&query=Warmuth%2C+M+K">Manfred K. Warmuth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07430" title="Abstract">arXiv:2309.07430</a> (replaced) [<a href="/pdf/2309.07430" title="Download PDF">pdf</a>, <a href="/format/2309.07430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adapted Large Language Models Can Outperform Medical Experts in Clinical  Text Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Van+Veen%2C+D">Dave Van Veen</a>, 
<a href="/search/cs?searchtype=author&query=Van+Uden%2C+C">Cara Van Uden</a>, 
<a href="/search/cs?searchtype=author&query=Blankemeier%2C+L">Louis Blankemeier</a>, 
<a href="/search/cs?searchtype=author&query=Delbrouck%2C+J">Jean-Benoit Delbrouck</a>, 
<a href="/search/cs?searchtype=author&query=Aali%2C+A">Asad Aali</a>, 
<a href="/search/cs?searchtype=author&query=Bluethgen%2C+C">Christian Bluethgen</a>, 
<a href="/search/cs?searchtype=author&query=Pareek%2C+A">Anuj Pareek</a>, 
<a href="/search/cs?searchtype=author&query=Polacin%2C+M">Malgorzata Polacin</a>, 
<a href="/search/cs?searchtype=author&query=Reis%2C+E+P">Eduardo Pontes Reis</a>, 
<a href="/search/cs?searchtype=author&query=Seehofnerova%2C+A">Anna Seehofnerova</a>, 
<a href="/search/cs?searchtype=author&query=Rohatgi%2C+N">Nidhi Rohatgi</a>, 
<a href="/search/cs?searchtype=author&query=Hosamani%2C+P">Poonam Hosamani</a>, 
<a href="/search/cs?searchtype=author&query=Collins%2C+W">William Collins</a>, 
<a href="/search/cs?searchtype=author&query=Ahuja%2C+N">Neera Ahuja</a>, 
<a href="/search/cs?searchtype=author&query=Langlotz%2C+C+P">Curtis P. Langlotz</a>, 
<a href="/search/cs?searchtype=author&query=Hom%2C+J">Jason Hom</a>, 
<a href="/search/cs?searchtype=author&query=Gatidis%2C+S">Sergios Gatidis</a>, 
<a href="/search/cs?searchtype=author&query=Pauly%2C+J">John Pauly</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhari%2C+A+S">Akshay S. Chaudhari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 19 figures. Compared to v3, this version adds downstream analyses and incorporates reviewer feedback to reinforce the initial findings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08047" title="Abstract">arXiv:2309.08047</a> (replaced) [<a href="/pdf/2309.08047" title="Download PDF">pdf</a>, <a href="/format/2309.08047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gender Bias in News Summarization: Measures, Pitfalls and Corpora
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Steen%2C+J">Julius Steen</a>, 
<a href="/search/cs?searchtype=author&query=Markert%2C+K">Katja Markert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08316" title="Abstract">arXiv:2309.08316</a> (replaced) [<a href="/pdf/2309.08316" title="Download PDF">pdf</a>, <a href="/format/2309.08316" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to Handle Different Types of Out-of-Distribution Scenarios in  Computational Argumentation? A Comprehensive and Fine-Grained Field Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Waldis%2C+A">Andreas Waldis</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y">Yufang Hou</a>, 
<a href="/search/cs?searchtype=author&query=Gurevych%2C+I">Iryna Gurevych</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11322" title="Abstract">arXiv:2309.11322</a> (replaced) [<a href="/pdf/2309.11322" title="Download PDF">pdf</a>, <a href="/format/2309.11322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vector database management systems: Fundamental concepts, use-cases, and  current challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Taipalus%2C+T">Toni Taipalus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11713" title="Abstract">arXiv:2309.11713</a> (replaced) [<a href="/pdf/2309.11713" title="Download PDF">pdf</a>, <a href="/format/2309.11713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quasi-Monte Carlo for 3D Sliced Wasserstein
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Nguyen%2C+K">Khai Nguyen</a>, 
<a href="/search/stat?searchtype=author&query=Bariletto%2C+N">Nicola Bariletto</a>, 
<a href="/search/stat?searchtype=author&query=Ho%2C+N">Nhat Ho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR 2024 (Spotlight), 25 pages, 13 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Graphics (cs.GR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13144" title="Abstract">arXiv:2309.13144</a> (replaced) [<a href="/pdf/2309.13144" title="Download PDF">pdf</a>, <a href="/format/2309.13144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SoRTS: Learned Tree Search for Long Horizon Social Robot Navigation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Navarro%2C+I">Ingrid Navarro</a>, 
<a href="/search/cs?searchtype=author&query=Patrikar%2C+J">Jay Patrikar</a>, 
<a href="/search/cs?searchtype=author&query=Dantas%2C+J+P+A">Joao P. A. Dantas</a>, 
<a href="/search/cs?searchtype=author&query=Baijal%2C+R">Rohan Baijal</a>, 
<a href="/search/cs?searchtype=author&query=Higgins%2C+I">Ian Higgins</a>, 
<a href="/search/cs?searchtype=author&query=Scherer%2C+S">Sebastian Scherer</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+J">Jean Oh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2304.01428">arXiv:2304.01428</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13360" title="Abstract">arXiv:2309.13360</a> (replaced) [<a href="/pdf/2309.13360" title="Download PDF">pdf</a>, <a href="/ps/2309.13360" title="Download PostScript">ps</a>, <a href="/format/2309.13360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Impacts of DEM Type and Resolution on Deep Learning-Based Flood  Inundation Mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fereshtehpour%2C+M">Mohammad Fereshtehpour</a>, 
<a href="/search/cs?searchtype=author&query=Esmaeilzadeh%2C+M">Mostafa Esmaeilzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Alipour%2C+R+S">Reza Saleh Alipour</a>, 
<a href="/search/cs?searchtype=author&query=Burian%2C+S+J">Steven J. Burian</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Earth Science Informatics, 1-21 (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15126" title="Abstract">arXiv:2309.15126</a> (replaced) [<a href="/pdf/2309.15126" title="Download PDF">pdf</a>, <a href="/format/2309.15126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Peptides to Nanostructures: A Euclidean Transformer for Fast and  Stable Machine Learned Force Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Frank%2C+J+T">J. Thorben Frank</a>, 
<a href="/search/physics?searchtype=author&query=Unke%2C+O+T">Oliver T. Unke</a>, 
<a href="/search/physics?searchtype=author&query=M%C3%BCller%2C+K">Klaus-Robert M&#xfc;ller</a>, 
<a href="/search/physics?searchtype=author&query=Chmiela%2C+S">Stefan Chmiela</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.15237" title="Abstract">arXiv:2309.15237</a> (replaced) [<a href="/pdf/2309.15237" title="Download PDF">pdf</a>, <a href="/ps/2309.15237" title="Download PostScript">ps</a>, <a href="/format/2309.15237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> User Experience Design Professionals&#x27; Perceptions of Generative  Artificial Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jie Li</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+H">Hancheng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Laura Lin</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y">Youyang Hou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+R">Ruihao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+A+E">Abdallah El Ali</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to CHI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16672" title="Abstract">arXiv:2309.16672</a> (replaced) [<a href="/pdf/2309.16672" title="Download PDF">pdf</a>, <a href="/format/2309.16672" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Transform for Generalizable Instance-wise Invariance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singhal%2C+U">Utkarsh Singhal</a>, 
<a href="/search/cs?searchtype=author&query=Esteves%2C+C">Carlos Esteves</a>, 
<a href="/search/cs?searchtype=author&query=Makadia%2C+A">Ameesh Makadia</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S+X">Stella X. Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICCV 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17384" title="Abstract">arXiv:2309.17384</a> (replaced) [<a href="/pdf/2309.17384" title="Download PDF">pdf</a>, <a href="/format/2309.17384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Universal Speech Enhancement for Diverse Input Conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+W">Wangyou Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Saijo%2C+K">Kohei Saijo</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Zhong-Qiu Wang</a>, 
<a href="/search/eess?searchtype=author&query=Watanabe%2C+S">Shinji Watanabe</a>, 
<a href="/search/eess?searchtype=author&query=Qian%2C+Y">Yanmin Qian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures, 5 tables, published in ASRU 2023 (corrected the results of noisy speech on CHiME-4 (Simu) in Table 4)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17452" title="Abstract">arXiv:2309.17452</a> (replaced) [<a href="/pdf/2309.17452" title="Download PDF">pdf</a>, <a href="/format/2309.17452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gou%2C+Z">Zhibin Gou</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Z">Zhihong Shao</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yeyun Gong</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yelong Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yujiu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+N">Nan Duan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weizhu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024; First two authors equal contribution
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00429" title="Abstract">arXiv:2310.00429</a> (replaced) [<a href="/pdf/2310.00429" title="Download PDF">pdf</a>, <a href="/format/2310.00429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Stability of Iterative Retraining of Generative Models on their  own Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bertrand%2C+Q">Quentin Bertrand</a>, 
<a href="/search/cs?searchtype=author&query=Bose%2C+A+J">Avishek Joey Bose</a>, 
<a href="/search/cs?searchtype=author&query=Duplessis%2C+A">Alexandre Duplessis</a>, 
<a href="/search/cs?searchtype=author&query=Jiralerspong%2C+M">Marco Jiralerspong</a>, 
<a href="/search/cs?searchtype=author&query=Gidel%2C+G">Gauthier Gidel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00796" title="Abstract">arXiv:2310.00796</a> (replaced) [<a href="/pdf/2310.00796" title="Download PDF">pdf</a>, <a href="/format/2310.00796" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Injecting a Structural Inductive Bias into a Seq2Seq Model by Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lindemann%2C+M">Matthias Lindemann</a>, 
<a href="/search/cs?searchtype=author&query=Koller%2C+A">Alexander Koller</a>, 
<a href="/search/cs?searchtype=author&query=Titov%2C+I">Ivan Titov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00867" title="Abstract">arXiv:2310.00867</a> (replaced) [<a href="/pdf/2310.00867" title="Download PDF">pdf</a>, <a href="/format/2310.00867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Compressed LLMs Forget Knowledge? An Experimental Study with  Practical Implications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoang%2C+D+N+M">Duc N.M Hoang</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+M">Minsik Cho</a>, 
<a href="/search/cs?searchtype=author&query=Merth%2C+T">Thomas Merth</a>, 
<a href="/search/cs?searchtype=author&query=Rastegari%2C+M">Mohammad Rastegari</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01101" title="Abstract">arXiv:2310.01101</a> (replaced) [<a href="/pdf/2310.01101" title="Download PDF">pdf</a>, <a href="/ps/2310.01101" title="Download PostScript">ps</a>, <a href="/format/2310.01101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed end-effector formation control for mixed fully- and  under-actuated manipulators with flexible joints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Peng%2C+Z">Zhiyu Peng</a>, 
<a href="/search/eess?searchtype=author&query=Jayawardhana%2C+B">Bayu Jayawardhana</a>, 
<a href="/search/eess?searchtype=author&query=Xin%2C+X">Xin Xin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01174" title="Abstract">arXiv:2310.01174</a> (replaced) [<a href="/pdf/2310.01174" title="Download PDF">pdf</a>, <a href="/format/2310.01174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Light Schr&#xf6;dinger Bridge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Korotin%2C+A">Alexander Korotin</a>, 
<a href="/search/cs?searchtype=author&query=Gushchin%2C+N">Nikita Gushchin</a>, 
<a href="/search/cs?searchtype=author&query=Burnaev%2C+E">Evgeny Burnaev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01195" title="Abstract">arXiv:2310.01195</a> (replaced) [<a href="/pdf/2310.01195" title="Download PDF">pdf</a>, <a href="/ps/2310.01195" title="Download PostScript">ps</a>, <a href="/format/2310.01195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated K-means Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garst%2C+S">Swier Garst</a>, 
<a href="/search/cs?searchtype=author&query=Reinders%2C+M">Marcel Reinders</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02473" title="Abstract">arXiv:2310.02473</a> (replaced) [<a href="/pdf/2310.02473" title="Download PDF">pdf</a>, <a href="/format/2310.02473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompting-based Temporal Domain Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hosseini%2C+S">Sepidehsadat Hosseini</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+M">Mengyao Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Hajimirsadegh%2C+H">Hossein Hajimirsadegh</a>, 
<a href="/search/cs?searchtype=author&query=Tung%2C+F">Frederick Tung</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04741" title="Abstract">arXiv:2310.04741</a> (replaced) [<a href="/pdf/2310.04741" title="Download PDF">pdf</a>, <a href="/format/2310.04741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Keep Moving: identifying task-relevant subspaces to maximise plasticity  for newly learned tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anthes%2C+D">Daniel Anthes</a>, 
<a href="/search/cs?searchtype=author&query=Thorat%2C+S">Sushrut Thorat</a>, 
<a href="/search/cs?searchtype=author&query=K%C3%B6nig%2C+P">Peter K&#xf6;nig</a>, 
<a href="/search/cs?searchtype=author&query=Kietzmann%2C+T+C">Tim C. Kietzmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 6 figures, Substantial Revision
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05120" title="Abstract">arXiv:2310.05120</a> (replaced) [<a href="/pdf/2310.05120" title="Download PDF">pdf</a>, <a href="/format/2310.05120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear Loop Synthesis for Quadratic Invariants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hitarth%2C+S">S. Hitarth</a>, 
<a href="/search/cs?searchtype=author&query=Kenison%2C+G">George Kenison</a>, 
<a href="/search/cs?searchtype=author&query=Kov%C3%A1cs%2C+L">Laura Kov&#xe1;cs</a>, 
<a href="/search/cs?searchtype=author&query=Varonka%2C+A">Anton Varonka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Extended version of our conference paper accepted to STACS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>; Number Theory (math.NT)

</div>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05212" title="Abstract">arXiv:2310.05212</a> (replaced) [<a href="/pdf/2310.05212" title="Download PDF">pdf</a>, <a href="/format/2310.05212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Semiotics Networks Representing Awareness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kupeev%2C+D">David Kupeev</a>, 
<a href="/search/cs?searchtype=author&query=Nitcany%2C+E">Eyal Nitcany</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05866" title="Abstract">arXiv:2310.05866</a> (replaced) [<a href="/pdf/2310.05866" title="Download PDF">pdf</a>, <a href="/format/2310.05866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative quantum machine learning via denoising diffusion  probabilistic models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Zhang%2C+B">Bingzhi Zhang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Xu%2C+P">Peng Xu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chen%2C+X">Xiaohui Chen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zhuang%2C+Q">Quntao Zhuang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5+10 pages, 16 figures. PRL accepted version. Code available at: <a href="https://github.com/francis-hsu/quantgenmdl">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06549" title="Abstract">arXiv:2310.06549</a> (replaced) [<a href="/pdf/2310.06549" title="Download PDF">pdf</a>, <a href="/format/2310.06549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield  but Also a Catalyst for Model Inversion Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Struppek%2C+L">Lukas Struppek</a>, 
<a href="/search/cs?searchtype=author&query=Hintersdorf%2C+D">Dominik Hintersdorf</a>, 
<a href="/search/cs?searchtype=author&query=Kersting%2C+K">Kristian Kersting</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07637" title="Abstract">arXiv:2310.07637</a> (replaced) [<a href="/pdf/2310.07637" title="Download PDF">pdf</a>, <a href="/format/2310.07637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OpsEval: A Comprehensive IT Operations Benchmark Suite for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuhe Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+C">Changhua Pei</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Longlong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bohan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mingze Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhirui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yongqian Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shenglin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haiming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianhui Li</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+G">Gaogang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+X">Xidao Wen</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+X">Xiaohui Nie</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+M">Minghua Ma</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+D">Dan Pei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.08644" title="Abstract">arXiv:2310.08644</a> (replaced) [<a href="/pdf/2310.08644" title="Download PDF">pdf</a>, <a href="/ps/2310.08644" title="Download PostScript">ps</a>, <a href="/format/2310.08644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of  Geoscientific Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuan-Heng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+H+V">Hoshin V. Gupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 65 pages, 7 figures in the main text, 10 figures, and 10 tables in the supplementary materials
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09789" title="Abstract">arXiv:2310.09789</a> (replaced) [<a href="/pdf/2310.09789" title="Download PDF">pdf</a>, <a href="/format/2310.09789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FLrce: Resource-Efficient Federated Learning with Early-Stopping  Strategy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Niu%2C+Z">Ziru Niu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Hai Dong</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+A+K">A. Kai Qin</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+T">Tao Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arxiv preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11878" title="Abstract">arXiv:2310.11878</a> (replaced) [<a href="/pdf/2310.11878" title="Download PDF">pdf</a>, <a href="/format/2310.11878" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Dissonance to Insights: Dissecting Disagreements in Rationale  Construction for Case Outcome Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shanshan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Santosh%2C+T+Y+S+S">T.Y.S.S Santosh</a>, 
<a href="/search/cs?searchtype=author&query=Ichim%2C+O">Oana Ichim</a>, 
<a href="/search/cs?searchtype=author&query=Risini%2C+I">Isabella Risini</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>, 
<a href="/search/cs?searchtype=author&query=Grabmair%2C+M">Matthias Grabmair</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12481" title="Abstract">arXiv:2310.12481</a> (replaced) [<a href="/pdf/2310.12481" title="Download PDF">pdf</a>, <a href="/format/2310.12481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiao%2C+W">Wenxiang Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jingyuan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+R">Ruyi Dai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jen-tse Huang</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zhaopeng Tu</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+M+R">Michael R. Lyu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15072" title="Abstract">arXiv:2310.15072</a> (replaced) [<a href="/pdf/2310.15072" title="Download PDF">pdf</a>, <a href="/format/2310.15072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RD-VIO: Robust Visual-Inertial Odometry for Mobile Augmented Reality in  Dynamic Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xiaokun Pan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+G">Gan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Ziyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Nan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+H">Hujun Bao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guofeng Zhang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Visualization and Computer Graphics, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15767" title="Abstract">arXiv:2310.15767</a> (replaced) [<a href="/pdf/2310.15767" title="Download PDF">pdf</a>, <a href="/ps/2310.15767" title="Download PostScript">ps</a>, <a href="/format/2310.15767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unpaired MRI Super Resolution with Contrastive Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+H">Hao Li</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Q">Quanwei Liu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+J">Jianan Liu</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+X">Xiling Liu</a>, 
<a href="/search/eess?searchtype=author&query=Dong%2C+Y">Yanni Dong</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+T">Tao Huang</a>, 
<a href="/search/eess?searchtype=author&query=Lv%2C+Z">Zhihan Lv</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.16077" title="Abstract">arXiv:2310.16077</a> (replaced) [<a href="/pdf/2310.16077" title="Download PDF">pdf</a>, <a href="/format/2310.16077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Femtosecond laser fabricated nitinol living hinges for millimeter-sized  robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hedrick%2C+A">Alexander Hedrick</a>, 
<a href="/search/cs?searchtype=author&query=Kabutz%2C+H">Heiko Kabutz</a>, 
<a href="/search/cs?searchtype=author&query=Smith%2C+L">Lawrence Smith</a>, 
<a href="/search/cs?searchtype=author&query=MacCurdy%2C+R">Robert MacCurdy</a>, 
<a href="/search/cs?searchtype=author&query=Jayaram%2C+K">Kaushik Jayaram</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 4 figures, submitted to IEEE RA-L
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.18884" title="Abstract">arXiv:2310.18884</a> (replaced) [<a href="/pdf/2310.18884" title="Download PDF">pdf</a>, <a href="/format/2310.18884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple and Asymmetric Graph Contrastive Learning without Augmentations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+T">Teng Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Huaisheng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhengyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Suhang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Main Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20329" title="Abstract">arXiv:2310.20329</a> (replaced) [<a href="/pdf/2310.20329" title="Download PDF">pdf</a>, <a href="/format/2310.20329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InstructCoder: Empowering Language Models for Code Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kaixin Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Q">Qisheng Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yuxi Xie</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tiedong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qizhe Xie</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Junxian He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00237" title="Abstract">arXiv:2311.00237</a> (replaced) [<a href="/pdf/2311.00237" title="Download PDF">pdf</a>, <a href="/format/2311.00237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Mystery of In-Context Learning: A Comprehensive Survey on  Interpretation and Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuxiang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiazheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Y">Yanzheng Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hanqi Yan</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+L">Lin Gui</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yulan He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02029" title="Abstract">arXiv:2311.02029</a> (replaced) [<a href="/pdf/2311.02029" title="Download PDF">pdf</a>, <a href="/ps/2311.02029" title="Download PostScript">ps</a>, <a href="/format/2311.02029" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MetaTrinity: Enabling Fast Metagenomic Classification via Seed Counting  and Edit Distance Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Gollwitzer%2C+A+E">Arvid E. Gollwitzer</a>, 
<a href="/search/q-bio?searchtype=author&query=Alser%2C+M">Mohammed Alser</a>, 
<a href="/search/q-bio?searchtype=author&query=Bergtholdt%2C+J">Joel Bergtholdt</a>, 
<a href="/search/q-bio?searchtype=author&query=Lindegger%2C+J">Joel Lindegger</a>, 
<a href="/search/q-bio?searchtype=author&query=Rumpf%2C+M">Maximilian-David Rumpf</a>, 
<a href="/search/q-bio?searchtype=author&query=Firtina%2C+C">Can Firtina</a>, 
<a href="/search/q-bio?searchtype=author&query=Mangul%2C+S">Serghei Mangul</a>, 
<a href="/search/q-bio?searchtype=author&query=Mutlu%2C+O">Onur Mutlu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Hardware Architecture (cs.AR); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02850" title="Abstract">arXiv:2311.02850</a> (replaced) [<a href="/pdf/2311.02850" title="Download PDF">pdf</a>, <a href="/format/2311.02850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IR-STP: Enhancing Autonomous Driving with Interaction Reasoning in  Spatio-Temporal Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yingbing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+J">Jie Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+L">Lu Gan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Sheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongji Liu</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+X">Xiaodong Mei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Ming Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 10 figures, accepted by IEEE-TITS at this January
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06861" title="Abstract">arXiv:2311.06861</a> (replaced) [<a href="/pdf/2311.06861" title="Download PDF">pdf</a>, <a href="/format/2311.06861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-efficient Beamforming for RISs-aided Communications: Gradient  Based Meta Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinquan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+F">Fenghao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qianyun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qihao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chongwen Huang</a>, 
<a href="/search/cs?searchtype=author&query=Alhammadi%2C+A">Ahmed Alhammadi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhaoyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yuen%2C+C">Chau Yuen</a>, 
<a href="/search/cs?searchtype=author&query=Debbah%2C+M">M&#xe9;rouane Debbah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 8 figures. Accepted in IEEE ICC 2024 (GCSN symposium)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07453" title="Abstract">arXiv:2311.07453</a> (replaced) [<a href="/pdf/2311.07453" title="Download PDF">pdf</a>, <a href="/format/2311.07453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChartCheck: Explainable Fact-Checking over Real-World Chart Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Akhtar%2C+M">Mubashara Akhtar</a>, 
<a href="/search/cs?searchtype=author&query=Subedi%2C+N">Nikesh Subedi</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+V">Vivek Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Tahmasebi%2C+S">Sahar Tahmasebi</a>, 
<a href="/search/cs?searchtype=author&query=Cocarascu%2C+O">Oana Cocarascu</a>, 
<a href="/search/cs?searchtype=author&query=Simperl%2C+E">Elena Simperl</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08011" title="Abstract">arXiv:2311.08011</a> (replaced) [<a href="/pdf/2311.08011" title="Download PDF">pdf</a>, <a href="/format/2311.08011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forgetting before Learning: Utilizing Parametric Arithmetic for  Knowledge Updating in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%2C+S">Shiwen Ni</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Dingwei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengming Li</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiping Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruifeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Min Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08370" title="Abstract">arXiv:2311.08370</a> (replaced) [<a href="/pdf/2311.08370" title="Download PDF">pdf</a>, <a href="/format/2311.08370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SimpleSafetyTests: a Test Suite for Identifying Critical Safety Risks in  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vidgen%2C+B">Bertie Vidgen</a>, 
<a href="/search/cs?searchtype=author&query=Scherrer%2C+N">Nino Scherrer</a>, 
<a href="/search/cs?searchtype=author&query=Kirk%2C+H+R">Hannah Rose Kirk</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+R">Rebecca Qian</a>, 
<a href="/search/cs?searchtype=author&query=Kannappan%2C+A">Anand Kannappan</a>, 
<a href="/search/cs?searchtype=author&query=Hale%2C+S+A">Scott A. Hale</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%B6ttger%2C+P">Paul R&#xf6;ttger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08803" title="Abstract">arXiv:2311.08803</a> (replaced) [<a href="/pdf/2311.08803" title="Download PDF">pdf</a>, <a href="/format/2311.08803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StrategyLLM: Large Language Models as Strategy Generators, Executors,  Optimizers, and Evaluators for Problem Solving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Haiyun Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+D">Deng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Shuming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+W">Wai Lam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08883" title="Abstract">arXiv:2311.08883</a> (replaced) [<a href="/pdf/2311.08883" title="Download PDF">pdf</a>, <a href="/format/2311.08883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling Large Language Models to Learn from Rules
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wenkai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yankai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Jirong Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08936" title="Abstract">arXiv:2311.08936</a> (replaced) [<a href="/pdf/2311.08936" title="Download PDF">pdf</a>, <a href="/format/2311.08936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Confident Naturalness Explanation (CNE): A Framework to Explain and  Assess Patterns Forming Naturalness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Emam%2C+A">Ahmed Emam</a>, 
<a href="/search/cs?searchtype=author&query=Farag%2C+M">Mohamed Farag</a>, 
<a href="/search/cs?searchtype=author&query=Roscher%2C+R">Ribana Roscher</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Geoscience and Remote Sensing Letters, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09204" title="Abstract">arXiv:2311.09204</a> (replaced) [<a href="/pdf/2311.09204" title="Download PDF">pdf</a>, <a href="/format/2311.09204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fusion-Eval: Integrating Evaluators with LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shu%2C+L">Lei Shu</a>, 
<a href="/search/cs?searchtype=author&query=Wichers%2C+N">Nevan Wichers</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+L">Liangchen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yinxiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jindong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+L">Lei Meng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09613" title="Abstract">arXiv:2311.09613</a> (replaced) [<a href="/pdf/2311.09613" title="Download PDF">pdf</a>, <a href="/format/2311.09613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Digital Socrates: Evaluating LLMs through Explanation Critiques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yuling Gu</a>, 
<a href="/search/cs?searchtype=author&query=Tafjord%2C+O">Oyvind Tafjord</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+P">Peter Clark</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09618" title="Abstract">arXiv:2311.09618</a> (replaced) [<a href="/pdf/2311.09618" title="Download PDF">pdf</a>, <a href="/format/2311.09618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simulating Opinion Dynamics with Networks of LLM-based Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Chuang%2C+Y">Yun-Shiuan Chuang</a>, 
<a href="/search/physics?searchtype=author&query=Goyal%2C+A">Agam Goyal</a>, 
<a href="/search/physics?searchtype=author&query=Harlalka%2C+N">Nikunj Harlalka</a>, 
<a href="/search/physics?searchtype=author&query=Suresh%2C+S">Siddharth Suresh</a>, 
<a href="/search/physics?searchtype=author&query=Hawkins%2C+R">Robert Hawkins</a>, 
<a href="/search/physics?searchtype=author&query=Yang%2C+S">Sijia Yang</a>, 
<a href="/search/physics?searchtype=author&query=Shah%2C+D">Dhavan Shah</a>, 
<a href="/search/physics?searchtype=author&query=Hu%2C+J">Junjie Hu</a>, 
<a href="/search/physics?searchtype=author&query=Rogers%2C+T+T">Timothy T. Rogers</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics and Society (physics.soc-ph)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09630" title="Abstract">arXiv:2311.09630</a> (replaced) [<a href="/pdf/2311.09630" title="Download PDF">pdf</a>, <a href="/format/2311.09630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding Susceptibility: Modeling Misbelief to Misinformation Through a  Computational Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanchen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+M+D">Mingyu Derek Ma</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+W">Wenna Qin</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+A">Azure Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiaao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+W">Weiyan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Diyi Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09665" title="Abstract">arXiv:2311.09665</a> (replaced) [<a href="/pdf/2311.09665" title="Download PDF">pdf</a>, <a href="/format/2311.09665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Wisdom of Partisan Crowds: Comparing Collective Intelligence in  Humans and LLM-based Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chuang%2C+Y">Yun-Shiuan Chuang</a>, 
<a href="/search/cs?searchtype=author&query=Suresh%2C+S">Siddharth Suresh</a>, 
<a href="/search/cs?searchtype=author&query=Harlalka%2C+N">Nikunj Harlalka</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+A">Agam Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Hawkins%2C+R">Robert Hawkins</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Sijia Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+D">Dhavan Shah</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Junjie Hu</a>, 
<a href="/search/cs?searchtype=author&query=Rogers%2C+T+T">Timothy T. Rogers</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09731" title="Abstract">arXiv:2311.09731</a> (replaced) [<a href="/pdf/2311.09731" title="Download PDF">pdf</a>, <a href="/format/2311.09731" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Examining LLMs&#x27; Uncertainty Expression Towards Questions Outside  Parametric Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Genglin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xingyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Lifan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yangyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Hao Peng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09832" title="Abstract">arXiv:2311.09832</a> (replaced) [<a href="/pdf/2311.09832" title="Download PDF">pdf</a>, <a href="/format/2311.09832" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WatME: Towards Lossless Watermarking Through Lexical Redundancy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Bian%2C+Y">Yatao Bian</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yang Deng</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+D">Deng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuaiyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+P">Peilin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kam-fai Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10162" title="Abstract">arXiv:2311.10162</a> (replaced) [<a href="/pdf/2311.10162" title="Download PDF">pdf</a>, <a href="/ps/2311.10162" title="Download PostScript">ps</a>, <a href="/format/2311.10162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> K-space Cold Diffusion: Learning to Reconstruct Accelerated MRI without  Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Shen%2C+G">Guoyao Shen</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+M">Mengyu Li</a>, 
<a href="/search/eess?searchtype=author&query=Farris%2C+C+W">Chad W. Farris</a>, 
<a href="/search/eess?searchtype=author&query=Anderson%2C+S">Stephan Anderson</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 5 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Medical Physics (physics.med-ph)

</div>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.11229" title="Abstract">arXiv:2311.11229</a> (replaced) [<a href="/pdf/2311.11229" title="Download PDF">pdf</a>, <a href="/format/2311.11229" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal ATE Mitigates Unintended Bias in Controlled Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Madhavan%2C+R">Rahul Madhavan</a>, 
<a href="/search/cs?searchtype=author&query=Wadhawan%2C+K">Kahini Wadhawan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.18048" title="Abstract">arXiv:2311.18048</a> (replaced) [<a href="/pdf/2311.18048" title="Download PDF">pdf</a>, <a href="/format/2311.18048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Interventional Perspective on Identifiability in Gaussian LTI Systems  with Independent Component Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rajendran%2C+G">Goutham Rajendran</a>, 
<a href="/search/cs?searchtype=author&query=Reizinger%2C+P">Patrik Reizinger</a>, 
<a href="/search/cs?searchtype=author&query=Brendel%2C+W">Wieland Brendel</a>, 
<a href="/search/cs?searchtype=author&query=Ravikumar%2C+P">Pradeep Ravikumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> CLeaR2024 camera ready. Code available at <a href="https://github.com/rpatrik96/lti-ica">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE); Systems and Control (eess.SY); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.18107" title="Abstract">arXiv:2311.18107</a> (replaced) [<a href="/pdf/2311.18107" title="Download PDF">pdf</a>, <a href="/format/2311.18107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Stochastic-Geometrical Framework for Object Pose Estimation based on  Mixture Models Avoiding the Correspondence Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hoegele%2C+W">Wolfgang Hoegele</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.18164" title="Abstract">arXiv:2311.18164</a> (replaced) [<a href="/pdf/2311.18164" title="Download PDF">pdf</a>, <a href="/format/2311.18164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Paradox Of Just-in-Time Liquidity in Decentralized Exchanges: More  Providers Can Sometimes Mean Less Liquidity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Capponi%2C+A">Agostino Capponi</a>, 
<a href="/search/q-fin?searchtype=author&query=Jia%2C+R">Ruizhe Jia</a>, 
<a href="/search/q-fin?searchtype=author&query=Zhu%2C+B">Brian Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">General Finance (q-fin.GN)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.02867" title="Abstract">arXiv:2312.02867</a> (replaced) [<a href="/pdf/2312.02867" title="Download PDF">pdf</a>, <a href="/format/2312.02867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-Supervised Health Index Monitoring with Feature Generation and  Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Frusque%2C+G">Ga&#xeb;tan Frusque</a>, 
<a href="/search/cs?searchtype=author&query=Nejjar%2C+I">Ismail Nejjar</a>, 
<a href="/search/cs?searchtype=author&query=Nabavi%2C+M">Majid Nabavi</a>, 
<a href="/search/cs?searchtype=author&query=Fink%2C+O">Olga Fink</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.04487" title="Abstract">arXiv:2312.04487</a> (replaced) [<a href="/pdf/2312.04487" title="Download PDF">pdf</a>, <a href="/format/2312.04487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On The Maximum Linear Arrangement Problem for Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alemany-Puig%2C+L">Llu&#xed;s Alemany-Puig</a>, 
<a href="/search/cs?searchtype=author&query=Esteban%2C+J+L">Juan Luis Esteban</a>, 
<a href="/search/cs?searchtype=author&query=Ferrer-i-Cancho%2C+R">Ramon Ferrer-i-Cancho</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.05780" title="Abstract">arXiv:2312.05780</a> (replaced) [<a href="/pdf/2312.05780" title="Download PDF">pdf</a>, <a href="/format/2312.05780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PULSAR: Graph based Positive Unlabeled Learning with Multi Stream  Adaptive Convolutions for Parkinson&#x27;s Disease Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alam%2C+M+Z+U">Md. Zarif Ul Alam</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M+S">Md Saiful Islam</a>, 
<a href="/search/cs?searchtype=author&query=Hoque%2C+E">Ehsan Hoque</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+S">M Saifur Rahman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.06528" title="Abstract">arXiv:2312.06528</a> (replaced) [<a href="/pdf/2312.06528" title="Download PDF">pdf</a>, <a href="/format/2312.06528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformers Implement Functional Gradient Descent to Learn Non-Linear  Functions In Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xiang Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuxin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sra%2C+S">Suvrit Sra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.06635" title="Abstract">arXiv:2312.06635</a> (replaced) [<a href="/pdf/2312.06635" title="Download PDF">pdf</a>, <a href="/ps/2312.06635" title="Download PostScript">ps</a>, <a href="/format/2312.06635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gated Linear Attention Transformers with Hardware-Efficient Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Songlin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bailin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yikang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Panda%2C+R">Rameswar Panda</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yoon Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> major update
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.09715" title="Abstract">arXiv:2312.09715</a> (replaced) [<a href="/pdf/2312.09715" title="Download PDF">pdf</a>, <a href="/format/2312.09715" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CETN: Contrast-enhanced Through Network for CTR Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Honghao Li</a>, 
<a href="/search/cs?searchtype=author&query=Sang%2C+L">Lei Sang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuyun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yiwen Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11075" title="Abstract">arXiv:2312.11075</a> (replaced) [<a href="/pdf/2312.11075" title="Download PDF">pdf</a>, <a href="/format/2312.11075" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Split and Rephrase with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ponce%2C+D">David Ponce</a>, 
<a href="/search/cs?searchtype=author&query=Etchegoyhen%2C+T">Thierry Etchegoyhen</a>, 
<a href="/search/cs?searchtype=author&query=P%C3%A9rez%2C+J+C">Jes&#xfa;s Calleja P&#xe9;rez</a>, 
<a href="/search/cs?searchtype=author&query=Gete%2C+H">Harritxu Gete</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11462" title="Abstract">arXiv:2312.11462</a> (replaced) [<a href="/pdf/2312.11462" title="Download PDF">pdf</a>, <a href="/format/2312.11462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cascade Speculative Drafting for Even Faster LLM Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ziyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaocong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jiacheng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Chenkai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jie Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K+C">Kevin Chen-Chuan Chang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11539" title="Abstract">arXiv:2312.11539</a> (replaced) [<a href="/pdf/2312.11539" title="Download PDF">pdf</a>, <a href="/format/2312.11539" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM  Does and Doesn&#x27;t Know
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Shangshang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+H">He Bai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yizhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yi Su</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+X">Xiaochuan Niu</a>, 
<a href="/search/cs?searchtype=author&query=Jaitly%2C+N">Navdeep Jaitly</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12263" title="Abstract">arXiv:2312.12263</a> (replaced) [<a href="/pdf/2312.12263" title="Download PDF">pdf</a>, <a href="/format/2312.12263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FedDiv: Collaborative Noise Filtering for Federated Learning with Noisy  Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jichang Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+G">Guanbin Li</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hui Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Z">Zicheng Liao</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yizhou Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in AAAI-2024; correct formats to meet standards of the AAAI manuscript
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12713" title="Abstract">arXiv:2312.12713</a> (replaced) [<a href="/pdf/2312.12713" title="Download PDF">pdf</a>, <a href="/format/2312.12713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Response Enhanced Semi-supervised Dialogue Query Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jianheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">Ante Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+L">Linfeng Gao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linfeng Song</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jinsong Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI-24 main track paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12716" title="Abstract">arXiv:2312.12716</a> (replaced) [<a href="/pdf/2312.12716" title="Download PDF">pdf</a>, <a href="/format/2312.12716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BloomVQA: Assessing Hierarchical Multi-modal Comprehension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yunye Gong</a>, 
<a href="/search/cs?searchtype=author&query=Shrestha%2C+R">Robik Shrestha</a>, 
<a href="/search/cs?searchtype=author&query=Claypoole%2C+J">Jared Claypoole</a>, 
<a href="/search/cs?searchtype=author&query=Cogswell%2C+M">Michael Cogswell</a>, 
<a href="/search/cs?searchtype=author&query=Ray%2C+A">Arijit Ray</a>, 
<a href="/search/cs?searchtype=author&query=Kanan%2C+C">Christopher Kanan</a>, 
<a href="/search/cs?searchtype=author&query=Divakaran%2C+A">Ajay Divakaran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Dataset available at <a href="https://huggingface.co/datasets/ygong/BloomVQA">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.13839" title="Abstract">arXiv:2312.13839</a> (replaced) [<a href="/pdf/2312.13839" title="Download PDF">pdf</a>, <a href="/format/2312.13839" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Q-SENN: Quantized Self-Explaining Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Norrenbrock%2C+T">Thomas Norrenbrock</a>, 
<a href="/search/cs?searchtype=author&query=Rudolph%2C+M">Marco Rudolph</a>, 
<a href="/search/cs?searchtype=author&query=Rosenhahn%2C+B">Bodo Rosenhahn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AAAI 2024, SRRAI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.14886" title="Abstract">arXiv:2312.14886</a> (replaced) [<a href="/pdf/2312.14886" title="Download PDF">pdf</a>, <a href="/format/2312.14886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sample Path Regularity of Gaussian Processes from the Covariance Kernel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Da+Costa%2C+N">Natha&#xeb;l Da Costa</a>, 
<a href="/search/cs?searchtype=author&query=Pf%C3%B6rtner%2C+M">Marvin Pf&#xf6;rtner</a>, 
<a href="/search/cs?searchtype=author&query=Da+Costa%2C+L">Lancelot Da Costa</a>, 
<a href="/search/cs?searchtype=author&query=Hennig%2C+P">Philipp Hennig</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Probability (math.PR); Statistics Theory (math.ST); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.16132" title="Abstract">arXiv:2312.16132</a> (replaced) [<a href="/pdf/2312.16132" title="Download PDF">pdf</a>, <a href="/format/2312.16132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RoleEval: A Bilingual Role Evaluation Benchmark for Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+T">Tianhao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sun Li</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Q">Quan Tu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+D">Deyi Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Our dataset is available at <a href="https://github.com/Magnetic2014/RoleEval">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.00025" title="Abstract">arXiv:2401.00025</a> (replaced) [<a href="/pdf/2401.00025" title="Download PDF">pdf</a>, <a href="/format/2401.00025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Any-point Trajectory Modeling for Policy Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+C">Chuan Wen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xingyu Lin</a>, 
<a href="/search/cs?searchtype=author&query=So%2C+J">John So</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Q">Qi Dou</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Abbeel%2C+P">Pieter Abbeel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.00685" title="Abstract">arXiv:2401.00685</a> (replaced) [<a href="/pdf/2401.00685" title="Download PDF">pdf</a>, <a href="/format/2401.00685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Communication-Efficient Federated Learning for LEO Satellite Networks  Integrated with HAPs Using Hybrid NOMA-OFDM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elmahallawy%2C+M">Mohamed Elmahallawy</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+T">Tie Luo</a>, 
<a href="/search/cs?searchtype=author&query=Ramadan%2C+K">Khaled Ramadan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.02897" title="Abstract">arXiv:2401.02897</a> (replaced) [<a href="/pdf/2401.02897" title="Download PDF">pdf</a>, <a href="/format/2401.02897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Bichromatic Classification using Two Lines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Glazenburg%2C+E">Erwin Glazenburg</a>, 
<a href="/search/cs?searchtype=author&query=van+der+Horst%2C+T">Thijs van der Horst</a>, 
<a href="/search/cs?searchtype=author&query=Peters%2C+T">Tom Peters</a>, 
<a href="/search/cs?searchtype=author&query=Speckmann%2C+B">Bettina Speckmann</a>, 
<a href="/search/cs?searchtype=author&query=Staals%2C+F">Frank Staals</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 11 figures. Updated to include new results
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04854" title="Abstract">arXiv:2401.04854</a> (replaced) [<a href="/pdf/2401.04854" title="Download PDF">pdf</a>, <a href="/ps/2401.04854" title="Download PostScript">ps</a>, <a href="/format/2401.04854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are Language Models More Like Libraries or Like Librarians?  Bibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lederman%2C+H">Harvey Lederman</a>, 
<a href="/search/cs?searchtype=author&query=Mahowald%2C+K">Kyle Mahowald</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.04883" title="Abstract">arXiv:2401.04883</a> (replaced) [<a href="/pdf/2401.04883" title="Download PDF">pdf</a>, <a href="/format/2401.04883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-User Chat Assistant (MUCA): a Framework Using LLMs to Facilitate  Group Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mao%2C+M">Manqing Mao</a>, 
<a href="/search/cs?searchtype=author&query=Ting%2C+P">Paishun Ting</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Y">Yijian Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Mingyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Julia Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jianzhe Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05146" title="Abstract">arXiv:2401.05146</a> (replaced) [<a href="/pdf/2401.05146" title="Download PDF">pdf</a>, <a href="/format/2401.05146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Unlearning: A Survey on Methods, Design Guidelines, and  Evaluation Metrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Romandini%2C+N">Nicol&#xf2; Romandini</a>, 
<a href="/search/cs?searchtype=author&query=Mora%2C+A">Alessio Mora</a>, 
<a href="/search/cs?searchtype=author&query=Mazzocca%2C+C">Carlo Mazzocca</a>, 
<a href="/search/cs?searchtype=author&query=Montanari%2C+R">Rebecca Montanari</a>, 
<a href="/search/cs?searchtype=author&query=Bellavista%2C+P">Paolo Bellavista</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 8 figures, and 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05268" title="Abstract">arXiv:2401.05268</a> (replaced) [<a href="/pdf/2401.05268" title="Download PDF">pdf</a>, <a href="/format/2401.05268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AUTOACT: Automatic Agent Learning from Scratch via Self-Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiao%2C+S">Shuofei Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+R">Runnan Fang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yujie Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wangchunshu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y+E">Yuchen Eleanor Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+C">Chengfei Lv</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05949" title="Abstract">arXiv:2401.05949</a> (replaced) [<a href="/pdf/2401.05949" title="Download PDF">pdf</a>, <a href="/format/2401.05949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Vulnerabilities in Large Language Models: Backdoor Attacks for  In-context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shuai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+M">Meihuizi Jia</a>, 
<a href="/search/cs?searchtype=author&query=Tuan%2C+L+A">Luu Anh Tuan</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+F">Fengjun Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Jinming Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06469" title="Abstract">arXiv:2401.06469</a> (replaced) [<a href="/pdf/2401.06469" title="Download PDF">pdf</a>, <a href="/format/2401.06469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaiyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+A">Ang Lv</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuhan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ha%2C+H">Hansen Ha</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+R">Rui Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07324" title="Abstract">arXiv:2401.07324</a> (replaced) [<a href="/pdf/2401.07324" title="Download PDF">pdf</a>, <a href="/format/2401.07324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Small LLMs Are Weak Tool Learners: A Multi-LLM Agent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+W">Weizhou Shen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenliang Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongzhan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Ming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+X">Xiaojun Quan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hehong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Ji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> On progress, github repo: <a href="https://github.com/X-PLUG/Multi-LLM-Agent">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08190" title="Abstract">arXiv:2401.08190</a> (replaced) [<a href="/pdf/2401.08190" title="Download PDF">pdf</a>, <a href="/format/2401.08190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible  Pipeline
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+M">Minpeng Liao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+W">Wei Luo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengxi Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+K">Kai Fan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08295" title="Abstract">arXiv:2401.08295</a> (replaced) [<a href="/pdf/2401.08295" title="Download PDF">pdf</a>, <a href="/format/2401.08295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAPT: A Shared Attention Framework for Parameter-Efficient Continual  Learning of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Weixiang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shilong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yulin Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yanyan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+B">Bing Qin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuanyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qing Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+D">Dongliang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Che%2C+W">Wanxiang Che</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08893" title="Abstract">arXiv:2401.08893</a> (replaced) [<a href="/pdf/2401.08893" title="Download PDF">pdf</a>, <a href="/format/2401.08893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MADA: Meta-Adaptive Optimizers through hyper-gradient Descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ozkara%2C+K">Kaan Ozkara</a>, 
<a href="/search/cs?searchtype=author&query=Karakus%2C+C">Can Karakus</a>, 
<a href="/search/cs?searchtype=author&query=Raman%2C+P">Parameswaran Raman</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+M">Mingyi Hong</a>, 
<a href="/search/cs?searchtype=author&query=Sabach%2C+S">Shoham Sabach</a>, 
<a href="/search/cs?searchtype=author&query=Kveton%2C+B">Branislav Kveton</a>, 
<a href="/search/cs?searchtype=author&query=Cevher%2C+V">Volkan Cevher</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08903" title="Abstract">arXiv:2401.08903</a> (replaced) [<a href="/pdf/2401.08903" title="Download PDF">pdf</a>, <a href="/format/2401.08903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PPR: Enhancing Dodging Attacks while Maintaining Impersonation Attacks  on Face Recognition Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Fengfan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+H">Heifei Ling</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+B">Bangjie Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Hui Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08961" title="Abstract">arXiv:2401.08961</a> (replaced) [<a href="/pdf/2401.08961" title="Download PDF">pdf</a>, <a href="/format/2401.08961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cascading Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yihan Du</a>, 
<a href="/search/cs?searchtype=author&query=Srikant%2C+R">R. Srikant</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10768" title="Abstract">arXiv:2401.10768</a> (replaced) [<a href="/pdf/2401.10768" title="Download PDF">pdf</a>, <a href="/format/2401.10768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Verification to Nip Hallucination in the Bud
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+F">Fanqi Wan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xinting Huang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+L">Leyang Cui</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+X">Xiaojun Quan</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+W">Wei Bi</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+S">Shuming Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11323" title="Abstract">arXiv:2401.11323</a> (replaced) [<a href="/pdf/2401.11323" title="Download PDF">pdf</a>, <a href="/format/2401.11323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying and Analyzing Task-Encoding Tokens in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yu Bai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heyan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Piano%2C+C+S">Cesare Spinoso-Di Piano</a>, 
<a href="/search/cs?searchtype=author&query=Rondeau%2C+M">Marc-Antoine Rondeau</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sanxing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Cheung%2C+J+C+K">Jackie Chi Kit Cheung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11771" title="Abstract">arXiv:2401.11771</a> (replaced) [<a href="/pdf/2401.11771" title="Download PDF">pdf</a>, <a href="/ps/2401.11771" title="Download PostScript">ps</a>, <a href="/format/2401.11771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empowering Communication: Speech Technology for Indian and Western  Accents through AI-powered Speech Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=R%2C+V">Vinotha R</a>, 
<a href="/search/eess?searchtype=author&query=D%2C+H">Hepsiba D</a>, 
<a href="/search/eess?searchtype=author&query=Anand%2C+L+D+V">L. D. Vijay Anand</a>, 
<a href="/search/eess?searchtype=author&query=Reji%2C+D+J">Deepak John Reji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12192" title="Abstract">arXiv:2401.12192</a> (replaced) [<a href="/pdf/2401.12192" title="Download PDF">pdf</a>, <a href="/format/2401.12192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text Embedding Inversion Security for Multilingual Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yiyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lent%2C+H">Heather Lent</a>, 
<a href="/search/cs?searchtype=author&query=Bjerva%2C+J">Johannes Bjerva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 17 Tables, 6 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12956" title="Abstract">arXiv:2401.12956</a> (replaced) [<a href="/pdf/2401.12956" title="Download PDF">pdf</a>, <a href="/format/2401.12956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Examining the Role of Peer Acknowledgements on Social Annotations:  Unraveling the Psychological Underpinnings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiaoshan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haolun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xue Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lajoie%2C+S">Susanne Lajoie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13200" title="Abstract">arXiv:2401.13200</a> (replaced) [<a href="/pdf/2401.13200" title="Download PDF">pdf</a>, <a href="/format/2401.13200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Topology-aware Embedding Memory for Continual Learning on Expanding  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xikun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+D">Dongjin Song</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yixin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13246" title="Abstract">arXiv:2401.13246</a> (replaced) [<a href="/pdf/2401.13246" title="Download PDF">pdf</a>, <a href="/format/2401.13246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SEER: Facilitating Structured Reasoning and Explanation via  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guoxin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+K">Kexin Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+F">Fuying Ye</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+Y">Yiming Qian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Ongoing Work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13956" title="Abstract">arXiv:2401.13956</a> (replaced) [<a href="/pdf/2401.13956" title="Download PDF">pdf</a>, <a href="/format/2401.13956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A New Image Quality Database for Multiple Industrial Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xuanchao Ma</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yanlin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongyan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+C">Chengxu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+K">Ke Gu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13964" title="Abstract">arXiv:2401.13964</a> (replaced) [<a href="/pdf/2401.13964" title="Download PDF">pdf</a>, <a href="/format/2401.13964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Extensible Framework for Open Heterogeneous Collaborative Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yifan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yue Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yiqi Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dequan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Siheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanfeng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR 2024. The code and data are open-sourced at <a href="https://github.com/yifanlu0227/HEAL">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14271" title="Abstract">arXiv:2401.14271</a> (replaced) [<a href="/pdf/2401.14271" title="Download PDF">pdf</a>, <a href="/format/2401.14271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Design of Input Condition Invariant Speech Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+W">Wangyou Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Jung%2C+J">Jee-weon Jung</a>, 
<a href="/search/eess?searchtype=author&query=Watanabe%2C+S">Shinji Watanabe</a>, 
<a href="/search/eess?searchtype=author&query=Qian%2C+Y">Yanmin Qian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICASSP 2024, 5 pages, 2 figures, 3 tables (corrected the results of no processing on CHiME-4 (Simu) in Table 2)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14624" title="Abstract">arXiv:2401.14624</a> (replaced) [<a href="/pdf/2401.14624" title="Download PDF">pdf</a>, <a href="/format/2401.14624" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Query of CC: Unearthing Large Scale Domain-Specific Knowledge from  Public Corpora
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fei%2C+Z">Zhaoye Fei</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Y">Yunfan Shao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Linyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Zhiyuan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15745" title="Abstract">arXiv:2401.15745</a> (replaced) [<a href="/pdf/2401.15745" title="Download PDF">pdf</a>, <a href="/format/2401.15745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The computation of approximate feedback Stackelberg equilibria in  multi-player nonlinear constrained dynamic games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+J">Jingqi Li</a>, 
<a href="/search/math?searchtype=author&query=Sojoudi%2C+S">Somayeh Sojoudi</a>, 
<a href="/search/math?searchtype=author&query=Tomlin%2C+C">Claire Tomlin</a>, 
<a href="/search/math?searchtype=author&query=Fridovich-Keil%2C+D">David Fridovich-Keil</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This manuscript is currently under review by SIAM Journal on Optimization
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16240" title="Abstract">arXiv:2401.16240</a> (replaced) [<a href="/pdf/2401.16240" title="Download PDF">pdf</a>, <a href="/format/2401.16240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Hierachical VAEs with LLMs for clinically meaningful timeline  summarisation in social media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jiayu Song</a>, 
<a href="/search/cs?searchtype=author&query=Chim%2C+J">Jenny Chim</a>, 
<a href="/search/cs?searchtype=author&query=Tsakalidis%2C+A">Adam Tsakalidis</a>, 
<a href="/search/cs?searchtype=author&query=Ive%2C+J">Julia Ive</a>, 
<a href="/search/cs?searchtype=author&query=Atzil-Slonim%2C+D">Dana Atzil-Slonim</a>, 
<a href="/search/cs?searchtype=author&query=Liakata%2C+M">Maria Liakata</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16292" title="Abstract">arXiv:2401.16292</a> (replaced) [<a href="/pdf/2401.16292" title="Download PDF">pdf</a>, <a href="/format/2401.16292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pilotfish: Distributed Transaction Execution for Lazy Blockchains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kniep%2C+Q">Quentin Kniep</a>, 
<a href="/search/cs?searchtype=author&query=Kokoris-Kogias%2C+L">Lefteris Kokoris-Kogias</a>, 
<a href="/search/cs?searchtype=author&query=Sonnino%2C+A">Alberto Sonnino</a>, 
<a href="/search/cs?searchtype=author&query=Zablotchi%2C+I">Igor Zablotchi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Nuda Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16461" title="Abstract">arXiv:2401.16461</a> (replaced) [<a href="/pdf/2401.16461" title="Download PDF">pdf</a>, <a href="/format/2401.16461" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Norm Enforcement with a Soft Touch: Faster Emergence, Happier Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tzeng%2C+S">Sz-Ting Tzeng</a>, 
<a href="/search/cs?searchtype=author&query=Ajmeri%2C+N">Nirav Ajmeri</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+M+P">Munindar P. Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 11 figures, 5 tables (and supplementary material with code availability and additional results), accepted at AAMAS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16659" title="Abstract">arXiv:2401.16659</a> (replaced) [<a href="/pdf/2401.16659" title="Download PDF">pdf</a>, <a href="/format/2401.16659" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> History-Aware Conversational Dense Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mo%2C+F">Fengran Mo</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+C">Chen Qu</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+K">Kelong Mao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Tianyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Z">Zhan Su</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaiyu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+J">Jian-Yun Nie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17167" title="Abstract">arXiv:2401.17167</a> (replaced) [<a href="/pdf/2401.17167" title="Download PDF">pdf</a>, <a href="/format/2401.17167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool  Utilization in Real-World Complex Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shijue Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+W">Wanjun Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jianqiao Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jiahui Gao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weiwen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+Y">Yutai Hou</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xingshan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yasheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+L">Lifeng Shang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruifeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qun Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17882" title="Abstract">arXiv:2401.17882</a> (replaced) [<a href="/pdf/2401.17882" title="Download PDF">pdf</a>, <a href="/format/2401.17882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> I Think, Therefore I am: Benchmarking Awareness of Large Language Models  Using AwareBench
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yue Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yuli Lin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Siyuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Y">Yao Wan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lichao Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00418" title="Abstract">arXiv:2402.00418</a> (replaced) [<a href="/pdf/2402.00418" title="Download PDF">pdf</a>, <a href="/ps/2402.00418" title="Download PostScript">ps</a>, <a href="/format/2402.00418" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking Transferable Adversarial Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhibo Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiayu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhiyu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huaming Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by NDSS 2024 Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00570" title="Abstract">arXiv:2402.00570</a> (replaced) [<a href="/pdf/2402.00570" title="Download PDF">pdf</a>, <a href="/format/2402.00570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CADICA: a new dataset for coronary artery disease detection by using  invasive coronary angiography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jim%C3%A9nez-Partinen%2C+A">Ariadna Jim&#xe9;nez-Partinen</a>, 
<a href="/search/eess?searchtype=author&query=Molina-Cabello%2C+M+A">Miguel A. Molina-Cabello</a>, 
<a href="/search/eess?searchtype=author&query=Thurnhofer-Hemsi%2C+K">Karl Thurnhofer-Hemsi</a>, 
<a href="/search/eess?searchtype=author&query=Palomo%2C+E+J">Esteban J. Palomo</a>, 
<a href="/search/eess?searchtype=author&query=Rodr%C3%ADguez-Capit%C3%A1n%2C+J">Jorge Rodr&#xed;guez-Capit&#xe1;n</a>, 
<a href="/search/eess?searchtype=author&query=Molina-Ramos%2C+A+I">Ana I. Molina-Ramos</a>, 
<a href="/search/eess?searchtype=author&query=Jim%C3%A9nez-Navarro%2C+M">Manuel Jim&#xe9;nez-Navarro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00593" title="Abstract">arXiv:2402.00593</a> (replaced) [<a href="/pdf/2402.00593" title="Download PDF">pdf</a>, <a href="/format/2402.00593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Coronary Artery Disease Classification with Different Lesion Degree  Ranges based on Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Jim%C3%A9nez-Partinen%2C+A">Ariadna Jim&#xe9;nez-Partinen</a>, 
<a href="/search/eess?searchtype=author&query=Thurnhofer-Hemsi%2C+K">Karl Thurnhofer-Hemsi</a>, 
<a href="/search/eess?searchtype=author&query=Palomo%2C+E+J">Esteban J. Palomo</a>, 
<a href="/search/eess?searchtype=author&query=Rodr%C3%ADguez-Capit%C3%A1n%2C+J">Jorge Rodr&#xed;guez-Capit&#xe1;n</a>, 
<a href="/search/eess?searchtype=author&query=Molina-Ramos%2C+A+I">Ana I. Molina-Ramos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00626" title="Abstract">arXiv:2402.00626</a> (replaced) [<a href="/pdf/2402.00626" title="Download PDF">pdf</a>, <a href="/format/2402.00626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qraitem%2C+M">Maan Qraitem</a>, 
<a href="/search/cs?searchtype=author&query=Tasnim%2C+N">Nazia Tasnim</a>, 
<a href="/search/cs?searchtype=author&query=Teterwak%2C+P">Piotr Teterwak</a>, 
<a href="/search/cs?searchtype=author&query=Saenko%2C+K">Kate Saenko</a>, 
<a href="/search/cs?searchtype=author&query=Plummer%2C+B+A">Bryan A. Plummer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01103" title="Abstract">arXiv:2402.01103</a> (replaced) [<a href="/pdf/2402.01103" title="Download PDF">pdf</a>, <a href="/format/2402.01103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compositional Generative Modeling: A Single Model is Not All You Need
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yilun Du</a>, 
<a href="/search/cs?searchtype=author&query=Kaelbling%2C+L">Leslie Kaelbling</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01679" title="Abstract">arXiv:2402.01679</a> (replaced) [<a href="/pdf/2402.01679" title="Download PDF">pdf</a>, <a href="/format/2402.01679" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STICKERCONV: Generating Multimodal Empathetic Responses from Scratch
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yiqun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+F">Fanheng Kong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peidong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Shuang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lingshuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shi Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Daling Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+K">Kaisong Song</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01681" title="Abstract">arXiv:2402.01681</a> (replaced) [<a href="/pdf/2402.01681" title="Download PDF">pdf</a>, <a href="/format/2402.01681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emojis Decoded: Leveraging ChatGPT for Enhanced Understanding in Social  Media Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuhang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Paiheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+X">Xuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+G">Ge Gao</a>, 
<a href="/search/cs?searchtype=author&query=Ai%2C+W">Wei Ai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 2 page appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03190" title="Abstract">arXiv:2402.03190</a> (replaced) [<a href="/pdf/2402.03190" title="Download PDF">pdf</a>, <a href="/format/2402.03190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified Hallucination Detection for Multimodal Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenxi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+Y">Yida Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaoyan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yue Shen</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+L">Lei Liang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jinjie Gu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03522" title="Abstract">arXiv:2402.03522</a> (replaced) [<a href="/pdf/2402.03522" title="Download PDF">pdf</a>, <a href="/format/2402.03522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Influencer Identification on Link Predicted Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schaposnik%2C+L+P">Laura P. Schaposnik</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Raina Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages + appendix. V2 has additional information on how our model differs from existing algorithms
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03699" title="Abstract">arXiv:2402.03699</a> (replaced) [<a href="/pdf/2402.03699" title="Download PDF">pdf</a>, <a href="/ps/2402.03699" title="Download PostScript">ps</a>, <a href="/format/2402.03699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Robotic Development through Collaborative Framework by Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luan%2C+Z">Zhirong Luan</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Y">Yujun Lai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Rundong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+X">Xiaruiqi Lan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liangjun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Badong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03703" title="Abstract">arXiv:2402.03703</a> (replaced) [<a href="/pdf/2402.03703" title="Download PDF">pdf</a>, <a href="/ps/2402.03703" title="Download PostScript">ps</a>, <a href="/format/2402.03703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Large Language Models in Cloud Edge End Architecture for  Heterogeneous Robot Cluster Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luan%2C+Z">Zhirong Luan</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Y">Yujun Lai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+R">Rundong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yan Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingwei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jizhou Lu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Badong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03746" title="Abstract">arXiv:2402.03746</a> (replaced) [<a href="/pdf/2402.03746" title="Download PDF">pdf</a>, <a href="/format/2402.03746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tuning Large Multimodal Models for Videos using Reinforcement Learning  from AI Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahn%2C+D">Daechul Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yura Choi</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Youngjae Yu</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dongyeop Kang</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jonghyun Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04139" title="Abstract">arXiv:2402.04139</a> (replaced) [<a href="/pdf/2402.04139" title="Download PDF">pdf</a>, <a href="/format/2402.04139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> U-shaped Vision Mamba for Single Image Dehazing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zhuoran Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chen Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04146" title="Abstract">arXiv:2402.04146</a> (replaced) [<a href="/pdf/2402.04146" title="Download PDF">pdf</a>, <a href="/format/2402.04146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Multi-Source Data Fusion Through Latent Variable Gaussian  Process
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ravi%2C+S+K">Sandipp Krishnan Ravi</a>, 
<a href="/search/stat?searchtype=author&query=Comlek%2C+Y">Yigitcan Comlek</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/stat?searchtype=author&query=Pathak%2C+A">Arjun Pathak</a>, 
<a href="/search/stat?searchtype=author&query=Gupta%2C+V">Vipul Gupta</a>, 
<a href="/search/stat?searchtype=author&query=Umretiya%2C+R">Rajnikant Umretiya</a>, 
<a href="/search/stat?searchtype=author&query=Hoffman%2C+A">Andrew Hoffman</a>, 
<a href="/search/stat?searchtype=author&query=Pilania%2C+G">Ghanshyam Pilania</a>, 
<a href="/search/stat?searchtype=author&query=Pandita%2C+P">Piyush Pandita</a>, 
<a href="/search/stat?searchtype=author&query=Ghosh%2C+S">Sayan Ghosh</a>, 
<a href="/search/stat?searchtype=author&query=Mckeever%2C+N">Nathaniel Mckeever</a>, 
<a href="/search/stat?searchtype=author&query=Wang%2C+L">Liping Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 Pages,9 Figures, 3 Supplementary Figures, 2 Supplementary Tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04298" title="Abstract">arXiv:2402.04298</a> (replaced) [<a href="/pdf/2402.04298" title="Download PDF">pdf</a>, <a href="/format/2402.04298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-View Symbolic Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Russeil%2C+E">Etienne Russeil</a>, 
<a href="/search/cs?searchtype=author&query=de+Fran%C3%A7a%2C+F+O">Fabr&#xed;cio Olivetti de Fran&#xe7;a</a>, 
<a href="/search/cs?searchtype=author&query=Malanchev%2C+K">Konstantin Malanchev</a>, 
<a href="/search/cs?searchtype=author&query=Burlacu%2C+B">Bogdan Burlacu</a>, 
<a href="/search/cs?searchtype=author&query=Ishida%2C+E+E+O">Emille E. O. Ishida</a>, 
<a href="/search/cs?searchtype=author&query=Leroux%2C+M">Marion Leroux</a>, 
<a href="/search/cs?searchtype=author&query=Michelin%2C+C">Cl&#xe9;ment Michelin</a>, 
<a href="/search/cs?searchtype=author&query=Moinard%2C+G">Guillaume Moinard</a>, 
<a href="/search/cs?searchtype=author&query=Gangler%2C+E">Emmanuel Gangler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to GECCO-2024. 10 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04534" title="Abstract">arXiv:2402.04534</a> (replaced) [<a href="/pdf/2402.04534" title="Download PDF">pdf</a>, <a href="/format/2402.04534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> M2fNet: Multi-modal Forest Monitoring Network on Large-scale Virtual  Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yawen Lu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yunhan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Su Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tansi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xuewen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+S">Songlin Fei</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yingjie Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04546" title="Abstract">arXiv:2402.04546</a> (replaced) [<a href="/pdf/2402.04546" title="Download PDF">pdf</a>, <a href="/format/2402.04546" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LiDAR-Forest Dataset: LiDAR Point Cloud Simulation Dataset for Forestry  Application
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yawen Lu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhuoyang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+J">Jinyuan Shao</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qianyu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yunhan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+S">Songlin Fei</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yingjie Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05273" title="Abstract">arXiv:2402.05273</a> (replaced) [<a href="/pdf/2402.05273" title="Download PDF">pdf</a>, <a href="/format/2402.05273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ASCENT: A Context-Aware Spectrum Coexistence Design and Implementation  Toolset for Policymakers in Satellite Bands
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Niloy%2C+T+R">Ta-seen Reaz Niloy</a>, 
<a href="/search/eess?searchtype=author&query=Kumar%2C+S">Saurav Kumar</a>, 
<a href="/search/eess?searchtype=author&query=Hore%2C+A">Aniruddha Hore</a>, 
<a href="/search/eess?searchtype=author&query=Hassan%2C+Z">Zoheb Hassan</a>, 
<a href="/search/eess?searchtype=author&query=Dietrich%2C+C">Carl Dietrich</a>, 
<a href="/search/eess?searchtype=author&query=Burger%2C+E+W">Eric W. Burger</a>, 
<a href="/search/eess?searchtype=author&query=Reed%2C+J+H">Jeffrey H. Reed</a>, 
<a href="/search/eess?searchtype=author&query=Shah%2C+V+K">Vijay K. Shah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05973" title="Abstract">arXiv:2402.05973</a> (replaced) [<a href="/pdf/2402.05973" title="Download PDF">pdf</a>, <a href="/format/2402.05973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blockchain-enabled Clustered and Scalable Federated Learning (BCS-FL)  Framework in UAV Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hafeez%2C+S">Sana Hafeez</a>, 
<a href="/search/cs?searchtype=author&query=Mohjazi%2C+L">Lina Mohjazi</a>, 
<a href="/search/cs?searchtype=author&query=Imran%2C+M+A">Muhammad Ali Imran</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yao Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 7 figures, 2023 IEEE International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (IEEE CAMAD), Edinburgh UK
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Networking and Internet Architecture (cs.NI); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06664" title="Abstract">arXiv:2402.06664</a> (replaced) [<a href="/pdf/2402.06664" title="Download PDF">pdf</a>, <a href="/format/2402.06664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM Agents can Autonomously Hack Websites
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+R">Richard Fang</a>, 
<a href="/search/cs?searchtype=author&query=Bindu%2C+R">Rohan Bindu</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+A">Akul Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+Q">Qiusi Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Daniel Kang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06733" title="Abstract">arXiv:2402.06733</a> (replaced) [<a href="/pdf/2402.06733" title="Download PDF">pdf</a>, <a href="/format/2402.06733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NICE: To Optimize In-Context Examples or Not?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+P">Pragya Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Golechha%2C+S">Satvik Golechha</a>, 
<a href="/search/cs?searchtype=author&query=Deshpande%2C+A">Amit Deshpande</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Amit Sharma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06764" title="Abstract">arXiv:2402.06764</a> (replaced) [<a href="/pdf/2402.06764" title="Download PDF">pdf</a>, <a href="/format/2402.06764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph  Alignment via Neighborhood Partitioning and Generative Subgraph Encoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dernbach%2C+S">Stefan Dernbach</a>, 
<a href="/search/cs?searchtype=author&query=Agarwal%2C+K">Khushbu Agarwal</a>, 
<a href="/search/cs?searchtype=author&query=Zuniga%2C+A">Alejandro Zuniga</a>, 
<a href="/search/cs?searchtype=author&query=Henry%2C+M">Michael Henry</a>, 
<a href="/search/cs?searchtype=author&query=Choudhury%2C+S">Sutanay Choudhury</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in AAAI Spring Symposium: AAAI-MAKE 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06782" title="Abstract">arXiv:2402.06782</a> (replaced) [<a href="/pdf/2402.06782" title="Download PDF">pdf</a>, <a href="/format/2402.06782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Debating with More Persuasive LLMs Leads to More Truthful Answers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khan%2C+A">Akbir Khan</a>, 
<a href="/search/cs?searchtype=author&query=Hughes%2C+J">John Hughes</a>, 
<a href="/search/cs?searchtype=author&query=Valentine%2C+D">Dan Valentine</a>, 
<a href="/search/cs?searchtype=author&query=Ruis%2C+L">Laura Ruis</a>, 
<a href="/search/cs?searchtype=author&query=Sachan%2C+K">Kshitij Sachan</a>, 
<a href="/search/cs?searchtype=author&query=Radhakrishnan%2C+A">Ansh Radhakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Grefenstette%2C+E">Edward Grefenstette</a>, 
<a href="/search/cs?searchtype=author&query=Bowman%2C+S+R">Samuel R. Bowman</a>, 
<a href="/search/cs?searchtype=author&query=Rockt%C3%A4schel%2C+T">Tim Rockt&#xe4;schel</a>, 
<a href="/search/cs?searchtype=author&query=Perez%2C+E">Ethan Perez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> For code please check: <a href="https://github.com/ucl-dark/llm_debate">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06900" title="Abstract">arXiv:2402.06900</a> (replaced) [<a href="/pdf/2402.06900" title="Download PDF">pdf</a>, <a href="/format/2402.06900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can LLMs Recognize Toxicity? Structured Toxicity Investigation Framework  and Semantic-Based Metric
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koh%2C+H">Hyukhun Koh</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D">Dohyung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Minwoo Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+K">Kyomin Jung</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 page long
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07214" title="Abstract">arXiv:2402.07214</a> (replaced) [<a href="/pdf/2402.07214" title="Download PDF">pdf</a>, <a href="/format/2402.07214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Through the Lens of Split Vote: Exploring Disagreement, Difficulty and  Calibration in Legal Case Outcome Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shanshan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Santosh%2C+T+Y+S+S">T.Y.S.S Santosh</a>, 
<a href="/search/cs?searchtype=author&query=Ichim%2C+O">Oana Ichim</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>, 
<a href="/search/cs?searchtype=author&query=Grabmair%2C+M">Matthias Grabmair</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07338" title="Abstract">arXiv:2402.07338</a> (replaced) [<a href="/pdf/2402.07338" title="Download PDF">pdf</a>, <a href="/format/2402.07338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Saliency Bias in Manipulation Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krinsky%2C+J">Joshua Krinsky</a>, 
<a href="/search/cs?searchtype=author&query=Bettis%2C+A">Alan Bettis</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Q">Qiuyu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Moreira%2C+D">Daniel Moreira</a>, 
<a href="/search/cs?searchtype=author&query=Bharati%2C+A">Aparna Bharati</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07470" title="Abstract">arXiv:2402.07470</a> (replaced) [<a href="/pdf/2402.07470" title="Download PDF">pdf</a>, <a href="/format/2402.07470" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pushing The Limit of LLM Capacity for Text Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yazhou Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengyao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+C">Chenyu Ren</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qiuchi Li</a>, 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+P">Prayag Tiwari</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Benyou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+J">Jing Qin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07616" title="Abstract">arXiv:2402.07616</a> (replaced) [<a href="/pdf/2402.07616" title="Download PDF">pdf</a>, <a href="/format/2402.07616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anchor-based Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+J">Jianhui Pang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+F">Fanghua Ye</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+D+F">Derek F. Wong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Longyue Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages. Work was done when Jianhui Pang and Fanghua Ye were interning at Tencent AI Lab. Longyue Wang is the corresponding author
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07637" title="Abstract">arXiv:2402.07637</a> (replaced) [<a href="/pdf/2402.07637" title="Download PDF">pdf</a>, <a href="/format/2402.07637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compressive Recovery of Signals Defined on Perturbed Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ghosh%2C+S">Sabyasachi Ghosh</a>, 
<a href="/search/eess?searchtype=author&query=Rajwade%2C+A">Ajit Rajwade</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 15 figures. v2: Minor correction in ref [32]
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08156" title="Abstract">arXiv:2402.08156</a> (replaced) [<a href="/pdf/2402.08156" title="Download PDF">pdf</a>, <a href="/ps/2402.08156" title="Download PostScript">ps</a>, <a href="/format/2402.08156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Group Decision-Making among Privacy-Aware Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papachristou%2C+M">Marios Papachristou</a>, 
<a href="/search/cs?searchtype=author&query=Rahimian%2C+M+A">M. Amin Rahimian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Multiagent Systems (cs.MA); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08225" title="Abstract">arXiv:2402.08225</a> (replaced) [<a href="/pdf/2402.08225" title="Download PDF">pdf</a>, <a href="/format/2402.08225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving Black-box Robustness with In-Context Rewriting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=O%27Brien%2C+K">Kyle O&#x27;Brien</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+N">Nathan Ng</a>, 
<a href="/search/cs?searchtype=author&query=Puri%2C+I">Isha Puri</a>, 
<a href="/search/cs?searchtype=author&query=Mendez%2C+J">Jorge Mendez</a>, 
<a href="/search/cs?searchtype=author&query=Palangi%2C+H">Hamid Palangi</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Yoon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ghassemi%2C+M">Marzyeh Ghassemi</a>, 
<a href="/search/cs?searchtype=author&query=Hartvigsen%2C+T">Thomas Hartvigsen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08277" title="Abstract">arXiv:2402.08277</a> (replaced) [<a href="/pdf/2402.08277" title="Download PDF">pdf</a>, <a href="/format/2402.08277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Faithful and Robust LLM Specialists for Evidence-Based  Question-Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schimanski%2C+T">Tobias Schimanski</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+J">Jingwei Ni</a>, 
<a href="/search/cs?searchtype=author&query=Kraus%2C+M">Mathias Kraus</a>, 
<a href="/search/cs?searchtype=author&query=Ash%2C+E">Elliott Ash</a>, 
<a href="/search/cs?searchtype=author&query=Leippold%2C+M">Markus Leippold</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08479" title="Abstract">arXiv:2402.08479</a> (replaced) [<a href="/pdf/2402.08479" title="Download PDF">pdf</a>, <a href="/format/2402.08479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Plausible Extractive Rationalization through Semi-Supervised Entailment  Signal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jie%2C+Y+W">Yeo Wei Jie</a>, 
<a href="/search/cs?searchtype=author&query=Satapathy%2C+R">Ranjan Satapathy</a>, 
<a href="/search/cs?searchtype=author&query=Cambria%2C+E">Erik Cambria</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08712" title="Abstract">arXiv:2402.08712</a> (replaced) [<a href="/pdf/2402.08712" title="Download PDF">pdf</a>, <a href="/format/2402.08712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BECoTTA: Input-dependent Online Blending of Experts for Continual  Test-time Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Daeun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+J">Jaehong Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+S+J">Sung Ju Hwang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, Preprint, Project page: <a href="https://becotta-ctta.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08787" title="Abstract">arXiv:2402.08787</a> (replaced) [<a href="/pdf/2402.08787" title="Download PDF">pdf</a>, <a href="/format/2402.08787" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Machine Unlearning for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sijia Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuanshun Yao</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jinghan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Casper%2C+S">Stephen Casper</a>, 
<a href="/search/cs?searchtype=author&query=Baracaldo%2C+N">Nathalie Baracaldo</a>, 
<a href="/search/cs?searchtype=author&query=Hase%2C+P">Peter Hase</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaojun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuguang Yao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hang Li</a>, 
<a href="/search/cs?searchtype=author&query=Varshney%2C+K+R">Kush R. Varshney</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Koyejo%2C+S">Sanmi Koyejo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08812" title="Abstract">arXiv:2402.08812</a> (replaced) [<a href="/pdf/2402.08812" title="Download PDF">pdf</a>, <a href="/format/2402.08812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Intelligent Canvas: Enabling Design-Like Exploratory Visual Data  Analysis with Generative AI through Rapid Prototyping, Iteration and Curation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+Z">Zijian Ding</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+J">Joel Chan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09091" title="Abstract">arXiv:2402.09091</a> (replaced) [<a href="/pdf/2402.09091" title="Download PDF">pdf</a>, <a href="/format/2402.09091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit  Clues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chang%2C+Z">Zhiyuan Chang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Mingyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09182" title="Abstract">arXiv:2402.09182</a> (replaced) [<a href="/pdf/2402.09182" title="Download PDF">pdf</a>, <a href="/format/2402.09182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design Space of Visual Feedforward And Corrective Feedback in XR-Based  Motion Guidance Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xingyao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+B">Benjamin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Sedlmair%2C+M">Michael Sedlmair</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in ACM CHI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09262" title="Abstract">arXiv:2402.09262</a> (replaced) [<a href="/pdf/2402.09262" title="Download PDF">pdf</a>, <a href="/format/2402.09262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical  Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Royer%2C+C">Corentin Royer</a>, 
<a href="/search/cs?searchtype=author&query=Menze%2C+B">Bjoern Menze</a>, 
<a href="/search/cs?searchtype=author&query=Sekuboyina%2C+A">Anjany Sekuboyina</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review at MIDL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09336" title="Abstract">arXiv:2402.09336</a> (replaced) [<a href="/pdf/2402.09336" title="Download PDF">pdf</a>, <a href="/ps/2402.09336" title="Download PostScript">ps</a>, <a href="/format/2402.09336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Modern Approach to Electoral Delimitation using the Quadtree Data  Structure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kale%2C+S">Sahil Kale</a>, 
<a href="/search/cs?searchtype=author&query=Khaire%2C+G">Gautam Khaire</a>, 
<a href="/search/cs?searchtype=author&query=Patankar%2C+J">Jay Patankar</a>, 
<a href="/search/cs?searchtype=author&query=Vidap%2C+P">Pujashree Vidap</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures, Accepted in 1st International Conference on Cognitive Computing and Engineering Education (ICCCEE), Pune, India, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09345" title="Abstract">arXiv:2402.09345</a> (replaced) [<a href="/pdf/2402.09345" title="Download PDF">pdf</a>, <a href="/format/2402.09345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating Reward Hacking via Information-Theoretic Reward Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miao%2C+Y">Yuchun Miao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+R">Rong Bao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lefei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 28 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09346" title="Abstract">arXiv:2402.09346</a> (replaced) [<a href="/pdf/2402.09346" title="Download PDF">pdf</a>, <a href="/format/2402.09346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Developing a Framework for Auditing Large Language Models Using  Human-in-the-Loop
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amirizaniani%2C+M">Maryam Amirizaniani</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jihan Yao</a>, 
<a href="/search/cs?searchtype=author&query=Lavergne%2C+A">Adrian Lavergne</a>, 
<a href="/search/cs?searchtype=author&query=Okada%2C+E+S">Elizabeth Snell Okada</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>, 
<a href="/search/cs?searchtype=author&query=Roosta%2C+T">Tanya Roosta</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+C">Chirag Shah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09410" title="Abstract">arXiv:2402.09410</a> (replaced) [<a href="/pdf/2402.09410" title="Download PDF">pdf</a>, <a href="/format/2402.09410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Computability of Computable Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khaliq%2C+A">Asad Khaliq</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 Pages,5 figures, and 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09492" title="Abstract">arXiv:2402.09492</a> (replaced) [<a href="/pdf/2402.09492" title="Download PDF">pdf</a>, <a href="/format/2402.09492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PMGDA: A Preference-based Multiple Gradient Descent Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qingfu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09604" title="Abstract">arXiv:2402.09604</a> (replaced) [<a href="/pdf/2402.09604" title="Download PDF">pdf</a>, <a href="/format/2402.09604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Medical Image Segmentation with InTEnt: Integrated Entropy Weighting for  Single Image Test-Time Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+H">Haoyu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Konz%2C+N">Nicholas Konz</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+H">Hanxue Gu</a>, 
<a href="/search/cs?searchtype=author&query=Mazurowski%2C+M+A">Maciej A. Mazurowski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and pre-trained weights: <a href="https://github.com/mazurowski-lab/single-image-test-time-adaptation">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09615" title="Abstract">arXiv:2402.09615</a> (replaced) [<a href="/pdf/2402.09615" title="Download PDF">pdf</a>, <a href="/format/2402.09615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> API Pack: A Massive Multilingual Dataset for API Call Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhen Guo</a>, 
<a href="/search/cs?searchtype=author&query=Soria%2C+A+M">Adriana Meza Soria</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yikang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Panda%2C+R">Rameswar Panda</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09631" title="Abstract">arXiv:2402.09631</a> (replaced) [<a href="/pdf/2402.09631" title="Download PDF">pdf</a>, <a href="/format/2402.09631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MiMiC: Minimally Modified Counterfactuals in the Representation Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Shashwat Singh</a>, 
<a href="/search/cs?searchtype=author&query=Ravfogel%2C+S">Shauli Ravfogel</a>, 
<a href="/search/cs?searchtype=author&query=Herzig%2C+J">Jonathan Herzig</a>, 
<a href="/search/cs?searchtype=author&query=Aharoni%2C+R">Roee Aharoni</a>, 
<a href="/search/cs?searchtype=author&query=Cotterell%2C+R">Ryan Cotterell</a>, 
<a href="/search/cs?searchtype=author&query=Kumaraguru%2C+P">Ponnurangam Kumaraguru</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09664" title="Abstract">arXiv:2402.09664</a> (replaced) [<a href="/pdf/2402.09664" title="Download PDF">pdf</a>, <a href="/format/2402.09664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodeMind: A Framework to Challenge Large Language Models for Code  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Changshu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S+D">Shizhuo Dylan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jabbarvand%2C+R">Reyhaneh Jabbarvand</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09696" title="Abstract">arXiv:2402.09696</a> (replaced) [<a href="/pdf/2402.09696" title="Download PDF">pdf</a>, <a href="/format/2402.09696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Analysis of Language Frequency and Error Correction for Esperanto
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Junhong Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09786" title="Abstract">arXiv:2402.09786</a> (replaced) [<a href="/pdf/2402.09786" title="Download PDF">pdf</a>, <a href="/format/2402.09786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Examining Pathological Bias in a Generative Adversarial Network  Discriminator: A Case Study on a StyleGAN3 Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grissom%2C+A">Alvin Grissom II</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+R+F">Ryan F. Lei</a>, 
<a href="/search/cs?searchtype=author&query=Neto%2C+J+F+S+R">Jeova Farias Sales Rocha Neto</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Bailey Lin</a>, 
<a href="/search/cs?searchtype=author&query=Trotter%2C+R">Ryan Trotter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09805" title="Abstract">arXiv:2402.09805</a> (replaced) [<a href="/pdf/2402.09805" title="Download PDF">pdf</a>, <a href="/ps/2402.09805" title="Download PostScript">ps</a>, <a href="/format/2402.09805" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling Edge processing on LoRaWAN architecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Milani%2C+S">Stefano Milani</a>, 
<a href="/search/cs?searchtype=author&query=Chatzigiannakis%2C+I">Ioannis Chatzigiannakis</a>, 
<a href="/search/cs?searchtype=author&query=Garlisi%2C+D">Domenico Garlisi</a>, 
<a href="/search/cs?searchtype=author&query=Di+Fraia%2C+M">Matteo Di Fraia</a>, 
<a href="/search/cs?searchtype=author&query=Pisani%2C+P">Patrizio Pisani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is a pre-print version of the conference paper that has been accepted to ACM MobiCom 2023: Proceedings of the 29th Annual International Conference on Mobile Computing and Networking
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10009" title="Abstract">arXiv:2402.10009</a> (replaced) [<a href="/pdf/2402.10009" title="Download PDF">pdf</a>, <a href="/format/2402.10009" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manor%2C+H">Hila Manor</a>, 
<a href="/search/cs?searchtype=author&query=Michaeli%2C+T">Tomer Michaeli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Examples and code available in <a href="https://hilamanor.github.io/AudioEditing/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10059" title="Abstract">arXiv:2402.10059</a> (replaced) [<a href="/pdf/2402.10059" title="Download PDF">pdf</a>, <a href="/format/2402.10059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Partial synchrony for free? New bounds for Byzantine agreement via a  generic transformation across network models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Civit%2C+P">Pierre Civit</a>, 
<a href="/search/cs?searchtype=author&query=Dzulfikar%2C+M+A">Muhammad Ayaz Dzulfikar</a>, 
<a href="/search/cs?searchtype=author&query=Gilbert%2C+S">Seth Gilbert</a>, 
<a href="/search/cs?searchtype=author&query=Guerraoui%2C+R">Rachid Guerraoui</a>, 
<a href="/search/cs?searchtype=author&query=Komatovic%2C+J">Jovan Komatovic</a>, 
<a href="/search/cs?searchtype=author&query=Vidigueira%2C+M">Manuel Vidigueira</a>, 
<a href="/search/cs?searchtype=author&query=Zablotchi%2C+I">Igor Zablotchi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10118" title="Abstract">arXiv:2402.10118</a> (replaced) [<a href="/pdf/2402.10118" title="Download PDF">pdf</a>, <a href="/format/2402.10118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reusing Softmax Hardware Unit for GELU Computation in Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peltekis%2C+C">Christodoulos Peltekis</a>, 
<a href="/search/cs?searchtype=author&query=Alexandridis%2C+K">Kosmas Alexandridis</a>, 
<a href="/search/cs?searchtype=author&query=Dimitrakopoulos%2C+G">Giorgos Dimitrakopoulos</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AICAS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10137" title="Abstract">arXiv:2402.10137</a> (replaced) [<a href="/pdf/2402.10137" title="Download PDF">pdf</a>, <a href="/format/2402.10137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TOAD: Task-Oriented Automatic Dialogs with Diverse Response Styles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yinhong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yimai Fang</a>, 
<a href="/search/cs?searchtype=author&query=Vandyke%2C+D">David Vandyke</a>, 
<a href="/search/cs?searchtype=author&query=Collier%2C+N">Nigel Collier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item307">Cross-lists</a></li>
<li><a href="#item353">Replacements</a></li>
</ul>
<small>[ total of 604 entries:  <b>1-604</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2402">2402</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
