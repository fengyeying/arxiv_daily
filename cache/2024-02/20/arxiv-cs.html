<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
<head>
<title>Computer Science  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215" />
<link rel="stylesheet" type="text/css" media="screen" href="/bibex/bibex.css?20181009">
<link rel="stylesheet" type="text/css" media="screen" href="https://static.arxiv.org/static/browse/0.3.8/css/browse_search.css" />
<link rel="alternate" type="application/rss+xml" title="Computer Science " href="http://arxiv.org/rss/cs"/>
<script src="/js/mathjaxToggle.min.js" type="text/javascript"></script>


</head>
<body class="with-cu-identity">

<div id="cu-identity">
<div id="cu-logo">
<a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1 class="header-breadcrumbs"><a href="/"><img src="//static.arxiv.org/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;margin-right:8px;"></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a></h1>
<div class="search-block level-right">
<form class="level-item mini-search" method="GET" action="https://arxiv.org/search" _lpchecked="1">
<div class="field has-addons">
<div class="control">
<input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" data-com.agilebits.onepassword.user-edited="yes">
<p class="help"><a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
</div>
<div class="control">
<div class="select is-small">
<select name="searchtype" aria-label="Field to search">
<option value="all" selected="selected">All fields</option>
<option value="title">Title</option>
<option value="author">Author</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option>
</select>
</div>
</div>
<button class="button is-small is-cul-darker">Search</button>
</div>
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Computer Science </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Fri 16 Feb 24  to  Mon 19 Feb 24, announced Tue, 20 Feb 24</div>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item750">Cross-lists</a></li>
<li><a href="#item821">Replacements</a></li>
</ul>
<small>[ total of 1306 entries:  <b>1-1306</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Tue, 20 Feb 24</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10899" title="Abstract">arXiv:2402.10899</a> [<a href="/pdf/2402.10899" title="Download PDF">pdf</a>, <a href="/format/2402.10899" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Taxonomy-based CheckList for Large Language Model Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Damin Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">As large language models (LLMs) have been used in many downstream tasks, the
internal stereotypical representation may affect the fairness of the outputs.
In this work, we introduce human knowledge into natural language interventions
and study pre-trained language models' (LMs) behaviors within the context of
gender bias. Inspired by CheckList behavioral testing, we present a
checklist-style task that aims to probe and quantify LMs' unethical behaviors
through question-answering (QA). We design three comparison studies to evaluate
LMs from four aspects: consistency, biased tendency, model preference, and
gender preference switch. We probe one transformer-based QA model trained on
SQuAD-v2 dataset and one autoregressive large language model. Our results
indicate that transformer-based QA model's biased tendency positively
correlates with its consistency, whereas LLM shows the opposite relation. Our
proposed task provides the first dataset that involves human knowledge for LLM
bias evaluation.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10906" title="Abstract">arXiv:2402.10906</a> [<a href="/pdf/2402.10906" title="Download PDF">pdf</a>, <a href="/format/2402.10906" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a sharper phase-field method: a hybrid diffuse-semisharp  approach for microstructure evolution problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dobrzanski%2C+J">Jedrzej Dobrzanski</a>, 
<a href="/search/math?searchtype=author&query=Stupkiewicz%2C+S">Stanislaw Stupkiewicz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">A new approach is developed for computational modelling of microstructure
evolution problems. The approach combines the phase-field method with the
recently-developed laminated element technique (LET) which is a simple and
efficient method to model weak discontinuities using nonconforming
finite-element meshes. The essence of LET is in treating the elements that are
cut by an interface as simple laminates of the two phases, and this idea is
here extended to propagating interfaces so that the volume fraction of the
phases and the lamination orientation vary accordingly. In the proposed LET-PF
approach, the phase-field variable (order parameter), which is governed by an
evolution equation of the Ginzburg-Landau type, plays the role of a level-set
function that implicitly defines the position of the (sharp) interface. The
mechanical equilibrium subproblem is then solved using the semisharp LET
technique. Performance of LET-PF is illustrated by numerical examples. In
particular, it is shown that, for the problems studied, LET-PF exhibits higher
accuracy than the conventional phase-field method so that, for instance,
qualitatively correct results can be obtained using a significantly coarser
mesh, and thus at a lower computational cost.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10908" title="Abstract">arXiv:2402.10908</a> [<a href="/pdf/2402.10908" title="Download PDF">pdf</a>, <a href="/ps/2402.10908" title="Download PostScript">ps</a>, <a href="/format/2402.10908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-Assisted Crisis Management: Building Advanced LLM Platforms for  Effective Emergency Response and Public Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Otal%2C+H+T">Hakan T. Otal</a>, 
<a href="/search/cs?searchtype=author&query=Canbaz%2C+M+A">M. Abdullah Canbaz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
<p class="mathjax">Emergencies and critical incidents often unfold rapidly, necessitating a
swift and effective response. In this research, we introduce a novel approach
to identify and classify emergency situations from social media posts and
direct emergency messages using an open source Large Language Model, LLAMA2.
The goal is to harness the power of natural language processing and machine
learning to assist public safety telecommunicators and huge crowds during
countrywide emergencies. Our research focuses on developing a language model
that can understand users describe their situation in the 911 call, enabling
LLAMA2 to analyze the content and offer relevant instructions to the
telecommunicator, while also creating workflows to notify government agencies
with the caller's information when necessary. Another benefit this language
model provides is its ability to assist people during a significant emergency
incident when the 911 system is overwhelmed, by assisting the users with simple
instructions and informing authorities with their location and emergency
information.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10913" title="Abstract">arXiv:2402.10913</a> [<a href="/pdf/2402.10913" title="Download PDF">pdf</a>, <a href="/format/2402.10913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A comparative study of explicit and implicit Large Eddy Simulations  using a high-order discontinuous Galerkin solver: application to a Formula 1  front wing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ntoukas%2C+G">Gerasimos Ntoukas</a>, 
<a href="/search/math?searchtype=author&query=Rubio%2C+G">Gonzalo Rubio</a>, 
<a href="/search/math?searchtype=author&query=Marino%2C+O">Oscar Marino</a>, 
<a href="/search/math?searchtype=author&query=Liosi%2C+A">Alexandra Liosi</a>, 
<a href="/search/math?searchtype=author&query=Bottone%2C+F">Francesco Bottone</a>, 
<a href="/search/math?searchtype=author&query=Hoessler%2C+J">Julien Hoessler</a>, 
<a href="/search/math?searchtype=author&query=Ferrer%2C+E">Esteban Ferrer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">This paper explores two Large Eddy Simulation (LES) approaches within the
framework of the high-order discontinuous Galerkin solver, Horses3D. The
investigation focuses on an Inverted Multi-element Wing in Ground Effect (i.e.
2.5D Imperial Front Wing section) representing a Formula 1 front wing, and
compares the strengths and limitations of the two LES methods. The explicit LES
formulation relies on the Vreman model, that adapts to laminar, transitional
and turbulent regimes. The numerical formulation uses nodal basis functions and
Gauss points. The implicit LES formulation, does not require explicit
turbulence modeling but relies in the discretization scheme. We use the
Kennedy-Gruber entropy stable formulation to enhance stability in under
resolved simulations, since we recover the continuous properties such as
entropy conservation at a discrete level. This formulation employs
Gauss-Lobatto points, which downgrades the accuracy of integration but allows
for larger time steps in explicit time integration. We compare our results to
Nektar++ [1] showing that both LES techniques provide results that agree well
with the reference values. The implicit LES shows to better capture transition
and allows for larger time steps at a similar cost per iteration. We conclude
that this implicit LES formulation is very attractive for complex simulations.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10914" title="Abstract">arXiv:2402.10914</a> [<a href="/pdf/2402.10914" title="Download PDF">pdf</a>, <a href="/format/2402.10914" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A robustness-enhanced reconstruction based on discontinuity feedback  factor for high-order finite volume scheme
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+H">Hong Zhang</a>, 
<a href="/search/math?searchtype=author&query=Ji%2C+X">Xing Ji</a>, 
<a href="/search/math?searchtype=author&query=Xu%2C+K">Kun Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to Journal of Scientific Computing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">In this paper, a robustness-enhanced reconstruction for the high-order finite
volume scheme is constructed on the 2-D structured mesh, and both the
high-order gas-kinetic scheme(GKS) and the Lax-Friedrichs(L-F) flux solver are
considered to verify the validity of this algorithm. The strategy of the
successful WENO reconstruction is adopted to select the smooth sub-stencils.
However, there are cases where strong discontinuities exist in all sub-stencils
of the WENO reconstruction, which leads to a decrease in the robustness. To
improve the robustness of the algorithm in discontinuous regions in
two-dimensional space, the hybrid reconstruction based on a combination of
discontinuity feedback factor(DF) \cite{ji2021gradient} and WENO reconstruction
is developed to deal with the possible discontinuities. Numerical results from
smooth to extreme cases have been presented and validate that the new finite
volume scheme is effective for robustness enhancement and maintains high
resolution compared to the WENO scheme.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10917" title="Abstract">arXiv:2402.10917</a> [<a href="/pdf/2402.10917" title="Download PDF">pdf</a>, <a href="/ps/2402.10917" title="Download PostScript">ps</a>, <a href="/format/2402.10917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SRAM Alpha-SER Estimation From Word-Line Voltage Margin Measurements:  Design Architecture and Experimental Results
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Torrens%2C+G">Gabriel Torrens</a> (1), 
<a href="/search/cs?searchtype=author&query=de+Paul%2C+I">Ivan de Paul</a> (1), 
<a href="/search/cs?searchtype=author&query=Alorda%2C+B">Bartomeu Alorda</a> (1), 
<a href="/search/cs?searchtype=author&query=Bota%2C+S">Sebastia Bota</a> (1), 
<a href="/search/cs?searchtype=author&query=Segura%2C+J">Jaume Segura</a> (1) ((1) University of the Balearic Islands)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 10 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Nuclear Science, vol. 61, no. 4, pp.
  1849-1855, Aug. 2014
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Experimental results from a 65 nm CMOS commercial technology SRAM test chip
reveal a linear correlation between a new electrical parameter -- the word-line
voltage margin (VWLVM) -- and the measured circuit alpha-SER. Additional
experiments show that no other memory cell electrical robustness-related
parameters exhibit such correlation. The technique proposed is based on
correlating the VWLVM to the SER measured on a small number of circuit samples
to determine the correlation parameters. Then, the remaining non-irradiated
circuits SER is determined from electrical measurements (VWLVM) without the
need of additional radiation experiments. This method represents a significant
improvement in time and cost, while simplifying the SER-determination methods
since most of the circuits do not require irradiation. The technique involves a
minor memory design modification that does not degrade circuit performance,
while circuit area increase is negligible.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10918" title="Abstract">arXiv:2402.10918</a> [<a href="/pdf/2402.10918" title="Download PDF">pdf</a>, <a href="/ps/2402.10918" title="Download PostScript">ps</a>, <a href="/format/2402.10918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recovering the Fragmentation Rate in the Growth-Fragmentation Equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gomez%2C+A+A">Alvaro Almeida Gomez</a>, 
<a href="/search/math?searchtype=author&query=Zubelli%2C+J">Jorge Zubelli</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Inverse Problems, 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We consider the inverse problem of determining the fragmentation rate from
noisy measurements in the growth-fragmentation equation. We use Fourier
transform theory on locally compact groups to treat this problem for general
fragmentation probabilities. We develop a regularization method based on
spectral filtering, which allows us to deal with the inverse problem in
weighted ${L}^2$ spaces. %Our approach regularizes the signal generated by
differential operators in the frequency domain. As a result, we obtain a
regularization method with error of order $O(\varepsilon^{\frac{2m}{2m+1}})$,
where $\varepsilon$ is the noise level and $m&gt;0$ is the {\em a priori}
regularity order of the fragmentation rate.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10920" title="Abstract">arXiv:2402.10920</a> [<a href="/pdf/2402.10920" title="Download PDF">pdf</a>, <a href="/format/2402.10920" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Designing Silicon Brains using LLM: Leveraging ChatGPT for Automated  Description of a Spiking Neuron Array
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tomlinson%2C+M">Michael Tomlinson</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Joe Li</a>, 
<a href="/search/cs?searchtype=author&query=Andreou%2C+A">Andreas Andreou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Artificial Intelligence (cs.AI); Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Large language models (LLMs) have made headlines for synthesizing
correct-sounding responses to a variety of prompts, including code generation.
In this paper, we present the prompts used to guide ChatGPT4 to produce a
synthesizable and functional verilog description for the entirety of a
programmable Spiking Neuron Array ASIC. This design flow showcases the current
state of using ChatGPT4 for natural language driven hardware design. The
AI-generated design was verified in simulation using handcrafted testbenches
and has been submitted for fabrication in Skywater 130nm through Tiny Tapeout 5
using an open-source EDA flow.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10921" title="Abstract">arXiv:2402.10921</a> [<a href="/pdf/2402.10921" title="Download PDF">pdf</a>, <a href="/format/2402.10921" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AM^2-EmoJE: Adaptive Missing-Modality Emotion Recognition in  Conversation via Joint Embedding Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Devulapally%2C+N+K">Naresh Kumar Devulapally</a>, 
<a href="/search/cs?searchtype=author&query=Anand%2C+S">Sidharth Anand</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharjee%2C+S+D">Sreyasee Das Bhattacharjee</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Junsong Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Human emotion can be presented in different modes i.e., audio, video, and
text. However, the contribution of each mode in exhibiting each emotion is not
uniform. Furthermore, the availability of complete mode-specific details may
not always be guaranteed in the test time. In this work, we propose AM^2-EmoJE,
a model for Adaptive Missing-Modality Emotion Recognition in Conversation via
Joint Embedding Learning model that is grounded on two-fold contributions:
First, a query adaptive fusion that can automatically learn the relative
importance of its mode-specific representations in a query-specific manner. By
this the model aims to prioritize the mode-invariant spatial query details of
the emotion patterns, while also retaining its mode-exclusive aspects within
the learned multimodal query descriptor. Second the multimodal joint embedding
learning module that explicitly addresses various missing modality scenarios in
test-time. By this, the model learns to emphasize on the correlated patterns
across modalities, which may help align the cross-attended mode-specific
descriptors pairwise within a joint-embedding space and thereby compensate for
missing modalities during inference. By leveraging the spatio-temporal details
at the dialogue level, the proposed AM^2-EmoJE not only demonstrates superior
performance compared to the best-performing state-of-the-art multimodal
methods, by effectively leveraging body language in place of face expression,
it also exhibits an enhanced privacy feature. By reporting around 2-5%
improvement in the weighted-F1 score, the proposed multimodal joint embedding
module facilitates an impressive performance gain in a variety of
missing-modality query scenarios during test time.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10923" title="Abstract">arXiv:2402.10923</a> [<a href="/pdf/2402.10923" title="Download PDF">pdf</a>, <a href="/format/2402.10923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Hessian-based energy-optimization study of morphoelasticity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wu%2C+M">Min Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Morphoelasticity describes a family of nonlinear PDE system and has been
widely applied to understand growth and morphogenesis in living and nonliving
structures. Most previous studies depend on locally linearizing either the PDE
system or its discretized numerical counterpart. This involves conducting a
linear stability analysis for a trivial solution or utilizing Newton-Raphson
iteration on the force residual equations derived from finite element
approximation, respectively. The paper presents a numerical approach that
frames the search for solutions as the optimization of a discretized energy
functional, similar to Tallinen et al., PRL, 2013. In contrast to Tallinen et
al., we utilize the Hessian matrix of the energy to develop a gradient descent
algorithm that ensures energy stability, which allows us to search solutions
robustly in different types of bifurcations. The solutions are further refined
locally by Newton-Raphson iterations. We showcase the effectiveness of our
approach by studying shape transitions induced by the growth in the interior
layer of a bilayer annulus, with the outer layer constrained at its outermost
boundary. By investigating three cases with varying elastic-moduli ratios
between the two layers, we illustrate that our new method can not only identify
and characterize supercritical bifurcations leading to smooth wrinkles but also
capture subcritical transitions from smooth solutions to non-smooth
crease-forming solutions. Moreover, it reveals aperiodic solutions, both smooth
and non-smooth, that have not been previously reported.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10926" title="Abstract">arXiv:2402.10926</a> [<a href="/pdf/2402.10926" title="Download PDF">pdf</a>, <a href="/format/2402.10926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical analysis of physics-informed neural networks and related  models in physics-informed machine learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=De+Ryck%2C+T">Tim De Ryck</a>, 
<a href="/search/math?searchtype=author&query=Mishra%2C+S">Siddhartha Mishra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Physics-informed neural networks (PINNs) and their variants have been very
popular in recent years as algorithms for the numerical simulation of both
forward and inverse problems for partial differential equations. This article
aims to provide a comprehensive review of currently available results on the
numerical analysis of PINNs and related models that constitute the backbone of
physics-informed machine learning. We provide a unified framework in which
analysis of the various components of the error incurred by PINNs in
approximating PDEs can be effectively carried out. A detailed review of
available results on approximation, generalization and training errors and
their behavior with respect to the type of the PDE and the dimension of the
underlying domain is presented. In particular, the role of the regularity of
the solutions and their stability to perturbations in the error analysis is
elucidated. Numerical results are also presented to illustrate the theory. We
identify training errors as a key bottleneck which can adversely affect the
overall performance of various models in physics-informed machine learning.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10930" title="Abstract">arXiv:2402.10930</a> [<a href="/pdf/2402.10930" title="Download PDF">pdf</a>, <a href="/format/2402.10930" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+G">Guanchen Tao</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Y">Yifei Zou</a>, 
<a href="/search/cs?searchtype=author&query=Chow%2C+D">Derek Chow</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Z">Zichen Fan</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+K">Kauna Lei</a>, 
<a href="/search/cs?searchtype=author&query=Sylvester%2C+D">Dennis Sylvester</a>, 
<a href="/search/cs?searchtype=author&query=Kielian%2C+G">Gregory Kielian</a>, 
<a href="/search/cs?searchtype=author&query=Saligane%2C+M">Mehdi Saligane</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The self-attention mechanism sets transformer-based large language model
(LLM) apart from the convolutional and recurrent neural networks. Despite the
performance improvement, achieving real-time LLM inference on silicon is
challenging due to the extensively used Softmax in self-attention. Apart from
the non-linearity, the low arithmetic intensity greatly reduces the processing
parallelism, which becomes the bottleneck especially when dealing with a longer
context. To address this challenge, we propose Constant Softmax (ConSmax), a
software-hardware co-design as an efficient Softmax alternative. ConSmax
employs differentiable normalization parameters to remove the maximum searching
and denominator summation in Softmax. It allows for massive parallelization
while performing the critical tasks of Softmax. In addition, a scalable ConSmax
hardware utilizing a bitwidth-split look-up table (LUT) can produce lossless
non-linear operation and support mix-precision computing. It further
facilitates efficient LLM inference. Experimental results show that ConSmax
achieves a minuscule power consumption of 0.43 mW and area of 0.001 mm2 at
1-GHz working frequency and 22-nm CMOS technology. Compared to state-of-the-art
Softmax hardware, ConSmax results in 14.5x energy and 14.0x area savings with a
comparable accuracy on a GPT-2 model and the WikiText103 dataset.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10934" title="Abstract">arXiv:2402.10934</a> [<a href="/pdf/2402.10934" title="Download PDF">pdf</a>, <a href="/format/2402.10934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Projective Holder-Minkowski Colors: A Generalized Set of Commutative &amp;  Associative Operations with Inverse Elements for Representing and  Manipulating Colors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Akleman%2C+E">Ergun Akleman</a>, 
<a href="/search/math?searchtype=author&query=Somyung">Somyung</a> (David)Oh, 
<a href="/search/math?searchtype=author&query=Wang%2C+Y">Youyou Wang</a>, 
<a href="/search/math?searchtype=author&query=Akgun%2C+B+T">Bekir Tevfik Akgun</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+J">Jianer Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">One of the key problems in dealing with color in rendering, shading,
compositing, or image manipulation is that we do not have algebraic structures
that support operations over colors. In this paper, we present an
all-encompassing framework that can support a set of algebraic structures with
associativity, commutativity, and inverse properties. To provide these three
properties, we build our algebraic structures on an extension of projective
space by allowing for negative and complex numbers. These properties are
important for (1) manipulating colors as periodic functions, (2) solving
inverse problems dealing with colors, and (3) being consistent with the wave
representation of the color. Allowance of negative and complex numbers is not a
problem for practical applications, since we can always convert the results
into desired range for display purposes as we do in High Dynamic Range imaging.
This set of algebraic structures can be considered as a generalization of the
Minkowski norm Lp in projective space. These structures also provide a new
version of the generalized Holder average with associativity property. Our
structures provide inverses of any operation by allowing for negative and
complex numbers. These structures provide all properties of the generalized
Holder average by providing a continuous bridge between the classical weighted
average, harmonic mean, maximum, and minimum operations using a single
parameter p.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10936" title="Abstract">arXiv:2402.10936</a> [<a href="/pdf/2402.10936" title="Download PDF">pdf</a>, <a href="/format/2402.10936" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sparse Polynomial Chaos Expansion for Universal Stochastic Kriging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Garc%C3%ADa-Marino%2C+J+C">Jos&#xe9; Calos Garc&#xed;a-Marino</a>, 
<a href="/search/math?searchtype=author&query=Calvo-Jurado%2C+C">Carmen Calvo-Jurado</a>, 
<a href="/search/math?searchtype=author&query=Garc%C3%ADa-Mac%C3%ADas%2C+E">Enrique Garc&#xed;a-Mac&#xed;as</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Computational and Applied Mathematics (2024), 115794
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Surrogate modelling techniques have opened up new possibilities to overcome
the limitations of computationally intensive numerical models in various areas
of engineering and science. However, while fundamental in many engineering
applications and decision-making, the incorporation of uncertainty
quantification into meta-models remains a challenging open area of research. To
address this issue, this paper presents a novel stochastic simulation approach
combining sparse polynomial chaos expansion (PCE) and Stochastic Kriging (SK).
Specifically, the proposed approach adopts adaptive sparse PCE as the trend
model in SK, achieving both global and local prediction capabilities and
maximizing the role of the stochastic term to conduct uncertainty
quantification. To maximize the generalization and computational efficiency of
the meta-model, the Least Angle Regression (LAR) algorithm is adopted to
automatically select the optimal polynomial basis in the PCE. The computational
effectiveness and accuracy of the proposed approach are appraised through a
comprehensive set of case studies and different quality metrics. The presented
numerical results and discussion demonstrate the superior performance of the
proposed approach compared to the classical ordinary SK model, offering high
flexibility for the characterization of both extrinsic and intrinsic
uncertainty for a wide variety of problems.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10937" title="Abstract">arXiv:2402.10937</a> [<a href="/pdf/2402.10937" title="Download PDF">pdf</a>, <a href="/ps/2402.10937" title="Download PostScript">ps</a>, <a href="/format/2402.10937" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Lightweight Inception Boosted U-Net Neural Network for Routability  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hailiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+Y">Yan Huo</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+M">Miaohui Hao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper is submitted to the International Symposium of EDA (2024, XiAn, China)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG)

</div>
<p class="mathjax">As the modern CPU, GPU, and NPU chip design complexity and transistor counts
keep increasing, and with the relentless shrinking of semiconductor technology
nodes to nearly 1 nanometer, the placement and routing have gradually become
the two most pivotal processes in modern very-large-scale-integrated (VLSI)
circuit back-end design. How to evaluate routability efficiently and accurately
in advance (at the placement and global routing stages) has grown into a
crucial research area in the field of artificial intelligence (AI) assisted
electronic design automation (EDA). In this paper, we propose a novel U-Net
variant model boosted by an Inception embedded module to predict Routing
Congestion (RC) and Design Rule Checking (DRC) hotspots. Experimental results
on the recently published CircuitNet dataset benchmark show that our proposed
method achieves up to 5% (RC) and 20% (DRC) rate reduction in terms of
Avg-NRMSE (Average Normalized Root Mean Square Error) compared to the classic
architecture. Furthermore, our approach consistently outperforms the prior
model on the SSIM (Structural Similarity Index Measure) metric.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10938" title="Abstract">arXiv:2402.10938</a> [<a href="/pdf/2402.10938" title="Download PDF">pdf</a>, <a href="/format/2402.10938" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> News Source Credibility Assessment: A Reddit Case Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Amini%2C+A">Arash Amini</a>, 
<a href="/search/cs?searchtype=author&query=Bayiz%2C+Y+E">Yigit Ege Bayiz</a>, 
<a href="/search/cs?searchtype=author&query=Ram%2C+A">Ashwin Ram</a>, 
<a href="/search/cs?searchtype=author&query=Marculescu%2C+R">Radu Marculescu</a>, 
<a href="/search/cs?searchtype=author&query=Topcu%2C+U">Ufuk Topcu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages; 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">In the era of social media platforms, identifying the credibility of online
content is crucial to combat misinformation. We present the CREDiBERT
(CREDibility assessment using Bi-directional Encoder Representations from
Transformers), a source credibility assessment model fine-tuned for Reddit
submissions focusing on political discourse as the main contribution. We adopt
a semi-supervised training approach for CREDiBERT, leveraging Reddit's
community-based structure. By encoding submission content using CREDiBERT and
integrating it into a Siamese neural network, we significantly improve the
binary classification of submission credibility, achieving a 9% increase in F1
score compared to existing methods. Additionally, we introduce a new version of
the post-to-post network in Reddit that efficiently encodes user interactions
to enhance the binary classification task by nearly 8% in F1 score. Finally, we
employ CREDiBERT to evaluate the susceptibility of subreddits with respect to
different topics.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10940" title="Abstract">arXiv:2402.10940</a> [<a href="/pdf/2402.10940" title="Download PDF">pdf</a>, <a href="/ps/2402.10940" title="Download PostScript">ps</a>, <a href="/format/2402.10940" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural machine translation of clinical procedure codes for medical  diagnosis and uncertainty quantification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chung%2C+P">Pei-Hung Chung</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shuhan He</a>, 
<a href="/search/cs?searchtype=author&query=Kijpaisalratana%2C+N">Norawit Kijpaisalratana</a>, 
<a href="/search/cs?searchtype=author&query=Ariss%2C+A+e">Abdel-badih el Ariss</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+B">Byung-Jun Yoon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">A Clinical Decision Support System (CDSS) is designed to enhance clinician
decision-making by combining system-generated recommendations with medical
expertise. Given the high costs, intensive labor, and time-sensitive nature of
medical treatments, there is a pressing need for efficient decision support,
especially in complex emergency scenarios. In these scenarios, where
information can be limited, an advanced CDSS framework that leverages AI
(artificial intelligence) models to effectively reduce diagnostic uncertainty
has utility. Such an AI-enabled CDSS framework with quantified uncertainty
promises to be practical and beneficial in the demanding context of real-world
medical care. In this study, we introduce the concept of Medical Entropy,
quantifying uncertainties in patient outcomes predicted by neural machine
translation based on the ICD-9 code of procedures. Our experimental results not
only show strong correlations between procedure and diagnosis sequences based
on the simple ICD-9 code but also demonstrate the promising capacity to model
trends of uncertainties during hospitalizations through a data-driven approach.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10941" title="Abstract">arXiv:2402.10941</a> [<a href="/pdf/2402.10941" title="Download PDF">pdf</a>, <a href="/format/2402.10941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text2Data: Low-Resource Data Generation with Textual Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shiyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yihao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+T">Tian Lan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+N">Ning Yu</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yu Bai</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ran Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Huan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Caiming Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Savarese%2C+S">Silvio Savarese</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> We propose a method that can achieve text-to-data generation under low-resource situation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Natural language serves as a common and straightforward control signal for
humans to interact seamlessly with machines. Recognizing the importance of this
interface, the machine learning community is investing considerable effort in
generating data that is semantically coherent with textual instructions. While
strides have been made in text-to-data generation spanning image editing, audio
synthesis, video creation, and beyond, low-resource areas characterized by
expensive annotations or complex data structures, such as molecules, motion
dynamics, and time series, often lack textual labels. This deficiency impedes
supervised learning, thereby constraining the application of advanced
generative models for text-to-data tasks. In response to these challenges in
the low-resource scenario, we propose Text2Data, a novel approach that utilizes
unlabeled data to understand the underlying data distribution through an
unsupervised diffusion model. Subsequently, it undergoes controllable
finetuning via a novel constraint optimization-based learning objective that
ensures controllability and effectively counteracts catastrophic forgetting.
Comprehensive experiments demonstrate that Text2Data is able to achieve
enhanced performance regarding controllability across various modalities,
including molecules, motions and time series, when compared to existing
baselines.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10943" title="Abstract">arXiv:2402.10943</a> [<a href="/pdf/2402.10943" title="Download PDF">pdf</a>, <a href="/ps/2402.10943" title="Download PostScript">ps</a>, <a href="/format/2402.10943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advances and Limitations in Open Source Arabic-Script OCR: A Case Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kiessling%2C+B">Benjamin Kiessling</a> (PSL), 
<a href="/search/cs?searchtype=author&query=Kurin%2C+G">Gennady Kurin</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+M+T">Matthew Thomas Miller</a>, 
<a href="/search/cs?searchtype=author&query=Smail%2C+K">Kader Smail</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Digital Studies / Le champ num{\'e}rique, 2021, 11 (1)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">This work presents an accuracy study of the open source OCR engine, Kraken,
on the leading Arabic scholarly journal, al-Abhath. In contrast with other
commercially available OCR engines, Kraken is shown to be capable of producing
highly accurate Arabic-script OCR. The study also assesses the relative
accuracy of typeface-specific and generalized models on the al-Abhath data and
provides a microanalysis of the ``error instances'' and the contextual features
that may have contributed to OCR misrecognition. Building on this analysis, the
paper argues that Arabic-script OCR can be significantly improved through (1) a
more systematic approach to training data production, and (2) the development
of key technological components, especially multi-language models and improved
line segmentation and layout analysis.
<br />Cet article pr{\'e}sente une {\'e}tude d'exactitude du moteur ROC open
source, Krakan, sur la revue acad{\'e}mique arabe de premier rang, al-Abhath.
Contrairement {\`a} d'autres moteurs ROC disponibles sur le march{\'e}, Kraken
se r{\'e}v{\`e}le {\^e}tre capable de produire de la ROC extr{\^e}mement exacte
de l'{\'e}criture arabe. L'{\'e}tude {\'e}value aussi l'exactitude relative des
mod{\`e}les sp{\'e}cifiquement configur{\'e}s {\`a} des polices et celle des
mod{\`e}les g{\'e}n{\'e}ralis{\'e}s sur les donn{\'e}es d'al-Abhath et fournit
une microanalyse des "occurrences d'erreurs", ainsi qu'une microanalyse des
{\'e}l{\'e}ments contextuels qui pourraient avoir contribu{\'e} {\`a} la
m{\'e}reconnaissance ROC. S'appuyant sur cette analyse, cet article fait valoir
que la ROC de l'{\'e}criture arabe peut {\^e}tre consid{\'e}rablement
am{\'e}lior{\'e}e gr{\^a}ce {\`a} (1) une approche plus syst{\'e}matique
d'entra{\^i}nement de la production de donn{\'e}es et (2) gr{\^a}ce au
d{\'e}veloppement de composants technologiques fondamentaux,
notammentl'am{\'e}lioration des mod{\`e}les multilingues, de la segmentation de
ligne et de l'analyse de la mise en page.
</p>
</div>
</dd>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10946" title="Abstract">arXiv:2402.10946</a> [<a href="/pdf/2402.10946" title="Download PDF">pdf</a>, <a href="/format/2402.10946" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CultureLLM: Incorporating Cultural Differences into Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Cheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Mengzhou Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jindong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sitaram%2C+S">Sunayana Sitaram</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+X">Xing Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report; 26 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) are reported to be partial to certain cultures
owing to the training data dominance from the English corpora. Since
multilingual cultural data are often expensive to collect, existing efforts
handle this by prompt engineering or culture-specific pre-training. However,
they might overlook the knowledge deficiency of low-resource culture and
require extensive computing resources. In this paper, we propose CultureLLM, a
cost-effective solution to incorporate cultural differences into LLMs.
CultureLLM adopts World Value Survey (WVS) as seed data and generates
semantically equivalent training data via the proposed semantic data
augmentation. Using only 50 seed samples from WVS with augmented data, we
fine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9
cultures covering rich and low-resource languages. Extensive experiments on 60
culture-related datasets demonstrate that CultureLLM significantly outperforms
various counterparts such as GPT-3.5 (by 8.1%) and Gemini Pro (by 9.5%) with
comparable performance to GPT-4 or even better. Our human study shows that the
generated samples are semantically equivalent to the original samples,
providing an effective solution for LLMs augmentation.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10948" title="Abstract">arXiv:2402.10948</a> [<a href="/pdf/2402.10948" title="Download PDF">pdf</a>, <a href="/format/2402.10948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero-shot Explainable Mental Health Analysis on Social Media by  incorporating Mental Scales
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yinuo Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+M">Ming Li</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Ziyue Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Ziqian Zeng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages,2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Traditional discriminative approaches in mental health analysis are known for
their strong capacity but lack interpretability and demand large-scale
annotated data. On the other hand, generative approaches, such as those based
on large language models (LLMs),have the potential to get rid of heavy
annotations and provide explanations. However, their capabilities still fall
short compared to discriminative approaches, and their explanations may be
unreliable due to the fact that the generation of explanation is a black-box
process. Inspired by the psychological assessment practice of using scales to
evaluate mental states, our method incorporates two procedures via LLMs. First,
the patient completes mental health questionnaires, and second, the
psychologist interprets the collected information from the mental health
questions and makes informed decisions. Experimental results show that our
method outperforms other zero-shot methods. Our method can generate more
rigorous explanation based on the outputs of mental questionnaires.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10949" title="Abstract">arXiv:2402.10949</a> [<a href="/pdf/2402.10949" title="Download PDF">pdf</a>, <a href="/ps/2402.10949" title="Download PostScript">ps</a>, <a href="/format/2402.10949" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Unreasonable Effectiveness of Eccentric Automatic Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Battle%2C+R">Rick Battle</a>, 
<a href="/search/cs?searchtype=author&query=Gollapudi%2C+T">Teja Gollapudi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated remarkable problem-solving and
basic mathematics abilities. However, their efficacy is highly contingent on
the formulation of the prompt. This study endeavors to quantify the influence
of incorporating "positive thinking" into the system message of the prompt,
then compare that to systematic prompt optimization. We assess the performance
of 60 combinations of system message snippets, tested with and without Chain of
Thought prompting, across three models with parameters ranging from 7 to 70
billion on the GSM8K dataset. Our findings reveal that results do not
universally generalize across models. In most instances, the inclusion of
"positive thinking" prompts positively affected model performance. Notably,
however, Llama2-70B exhibited an exception when not utilizing Chain of Thought,
as the optimal system message was found to be none at all. Given the
combinatorial complexity, and thus computation time, of experimenting with
hand-tuning prompts for large black-box models, we then compared the
performance of the best "positive thinking" prompt against the output of
systematic prompt optimization. We show that employing an automated prompt
optimizer emerges as the most effective method for enhancing performance, even
when working with smaller open-source models. Additionally, our findings reveal
that the highest-scoring, automatically-optimized prompt exhibits a degree of
peculiarity far beyond expectations.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10951" title="Abstract">arXiv:2402.10951</a> [<a href="/pdf/2402.10951" title="Download PDF">pdf</a>, <a href="/format/2402.10951" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DAEDRA: A language model for predicting outcomes in passive  pharmacovigilance reporting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=von+Csefalvay%2C+C">Chris von Csefalvay</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Over the recent years, the emergence of large language models (LLMs) has
given rise to a proliferation of domain-specific models that are intended to
reflect the particularities of linguistic context and content as a correlate of
the originating domain. This paper details the conception, design, training and
evaluation of DAEDRA, a LLM designed to detect regulatory-relevant outcomes
(mortality, ER attendance and hospitalisation) in adverse event reports
elicited through passive reporting (PR). While PR is a highly cost-efficient
way of eliciting information from a wide and diverse audience -- typically
including not only physicians and healthcare providers but also patients,
family members and other lay stakeholders --, this diversity makes PR corpora
difficult to analyse. Generic language models may not capture the complex
clinical dimensions while specific clinical or biomedical models may not
perform well on lay reports. To evaluate the utility of a subdomain-specific
language model, an adaptive training approach was adapted, wherein base
language model candidates were evaluated on a subset of the corpus, and the
best performer was trained on the entire corpus. This yielded a small but
significant improvement in $F_1$ (+1%), precision (+2.5%) and recall (+3.8%),
at a relatively low training cost and a single-day training time.
Subdomain-specific LLMs continue to be viable options for better results when
analysing highly specialised corpora.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10956" title="Abstract">arXiv:2402.10956</a> [<a href="/pdf/2402.10956" title="Download PDF">pdf</a>, <a href="/format/2402.10956" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sleep-Like Unsupervised Replay Improves Performance when Data are  Limited or Unbalanced
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bazhenov%2C+A">Anthony Bazhenov</a>, 
<a href="/search/cs?searchtype=author&query=Dewasurendra%2C+P">Pahan Dewasurendra</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+G">Giri Krishnan</a>, 
<a href="/search/cs?searchtype=author&query=Delanois%2C+J+E">Jean Erik Delanois</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 pages, 1 Figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">The performance of artificial neural networks (ANNs) degrades when training
data are limited or imbalanced. In contrast, the human brain can learn quickly
from just a few examples. Here, we investigated the role of sleep in improving
the performance of ANNs trained with limited data on the MNIST and Fashion
MNIST datasets. Sleep was implemented as an unsupervised phase with local
Hebbian type learning rules. We found a significant boost in accuracy after the
sleep phase for models trained with limited data in the range of 0.5-10% of
total MNIST or Fashion MNIST datasets. When more than 10% of the total data was
used, sleep alone had a slight negative impact on performance, but this was
remedied by fine-tuning on the original data. This study sheds light on a
potential synaptic weight dynamics strategy employed by the brain during sleep
to enhance memory performance when training data are limited or imbalanced.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10957" title="Abstract">arXiv:2402.10957</a> [<a href="/pdf/2402.10957" title="Download PDF">pdf</a>, <a href="/format/2402.10957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hyper-differential sensitivity analysis with respect to model  discrepancy: Posterior Optimal Solution Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hart%2C+J">Joseph Hart</a>, 
<a href="/search/math?searchtype=author&query=van+Bloemen+Waanders%2C+B">Bart van Bloemen Waanders</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Optimization constrained by high-fidelity computational models has potential
for transformative impact. However, such optimization is frequently
unattainable in practice due to the complexity and computational intensity of
the model. An alternative is to optimize a low-fidelity model and use limited
evaluations of the high-fidelity model to assess the quality of the solution.
This article develops a framework to use limited high-fidelity simulations to
update the optimization solution computed using the low-fidelity model.
Building off a previous article [22], which introduced hyper-differential
sensitivity analysis with respect to model discrepancy, this article provides
novel extensions of the algorithm to enable uncertainty quantification of the
optimal solution update via a Bayesian framework. Specifically, we formulate a
Bayesian inverse problem to estimate the model discrepancy and propagate the
posterior model discrepancy distribution through the post-optimality
sensitivity operator for the low-fidelity optimization problem. We provide a
rigorous treatment of the Bayesian formulation, a computationally efficient
algorithm to compute posterior samples, a guide to specify and interpret the
algorithm hyper-parameters, and a demonstration of the approach on three
examples which highlight various types of discrepancy between low and
high-fidelity models.
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10958" title="Abstract">arXiv:2402.10958</a> [<a href="/pdf/2402.10958" title="Download PDF">pdf</a>, <a href="/format/2402.10958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Relative Preference Optimization: Enhancing LLM Alignment through  Contrasting Responses across Identical and Diverse Prompts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yueqin Yin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhendong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Y">Yi Gu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Hai Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Weizhu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mingyuan Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">In the field of large language models (LLMs), aligning models with the
diverse preferences of users is a critical challenge. Direct Preference
Optimization (DPO) has played a key role in this area. It works by using pairs
of preferences derived from the same prompts, and it functions without needing
an additional reward model. However, DPO does not fully reflect the complex
nature of human learning, which often involves understanding contrasting
responses to not only identical but also similar questions. To overcome this
shortfall, we propose Relative Preference Optimization (RPO). RPO is designed
to discern between more and less preferred responses derived from both
identical and related prompts. It introduces a contrastive weighting mechanism,
enabling the tuning of LLMs using a broader range of preference data, including
both paired and unpaired sets. This approach expands the learning capabilities
of the model, allowing it to leverage insights from a more varied set of
prompts. Through empirical tests, including dialogue and summarization tasks,
and evaluations using the AlpacaEval2.0 leaderboard, RPO has demonstrated a
superior ability to align LLMs with user preferences and to improve their
adaptability during the training process. The PyTorch code necessary to
reproduce the results presented in the paper will be made available on GitHub
for public access.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10962" title="Abstract">arXiv:2402.10962</a> [<a href="/pdf/2402.10962" title="Download PDF">pdf</a>, <a href="/format/2402.10962" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring and Controlling Persona Drift in Language Model Dialogs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+K">Kenneth Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianle Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bashkansky%2C+N">Naomi Bashkansky</a>, 
<a href="/search/cs?searchtype=author&query=Bau%2C+D">David Bau</a>, 
<a href="/search/cs?searchtype=author&query=Vi%C3%A9gas%2C+F">Fernanda Vi&#xe9;gas</a>, 
<a href="/search/cs?searchtype=author&query=Pfister%2C+H">Hanspeter Pfister</a>, 
<a href="/search/cs?searchtype=author&query=Wattenberg%2C+M">Martin Wattenberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code: <a href="https://github.com/likenneth/persona_drift">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Prompting is a standard tool for customizing language-model chatbots,
enabling them to take on a specific "persona". An implicit assumption in the
use of prompts is that they will be stable, so the chatbot will continue to
generate text according to the stipulated persona for the duration of a
conversation. We propose a quantitative benchmark to test this assumption,
evaluating persona stability via self-chats between two personalized chatbots.
Testing popular models like LLaMA2-chat-70B, we reveal a significant persona
drift within eight rounds of conversations. An empirical and theoretical
analysis of this phenomenon suggests the transformer attention mechanism plays
a role, due to attention decay over long exchanges. To combat attention decay
and persona drift, we propose a lightweight method called split-softmax, which
compares favorably against two strong baselines.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10963" title="Abstract">arXiv:2402.10963</a> [<a href="/pdf/2402.10963" title="Download PDF">pdf</a>, <a href="/format/2402.10963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GLoRe: When, Where, and How to Improve LLM Reasoning via Global and  Local Refinements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Havrilla%2C+A">Alex Havrilla</a>, 
<a href="/search/cs?searchtype=author&query=Raparthy%2C+S">Sharath Raparthy</a>, 
<a href="/search/cs?searchtype=author&query=Nalmpantis%2C+C">Christoforus Nalmpantis</a>, 
<a href="/search/cs?searchtype=author&query=Dwivedi-Yu%2C+J">Jane Dwivedi-Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhuravinskyi%2C+M">Maksym Zhuravinskyi</a>, 
<a href="/search/cs?searchtype=author&query=Hambro%2C+E">Eric Hambro</a>, 
<a href="/search/cs?searchtype=author&query=Railneau%2C+R">Roberta Railneau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">State-of-the-art language models can exhibit impressive reasoning refinement
capabilities on math, science or coding tasks. However, recent work
demonstrates that even the best models struggle to identify \textit{when and
where to refine} without access to external feedback. Outcome-based Reward
Models (\textbf{ORMs}), trained to predict correctness of the final answer
indicating when to refine, offer one convenient solution for deciding when to
refine. Process Based Reward Models (\textbf{PRMs}), trained to predict
correctness of intermediate steps, can then be used to indicate where to
refine. But they are expensive to train, requiring extensive human annotations.
In this paper, we propose Stepwise ORMs (\textbf{SORMs}) which are trained,
only on synthetic data, to approximate the expected future reward of the
optimal policy or $V^{\star}$. More specifically, SORMs are trained to predict
the correctness of the final answer when sampling the current policy many times
(rather than only once as in the case of ORMs). Our experiments show that SORMs
can more accurately detect incorrect reasoning steps compared to ORMs, thus
improving downstream accuracy when doing refinements. We then train
\textit{global} refinement models, which take only the question and a draft
solution as input and predict a corrected solution, and \textit{local}
refinement models which also take as input a critique indicating the location
of the first reasoning error. We generate training data for both models
synthetically by reusing data used to train the SORM. We find combining global
and local refinements, using the ORM as a reranker, significantly outperforms
either one individually, as well as a best of three sample baseline. With this
strategy we can improve the accuracy of a LLaMA-2 13B model (already fine-tuned
with RL) on GSM8K from 53\% to 65\% when greedily sampled.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10964" title="Abstract">arXiv:2402.10964</a> [<a href="/pdf/2402.10964" title="Download PDF">pdf</a>, <a href="/format/2402.10964" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal feature rescaling in machine learning based on neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vitr%C3%B2%2C+F+M">Federico Maria Vitr&#xf2;</a>, 
<a href="/search/cs?searchtype=author&query=Leonesio%2C+M">Manrco Leonesio</a>, 
<a href="/search/cs?searchtype=author&query=Fagiano%2C+L">Lorenzo Fagiano</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">This paper proposes a novel approach to improve the training efficiency and
the generalization performance of Feed Forward Neural Networks (FFNNs)
resorting to an optimal rescaling of input features (OFR) carried out by a
Genetic Algorithm (GA). The OFR reshapes the input space improving the
conditioning of the gradient-based algorithm used for the training. Moreover,
the scale factors exploration entailed by GA trials and selection corresponds
to different initialization of the first layer weights at each training
attempt, thus realizing a multi-start global search algorithm (even though
restrained to few weights only) which fosters the achievement of a global
minimum. The approach has been tested on a FFNN modeling the outcome of a real
industrial process (centerless grinding).
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10965" title="Abstract">arXiv:2402.10965</a> [<a href="/pdf/2402.10965" title="Download PDF">pdf</a>, <a href="/format/2402.10965" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalization in Healthcare AI: Evaluation of a Clinical Large Language  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahman%2C+S">Salman Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+L+Y">Lavender Yao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Gabriel%2C+S">Saadia Gabriel</a>, 
<a href="/search/cs?searchtype=author&query=Aphinyanaphongs%2C+Y">Yindalon Aphinyanaphongs</a>, 
<a href="/search/cs?searchtype=author&query=Oermann%2C+E+K">Eric Karl Oermann</a>, 
<a href="/search/cs?searchtype=author&query=Chunara%2C+R">Rumi Chunara</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">Advances in large language models (LLMs) provide new opportunities in
healthcare for improved patient care, clinical decision-making, and enhancement
of physician and administrator workflows. However, the potential of these
models importantly depends on their ability to generalize effectively across
clinical environments and populations, a challenge often underestimated in
early development. To better understand reasons for these challenges and inform
mitigation approaches, we evaluated ClinicLLM, an LLM trained on [HOSPITAL]'s
clinical notes, analyzing its performance on 30-day all-cause readmission
prediction focusing on variability across hospitals and patient
characteristics. We found poorer generalization particularly in hospitals with
fewer samples, among patients with government and unspecified insurance, the
elderly, and those with high comorbidities. To understand reasons for lack of
generalization, we investigated sample sizes for fine-tuning, note content
(number of words per note), patient characteristics (comorbidity level, age,
insurance type, borough), and health system aspects (hospital, all-cause 30-day
readmission, and mortality rates). We used descriptive statistics and
supervised classification to identify features. We found that, along with
sample size, patient age, number of comorbidities, and the number of words in
notes are all important factors related to generalization. Finally, we compared
local fine-tuning (hospital specific), instance-based augmented fine-tuning and
cluster-based fine-tuning for improving generalization. Among these, local
fine-tuning proved most effective, increasing AUC by 0.25% to 11.74% (most
helpful in settings with limited data). Overall, this study provides new
insights for enhancing the deployment of large language models in the
societally important domain of healthcare, and improving their performance for
broader populations.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10967" title="Abstract">arXiv:2402.10967</a> [<a href="/pdf/2402.10967" title="Download PDF">pdf</a>, <a href="/ps/2402.10967" title="Download PostScript">ps</a>, <a href="/format/2402.10967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social network analysis for personalized characterization and risk  assessment of alcohol use disorders in adolescents using semantic  technologies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-Rodr%C3%ADguez%2C+I">Isa&#xed;as Garc&#xed;a-Rodr&#xed;guez</a>, 
<a href="/search/cs?searchtype=author&query=Benavides%2C+C">Carmen Benavides</a>, 
<a href="/search/cs?searchtype=author&query=Alaiz-Moret%C3%B3n%2C+H">H&#xe9;ctor Alaiz-Moret&#xf3;n</a>, 
<a href="/search/cs?searchtype=author&query=Rodr%C3%ADguez-Gonz%C3%A1lez%2C+A">Alejandro Rodr&#xed;guez-Gonz&#xe1;lez</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Future Generation Computer Systems, Volume 106, May 2020, Pages
  154-170
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Alcohol Use Disorder (AUD) is a major concern for public health organizations
worldwide, especially as regards the adolescent population. The consumption of
alcohol in adolescents is known to be influenced by seeing friends and even
parents drinking alcohol. Building on this fact, a number of studies into
alcohol consumption among adolescents have made use of Social Network Analysis
(SNA) techniques to study the different social networks (peers, friends,
family, etc.) with whom the adolescent is involved. These kinds of studies need
an initial phase of data gathering by means of questionnaires and a subsequent
analysis phase using the SNA techniques. The process involves a number of
manual data handling stages that are time consuming and error-prone. The use of
knowledge engineering techniques (including the construction of a domain
ontology) to represent the information, allows the automation of all the
activities, from the initial data collection to the results of the SNA study.
This paper shows how a knowledge model is constructed, and compares the results
obtained using the traditional method with this, fully automated model,
detailing the main advantages of the latter. In the case of the SNA analysis,
the validity of the results obtained with the knowledge engineering approach
are compared to those obtained manually using the UCINET, Cytoscape, Pajek and
Gephi to test the accuracy of the knowledge model.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10968" title="Abstract">arXiv:2402.10968</a> [<a href="/pdf/2402.10968" title="Download PDF">pdf</a>, <a href="/ps/2402.10968" title="Download PostScript">ps</a>, <a href="/format/2402.10968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thermal Infrared Imaging to Evaluate Emotional Competences in Nursing  Students: A First Approach through a Case Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marqu%C3%A9s-S%C3%A1nchez%2C+P">Pilar Marqu&#xe9;s-S&#xe1;nchez</a>, 
<a href="/search/cs?searchtype=author&query=Li%C3%A9bana-Presa%2C+C">Cristina Li&#xe9;bana-Presa</a>, 
<a href="/search/cs?searchtype=author&query=Ben%C3%ADtez-Andrades%2C+J+A">Jos&#xe9; Alberto Ben&#xed;tez-Andrades</a>, 
<a href="/search/cs?searchtype=author&query=Gund%C3%ADn-Gallego%2C+R">Raquel Gund&#xed;n-Gallego</a>, 
<a href="/search/cs?searchtype=author&query=%C3%81lvarez-Barrio%2C+L">Lorena &#xc1;lvarez-Barrio</a>, 
<a href="/search/cs?searchtype=author&query=Rodr%C3%ADguez-Gonz%C3%A1lvez%2C+P">Pablo Rodr&#xed;guez-Gonz&#xe1;lvez</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Sensors 2020, 20(9), 2502
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">During nursing studies, it is crucial to develop emotional skills for both
academic success and quality patient care. Utilizing technologies like
thermography can be instrumental in nursing education to assess and enhance
these skills. The study aims to evaluate the effectiveness of thermography in
monitoring and improving the emotional skills of nursing students through a
case study approach. The case study involved exposing a student to various
emotional stimuli, including videos and music, and measuring facial temperature
changes. These changes were recorded using a FLIR E6 camera across three
phases: acclimatization, stimulus, and response. Environmental factors such as
temperature and humidity were also recorded. Distinct thermal responses were
observed for different emotions. For instance, during the acclimatization phase
with video stimuli, forehead temperatures varied between positive emotions
(joy: 34.5\textdegree C to 34.5\textdegree C) and negative emotions (anger:
36.1\textdegree C to 35.1\textdegree C). However, there was a uniform change in
temperature during both stimulus (joy: 34.7\textdegree C to 35.0\textdegree C,
anger: 35.0\textdegree C to 35.0\textdegree C) and response phases (joy:
35.0\textdegree C to 35.0\textdegree C, anger: 34.8\textdegree C to
35.0\textdegree C). Music stimuli also induced varying thermal patterns (joy:
34.2\textdegree C to 33.9\textdegree C to 33.4\textdegree C, anger:
33.8\textdegree C to 33.4\textdegree C to 33.8\textdegree C).Thermography
revealed consistent thermal patterns in response to emotional stimuli, with the
exception of the nose area, suggesting its suitability as a non-invasive,
quantifiable, and accessible method for emotional skill training in nursing
education.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10971" title="Abstract">arXiv:2402.10971</a> [<a href="/pdf/2402.10971" title="Download PDF">pdf</a>, <a href="/format/2402.10971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling data-driven and bidirectional model development in Verilog-A  for photonic devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azhigulov%2C+D">Dias Azhigulov</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zeqin Lu</a>, 
<a href="/search/cs?searchtype=author&query=Pond%2C+J">James Pond</a>, 
<a href="/search/cs?searchtype=author&query=Chrostowski%2C+L">Lukas Chrostowski</a>, 
<a href="/search/cs?searchtype=author&query=Shekhar%2C+S">Sudip Shekhar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>

</div>
<p class="mathjax">We present a method to model photonic components in Verilog-A by introducing
bidirectional signaling through a single port. To achieve this, the concept of
power waves and scattering parameters from electromagnetism are employed. As a
consequence, one can simultaneously transmit forward and backward propagating
waves on a single wire while also capturing realistic, measurement-backed
response of photonic components in Verilog-A. We demonstrate examples to show
the efficacy of the proposed technique in accounting for critical effects in
photonic integrated circuits such as Fabry-Perot cavity resonance, reflections
to lasers, etc. Our solution makes electronic-photonic co-simulation more
intuitive and accurate.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10974" title="Abstract">arXiv:2402.10974</a> [<a href="/pdf/2402.10974" title="Download PDF">pdf</a>, <a href="/format/2402.10974" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Cross-Dataset Generalization of Machine Learning for Network  Intrusion Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cantone%2C+M">Marco Cantone</a>, 
<a href="/search/cs?searchtype=author&query=Marrocco%2C+C">Claudio Marrocco</a>, 
<a href="/search/cs?searchtype=author&query=Bria%2C+A">Alessandro Bria</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Network Intrusion Detection Systems (NIDS) are a fundamental tool in
cybersecurity. Their ability to generalize across diverse networks is a
critical factor in their effectiveness and a prerequisite for real-world
applications. In this study, we conduct a comprehensive analysis on the
generalization of machine-learning-based NIDS through an extensive
experimentation in a cross-dataset framework. We employ four machine learning
classifiers and utilize four datasets acquired from different networks:
CIC-IDS-2017, CSE-CIC-IDS2018, LycoS-IDS2017, and LycoS-Unicas-IDS2018.
Notably, the last dataset is a novel contribution, where we apply corrections
based on LycoS-IDS2017 to the well-known CSE-CIC-IDS2018 dataset. The results
show nearly perfect classification performance when the models are trained and
tested on the same dataset. However, when training and testing the models in a
cross-dataset fashion, the classification accuracy is largely commensurate with
random chance except for a few combinations of attacks and datasets. We employ
data visualization techniques in order to provide valuable insights on the
patterns in the data. Our analysis unveils the presence of anomalies in the
data that directly hinder the classifiers capability to generalize the learned
knowledge to new scenarios. This study enhances our comprehension of the
generalization capabilities of machine-learning-based NIDS, highlighting the
significance of acknowledging data heterogeneity.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10977" title="Abstract">arXiv:2402.10977</a> [<a href="/pdf/2402.10977" title="Download PDF">pdf</a>, <a href="/ps/2402.10977" title="Download PostScript">ps</a>, <a href="/format/2402.10977" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative AI and Process Systems Engineering: The Next Frontier
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Decardi-Nelson%2C+B">Benjamin Decardi-Nelson</a>, 
<a href="/search/cs?searchtype=author&query=Alshehri%2C+A+S">Abdulelah S. Alshehri</a>, 
<a href="/search/cs?searchtype=author&query=Ajagekar%2C+A">Akshay Ajagekar</a>, 
<a href="/search/cs?searchtype=author&query=You%2C+F">Fengqi You</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
<p class="mathjax">This article explores how emerging generative artificial intelligence (GenAI)
models, such as large language models (LLMs), can enhance solution
methodologies within process systems engineering (PSE). These cutting-edge
GenAI models, particularly foundation models (FMs), which are pre-trained on
extensive, general-purpose datasets, offer versatile adaptability for a broad
range of tasks, including responding to queries, image generation, and complex
decision-making. Given the close relationship between advancements in PSE and
developments in computing and systems technologies, exploring the synergy
between GenAI and PSE is essential. We begin our discussion with a compact
overview of both classic and emerging GenAI models, including FMs, and then
dive into their applications within key PSE domains: synthesis and design,
optimization and integration, and process monitoring and control. In each
domain, we explore how GenAI models could potentially advance PSE
methodologies, providing insights and prospects for each area. Furthermore, the
article identifies and discusses potential challenges in fully leveraging GenAI
within PSE, including multiscale modeling, data requirements, evaluation
metrics and benchmarks, and trust and safety, thereby deepening the discourse
on effective GenAI integration into systems analysis, design, optimization,
operations, monitoring, and control. This paper provides a guide for future
research focused on the applications of emerging GenAI in PSE.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10978" title="Abstract">arXiv:2402.10978</a> [<a href="/pdf/2402.10978" title="Download PDF">pdf</a>, <a href="/format/2402.10978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Models with Conformal Factuality Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohri%2C+C">Christopher Mohri</a>, 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+T">Tatsunori Hashimoto</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Guaranteeing the correctness and factuality of language model (LM) outputs is
a major open problem. In this work, we propose conformal factuality, a
framework that can ensure high probability correctness guarantees for LMs by
connecting language modeling and conformal prediction. We observe that the
correctness of an LM output is equivalent to an uncertainty quantification
problem, where the uncertainty sets are defined as the entailment set of an
LM's output. Using this connection, we show that conformal prediction in
language models corresponds to a back-off algorithm that provides high
probability correctness guarantees by progressively making LM outputs less
specific (and expanding the associated uncertainty sets). This approach applies
to any black-box LM and requires very few human-annotated samples. Evaluations
of our approach on closed book QA (FActScore, NaturalQuestions) and reasoning
tasks (MATH) show that our approach can provide 80-90% correctness guarantees
while retaining the majority of the LM's original output.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10979" title="Abstract">arXiv:2402.10979</a> [<a href="/pdf/2402.10979" title="Download PDF">pdf</a>, <a href="/format/2402.10979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SportsMetrics: Blending Text and Numerical Data to Understand  Information Fusion in LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yebowen Hu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+K">Kaiqiang Song</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+S">Sangwoo Cho</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Foroosh%2C+H">Hassan Foroosh</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fei Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models hold significant potential for integrating various data
types, such as text documents and database records, for advanced analytics.
However, blending text and numerical data presents substantial challenges. LLMs
need to process and cross-reference entities and numbers, handle data
inconsistencies and redundancies, and develop planning capabilities such as
building a working memory for managing complex data queries. In this paper, we
introduce four novel tasks centered around sports data analytics to evaluate
the numerical reasoning and information fusion capabilities of LLMs. These
tasks involve providing LLMs with detailed, play-by-play sports game
descriptions, then challenging them with adversarial scenarios such as new game
rules, longer durations, scrambled narratives, and analyzing key statistics in
game summaries. We conduct extensive experiments on NBA and NFL games to assess
the performance of LLMs on these tasks. Our benchmark, SportsMetrics,
introduces a new mechanism for assessing LLMs' numerical reasoning and fusion
skills.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10981" title="Abstract">arXiv:2402.10981</a> [<a href="/pdf/2402.10981" title="Download PDF">pdf</a>, <a href="/ps/2402.10981" title="Download PostScript">ps</a>, <a href="/format/2402.10981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stuck-at Faults in ReRAM Neuromorphic Circuit Array and their Correction  through Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sawal%2C+V">Vedant Sawal</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+H+Y">Hiu Yung Wong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">In this paper, we study the inference accuracy of the Resistive Random Access
Memory (ReRAM) neuromorphic circuit due to stuck-at faults (stuck-on,
stuck-off, and stuck at a certain resistive value). A simulation framework
using Python is used to perform supervised machine learning (neural network
with 3 hidden layers, 1 input layer, and 1 output layer) of handwritten digits
and construct a corresponding fully analog neuromorphic circuit (4 synaptic
arrays) simulated by Spectre. A generic 45nm Process Development Kit (PDK) was
used. We study the difference in the inference accuracy degradation due to
stuck-on and stuck-off defects. Various defect patterns are studied including
circular, ring, row, column, and circular-complement defects. It is found that
stuck-on and stuck-off defects have a similar effect on inference accuracy.
However, it is also found that if there is a spatial defect variation across
the columns, the inference accuracy may be degraded significantly. We also
propose a machine learning (ML) strategy to recover the inference accuracy
degradation due to stuck-at faults. The inference accuracy is improved from 48%
to 85% in a defective neuromorphic circuit.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10982" title="Abstract">arXiv:2402.10982</a> [<a href="/pdf/2402.10982" title="Download PDF">pdf</a>, <a href="/ps/2402.10982" title="Download PostScript">ps</a>, <a href="/format/2402.10982" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> mshw, a forecasting library to predict short-term electricity demand  based on multiple seasonal Holt-Winters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Trull%2C+O">Oscar Trull</a>, 
<a href="/search/cs?searchtype=author&query=Garc%C3%ADa-D%C3%ADaz%2C+J+C">J. Carlos Garc&#xed;a-D&#xed;az</a>, 
<a href="/search/cs?searchtype=author&query=Peir%C3%B3-Signes%2C+A">Angel Peir&#xf3;-Signes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 8 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Econometrics (econ.EM); Applications (stat.AP)

</div>
<p class="mathjax">Transmission system operators have a growing need for more accurate
forecasting of electricity demand. Current electricity systems largely require
demand forecasting so that the electricity market establishes electricity
prices as well as the programming of production units. The companies that are
part of the electrical system use exclusive software to obtain predictions,
based on the use of time series and prediction tools, whether statistical or
artificial intelligence. However, the most common form of prediction is based
on hybrid models that use both technologies. In any case, it is software with a
complicated structure, with a large number of associated variables and that
requires a high computational load to make predictions. The predictions they
can offer are not much better than those that simple models can offer. In this
paper we present a MATLAB toolbox created for the prediction of electrical
demand. The toolbox implements multiple seasonal Holt-Winters exponential
smoothing models and neural network models. The models used include the use of
discrete interval mobile seasonalities (DIMS) to improve forecasting on special
days. Additionally, the results of its application in various electrical
systems in Europe are shown, where the results obtained can be seen. The use of
this library opens a new avenue of research for the use of models with discrete
and complex seasonalities in other fields of application.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10983" title="Abstract">arXiv:2402.10983</a> [<a href="/pdf/2402.10983" title="Download PDF">pdf</a>, <a href="/format/2402.10983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum-Inspired Analysis of Neural Network Vulnerabilities: The Role of  Conjugate Variables in System Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jun-Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+D">Deyu Meng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Quantum Physics (quant-ph)

</div>
<p class="mathjax">Neural networks demonstrate inherent vulnerability to small, non-random
perturbations, emerging as adversarial attacks. Such attacks, born from the
gradient of the loss function relative to the input, are discerned as input
conjugates, revealing a systemic fragility within the network structure.
Intriguingly, a mathematical congruence manifests between this mechanism and
the quantum physics' uncertainty principle, casting light on a hitherto
unanticipated interdisciplinarity. This inherent susceptibility within neural
network systems is generally intrinsic, highlighting not only the innate
vulnerability of these networks but also suggesting potential advancements in
the interdisciplinary area for understanding these black-box networks.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10985" title="Abstract">arXiv:2402.10985</a> [<a href="/pdf/2402.10985" title="Download PDF">pdf</a>, <a href="/format/2402.10985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging AI Planning For Detecting Cloud Security Vulnerabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kazdagli%2C+M">Mikhail Kazdagli</a>, 
<a href="/search/cs?searchtype=author&query=Tiwari%2C+M">Mohit Tiwari</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Akshat Kumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Cloud computing services provide scalable and cost-effective solutions for
data storage, processing, and collaboration. Alongside their growing
popularity, concerns related to their security vulnerabilities leading to data
breaches and sophisticated attacks such as ransomware are growing. To address
these, first, we propose a generic framework to express relations between
different cloud objects such as users, datastores, security roles, to model
access control policies in cloud systems. Access control misconfigurations are
often the primary driver for cloud attacks. Second, we develop a PDDL model for
detecting security vulnerabilities which can for example lead to widespread
attacks such as ransomware, sensitive data exfiltration among others. A planner
can then generate attacks to identify such vulnerabilities in the cloud.
Finally, we test our approach on 14 real Amazon AWS cloud configurations of
different commercial organizations. Our system can identify a broad range of
security vulnerabilities, which state-of-the-art industry tools cannot detect.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10986" title="Abstract">arXiv:2402.10986</a> [<a href="/pdf/2402.10986" title="Download PDF">pdf</a>, <a href="/format/2402.10986" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FinTral: A Family of GPT-4 Level Multimodal Financial Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhatia%2C+G">Gagan Bhatia</a>, 
<a href="/search/cs?searchtype=author&query=Nagoudi%2C+E+M+B">El Moatez Billah Nagoudi</a>, 
<a href="/search/cs?searchtype=author&query=Cavusoglu%2C+H">Hasan Cavusoglu</a>, 
<a href="/search/cs?searchtype=author&query=Abdul-Mageed%2C+M">Muhammad Abdul-Mageed</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ACL 2024 (under review)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce FinTral, a suite of state-of-the-art multimodal large language
models (LLMs) built upon the Mistral-7b model and tailored for financial
analysis. FinTral integrates textual, numerical, tabular, and image data. We
enhance FinTral with domain-specific pretraining, instruction fine-tuning, and
RLAIF training by exploiting a large collection of textual and visual datasets
we curate for this work. We also introduce an extensive benchmark featuring
nine tasks and 25 datasets for evaluation, including hallucinations in the
financial domain. Our FinTral model trained with direct preference optimization
employing advanced Tools and Retrieval methods, dubbed FinTral-DPO-T&amp;R,
demonstrates an exceptional zero-shot performance. It outperforms ChatGPT-3.5
in all tasks and surpasses GPT-4 in five out of nine tasks, marking a
significant advancement in AI-driven financial technology. We also demonstrate
that FinTral has the potential to excel in real-time analysis and
decision-making in diverse financial contexts.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10987" title="Abstract">arXiv:2402.10987</a> [<a href="/pdf/2402.10987" title="Download PDF">pdf</a>, <a href="/format/2402.10987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+C">Chenhui Hu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+P">Pengfei Cao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yubo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Knowledge editing aims to rectify inaccuracies in large language models
(LLMs) without costly retraining for outdated or erroneous knowledge. However,
current knowledge editing methods primarily focus on single editing, failing to
meet the requirements for lifelong editing. In this paper, lifelong editing is
synonymous with lifelong knowledge editing. This study reveals a performance
degradation encountered by knowledge editing in lifelong editing, characterized
by toxicity buildup and toxicity flash, with the primary cause identified as
pattern unmatch. We introduce a knowledge editing approach named WilKE, which
selects editing layer based on the pattern matching degree of editing knowledge
across different layers. Experimental results demonstrate that, in lifelong
editing, WilKE exhibits an average improvement of 46.2\% and 67.8\% on editing
GPT2-XL and GPT-J relative to state-of-the-art knowledge editing methods.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10991" title="Abstract">arXiv:2402.10991</a> [<a href="/pdf/2402.10991" title="Download PDF">pdf</a>, <a href="/format/2402.10991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Semi-Asynchronous Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Changxin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yuxin Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhanxin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+F">Fanghao Ni</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+J">Jize Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Federated Learning (FL) is a distributed machine learning paradigm that
allows clients to train models on their data while preserving their privacy. FL
algorithms, such as Federated Averaging (FedAvg) and its variants, have been
shown to converge well in many scenarios. However, these methods require
clients to upload their local updates to the server in a synchronous manner,
which can be slow and unreliable in realistic FL settings. To address this
issue, researchers have developed asynchronous FL methods that allow clients to
continue training on their local data using a stale global model. However, most
of these methods simply aggregate all of the received updates without
considering their relative contributions, which can slow down convergence. In
this paper, we propose a contribution-aware asynchronous FL method that takes
into account the staleness and statistical heterogeneity of the received
updates. Our method dynamically adjusts the contribution of each update based
on these factors, which can speed up convergence compared to existing methods.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10992" title="Abstract">arXiv:2402.10992</a> [<a href="/pdf/2402.10992" title="Download PDF">pdf</a>, <a href="/ps/2402.10992" title="Download PostScript">ps</a>, <a href="/format/2402.10992" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> &quot;Understanding AI&quot;: Semantic Grounding in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyre%2C+H">Holger Lyre</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Do LLMs understand the meaning of the texts they generate? Do they possess a
semantic grounding? And how could we understand whether and what they
understand? I start the paper with the observation that we have recently
witnessed a generative turn in AI, since generative models, including LLMs, are
key for self-supervised learning. To assess the question of semantic grounding,
I distinguish and discuss five methodological ways. The most promising way is
to apply core assumptions of theories of meaning in philosophy of mind and
language to LLMs. Grounding proves to be a gradual affair with a
three-dimensional distinction between functional, social and causal grounding.
LLMs show basic evidence in all three dimensions. A strong argument is that
LLMs develop world models. Hence, LLMs are neither stochastic parrots nor
semantic zombies, but already understand the language they generate, at least
in an elementary sense.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10998" title="Abstract">arXiv:2402.10998</a> [<a href="/pdf/2402.10998" title="Download PDF">pdf</a>, <a href="/format/2402.10998" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Provably Safe Neural Network Controllers via Differential Dynamic Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Teuber%2C+S">Samuel Teuber</a>, 
<a href="/search/cs?searchtype=author&query=Mitsch%2C+S">Stefan Mitsch</a>, 
<a href="/search/cs?searchtype=author&query=Platzer%2C+A">Andr&#xe9; Platzer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 42 pages (main paper has 19 pages), 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">While neural networks (NNs) have a large potential as goal-oriented
controllers for Cyber-Physical Systems, verifying the safety of neural network
based control systems (NNCSs) poses significant challenges for the practical
use of NNs -- especially when safety is needed for unbounded time horizons. One
reason for this is the intractability of NN and hybrid system analysis. We
introduce VerSAILLE (Verifiably Safe AI via Logically Linked Envelopes): The
first approach for the combination of differential dynamic logic (dL) and NN
verification. By joining forces, we can exploit the efficiency of NN
verification tools while retaining the rigor of dL. We reflect a safety proof
for a controller envelope in an NN to prove the safety of concrete NNCS on an
infinite-time horizon. The NN verification properties resulting from VerSAILLE
typically require nonlinear arithmetic while efficient NN verification tools
merely support linear arithmetic. To overcome this divide, we present Mosaic:
The first sound and complete verification approach for polynomial real
arithmetic properties on piece-wise linear NNs. Mosaic lifts off-the-shelf
tools for linear properties to the nonlinear setting. An evaluation on case
studies, including adaptive cruise control and airborne collision avoidance,
demonstrates the versatility of VerSAILLE and Mosaic: It supports the
certification of infinite-time horizon safety and the exhaustive enumeration of
counterexample regions while significantly outperforming State-of-the-Art tools
in closed-loop NNV.
</p>
</div>
</dd>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10999" title="Abstract">arXiv:2402.10999</a> [<a href="/pdf/2402.10999" title="Download PDF">pdf</a>, <a href="/ps/2402.10999" title="Download PostScript">ps</a>, <a href="/format/2402.10999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis and Mortality Prediction using Multiclass Classification for  Older Adults with Type 2 Diabetes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Desure%2C+R">Ruchika Desure</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+G+J">Gutha Jaya Krishna</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 146 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Designing proper treatment plans to manage diabetes requires health
practitioners to pay heed to the individuals remaining life along with the
comorbidities affecting them. Older adults with Type 2 Diabetes Mellitus (T2DM)
are prone to experience premature death or even hypoglycaemia. The structured
dataset utilized has 68 potential mortality predictors for 275,190 diabetic
U.S. military Veterans aged 65 years or older. A new target variable is
invented by combining the two original target variables. Outliers are handled
by discretizing the continuous variables. Categorical variables have been dummy
encoded. Class balancing is achieved by random under-sampling. A benchmark
regression model is built using Multinomial Logistic Regression with LASSO.
Chi-Squared and Information Gain are the filter-based feature selection
techniques utilized. Classifiers such as Multinomial Logistic Regression,
Random Forest, Extreme Gradient Boosting (XGBoost), and One-vs-Rest classifier
are employed to build various models. Contrary to expectations, all the models
have constantly underperformed. XGBoost has given the highest accuracy of 53.03
percent with Chi-Squared feature selection. All the models have consistently
shown an acceptable performance for Class 3 (remaining life is more than 10
years), significantly low for Class 1 (remaining life is up to 5 years), and
the worst for Class 2 (remaining life is more than 5 but up to 10 years).
Features analysis has deduced that almost all input variables are associated
with multiple target classes. The high dimensionality of the input data after
dummy encoding seems to have confused the models, leading to
misclassifications. The approach taken in this study is ineffective in
producing a high-performing predictive model but lays a foundation as this
problem has never been viewed from a multiclass classification perspective.
</p>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11000" title="Abstract">arXiv:2402.11000</a> [<a href="/pdf/2402.11000" title="Download PDF">pdf</a>, <a href="/format/2402.11000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ASGEA: Exploiting Logic Rules from Align-Subgraphs for Entity Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yangyifei Luo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Lingbing Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qian Li</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+W">Wenxuan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Z">Zhixin Cai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianxin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Ongoing work; 16 pages, 9 Tables, 8 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Entity alignment (EA) aims to identify entities across different knowledge
graphs that represent the same real-world objects. Recent embedding-based EA
methods have achieved state-of-the-art performance in EA yet faced
interpretability challenges as they purely rely on the embedding distance and
neglect the logic rules behind a pair of aligned entities. In this paper, we
propose the Align-Subgraph Entity Alignment (ASGEA) framework to exploit logic
rules from Align-Subgraphs. ASGEA uses anchor links as bridges to construct
Align-Subgraphs and spreads along the paths across KGs, which distinguishes it
from the embedding-based methods. Furthermore, we design an interpretable
Path-based Graph Neural Network, ASGNN, to effectively identify and integrate
the logic rules across KGs. We also introduce a node-level multi-modal
attention mechanism coupled with multi-modal enriched anchors to augment the
Align-Subgraph. Our experimental results demonstrate the superior performance
of ASGEA over the existing embedding-based methods in both EA and Multi-Modal
EA (MMEA) tasks. Our code will be available soon.
</p>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11001" title="Abstract">arXiv:2402.11001</a> [<a href="/pdf/2402.11001" title="Download PDF">pdf</a>, <a href="/ps/2402.11001" title="Download PostScript">ps</a>, <a href="/format/2402.11001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> idwMapper: An interactive and data-driven web mapping framework for  visualizing and sensing high-dimensional geospatial (big) data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sarigai%2C+S">Sarigai Sarigai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liping Yang</a>, 
<a href="/search/cs?searchtype=author&query=Slack%2C+K">Katie Slack</a>, 
<a href="/search/cs?searchtype=author&query=Lane%2C+K+M+D">K. Maria D. Lane</a>, 
<a href="/search/cs?searchtype=author&query=Buenemann%2C+M">Michaela Buenemann</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qiusheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Woodhull%2C+G">Gordon Woodhull</a>, 
<a href="/search/cs?searchtype=author&query=Driscol%2C+J">Joshua Driscol</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 11 figures, 3 open-source web map tools
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Databases (cs.DB)</span>

</div>
<p class="mathjax">We are surrounded by overwhelming big data, which brings substantial advances
but meanwhile poses many challenges. Geospatial big data comprises a big
portion of big data, and is essential and powerful for decision-making if being
utilized strategically. Volumes in size and high dimensions are two of the
major challenges that prevent strategic decision-making from (geospatial) big
data. Interactive map-based and geovisualization enabled web applications are
intuitive and useful to construct knowledge and reveal insights from
high-dimensional (geospatial) big data for actionable decision-making. We
propose an interactive and data-driven web mapping framework, named idwMapper,
for visualizing and sensing high dimensional geospatial (big) data in an
interactive and scalable manner. To demonstrate the wide applicability and
usefulness of our framework, we have applied our idwMapper framework to three
real-world case studies and implemented three corresponding web map
applications: iLit4GEE-AI, iWURanking, and iTRELISmap. We expect and hope the
three web maps demonstrated in different domains, from literature big data
analysis through world university ranking to scholar mapping, will provide a
good start and inspire researchers and practitioners in various domains to
apply our idwMapper to solve (or at least aid them in solving) their impactful
problems.
</p>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11004" title="Abstract">arXiv:2402.11004</a> [<a href="/pdf/2402.11004" title="Download PDF">pdf</a>, <a href="/format/2402.11004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Evolution of Statistical Induction Heads: In-Context Learning Markov  Chains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Edelman%2C+B+L">Benjamin L. Edelman</a>, 
<a href="/search/cs?searchtype=author&query=Edelman%2C+E">Ezra Edelman</a>, 
<a href="/search/cs?searchtype=author&query=Goel%2C+S">Surbhi Goel</a>, 
<a href="/search/cs?searchtype=author&query=Malach%2C+E">Eran Malach</a>, 
<a href="/search/cs?searchtype=author&query=Tsilivis%2C+N">Nikolaos Tsilivis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Large language models have the ability to generate text that mimics patterns
in their inputs. We introduce a simple Markov Chain sequence modeling task in
order to study how this in-context learning (ICL) capability emerges. In our
setting, each example is sampled from a Markov chain drawn from a prior
distribution over Markov chains. Transformers trained on this task form
\emph{statistical induction heads} which compute accurate next-token
probabilities given the bigram statistics of the context. During the course of
training, models pass through multiple phases: after an initial stage in which
predictions are uniform, they learn to sub-optimally predict using in-context
single-token statistics (unigrams); then, there is a rapid phase transition to
the correct in-context bigram solution. We conduct an empirical and theoretical
investigation of this multi-phase process, showing how successful learning
results from the interaction between the transformer's layers, and uncovering
evidence that the presence of the simpler unigram solution may delay formation
of the final bigram solution. We examine how learning is affected by varying
the prior distribution over Markov chains, and consider the generalization of
our in-context learning of Markov chains (ICL-MC) task to $n$-grams for $n &gt;
2$.
</p>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11005" title="Abstract">arXiv:2402.11005</a> [<a href="/pdf/2402.11005" title="Download PDF">pdf</a>, <a href="/format/2402.11005" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Value Biases: How LLMs Deviate Towards the Ideal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sivaprasad%2C+S">Sarath Sivaprasad</a>, 
<a href="/search/cs?searchtype=author&query=Kaushik%2C+P">Pramod Kaushik</a>, 
<a href="/search/cs?searchtype=author&query=Abdelnabi%2C+S">Sahar Abdelnabi</a>, 
<a href="/search/cs?searchtype=author&query=Fritz%2C+M">Mario Fritz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large-Language-Models (LLMs) are deployed in a wide range of applications,
and their response has an increasing social impact. Understanding the
non-deliberate(ive) mechanism of LLMs in giving responses is essential in
explaining their performance and discerning their biases in real-world
applications. This is analogous to human studies, where such inadvertent
responses are referred to as sampling. We study this sampling of LLMs in light
of value bias and show that the sampling of LLMs tends to favour high-value
options. Value bias corresponds to this shift of response from the most likely
towards an ideal value represented in the LLM. In fact, this effect can be
reproduced even with new entities learnt via in-context prompting. We show that
this bias manifests in unexpected places and has implications on relevant
application scenarios, like choosing exemplars. The results show that value
bias is strong in LLMs across different categories, similar to the results
found in human studies.
</p>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11006" title="Abstract">arXiv:2402.11006</a> [<a href="/pdf/2402.11006" title="Download PDF">pdf</a>, <a href="/format/2402.11006" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Detection and Analysis of Data Practices Using A Real-World  Corpus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Srinath%2C+M">Mukund Srinath</a>, 
<a href="/search/cs?searchtype=author&query=Venkit%2C+P">Pranav Venkit</a>, 
<a href="/search/cs?searchtype=author&query=Badillo%2C+M">Maria Badillo</a>, 
<a href="/search/cs?searchtype=author&query=Schaub%2C+F">Florian Schaub</a>, 
<a href="/search/cs?searchtype=author&query=Giles%2C+C+L">C. Lee Giles</a>, 
<a href="/search/cs?searchtype=author&query=Wilson%2C+S">Shomir Wilson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Privacy policies are crucial for informing users about data practices, yet
their length and complexity often deter users from reading them. In this paper,
we propose an automated approach to identify and visualize data practices
within privacy policies at different levels of detail. Leveraging crowd-sourced
annotations from the ToS;DR platform, we experiment with various methods to
match policy excerpts with predefined data practice descriptions. We further
conduct a case study to evaluate our approach on a real-world policy,
demonstrating its effectiveness in simplifying complex policies. Experiments
show that our approach accurately matches data practice descriptions with
policy excerpts, facilitating the presentation of simplified privacy
information to users.
</p>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11025" title="Abstract">arXiv:2402.11025</a> [<a href="/pdf/2402.11025" title="Download PDF">pdf</a>, <a href="/format/2402.11025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Bayesian Neural Networks with Sparse Subspace Variational  Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junbo Li</a>, 
<a href="/search/cs?searchtype=author&query=Miao%2C+Z">Zichen Miao</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Q">Qiang Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruqi Zhang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Published at International Conference on Learning Representations
  (ICLR) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Bayesian neural networks (BNNs) offer uncertainty quantification but come
with the downside of substantially increased training and inference costs.
Sparse BNNs have been investigated for efficient inference, typically by either
slowly introducing sparsity throughout the training or by post-training
compression of dense BNNs. The dilemma of how to cut down massive training
costs remains, particularly given the requirement to learn about the
uncertainty. To solve this challenge, we introduce Sparse Subspace Variational
Inference (SSVI), the first fully sparse BNN framework that maintains a
consistently highly sparse Bayesian model throughout the training and inference
phases. Starting from a randomly initialized low-dimensional sparse subspace,
our approach alternately optimizes the sparse subspace basis selection and its
associated parameters. While basis selection is characterized as a
non-differentiable problem, we approximate the optimal solution with a
removal-and-addition strategy, guided by novel criteria based on weight
distribution statistics. Our extensive experiments show that SSVI sets new
benchmarks in crafting sparse BNNs, achieving, for instance, a 10-20x
compression in model size with under 3\% performance drop, and up to 20x FLOPs
reduction during training compared with dense VI training. Remarkably, SSVI
also demonstrates enhanced robustness to hyperparameters, reducing the need for
intricate tuning in VI and occasionally even surpassing VI-trained dense BNNs
on both accuracy and uncertainty metrics.
</p>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11028" title="Abstract">arXiv:2402.11028</a> [<a href="/pdf/2402.11028" title="Download PDF">pdf</a>, <a href="/format/2402.11028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incremental Topological Ordering and Cycle Detection with Predictions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McCauley%2C+S">Samuel McCauley</a>, 
<a href="/search/cs?searchtype=author&query=Moseley%2C+B">Benjamin Moseley</a>, 
<a href="/search/cs?searchtype=author&query=Niaparast%2C+A">Aidin Niaparast</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Shikha Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">This paper leverages the framework of algorithms-with-predictions to design
data structures for two fundamental dynamic graph problems: incremental
topological ordering and cycle detection. In these problems, the input is a
directed graph on $n$ nodes, and the $m$ edges arrive one by one. The data
structure must maintain a topological ordering of the vertices at all times and
detect if the newly inserted edge creates a cycle. The theoretically best
worst-case algorithms for these problems have high update cost (polynomial in
$n$ and $m$). In practice, greedy heuristics (that recompute the solution from
scratch each time) perform well but can have high update cost in the worst
case.
<br />In this paper, we bridge this gap by leveraging predictions to design a
learned new data structure for the problems. Our data structure guarantees
consistency, robustness, and smoothness with respect to predictions -- that is,
it has the best possible running time under perfect predictions, never performs
worse than the best-known worst-case methods, and its running time degrades
smoothly with the prediction error. Moreover, we demonstrate empirically that
predictions, learned from a very small training dataset, are sufficient to
provide significant speed-ups on real datasets.
</p>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11034" title="Abstract">arXiv:2402.11034</a> [<a href="/pdf/2402.11034" title="Download PDF">pdf</a>, <a href="/format/2402.11034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal  Question-Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Meem%2C+J+A">Jannat Ara Meem</a>, 
<a href="/search/cs?searchtype=author&query=Rashid%2C+M+S">Muhammad Shihab Rashid</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yue Dong</a>, 
<a href="/search/cs?searchtype=author&query=Hristidis%2C+V">Vagelis Hristidis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Existing work on Temporal Question Answering (TQA) has predominantly focused
on questions anchored to specific timestamps or events (e.g. "Who was the US
president in 1970?"). Little work has studied questions whose temporal context
is relative to the present time (e.g. "Who was the previous US president?"). We
refer to this problem as Present-Anchored Temporal QA (PATQA). PATQA poses
unique challenges: (1) large language models (LLMs) may have outdated
knowledge, (2) complex temporal relationships (e.g. 'before', 'previous') are
hard to reason, (3) multi-hop reasoning may be required, and (4) the gold
answers of benchmarks must be continuously updated. To address these
challenges, we introduce the PAT-Questions benchmark, which includes single and
multi-hop temporal questions. The answers in PAT-Questions can be automatically
refreshed by re-running SPARQL queries on a knowledge graph, if available. We
evaluate several state-of-the-art LLMs and a SOTA temporal reasoning model
(TEMPREASON-T5) on PAT-Questions through direct prompting and
retrieval-augmented generation (RAG). The results highlight the limitations of
existing solutions in PATQA and motivate the need for new methods to improve
PATQA reasoning capabilities.
</p>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11035" title="Abstract">arXiv:2402.11035</a> [<a href="/pdf/2402.11035" title="Download PDF">pdf</a>, <a href="/format/2402.11035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Retrieval-Augmented Generation: Is Dense Passage Retrieval Retrieving?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reichman%2C+B">Benjamin Reichman</a>, 
<a href="/search/cs?searchtype=author&query=Heck%2C+L">Larry Heck</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Dense passage retrieval (DPR) is the first step in the retrieval augmented
generation (RAG) paradigm for improving the performance of large language
models (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of
the embeddings between queries and relevant textual data. A deeper
understanding of DPR fine-tuning will be required to fundamentally unlock the
full potential of this approach. In this work, we explore DPR-trained models
mechanistically by using a combination of probing, layer activation analysis,
and model editing. Our experiments show that DPR training decentralizes how
knowledge is stored in the network, creating multiple access pathways to the
same information. We also uncover a limitation in this training style: the
internal knowledge of the pre-trained model bounds what the retrieval model can
retrieve. These findings suggest a few possible directions for dense retrieval:
(1) expose the DPR training process to more knowledge so more can be
decentralized, (2) inject facts as decentralized representations, (3) model and
incorporate knowledge uncertainty in the retrieval process, and (4) directly
map internal model knowledge to a knowledge base.
</p>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11036" title="Abstract">arXiv:2402.11036</a> [<a href="/pdf/2402.11036" title="Download PDF">pdf</a>, <a href="/format/2402.11036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Occlusion Resilient 3D Human Pose Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roy%2C+S+K">Soumava Kumar Roy</a>, 
<a href="/search/cs?searchtype=author&query=Badanin%2C+I">Ilia Badanin</a>, 
<a href="/search/cs?searchtype=author&query=Honari%2C+S">Sina Honari</a>, 
<a href="/search/cs?searchtype=author&query=Fua%2C+P">Pascal Fua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Occlusions remain one of the key challenges in 3D body pose estimation from
single-camera video sequences. Temporal consistency has been extensively used
to mitigate their impact but the existing algorithms in the literature do not
explicitly model them.
<br />Here, we apply this by representing the deforming body as a spatio-temporal
graph. We then introduce a refinement network that performs graph convolutions
over this graph to output 3D poses. To ensure robustness to occlusions, we
train this network with a set of binary masks that we use to disable some of
the edges as in drop-out techniques.
<br />In effect, we simulate the fact that some joints can be hidden for periods of
time and train the network to be immune to that. We demonstrate the
effectiveness of this approach compared to state-of-the-art techniques that
infer poses from single-camera sequences.
</p>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11039" title="Abstract">arXiv:2402.11039</a> [<a href="/pdf/2402.11039" title="Download PDF">pdf</a>, <a href="/format/2402.11039" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robustness to Subpopulation Shift with Domain Label Noise via  Regularized Annotation of Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stromberg%2C+N">Nathan Stromberg</a>, 
<a href="/search/cs?searchtype=author&query=Ayyagari%2C+R">Rohan Ayyagari</a>, 
<a href="/search/cs?searchtype=author&query=Welfert%2C+M">Monica Welfert</a>, 
<a href="/search/cs?searchtype=author&query=Koyejo%2C+S">Sanmi Koyejo</a>, 
<a href="/search/cs?searchtype=author&query=Sankar%2C+L">Lalitha Sankar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Existing methods for last layer retraining that aim to optimize worst-group
accuracy (WGA) rely heavily on well-annotated groups in the training data. We
show, both in theory and practice, that annotation-based data augmentations
using either downsampling or upweighting for WGA are susceptible to domain
annotation noise, and in high-noise regimes approach the WGA of a model trained
with vanilla empirical risk minimization. We introduce Regularized Annotation
of Domains (RAD) in order to train robust last layer classifiers without the
need for explicit domain annotations. Our results show that RAD is competitive
with other recently proposed domain annotation-free techniques. Most
importantly, RAD outperforms state-of-the-art annotation-reliant methods even
with only 5% noise in the training data for several publicly available
datasets.
</p>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11040" title="Abstract">arXiv:2402.11040</a> [<a href="/pdf/2402.11040" title="Download PDF">pdf</a>, <a href="/format/2402.11040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Surpassing legacy approaches and human intelligence with hybrid single-  and multi-objective Reinforcement Learning-based optimization and  interpretable AI to enable the economic operation of the US nuclear fleet
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seurin%2C+P">Paul Seurin</a>, 
<a href="/search/cs?searchtype=author&query=Shirvan%2C+K">Koroush Shirvan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Machine Learning (cs.LG); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">The nuclear sector represents the primary source of carbon-free energy in the
United States. Nevertheless, existing nuclear power plants face the threat of
early shutdowns due to their inability to compete economically against
alternatives such as gas power plants. Optimizing the fuel cycle cost through
the optimization of core loading patterns is one approach to addressing this
lack of competitiveness. However, this optimization task involves multiple
objectives and constraints, resulting in a vast number of candidate solutions
that cannot be explicitly solved. While stochastic optimization (SO)
methodologies are utilized by various nuclear utilities and vendors for fuel
cycle reload design, manual design remains the preferred approach. To advance
the state-of-the-art in core reload patterns, we have developed methods based
on Deep Reinforcement Learning. Previous research has laid the groundwork for
this approach and demonstrated its ability to discover high-quality patterns
within a reasonable timeframe. However, there is a need for comparison against
legacy methods to demonstrate its utility in a single-objective setting. While
RL methods have shown superiority in multi-objective settings, they have not
yet been applied to address the competitiveness issue effectively. In this
paper, we rigorously compare our RL-based approach against the most commonly
used SO-based methods, namely Genetic Algorithm (GA), Simulated Annealing (SA),
and Tabu Search (TS). Subsequently, we introduce a new hybrid paradigm to
devise innovative designs, resulting in economic gains ranging from 2.8 to 3.3
million dollars per year per plant. This development leverages interpretable
AI, enabling improved algorithmic efficiency by making black-box optimizations
interpretable. Future work will focus on scaling this method to address a
broader range of core designs.
</p>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11041" title="Abstract">arXiv:2402.11041</a> [<a href="/pdf/2402.11041" title="Download PDF">pdf</a>, <a href="/format/2402.11041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How good are my search strings? Reflections on using an existing review  as a quasi-gold standard
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tran%2C+H+K+V">Huynh Khanh Vi Tran</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%B6rstler%2C+J">J&#xfc;rgen B&#xf6;rstler</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+N+B">Nauman Bin Ali</a>, 
<a href="/search/cs?searchtype=author&query=Unterkalmsteiner%2C+M">Michael Unterkalmsteiner</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> e Informatica Softw. Eng. J. 16(1) (2022)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Background: Systematic literature studies (SLS) have become a core research
methodology in Evidence-based Software Engineering (EBSE). Search completeness,
ie, finding all relevant papers on the topic of interest, has been recognized
as one of the most commonly discussed validity issues of SLSs. Aim: This study
aims at raising awareness on the issues related to search string construction
and on search validation using a quasi-gold standard (QGS). Furthermore, we aim
at providing guidelines for search string validation. Method: We use a recently
completed tertiary study as a case and complement our findings with the
observations from other researchers studying and advancing EBSE. Results: We
found that the issue of assessing QGS quality has not seen much attention in
the literature, and the validation of automated searches in SLSs could be
improved. Hence, we propose to extend the current search validation approach by
the additional analysis step of the automated search validation results and
provide recommendations for the QGS construction. Conclusion: In this paper, we
report on new issues which could affect search completeness in SLSs.
Furthermore, the proposed guideline and recommendations could help researchers
implement a more reliable search strategy in their SLSs.
</p>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11047" title="Abstract">arXiv:2402.11047</a> [<a href="/pdf/2402.11047" title="Download PDF">pdf</a>, <a href="/format/2402.11047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Low-Dissipation and Scalable GEMM Accelerator with Silicon Nitride  Photonics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karempudi%2C+V+S+P">Venkata Sai Praneeth Karempudi</a>, 
<a href="/search/cs?searchtype=author&query=Vatsavai%2C+S+S">Sairam Sri Vatsavai</a>, 
<a href="/search/cs?searchtype=author&query=Thakkar%2C+I">Ishan Thakkar</a>, 
<a href="/search/cs?searchtype=author&query=Alo%2C+O+A">Oluwaseun Adewunmi Alo</a>, 
<a href="/search/cs?searchtype=author&query=Hastings%2C+J+T">Jeffrey Todd Hastings</a>, 
<a href="/search/cs?searchtype=author&query=Woods%2C+J+S">Justin Scott Woods</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To Appear at ISQED 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Emerging Technologies (cs.ET); Performance (cs.PF); Optics (physics.optics)

</div>
<p class="mathjax">Over the past few years, several microring resonator (MRR)-based analog
photonic architectures have been proposed to accelerate general matrix-matrix
multiplications (GEMMs), which are found in abundance in deep learning
workloads.These architectures have dramatically grown in popularity because
they offer exceptional throughput and energy efficiency compared to their
electronic counterparts. However, such architectures, due to their traditional
realization based on the silicon-on-insulator (SOI) material platform, face two
shortcomings. First, the high-index contrast of the SOI platform incurs high
scattering losses, which mandates the provisioning of high optical input
power.Second, SOI waveguides are susceptible to two-photon absorption, which
can incur substantial optical signal losses at moderate-to-high signal fan-in.
These shortcomings have severely detrimental effects on the achievable
parallelism, throughput, and energy efficiency of SOI MRR-based GEMM
accelerators. To address these shortcomings, we present a novel Silicon Nitride
(SiN)-Based Photonic GEMM Accelerator called SiNPhAR. SiNPhAR architecture
employs SiN-based active and passive devices to implement analog GEMM
functions. Since the SiN material exhibits lower index contrast and no TPA, the
optical signal losses in our SiNPhAR architecture are very low. This advantage
significantly enhances the achievable processing parallelism, throughput, and
energy efficiency of SiNPhAR architecture, compared to SOI-based photonic GEMM
accelerators from prior work. We quantify and compare these benefits of SiNPhAR
architecture via our cross-layer evaluation for a benchmark workload comprising
four modern deep neural network models. From the system-level performance
analysis, SiNPhAR demonstrates at least 1.7x better throughput FPS while
consuming at least 2.8x better energy efficiency (FPS/W) than prior SOI-based
GEMM accelerators.
</p>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11048" title="Abstract">arXiv:2402.11048</a> [<a href="/pdf/2402.11048" title="Download PDF">pdf</a>, <a href="/format/2402.11048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards identifying and minimizing customer-facing documentation debt
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Silva%2C+L">Lakmal Silva</a>, 
<a href="/search/cs?searchtype=author&query=Unterkalmsteiner%2C+M">Michael Unterkalmsteiner</a>, 
<a href="/search/cs?searchtype=author&query=Wnuk%2C+K">Krzysztof Wnuk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> TechDebt@ICSE 2023: 72-81
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Software documentation often struggles to catch up with the pace of software
evolution. The lack of correct, complete, and up-to-date documentation results
in an increasing number of documentation defects which could introduce delays
in integrating software systems. In our previous study on a bug analysis tool
called MultiDimEr, we provided evidence that documentation-related defects
contribute to many bug reports. First, we want to identify documentation defect
types contributing to documentation defects, thereby identifying documentation
debt. Secondly, we aim to find pragmatic solutions to minimize most common
documentation defects to pay off the documentation debt in the long run. We
investigated documentation defects related to an industrial software system.
First, we looked at different documentation types and associated bug reports.
We categorized the defects according to an existing documentation defect
taxonomy. Based on a sample of 101 defects, we found that most defects are
caused by documentation defects falling into the Information Content (What)
category (86). Within this category, the documentation defect types Erroneous
code examples (23), Missing documentation (35), and Outdated content (19)
contributed to most of the documentation defects. We propose to adapt two
solutions to mitigate these types of documentation defects. In practice,
documentation debt can easily go undetected since a large share of resources
and focus is dedicated to delivering high-quality software. We suggest adapting
two main solutions to tackle documentation debt by implementing (i) Dynamic
Documentation Generation (DDG) and/or (ii) Automated Documentation Testing
(ADT), which are both based on defining a single and robust information source
for documentation.
</p>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11050" title="Abstract">arXiv:2402.11050</a> [<a href="/pdf/2402.11050" title="Download PDF">pdf</a>, <a href="/format/2402.11050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Constellation Multiple Access for Beyond 5G Wireless Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shakya%2C+I+L">Indu L. Shakya</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+F+H">Falah H. Ali</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 6 figures, Submission to an IEEE Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">We propose a novel nonorthogonal multiple access (NOMA) scheme referred as
adaptive constellation multiple access (ACMA) which addresses key limitations
of existing NOMA schemes for beyond 5G wireless systems. Unlike the latter,
that are often constrained in choices of allocation of power, modulations and
phases to allow enough separation of clusters from users combined signals, ACMA
is power, modulation and phase agnostic forming unified constellations instead
where distances of all possible neighbouring points are optimized. It includes
an algorithm at basestation (BS) calculating phase offsets for users signals
such that, when combined, it gives best minimum Euclidean distance of points
from all possibilities. The BS adaptively changes the phase offsets whenever
system parameters change. We also propose an enhanced receiver using a modified
maximum likelihood (MML) method that dynamically exploits information from the
BS to blindly estimate correct phase offsets and exploit them to enhance data
rate and error performances. Superiority of this scheme, which may also be
referred to as AC NOMA, is verified through extensive analyses and simulations.
</p>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11051" title="Abstract">arXiv:2402.11051</a> [<a href="/pdf/2402.11051" title="Download PDF">pdf</a>, <a href="/format/2402.11051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models Fall Short: Understanding Complex Relationships in  Detective Narratives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Runcong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qinglin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hainiu Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiazheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yuxiang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yulan He</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+L">Lin Gui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Existing datasets for narrative understanding often fail to represent the
complexity and uncertainty of relationships in real-life social scenarios. To
address this gap, we introduce a new benchmark, Conan, designed for extracting
and analysing intricate character relation graphs from detective narratives.
Specifically, we designed hierarchical relationship categories and manually
extracted and annotated role-oriented relationships from the perspectives of
various characters, incorporating both public relationships known to most
characters and secret ones known to only a few. Our experiments with advanced
Large Language Models (LLMs) like GPT-3.5, GPT-4, and Llama2 reveal their
limitations in inferencing complex relationships and handling longer
narratives. The combination of the Conan dataset and our pipeline strategy is
geared towards understanding the ability of LLMs to comprehend nuanced
relational dynamics in narrative contexts.
</p>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11054" title="Abstract">arXiv:2402.11054</a> [<a href="/pdf/2402.11054" title="Download PDF">pdf</a>, <a href="/format/2402.11054" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Resource Allocation in Mobile Networks: A Decision Model Of Jockeying in  Queues
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kiggundu%2C+A">Anthony Kiggundu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bin Han</a>, 
<a href="/search/cs?searchtype=author&query=Krummacker%2C+D">Dennis Krummacker</a>, 
<a href="/search/cs?searchtype=author&query=Schotten%2C+H+D">Hans D. Schotten</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to the 2024 EuCNC &amp; 6G Summit
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Application specific network slices offered by different vendors is one of
those approaches proposed to bridge the gap between the demand and supply of
resources in next generation communication networks.
<br />We present a decision model as an empirical study that associates different
slice configurations to queues in a multi-vendor setting such that consumers
continuously assess their preferences and make rational decisions that guide
the optimal usage of the resource pools. The model is based on the findings
from our developmental tooling which provides measures about the sensitivity to
variations in buffer parameters on the impatient customer's decision to jockey
and the frequency of this behavior. We analytically express for these
sensitivities to formulate for approximations of the number of times a task is
switched from one queue to another and validate the decision model against
empirical data from the Monte Carlo simulation.
</p>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11057" title="Abstract">arXiv:2402.11057</a> [<a href="/pdf/2402.11057" title="Download PDF">pdf</a>, <a href="/format/2402.11057" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are you Struggling? Dataset and Baselines for Struggle Determination in  Assembly Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shijia Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wray%2C+M">Michael Wray</a>, 
<a href="/search/cs?searchtype=author&query=Sullivan%2C+B">Brian Sullivan</a>, 
<a href="/search/cs?searchtype=author&query=Ludwig%2C+C">Casimir Ludwig</a>, 
<a href="/search/cs?searchtype=author&query=Gilchrist%2C+I">Iain Gilchrist</a>, 
<a href="/search/cs?searchtype=author&query=Mayol-Cuevas%2C+W">Walterio Mayol-Cuevas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Determining when people are struggling from video enables a finer-grained
understanding of actions and opens opportunities for building intelligent
support visual interfaces. In this paper, we present a new dataset with three
assembly activities and corresponding performance baselines for the
determination of struggle from video. Three real-world problem-solving
activities including assembling plumbing pipes (Pipes-Struggle), pitching
camping tents (Tent-Struggle) and solving the Tower of Hanoi puzzle
(Tower-Struggle) are introduced. Video segments were scored w.r.t. the level of
struggle as perceived by annotators using a forced choice 4-point scale. Each
video segment was annotated by a single expert annotator in addition to
crowd-sourced annotations. The dataset is the first struggle annotation dataset
and contains 5.1 hours of video and 725,100 frames from 73 participants in
total. We evaluate three decision-making tasks: struggle classification,
struggle level regression, and struggle label distribution learning. We provide
baseline results for each of the tasks utilising several mainstream deep neural
networks, along with an ablation study and visualisation of results. Our work
is motivated toward assistive systems that analyze struggle, support users
during manual activities and encourage learning, as well as other video
understanding competencies.
</p>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11058" title="Abstract">arXiv:2402.11058</a> [<a href="/pdf/2402.11058" title="Download PDF">pdf</a>, <a href="/format/2402.11058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> II-MMR: Identifying and Improving Multi-modal Multi-hop Reasoning in  Visual Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kil%2C+J">Jihyung Kil</a>, 
<a href="/search/cs?searchtype=author&query=Tavazoee%2C+F">Farideh Tavazoee</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dongyeop Kang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Joo-Kyung Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Visual Question Answering (VQA) often involves diverse reasoning scenarios
across Vision and Language (V&amp;L). Most prior VQA studies, however, have merely
focused on assessing the model's overall accuracy without evaluating it on
different reasoning cases. Furthermore, some recent works observe that
conventional Chain-of-Thought (CoT) prompting fails to generate effective
reasoning for VQA, especially for complex scenarios requiring multi-hop
reasoning. In this paper, we propose II-MMR, a novel idea to identify and
improve multi-modal multi-hop reasoning in VQA. In specific, II-MMR takes a VQA
question with an image and finds a reasoning path to reach its answer using two
novel language promptings: (i) answer prediction-guided CoT prompt, or (ii)
knowledge triplet-guided prompt. II-MMR then analyzes this path to identify
different reasoning cases in current VQA benchmarks by estimating how many hops
and what types (i.e., visual or beyond-visual) of reasoning are required to
answer the question. On popular benchmarks including GQA and A-OKVQA, II-MMR
observes that most of their VQA questions are easy to answer, simply demanding
"single-hop" reasoning, whereas only a few questions require "multi-hop"
reasoning. Moreover, while the recent V&amp;L model struggles with such complex
multi-hop reasoning questions even using the traditional CoT method, II-MMR
shows its effectiveness across all reasoning cases in both zero-shot and
fine-tuning settings.
</p>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11060" title="Abstract">arXiv:2402.11060</a> [<a href="/pdf/2402.11060" title="Download PDF">pdf</a>, <a href="/format/2402.11060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Persona-DB: Efficient Large Language Model Personalization for Response  Prediction with Collaborative Data Refinement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Chenkai Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+K">Ke Yang</a>, 
<a href="/search/cs?searchtype=author&query=Reddy%2C+R+G">Revanth Gangi Reddy</a>, 
<a href="/search/cs?searchtype=author&query=Fung%2C+Y+R">Yi R. Fung</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+H+P">Hou Pong Chan</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+C">ChengXiang Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
<p class="mathjax">The increasing demand for personalized interactions with large language
models (LLMs) calls for the development of methodologies capable of accurately
and efficiently identifying user opinions and preferences. Retrieval
augmentation emerges as an effective strategy, as it can accommodate a vast
number of users without the costs from fine-tuning. Existing research, however,
has largely focused on enhancing the retrieval stage and devoted limited
exploration toward optimizing the representation of the database, a crucial
aspect for tasks such as personalization. In this work, we examine the problem
from a novel angle, focusing on how data can be better represented for more
efficient retrieval in the context of LLM customization. To tackle this
challenge, we introduce Persona-DB, a simple yet effective framework consisting
of a hierarchical construction process to improve generalization across task
contexts and collaborative refinement to effectively bridge knowledge gaps
among users. In the task of response forecasting, Persona-DB demonstrates
superior efficiency in maintaining accuracy with a significantly reduced
retrieval size, a critical advantage in scenarios with extensive histories or
limited context windows. Our experiments also indicate a marked improvement of
over 15% under cold-start scenarios, when users have extremely sparse data.
Furthermore, our analysis reveals the increasing importance of collaborative
knowledge as the retrieval capacity expands.
</p>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11061" title="Abstract">arXiv:2402.11061</a> [<a href="/pdf/2402.11061" title="Download PDF">pdf</a>, <a href="/format/2402.11061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chronicles of jockeying in queuing systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kiggundu%2C+A">Anthony Kiggundu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+B">Bin Han</a>, 
<a href="/search/cs?searchtype=author&query=Krummacker%2C+D">Dennis Krummacker</a>, 
<a href="/search/cs?searchtype=author&query=Schotten%2C+H+D">Hans D. Schotten</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working paper, under revision
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The relevance of studies in queuing theory in social systems has inspired its
adoption in other mainstream technologies with its application in distributed
and communication systems becoming an intense research domain. Considerable
work has been done regarding the application of the impatient queuing
phenomenon in distributed computing to achieve optimal resource sharing and
allocation for performance improvement. Generally, there are two types of
common impatient queuing behaviour that have been well studied, namely balking
and reneging, respectively. In this survey, we are interested in the third type
of impatience: jockeying, a phenomenon that draws origins from impatient
customers switching from one queue to another.
<br />This survey chronicles classical and latest efforts that labor to model and
exploit the jockeying behaviour in queuing systems, with a special focus on
those related to information and communication systems, especially in the
context of Multi-Access Edge Computing. We comparatively summarize the reviewed
literature regarding their methodologies, invoked models, and use cases.
</p>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11066" title="Abstract">arXiv:2402.11066</a> [<a href="/pdf/2402.11066" title="Download PDF">pdf</a>, <a href="/format/2402.11066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Financially Inclusive Credit Products Through Financial Time  Series Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bester%2C+T">Tristan Bester</a>, 
<a href="/search/cs?searchtype=author&query=Rosman%2C+B">Benjamin Rosman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 Pages, 9 Figures, Published in AAAI W5: AI in Finance for Social Impact
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Statistical Finance (q-fin.ST)

</div>
<p class="mathjax">Financial inclusion ensures that individuals have access to financial
products and services that meet their needs. As a key contributing factor to
economic growth and investment opportunity, financial inclusion increases
consumer spending and consequently business development. It has been shown that
institutions are more profitable when they provide marginalised social groups
access to financial services. Customer segmentation based on consumer
transaction data is a well-known strategy used to promote financial inclusion.
While the required data is available to modern institutions, the challenge
remains that segment annotations are usually difficult and/or expensive to
obtain. This prevents the usage of time series classification models for
customer segmentation based on domain expert knowledge. As a result, clustering
is an attractive alternative to partition customers into homogeneous groups
based on the spending behaviour encoded within their transaction data. In this
paper, we present a solution to one of the key challenges preventing modern
financial institutions from providing financially inclusive credit, savings and
insurance products: the inability to understand consumer financial behaviour,
and hence risk, without the introduction of restrictive conventional credit
scoring techniques. We present a novel time series clustering algorithm that
allows institutions to understand the financial behaviour of their customers.
This enables unique product offerings to be provided based on the needs of the
customer, without reliance on restrictive credit practices.
</p>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11068" title="Abstract">arXiv:2402.11068</a> [<a href="/pdf/2402.11068" title="Download PDF">pdf</a>, <a href="/format/2402.11068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging Causal Discovery and Large Language Models: A Comprehensive  Survey of Integrative Approaches and Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+G">Guangya Wan</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yuqi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+M">Mengxuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+Z">Zhixuan Chu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sheng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Causal discovery (CD) and Large Language Models (LLMs) represent two emerging
fields of study with significant implications for artificial intelligence.
Despite their distinct origins, CD focuses on uncovering cause-effect
relationships from data, and LLMs on processing and generating humanlike text,
the convergence of these domains offers novel insights and methodologies for
understanding complex systems. This paper presents a comprehensive survey of
the integration of LLMs, such as GPT4, into CD tasks. We systematically review
and compare existing approaches that leverage LLMs for various CD tasks and
highlight their innovative use of metadata and natural language to infer causal
structures. Our analysis reveals the strengths and potential of LLMs in both
enhancing traditional CD methods and as an imperfect expert, alongside the
challenges and limitations inherent in current practices. Furthermore, we
identify gaps in the literature and propose future research directions aimed at
harnessing the full potential of LLMs in causality research. To our knowledge,
this is the first survey to offer a unified and detailed examination of the
synergy between LLMs and CD, setting the stage for future advancements in the
field.
</p>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11073" title="Abstract">arXiv:2402.11073</a> [<a href="/pdf/2402.11073" title="Download PDF">pdf</a>, <a href="/format/2402.11073" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AFaCTA: Assisting the Annotation of Factual Claim Detection with  Reliable LLM Annotators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%2C+J">Jingwei Ni</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+M">Minjing Shi</a>, 
<a href="/search/cs?searchtype=author&query=Stammbach%2C+D">Dominik Stammbach</a>, 
<a href="/search/cs?searchtype=author&query=Sachan%2C+M">Mrinmaya Sachan</a>, 
<a href="/search/cs?searchtype=author&query=Ash%2C+E">Elliott Ash</a>, 
<a href="/search/cs?searchtype=author&query=Leippold%2C+M">Markus Leippold</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the rise of generative AI, automated fact-checking methods to combat
misinformation are becoming more and more important. However, factual claim
detection, the first step in a fact-checking pipeline, suffers from two key
issues that limit its scalability and generalizability: (1) inconsistency in
definitions of the task and what a claim is, and (2) the high cost of manual
annotation. To address (1), we review the definitions in related work and
propose a unifying definition of factual claims that focuses on verifiability.
To address (2), we introduce AFaCTA (Automatic Factual Claim deTection
Annotator), a novel framework that assists in the annotation of factual claims
with the help of large language models (LLMs). AFaCTA calibrates its annotation
confidence with consistency along three predefined reasoning paths. Extensive
evaluation and experiments in the domain of political speech reveal that AFaCTA
can efficiently assist experts in annotating factual claims and training
high-quality classifiers, and can work with or without expert supervision. Our
analyses also result in PoliClaim, a comprehensive claim detection dataset
spanning diverse political topics.
</p>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11078" title="Abstract">arXiv:2402.11078</a> [<a href="/pdf/2402.11078" title="Download PDF">pdf</a>, <a href="/format/2402.11078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Editing by Pure Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gangadhar%2C+G">Govind Gangadhar</a>, 
<a href="/search/cs?searchtype=author&query=Stratos%2C+K">Karl Stratos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Fine-tuning is dismissed as not effective for model editing due to its poor
performance compared to more specialized methods. However, fine-tuning is
simple, agnostic to the architectural details of the model being edited, and
able to leverage ongoing advances in standard training methods (e.g., PEFT),
making it an appealing choice for a model editor. In this work, we show that
pure fine-tuning can be a viable approach to model editing. We propose a slight
modification of naive fine-tuning with two key ingredients. First, we optimize
the conditional likelihood rather than the full likelihood. Second, we augment
the data with random paraphrases and facts to encourage generalization and
locality. Our experiments on ZsRE and CounterFact show that this simple
modification allows fine-tuning to often match or outperform specialized
editors in the edit score.
</p>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11082" title="Abstract">arXiv:2402.11082</a> [<a href="/pdf/2402.11082" title="Download PDF">pdf</a>, <a href="/format/2402.11082" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The AI Security Pyramid of Pain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ward%2C+C+M">Chris M. Ward</a>, 
<a href="/search/cs?searchtype=author&query=Harguess%2C+J">Josh Harguess</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+J">Julia Tao</a>, 
<a href="/search/cs?searchtype=author&query=Christman%2C+D">Daniel Christman</a>, 
<a href="/search/cs?searchtype=author&query=Spicer%2C+P">Paul Spicer</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+M">Mike Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SPIE DCS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We introduce the AI Security Pyramid of Pain, a framework that adapts the
cybersecurity Pyramid of Pain to categorize and prioritize AI-specific threats.
This framework provides a structured approach to understanding and addressing
various levels of AI threats. Starting at the base, the pyramid emphasizes Data
Integrity, which is essential for the accuracy and reliability of datasets and
AI models, including their weights and parameters. Ensuring data integrity is
crucial, as it underpins the effectiveness of all AI-driven decisions and
operations. The next level, AI System Performance, focuses on MLOps-driven
metrics such as model drift, accuracy, and false positive rates. These metrics
are crucial for detecting potential security breaches, allowing for early
intervention and maintenance of AI system integrity. Advancing further, the
pyramid addresses the threat posed by Adversarial Tools, identifying and
neutralizing tools used by adversaries to target AI systems. This layer is key
to staying ahead of evolving attack methodologies. At the Adversarial Input
layer, the framework addresses the detection and mitigation of inputs designed
to deceive or exploit AI models. This includes techniques like adversarial
patterns and prompt injection attacks, which are increasingly used in
sophisticated attacks on AI systems. Data Provenance is the next critical
layer, ensuring the authenticity and lineage of data and models. This layer is
pivotal in preventing the use of compromised or biased data in AI systems. At
the apex is the tactics, techniques, and procedures (TTPs) layer, dealing with
the most complex and challenging aspects of AI security. This involves a deep
understanding and strategic approach to counter advanced AI-targeted attacks,
requiring comprehensive knowledge and planning.
</p>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11083" title="Abstract">arXiv:2402.11083</a> [<a href="/pdf/2402.11083" title="Download PDF">pdf</a>, <a href="/format/2402.11083" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VQAttack: Transferable Adversarial Attacks on Visual Question Answering  via Pre-trained Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Ziyi Yin</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+M">Muchao Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianrong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiaqi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Han Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jinghui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Ting Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+F">Fenglong Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 2024, 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Visual Question Answering (VQA) is a fundamental task in computer vision and
natural language process fields. Although the ``pre-training &amp; finetuning''
learning paradigm significantly improves the VQA performance, the adversarial
robustness of such a learning paradigm has not been explored. In this paper, we
delve into a new problem: using a pre-trained multimodal source model to create
adversarial image-text pairs and then transferring them to attack the target
VQA models. Correspondingly, we propose a novel VQAttack model, which can
iteratively generate both image and text perturbations with the designed
modules: the large language model (LLM)-enhanced image attack and the
cross-modal joint attack module. At each iteration, the LLM-enhanced image
attack module first optimizes the latent representation-based loss to generate
feature-level image perturbations. Then it incorporates an LLM to further
enhance the image perturbations by optimizing the designed masked answer
anti-recovery loss. The cross-modal joint attack module will be triggered at a
specific iteration, which updates the image and text perturbations
sequentially. Notably, the text perturbation updates are based on both the
learned gradients in the word embedding space and word synonym-based
substitution. Experimental results on two VQA datasets with five validated
models demonstrate the effectiveness of the proposed VQAttack in the
transferable attack setting, compared with state-of-the-art baselines. This
work reveals a significant blind spot in the ``pre-training &amp; fine-tuning''
paradigm on VQA tasks. Source codes will be released.
</p>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11084" title="Abstract">arXiv:2402.11084</a> [<a href="/pdf/2402.11084" title="Download PDF">pdf</a>, <a href="/ps/2402.11084" title="Download PostScript">ps</a>, <a href="/format/2402.11084" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Competition Complexity of Prophet Inequalities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brustle%2C+J">Johannes Brustle</a>, 
<a href="/search/cs?searchtype=author&query=Correa%2C+J">Jos&#xe9; Correa</a>, 
<a href="/search/cs?searchtype=author&query=D%C3%BCtting%2C+P">Paul D&#xfc;tting</a>, 
<a href="/search/cs?searchtype=author&query=Ezra%2C+T">Tomer Ezra</a>, 
<a href="/search/cs?searchtype=author&query=Feldman%2C+M">Michal Feldman</a>, 
<a href="/search/cs?searchtype=author&query=Verdugo%2C+V">Victor Verdugo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We study the classic single-choice prophet inequality problem through a
resource augmentation lens. Our goal is to bound the
$(1-\varepsilon)$-competition complexity of different types of online
algorithms. This metric asks for the smallest $k$ such that the expected value
of the online algorithm on $k$ copies of the original instance, is at least a
$(1-\varepsilon)$-approximation to the expected offline optimum on a single
copy.
<br />We show that block threshold algorithms, which set one threshold per copy,
are optimal and give a tight bound of $k = \Theta(\log \log 1/\varepsilon)$.
<br />This shows that block threshold algorithms approach the offline optimum
doubly-exponentially fast. For single threshold algorithms, we give a tight
bound of $k = \Theta(\log 1/\varepsilon)$, establishing an exponential gap
between block threshold algorithms and single threshold algorithms.
<br />Our model and results pave the way for exploring resource-augmented prophet
inequalities in combinatorial settings. In line with this, we present
preliminary findings for bipartite matching with one-sided vertex arrivals, as
well as in XOS combinatorial auctions. Our results have a natural competition
complexity interpretation in mechanism design and pricing applications.
</p>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11089" title="Abstract">arXiv:2402.11089</a> [<a href="/pdf/2402.11089" title="Download PDF">pdf</a>, <a href="/format/2402.11089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Male CEO and the Female Assistant: Probing Gender Biases in  Text-To-Image Models Through Paired Stereotype Test
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+Y">Yixin Wan</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Recent large-scale Text-To-Image (T2I) models such as DALLE-3 demonstrate
great potential in new applications, but also face unprecedented fairness
challenges. Prior studies revealed gender biases in single-person image
generation, but T2I model applications might require portraying two or more
people simultaneously. Potential biases in this setting remain unexplored,
leading to fairness-related risks in usage. To study these underlying facets of
gender biases in T2I models, we propose a novel Paired Stereotype Test (PST)
bias evaluation framework. PST prompts the model to generate two individuals in
the same image. They are described with two social identities that are
stereotypically associated with the opposite gender. Biases can then be
measured by the level of conformation to gender stereotypes in generated
images. Using PST, we evaluate DALLE-3 from 2 perspectives: biases in gendered
occupation and biases in organizational power. Despite seemingly fair or even
anti-stereotype single-person generations, PST still unveils gendered
occupational and power associations. Moreover, compared to single-person
settings, DALLE-3 generates noticeably more masculine figures under PST for
individuals with male-stereotypical identities. PST is therefore effective in
revealing underlying gender biases in DALLE-3 that single-person settings
cannot capture. Our findings reveal the complicated patterns of gender biases
in modern T2I models, further highlighting the critical fairness challenges in
multimodal generative systems.
</p>
</div>
</dd>
<dt><a name="item78">[78]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11091" title="Abstract">arXiv:2402.11091</a> [<a href="/pdf/2402.11091" title="Download PDF">pdf</a>, <a href="/format/2402.11091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Novel Multivariate Skew-Normal Mixture Model and Its Application in  Path-Planning for Very-Large-Scale Robotic Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+P">Pingping Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Estephan%2C+P">Peter Estephan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> American Control Conference (ACC) 2024, July 10 - 12, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">This paper addresses the path-planning challenge for very large-scale robotic
systems (VLSR) operating in complex and cluttered environments. VLSR systems
consist of numerous cooperative agents or robots working together autonomously.
Traditionally, many approaches for VLSR systems are developed based on Gaussian
mixture models (GMMs), where the GMMs represent agents' evolving spatial
distribution, serving as a macroscopic view of the system's state. However, our
recent research into VLSR systems has unveiled limitations in using GMMs to
represent agent distributions, especially in cluttered environments. To
overcome these limitations, we propose a novel model called the skew-normal
mixture model (SNMM) for representing agent distributions. Additionally, we
present a parameter learning algorithm designed to estimate the SNMM's
parameters using sample data. Furthermore, we develop two SNMM-based
path-planning algorithms to guide VLSR systems through complex and cluttered
environments. Our simulation results demonstrate the effectiveness and
superiority of these algorithms compared to GMM-based path-planning methods.
</p>
</div>
</dd>
<dt><a name="item79">[79]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11093" title="Abstract">arXiv:2402.11093</a> [<a href="/pdf/2402.11093" title="Download PDF">pdf</a>, <a href="/format/2402.11093" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modular Graph Extraction for Handwritten Circuit Diagram Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bayer%2C+J">Johannes Bayer</a>, 
<a href="/search/cs?searchtype=author&query=van+Waveren%2C+L">Leo van Waveren</a>, 
<a href="/search/cs?searchtype=author&query=Dengel%2C+A">Andreas Dengel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> As submitted to ICDAR24; 11 pages, 9 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">As digitization in engineering progressed, circuit diagrams (also referred to
as schematics) are typically developed and maintained in computer-aided
engineering (CAE) systems, thus allowing for automated verification, simulation
and further processing in downstream engineering steps. However, apart from
printed legacy schematics, hand-drawn circuit diagrams are still used today in
the educational domain, where they serve as an easily accessible mean for
trainees and students to learn drawing this type of diagrams. Furthermore,
hand-drawn schematics are typically used in examinations due to legal
constraints. In order to harness the capabilities of digital circuit
representations, automated means for extracting the electrical graph from
raster graphics are required.
<br />While respective approaches have been proposed in literature, they are
typically conducted on small or non-disclosed datasets. This paper describes a
modular end-to-end solution on a larger, public dataset, in which approaches
for the individual sub-tasks are evaluated to form a new baseline. These
sub-tasks include object detection (for electrical symbols and texts), binary
segmentation (drafter's stroke vs. background), handwritten character
recognition and orientation regression for electrical symbols and texts.
Furthermore, computer-vision graph assembly and rectification algorithms are
presented. All methods are integrated in a publicly available prototype.
</p>
</div>
</dd>
<dt><a name="item80">[80]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11094" title="Abstract">arXiv:2402.11094</a> [<a href="/pdf/2402.11094" title="Download PDF">pdf</a>, <a href="/format/2402.11094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Word Embeddings Revisited: Do LLMs Offer Something New?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Freestone%2C+M">Matthew Freestone</a>, 
<a href="/search/cs?searchtype=author&query=Santu%2C+S+K+K">Shubhra Kanti Karmaker Santu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Learning meaningful word embeddings is key to training a robust language
model. The recent rise of Large Language Models (LLMs) has provided us with
many new word/sentence/document embedding models. Although LLMs have shown
remarkable advancement in various NLP tasks, it is still unclear whether the
performance improvement is merely because of scale or whether underlying
embeddings they produce significantly differ from classical encoding models
like Sentence-BERT (SBERT) or Universal Sentence Encoder (USE). This paper
systematically investigates this issue by comparing classical word embedding
techniques against LLM-based word embeddings in terms of their latent vector
semantics. Our results show that LLMs tend to cluster semantically related
words more tightly than classical models. LLMs also yield higher average
accuracy on the Bigger Analogy Test Set (BATS) over classical methods. Finally,
some LLMs tend to produce word embeddings similar to SBERT, a relatively
lighter classical model.
</p>
</div>
</dd>
<dt><a name="item81">[81]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11095" title="Abstract">arXiv:2402.11095</a> [<a href="/pdf/2402.11095" title="Download PDF">pdf</a>, <a href="/format/2402.11095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GIM: Learning Generalizable Image Matcher From Internet Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xuelun Shen</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+Z">Zhipeng Cai</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+W">Wei Yin</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+M">Matthias M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zijun Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaixuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaozhi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cheng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR 2024 for spotlight presentation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Image matching is a fundamental computer vision problem. While learning-based
methods achieve state-of-the-art performance on existing benchmarks, they
generalize poorly to in-the-wild images. Such methods typically need to train
separate models for different scene types and are impractical when the scene
type is unknown in advance. One of the underlying problems is the limited
scalability of existing data construction pipelines, which limits the diversity
of standard image matching datasets. To address this problem, we propose GIM, a
self-training framework for learning a single generalizable model based on any
image matching architecture using internet videos, an abundant and diverse data
source. Given an architecture, GIM first trains it on standard domain-specific
datasets and then combines it with complementary matching methods to create
dense labels on nearby frames of novel videos. These labels are filtered by
robust fitting, and then enhanced by propagating them to distant frames. The
final model is trained on propagated data with strong augmentations. We also
propose ZEB, the first zero-shot evaluation benchmark for image matching. By
mixing data from diverse domains, ZEB can thoroughly assess the cross-domain
generalization performance of different methods. Applying GIM consistently
improves the zero-shot performance of 3 state-of-the-art image matching
architectures; with 50 hours of YouTube videos, the relative zero-shot
performance improves by 8.4%-18.1%. GIM also enables generalization to extreme
cross-domain data such as Bird Eye View (BEV) images of projected 3D point
clouds (Fig. 1(c)). More importantly, our single zero-shot model consistently
outperforms domain-specific baselines when evaluated on downstream tasks
inherent to their respective domains. The video presentation is available at
https://www.youtube.com/watch?v=FU_MJLD8LeY.
</p>
</div>
</dd>
<dt><a name="item82">[82]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11100" title="Abstract">arXiv:2402.11100</a> [<a href="/pdf/2402.11100" title="Download PDF">pdf</a>, <a href="/format/2402.11100" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When LLMs Meet Cunning Questions: A Fallacy Understanding Benchmark for  Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yinghui Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qingyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yuanzhen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shirong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yangning Li</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Hai-Tao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xuming Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recently, Large Language Models (LLMs) have made remarkable evolutions in
language understanding and generation. Following this, various benchmarks for
measuring all kinds of capabilities of LLMs have sprung up. In this paper, we
challenge the reasoning and understanding abilities of LLMs by proposing a
FaLlacy Understanding Benchmark (FLUB) containing cunning questions that are
easy for humans to understand but difficult for models to grasp. Specifically,
the cunning questions that FLUB focuses on mainly consist of the tricky,
humorous, and misleading questions collected from the real internet
environment. And we design three tasks with increasing difficulty in the FLUB
benchmark to evaluate the fallacy understanding ability of LLMs. Based on FLUB,
we investigate the performance of multiple representative and advanced LLMs,
reflecting our FLUB is challenging and worthy of more future study. Interesting
discoveries and valuable insights are achieved in our extensive experiments and
detailed analyses. We hope that our benchmark can encourage the community to
improve LLMs' ability to understand fallacies.
</p>
</div>
</dd>
<dt><a name="item83">[83]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11103" title="Abstract">arXiv:2402.11103</a> [<a href="/pdf/2402.11103" title="Download PDF">pdf</a>, <a href="/format/2402.11103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Learning Latent-Variable Representations of Microstructures by  Optimizing in Spatial Statistics Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hashemi%2C+S+S">Sayed Sajad Hashemi</a>, 
<a href="/search/cs?searchtype=author&query=Guerzhoy%2C+M">Michael Guerzhoy</a>, 
<a href="/search/cs?searchtype=author&query=Paulson%2C+N+H">Noah H. Paulson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Materials Science (cond-mat.mtrl-sci)

</div>
<p class="mathjax">In Materials Science, material development involves evaluating and optimizing
the internal structures of the material, generically referred to as
microstructures. Microstructures structure is stochastic, analogously to image
textures. A particular microstructure can be well characterized by its spatial
statistics, analogously to image texture being characterized by the response to
a Fourier-like filter bank. Material design would benefit from low-dimensional
representation of microstructures Paulson et al. (2017).
<br />In this work, we train a Variational Autoencoders (VAE) to produce
reconstructions of textures that preserve the spatial statistics of the
original texture, while not necessarily reconstructing the same image in data
space. We accomplish this by adding a differentiable term to the cost function
in order to minimize the distance between the original and the reconstruction
in spatial statistics space.
<br />Our experiments indicate that it is possible to train a VAE that minimizes
the distance in spatial statistics space between the original and the
reconstruction of synthetic images. In future work, we will apply the same
techniques to microstructures, with the goal of obtaining low-dimensional
representations of material microstructures.
</p>
</div>
</dd>
<dt><a name="item84">[84]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11104" title="Abstract">arXiv:2402.11104</a> [<a href="/pdf/2402.11104" title="Download PDF">pdf</a>, <a href="/format/2402.11104" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing Voting Rules with Elicited Incomplete Votes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Halpern%2C+D">Daniel Halpern</a>, 
<a href="/search/cs?searchtype=author&query=Hossain%2C+S">Safwan Hossain</a>, 
<a href="/search/cs?searchtype=author&query=Tucker-Foltz%2C+J">Jamie Tucker-Foltz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Motivated by the difficulty of specifying complete ordinal preferences over a
large set of $m$ candidates, we study voting rules that are computable by
querying voters about $t &lt; m$ candidates. Generalizing prior works that focused
on specific instances of this problem, our paper fully characterizes the set of
positional scoring rules that can be computed for any $1 \leq t &lt; m$, which
notably does not include plurality. We then extend this to show a similar
impossibility result for single transferable vote (elimination voting). These
negative results are information-theoretic and agnostic to the number of
queries. Finally, for scoring rules that are computable with limited-sized
queries, we give parameterized upper and lower bounds on the number of such
queries a deterministic or randomized algorithm must make to determine the
score-maximizing candidate. While there is no gap between our bounds for
deterministic algorithms, identifying the exact query complexity for randomized
algorithms is a challenging open problem, of which we solve one special case.
</p>
</div>
</dd>
<dt><a name="item85">[85]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11107" title="Abstract">arXiv:2402.11107</a> [<a href="/pdf/2402.11107" title="Download PDF">pdf</a>, <a href="/format/2402.11107" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic nowcast of the New Zealand greenhouse gas inventory
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jones%2C+M">Malcolm Jones</a>, 
<a href="/search/cs?searchtype=author&query=Chorley%2C+H">Hannah Chorley</a>, 
<a href="/search/cs?searchtype=author&query=Owen%2C+F">Flynn Owen</a>, 
<a href="/search/cs?searchtype=author&query=Hilder%2C+T">Tamsyn Hilder</a>, 
<a href="/search/cs?searchtype=author&query=Trowland%2C+H">Holly Trowland</a>, 
<a href="/search/cs?searchtype=author&query=Bracewell%2C+P">Paul Bracewell</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Environmental Modelling &amp; Software 167 (2023), 105745
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
<p class="mathjax">As efforts to mitigate the effects of climate change grow, reliable and
thorough reporting of greenhouse gas emissions are essential for measuring
progress towards international and domestic emissions reductions targets. New
Zealand's national emissions inventories are currently reported between 15 to
27 months out-of-date. We present a machine learning approach to nowcast
(dynamically estimate) national greenhouse gas emissions in New Zealand in
advance of the national emissions inventory's release, with just a two month
latency due to current data availability. Key findings include an estimated
0.2% decrease in national gross emissions since 2020 (as at July 2022). Our
study highlights the predictive power of a dynamic view of emissions intensive
activities. This methodology is a proof of concept that a machine learning
approach can make sub-annual estimates of national greenhouse gas emissions by
sector with a relatively low error that could be of value for policy makers.
</p>
</div>
</dd>
<dt><a name="item86">[86]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11109" title="Abstract">arXiv:2402.11109</a> [<a href="/pdf/2402.11109" title="Download PDF">pdf</a>, <a href="/format/2402.11109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Flexible Busy Time Scheduling on Heterogeneous Machines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Calinescu%2C+G">Gruia Calinescu</a>, 
<a href="/search/cs?searchtype=author&query=Davies%2C+S">Sami Davies</a>, 
<a href="/search/cs?searchtype=author&query=Khuller%2C+S">Samir Khuller</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shirley Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We study the online busy time scheduling model on heterogeneous machines. In
our setting, unit-length jobs arrive online with a deadline that is known to
the algorithm at the job's arrival time. An algorithm has access to machines,
each with different associated capacities and costs. The goal is to schedule
jobs on machines before their deadline, so that the total cost incurred by the
scheduling algorithm is minimized. Relatively little is known about online busy
time scheduling when machines are heterogeneous (i.e., have different costs and
capacities), despite this being the most practical model for clients using
cloud computing services. We make significant progress in understanding this
model by designing an 8-competitive algorithm for the problem on unit-length
jobs, and providing a lower bound on the competitive ratio of 2. We further
prove that our lower bound is tight in the natural setting when jobs have
agreeable deadlines.
</p>
</div>
</dd>
<dt><a name="item87">[87]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11110" title="Abstract">arXiv:2402.11110</a> [<a href="/pdf/2402.11110" title="Download PDF">pdf</a>, <a href="/ps/2402.11110" title="Download PostScript">ps</a>, <a href="/format/2402.11110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The weak relationship between ankle proprioception and gait speed after  stroke a robotic assessment study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Johnson%2C+C+A">Christopher A. Johnson</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+P">Piyashi Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Tapia%2C+R">Rubi Tapia</a>, 
<a href="/search/cs?searchtype=author&query=See%2C+J">Jill See</a>, 
<a href="/search/cs?searchtype=author&query=Dodakian%2C+L">Lucy Dodakian</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+V">Vicky Chan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P+T">Po T. Wang</a>, 
<a href="/search/cs?searchtype=author&query=Nenadic%2C+Z">Zoran Nenadic</a>, 
<a href="/search/cs?searchtype=author&query=Do%2C+A+H">An H. Do</a>, 
<a href="/search/cs?searchtype=author&query=Reinkensmeyer%2C+D+J">David J. Reinkensmeyer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Ankle proprioceptive deficits are common after stroke and occur independently
of ankle motor impairments. Despite this independence, some studies have found
that ankle proprioceptive deficits predict gait function, consistent with the
concept that somatosensory input plays a key role in gait control. Other
studies, however, have not found a relationship, possibly because of
variability in proprioception assessments. Robotic assessments of
proprioception offer improved consistency and sensitivity. Here we
relationships between ankle proprioception, ankle motor impairment, and gait
function after stroke using robotic assessments of ankle proprioception. We
quantified ankle proprioception using two different robotic tests (Joint
Position Reproduction and Crisscross) in 39 persons in the chronic phase of
stroke. We analyzed the extent to which these robotic proprioception measures
predicted gait speed, measured over a long distance (6-minute walk test) and a
short distance (10-meter walk test). We also studied the relationship between
robotic proprioception measures and lower extremity motor impairment,
quantified with measures of ankle strength, active range of motion, and the
lower extremity Fugl-Meyer exam. Impairment in ankle proprioception was present
in 87% of the participants. Ankle proprioceptive acuity measured with JPR was
weakly correlated with 6MWT gait speed (\r{ho} = -0.34, p = 0.039) but not
10mWT (\r{ho} = -0.29, p = 0.08). Ankle proprioceptive acuity was not
correlated with lower extremity motor impairment (p &gt; 0.2). These results
confirm the presence of a weak relationship between ankle proprioception and
gait after stroke that is independent of motor impairment.
</p>
</div>
</dd>
<dt><a name="item88">[88]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11112" title="Abstract">arXiv:2402.11112</a> [<a href="/pdf/2402.11112" title="Download PDF">pdf</a>, <a href="/format/2402.11112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Soft Covering and Decoupling with Relative Entropy Criterion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xingyi He</a>, 
<a href="/search/cs?searchtype=author&query=Atif%2C+T+A">Touheed Anwar Atif</a>, 
<a href="/search/cs?searchtype=author&query=Pradhan%2C+S+S">S. Sandeep Pradhan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">We propose quantum soft covering problems for fully quantum channels and
classical-quantum (CQ) channels using relative entropy as a criterion of
operator closeness. We prove covering lemmas by deriving one-shot bounds on the
rates in terms of smooth min-entropies and smooth max-divergences,
respectively. In the asymptotic regime, we show that for quantum channels, the
rate infimum defined as the logarithm of the minimum rank of the input state is
the coherent information between the reference and output state; for CQ
channels, the rate infimum defined as the logarithm of the minimum number of
input codewords is the Helovo information between the input and output state.
Furthermore, we present a one-shot quantum decoupling theorem with relative
entropy criterion. Our results based on the relative-entropy criterion are
tighter than the corresponding results based on the trace norm considered in
the literature due to the Pinsker inequality.
</p>
</div>
</dd>
<dt><a name="item89">[89]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11114" title="Abstract">arXiv:2402.11114</a> [<a href="/pdf/2402.11114" title="Download PDF">pdf</a>, <a href="/format/2402.11114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Whose Emotions and Moral Sentiments Do Language Models Reflect?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zihao He</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Siyi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+A">Ashwin Rao</a>, 
<a href="/search/cs?searchtype=author&query=Lerman%2C+K">Kristina Lerman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Language models (LMs) are known to represent the perspectives of some social
groups better than others, which may impact their performance, especially on
subjective tasks such as content moderation and hate speech detection. To
explore how LMs represent different perspectives, existing research focused on
positional alignment, i.e., how closely the models mimic the opinions and
stances of different groups, e.g., liberals or conservatives. However, human
communication also encompasses emotional and moral dimensions. We define the
problem of affective alignment, which measures how LMs' emotional and moral
tone represents those of different groups. By comparing the affect of responses
generated by 36 LMs to the affect of Twitter messages, we observe significant
misalignment of LMs with both ideological groups. This misalignment is larger
than the partisan divide in the U.S. Even after steering the LMs towards
specific ideological perspectives, the misalignment and liberal tendencies of
the model persist, suggesting a systemic bias within LMs.
</p>
</div>
</dd>
<dt><a name="item90">[90]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11119" title="Abstract">arXiv:2402.11119</a> [<a href="/pdf/2402.11119" title="Download PDF">pdf</a>, <a href="/ps/2402.11119" title="Download PostScript">ps</a>, <a href="/format/2402.11119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Private PAC Learning May be Harder than Online Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bun%2C+M">Mark Bun</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+A">Aloni Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Desai%2C+R">Rathin Desai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We continue the study of the computational complexity of differentially
private PAC learning and how it is situated within the foundations of machine
learning. A recent line of work uncovered a qualitative equivalence between the
private PAC model and Littlestone's mistake-bounded model of online learning,
in particular, showing that any concept class of Littlestone dimension $d$ can
be privately PAC learned using $\mathrm{poly}(d)$ samples. This raises the
natural question of whether there might be a generic conversion from online
learners to private PAC learners that also preserves computational efficiency.
<br />We give a negative answer to this question under reasonable cryptographic
assumptions (roughly, those from which it is possible to build
indistinguishability obfuscation for all circuits). We exhibit a concept class
that admits an online learner running in polynomial time with a polynomial
mistake bound, but for which there is no computationally-efficient
differentially private PAC learner. Our construction and analysis strengthens
and generalizes that of Bun and Zhandry (TCC 2016-A), who established such a
separation between private and non-private PAC learner.
</p>
</div>
</dd>
<dt><a name="item91">[91]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11120" title="Abstract">arXiv:2402.11120</a> [<a href="/pdf/2402.11120" title="Download PDF">pdf</a>, <a href="/format/2402.11120" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DART: A Principled Approach to Adversarially Robust Unsupervised Domain  Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yunjuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hazimeh%2C+H">Hussein Hazimeh</a>, 
<a href="/search/cs?searchtype=author&query=Ponomareva%2C+N">Natalia Ponomareva</a>, 
<a href="/search/cs?searchtype=author&query=Kurakin%2C+A">Alexey Kurakin</a>, 
<a href="/search/cs?searchtype=author&query=Hammoud%2C+I">Ibrahim Hammoud</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+R">Raman Arora</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">Distribution shifts and adversarial examples are two major challenges for
deploying machine learning models. While these challenges have been studied
individually, their combination is an important topic that remains relatively
under-explored. In this work, we study the problem of adversarial robustness
under a common setting of distribution shift - unsupervised domain adaptation
(UDA). Specifically, given a labeled source domain $D_S$ and an unlabeled
target domain $D_T$ with related but different distributions, the goal is to
obtain an adversarially robust model for $D_T$. The absence of target domain
labels poses a unique challenge, as conventional adversarial robustness
defenses cannot be directly applied to $D_T$. To address this challenge, we
first establish a generalization bound for the adversarial target loss, which
consists of (i) terms related to the loss on the data, and (ii) a measure of
worst-case domain divergence. Motivated by this bound, we develop a novel
unified defense framework called Divergence Aware adveRsarial Training (DART),
which can be used in conjunction with a variety of standard UDA methods; e.g.,
DANN [Ganin and Lempitsky, 2015]. DART is applicable to general threat models,
including the popular $\ell_p$-norm model, and does not require heuristic
regularizers or architectural changes. We also release DomainRobust: a testbed
for evaluating robustness of UDA models to adversarial attacks. DomainRobust
consists of 4 multi-domain benchmark datasets (with 46 source-target pairs) and
7 meta-algorithms with a total of 11 variants. Our large-scale experiments
demonstrate that on average, DART significantly enhances model robustness on
all benchmarks compared to the state of the art, while maintaining competitive
standard accuracy. The relative improvement in robustness from DART reaches up
to 29.2% on the source-target domain pairs considered.
</p>
</div>
</dd>
<dt><a name="item92">[92]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11122" title="Abstract">arXiv:2402.11122</a> [<a href="/pdf/2402.11122" title="Download PDF">pdf</a>, <a href="/format/2402.11122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Navigating the Dual Facets: A Comprehensive Evaluation of Sequential  Memory Editing in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zihao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Beigi%2C+M">Mohammad Beigi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongxuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yufan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuxiang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+W">Wenpeng Yin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Lifu Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint, 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Memory Editing (ME) has emerged as an efficient method to modify erroneous
facts or inject new facts into Large Language Models (LLMs). Two mainstream ME
methods exist: parameter-modifying ME and parameter-preserving ME (integrating
extra modules while preserving original parameters). Regrettably, previous
studies on ME evaluation have two critical limitations: (i) evaluating LLMs
with single edit only, neglecting the need for continuous editing, and (ii)
evaluations focusing solely on basic factual triples, overlooking broader LLM
capabilities like logical reasoning and reading understanding. This study
addresses these limitations with contributions threefold: (i) We explore how ME
affects a wide range of fundamental capabilities of LLMs under sequential
editing. Experimental results reveal an intriguing phenomenon: Most
parameter-modifying ME consistently degrade performance across all tasks after
a few sequential edits. In contrast, parameter-preserving ME effectively
maintains LLMs' fundamental capabilities but struggles to accurately recall
edited knowledge presented in a different format. (ii) We extend our evaluation
to different editing settings, such as layers to edit, model size, instruction
tuning, etc. Experimental findings indicate several strategies that can
potentially mitigate the adverse effects of ME. (iii) We further explain why
parameter-modifying ME damages LLMs from three dimensions: parameter changes
after editing, language modeling capability, and the in-context learning
capability. Our in-depth study advocates more careful use of ME in real-world
scenarios.
</p>
</div>
</dd>
<dt><a name="item93">[93]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11123" title="Abstract">arXiv:2402.11123</a> [<a href="/pdf/2402.11123" title="Download PDF">pdf</a>, <a href="/format/2402.11123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Warfarin Dosing Using Contextual Bandit: An Offline Policy  Learning and Evaluation Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yong Huang</a>, 
<a href="/search/cs?searchtype=author&query=Downs%2C+C+A">Charles A. Downs</a>, 
<a href="/search/cs?searchtype=author&query=Rahmani%2C+A+M">Amir M. Rahmani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Warfarin, an anticoagulant medication, is formulated to prevent and address
conditions associated with abnormal blood clotting, making it one of the most
prescribed drugs globally. However, determining the suitable dosage remains
challenging due to individual response variations, and prescribing an incorrect
dosage may lead to severe consequences. Contextual bandit and reinforcement
learning have shown promise in addressing this issue. Given the wide
availability of observational data and safety concerns of decision-making in
healthcare, we focused on using exclusively observational data from historical
policies as demonstrations to derive new policies; we utilized offline policy
learning and evaluation in a contextual bandit setting to establish the optimal
personalized dosage strategy. Our learned policies surpassed these baseline
approaches without genotype inputs, even when given a suboptimal demonstration,
showcasing promising application potential.
</p>
</div>
</dd>
<dt><a name="item94">[94]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11124" title="Abstract">arXiv:2402.11124</a> [<a href="/pdf/2402.11124" title="Download PDF">pdf</a>, <a href="/format/2402.11124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disentanglement in Implicit Causal Models via Switch Variable
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bagi%2C+S+S+G">Shayan Shirahmad Gale Bagi</a>, 
<a href="/search/cs?searchtype=author&query=Gharaee%2C+Z">Zahra Gharaee</a>, 
<a href="/search/cs?searchtype=author&query=Schulte%2C+O">Oliver Schulte</a>, 
<a href="/search/cs?searchtype=author&query=Crowley%2C+M">Mark Crowley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Learning causal representations from observational and interventional data in
the absence of known ground-truth graph structures necessitates implicit latent
causal representation learning. Implicitly learning causal mechanisms typically
involves two categories of interventional data: hard and soft interventions. In
real-world scenarios, soft interventions are often more realistic than hard
interventions, as the latter require fully controlled environments. Unlike hard
interventions, which directly force changes in a causal variable, soft
interventions exert influence indirectly by affecting the causal mechanism. In
this paper, we tackle implicit latent causal representation learning in a
Variational Autoencoder (VAE) framework through soft interventions. Our
approach models soft interventions effects by employing a causal mechanism
switch variable designed to toggle between different causal mechanisms. In our
experiments, we consistently observe improved learning of identifiable, causal
representations, compared to baseline approaches.
</p>
</div>
</dd>
<dt><a name="item95">[95]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11125" title="Abstract">arXiv:2402.11125</a> [<a href="/pdf/2402.11125" title="Download PDF">pdf</a>, <a href="/format/2402.11125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> See Spot Guide: Accessible Interfaces for an Assistive Quadruped Robot
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hata%2C+R">Rayna Hata</a>, 
<a href="/search/cs?searchtype=author&query=Trikasemsak%2C+N">Narit Trikasemsak</a>, 
<a href="/search/cs?searchtype=author&query=Giudice%2C+A">Andrea Giudice</a>, 
<a href="/search/cs?searchtype=author&query=Doore%2C+S+A">Stacy A. Doore</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">While there is no replacement for the learned expertise, devotion, and social
benefits of a guide dog, there are cases in which a robot navigation assistant
could be helpful for individuals with blindness or low vision (BLV). This study
investigated the potential for an industrial agile robot to perform guided
navigation tasks. We developed two interface prototypes that allowed for
spatial information between a human-robot pair: a voice-based app and a
flexible, responsive handle. The participants (n=21) completed simple
navigation tasks and a post-study survey about the prototype functionality and
their trust in the robot. All participants successfully completed the
navigation tasks and demonstrated the interface prototypes were able to pass
spatial information between the human and the robot. Future work will include
expanding the voice-based app to allow the robot to communicate obstacles to
the handler and adding haptic signals to the handle design.
</p>
</div>
</dd>
<dt><a name="item96">[96]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11126" title="Abstract">arXiv:2402.11126</a> [<a href="/pdf/2402.11126" title="Download PDF">pdf</a>, <a href="/format/2402.11126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kolmogorov n-Widths for Multitask Physics-Informed Machine Learning  (PIML) Methods: Towards Robust Metrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Penwarden%2C+M">Michael Penwarden</a>, 
<a href="/search/cs?searchtype=author&query=Owhadi%2C+H">Houman Owhadi</a>, 
<a href="/search/cs?searchtype=author&query=Kirby%2C+R+M">Robert M. Kirby</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">Physics-informed machine learning (PIML) as a means of solving partial
differential equations (PDE) has garnered much attention in the Computational
Science and Engineering (CS&amp;E) world. This topic encompasses a broad array of
methods and models aimed at solving a single or a collection of PDE problems,
called multitask learning. PIML is characterized by the incorporation of
physical laws into the training process of machine learning models in lieu of
large data when solving PDE problems. Despite the overall success of this
collection of methods, it remains incredibly difficult to analyze, benchmark,
and generally compare one approach to another. Using Kolmogorov n-widths as a
measure of effectiveness of approximating functions, we judiciously apply this
metric in the comparison of various multitask PIML architectures. We compute
lower accuracy bounds and analyze the model's learned basis functions on
various PDE problems. This is the first objective metric for comparing
multitask PIML architectures and helps remove uncertainty in model validation
from selective sampling and overfitting. We also identify avenues of
improvement for model architectures, such as the choice of activation function,
which can drastically affect model generalization to "worst-case" scenarios,
which is not observed when reporting task-specific errors. We also incorporate
this metric into the optimization process through regularization, which
improves the models' generalizability over the multitask PDE problem.
</p>
</div>
</dd>
<dt><a name="item97">[97]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11129" title="Abstract">arXiv:2402.11129</a> [<a href="/pdf/2402.11129" title="Download PDF">pdf</a>, <a href="/format/2402.11129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BlendFilter: Advancing Retrieval-Augmented Large Language Models via  Query Generation Blending and Knowledge Filtering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tuo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jing Gao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Retrieval-augmented Large Language Models (LLMs) offer substantial benefits
in enhancing performance across knowledge-intensive scenarios. However, these
methods often face challenges with complex inputs and encounter difficulties
due to noisy knowledge retrieval, notably hindering model effectiveness. To
address this issue, we introduce BlendFilter, a novel approach that elevates
retrieval-augmented LLMs by integrating query generation blending with
knowledge filtering. BlendFilter proposes the blending process through its
query generation method, which integrates both external and internal knowledge
augmentation with the original query, ensuring comprehensive information
gathering. Additionally, our distinctive knowledge filtering module capitalizes
on the intrinsic capabilities of the LLM, effectively eliminating extraneous
data. We conduct extensive experiments on three open-domain question answering
benchmarks, and the findings clearly indicate that our innovative BlendFilter
surpasses state-of-the-art baselines significantly.
</p>
</div>
</dd>
<dt><a name="item98">[98]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11131" title="Abstract">arXiv:2402.11131</a> [<a href="/pdf/2402.11131" title="Download PDF">pdf</a>, <a href="/format/2402.11131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speculative Streaming: Fast LLM Inference without Auxiliary Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhendawade%2C+N">Nikhil Bhendawade</a>, 
<a href="/search/cs?searchtype=author&query=Belousova%2C+I">Irina Belousova</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Q">Qichen Fu</a>, 
<a href="/search/cs?searchtype=author&query=Mason%2C+H">Henry Mason</a>, 
<a href="/search/cs?searchtype=author&query=Rastegari%2C+M">Mohammad Rastegari</a>, 
<a href="/search/cs?searchtype=author&query=Najibi%2C+M">Mahyar Najibi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Speculative decoding is a prominent technique to speed up the inference of a
large target language model based on predictions of an auxiliary draft model.
While effective, in application-specific settings, it often involves
fine-tuning both draft and target models to achieve high acceptance rates. As
the number of downstream tasks grows, these draft models add significant
complexity to inference systems. We propose Speculative Streaming, a
single-model speculative decoding method that fuses drafting into the target
model by changing the fine-tuning objective from next token prediction to
future n-gram prediction. Speculative Streaming speeds up decoding by 1.8 -
3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and
Meaning Representation, without sacrificing generation quality. Additionally,
Speculative Streaming is parameter-efficient. It achieves on-par/higher
speed-ups than Medusa-style architectures while using ~10000X fewer extra
parameters, making it well-suited for resource-constrained devices.
</p>
</div>
</dd>
<dt><a name="item99">[99]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11137" title="Abstract">arXiv:2402.11137</a> [<a href="/pdf/2402.11137" title="Download PDF">pdf</a>, <a href="/format/2402.11137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feuer%2C+B">Benjamin Feuer</a>, 
<a href="/search/cs?searchtype=author&query=Schirrmeister%2C+R+T">Robin Tibor Schirrmeister</a>, 
<a href="/search/cs?searchtype=author&query=Cherepanova%2C+V">Valeriia Cherepanova</a>, 
<a href="/search/cs?searchtype=author&query=Hegde%2C+C">Chinmay Hegde</a>, 
<a href="/search/cs?searchtype=author&query=Hutter%2C+F">Frank Hutter</a>, 
<a href="/search/cs?searchtype=author&query=Goldblum%2C+M">Micah Goldblum</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+N">Niv Cohen</a>, 
<a href="/search/cs?searchtype=author&query=White%2C+C">Colin White</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">While tabular classification has traditionally relied on from-scratch
training, a recent breakthrough called prior-data fitted networks (PFNs)
challenges this approach. Similar to large language models, PFNs make use of
pretraining and in-context learning to achieve strong performance on new tasks
in a single forward pass. However, current PFNs have limitations that prohibit
their widespread adoption. Notably, TabPFN achieves very strong performance on
small tabular datasets but is not designed to make predictions for datasets of
size larger than 1000. In this work, we overcome these limitations and
substantially improve the performance of PFNs by developing context
optimization techniques for PFNs. Specifically, we propose TuneTables, a novel
prompt-tuning strategy that compresses large datasets into a smaller learned
context. TuneTables scales TabPFN to be competitive with state-of-the-art
tabular classification methods on larger datasets, while having a substantially
lower inference time than TabPFN. Furthermore, we show that TuneTables can be
used as an interpretability tool and can even be used to mitigate biases by
optimizing a fairness objective.
</p>
</div>
</dd>
<dt><a name="item100">[100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11138" title="Abstract">arXiv:2402.11138</a> [<a href="/pdf/2402.11138" title="Download PDF">pdf</a>, <a href="/format/2402.11138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrastive Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+T">Tianyi Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J+Y">James Y. Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wenxuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+F">Fan Yin</a>, 
<a href="/search/cs?searchtype=author&query=Galstyan%2C+A">Aram Galstyan</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+W">Wenpeng Yin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Muhao Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Instruction tuning has been used as a promising approach to improve the
performance of large language models (LLMs) on unseen tasks. However, current
LLMs exhibit limited robustness to unseen instructions, generating inconsistent
outputs when the same instruction is phrased with slightly varied forms or
language styles. This behavior indicates LLMs' lack of robustness to textual
variations and generalizability to unseen instructions, potentially leading to
trustworthiness issues. Accordingly, we propose Contrastive Instruction Tuning,
which maximizes the similarity between the hidden representations of
semantically equivalent instruction-instance pairs while minimizing the
similarity between semantically different ones. To facilitate this approach, we
augment the existing FLAN collection by paraphrasing task instructions.
Experiments on the PromptBench benchmark show that CoIN consistently improves
LLMs' robustness to unseen instructions with variations across character, word,
sentence, and semantic levels by an average of +2.5% in accuracy.
</p>
</div>
</dd>
<dt><a name="item101">[101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11139" title="Abstract">arXiv:2402.11139</a> [<a href="/pdf/2402.11139" title="Download PDF">pdf</a>, <a href="/format/2402.11139" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LiGNN: Graph Neural Networks at LinkedIn
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Borisyuk%2C+F">Fedor Borisyuk</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shihai He</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+Y">Yunbo Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Ramezani%2C+M">Morteza Ramezani</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+P">Peng Du</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+X">Xiaochen Hou</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+C">Chengming Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Pasumarthy%2C+N">Nitin Pasumarthy</a>, 
<a href="/search/cs?searchtype=author&query=Bannur%2C+P">Priya Bannur</a>, 
<a href="/search/cs?searchtype=author&query=Tiwana%2C+B">Birjodh Tiwana</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Ping Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dangi%2C+S">Siddharth Dangi</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+D">Daqi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+Z">Zhoutao Pei</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xiao Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Sirou Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Q">Qianqi Shen</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kuang-Hsuan Lee</a>, 
<a href="/search/cs?searchtype=author&query=Stein%2C+D">David Stein</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Baolei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+H">Haichao Wei</a>, 
<a href="/search/cs?searchtype=author&query=Ghoting%2C+A">Amol Ghoting</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Souvik Ghosh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper, we present LiGNN, a deployed large-scale Graph Neural Networks
(GNNs) Framework. We share our insight on developing and deployment of GNNs at
large scale at LinkedIn. We present a set of algorithmic improvements to the
quality of GNN representation learning including temporal graph architectures
with long term losses, effective cold start solutions via graph densification,
ID embeddings and multi-hop neighbor sampling. We explain how we built and sped
up by 7x our large-scale training on LinkedIn graphs with adaptive sampling of
neighbors, grouping and slicing of training data batches, specialized
shared-memory queue and local gradient optimization. We summarize our
deployment lessons and learnings gathered from A/B test experiments. The
techniques presented in this work have contributed to an approximate relative
improvements of 1% of Job application hearing back rate, 2% Ads CTR lift, 0.5%
of Feed engaged daily active users, 0.2% session lift and 0.1% weekly active
user lift from people recommendation. We believe that this work can provide
practical solutions and insights for engineers who are interested in applying
Graph neural networks at large scale.
</p>
</div>
</dd>
<dt><a name="item102">[102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11140" title="Abstract">arXiv:2402.11140</a> [<a href="/pdf/2402.11140" title="Download PDF">pdf</a>, <a href="/format/2402.11140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting of Thoughts: Trial-and-Error Problem Solving with Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sijia Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Baochun Li</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+D">Di Niu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a poster paper by ICLR2024. 27 pages, 5 figures, 18 tables. [Source Code](<a href="https://github.com/iQua/llmpebase/tree/main/examples/BoTReasoning">this https URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The reasoning performance of Large Language Models (LLMs) on a wide range of
problems critically relies on chain-of-thought prompting, which involves
providing a few chain of thought demonstrations as exemplars in prompts. Recent
work, e.g., Tree of Thoughts, has pointed out the importance of exploration and
self-evaluation in reasoning step selection for complex problem solving. In
this paper, we present Boosting of Thoughts (BoT), an automated prompting
framework for problem solving with LLMs by iteratively exploring and
self-evaluating many trees of thoughts in order to acquire an ensemble of
trial-and-error reasoning experiences, which will serve as a new form of
prompting to solve the complex problem. Starting from a simple prompt without
requiring examples, BoT iteratively explores and evaluates a large collection
of reasoning steps, and more importantly, uses error analysis obtained from the
LLM on them to explicitly revise prompting, which in turn enhances reasoning
step generation, until a final answer is attained. Our experiments with GPT-4
and Llama2 across extensive complex mathematical problems demonstrate that BoT
consistently achieves higher or comparable problem-solving rates than other
advanced prompting approaches.
</p>
</div>
</dd>
<dt><a name="item103">[103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11141" title="Abstract">arXiv:2402.11141</a> [<a href="/pdf/2402.11141" title="Download PDF">pdf</a>, <a href="/format/2402.11141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantically-aware Neural Radiance Fields for Visual Scene  Understanding: A Comprehensive Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T">Thang-Anh-Quan Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Bourki%2C+A">Amine Bourki</a>, 
<a href="/search/cs?searchtype=author&query=Macudzinski%2C+M">M&#xe1;ty&#xe1;s Macudzinski</a>, 
<a href="/search/cs?searchtype=author&query=Brunel%2C+A">Anthony Brunel</a>, 
<a href="/search/cs?searchtype=author&query=Bennamoun%2C+M">Mohammed Bennamoun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This review thoroughly examines the role of semantically-aware Neural
Radiance Fields (NeRFs) in visual scene understanding, covering an analysis of
over 250 scholarly papers. It explores how NeRFs adeptly infer 3D
representations for both stationary and dynamic objects in a scene. This
capability is pivotal for generating high-quality new viewpoints, completing
missing scene details (inpainting), conducting comprehensive scene segmentation
(panoptic segmentation), predicting 3D bounding boxes, editing 3D scenes, and
extracting object-centric 3D models. A significant aspect of this study is the
application of semantic labels as viewpoint-invariant functions, which
effectively map spatial coordinates to a spectrum of semantic labels, thus
facilitating the recognition of distinct objects within the scene. Overall,
this survey highlights the progression and diverse applications of
semantically-aware neural radiance fields in the context of visual scene
interpretation.
</p>
</div>
</dd>
<dt><a name="item104">[104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11142" title="Abstract">arXiv:2402.11142</a> [<a href="/pdf/2402.11142" title="Download PDF">pdf</a>, <a href="/format/2402.11142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grasping the Essentials: Tailoring Large Language Models for Zero-Shot  Relation Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Sizhe Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Y">Yu Meng</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+B">Bowen Jin</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+J">Jiawei Han</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 12 Tables, 9 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Relation extraction (RE), a crucial task in NLP, aims to identify semantic
relationships between entities mentioned in texts. Despite significant
advancements in this field, existing models typically rely on extensive
annotated data for training, which can be both costly and time-consuming to
acquire. Moreover, these models often struggle to adapt to new or unseen
relationships. In contrast, few-shot learning settings, which aim to reduce
annotation requirements, may offer incomplete and biased supervision for
understanding target relation semantics, leading to degraded and unstable
performance. To provide the model with accurate and explicit descriptions of
the relations types and meanwhile minimize the annotation requirements, we
study the definition only zero-shot RE setting where only relation definitions
expressed in natural language are used to train a RE model. Motivated by the
strong synthetic data generation power of LLMs, we propose a framework REPaL
which consists of three stages: (1) We utilize LLMs to generate initial seed
instances based on relation definitions and an unlabeled corpora. (2) We
fine-tune a bidirectional Small Language Model (SLM) using these initial seeds
to learn the relations for the target domain. (3) We enhance pattern coverage
and mitigate bias resulting from the limited number of initial seeds by
incorporating feedback acquired from SLM's predictions on unlabeled corpora. To
accomplish this, we leverage the multi-turn conversation ability of LLMs to
generate new instances in follow-up dialogues. Experiments on two datasets show
REPaL achieves better zero-shot performance with large margins over baseline
methods.
</p>
</div>
</dd>
<dt><a name="item105">[105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11143" title="Abstract">arXiv:2402.11143</a> [<a href="/pdf/2402.11143" title="Download PDF">pdf</a>, <a href="/format/2402.11143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foundation Models for Recommender Systems: A Survey and New Perspectives
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chengkai Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+K">Kaige Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+L">Lina Yao</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Recently, Foundation Models (FMs), with their extensive knowledge bases and
complex architectures, have offered unique opportunities within the realm of
recommender systems (RSs). In this paper, we attempt to thoroughly examine
FM-based recommendation systems (FM4RecSys). We start by reviewing the research
background of FM4RecSys. Then, we provide a systematic taxonomy of existing
FM4RecSys research works, which can be divided into four different parts
including data characteristics, representation learning, model type, and
downstream tasks. Within each part, we review the key recent research
developments, outlining the representative models and discussing their
characteristics. Moreover, we elaborate on the open problems and opportunities
of FM4RecSys aiming to shed light on future research directions in this area.
In conclusion, we recap our findings and discuss the emerging trends in this
field.
</p>
</div>
</dd>
<dt><a name="item106">[106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11145" title="Abstract">arXiv:2402.11145</a> [<a href="/pdf/2402.11145" title="Download PDF">pdf</a>, <a href="/format/2402.11145" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Supporting Experts with a Multimodal Machine-Learning-Based Tool for  Human Behavior Analysis of Conversational Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arakawa%2C+R">Riku Arakawa</a>, 
<a href="/search/cs?searchtype=author&query=Maeda%2C+K">Kiyosu Maeda</a>, 
<a href="/search/cs?searchtype=author&query=Yakura%2C+H">Hiromu Yakura</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Multimodal scene search of conversations is essential for unlocking valuable
insights into social dynamics and enhancing our communication. While experts in
conversational analysis have their own knowledge and skills to find key scenes,
a lack of comprehensive, user-friendly tools that streamline the processing of
diverse multimodal queries impedes efficiency and objectivity. To solve it, we
developed Providence, a visual-programming-based tool based on design
considerations derived from a formative study with experts. It enables experts
to combine various machine learning algorithms to capture human behavioral cues
without writing code. Our study showed its preferable usability and
satisfactory output with less cognitive load imposed in accomplishing scene
search tasks of conversations, verifying the importance of its customizability
and transparency. Furthermore, through the in-the-wild trial, we confirmed the
objectivity and reusability of the tool transform experts' workflow, suggesting
the advantage of expert-AI teaming in a highly human-contextual domain.
</p>
</div>
</dd>
<dt><a name="item107">[107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11148" title="Abstract">arXiv:2402.11148</a> [<a href="/pdf/2402.11148" title="Download PDF">pdf</a>, <a href="/format/2402.11148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Distillation Based on Transformed Teacher Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+K">Kaixiang Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+E">En-Hui Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">As a technique to bridge logit matching and probability distribution
matching, temperature scaling plays a pivotal role in knowledge distillation
(KD). Conventionally, temperature scaling is applied to both teacher's logits
and student's logits in KD. Motivated by some recent works, in this paper, we
drop instead temperature scaling on the student side, and systematically study
the resulting variant of KD, dubbed transformed teacher matching (TTM). By
reinterpreting temperature scaling as a power transform of probability
distribution, we show that in comparison with the original KD, TTM has an
inherent R\'enyi entropy term in its objective function, which serves as an
extra regularization term. Extensive experiment results demonstrate that thanks
to this inherent regularization, TTM leads to trained students with better
generalization than the original KD. To further enhance student's capability to
match teacher's power transformed probability distribution, we introduce a
sample-adaptive weighting coefficient into TTM, yielding a novel distillation
approach dubbed weighted TTM (WTTM). It is shown, by comprehensive experiments,
that although WTTM is simple, it is effective, improves upon TTM, and achieves
state-of-the-art accuracy performance. Our source code is available at
https://github.com/zkxufo/TTM.
</p>
</div>
</dd>
<dt><a name="item108">[108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11151" title="Abstract">arXiv:2402.11151</a> [<a href="/pdf/2402.11151" title="Download PDF">pdf</a>, <a href="/ps/2402.11151" title="Download PostScript">ps</a>, <a href="/format/2402.11151" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Landscape Study of Open Source and Proprietary Tools for Software Bill  of Materials (SBOM)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mirakhorli%2C+M">Mehdi Mirakhorli</a>, 
<a href="/search/cs?searchtype=author&query=Garcia%2C+D">Derek Garcia</a>, 
<a href="/search/cs?searchtype=author&query=Dillon%2C+S">Schuyler Dillon</a>, 
<a href="/search/cs?searchtype=author&query=Laporte%2C+K">Kevin Laporte</a>, 
<a href="/search/cs?searchtype=author&query=Morrison%2C+M">Matthew Morrison</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Henry Lu</a>, 
<a href="/search/cs?searchtype=author&query=Koscinski%2C+V">Viktoria Koscinski</a>, 
<a href="/search/cs?searchtype=author&query=Enoch%2C+C">Christopher Enoch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Modern software applications heavily rely on diverse third-party components,
libraries, and frameworks sourced from various vendors and open source
repositories, presenting a complex challenge for securing the software supply
chain. To address this complexity, the adoption of a Software Bill of Materials
(SBOM) has emerged as a promising solution, offering a centralized repository
that inventories all third-party components and dependencies used in an
application. Recent supply chain breaches, exemplified by the SolarWinds
attack, underscore the urgent need to enhance software security and mitigate
vulnerability risks, with SBOMs playing a pivotal role in this endeavor by
revealing potential vulnerabilities, outdated components, and unsupported
elements. This research paper conducts an extensive empirical analysis to
assess the current landscape of open-source and proprietary tools related to
SBOM. We investigate emerging use cases in software supply chain security and
identify gaps in SBOM technologies. Our analysis encompasses 84 tools,
providing a snapshot of the current market and highlighting areas for
improvement.
</p>
</div>
</dd>
<dt><a name="item109">[109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11153" title="Abstract">arXiv:2402.11153</a> [<a href="/pdf/2402.11153" title="Download PDF">pdf</a>, <a href="/format/2402.11153" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Generalization: A Survey of Out-Of-Distribution Adaptation on  Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuhan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+K">Kaize Ding</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Distribution shifts on graphs -- the data distribution discrepancies between
training and testing a graph machine learning model, are often ubiquitous and
unavoidable in real-world scenarios. Such shifts may severely deteriorate the
performance of the model, posing significant challenges for reliable graph
machine learning. Consequently, there has been a surge in research on graph
Out-Of-Distribution (OOD) adaptation methods that aim to mitigate the
distribution shifts and adapt the knowledge from one distribution to another.
In our survey, we provide an up-to-date and forward-looking review of graph OOD
adaptation methods, covering two main problem scenarios including training-time
as well as test-time graph OOD adaptation. We start by formally formulating the
two problems and then discuss different types of distribution shifts on graphs.
Based on our proposed taxonomy for graph OOD adaptation, we systematically
categorize the existing methods according to their learning paradigm and
investigate the techniques behind them. Finally, we point out promising
research directions and the corresponding challenges. We also provide a
continuously updated reading list at
https://github.com/kaize0409/Awesome-Graph-OOD-Adaptation.git
</p>
</div>
</dd>
<dt><a name="item110">[110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11155" title="Abstract">arXiv:2402.11155</a> [<a href="/pdf/2402.11155" title="Download PDF">pdf</a>, <a href="/format/2402.11155" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automated Optimization of Parameterized Data-Plane Programs with Parasol
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hogan%2C+M">Mary Hogan</a>, 
<a href="/search/cs?searchtype=author&query=Loehr%2C+D">Devon Loehr</a>, 
<a href="/search/cs?searchtype=author&query=Sonchack%2C+J">John Sonchack</a>, 
<a href="/search/cs?searchtype=author&query=Feibish%2C+S+L">Shir Landau Feibish</a>, 
<a href="/search/cs?searchtype=author&query=Rexford%2C+J">Jennifer Rexford</a>, 
<a href="/search/cs?searchtype=author&query=Walker%2C+D">David Walker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">Programmable data planes allow for sophisticated applications that give
operators the power to customize the functionality of their networks. Deploying
these applications, however, often requires tedious and burdensome optimization
of their layout and design, in which programmers must manually write, compile,
and test an implementation, adjust the design, and repeat. In this paper we
present Parasol, a framework that allows programmers to define general,
parameterized network algorithms and automatically optimize their various
parameters. The parameters of a Parasol program can represent a wide variety of
implementation decisions, and may be optimized for arbitrary, high-level
objectives defined by the programmer. Furthermore, optimization may be tailored
to particular environments by providing a representative sample of traffic. We
show how we implement the Parasol framework, which consists of a sketching
language for writing parameterized programs, and a simulation-based optimizer
for testing different parameter settings. We evaluate Parasol by implementing a
suite of ten data-plane applications, and find that Parasol produces a solution
with comparable performance to hand-optimized P4 code within a two-hour time
budget.
</p>
</div>
</dd>
<dt><a name="item111">[111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11159" title="Abstract">arXiv:2402.11159</a> [<a href="/pdf/2402.11159" title="Download PDF">pdf</a>, <a href="/format/2402.11159" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding News Thumbnail Representativeness by Counterfactual  Text-Guided Contrastive Language-Image Pretraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoon%2C+Y">Yejun Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+S">Seunghyun Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+K">Kunwoo Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">This paper delves into the critical challenge of understanding the
representativeness of news thumbnail images, which often serve as the first
visual engagement for readers when an article is disseminated on social media.
We focus on whether a news image represents the main subject discussed in the
news text. To serve the challenge, we introduce \textsc{NewsTT}, a manually
annotated dataset of news thumbnail image and text pairs. We found that
pretrained vision and language models, such as CLIP and BLIP-2, struggle with
this task. Since news subjects frequently involve named entities or proper
nouns, a pretrained model could not have the ability to match its visual and
textual appearances. To fill the gap, we propose CFT-CLIP, a counterfactual
text-guided contrastive language-image pretraining framework. We hypothesize
that learning to contrast news text with its counterfactual, of which named
entities are replaced, can enhance the cross-modal matching ability in the
target task. Evaluation experiments using NewsTT show that CFT-CLIP outperforms
the pretrained models, such as CLIP and BLIP-2. Our code and data will be made
accessible to the public after the paper is accepted.
</p>
</div>
</dd>
<dt><a name="item112">[112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11161" title="Abstract">arXiv:2402.11161</a> [<a href="/pdf/2402.11161" title="Download PDF">pdf</a>, <a href="/format/2402.11161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PANDA (Pedantic ANswer-correctness Determination and  Adjudication):Improving Automatic Evaluation for Question Answering and Text  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zongxia Li</a>, 
<a href="/search/cs?searchtype=author&query=Mondal%2C+I">Ishani Mondal</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yijun Liang</a>, 
<a href="/search/cs?searchtype=author&query=Nghiem%2C+H">Huy Nghiem</a>, 
<a href="/search/cs?searchtype=author&query=Boyd-Graber%2C+J+L">Jordan Lee Boyd-Graber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 5 figures, 11 tables. arXiv admin note: substantial text overlap with <a href="/abs/2401.13170">arXiv:2401.13170</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Question answering (QA) can only make progress if we know if an answer is
correct, but for many of the most challenging and interesting QA examples,
current answer correctness (AC) metrics do not align with human judgments,
particularly verbose, free form answers from large language models (LLM). There
are two challenges: a lack of data and that models are too big. LLM based
scorers correlate better with humans, but this expensive task has only been
tested on limited QA datasets. We rectify these issues by providing clear
guidelines for evaluating machine QA adopted from human QA contests. We also
introduce Precise ANswer correctness Determination and Adjudication (PANDA), a
small, efficient, deterministic AC classifier (812 KB) that more accurately
evaluates answer correctness.
</p>
</div>
</dd>
<dt><a name="item113">[113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11163" title="Abstract">arXiv:2402.11163</a> [<a href="/pdf/2402.11163" title="Download PDF">pdf</a>, <a href="/format/2402.11163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning  over Knowledge Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jinhao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W+X">Wayne Xin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yang Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hengshu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Ji-Rong Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> work in progress; efficient 7B LLM-based agent
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this paper, we aim to improve the reasoning ability of large language
models (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspired
by existing methods that design the interaction strategy between LLMs and KG,
we propose an autonomous LLM-based agent framework, called KG-Agent, which
enables a small LLM to actively make decisions until finishing the reasoning
process over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox,
KG-based executor, and knowledge memory, and develop an iteration mechanism
that autonomously selects the tool then updates the memory for reasoning over
KG. To guarantee the effectiveness, we leverage program language to formulate
the multi-hop reasoning process over the KG, and synthesize a code-based
instruction dataset to fine-tune the base LLM. Extensive experiments
demonstrate that only using 10K samples for tuning LLaMA-7B can outperform
state-of-the-art methods using larger LLMs or more data, on both in-domain and
out-domain datasets. Our code and data will be publicly released.
</p>
</div>
</dd>
<dt><a name="item114">[114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11166" title="Abstract">arXiv:2402.11166</a> [<a href="/pdf/2402.11166" title="Download PDF">pdf</a>, <a href="/format/2402.11166" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenDec: A robust generative Question-decomposition method for Multi-hop  reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Linyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+Y">Yuliang Ji</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenhao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Karlsson%2C+B+F">B&#xf6;rje F. Karlsson</a>, 
<a href="/search/cs?searchtype=author&query=Okumura%2C+M">Manabu Okumura</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multi-hop QA (MHQA) involves step-by-step reasoning to answer complex
questions and find multiple relevant supporting facts. However, Existing large
language models'(LLMs) reasoning ability in multi-hop question answering
remains exploration, which is inadequate in answering multi-hop questions.
Moreover, it is unclear whether LLMs follow a desired reasoning chain to reach
the right final answer. In this paper, we propose a \textbf{gen}erative
question \textbf{dec}omposition method (GenDec) from the perspective of
explainable QA by generating independent and complete sub-questions based on
incorporating additional extracted evidence for enhancing LLMs' reasoning
ability in RAG. To demonstrate the impact, generalization, and robustness of
Gendec, we conduct two experiments, the first is combining GenDec with small QA
systems on paragraph retrieval and QA tasks. We secondly examine the reasoning
capabilities of various state-of-the-art LLMs including GPT-4 and GPT-3.5
combined with GenDec. We experiment on the HotpotQA, 2WikihopMultiHopQA,
MuSiQue, and PokeMQA datasets.
</p>
</div>
</dd>
<dt><a name="item115">[115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11167" title="Abstract">arXiv:2402.11167</a> [<a href="/pdf/2402.11167" title="Download PDF">pdf</a>, <a href="/format/2402.11167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated  Text Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Kwak%2C+H">Haewoon Kwak</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+J">Jisun An</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The robustness of AI-content detection models against cultivated attacks
(e.g., paraphrasing or word switching) remains a significant concern. This
study proposes a novel token-ensemble generation strategy to challenge the
robustness of current AI-content detection approaches. We explore the ensemble
attack strategy by completing the prompt with the next token generated from
random candidate LLMs. We find the token-ensemble approach significantly drops
the performance of AI-content detection models (The code and test sets will be
released). Our findings reveal that token-ensemble generation poses a vital
challenge to current detection models and underlines the need for advancing
detection technologies to counter sophisticated adversarial strategies.
</p>
</div>
</dd>
<dt><a name="item116">[116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11168" title="Abstract">arXiv:2402.11168</a> [<a href="/pdf/2402.11168" title="Download PDF">pdf</a>, <a href="/format/2402.11168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Trust Regions for Explanations via Black-Box Probabilistic Certification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dhurandhar%2C+A">Amit Dhurandhar</a>, 
<a href="/search/cs?searchtype=author&query=Haldar%2C+S">Swagatam Haldar</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+D">Dennis Wei</a>, 
<a href="/search/cs?searchtype=author&query=Ramamurthy%2C+K+N">Karthikeyan Natesan Ramamurthy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Given the black box nature of machine learning models, a plethora of
explainability methods have been developed to decipher the factors behind
individual decisions. In this paper, we introduce a novel problem of black box
(probabilistic) explanation certification. We ask the question: Given a black
box model with only query access, an explanation for an example and a quality
metric (viz. fidelity, stability), can we find the largest hypercube (i.e.,
$\ell_{\infty}$ ball) centered at the example such that when the explanation is
applied to all examples within the hypercube, (with high probability) a quality
criterion is met (viz. fidelity greater than some value)? Being able to
efficiently find such a \emph{trust region} has multiple benefits: i) insight
into model behavior in a \emph{region}, with a \emph{guarantee}; ii)
ascertained \emph{stability} of the explanation; iii) \emph{explanation reuse},
which can save time, energy and money by not having to find explanations for
every example; and iv) a possible \emph{meta-metric} to compare explanation
methods. Our contributions include formalizing this problem, proposing
solutions, providing theoretical guarantees for these solutions that are
computable, and experimentally showing their efficacy on synthetic and real
data.
</p>
</div>
</dd>
<dt><a name="item117">[117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11173" title="Abstract">arXiv:2402.11173</a> [<a href="/pdf/2402.11173" title="Download PDF">pdf</a>, <a href="/format/2402.11173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to Make the Gradients Small Privately: Improved Rates for  Differentially Private Non-Convex Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lowy%2C+A">Andrew Lowy</a>, 
<a href="/search/cs?searchtype=author&query=Ullman%2C+J">Jonathan Ullman</a>, 
<a href="/search/cs?searchtype=author&query=Wright%2C+S+J">Stephen J. Wright</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Optimization and Control (math.OC)

</div>
<p class="mathjax">We provide a simple and flexible framework for designing differentially
private algorithms to find approximate stationary points of non-convex loss
functions. Our framework is based on using a private approximate risk minimizer
to "warm start" another private algorithm for finding stationary points. We use
this framework to obtain improved, and sometimes optimal, rates for several
classes of non-convex loss functions. First, we obtain improved rates for
finding stationary points of smooth non-convex empirical loss functions.
Second, we specialize to quasar-convex functions, which generalize star-convex
functions and arise in learning dynamical systems and training some neural
nets. We achieve the optimal rate for this class. Third, we give an optimal
algorithm for finding stationary points of functions satisfying the
Kurdyka-Lojasiewicz (KL) condition. For example, over-parameterized neural
networks often satisfy this condition. Fourth, we provide new state-of-the-art
rates for stationary points of non-convex population loss functions. Fifth, we
obtain improved rates for non-convex generalized linear models. A modification
of our algorithm achieves nearly the same rates for second-order stationary
points of functions with Lipschitz Hessian, improving over the previous
state-of-the-art for each of the above problems.
</p>
</div>
</dd>
<dt><a name="item118">[118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11175" title="Abstract">arXiv:2402.11175</a> [<a href="/pdf/2402.11175" title="Download PDF">pdf</a>, <a href="/format/2402.11175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuxia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Mansurov%2C+J">Jonibek Mansurov</a>, 
<a href="/search/cs?searchtype=author&query=Ivanov%2C+P">Petar Ivanov</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+J">Jinyan Su</a>, 
<a href="/search/cs?searchtype=author&query=Shelmanov%2C+A">Artem Shelmanov</a>, 
<a href="/search/cs?searchtype=author&query=Tsvigun%2C+A">Akim Tsvigun</a>, 
<a href="/search/cs?searchtype=author&query=Afzal%2C+O+M">Osama Mohanned Afzal</a>, 
<a href="/search/cs?searchtype=author&query=Mahmoud%2C+T">Tarek Mahmoud</a>, 
<a href="/search/cs?searchtype=author&query=Puccetti%2C+G">Giovanni Puccetti</a>, 
<a href="/search/cs?searchtype=author&query=Arnold%2C+T">Thomas Arnold</a>, 
<a href="/search/cs?searchtype=author&query=Aji%2C+A+F">Alham Fikri Aji</a>, 
<a href="/search/cs?searchtype=author&query=Habash%2C+N">Nizar Habash</a>, 
<a href="/search/cs?searchtype=author&query=Gurevych%2C+I">Iryna Gurevych</a>, 
<a href="/search/cs?searchtype=author&query=Nakov%2C+P">Preslav Nakov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The advent of Large Language Models (LLMs) has brought an unprecedented surge
in machine-generated text (MGT) across diverse channels. This raises legitimate
concerns about its potential misuse and societal implications. The need to
identify and differentiate such content from genuine human-generated text is
critical in combating disinformation, preserving the integrity of education and
scientific fields, and maintaining trust in communication. In this work, we
address this problem by introducing a new benchmark involving multilingual,
multi-domain and multi-generator for MGT detection -- M4GT-Bench. It is
collected for three task formulations: (1) mono-lingual and multi-lingual
binary MGT detection; (2) multi-way detection identifies which particular model
generates the text; and (3) human-machine mixed text detection, where a word
boundary delimiting MGT from human-written content should be determined. Human
evaluation for Task 2 shows less than random guess performance, demonstrating
the challenges to distinguish unique LLMs. Promising results always occur when
training and test data distribute within the same domain or generators.
</p>
</div>
</dd>
<dt><a name="item119">[119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11176" title="Abstract">arXiv:2402.11176</a> [<a href="/pdf/2402.11176" title="Download PDF">pdf</a>, <a href="/format/2402.11176" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KnowTuning: Knowledge-aware Fine-tuning for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+Y">Yougang Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+L">Lingyong Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuaiqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Haibo Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+P">Pengjie Ren</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhumin Chen</a>, 
<a href="/search/cs?searchtype=author&query=de+Rijke%2C+M">Maarten de Rijke</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhaochun Ren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Despite their success at many natural language processing (NLP) tasks, large
language models (LLMs) still struggle to effectively leverage knowledge for
knowledge-intensive tasks, manifesting limitations such as generating
incomplete, non-factual, or illogical answers. These limitations stem from
inadequate knowledge awareness of LLMs during vanilla fine-tuning. To address
these problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to
explicitly and implicitly improve the knowledge awareness of LLMs. We devise an
explicit knowledge-aware generation stage to train LLMs to explicitly identify
knowledge triples in answers. We also propose an implicit knowledge-aware
comparison stage to train LLMs to implicitly distinguish between reliable and
unreliable knowledge, in three aspects: completeness, factuality, and
logicality. Extensive experiments on both generic and medical question
answering (QA) datasets confirm the effectiveness of KnowTuning, through
automatic and human evaluations, across various sizes of LLMs. Finally, we
demonstrate that the improvements of KnowTuning generalize to unseen QA
datasets.
</p>
</div>
</dd>
<dt><a name="item120">[120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11177" title="Abstract">arXiv:2402.11177</a> [<a href="/pdf/2402.11177" title="Download PDF">pdf</a>, <a href="/format/2402.11177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Question Answering Based Pipeline for Comprehensive Chinese EHR  Information Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ying%2C+H">Huaiyuan Ying</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Sheng Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Electronic health records (EHRs) hold significant value for research and
applications. As a new way of information extraction, question answering (QA)
can extract more flexible information than conventional methods and is more
accessible to clinical researchers, but its progress is impeded by the scarcity
of annotated data. In this paper, we propose a novel approach that
automatically generates training data for transfer learning of QA models. Our
pipeline incorporates a preprocessing module to handle challenges posed by
extraction types that are not readily compatible with extractive QA frameworks,
including cases with discontinuous answers and many-to-one relationships. The
obtained QA model exhibits excellent performance on subtasks of information
extraction in EHRs, and it can effectively handle few-shot or zero-shot
settings involving yes-no questions. Case studies and ablation studies
demonstrate the necessity of each component in our design, and the resulting
model is deemed suitable for practical use.
</p>
</div>
</dd>
<dt><a name="item121">[121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11178" title="Abstract">arXiv:2402.11178</a> [<a href="/pdf/2402.11178" title="Download PDF">pdf</a>, <a href="/format/2402.11178" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RENOVI: A Benchmark Towards Remediating Norm Violations in  Socio-Cultural Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhan%2C+H">Haolan Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhuang Li</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+X">Xiaoxi Kang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+T">Tao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+Y">Yuncheng Hua</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+L">Lizhen Qu</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+Y">Yi Ying</a>, 
<a href="/search/cs?searchtype=author&query=Chandra%2C+M+R">Mei Rianto Chandra</a>, 
<a href="/search/cs?searchtype=author&query=Rosalin%2C+K">Kelly Rosalin</a>, 
<a href="/search/cs?searchtype=author&query=Jureynolds%2C+J">Jureynolds Jureynolds</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+S">Suraj Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+S">Shilin Qu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+L">Linhao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Soon%2C+L">Lay-Ki Soon</a>, 
<a href="/search/cs?searchtype=author&query=Azad%2C+Z+S">Zhaleh Semnani Azad</a>, 
<a href="/search/cs?searchtype=author&query=Zukerman%2C+I">Ingrid Zukerman</a>, 
<a href="/search/cs?searchtype=author&query=Haffari%2C+G">Gholamreza Haffari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> work in progress. 15 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Norm violations occur when individuals fail to conform to culturally accepted
behaviors, which may lead to potential conflicts. Remediating norm violations
requires social awareness and cultural sensitivity of the nuances at play. To
equip interactive AI systems with a remediation ability, we offer ReNoVi - a
large-scale corpus of 9,258 multi-turn dialogues annotated with social norms,
as well as define a sequence of tasks to help understand and remediate norm
violations step by step. ReNoVi consists of two parts: 512 human-authored
dialogues (real data), and 8,746 synthetic conversations generated by ChatGPT
through prompt learning. While collecting sufficient human-authored data is
costly, synthetic conversations provide suitable amounts of data to help
mitigate the scarcity of training data, as well as the chance to assess the
alignment between LLMs and humans in the awareness of social norms. We thus
harness the power of ChatGPT to generate synthetic training data for our task.
To ensure the quality of both human-authored and synthetic data, we follow a
quality control protocol during data collection. Our experimental results
demonstrate the importance of remediating norm violations in socio-cultural
conversations, as well as the improvement in performance obtained from
synthetic data.
</p>
</div>
</dd>
<dt><a name="item122">[122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11179" title="Abstract">arXiv:2402.11179</a> [<a href="/pdf/2402.11179" title="Download PDF">pdf</a>, <a href="/format/2402.11179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty Quantification of Graph Convolution Neural Network Models of  Evolving Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hauth%2C+J">Jeremiah Hauth</a>, 
<a href="/search/cs?searchtype=author&query=Safta%2C+C">Cosmin Safta</a>, 
<a href="/search/cs?searchtype=author&query=Huan%2C+X">Xun Huan</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+R+G">Ravi G. Patel</a>, 
<a href="/search/cs?searchtype=author&query=Jones%2C+R+E">Reese E. Jones</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 20 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Statistics Theory (math.ST); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">The application of neural network models to scientific machine learning tasks
has proliferated in recent years. In particular, neural network models have
proved to be adept at modeling processes with spatial-temporal complexity.
Nevertheless, these highly parameterized models have garnered skepticism in
their ability to produce outputs with quantified error bounds over the regimes
of interest. Hence there is a need to find uncertainty quantification methods
that are suitable for neural networks. In this work we present comparisons of
the parametric uncertainty quantification of neural networks modeling complex
spatial-temporal processes with Hamiltonian Monte Carlo and Stein variational
gradient descent and its projected variant. Specifically we apply these methods
to graph convolutional neural network models of evolving systems modeled with
recurrent neural network and neural ordinary differential equations
architectures. We show that Stein variational inference is a viable alternative
to Monte Carlo methods with some clear advantages for complex neural network
models. For our exemplars, Stein variational interference gave similar
uncertainty profiles through time compared to Hamiltonian Monte Carlo, albeit
with generally more generous variance.Projected Stein variational gradient
descent also produced similar uncertainty profiles to the non-projected
counterpart, but large reductions in the active weight space were confounded by
the stability of the neural network predictions and the convoluted likelihood
landscape.
</p>
</div>
</dd>
<dt><a name="item123">[123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11180" title="Abstract">arXiv:2402.11180</a> [<a href="/pdf/2402.11180" title="Download PDF">pdf</a>, <a href="/format/2402.11180" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PureNav: A Personalized Navigation Service for Environmental Justice  Communities Impacted by Planned Disruptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hammad%2C+O">Omar Hammad</a>, 
<a href="/search/cs?searchtype=author&query=Rahman%2C+M+R">Md Rezwanur Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Clements%2C+N">Nicholas Clements</a>, 
<a href="/search/cs?searchtype=author&query=Mishra%2C+S">Shivakant Mishra</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+S">Shelly Miller</a>, 
<a href="/search/cs?searchtype=author&query=Sullivan%2C+E">Esther Sullivan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in the proceedings of the 2023 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Planned disruptions such as highway constructions are commonplace nowadays
and the communities living near these disruptions generally tend to be
environmental justice communities -- low socioeconomic status with
disproportionately high and adverse human health and environmental effects. A
major concern is that such activities negatively impact people's well-being by
disrupting their daily commutes via frequent road closures and increased dust
and air pollution. This paper addresses this concern by developing a
personalized navigation service called PureNav to mitigate the negative impacts
of disruptions in daily commutes on people's well-being. PureNav has been
designed using active engagement with four environmental justice communities
affected by major highway construction. It has been deployed in the real world
among the members of the four communities, and a detailed analysis of the data
collected from this deployment as well as surveys show that PureNav is
potentially useful in improving people's well-being. The paper describes the
design, implementation, and evaluation of PureNav, and offers suggestions for
further improving its efficacy.
</p>
</div>
</dd>
<dt><a name="item124">[124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11183" title="Abstract">arXiv:2402.11183</a> [<a href="/pdf/2402.11183" title="Download PDF">pdf</a>, <a href="/format/2402.11183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Materiality and Risk in the Age of Pervasive AI Sensors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stewart%2C+M">Matthew Stewart</a>, 
<a href="/search/cs?searchtype=author&query=Moss%2C+E">Emanuel Moss</a>, 
<a href="/search/cs?searchtype=author&query=Warden%2C+P">Pete Warden</a>, 
<a href="/search/cs?searchtype=author&query=Plancher%2C+B">Brian Plancher</a>, 
<a href="/search/cs?searchtype=author&query=Kennedy%2C+S">Susan Kennedy</a>, 
<a href="/search/cs?searchtype=author&query=Sloane%2C+M">Mona Sloane</a>, 
<a href="/search/cs?searchtype=author&query=Reddi%2C+V+J">Vijay Janapa Reddi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">Artificial intelligence systems connected to sensor-laden devices are
becoming pervasive, which has significant implications for a range of AI risks,
including to privacy, the environment, autonomy, and more. There is therefore a
growing need for increased accountability around the responsible development
and deployment of these technologies. In this paper, we provide a comprehensive
analysis of the evolution of sensors, the risks they pose by virtue of their
material existence in the world, and the impacts of ubiquitous sensing and
on-device AI. We propose incorporating sensors into risk management frameworks
and call for more responsible sensor and system design paradigms that address
risks of such systems. To do so, we trace the evolution of sensors from analog
devices to intelligent, networked systems capable of real-time data analysis
and decision-making at the extreme edge of the network. We show that the
proliferation of sensors is driven by calculative models that prioritize data
collection and cost reduction and produce risks that emerge around privacy,
surveillance, waste, and power dynamics. We then analyze these risks,
highlighting issues of validity, safety, security, accountability,
interpretability, and bias. We surface sensor-related risks not commonly
captured in existing approaches to AI risk management, using a materiality lens
that reveals how physical sensor properties shape data and algorithmic models.
We conclude by advocating for increased attention to the materiality of
algorithmic systems, and of on-device AI sensors in particular, and highlight
the need for development of a responsible sensor design paradigm that empowers
users and communities and leads to a future of increased fairness,
accountability and transparency.
</p>
</div>
</dd>
<dt><a name="item125">[125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11184" title="Abstract">arXiv:2402.11184</a> [<a href="/pdf/2402.11184" title="Download PDF">pdf</a>, <a href="/ps/2402.11184" title="Download PostScript">ps</a>, <a href="/format/2402.11184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An efficient preconditioner for a class of non-Hermitian two-by-two  block complex system of linear equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Axelsson%2C+O">Owe Axelsson</a>, 
<a href="/search/math?searchtype=author&query=Slakuyeh%2C+D+K">Dovod Khojasteh Slakuyeh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 1 Figure, Submitted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We present an efficient preconditioner for two-by-two block system of linear
equations with the coefficient matrix
<br />$ \begin{pmatrix}
<br />F &amp; -G^H
<br />G &amp; F
<br />\end{pmatrix}$ where $F\in\mathbb{C}^{n\times n}$ is Hermitian positive
definite and $G\in\mathbb{C}^{n\times n}$ is positive semidefinite. Spectral
analysis of the preconditioned matrix is analyzed. In each iteration of a
Krylov subspace method, like GMRES, for solving the preconditioned system in
conjunction with proposed preconditioner two subsystems with Hermitian positive
definite coefficient matrix should be solved which can be accomplished exactly
using the Cholesky factorization or inexactly using the conjugate gradient
method. Application of the proposed preconditioner to the systems arising from
finite element discretization of PDE-constrained optimization problems is
presented. Numerical results are given to demonstrate the efficiency of the
preconditioner.
</p>
</div>
</dd>
<dt><a name="item126">[126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11185" title="Abstract">arXiv:2402.11185</a> [<a href="/pdf/2402.11185" title="Download PDF">pdf</a>, <a href="/format/2402.11185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Minimally Supervised Topological Projections of Self-Organizing Maps for  Phase of Flight Identification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+Z">Zimeng Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Thapa%2C+P">Pujan Thapa</a>, 
<a href="/search/cs?searchtype=author&query=Desell%2C+T">Travis Desell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Identifying phases of flight is important in the field of general aviation,
as knowing which phase of flight data is collected from aircraft flight data
recorders can aid in the more effective detection of safety or hazardous
events. General aviation flight data for phase of flight identification is
usually per-second data, comes on a large scale, and is class imbalanced. It is
expensive to manually label the data and training classification models usually
faces class imbalance problems. This work investigates the use of a novel
method for minimally supervised self-organizing maps (MS-SOMs) which utilize
nearest neighbor majority votes in the SOM U-matrix for class estimation.
Results show that the proposed method can reach or exceed a naive SOM approach
which utilized a full data file of labeled data, with only 30 labeled
datapoints per class. Additionally, the minimally supervised SOM is
significantly more robust to the class imbalance of the phase of flight data.
These results highlight how little data is required for effective phase of
flight identification.
</p>
</div>
</dd>
<dt><a name="item127">[127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11187" title="Abstract">arXiv:2402.11187</a> [<a href="/pdf/2402.11187" title="Download PDF">pdf</a>, <a href="/format/2402.11187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LaCo: Large Language Model Pruning via Layer Collapse
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yifei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zouying Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hai Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) based on transformer are witnessing a notable
trend of size expansion, which brings considerable costs to both model training
and inference. However, existing methods such as model quantization, knowledge
distillation, and model pruning are constrained by various issues, including
hardware support limitations, the need for extensive training, and alterations
to the internal structure of the model. In this paper, we propose a concise
layer-wise pruning method called \textit{Layer Collapse (LaCo)}, in which rear
model layers collapse into a prior layer, enabling a rapid reduction in model
size while preserving the model structure. Comprehensive experiments show that
our method maintains an average task performance of over 80\% at pruning ratios
of 25-30\%, significantly outperforming existing state-of-the-art structured
pruning methods. We also conduct post-training experiments to confirm that the
proposed pruning method effectively inherits the parameters of the original
model. Finally, we discuss our motivation from the perspective of layer-wise
similarity and evaluate the performance of the pruned LLMs across various
pruning ratios.
</p>
</div>
</dd>
<dt><a name="item128">[128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11190" title="Abstract">arXiv:2402.11190</a> [<a href="/pdf/2402.11190" title="Download PDF">pdf</a>, <a href="/format/2402.11190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Disclosure and Mitigation of Gender Bias in LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xiangjue Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yibo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>, 
<a href="/search/cs?searchtype=author&query=Caverlee%2C+J">James Caverlee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contribute equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) can generate biased responses. Yet previous
direct probing techniques contain either gender mentions or predefined gender
stereotypes, which are challenging to comprehensively collect. Hence, we
propose an indirect probing framework based on conditional generation. This
approach aims to induce LLMs to disclose their gender bias even without
explicit gender or stereotype mentions. We explore three distinct strategies to
disclose explicit and implicit gender bias in LLMs. Our experiments demonstrate
that all tested LLMs exhibit explicit and/or implicit gender bias, even when
gender stereotypes are not present in the inputs. In addition, an increased
model size or model alignment amplifies bias in most cases. Furthermore, we
investigate three methods to mitigate bias in LLMs via Hyperparameter Tuning,
Instruction Guiding, and Debias Tuning. Remarkably, these methods prove
effective even in the absence of explicit genders or stereotypes.
</p>
</div>
</dd>
<dt><a name="item129">[129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11191" title="Abstract">arXiv:2402.11191</a> [<a href="/pdf/2402.11191" title="Download PDF">pdf</a>, <a href="/ps/2402.11191" title="Download PostScript">ps</a>, <a href="/format/2402.11191" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Graph Assisted Automatic Sports News Writing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinyi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Siying Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this paper, we present a novel method for automatically generating sports
news, which employs a unique algorithm that extracts pivotal moments from live
text broadcasts and uses them to create an initial draft of the news. This
draft is further refined by incorporating key details and background
information from a specially designed sports knowledge graph. This graph
contains 5,893 entities, which are classified into three distinct conceptual
categories, interconnected through four relationship types, and characterized
by 27 unique attributes. In addition, we create a multi-stage learning model by
combining convolutional neural networks and a transformer encoder. This model
expresses entity-task interactions using convolutional neural networks and
enriches entity representations in the query set with the transformer encoder.
It also includes a processor to compute matching scores for incomplete triples,
addressing few-shot knowledge graph completion problem. The efficiency of this
approach has been confirmed through both subjective and objective evaluations
of 50 selected test cases, demonstrating its capability in revolutionizing the
creation of sports news.
</p>
</div>
</dd>
<dt><a name="item130">[130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11192" title="Abstract">arXiv:2402.11192</a> [<a href="/pdf/2402.11192" title="Download PDF">pdf</a>, <a href="/format/2402.11192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> I Learn Better If You Speak My Language: Enhancing Large Language Model  Fine-Tuning with Style-Aligned Response Adjustments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xuan Ren</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Biao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lingqiao Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Fine-tuning large language models (LLMs) with a small data set for particular
tasks is a widely encountered yet complex challenge. The potential for
overfitting on a limited number of examples can negatively impact the model's
ability to generalize and retain its original skills. Our research explores the
impact of the style of ground-truth responses during the fine-tuning process.
We found that matching the ground-truth response style with the LLM's inherent
style results in better learning outcomes. Building on this insight, we
developed a method that minimally alters the LLM's pre-existing responses to
correct errors, using these adjusted responses as training targets. This
technique enables precise corrections in line with the model's native response
style, safeguarding the model's core capabilities and thus avoid overfitting.
Our findings show that this approach not only improves the LLM's task-specific
accuracy but also crucially maintains its original competencies and
effectiveness.
</p>
</div>
</dd>
<dt><a name="item131">[131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11193" title="Abstract">arXiv:2402.11193</a> [<a href="/pdf/2402.11193" title="Download PDF">pdf</a>, <a href="/format/2402.11193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy Impact Assessments in the Wild: A Scoping Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Iwaya%2C+L+H">Leonardo Horn Iwaya</a>, 
<a href="/search/cs?searchtype=author&query=Alaqra%2C+A+S">Ala Sarah Alaqra</a>, 
<a href="/search/cs?searchtype=author&query=Hansen%2C+M">Marit Hansen</a>, 
<a href="/search/cs?searchtype=author&query=Fischer-H%C3%BCbner%2C+S">Simone Fischer-H&#xfc;bner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 65 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Privacy Impact Assessments (PIAs) offer a systematic process for assessing
the privacy impacts of a project or system. As a privacy engineering strategy,
PIAs are heralded as one of the main approaches to privacy by design,
supporting the early identification of threats and controls. However, there is
still a shortage of empirical evidence on their uptake and proven effectiveness
in practice. To better understand the current state of literature and research,
this paper provides a comprehensive Scoping Review (ScR) on the topic of PIAs
"in the wild", following the well-established Preferred Reporting Items for
Systematic reviews and Meta-Analyses (PRISMA) guidelines. As a result, this ScR
includes 45 studies, providing an extensive synthesis of the existing body of
knowledge, classifying types of research and publications, appraising the
methodological quality of primary research, and summarising the positive and
negative aspects of PIAs in practice, as reported by studies. This ScR also
identifies significant research gaps (e.g., evidence gaps from contradictory
results and methodological gaps from research design deficiencies), future
research pathways, and implications for researchers, practitioners, and
policymakers developing and evaluating PIA frameworks. As we conclude, there is
still a significant need for more primary research on the topic, both
qualitative and quantitative. A critical appraisal of qualitative studies
(n=28) revealed deficiencies in the methodological quality, and only four
quantitative studies were identified, suggesting that current primary research
remains incipient. Nonetheless, PIAs can be regarded as a prominent sub-area in
the broader field of Empirical Privacy Engineering, warranting further research
toward more evidence-based practices.
</p>
</div>
</dd>
<dt><a name="item132">[132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11194" title="Abstract">arXiv:2402.11194</a> [<a href="/pdf/2402.11194" title="Download PDF">pdf</a>, <a href="/format/2402.11194" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assessing LLMs&#x27; Mathematical Reasoning in Financial Document Question  Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+P">Pragya Srivastava</a>, 
<a href="/search/cs?searchtype=author&query=Malik%2C+M">Manuj Malik</a>, 
<a href="/search/cs?searchtype=author&query=Ganu%2C+T">Tanuja Ganu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 17 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs), excel in natural language understanding, but
their capability for complex mathematical reasoning with an amalgamation of
structured tables and unstructured text is uncertain. This study explores LLMs'
mathematical reasoning on four financial tabular question-answering datasets:
TATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with
various models and prompting techniques, we assess how LLMs adapt to complex
tables and mathematical tasks. We focus on sensitivity to table complexity and
performance variations with an increasing number of arithmetic reasoning steps.
The results provide insights into LLMs' capabilities and limitations in
handling complex mathematical scenarios for semi-structured tables. Ultimately,
we introduce a novel prompting technique tailored to semi-structured documents,
matching or outperforming other baselines in performance while providing a
nuanced understanding of LLMs abilities for such a task.
</p>
</div>
</dd>
<dt><a name="item133">[133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11196" title="Abstract">arXiv:2402.11196</a> [<a href="/pdf/2402.11196" title="Download PDF">pdf</a>, <a href="/format/2402.11196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maintaining Adversarial Robustness in Continuous Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ru%2C+X">Xiaolei Ru</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xiaowei Cao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zijia Liu</a>, 
<a href="/search/cs?searchtype=author&query=Moore%2C+J+M">Jack Murdoch Moore</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin-Ya Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xia Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wenjia Wei</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+G">Gang Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Adversarial robustness is essential for security and reliability of machine
learning systems. However, the adversarial robustness gained by sophisticated
defense algorithms is easily erased as the neural network evolves to learn new
tasks. This vulnerability can be addressed by fostering a novel capability for
neural networks, termed continual robust learning, which focuses on both the
(classification) performance and adversarial robustness on previous tasks
during continuous learning. To achieve continuous robust learning, we propose
an approach called Double Gradient Projection that projects the gradients for
weight updates orthogonally onto two crucial subspaces -- one for stabilizing
the smoothed sample gradients and another for stabilizing the final outputs of
the neural network. The experimental results on four benchmarks demonstrate
that the proposed approach effectively maintains continuous robustness against
strong adversarial attacks, outperforming the baselines formed by combining the
existing defense strategies and continual learning methods.
</p>
</div>
</dd>
<dt><a name="item134">[134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11197" title="Abstract">arXiv:2402.11197</a> [<a href="/pdf/2402.11197" title="Download PDF">pdf</a>, <a href="/format/2402.11197" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Centroid-Based Efficient Minimum Bayes Risk Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deguchi%2C+H">Hiroyuki Deguchi</a>, 
<a href="/search/cs?searchtype=author&query=Sakai%2C+Y">Yusuke Sakai</a>, 
<a href="/search/cs?searchtype=author&query=Kamigaito%2C+H">Hidetaka Kamigaito</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+T">Taro Watanabe</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+H">Hideki Tanaka</a>, 
<a href="/search/cs?searchtype=author&query=Utiyama%2C+M">Masao Utiyama</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Minimum Bayes risk (MBR) decoding achieved state-of-the-art translation
performance by using COMET, a neural metric that has a high correlation with
human evaluation. However, MBR decoding requires quadratic time since it
computes the expected score between a translation hypothesis and all reference
translations. We propose centroid-based MBR (CBMBR) decoding to improve the
speed of MBR decoding. Our method clusters the reference translations in the
feature space, and then calculates the score using the centroids of each
cluster. The experimental results show that our CBMBR not only improved the
decoding speed of the expected score calculation 6.9 times, but also
outperformed vanilla MBR decoding in translation quality by up to 0.5 COMET in
the WMT'22 En$\leftrightarrow$Ja, En$\leftrightarrow$De, En$\leftrightarrow$Zh,
and WMT'23 En$\leftrightarrow$Ja translation tasks.
</p>
</div>
</dd>
<dt><a name="item135">[135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11198" title="Abstract">arXiv:2402.11198</a> [<a href="/pdf/2402.11198" title="Download PDF">pdf</a>, <a href="/format/2402.11198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Achieving Linear Speedup in Asynchronous Federated Learning with  Heterogeneous Clients
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaolu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zijian Li</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Shi Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated learning (FL) is an emerging distributed training paradigm that
aims to learn a common global model without exchanging or transferring the data
that are stored locally at different clients. The Federated Averaging
(FedAvg)-based algorithms have gained substantial popularity in FL to reduce
the communication overhead, where each client conducts multiple localized
iterations before communicating with a central server. In this paper, we focus
on FL where the clients have diverse computation and/or communication
capabilities. Under this circumstance, FedAvg can be less efficient since it
requires all clients that participate in the global aggregation in a round to
initiate iterations from the latest global model, and thus the synchronization
among fast clients and straggler clients can severely slow down the overall
training process. To address this issue, we propose an efficient asynchronous
federated learning (AFL) framework called Delayed Federated Averaging
(DeFedAvg). In DeFedAvg, the clients are allowed to perform local training with
different stale global models at their own paces. Theoretical analyses
demonstrate that DeFedAvg achieves asymptotic convergence rates that are on par
with the results of FedAvg for solving nonconvex problems. More importantly,
DeFedAvg is the first AFL algorithm that provably achieves the desirable linear
speedup property, which indicates its high scalability. Additionally, we carry
out extensive numerical experiments using real datasets to validate the
efficiency and scalability of our approach when training deep neural networks.
</p>
</div>
</dd>
<dt><a name="item136">[136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11199" title="Abstract">arXiv:2402.11199</a> [<a href="/pdf/2402.11199" title="Download PDF">pdf</a>, <a href="/format/2402.11199" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with  Knowledge Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+M">Minh-Vuong Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+L">Linhao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Shiri%2C+F">Fatemeh Shiri</a>, 
<a href="/search/cs?searchtype=author&query=Phung%2C+D">Dinh Phung</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan-Fang Li</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+T">Thuy-Trang Vu</a>, 
<a href="/search/cs?searchtype=author&query=Haffari%2C+G">Gholamreza Haffari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Minh-Vuong Nguyen and Linhao Luo are co-first authors and contributed equally to the preparation of this manuscript
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) demonstrate strong reasoning abilities when
prompted to generate chain-of-thought (CoT) explanations alongside answers.
However, previous research on evaluating LLMs has solely focused on answer
accuracy, neglecting the correctness of the generated CoT. In this paper, we
delve deeper into the CoT reasoning capabilities of LLMs in multi-hop question
answering by utilizing knowledge graphs (KGs). We propose a novel
discriminative and generative CoT evaluation paradigm to assess LLMs' knowledge
of reasoning and the accuracy of the generated CoT. Through experiments
conducted on 5 different families of LLMs across 2 multi-hop question-answering
datasets, we find that LLMs possess sufficient knowledge to perform reasoning.
However, there exists a significant disparity between answer accuracy and
faithfulness of the CoT reasoning generated by LLMs, indicating that they often
arrive at correct answers through incorrect reasoning.
</p>
</div>
</dd>
<dt><a name="item137">[137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11200" title="Abstract">arXiv:2402.11200</a> [<a href="/pdf/2402.11200" title="Download PDF">pdf</a>, <a href="/format/2402.11200" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contraction of Markovian Operators in Orlicz Spaces and Error Bounds for  Markov Chain Monte Carlo
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Esposito%2C+A+R">Amedeo Roberto Esposito</a>, 
<a href="/search/cs?searchtype=author&query=Mondelli%2C+M">Marco Mondelli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Functional Analysis (math.FA); Probability (math.PR)

</div>
<p class="mathjax">We introduce a novel concept of convergence for Markovian processes within
Orlicz spaces, extending beyond the conventional approach associated with $L_p$
spaces. After showing that Markovian operators are contractive in Orlicz
spaces, our key technical contribution is an upper bound on their contraction
coefficient, which admits a closed-form expression. The bound is tight in some
settings, and it recovers well-known results, such as the connection between
contraction and ergodicity, ultra-mixing and Doeblin's minorisation.
Specialising our approach to $L_p$ spaces leads to a significant improvement
upon classical Riesz-Thorin's interpolation methods. Furthermore, by exploiting
the flexibility offered by Orlicz spaces, we can tackle settings where the
stationary distribution is heavy-tailed, a severely under-studied setup. As an
application of the framework put forward in the paper, we introduce tighter
bounds on the mixing time of Markovian processes, better exponential
concentration bounds for MCMC methods, and better lower bounds on the burn-in
period. To conclude, we show how our results can be used to prove the
concentration of measure phenomenon for a sequence of Markovian random
variables.
</p>
</div>
</dd>
<dt><a name="item138">[138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11201" title="Abstract">arXiv:2402.11201</a> [<a href="/pdf/2402.11201" title="Download PDF">pdf</a>, <a href="/format/2402.11201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Decoding Scheme with Successive Aggregation of Multi-Level Features  for Light-Weight Semantic Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoo%2C+J">Jiwon Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jangwon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+G">Gyeonghwan Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multi-scale architecture, including hierarchical vision transformer, has been
commonly applied to high-resolution semantic segmentation to deal with
computational complexity with minimum performance loss. In this paper, we
propose a novel decoding scheme for semantic segmentation in this regard, which
takes multi-level features from the encoder with multi-scale architecture. The
decoding scheme based on a multi-level vision transformer aims to achieve not
only reduced computational expense but also higher segmentation accuracy, by
introducing successive cross-attention in aggregation of the multi-level
features. Furthermore, a way to enhance the multi-level features by the
aggregated semantics is proposed. The effort is focused on maintaining the
contextual consistency from the perspective of attention allocation and brings
improved performance with significantly lower computational cost. Set of
experiments on popular datasets demonstrates superiority of the proposed scheme
to the state-of-the-art semantic segmentation models in terms of computational
cost without loss of accuracy, and extensive ablation studies prove the
effectiveness of ideas proposed.
</p>
</div>
</dd>
<dt><a name="item139">[139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11202" title="Abstract">arXiv:2402.11202</a> [<a href="/pdf/2402.11202" title="Download PDF">pdf</a>, <a href="/format/2402.11202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Scalability and Extensibility of Query Reformulation Modeling in  E-commerce Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Ziqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yupin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Q">Quan Deng</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+J">Jinghui Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Mittal%2C+V">Vivek Mittal</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+J">Jingyuan Deng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Customer behavioral data significantly impacts e-commerce search systems.
However, in the case of less common queries, the associated behavioral data
tends to be sparse and noisy, offering inadequate support to the search
mechanism. To address this challenge, the concept of query reformulation has
been introduced. It suggests that less common queries could utilize the
behavior patterns of their popular counterparts with similar meanings. In
Amazon product search, query reformulation has displayed its effectiveness in
improving search relevance and bolstering overall revenue. Nonetheless,
adapting this method for smaller or emerging businesses operating in regions
with lower traffic and complex multilingual settings poses the challenge in
terms of scalability and extensibility. This study focuses on overcoming this
challenge by constructing a query reformulation solution capable of functioning
effectively, even when faced with limited training data, in terms of quality
and scale, along with relatively complex linguistic characteristics. In this
paper we provide an overview of the solution implemented within Amazon product
search infrastructure, which encompasses a range of elements, including
refining the data mining process, redefining model training objectives, and
reshaping training strategies. The effectiveness of the proposed solution is
validated through online A/B testing on search ranking and Ads matching.
Notably, employing the proposed solution in search ranking resulted in 0.14%
and 0.29% increase in overall revenue in Japanese and Hindi cases,
respectively, and a 0.08\% incremental gain in the English case compared to the
legacy implementation; while in search Ads matching led to a 0.36% increase in
Ads revenue in the Japanese case.
</p>
</div>
</dd>
<dt><a name="item140">[140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11203" title="Abstract">arXiv:2402.11203</a> [<a href="/pdf/2402.11203" title="Download PDF">pdf</a>, <a href="/ps/2402.11203" title="Download PostScript">ps</a>, <a href="/format/2402.11203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring ChatGPT for Next-generation Information Retrieval:  Opportunities and Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yizheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jimmy Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Survey Paper
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Web Intelligence, vol. Pre-press, no. Pre-press, pp. 1-14, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">The rapid advancement of artificial intelligence (AI) has highlighted ChatGPT
as a pivotal technology in the field of information retrieval (IR).
Distinguished from its predecessors, ChatGPT offers significant benefits that
have attracted the attention of both the industry and academic communities.
While some view ChatGPT as a groundbreaking innovation, others attribute its
success to the effective integration of product development and market
strategies. The emergence of ChatGPT, alongside GPT-4, marks a new phase in
Generative AI, generating content that is distinct from training examples and
exceeding the capabilities of the prior GPT-3 model by OpenAI. Unlike the
traditional supervised learning approach in IR tasks, ChatGPT challenges
existing paradigms, bringing forth new challenges and opportunities regarding
text quality assurance, model bias, and efficiency. This paper seeks to examine
the impact of ChatGPT on IR tasks and offer insights into its potential future
developments.
</p>
</div>
</dd>
<dt><a name="item141">[141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11206" title="Abstract">arXiv:2402.11206</a> [<a href="/pdf/2402.11206" title="Download PDF">pdf</a>, <a href="/ps/2402.11206" title="Download PostScript">ps</a>, <a href="/format/2402.11206" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hand Biometrics in Digital Forensics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bera%2C+A">Asish Bera</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharjee%2C+D">Debotosh Bhattacharjee</a>, 
<a href="/search/cs?searchtype=author&query=Nasipuri%2C+M">Mita Nasipuri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Digital forensic is now an unavoidable part for securing the digital world
from identity theft. Higher order of crimes, dealing with a massive database is
really very challenging problem for any intelligent system. Biometric is a
better solution to win over the problems encountered by digital forensics. Many
biometric characteristics are playing their significant roles in forensics over
the decades. The potential benefits and scope of hand based modes in forensics
have been investigated with an illustration of hand geometry verifi-cation
method. It can be applied when effective biometric evidences are properly
unavailable; gloves are damaged, and dirt or any kind of liquid can minimize
the accessibility and reliability of the fingerprint or palmprint. Due to the
crisis of pure uniqueness of hand features for a very large database, it may be
relevant for verification only. Some unimodal and multimodal hand based
biometrics (e.g. hand geometry, palmprint and hand vein) with several feature
extractions, database and verification methods have been discussed with 2D, 3D
and infrared images.
</p>
</div>
</dd>
<dt><a name="item142">[142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11208" title="Abstract">arXiv:2402.11208</a> [<a href="/pdf/2402.11208" title="Download PDF">pdf</a>, <a href="/format/2402.11208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based  Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wenkai Yang</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+X">Xiaohan Bi</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Y">Yankai Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sishuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xu Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contribute equally. Code and data are available at <a href="https://github.com/lancopku/agent-backdoor-attacks">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Leveraging the rapid development of Large Language Models LLMs, LLM-based
agents have been developed to handle various real-world applications, including
finance, healthcare, and shopping, etc. It is crucial to ensure the reliability
and security of LLM-based agents during applications. However, the safety
issues of LLM-based agents are currently under-explored. In this work, we take
the first step to investigate one of the typical safety threats, backdoor
attack, to LLM-based agents. We first formulate a general framework of agent
backdoor attacks, then we present a thorough analysis on the different forms of
agent backdoor attacks. Specifically, from the perspective of the final
attacking outcomes, the attacker can either choose to manipulate the final
output distribution, or only introduce malicious behavior in the intermediate
reasoning process, while keeping the final output correct. Furthermore, the
former category can be divided into two subcategories based on trigger
locations: the backdoor trigger can be hidden either in the user query or in an
intermediate observation returned by the external environment. We propose the
corresponding data poisoning mechanisms to implement the above variations of
agent backdoor attacks on two typical agent tasks, web shopping and tool
utilization. Extensive experiments show that LLM-based agents suffer severely
from backdoor attacks, indicating an urgent need for further research on the
development of defenses against backdoor attacks on LLM-based agents. Warning:
This paper may contain biased content.
</p>
</div>
</dd>
<dt><a name="item143">[143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11209" title="Abstract">arXiv:2402.11209</a> [<a href="/pdf/2402.11209" title="Download PDF">pdf</a>, <a href="/format/2402.11209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Simple in Near-Optimal in Security Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jalota%2C+D">Devansh Jalota</a>, 
<a href="/search/cs?searchtype=author&query=Ostrovsky%2C+M">Michael Ostrovsky</a>, 
<a href="/search/cs?searchtype=author&query=Pavone%2C+M">Marco Pavone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Computational Complexity (cs.CC); Theoretical Economics (econ.TH); Optimization and Control (math.OC)

</div>
<p class="mathjax">Fraudulent or illegal activities are ubiquitous across applications and
involve users bypassing the rule of law, often with the strategic aim of
obtaining some benefit that would otherwise be unattainable within the bounds
of lawful conduct. However, user fraud is detrimental, as it may compromise
safety or impose disproportionate negative externalities on particular
population groups.
<br />To mitigate the potential harms of user fraud, we study the problem of
policing such fraud as a security game between an administrator and users. In
this game, an administrator deploys $R$ security resources (e.g., police
officers) across $L$ locations and levies fines against users engaging in fraud
at those locations. For this security game, we study both welfare and revenue
maximization administrator objectives. In both settings, we show that computing
the optimal administrator strategy is NP-hard and develop natural greedy
algorithm variants for the respective settings that achieve at least half the
welfare or revenue as the welfare-maximizing or revenue-maximizing solutions,
respectively. We also establish a resource augmentation guarantee that our
proposed greedy algorithms with one extra resource, i.e., $R+1$ resources,
achieve at least the same welfare (revenue) as the welfare-maximizing
(revenue-maximizing) outcome with $R$ resources.
<br />Finally, since the welfare and revenue-maximizing solutions can differ
significantly, we present a framework inspired by contract theory, wherein a
revenue-maximizing administrator is compensated through contracts for the
welfare it contributes. Beyond extending our theoretical results in the welfare
and revenue maximization settings to studying equilibrium strategies in the
contract game, we also present numerical experiments highlighting the efficacy
of contracts in bridging the gap between the revenue and welfare-maximizing
administrator outcomes.
</p>
</div>
</dd>
<dt><a name="item144">[144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11215" title="Abstract">arXiv:2402.11215</a> [<a href="/pdf/2402.11215" title="Download PDF">pdf</a>, <a href="/format/2402.11215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdAdaGrad: Adaptive Batch Size Schemes for Adaptive Gradient Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lau%2C+T+T">Tim Tsz-Kit Lau</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Han Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kolar%2C+M">Mladen Kolar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">The choice of batch sizes in stochastic gradient optimizers is critical for
model training. However, the practice of varying batch sizes throughout the
training process is less explored compared to other hyperparameters. We
investigate adaptive batch size strategies derived from adaptive sampling
methods, traditionally applied only in stochastic gradient descent. Given the
significant interplay between learning rates and batch sizes, and considering
the prevalence of adaptive gradient methods in deep learning, we emphasize the
need for adaptive batch size strategies in these contexts. We introduce
AdAdaGrad and its scalar variant AdAdaGradNorm, which incrementally increase
batch sizes during training, while model updates are performed using AdaGrad
and AdaGradNorm. We prove that AdaGradNorm converges with high probability at a
rate of $\mathscr{O}(1/K)$ for finding a first-order stationary point of smooth
nonconvex functions within $K$ iterations. AdaGrad also demonstrates similar
convergence properties when integrated with a novel coordinate-wise variant of
our adaptive batch size strategies. Our theoretical claims are supported by
numerical experiments on various image classification tasks, highlighting the
enhanced adaptability of progressive batching protocols in deep learning and
the potential of such adaptive batch size strategies with adaptive gradient
optimizers in large-scale model training.
</p>
</div>
</dd>
<dt><a name="item145">[145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11217" title="Abstract">arXiv:2402.11217</a> [<a href="/pdf/2402.11217" title="Download PDF">pdf</a>, <a href="/format/2402.11217" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yihang Su</a>, 
<a href="/search/cs?searchtype=author&query=Huan%2C+J">Jingyuan Huan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wenting Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yudi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Cheng-Yi Li</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kao-Jung Chang</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+X">Xiaohan Xin</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Linlin Shen</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+M+R">Michael R. Lyu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The significant breakthroughs of Medical Multi-Modal Large Language Models
(Med-MLLMs) renovate modern healthcare with robust information synthesis and
medical decision support. However, these models are often evaluated on
benchmarks that are unsuitable for the Med-MLLMs due to the intricate nature of
the real-world diagnostic frameworks, which encompass diverse medical
specialties and involve complex clinical decisions. Moreover, these benchmarks
are susceptible to data leakage, since Med-MLLMs are trained on large
assemblies of publicly available data. Thus, an isolated and clinically
representative benchmark is highly desirable for credible Med-MLLMs evaluation.
To this end, we introduce Asclepius, a novel Med-MLLM benchmark that rigorously
and comprehensively assesses model capability in terms of: distinct medical
specialties (cardiovascular, gastroenterology, etc.) and different diagnostic
capacities (perception, disease analysis, etc.). Grounded in 3 proposed core
principles, Asclepius ensures a comprehensive evaluation by encompassing 15
medical specialties, stratifying into 3 main categories and 8 sub-categories of
clinical tasks, and exempting from train-validate contamination. We further
provide an in-depth analysis of 6 Med-MLLMs and compare them with 5 human
specialists, providing insights into their competencies and limitations in
various medical contexts. Our work not only advances the understanding of
Med-MLLMs' capabilities but also sets a precedent for future evaluations and
the safe deployment of these models in clinical environments. We launch and
maintain a leaderboard for community assessment of Med-MLLM capabilities
(https://asclepius-med.github.io/).
</p>
</div>
</dd>
<dt><a name="item146">[146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11218" title="Abstract">arXiv:2402.11218</a> [<a href="/pdf/2402.11218" title="Download PDF">pdf</a>, <a href="/format/2402.11218" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Controlled Text Generation for Large Language Model with Dynamic  Attribute Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xun Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hanyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shichao Song</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+M">Mengting Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xunzhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+F">Feiyu Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+B">Bo Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Controlled Text Generation (CTG) aims to produce texts that exhibit specific
desired attributes. In this study, we introduce a pluggable CTG framework for
Large Language Models (LLMs) named Dynamic Attribute Graphs-based controlled
text generation (DATG). This framework utilizes an attribute scorer to evaluate
the attributes of sentences generated by LLMs and constructs dynamic attribute
graphs. DATG modulates the occurrence of key attribute words and key
anti-attribute words, achieving effective attribute control without
compromising the original capabilities of the model. We conduct experiments
across four datasets in two tasks: toxicity mitigation and sentiment
transformation, employing five LLMs as foundational models. Our findings
highlight a remarkable enhancement in control accuracy, achieving a peak
improvement of 19.29% over baseline methods in the most favorable task across
four datasets. Additionally, we observe a significant decrease in perplexity,
markedly improving text fluency.
</p>
</div>
</dd>
<dt><a name="item147">[147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11221" title="Abstract">arXiv:2402.11221</a> [<a href="/pdf/2402.11221" title="Download PDF">pdf</a>, <a href="/format/2402.11221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MOB-Net: Limb-modularized Uncertainty Torque Learning of Humanoids for  Sensorless External Torque Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lim%2C+D">Daegyu Lim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+M">Myeong-Ju Kim</a>, 
<a href="/search/cs?searchtype=author&query=Cha%2C+J">Junhyeok Cha</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jaeheung Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> submitted to IJRR
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Momentum observer (MOB) can estimate external joint torque without requiring
additional sensors, such as force/torque or joint torque sensors. However, the
estimation performance of MOB deteriorates due to the model uncertainty which
encompasses the modeling errors and the joint friction. Moreover, the
estimation error is significant when MOB is applied to high-dimensional
floating-base humanoids, which prevents the estimated external joint torque
from being used for force control or collision detection in the real humanoid
robot. In this paper, the pure external joint torque estimation method named
MOB-Net, is proposed for humanoids. MOB-Net learns the model uncertainty torque
and calibrates the estimated signal of MOB. The external joint torque can be
estimated in the generalized coordinate including whole-body and virtual joints
of the floating-base robot with only internal sensors (an IMU on the pelvis and
encoders in the joints). Our method substantially reduces the estimation errors
of MOB, and the robust performance of MOB-Net for the unseen data is validated
through extensive simulations, real robot experiments, and ablation studies.
Finally, various collision handling scenarios are presented using the estimated
external joint torque from MOB-Net: contact wrench feedback control for
locomotion, collision detection, and collision reaction for safety.
</p>
</div>
</dd>
<dt><a name="item148">[148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11223" title="Abstract">arXiv:2402.11223</a> [<a href="/pdf/2402.11223" title="Download PDF">pdf</a>, <a href="/format/2402.11223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HEAL: Brain-inspired Hyperdimensional Efficient Active Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%2C+Y">Yang Ni</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Z">Zhuowen Zou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenjun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hanning Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chung%2C+W+Y">William Youngwoo Chung</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+S">Samuel Cho</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+R">Ranganath Krishnan</a>, 
<a href="/search/cs?searchtype=author&query=Mercati%2C+P">Pietro Mercati</a>, 
<a href="/search/cs?searchtype=author&query=Imani%2C+M">Mohsen Imani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Drawing inspiration from the outstanding learning capability of our human
brains, Hyperdimensional Computing (HDC) emerges as a novel computing paradigm,
and it leverages high-dimensional vector presentation and operations for
brain-like lightweight Machine Learning (ML). Practical deployments of HDC have
significantly enhanced the learning efficiency compared to current deep ML
methods on a broad spectrum of applications. However, boosting the data
efficiency of HDC classifiers in supervised learning remains an open question.
In this paper, we introduce Hyperdimensional Efficient Active Learning (HEAL),
a novel Active Learning (AL) framework tailored for HDC classification. HEAL
proactively annotates unlabeled data points via uncertainty and
diversity-guided acquisition, leading to a more efficient dataset annotation
and lowering labor costs. Unlike conventional AL methods that only support
classifiers built upon deep neural networks (DNN), HEAL operates without the
need for gradient or probabilistic computations. This allows it to be
effortlessly integrated with any existing HDC classifier architecture. The key
design of HEAL is a novel approach for uncertainty estimation in HDC
classifiers through a lightweight HDC ensemble with prior hypervectors.
Additionally, by exploiting hypervectors as prototypes (i.e., compact
representations), we develop an extra metric for HEAL to select diverse samples
within each batch for annotation. Our evaluation shows that HEAL surpasses a
diverse set of baselines in AL quality and achieves notably faster acquisition
than many BNN-powered or diversity-guided AL methods, recording 11 times to
40,000 times speedup in acquisition runtime per batch.
</p>
</div>
</dd>
<dt><a name="item149">[149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11224" title="Abstract">arXiv:2402.11224</a> [<a href="/pdf/2402.11224" title="Download PDF">pdf</a>, <a href="/format/2402.11224" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neural Networks with (Low-Precision) Polynomial Approximations: New  Insights and Techniques for Accuracy Improvement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Au%2C+M+H">Man Ho Au</a>, 
<a href="/search/cs?searchtype=author&query=Yiu%2C+S+M">Siu Ming Yiu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Replacing non-polynomial functions (e.g., non-linear activation functions
such as ReLU) in a neural network with their polynomial approximations is a
standard practice in privacy-preserving machine learning. The resulting neural
network, called polynomial approximation of neural network (PANN) in this
paper, is compatible with advanced cryptosystems to enable privacy-preserving
model inference. Using ``highly precise'' approximation, state-of-the-art PANN
offers similar inference accuracy as the underlying backbone model. However,
little is known about the effect of approximation, and existing literature
often determined the required approximation precision empirically. In this
paper, we initiate the investigation of PANN as a standalone object.
Specifically, our contribution is two-fold. Firstly, we provide an explanation
on the effect of approximate error in PANN. In particular, we discovered that
(1) PANN is susceptible to some type of perturbations; and (2) weight
regularisation significantly reduces PANN's accuracy. We support our
explanation with experiments. Secondly, based on the insights from our
investigations, we propose solutions to increase inference accuracy for PANN.
Experiments showed that combination of our solutions is very effective: at the
same precision, our PANN is 10% to 50% more accurate than state-of-the-arts;
and at the same accuracy, our PANN only requires a precision of $2^{-9}$ while
state-of-the-art solution requires a precision of $2^{-12}$ using the ResNet-20
model on CIFAR-10 dataset.
</p>
</div>
</dd>
<dt><a name="item150">[150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11227" title="Abstract">arXiv:2402.11227</a> [<a href="/pdf/2402.11227" title="Download PDF">pdf</a>, <a href="/ps/2402.11227" title="Download PostScript">ps</a>, <a href="/format/2402.11227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Role of Similarity in Detecting Masquerading Files
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oliver%2C+J">Jonathan Oliver</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+J">Jue Mo</a>, 
<a href="/search/cs?searchtype=author&query=Yenkar%2C+S">Susmit Yenkar</a>, 
<a href="/search/cs?searchtype=author&query=Batta%2C+R">Raghav Batta</a>, 
<a href="/search/cs?searchtype=author&query=Josyoula%2C+S">Sekhar Josyoula</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Similarity has been applied to a wide range of security applications,
typically used in machine learning models. We examine the problem posed by
masquerading samples; that is samples crafted by bad actors to be similar or
near identical to legitimate samples. We find that these samples potentially
create significant problems for machine learning solutions. The primary problem
being that bad actors can circumvent machine learning solutions by using
masquerading samples.
<br />We then examine the interplay between digital signatures and machine learning
solutions. In particular, we focus on executable files and code signing. We
offer a taxonomy for masquerading files. We use a combination of similarity and
clustering to find masquerading files. We use the insights gathered in this
process to offer improvements to similarity based and machine learning security
solutions.
</p>
</div>
</dd>
<dt><a name="item151">[151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11231" title="Abstract">arXiv:2402.11231</a> [<a href="/pdf/2402.11231" title="Download PDF">pdf</a>, <a href="/ps/2402.11231" title="Download PostScript">ps</a>, <a href="/format/2402.11231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Security in Blockchain Networks: Anomalies, Frauds, and  Advanced Detection Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Osterrieder%2C+J">Joerg Osterrieder</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+S">Stephen Chan</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+J">Jeffrey Chu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuanyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Misheva%2C+B+H">Branka Hadji Misheva</a>, 
<a href="/search/cs?searchtype=author&query=Mare%2C+C">Codruta Mare</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; General Finance (q-fin.GN)

</div>
<p class="mathjax">Blockchain technology, a foundational distributed ledger system, enables
secure and transparent multi-party transactions. Despite its advantages,
blockchain networks are susceptible to anomalies and frauds, posing significant
risks to their integrity and security. This paper offers a detailed examination
of blockchain's key definitions and properties, alongside a thorough analysis
of the various anomalies and frauds that undermine these networks. It describes
an array of detection and prevention strategies, encompassing statistical and
machine learning methods, game-theoretic solutions, digital forensics,
reputation-based systems, and comprehensive risk assessment techniques. Through
case studies, we explore practical applications of anomaly and fraud detection
in blockchain networks, extracting valuable insights and implications for both
current practice and future research. Moreover, we spotlight emerging trends
and challenges within the field, proposing directions for future investigation
and technological development. Aimed at both practitioners and researchers,
this paper seeks to provide a technical, in-depth overview of anomaly and fraud
detection within blockchain networks, marking a significant step forward in the
search for enhanced network security and reliability.
</p>
</div>
</dd>
<dt><a name="item152">[152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11235" title="Abstract">arXiv:2402.11235</a> [<a href="/pdf/2402.11235" title="Download PDF">pdf</a>, <a href="/format/2402.11235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuhan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peisong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhixun Li</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J+X">Jeffrey Xu Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jia Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">With the development of foundation models such as large language models,
zero-shot transfer learning has become increasingly significant. This is
highlighted by the generative capabilities of NLP models like GPT-4, and the
retrieval-based approaches of CV models like CLIP, both of which effectively
bridge the gap between seen and unseen data. In the realm of graph learning,
the continuous emergence of new graphs and the challenges of human labeling
also amplify the necessity for zero-shot transfer learning, driving the
exploration of approaches that can generalize across diverse graph data without
necessitating dataset-specific and label-specific fine-tuning. In this study,
we extend such paradigms to zero-shot transferability in graphs by introducing
ZeroG, a new framework tailored to enable cross-dataset generalization.
Addressing the inherent challenges such as feature misalignment, mismatched
label spaces, and negative transfer, we leverage a language model to encode
both node attributes and class semantics, ensuring consistent feature
dimensions across datasets. We also propose a prompt-based subgraph sampling
module that enriches the semantic information and structure information of
extracted subgraphs using prompting nodes and neighborhood aggregation,
respectively. We further adopt a lightweight fine-tuning strategy that reduces
the risk of overfitting and maintains the zero-shot learning efficacy of the
language model. The results underscore the effectiveness of our model in
achieving significant cross-dataset zero-shot transferability, opening pathways
for the development of graph foundation models. Especially, ZeroG, as a
zero-shot method, can even achieve results comparable to those of
semi-supervised learning on Pubmed.
</p>
</div>
</dd>
<dt><a name="item153">[153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11237" title="Abstract">arXiv:2402.11237</a> [<a href="/pdf/2402.11237" title="Download PDF">pdf</a>, <a href="/format/2402.11237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Be Persistent: Towards a Unified Solution for Mitigating Shortcuts in  Deep Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dolatabadi%2C+H+M">Hadi M. Dolatabadi</a>, 
<a href="/search/cs?searchtype=author&query=Erfani%2C+S+M">Sarah M. Erfani</a>, 
<a href="/search/cs?searchtype=author&query=Leckie%2C+C">Christopher Leckie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Deep neural networks (DNNs) are vulnerable to shortcut learning: rather than
learning the intended task, they tend to draw inconclusive relationships
between their inputs and outputs. Shortcut learning is ubiquitous among many
failure cases of neural networks, and traces of this phenomenon can be seen in
their generalizability issues, domain shift, adversarial vulnerability, and
even bias towards majority groups. In this paper, we argue that this
commonality in the cause of various DNN issues creates a significant
opportunity that should be leveraged to find a unified solution for shortcut
learning. To this end, we outline the recent advances in topological data
analysis~(TDA), and persistent homology~(PH) in particular, to sketch a unified
roadmap for detecting shortcuts in deep learning. We demonstrate our arguments
by investigating the topological features of computational graphs in DNNs using
two cases of unlearnable examples and bias in decision-making as our test
studies. Our analysis of these two failure cases of DNNs reveals that finding a
unified solution for shortcut learning in DNNs is not out of reach, and TDA can
play a significant role in forming such a framework.
</p>
</div>
</dd>
<dt><a name="item154">[154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11238" title="Abstract">arXiv:2402.11238</a> [<a href="/pdf/2402.11238" title="Download PDF">pdf</a>, <a href="/format/2402.11238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring sustainable alternatives for the deployment of microservices  architectures in the cloud
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cortellessa%2C+V">Vittorio Cortellessa</a>, 
<a href="/search/cs?searchtype=author&query=Di+Pompeo%2C+D">Daniele Di Pompeo</a>, 
<a href="/search/cs?searchtype=author&query=Tucci%2C+M">Michele Tucci</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Conference on Software Architecture 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">As organizations increasingly migrate their applications to the cloud, the
optimization of microservices architectures becomes imperative for achieving
sustainability goals. Nonetheless, sustainable deployments may increase costs
and deteriorate performance, thus the identification of optimal tradeoffs among
these conflicting requirements is a key objective not easy to achieve. This
paper introduces a novel approach to support cloud deployment of microservices
architectures by targeting optimal combinations of application performance,
deployment costs, and power consumption. By leveraging genetic algorithms,
specifically NSGA-II, we automate the generation of alternative architectural
deployments. The results demonstrate the potential of our approach through a
comprehensive assessment of the Train Ticket case study.
</p>
</div>
</dd>
<dt><a name="item155">[155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11239" title="Abstract">arXiv:2402.11239</a> [<a href="/pdf/2402.11239" title="Download PDF">pdf</a>, <a href="/format/2402.11239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CARLA-Autoware-Bridge: Facilitating Autonomous Driving Research with a  Unified Framework for Simulation and Module Development
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaljavesi%2C+G">Gemb Kaljavesi</a>, 
<a href="/search/cs?searchtype=author&query=Kerbl%2C+T">Tobias Kerbl</a>, 
<a href="/search/cs?searchtype=author&query=Betz%2C+T">Tobias Betz</a>, 
<a href="/search/cs?searchtype=author&query=Mitkovskii%2C+K">Kirill Mitkovskii</a>, 
<a href="/search/cs?searchtype=author&query=Diermeyer%2C+F">Frank Diermeyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to 2024 IEEE Intelligent Vehicles Symposium (IV)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Extensive testing is necessary to ensure the safety of autonomous driving
modules. In addition to component tests, the safety assessment of individual
modules also requires a holistic view at system level, which can be carried out
efficiently with the help of simulation. Achieving seamless compatibility
between a modular software stack and simulation is complex and poses a
significant challenge for many researchers. To ensure testing at the system
level with state-of-the-art AV software and simulation software, we have
developed and analyzed a bridge connecting the CARLA simulator with the AV
software Autoware Core/Universe. This publicly available bridge enables
researchers to easily test their modules within the overall software. Our
investigations show that an efficient and reliable communication system has
been established. We provide the simulation bridge as open-source software at
https://github.com/TUMFTM/Carla-Autoware-Bridge
</p>
</div>
</dd>
<dt><a name="item156">[156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11241" title="Abstract">arXiv:2402.11241</a> [<a href="/pdf/2402.11241" title="Download PDF">pdf</a>, <a href="/format/2402.11241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiffPoint: Single and Multi-view Point Cloud Reconstruction with ViT  Based Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yu Feng</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xing Shi</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+M">Mengli Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Y">Yun Xiong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As the task of 2D-to-3D reconstruction has gained significant attention in
various real-world scenarios, it becomes crucial to be able to generate
high-quality point clouds. Despite the recent success of deep learning models
in generating point clouds, there are still challenges in producing
high-fidelity results due to the disparities between images and point clouds.
While vision transformers (ViT) and diffusion models have shown promise in
various vision tasks, their benefits for reconstructing point clouds from
images have not been demonstrated yet. In this paper, we first propose a neat
and powerful architecture called DiffPoint that combines ViT and diffusion
models for the task of point cloud reconstruction. At each diffusion step, we
divide the noisy point clouds into irregular patches. Then, using a standard
ViT backbone that treats all inputs as tokens (including time information,
image embeddings, and noisy patches), we train our model to predict target
points based on input images. We evaluate DiffPoint on both single-view and
multi-view reconstruction tasks and achieve state-of-the-art results.
Additionally, we introduce a unified and flexible feature fusion module for
aggregating image features from single or multiple input images. Furthermore,
our work demonstrates the feasibility of applying unified architectures across
languages and images to improve 3D reconstruction tasks.
</p>
</div>
</dd>
<dt><a name="item157">[157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11242" title="Abstract">arXiv:2402.11242</a> [<a href="/pdf/2402.11242" title="Download PDF">pdf</a>, <a href="/format/2402.11242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning with Imbalanced Noisy Data by Preventing Bias in Sample  Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huafeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+M">Mengmeng Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zeren Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yazhou Yao</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+X">Xian-Sheng Hua</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+H">Heng-Tao Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by IEEE Transactions on Multimedia
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Learning with noisy labels has gained increasing attention because the
inevitable imperfect labels in real-world scenarios can substantially hurt the
deep model performance. Recent studies tend to regard low-loss samples as clean
ones and discard high-loss ones to alleviate the negative impact of noisy
labels. However, real-world datasets contain not only noisy labels but also
class imbalance. The imbalance issue is prone to causing failure in the
loss-based sample selection since the under-learning of tail classes also leans
to produce high losses. To this end, we propose a simple yet effective method
to address noisy labels in imbalanced datasets. Specifically, we propose
Class-Balance-based sample Selection (CBS) to prevent the tail class samples
from being neglected during training. We propose Confidence-based Sample
Augmentation (CSA) for the chosen clean samples to enhance their reliability in
the training process. To exploit selected noisy samples, we resort to
prediction history to rectify labels of noisy samples. Moreover, we introduce
the Average Confidence Margin (ACM) metric to measure the quality of corrected
labels by leveraging the model's evolving training dynamics, thereby ensuring
that low-quality corrected noisy samples are appropriately masked out. Lastly,
consistency regularization is imposed on filtered label-corrected noisy samples
to boost model performance. Comprehensive experimental results on synthetic and
real-world datasets demonstrate the effectiveness and superiority of our
proposed method, especially in imbalanced scenarios. Comprehensive experimental
results on synthetic and real-world datasets demonstrate the effectiveness and
superiority of our proposed method, especially in imbalanced scenarios.
</p>
</div>
</dd>
<dt><a name="item158">[158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11243" title="Abstract">arXiv:2402.11243</a> [<a href="/pdf/2402.11243" title="Download PDF">pdf</a>, <a href="/format/2402.11243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Large Language Models perform Relation-based Argument Mining?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gorur%2C+D">Deniz Gorur</a>, 
<a href="/search/cs?searchtype=author&query=Rago%2C+A">Antonio Rago</a>, 
<a href="/search/cs?searchtype=author&query=Toni%2C+F">Francesca Toni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 9 figures, submitted to ACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Argument mining (AM) is the process of automatically extracting arguments,
their components and/or relations amongst arguments and components from text.
As the number of platforms supporting online debate increases, the need for AM
becomes ever more urgent, especially in support of downstream tasks.
Relation-based AM (RbAM) is a form of AM focusing on identifying agreement
(support) and disagreement (attack) relations amongst arguments. RbAM is a
challenging classification task, with existing methods failing to perform
satisfactorily. In this paper, we show that general-purpose Large Language
Models (LLMs), appropriately primed and prompted, can significantly outperform
the best performing (RoBERTa-based) baseline. Specifically, we experiment with
two open-source LLMs (Llama-2 and Mistral) with ten datasets.
</p>
</div>
</dd>
<dt><a name="item159">[159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11245" title="Abstract">arXiv:2402.11245</a> [<a href="/pdf/2402.11245" title="Download PDF">pdf</a>, <a href="/format/2402.11245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI Model Placement for 6G Networks under Epistemic Uncertainty  Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Liming Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yulei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Parra-Ullauri%2C+J+M">Juan Marcelo Parra-Ullauri</a>, 
<a href="/search/cs?searchtype=author&query=Nejabati%2C+R">Reza Nejabati</a>, 
<a href="/search/cs?searchtype=author&query=Simeonidou%2C+D">Dimitra Simeonidou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">The adoption of Artificial Intelligence (AI) based Virtual Network Functions
(VNFs) has witnessed significant growth, posing a critical challenge in
orchestrating AI models within next-generation 6G networks. Finding optimal AI
model placement is significantly more challenging than placing traditional
software-based VNFs, due to the introduction of numerous uncertain factors by
AI models, such as varying computing resource consumption, dynamic storage
requirements, and changing model performance. To address the AI model placement
problem under uncertainties, this paper presents a novel approach employing a
sequence-to-sequence (S2S) neural network which considers uncertainty
estimations. The S2S model, characterized by its encoding-decoding
architecture, is designed to take the service chain with a number of AI models
as input and produce the corresponding placement of each AI model. To address
the introduced uncertainties, our methodology incorporates the orthonormal
certificate module for uncertainty estimation and utilizes fuzzy logic for
uncertainty representation, thereby enhancing the capabilities of the S2S
model. Experiments demonstrate that the proposed method achieves competitive
results across diverse AI model profiles, network environments, and service
chain requests.
</p>
</div>
</dd>
<dt><a name="item160">[160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11248" title="Abstract">arXiv:2402.11248</a> [<a href="/pdf/2402.11248" title="Download PDF">pdf</a>, <a href="/format/2402.11248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoLLaVO: Crayon Large Language and Vision mOdel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+B">Byung-Kwan Lee</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+B">Beomchan Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+C+W">Chae Won Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ro%2C+Y+M">Yong Man Ro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The remarkable success of Large Language Models (LLMs) and instruction tuning
drives the evolution of Vision Language Models (VLMs) towards a versatile
general-purpose model. Yet, it remains unexplored whether current VLMs
genuinely possess quality object-level image understanding capabilities
determined from 'what objects are in the image?' or 'which object corresponds
to a specified bounding box?'. Our findings reveal that the image understanding
capabilities of current VLMs are strongly correlated with their zero-shot
performance on Vision Language (VL) tasks. This suggests that prioritizing
basic image understanding is crucial for VLMs to excel at VL tasks. To enhance
object-level image understanding, we propose Crayon Large Language and Vision
mOdel (CoLLaVO), which incorporates instruction tuning with crayon prompt as a
new visual prompt tuning scheme based on panoptic color maps. Furthermore, we
present a learning strategy of Dual QLoRA to preserve object-level image
understanding without forgetting it during visual instruction tuning, thereby
achieving a significant leap in zero-shot numerous VL benchmarks.
</p>
</div>
</dd>
<dt><a name="item161">[161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11251" title="Abstract">arXiv:2402.11251</a> [<a href="/pdf/2402.11251" title="Download PDF">pdf</a>, <a href="/format/2402.11251" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM can Achieve Self-Regulation via Hyperparameter Aware Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Siyin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shimin Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tianxiang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jinlan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Q">Qinyuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jiasheng Ye</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Junjie Ye</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In the realm of Large Language Models (LLMs), users commonly employ diverse
decoding strategies and adjust hyperparameters to control the generated text.
However, a critical question emerges: Are LLMs conscious of the existence of
these decoding strategies and capable of regulating themselves? The current
decoding generation process often relies on empirical and heuristic manual
adjustments to hyperparameters based on types of tasks and demands. However,
this process is typically cumbersome, and the decoding hyperparameters may not
always be optimal for each sample. To address the aforementioned challenges, we
propose a novel text generation paradigm termed Hyperparameter Aware Generation
(HAG). By leveraging hyperparameter-aware instruction tuning, the LLM
autonomously determines the optimal decoding strategy and configs based on the
input samples, enabling self-regulation. Our approach eliminates the need for
extensive manual tuning, offering a more autonomous, self-regulate model
behavior. Experimental results spanning six datasets across reasoning,
creativity, translation, and mathematics tasks demonstrate that
hyperparameter-aware instruction tuning empowers the LLMs to self-regulate the
decoding strategy and hyperparameter. HAG extends the current paradigm in the
text generation process, highlighting the feasibility of endowing the LLMs with
self-regulate decoding strategies.
</p>
</div>
</dd>
<dt><a name="item162">[162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11253" title="Abstract">arXiv:2402.11253</a> [<a href="/pdf/2402.11253" title="Download PDF">pdf</a>, <a href="/format/2402.11253" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aligning Large Language Models by On-Policy Self-Judgment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sangkyu Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungdong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Yousefpour%2C+A">Ashkan Yousefpour</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+M">Minjoon Seo</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+K+M">Kang Min Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Youngjae Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint; Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">To align large language models with human preferences, existing research
either utilizes a separate reward model (RM) to perform on-policy learning or
simplifies the training procedure by discarding the on-policy learning and the
need for a separate RM. In this paper, we present a novel alignment framework,
SELF-JUDGE that is (1) on-policy learning and 2) parameter efficient, as it
does not require an additional RM for evaluating the samples for on-policy
learning. To this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT)
to train a single model acting as both a policy and a judge. Specifically, we
view the pairwise judgment task as a special case of the instruction-following
task, choosing the better response from a response pair. Thus, the resulting
model can judge preferences of on-the-fly responses from current policy
initialized from itself. Experimental results show the efficacy of SELF-JUDGE,
outperforming baselines in preference benchmarks. We also show that
self-rejection with oversampling can improve further without an additional
evaluator. Our code is available at https://github.com/oddqueue/self-judge.
</p>
</div>
</dd>
<dt><a name="item163">[163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11254" title="Abstract">arXiv:2402.11254</a> [<a href="/pdf/2402.11254" title="Download PDF">pdf</a>, <a href="/format/2402.11254" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> C-ICL: Contrastive In-context Learning for Information Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mo%2C+Y">Ying Mo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiahao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhoujun Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recently, there has been increasing interest in exploring the capabilities of
advanced large language models (LLMs) in the field of information extraction
(IE), specifically focusing on tasks related to named entity recognition (NER)
and relation extraction (RE). Although researchers are exploring the use of
few-shot information extraction through in-context learning with LLMs, they
tend to focus only on using correct or positive examples for demonstration,
neglecting the potential value of incorporating incorrect or negative examples
into the learning process. In this paper, we present c-ICL, a novel few-shot
technique that leverages both correct and incorrect sample constructions to
create in-context learning demonstrations. This approach enhances the ability
of LLMs to extract entities and relations by utilizing prompts that incorporate
not only the positive samples but also the reasoning behind them. This method
allows for the identification and correction of potential interface errors.
Specifically, our proposed method taps into the inherent contextual information
and valuable information in hard negative samples and the nearest positive
neighbors to the test and then applies the in-context learning demonstrations
based on LLMs. Our experiments on various datasets indicate that c-ICL
outperforms previous few-shot in-context learning methods, delivering
substantial enhancements in performance across a broad spectrum of related
tasks. These improvements are noteworthy, showcasing the versatility of our
approach in miscellaneous scenarios.
</p>
</div>
</dd>
<dt><a name="item164">[164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11260" title="Abstract">arXiv:2402.11260</a> [<a href="/pdf/2402.11260" title="Download PDF">pdf</a>, <a href="/format/2402.11260" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MoRAL: MoE Augmented LoRA for LLMs&#x27; Lifelong Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+M+A">Muhammad Asif Ali</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Cheng-Long Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+L">Lijie Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Di Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Adapting large language models (LLMs) to new domains/tasks and enabling them
to be efficient lifelong learners is a pivotal challenge. In this paper, we
propose MoRAL, i.e., Mixture-of-Experts augmented Low-Rank Adaptation for
Lifelong Learning. MoRAL combines the multi-tasking abilities of MoE with the
fine-tuning abilities of LoRA for effective life-long learning of LLMs. In
contrast to the conventional approaches that use factual triplets as inputs
MoRAL relies on simple question-answer pairs, which is a more practical and
effective strategy for robust and efficient learning. Owing to new data
settings, we introduce a new evaluation benchmark namely: Life Long Learning of
LLM (5L-bench) encompassing a newly curated dataset of question-answer pairs,
and a set of evaluation metrics for rigorous evaluation of MoRAL in open-book
and closed-book settings. Experimental evaluation shows (i) LLMs learn fast in
open-book settings with up to 30.15% improvement in "RA" for Phi-2-2.7B
compared to closed-book (for models fine-tuned with MoRAL); (ii) MoRAL shows
higher performance improvement for models with a greater number of parameters;
(iii) MoRAL is robust to catastrophic forgetting offering better knowledge
retention compared to baselines.
</p>
</div>
</dd>
<dt><a name="item165">[165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11262" title="Abstract">arXiv:2402.11262</a> [<a href="/pdf/2402.11262" title="Download PDF">pdf</a>, <a href="/format/2402.11262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mirror Gradient: Towards Robust Multimodal Recommender Systems via  Exploring Flat Local Minima
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+S">Shanshan Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhongzhan Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Daifeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+W">Wushao Wen</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+J">Jinghui Qin</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Liang Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Multimodal recommender systems utilize various types of information to model
user preferences and item features, helping users discover items aligned with
their interests. The integration of multimodal information mitigates the
inherent challenges in recommender systems, e.g., the data sparsity problem and
cold-start issues. However, it simultaneously magnifies certain risks from
multimodal information inputs, such as information adjustment risk and inherent
noise risk. These risks pose crucial challenges to the robustness of
recommendation models. In this paper, we analyze multimodal recommender systems
from the novel perspective of flat local minima and propose a concise yet
effective gradient strategy called Mirror Gradient (MG). This strategy can
implicitly enhance the model's robustness during the optimization process,
mitigating instability risks arising from multimodal information inputs. We
also provide strong theoretical evidence and conduct extensive empirical
experiments to show the superiority of MG across various multimodal
recommendation models and benchmarks. Furthermore, we find that the proposed MG
can complement existing robust training methods and be easily extended to
diverse advanced recommendation models, making it a promising new and
fundamental paradigm for training multimodal recommender systems. The code is
released at https://github.com/Qrange-group/Mirror-Gradient.
</p>
</div>
</dd>
<dt><a name="item166">[166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11265" title="Abstract">arXiv:2402.11265</a> [<a href="/pdf/2402.11265" title="Download PDF">pdf</a>, <a href="/format/2402.11265" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Literal Descriptions: Understanding and Locating Open-World  Objects Aligned with Human Intentions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yisi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xingjian He</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yichen Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zijia Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinlong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jing Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Visual grounding (VG) aims at locating the foreground entities that match the
given natural language expression. Previous datasets and methods for classic VG
task mainly rely on the prior assumption that the given expression must
literally refer to the target object, which greatly impedes the practical
deployment of agents in real-world scenarios. Since users usually prefer to
provide the intention-based expressions for the desired object instead of
covering all the details, it is necessary for the agents to interpret the
intention-driven instructions. Thus, in this work, we take a step further to
the intention-driven visual-language (V-L) understanding. To promote classic VG
towards human intention interpretation, we propose a new intention-driven
visual grounding (IVG) task and build a largest-scale IVG dataset named
IntentionVG with free-form intention expressions. Considering that practical
agents need to move and find specific targets among various scenarios to
realize the grounding task, our IVG task and IntentionVG dataset have taken the
crucial properties of both multi-scenario perception and egocentric view into
consideration. Besides, various types of models are set up as the baselines to
realize our IVG task. Extensive experiments on our IntentionVG dataset and
baselines demonstrate the necessity and efficacy of our method for the V-L
field. To foster future research in this direction, our newly built dataset and
baselines will be publicly available.
</p>
</div>
</dd>
<dt><a name="item167">[167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11266" title="Abstract">arXiv:2402.11266</a> [<a href="/pdf/2402.11266" title="Download PDF">pdf</a>, <a href="/ps/2402.11266" title="Download PostScript">ps</a>, <a href="/format/2402.11266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Filtered Lie-Trotter splitting for the &quot;good&quot; Boussinesq equation: low  regularity error estimates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ji%2C+L">Lun Ji</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+H">Hang Li</a>, 
<a href="/search/math?searchtype=author&query=Ostermann%2C+A">Alexander Ostermann</a>, 
<a href="/search/math?searchtype=author&query=Su%2C+C">Chunmei Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We investigate a filtered Lie-Trotter splitting scheme for the ``good"
Boussinesq equation and derive an error estimate for initial data with very low
regularity. Through the use of discrete Bourgain spaces, our analysis extends
to initial data in $H^{s}$ for $0&lt;s\leq 2$, overcoming the constraint of
$s&gt;1/2$ imposed by the bilinear estimate in smooth Sobolev spaces. We establish
convergence rates of order $\tau^{s/2}$ in $L^2$ for such levels of regularity.
Our analytical findings are supported by numerical experiments.
</p>
</div>
</dd>
<dt><a name="item168">[168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11271" title="Abstract">arXiv:2402.11271</a> [<a href="/pdf/2402.11271" title="Download PDF">pdf</a>, <a href="/format/2402.11271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human-AI Interactions in the Communication Era: Autophagy Makes Large  Models Achieving Local Optima
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+L">Lijie Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lu Yu</a>, 
<a href="/search/cs?searchtype=author&query=Ali%2C+M+A">Muhammad Asif Ali</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Di Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
<p class="mathjax">The increasing significance of large language and multimodal models in
societal information processing has ignited debates on social safety and
ethics. However, few studies have approached the analysis of these limitations
from the comprehensive perspective of human and artificial intelligence system
interactions. This study investigates biases and preferences when humans and
large models are used as key links in communication. To achieve this, we design
a multimodal dataset and three different experiments to evaluate generative
models in their roles as producers and disseminators of information. Our main
findings highlight that synthesized information is more likely to be
incorporated into model training datasets and messaging than human-generated
information. Additionally, large models, when acting as transmitters of
information, tend to modify and lose specific content selectively.
Conceptually, we present two realistic models of autophagic
("self-consumption") loops to account for the suppression of human-generated
information in the exchange of information between humans and AI systems. We
generalize the declining diversity of social information and the bottleneck in
model performance caused by the above trends to the local optima of large
models.
</p>
</div>
</dd>
<dt><a name="item169">[169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11273" title="Abstract">arXiv:2402.11273</a> [<a href="/pdf/2402.11273" title="Download PDF">pdf</a>, <a href="/format/2402.11273" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-supervised Medical Image Segmentation Method Based on Cross-pseudo  Labeling Leveraging Strong and Weak Data Augmentation Strategies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yifei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenyan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+Y">Yifan Ke</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yiyu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xuezhou Dai</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+F">Feiwei Qin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongquan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaodong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Changmiao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures, accept ISBI2024
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Traditional supervised learning methods have historically encountered certain
constraints in medical image segmentation due to the challenging collection
process, high labeling cost, low signal-to-noise ratio, and complex features
characterizing biomedical images. This paper proposes a semi-supervised model,
DFCPS, which innovatively incorporates the Fixmatch concept. This significantly
enhances the model's performance and generalizability through data augmentation
processing, employing varied strategies for unlabeled data. Concurrently, the
model design gives appropriate emphasis to the generation, filtration, and
refinement processes of pseudo-labels. The novel concept of
cross-pseudo-supervision is introduced, integrating consistency learning with
self-training. This enables the model to fully leverage pseudo-labels from
multiple perspectives, thereby enhancing training diversity. The DFCPS model is
compared with both baseline and advanced models using the publicly accessible
Kvasir-SEG dataset. Across all four subdivisions containing different
proportions of unlabeled data, our model consistently exhibits superior
performance. Our source code is available at
https://github.com/JustlfC03/DFCPS.
</p>
</div>
</dd>
<dt><a name="item170">[170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11279" title="Abstract">arXiv:2402.11279</a> [<a href="/pdf/2402.11279" title="Download PDF">pdf</a>, <a href="/format/2402.11279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Perspective Consistency Enhances Confidence Estimation in Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yejie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Diao%2C+M">Muxi Diao</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+K">Keqing He</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+G">Guanting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weiran Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the deployment of large language models (LLMs), accurate confidence
estimation is critical for assessing the credibility of model predictions.
However, existing methods often fail to overcome the issue of overconfidence on
incorrect answers. In this work, we focus on improving the confidence
estimation of large language models. Considering the fragility of
self-awareness in language models, we introduce a Multi-Perspective Consistency
(MPC) method. We leverage complementary insights from different perspectives
within models (MPC-Internal) and across different models (MPC-Across) to
mitigate the issue of overconfidence arising from a singular viewpoint. The
experimental results on eight publicly available datasets show that our MPC
achieves state-of-the-art performance. Further analyses indicate that MPC can
mitigate the problem of overconfidence and is effectively scalable to other
models.
</p>
</div>
</dd>
<dt><a name="item171">[171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11280" title="Abstract">arXiv:2402.11280</a> [<a href="/pdf/2402.11280" title="Download PDF">pdf</a>, <a href="/ps/2402.11280" title="Download PostScript">ps</a>, <a href="/format/2402.11280" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding high-index saddle dynamics via numerical analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+P">Pingwen Zhang</a>, 
<a href="/search/math?searchtype=author&query=Zheng%2C+X">Xiangcheng Zheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">High-index saddle dynamics (HiSD) serves as a competitive instrument in
searching the any-index saddle points and constructing the solution landscape
of complex systems. The Lagrangian multiplier terms in HiSD ensure the Stiefel
manifold constraint, which, however, are dropped in the commonly-used discrete
HiSD scheme and are replaced by an additional Gram-Schmidt orthonormalization.
Though this scheme has been successfully applied in various fields, it is still
unclear why the above modification does not affect its effectiveness. We
recover the same form as HiSD from this scheme, which not only leads to error
estimates naturally, but indicates that the mechanism of Stiefel manifold
preservation by Lagrangian multiplier terms in HiSD is nearly a Gram-Schmidt
process (such that the above modification is appropriate). The developed
methods are further extended to analyze the more complicated constrained HiSD
on high-dimensional sphere, which reveals more mechanisms of the constrained
HiSD in preserving several manifold properties.
</p>
</div>
</dd>
<dt><a name="item172">[172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11281" title="Abstract">arXiv:2402.11281</a> [<a href="/pdf/2402.11281" title="Download PDF">pdf</a>, <a href="/format/2402.11281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Large Multimodal Models Uncover Deep Semantics Behind Images?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yixin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Q">Qingxiu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+H">Heming Xia</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+Z">Zhifang Sui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Understanding the deep semantics of images is essential in the era dominated
by social media. However, current research works primarily on the superficial
description of images, revealing a notable deficiency in the systematic
investigation of the inherent deep semantics. In this work, we introduce
DEEPEVAL, a comprehensive benchmark to assess Large Multimodal Models' (LMMs)
capacities of visual deep semantics. DEEPEVAL includes human-annotated dataset
and three progressive subtasks: fine-grained description selection, in-depth
title matching, and deep semantics understanding. Utilizing DEEPEVAL, we
evaluate 9 open-source LMMs and GPT-4V(ision).Our evaluation demonstrates a
substantial gap between the deep semantic comprehension capabilities of
existing LMMs and humans. For example, GPT-4V is 30% behind humans in
understanding deep semantics, even though it achieves human-comparable
performance in image description. Further analysis indicates that the
integration of description texts during the inference process notably enhances
LMMs' ability to perceive deep semantics. Furthermore, our dataset is divided
into multiple categories, and we conducted a more detailed analysis within
these categories.
</p>
</div>
</dd>
<dt><a name="item173">[173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11282" title="Abstract">arXiv:2402.11282</a> [<a href="/pdf/2402.11282" title="Download PDF">pdf</a>, <a href="/ps/2402.11282" title="Download PostScript">ps</a>, <a href="/format/2402.11282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grammaticality illusion or ambiguous interpretation? Event-related  potentials reveal the nature of the missing-NP effect in Mandarin  centre-embedded structures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qihang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Caimei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Y">Yu Liao</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Z">Ziman Zhuang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In several languages, omitting a verb phrase (VP) in double centre-embedded
structures creates a grammaticality illusion. Similar illusion also exhibited
in Mandarin missing-NP double centre-embedded structures. However, there is no
consensus on its very nature. Instead of treating it as grammaticality
illusion, we argue that ambiguous interpretations of verbs can best account for
this phenomenon in Mandarin. To further support this hypothesis, we conducted
two electroencephalography (EEG) experiments on quasi double centre-embedded
structures whose complexity is reduced by placing the self-embedding relative
clauses into the sentence's subject position. Experiment 1 showed that similar
phenomenon even exhibited in this structure, evidenced by an absence of P600
effect and a presence of N400 effect. In Experiment 2, providing semantic cues
to reduce ambiguity dispelled this illusion, as evidenced by a P600 effect. We
interpret the results under garden-path theory and propose that word-order
difference may account for this cross-linguistic variation.
</p>
</div>
</dd>
<dt><a name="item174">[174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11283" title="Abstract">arXiv:2402.11283</a> [<a href="/pdf/2402.11283" title="Download PDF">pdf</a>, <a href="/format/2402.11283" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep adaptive sampling for surrogate modeling without labeled data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Wang%2C+X">Xili Wang</a>, 
<a href="/search/math?searchtype=author&query=Tang%2C+K">Kejun Tang</a>, 
<a href="/search/math?searchtype=author&query=Zhai%2C+J">Jiayu Zhai</a>, 
<a href="/search/math?searchtype=author&query=Wan%2C+X">Xiaoliang Wan</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+C">Chao Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Surrogate modeling is of great practical significance for parametric
differential equation systems. In contrast to classical numerical methods,
using physics-informed deep learning methods to construct simulators for such
systems is a promising direction due to its potential to handle high
dimensionality, which requires minimizing a loss over a training set of random
samples. However, the random samples introduce statistical errors, which may
become the dominant errors for the approximation of low-regularity and
high-dimensional problems. In this work, we present a deep adaptive sampling
method for surrogate modeling ($\text{DAS}^2$), where we generalize the deep
adaptive sampling (DAS) method [62] [Tang, Wan and Yang, 2023] to build
surrogate models for low-regularity parametric differential equations. In the
parametric setting, the residual loss function can be regarded as an
unnormalized probability density function (PDF) of the spatial and parametric
variables. This PDF is approximated by a deep generative model, from which new
samples are generated and added to the training set. Since the new samples
match the residual-induced distribution, the refined training set can further
reduce the statistical error in the current approximate solution. We
demonstrate the effectiveness of $\text{DAS}^2$ with a series of numerical
experiments, including the parametric lid-driven 2D cavity flow problem with a
continuous range of Reynolds numbers from 100 to 1000.
</p>
</div>
</dd>
<dt><a name="item175">[175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11285" title="Abstract">arXiv:2402.11285</a> [<a href="/pdf/2402.11285" title="Download PDF">pdf</a>, <a href="/format/2402.11285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair Resource Allocation in Virtualized O-RAN Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aslan%2C+F">Fatih Aslan</a>, 
<a href="/search/cs?searchtype=author&query=Iosifidis%2C+G">George Iosifidis</a>, 
<a href="/search/cs?searchtype=author&query=Ayala-Romero%2C+J+A">Jose A. Ayala-Romero</a>, 
<a href="/search/cs?searchtype=author&query=Garcia-Saavedra%2C+A">Andres Garcia-Saavedra</a>, 
<a href="/search/cs?searchtype=author&query=Costa-Perez%2C+X">Xavier Costa-Perez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to appear in ACM Sigmetrics 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">O-RAN systems and their deployment in virtualized general-purpose computing
platforms (O-Cloud) constitute a paradigm shift expected to bring unprecedented
performance gains. However, these architectures raise new implementation
challenges and threaten to worsen the already-high energy consumption of mobile
networks. This paper presents first a series of experiments which assess the
O-Cloud's energy costs and their dependency on the servers' hardware, capacity
and data traffic properties which, typically, change over time. Next, it
proposes a compute policy for assigning the base station data loads to O-Cloud
servers in an energy-efficient fashion; and a radio policy that determines at
near-real-time the minimum transmission block size for each user so as to avoid
unnecessary energy costs. The policies balance energy savings with performance,
and ensure that both of them are dispersed fairly across the servers and users,
respectively. To cater for the unknown and time-varying parameters affecting
the policies, we develop a novel online learning framework with fairness
guarantees that apply to the entire operation horizon of the system (long-term
fairness). The policies are evaluated using trace-driven simulations and are
fully implemented in an O-RAN compatible system where we measure the energy
costs and throughput in realistic scenarios.
</p>
</div>
</dd>
<dt><a name="item176">[176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11287" title="Abstract">arXiv:2402.11287</a> [<a href="/pdf/2402.11287" title="Download PDF">pdf</a>, <a href="/format/2402.11287" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dense Matchers for Dense Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jel%C3%ADnek%2C+T">Tom&#xe1;&#x161; Jel&#xed;nek</a>, 
<a href="/search/cs?searchtype=author&query=%C5%A0er%C3%BDch%2C+J">Jon&#xe1;&#x161; &#x160;er&#xfd;ch</a>, 
<a href="/search/cs?searchtype=author&query=Matas%2C+J">Ji&#x159;&#xed; Matas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Optical flow is a useful input for various applications, including 3D
reconstruction, pose estimation, tracking, and structure-from-motion. Despite
its utility, the field of dense long-term tracking, especially over wide
baselines, has not been extensively explored. This paper extends the concept of
combining multiple optical flows over logarithmically spaced intervals as
proposed by MFT. We demonstrate the compatibility of MFT with different optical
flow networks, yielding results that surpass their individual performance.
Moreover, we present a simple yet effective combination of these networks
within the MFT framework. This approach proves to be competitive with more
sophisticated, non-causal methods in terms of position prediction accuracy,
highlighting the potential of MFT in enhancing long-term tracking applications.
</p>
</div>
</dd>
<dt><a name="item177">[177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11288" title="Abstract">arXiv:2402.11288</a> [<a href="/pdf/2402.11288" title="Download PDF">pdf</a>, <a href="/ps/2402.11288" title="Download PostScript">ps</a>, <a href="/format/2402.11288" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Surgical Performance in Cardiothoracic Surgery with  Innovations from Computer Vision and Artificial Intelligence: A Narrative  Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Constable%2C+M+D">Merryn D. Constable</a>, 
<a href="/search/cs?searchtype=author&query=Shum%2C+H+P+H">Hubert P. H. Shum</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+S">Stephen Clark</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">When technical requirements are high, and patient outcomes are critical,
opportunities for monitoring and improving surgical skills via objective motion
analysis feedback may be particularly beneficial. This narrative review
synthesises work on technical and non-technical surgical skills, collaborative
task performance, and pose estimation to illustrate new opportunities to
advance cardiothoracic surgical performance with innovations from computer
vision and artificial intelligence. These technological innovations are
critically evaluated in terms of the benefits they could offer the
cardiothoracic surgical community, and any barriers to the uptake of the
technology are elaborated upon. Like some other specialities, cardiothoracic
surgery has relatively few opportunities to benefit from tools with data
capture technology embedded within them (as with robotic-assisted laparoscopic
surgery, for example). In such cases, pose estimation techniques that allow for
movement tracking across a conventional operating field without using
specialist equipment or markers offer considerable potential. With video data
from either simulated or real surgical procedures, these tools can (1) provide
insight into the development of expertise and surgical performance over a
surgeon's career, (2) provide feedback to trainee surgeons regarding areas for
improvement, (3) provide the opportunity to investigate what aspects of skill
may be linked to patient outcomes which can (4) inform the aspects of surgical
skill which should be focused on within training or mentoring programmes.
Classifier or assessment algorithms that use artificial intelligence to 'learn'
what expertise is from expert surgical evaluators could further assist
educators in determining if trainees meet competency thresholds.
</p>
</div>
</dd>
<dt><a name="item178">[178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11290" title="Abstract">arXiv:2402.11290</a> [<a href="/pdf/2402.11290" title="Download PDF">pdf</a>, <a href="/ps/2402.11290" title="Download PostScript">ps</a>, <a href="/format/2402.11290" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring the Emerging Technologies within the Blockchain Landscape
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tareq%2C+M+A">Mohammad Ali Tareq</a>, 
<a href="/search/cs?searchtype=author&query=Tripathi%2C+P">Piyush Tripathi</a>, 
<a href="/search/cs?searchtype=author&query=Issa%2C+N+M">Nurhayati Md Issa</a>, 
<a href="/search/cs?searchtype=author&query=Miraz%2C+M+H">Mahdi H. Miraz</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In: Miraz, M.H., Southall, G., Ali, M., Ware, A. (eds) Emerging
  Technologies in Computing. iCETiC 2023. Lecture Notes of the Institute for
  Computer Sciences, Social Informatics and Telecommunications Engineering, vol
  538. Springer, Cham
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Although blockchain technology was first introduced in 2008 and materialised
in 2009, the early usage of blockchain were mainly limited to financial
technologies, particularly cryptocurrencies. Later, blockchain became a
widespread emerging technology, utilised in multifaceted sectors and
applications. In fact, various new and innovative application of blockchain and
distributed ledger technologies are still continuously being researched and
explored. On the other hand, smart-contracts were first introduced in 1990s,
however, it did not gain enough popularity until being integrated with
blockchain technologies lately. The duo lately been seen as the key to many
innovations in various industries and sectors. So, we took data from 1445
blockchain-related patent documents and tried to map out the historical and
current trends in patenting activities in the blockchain field. This helps us
get a better grasp of how blockchain technologies are evolving and being
tracked. In addition to serving as an indicator of science and technology
growth, patents are also used to judge the research potential and development
of a particular technology.
</p>
</div>
</dd>
<dt><a name="item179">[179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11291" title="Abstract">arXiv:2402.11291</a> [<a href="/pdf/2402.11291" title="Download PDF">pdf</a>, <a href="/format/2402.11291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Puzzle Solving using Reasoning of Large Language Models: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Giadikiaroglou%2C+P">Panagiotis Giadikiaroglou</a>, 
<a href="/search/cs?searchtype=author&query=Lymperaiou%2C+M">Maria Lymperaiou</a>, 
<a href="/search/cs?searchtype=author&query=Filandrianos%2C+G">Giorgos Filandrianos</a>, 
<a href="/search/cs?searchtype=author&query=Stamou%2C+G">Giorgos Stamou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Exploring the capabilities of Large Language Models (LLMs) in puzzle solving
unveils critical insights into their potential and challenges in artificial
intelligence, marking a significant step towards understanding their
applicability in complex reasoning tasks. This survey leverages a unique
taxonomy -- dividing puzzles into rule-based and rule-less categories -- to
critically assess LLMs through various methodologies, including prompting
techniques, neuro-symbolic approaches, and fine-tuning. Through a critical
review of relevant datasets and benchmarks, we assess LLMs' performance,
identifying significant challenges in complex puzzle scenarios. Our findings
highlight the disparity between LLM capabilities and human-like reasoning,
particularly in those requiring advanced logical inference. The survey
underscores the necessity for novel strategies and richer datasets to advance
LLMs' puzzle-solving proficiency and contribute to AI's logical reasoning and
creative problem-solving advancements.
</p>
</div>
</dd>
<dt><a name="item180">[180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11294" title="Abstract">arXiv:2402.11294</a> [<a href="/pdf/2402.11294" title="Download PDF">pdf</a>, <a href="/format/2402.11294" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Power Optimization for Integrated Active and Passive Sensing in DFRC  Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lou%2C+X">Xingliang Lou</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+W">Wenchao Xia</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+K">Kai-Kit Wong</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haitao Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Quek%2C+T+Q+S">Tony Q. S. Quek</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hongbo Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Most existing works on dual-function radar-communication (DFRC) systems
mainly focus on active sensing, but ignore passive sensing. To leverage
multi-static sensing capability, we explore integrated active and passive
sensing (IAPS) in DFRC systems to remedy sensing performance. The multi-antenna
base station (BS) is responsible for communication and active sensing by
transmitting signals to user equipments while detecting a target according to
echo signals. In contrast, passive sensing is performed at the receive access
points (RAPs). We consider both the cases where the capacity of the backhaul
links between the RAPs and BS is unlimited or limited and adopt different
fusion strategies. Specifically, when the backhaul capacity is unlimited, the
BS and RAPs transfer sensing signals they have received to the central
controller (CC) for signal fusion. The CC processes the signals and leverages
the generalized likelihood ratio test detector to determine the present of a
target. However, when the backhaul capacity is limited, each RAP, as well as
the BS, makes decisions independently and sends its binary inference results to
the CC for result fusion via voting aggregation. Then, aiming at maximize the
target detection probability under communication quality of service
constraints, two power optimization algorithms are proposed. Finally, numerical
simulations demonstrate that the sensing performance in case of unlimited
backhaul capacity is much better than that in case of limited backhaul
capacity. Moreover, it implied that the proposed IAPS scheme outperforms
only-passive and only-active sensing schemes, especially in unlimited capacity
case.
</p>
</div>
</dd>
<dt><a name="item181">[181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11295" title="Abstract">arXiv:2402.11295</a> [<a href="/pdf/2402.11295" title="Download PDF">pdf</a>, <a href="/format/2402.11295" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OneBit: Towards Extremely Low-bit Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuzhuang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zonghan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qingfu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Weidong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Che%2C+W">Wanxiang Che</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 6 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Model quantification uses low bit-width values to represent the weight
matrices of models, which is a promising approach to reduce both storage and
computational overheads of deploying highly anticipated LLMs. However, existing
quantization methods suffer severe performance degradation when the bit-width
is extremely reduced, and thus focus on utilizing 4-bit or 8-bit values to
quantize models. This paper boldly quantizes the weight matrices of LLMs to
1-bit, paving the way for the extremely low bit-width deployment of LLMs. For
this target, we introduce a 1-bit quantization-aware training (QAT) framework
named OneBit, including a novel 1-bit parameter representation method to better
quantize LLMs as well as an effective parameter initialization method based on
matrix decomposition to improve the convergence speed of the QAT framework.
Sufficient experimental results indicate that OneBit achieves good performance
(at least 83% of the non-quantized performance) with robust training processes
when only using 1-bit weight matrices.
</p>
</div>
</dd>
<dt><a name="item182">[182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11296" title="Abstract">arXiv:2402.11296</a> [<a href="/pdf/2402.11296" title="Download PDF">pdf</a>, <a href="/format/2402.11296" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dissecting Human and LLM Preferences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junlong Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+F">Fan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+S">Shichao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yikai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Pengfei Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As a relative quality comparison of model responses, human and Large Language
Model (LLM) preferences serve as common alignment goals in model fine-tuning
and criteria in evaluation. Yet, these preferences merely reflect broad
tendencies, resulting in less explainable and controllable models with
potential safety risks. In this work, we dissect the preferences of human and
32 different LLMs to understand their quantitative composition, using
annotations from real-world user-model conversations for a fine-grained,
scenario-wise analysis. We find that humans are less sensitive to errors, favor
responses that support their stances, and show clear dislike when models admit
their limits. On the contrary, advanced LLMs like GPT-4-Turbo emphasize
correctness, clarity, and harmlessness more. Additionally, LLMs of similar
sizes tend to exhibit similar preferences, regardless of their training
methods, and fine-tuning for alignment does not significantly alter the
preferences of pretrained-only LLMs. Finally, we show that preference-based
evaluation can be intentionally manipulated. In both training-free and
training-based settings, aligning a model with the preferences of judges boosts
scores, while injecting the least preferred properties lowers them. This
results in notable score shifts: up to 0.59 on MT-Bench (1-10 scale) and 31.94
on AlpacaEval 2.0 (0-100 scale), highlighting the significant impact of this
strategic adaptation. Interactive Demo:
https://huggingface.co/spaces/GAIR/Preference-Dissection-Visualization Dataset:
https://huggingface.co/datasets/GAIR/preference-dissection Code:
https://github.com/GAIR-NLP/Preference-Dissection
</p>
</div>
</dd>
<dt><a name="item183">[183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11297" title="Abstract">arXiv:2402.11297</a> [<a href="/pdf/2402.11297" title="Download PDF">pdf</a>, <a href="/format/2402.11297" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MMMModal -- Multi-Images Multi-Audio Multi-turn Multi-Modal
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zolkepli%2C+H">Husein Zolkepli</a>, 
<a href="/search/cs?searchtype=author&query=Razak%2C+A">Aisyah Razak</a>, 
<a href="/search/cs?searchtype=author&query=Adha%2C+K">Kamarul Adha</a>, 
<a href="/search/cs?searchtype=author&query=Nazhan%2C+A">Ariff Nazhan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Our contribution introduces a groundbreaking multimodal large language model
designed to comprehend multi-images, multi-audio, and multi-images-multi-audio
within a single multiturn session. Leveraging state-of-the-art models, we
utilize the SigLIP encoder for visual inputs and the Whisper Encoder for audio
inputs. Notably, this multimodal large language model is bilingual, proficient
in understanding both English and Malay simultaneously. We proudly unveil two
versions of this model: TinyLlama with 1.1B parameters, and Mistral with 7B
parameters. With its ability to navigate diverse modalities and languages, our
model represents a significant advancement for the Malaysian context and
beyond.
<br />All models released at
https://huggingface.co/collections/mesolitica/multimodal-malaysian-llm-65c6f893e03f78fa9e5c8859
</p>
</div>
</dd>
<dt><a name="item184">[184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11299" title="Abstract">arXiv:2402.11299</a> [<a href="/pdf/2402.11299" title="Download PDF">pdf</a>, <a href="/format/2402.11299" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quasi-optimal complexity $hp$-FEM for Poisson on a rectangle
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Knook%2C+K">Kars Knook</a>, 
<a href="/search/math?searchtype=author&query=Olver%2C+S">Sheehan Olver</a>, 
<a href="/search/math?searchtype=author&query=Papadopoulos%2C+I+P+A">Ioannis P. A. Papadopoulos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We show, in one dimension, that an $hp$-Finite Element Method ($hp$-FEM)
discretisation can be solved in optimal complexity because the discretisation
has a special sparsity structure that ensures that the \emph{reverse Cholesky
factorisation} -- Cholesky starting from the bottom right instead of the top
left -- remains sparse. Moreover, computing and inverting the factorisation
almost entirely trivially parallelises across the different elements. By
incorporating this approach into an Alternating Direction Implicit (ADI) method
\`a la Fortunato and Townsend (2020) we can solve, within a prescribed
tolerance, an $hp$-FEM discretisation of the (screened) Poisson equation on a
rectangle, in parallel, with quasi-optimal complexity: $O(N^2 \log N)$
operations where $N$ is the maximal total degrees of freedom in each dimension.
When combined with fast Legendre transforms we can also solve nonlinear
time-evolution partial differential equations in a quasi-optimal complexity of
$O(N^2 \log^2 N)$ operations, which we demonstrate on the (viscid) Burgers'
equation.
</p>
</div>
</dd>
<dt><a name="item185">[185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11301" title="Abstract">arXiv:2402.11301</a> [<a href="/pdf/2402.11301" title="Download PDF">pdf</a>, <a href="/format/2402.11301" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReViT: Enhancing Vision Transformers with Attention Residual Connections  for Visual Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Diko%2C+A">Anxhelo Diko</a>, 
<a href="/search/cs?searchtype=author&query=Avola%2C+D">Danilo Avola</a>, 
<a href="/search/cs?searchtype=author&query=Cascio%2C+M">Marco Cascio</a>, 
<a href="/search/cs?searchtype=author&query=Cinque%2C+L">Luigi Cinque</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, single column, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Vision Transformer (ViT) self-attention mechanism is characterized by feature
collapse in deeper layers, resulting in the vanishing of low-level visual
features. However, such features can be helpful to accurately represent and
identify elements within an image and increase the accuracy and robustness of
vision-based recognition systems. Following this rationale, we propose a novel
residual attention learning method for improving ViT-based architectures,
increasing their visual feature diversity and model robustness. In this way,
the proposed network can capture and preserve significant low-level features,
providing more details about the elements within the scene being analyzed. The
effectiveness and robustness of the presented method are evaluated on five
image classification benchmarks, including ImageNet1k, CIFAR10, CIFAR100,
Oxford Flowers-102, and Oxford-IIIT Pet, achieving improved performances.
Additionally, experiments on the COCO2017 dataset show that the devised
approach discovers and incorporates semantic and spatial relationships for
object detection and instance segmentation when implemented into spatial-aware
transformer models.
</p>
</div>
</dd>
<dt><a name="item186">[186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11302" title="Abstract">arXiv:2402.11302</a> [<a href="/pdf/2402.11302" title="Download PDF">pdf</a>, <a href="/format/2402.11302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Graph-based Session Recommendation with Adaptive Propagation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Javari%2C+A">Amin Javari</a>, 
<a href="/search/cs?searchtype=author&query=Balaji%2C+J">Janani Balaji</a>, 
<a href="/search/cs?searchtype=author&query=Shalaby%2C+W">Walid Shalaby</a>, 
<a href="/search/cs?searchtype=author&query=Derr%2C+T">Tyler Derr</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+X">Xiquan Cui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Session-based recommender systems (SBRSs) predict users' next interacted
items based on their historical activities. While most SBRSs capture purchasing
intentions locally within each session, capturing items' global information
across different sessions is crucial in characterizing their general
properties. Previous works capture this cross-session information by
constructing graphs and incorporating neighbor information. However, this
incorporation cannot vary adaptively according to the unique intention of each
session, and the constructed graphs consist of only one type of user-item
interaction. To address these limitations, we propose knowledge graph-based
session recommendation with session-adaptive propagation. Specifically, we
build a knowledge graph by connecting items with multi-typed edges to
characterize various user-item interactions. Then, we adaptively aggregate
items' neighbor information considering user intention within the learned
session. Experimental results demonstrate that equipping our constructed
knowledge graph and session-adaptive propagation enhances session
recommendation backbones by 10%-20%. Moreover, we provide an industrial case
study showing our proposed framework achieves 2% performance boost over an
existing well-deployed model at The Home Depot e-platform.
</p>
</div>
</dd>
<dt><a name="item187">[187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11303" title="Abstract">arXiv:2402.11303</a> [<a href="/pdf/2402.11303" title="Download PDF">pdf</a>, <a href="/format/2402.11303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FViT: A Focal Vision Transformer with Gabor Filter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yulong Shi</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Mingwei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yongshuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hui Sun</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zengqiang Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Vision transformers have achieved encouraging progress in various computer
vision tasks. A common belief is that this is attributed to the competence of
self-attention in modeling the global dependencies among feature tokens.
Unfortunately, self-attention still faces some challenges in dense prediction
tasks, such as the high computational complexity and absence of desirable
inductive bias. To address these above issues, we revisit the potential
benefits of integrating vision transformer with Gabor filter, and propose a
Learnable Gabor Filter (LGF) by using convolution. As an alternative to
self-attention, we employ LGF to simulate the response of simple cells in the
biological visual system to input images, prompting models to focus on
discriminative feature representations of targets from various scales and
orientations. Additionally, we designed a Bionic Focal Vision (BFV) block based
on the LGF. This block draws inspiration from neuroscience and introduces a
Multi-Path Feed Forward Network (MPFFN) to emulate the working way of
biological visual cortex processing information in parallel. Furthermore, we
develop a unified and efficient pyramid backbone network family called Focal
Vision Transformers (FViTs) by stacking BFV blocks. Experimental results show
that FViTs exhibit highly competitive performance in various vision tasks.
Especially in terms of computational efficiency and scalability, FViTs show
significantly advantages compared with other counterparts.
</p>
</div>
</dd>
<dt><a name="item188">[188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11305" title="Abstract">arXiv:2402.11305</a> [<a href="/pdf/2402.11305" title="Download PDF">pdf</a>, <a href="/format/2402.11305" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Good Practices for Task-Specific Distillation of Large Pretrained  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marrie%2C+J">Juliette Marrie</a>, 
<a href="/search/cs?searchtype=author&query=Arbel%2C+M">Michael Arbel</a>, 
<a href="/search/cs?searchtype=author&query=Mairal%2C+J">Julien Mairal</a>, 
<a href="/search/cs?searchtype=author&query=Larlus%2C+D">Diane Larlus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Large pretrained visual models exhibit remarkable generalization across
diverse recognition tasks. Yet, real-world applications often demand compact
models tailored to specific problems. Variants of knowledge distillation have
been devised for such a purpose, enabling task-specific compact models (the
students) to learn from a generic large pretrained one (the teacher). In this
paper, we show that the excellent robustness and versatility of recent
pretrained models challenge common practices established in the literature,
calling for a new set of optimal guidelines for task-specific distillation. To
address the lack of samples in downstream tasks, we also show that a variant of
Mixup based on stable diffusion complements standard data augmentation. This
strategy eliminates the need for engineered text prompts and improves
distillation of generic models into streamlined specialized networks.
</p>
</div>
</dd>
<dt><a name="item189">[189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11306" title="Abstract">arXiv:2402.11306</a> [<a href="/pdf/2402.11306" title="Download PDF">pdf</a>, <a href="/ps/2402.11306" title="Download PostScript">ps</a>, <a href="/format/2402.11306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear and Non-Linear Models for Master Scheduling of Dynamic Resources  Product Mix
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mohammed%2C+A+R">Ayman R. Mohammed</a>, 
<a href="/search/cs?searchtype=author&query=Sleem%2C+A+A">Ahmad Abu Sleem</a>, 
<a href="/search/cs?searchtype=author&query=Abdel-Aal%2C+M+A+M">Mohammad A. M. Abdel-Aal</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Computers and Industrial Engineering, ISBN 9781713886952 (2023),
  917-928
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">The literature on master production scheduling for product mix problems under
the Theory of Constraints (TOC) was considered by many previous studies. Most
studies assume a static resources availability. In this study, the raw
materials supplied to the manufacturer is considered as dynamic depending on
the results of the problem. Thus, an integer linear heuristic, an integer
non-linear optimization model, and a basic non-linear model are developed to
find a good solution of the problem. The results of the three models were
compared to each other in terms of profit, raw materials costs, inventory costs
and raw materials utilization. Recent studies in the field are reviewed and
conclusions are drawn.
</p>
</div>
</dd>
<dt><a name="item190">[190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11307" title="Abstract">arXiv:2402.11307</a> [<a href="/pdf/2402.11307" title="Download PDF">pdf</a>, <a href="/format/2402.11307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ICHPro: Intracerebral Hemorrhage Prognosis Classification Via  Joint-attention Fusion-based 3d Cross-modal Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xinlei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+R">Ruiquan Ge</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shibin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Elazab%2C+A">Ahmed Elazab</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jichao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lingyan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+G">Gangyong Jia</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Taosheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Changmiao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages,4 figures, 4 tables, accepted by ISBI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Intracerebral Hemorrhage (ICH) is the deadliest subtype of stroke,
necessitating timely and accurate prognostic evaluation to reduce mortality and
disability. However, the multi-factorial nature and complexity of ICH make
methods based solely on computed tomography (CT) image features inadequate.
Despite the capacity of cross-modal networks to fuse additional information,
the effective combination of different modal features remains a significant
challenge. In this study, we propose a joint-attention fusion-based 3D
cross-modal network termed ICHPro that simulates the ICH prognosis
interpretation process utilized by neurosurgeons. ICHPro includes a
joint-attention fusion module to fuse features from CT images with demographic
and clinical textual data. To enhance the representation of cross-modal
features, we introduce a joint loss function. ICHPro facilitates the extraction
of richer cross-modal features, thereby improving classification performance.
Upon testing our method using a five-fold cross-validation, we achieved an
accuracy of 89.11%, an F1 score of 0.8767, and an AUC value of 0.9429. These
results outperform those obtained from other advanced methods based on the test
dataset, thereby demonstrating the superior efficacy of ICHPro. The code is
available at our Github: https://github.com/YU-deep/ICH.
</p>
</div>
</dd>
<dt><a name="item191">[191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11314" title="Abstract">arXiv:2402.11314</a> [<a href="/pdf/2402.11314" title="Download PDF">pdf</a>, <a href="/format/2402.11314" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Generative Agent Collective Decision-Making in Urban Planning: A  Case Study for Kendall Square Renovation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hanyong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Dao%2C+L">Luc Dao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this study, we develop a multiple-generative agent system to simulate
community decision-making for the redevelopment of Kendall Square's Volpe
building. Drawing on interviews with local stakeholders, our simulations
incorporated varying degrees of communication, demographic data, and life
values in the agent prompts. The results revealed that communication among
agents improved collective reasoning, while the inclusion of demographic and
life values led to more distinct opinions. These findings highlight the
potential application of AI in understanding complex social interactions and
decision-making processes, offering valuable insights for urban planning and
community engagement in diverse settings like Kendall Square.
</p>
</div>
</dd>
<dt><a name="item192">[192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11317" title="Abstract">arXiv:2402.11317</a> [<a href="/pdf/2402.11317" title="Download PDF">pdf</a>, <a href="/format/2402.11317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Debiased Offline Representation Learning for Fast Online Adaptation in  Non-stationary Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+W">Wenjie Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yi-Chen Li</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Lei Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+C">Chengxing Jia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zongzhang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yang Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Developing policies that can adjust to non-stationary environments is
essential for real-world reinforcement learning applications. However, learning
such adaptable policies in offline settings, with only a limited set of
pre-collected trajectories, presents significant challenges. A key difficulty
arises because the limited offline data makes it hard for the context encoder
to differentiate between changes in the environment dynamics and shifts in the
behavior policy, often leading to context misassociations. To address this
issue, we introduce a novel approach called Debiased Offline Representation for
fast online Adaptation (DORA). DORA incorporates an information bottleneck
principle that maximizes mutual information between the dynamics encoding and
the environmental data, while minimizing mutual information between the
dynamics encoding and the actions of the behavior policy. We present a
practical implementation of DORA, leveraging tractable bounds of the
information bottleneck principle. Our experimental evaluation across six
benchmark MuJoCo tasks with variable parameters demonstrates that DORA not only
achieves a more precise dynamics encoding but also significantly outperforms
existing baselines in terms of performance.
</p>
</div>
</dd>
<dt><a name="item193">[193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11318" title="Abstract">arXiv:2402.11318</a> [<a href="/pdf/2402.11318" title="Download PDF">pdf</a>, <a href="/format/2402.11318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BiasBuster: a Neural Approach for Accurate Estimation of Population  Statistics using Biased Location Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeighami%2C+S">Sepanta Zeighami</a>, 
<a href="/search/cs?searchtype=author&query=Shahabi%2C+C">Cyrus Shahabi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Databases (cs.DB)

</div>
<p class="mathjax">While extremely useful (e.g., for COVID-19 forecasting and policy-making,
urban mobility analysis and marketing, and obtaining business insights),
location data collected from mobile devices often contain data from a biased
population subset, with some communities over or underrepresented in the
collected datasets. As a result, aggregate statistics calculated from such
datasets (as is done by various companies including Safegraph, Google, and
Facebook), while ignoring the bias, leads to an inaccurate representation of
population statistics. Such statistics will not only be generally inaccurate,
but the error will disproportionately impact different population subgroups
(e.g., because they ignore the underrepresented communities). This has dire
consequences, as these datasets are used for sensitive decision-making such as
COVID-19 policymaking. This paper tackles the problem of providing accurate
population statistics using such biased datasets. We show that statistical
debiasing, although in some cases useful, often fails to improve accuracy. We
then propose BiasBuster, a neural network approach that utilizes the
correlations between population statistics and location characteristics to
provide accurate estimates of population statistics. Extensive experiments on
real-world data show that BiasBuster improves accuracy by up to 2 times in
general and up to 3 times for underrepresented populations.
</p>
</div>
</dd>
<dt><a name="item194">[194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11319" title="Abstract">arXiv:2402.11319</a> [<a href="/pdf/2402.11319" title="Download PDF">pdf</a>, <a href="/ps/2402.11319" title="Download PostScript">ps</a>, <a href="/format/2402.11319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hysteresis Compensation of Flexible Continuum Manipulator using RGBD  Sensing and Temporal Convolutional Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Junhyun Park</a>, 
<a href="/search/cs?searchtype=author&query=Jang%2C+S">Seonghyeok Jang</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+H">Hyojae Park</a>, 
<a href="/search/cs?searchtype=author&query=Bae%2C+S">Seongjun Bae</a>, 
<a href="/search/cs?searchtype=author&query=Hwang%2C+M">Minho Hwang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 12 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Flexible continuum manipulators are valued for minimally invasive surgery,
offering access to confined spaces through nonlinear paths. However,
cable-driven manipulators face control difficulties due to hysteresis from
cabling effects such as friction, elongation, and coupling. These effects are
difficult to model due to nonlinearity and the difficulties become even more
evident when dealing with long and multi-segmented manipulator. This paper
proposes a data-driven approach based on recurrent neural networks to capture
these nonlinear and previous states-dependent characteristics of cable
actuation. We design customized fiducial markers to collect physical joint
configurations as a dataset. Result on a study comparing the learning
performance of four Deep Neural Network (DNN) models show that the Temporal
Convolution Network (TCN) demonstrates the highest predictive capability.
Leveraging trained TCNs, we build a control algorithm to compensate for
hysteresis. Tracking tests in task space using unseen trajectories show that
the best controller reduces the mean position and orientation error by 61.39%
(from 13.7 mm to 5.29 mm) and 64.04% (from 31.17{\deg} to 11.21{\deg}),
respectively.
</p>
</div>
</dd>
<dt><a name="item195">[195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11322" title="Abstract">arXiv:2402.11322</a> [<a href="/pdf/2402.11322" title="Download PDF">pdf</a>, <a href="/format/2402.11322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for  Spiking Neural Network Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Putra%2C+R+V+W">Rachmad Vidya Wicaksana Putra</a>, 
<a href="/search/cs?searchtype=author&query=Shafique%2C+M">Muhammad Shafique</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Spiking Neural Networks (SNNs) offer a promising solution to achieve ultra
low-power/energy computation for solving machine learning tasks. Currently,
most of the SNN architectures are derived from Artificial Neural Networks whose
neurons' architectures and operations are different from SNNs, or developed
without considering memory budgets from the underlying processing hardware.
These limitations hinder the SNNs from reaching their full potential in
accuracy and efficiency. Towards this, we propose SpikeNAS, a novel
memory-aware neural architecture search (NAS) framework for SNNs that can
quickly find an appropriate SNN architecture with high accuracy under the given
memory budgets. To do this, our SpikeNAS employs several key steps: analyzing
the impacts of network operations on the accuracy, enhancing the network
architecture to improve the learning quality, and developing a fast
memory-aware search algorithm. The experimental results show that our SpikeNAS
improves the searching time and maintains high accuracy as compared to
state-of-the-art while meeting the given memory budgets (e.g., 4.4x faster
search with 1.3% accuracy improvement for CIFAR100, using an Nvidia RTX 6000
Ada GPU machine), thereby quickly providing the appropriate SNN architecture
for memory-constrained SNN-based systems.
</p>
</div>
</dd>
<dt><a name="item196">[196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11323" title="Abstract">arXiv:2402.11323</a> [<a href="/pdf/2402.11323" title="Download PDF">pdf</a>, <a href="/format/2402.11323" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Development of Automated Knowledge Maps and Databases for  Materials Engineering using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prasad%2C+D">Deepak Prasad</a>, 
<a href="/search/cs?searchtype=author&query=Pimpude%2C+M">Mayur Pimpude</a>, 
<a href="/search/cs?searchtype=author&query=Alankar%2C+A">Alankar Alankar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
<p class="mathjax">In this work a Large Language Model (LLM) based workflow is presented that
utilizes OpenAI ChatGPT model GPT-3.5-turbo-1106 and Google Gemini Pro model to
create summary of text, data and images from research articles. It is
demonstrated that by using a series of processing, the key information can be
arranged in tabular form and knowledge graphs to capture underlying concepts.
Our method offers efficiency and comprehension, enabling researchers to extract
insights more effectively. Evaluation based on a diverse Scientific Paper
Collection demonstrates our approach in facilitating discovery of knowledge.
This work contributes to accelerated material design by smart literature
review. The method has been tested based on various qualitative and
quantitative measures of gathered information. The ChatGPT model achieved an F1
score of 0.40 for an exact match (ROUGE-1, ROUGE-2) but an impressive 0.479 for
a relaxed match (ROUGE-L, ROUGE-Lsum) structural data format in performance
evaluation. The Google Gemini Pro outperforms ChatGPT with an F1 score of 0.50
for an exact match and 0.63 for a relaxed match. This method facilitates
high-throughput development of a database relevant to materials informatics.
For demonstration, an example of data extraction and knowledge graph formation
based on a manuscript about a titanium alloy is discussed.
</p>
</div>
</dd>
<dt><a name="item197">[197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11324" title="Abstract">arXiv:2402.11324</a> [<a href="/pdf/2402.11324" title="Download PDF">pdf</a>, <a href="/format/2402.11324" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EVEDIT: Event-based Knowledge Editing with Deductive Editing Boundaries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiateng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P">Pengfei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sha Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zixuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The dynamic nature of real-world information necessitates efficient knowledge
editing (KE) in large language models (LLMs) for knowledge updating. However,
current KE approaches, which typically operate on (subject, relation, object)
triples, ignore the contextual information and the relation among different
knowledge. Such editing methods could thus encounter an uncertain editing
boundary, leaving a lot of relevant knowledge in ambiguity: Queries that could
be answered pre-edit cannot be reliably answered afterward. In this work, we
analyze this issue by introducing a theoretical framework for KE that
highlights an overlooked set of knowledge that remains unchanged and aids in
knowledge deduction during editing, which we name as the deduction anchor. We
further address this issue by proposing a novel task of event-based knowledge
editing that pairs facts with event descriptions. This task manifests not only
a closer simulation of real-world editing scenarios but also a more logically
sound setting, implicitly defining the deduction anchor to address the issue of
indeterminate editing boundaries. We empirically demonstrate the superiority of
event-based editing over the existing setting on resolving uncertainty in
edited models, and curate a new benchmark dataset EvEdit derived from the
CounterFact dataset. Moreover, while we observe that the event-based setting is
significantly challenging for existing approaches, we propose a novel approach
Self-Edit that showcases stronger performance, achieving 55.6% consistency
improvement while maintaining the naturalness of generation.
</p>
</div>
</dd>
<dt><a name="item198">[198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11325" title="Abstract">arXiv:2402.11325</a> [<a href="/pdf/2402.11325" title="Download PDF">pdf</a>, <a href="/format/2402.11325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatEarthNet: A Global-Scale, High-Quality Image-Text Dataset for Remote  Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zhenghang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zhitong Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Mou%2C+L">Lichao Mou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X+X">Xiao Xiang Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">An in-depth comprehension of global land cover is essential in Earth
observation, forming the foundation for a multitude of applications. Although
remote sensing technology has advanced rapidly, leading to a proliferation of
satellite imagery, the inherent complexity of these images often makes them
difficult for non-expert users to understand. Natural language, as a carrier of
human knowledge, can be a bridge between common users and complicated satellite
imagery. In this context, we introduce a global-scale, high-quality image-text
dataset for remote sensing, providing natural language descriptions for
Sentinel-2 data to facilitate the understanding of satellite imagery for common
users. Specifically, we utilize Sentinel-2 data for its global coverage as the
foundational image source, employing semantic segmentation labels from the
European Space Agency's (ESA) WorldCover project to enrich the descriptions of
land covers. By conducting in-depth semantic analysis, we formulate detailed
prompts to elicit rich descriptions from ChatGPT. To enhance the dataset's
quality, we introduce the manual verification process. This step involves
manual inspection and correction to refine the dataset, thus significantly
improving its accuracy and quality. Finally, we offer the community
ChatEarthNet, a large-scale image-text dataset characterized by global
coverage, high quality, wide-ranging diversity, and detailed descriptions.
ChatEarthNet consists of 163,488 image-text pairs with captions generated by
ChatGPT-3.5 and an additional 10,000 image-text pairs with captions generated
by ChatGPT-4V(ision). This dataset has significant potential for training
vision-language foundation models and evaluating large vision-language models
for remote sensing. The dataset will be made publicly available.
</p>
</div>
</dd>
<dt><a name="item199">[199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11333" title="Abstract">arXiv:2402.11333</a> [<a href="/pdf/2402.11333" title="Download PDF">pdf</a>, <a href="/format/2402.11333" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Cross-Cultural Analysis of Social Norms in Bollywood and Hollywood  Movies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rai%2C+S">Sunny Rai</a>, 
<a href="/search/cs?searchtype=author&query=Zaveri%2C+K+Z">Khushang Zilesh Zaveri</a>, 
<a href="/search/cs?searchtype=author&query=Havaldar%2C+S">Shreya Havaldar</a>, 
<a href="/search/cs?searchtype=author&query=Nema%2C+S">Soumna Nema</a>, 
<a href="/search/cs?searchtype=author&query=Ungar%2C+L">Lyle Ungar</a>, 
<a href="/search/cs?searchtype=author&query=Guntuku%2C+S+C">Sharath Chandra Guntuku</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Understanding how social norms vary across cultures can help us build
culturally aligned NLP systems. We propose a culture agnostic approach to norm
discovery, using moral emotions, shame and pride, to identify examples of
normative expectations and extract corresponding social norms. We present the
first cross cultural self-conscious emotions dataset, obtained from 5.4K
Bollywood and Hollywood movies, along with over 10K extracted social norms. We
validate our dataset using native speakers and demonstrate how our dataset
reveals variations in social norms that align with the cultural dichotomy
observed in these nations e.g., Bollywood movies emphasize shame due to
deviation from social roles, and express pride in family honor, while Hollywood
shames poverty and incompetence, and takes pride in ethical behavior. Notably,
females are shamed more across both cultures and both cultures shame women for
violating similar normative expectations.
</p>
</div>
</dd>
<dt><a name="item200">[200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11337" title="Abstract">arXiv:2402.11337</a> [<a href="/pdf/2402.11337" title="Download PDF">pdf</a>, <a href="/format/2402.11337" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning by Reconstruction Produces Uninformative Features For  Perception
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balestriero%2C+R">Randall Balestriero</a>, 
<a href="/search/cs?searchtype=author&query=LeCun%2C+Y">Yann LeCun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Input space reconstruction is an attractive representation learning paradigm.
Despite interpretability of the reconstruction and generation, we identify a
misalignment between learning by reconstruction, and learning for perception.
We show that the former allocates a model's capacity towards a subspace of the
data explaining the observed variance--a subspace with uninformative features
for the latter. For example, the supervised TinyImagenet task with images
projected onto the top subspace explaining 90\% of the pixel variance can be
solved with 45\% test accuracy. Using the bottom subspace instead, accounting
for only 20\% of the pixel variance, reaches 55\% test accuracy. The features
for perception being learned last explains the need for long training time,
e.g., with Masked Autoencoders. Learning by denoising is a popular strategy to
alleviate that misalignment. We prove that while some noise strategies such as
masking are indeed beneficial, others such as additive Gaussian noise are not.
Yet, even in the case of masking, we find that the benefits vary as a function
of the mask's shape, ratio, and the considered dataset. While tuning the noise
strategy without knowledge of the perception task seems challenging, we provide
first clues on how to detect if a noise strategy is never beneficial regardless
of the perception task.
</p>
</div>
</dd>
<dt><a name="item201">[201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11338" title="Abstract">arXiv:2402.11338</a> [<a href="/pdf/2402.11338" title="Download PDF">pdf</a>, <a href="/format/2402.11338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fair Classification with Partial Feedback: An Exploration-Based  Data-Collection Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Keswani%2C+V">Vijay Keswani</a>, 
<a href="/search/cs?searchtype=author&query=Mehrotra%2C+A">Anay Mehrotra</a>, 
<a href="/search/cs?searchtype=author&query=Celis%2C+L+E">L. Elisa Celis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (stat.ML)

</div>
<p class="mathjax">In many predictive contexts (e.g., credit lending), true outcomes are only
observed for samples that were positively classified in the past. These past
observations, in turn, form training datasets for classifiers that make future
predictions. However, such training datasets lack information about the
outcomes of samples that were (incorrectly) negatively classified in the past
and can lead to erroneous classifiers. We present an approach that trains a
classifier using available data and comes with a family of exploration
strategies to collect outcome data about subpopulations that otherwise would
have been ignored. For any exploration strategy, the approach comes with
guarantees that (1) all sub-populations are explored, (2) the fraction of false
positives is bounded, and (3) the trained classifier converges to a "desired"
classifier. The right exploration strategy is context-dependent; it can be
chosen to improve learning guarantees and encode context-specific group
fairness properties. Evaluation on real-world datasets shows that this approach
consistently boosts the quality of collected outcome data and improves the
fraction of true positives for all groups, with only a small reduction in
predictive utility.
</p>
</div>
</dd>
<dt><a name="item202">[202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11339" title="Abstract">arXiv:2402.11339</a> [<a href="/pdf/2402.11339" title="Download PDF">pdf</a>, <a href="/format/2402.11339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Expressive Higher-Order Link Prediction through Hypergraph Symmetry  Breaking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Simon Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+C">Cheng Xin</a>, 
<a href="/search/cs?searchtype=author&query=Dey%2C+T+K">Tamal K. Dey</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">A hypergraph consists of a set of nodes along with a collection of subsets of
the nodes called hyperedges. Higher-order link prediction is the task of
predicting the existence of a missing hyperedge in a hypergraph. A hyperedge
representation learned for higher order link prediction is fully expressive
when it does not lose distinguishing power up to an isomorphism. Many existing
hypergraph representation learners, are bounded in expressive power by the
Generalized Weisfeiler Lehman-1 (GWL-1) algorithm, a generalization of the
Weisfeiler Lehman-1 algorithm. However, GWL-1 has limited expressive power. In
fact, induced subhypergraphs with identical GWL-1 valued nodes are
indistinguishable. Furthermore, message passing on hypergraphs can already be
computationally expensive, especially on GPU memory. To address these
limitations, we devise a preprocessing algorithm that can identify certain
regular subhypergraphs exhibiting symmetry. Our preprocessing algorithm runs
once with complexity the size of the input hypergraph. During training, we
randomly replace subhypergraphs identified by the algorithm with covering
hyperedges to break symmetry. We show that our method improves the expressivity
of GWL-1. Our extensive experiments also demonstrate the effectiveness of our
approach for higher-order link prediction on both graph and hypergraph datasets
with negligible change in computation.
</p>
</div>
</dd>
<dt><a name="item203">[203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11342" title="Abstract">arXiv:2402.11342</a> [<a href="/pdf/2402.11342" title="Download PDF">pdf</a>, <a href="/ps/2402.11342" title="Download PostScript">ps</a>, <a href="/format/2402.11342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ransomware detection using stacked autoencoder for feature selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nkongolo%2C+M">Mike Nkongolo</a>, 
<a href="/search/cs?searchtype=author&query=Tokmak%2C+M">Mahmut Tokmak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The aim of this study is to propose and evaluate an advanced ransomware
detection and classification method that combines a Stacked Autoencoder (SAE)
for precise feature selection with a Long Short Term Memory (LSTM) classifier
to enhance ransomware stratification accuracy. The proposed approach involves
thorough pre processing of the UGRansome dataset and training an unsupervised
SAE for optimal feature selection or fine tuning via supervised learning to
elevate the LSTM model's classification capabilities. The study meticulously
analyzes the autoencoder's learned weights and activations to identify
essential features for distinguishing ransomware families from other malware
and creates a streamlined feature set for precise classification. Extensive
experiments, including up to 400 epochs and varying learning rates, are
conducted to optimize the model's performance. The results demonstrate the
outstanding performance of the SAE-LSTM model across all ransomware families,
boasting high precision, recall, and F1 score values that underscore its robust
classification capabilities. Furthermore, balanced average scores affirm the
proposed model's ability to generalize effectively across various malware
types. The proposed model achieves an exceptional 99% accuracy in ransomware
classification, surpassing the Extreme Gradient Boosting (XGBoost) algorithm
primarily due to its effective SAE feature selection mechanism. The model also
demonstrates outstanding performance in identifying signature attacks,
achieving a 98% accuracy rate.
</p>
</div>
</dd>
<dt><a name="item204">[204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11346" title="Abstract">arXiv:2402.11346</a> [<a href="/pdf/2402.11346" title="Download PDF">pdf</a>, <a href="/format/2402.11346" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Odor Perceptual Shift Keying (OPSK) for Odor-Based Molecular  Communication
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bilgen%2C+F+E">Fatih E. Bilgen</a>, 
<a href="/search/cs?searchtype=author&query=Kilic%2C+A+B">Ahmet B. Kilic</a>, 
<a href="/search/cs?searchtype=author&query=Akan%2C+O+B">Ozgur B. Akan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Molecular communication (MC) has promising potential and a wide range of
applications. However, odor-based communication which is common in nature, has
not been sufficiently examined within the context of MC, yet. In this paper, we
introduce a novel approach for implementing odor-based MC systems. We propose a
new modulation scheme called Odor Perceptual Shift Keying (OPSK), which encodes
information by shifting the perceptual values of odor molecules in
pleasantness, intensity and edibility dimensions. We construct a system which
transmits OPSK modulated signals between a transmitter and receiver. We conduct
analyses on the system parameters to simulate performance metrics such as
symbol error rate (SER) and symbol rate (SR). Our analyses indicate that OPSK
has a potential for realizing odor-based MC systems. We find that under certain
conditions, reliable odor-based MC systems can be implemented using OPSK across
a variety of distance ranges from millimeters up to kilometers. Additionally,
we introduce adaptive symbol transmission to our system for input symbol
sequences featuring symbols that occur with unequal probabilities. We further
demonstrate that the proposed algorithm at the transmitter side can achieve
extended operation times.
</p>
</div>
</dd>
<dt><a name="item205">[205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11347" title="Abstract">arXiv:2402.11347</a> [<a href="/pdf/2402.11347" title="Download PDF">pdf</a>, <a href="/format/2402.11347" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PhaseEvo: Towards Unified In-Context Prompt Optimization for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cui%2C+W">Wendi Cui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaxin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhuohang Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Lopez%2C+D">Damien Lopez</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+K">Kamalika Das</a>, 
<a href="/search/cs?searchtype=author&query=Malin%2C+B">Bradley Malin</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sricharan Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 50 pages, 9 figures, 26 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Crafting an ideal prompt for Large Language Models (LLMs) is a challenging
task that demands significant resources and expert human input. Existing work
treats the optimization of prompt instruction and in-context learning examples
as distinct problems, leading to sub-optimal prompt performance. This research
addresses this limitation by establishing a unified in-context prompt
optimization framework, which aims to achieve joint optimization of the prompt
instruction and examples. However, formulating such optimization in the
discrete and high-dimensional natural language space introduces challenges in
terms of convergence and computational efficiency. To overcome these issues, we
present PhaseEvo, an efficient automatic prompt optimization framework that
combines the generative capability of LLMs with the global search proficiency
of evolution algorithms. Our framework features a multi-phase design
incorporating innovative LLM-based mutation operators to enhance search
efficiency and accelerate convergence. We conduct an extensive evaluation of
our approach across 35 benchmark tasks. The results demonstrate that PhaseEvo
significantly outperforms the state-of-the-art baseline methods by a large
margin whilst maintaining good efficiency.
</p>
</div>
</dd>
<dt><a name="item206">[206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11349" title="Abstract">arXiv:2402.11349</a> [<a href="/pdf/2402.11349" title="Download PDF">pdf</a>, <a href="/format/2402.11349" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tasks That Language Models Don&#x27;t Learn
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+B+W">Bruce W. Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+J">JaeHyuk Lim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We argue that there are certain properties of language that our current large
language models (LLMs) don't learn. We present an empirical investigation of
visual-auditory properties of language through a series of tasks, termed
H-TEST. This benchmark highlights a fundamental gap between human linguistic
comprehension, which naturally integrates sensory experiences, and the
sensory-deprived processing capabilities of LLMs. In support of our hypothesis,
1. deliberate reasoning (Chain-of-Thought), 2. few-shot examples, or 3.
stronger LLM from the same model family (LLaMA 2 13B -&gt; LLaMA 2 70B) do not
trivially bring improvements in H-TEST performance. Therefore, we make a
particular connection to the philosophical case of Mary, who learns about the
world in a sensory-deprived environment (Jackson, 1986). Our experiments show
that some of the strongest proprietary LLMs stay near random chance baseline
accuracy of 50%, highlighting the limitations of knowledge acquired in the
absence of sensory experience.
</p>
</div>
</dd>
<dt><a name="item207">[207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11351" title="Abstract">arXiv:2402.11351</a> [<a href="/pdf/2402.11351" title="Download PDF">pdf</a>, <a href="/format/2402.11351" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling the amplification of epidemic spread by misinformed populations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=DeVerna%2C+M+R">Matthew R. DeVerna</a>, 
<a href="/search/cs?searchtype=author&query=Pierri%2C+F">Francesco Pierri</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+Y">Yong-Yeol Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Fortunato%2C+S">Santo Fortunato</a>, 
<a href="/search/cs?searchtype=author&query=Flammini%2C+A">Alessandro Flammini</a>, 
<a href="/search/cs?searchtype=author&query=Menczer%2C+F">Filippo Menczer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">Understanding how misinformation affects the spread of disease is crucial for
public health, especially given recent research indicating that misinformation
can increase vaccine hesitancy and discourage vaccine uptake. However, it is
difficult to investigate the interaction between misinformation and epidemic
outcomes due to the dearth of data-informed holistic epidemic models. Here, we
propose an epidemic model that incorporates a large, mobility-informed physical
contact network as well as the distribution of misinformed individuals across
counties derived from social media data. Our model allows us to simulate and
estimate various scenarios to understand the impact of misinformation on
epidemic spreading. Using this model, we estimate that misinformation could
have led to 47 million additional COVID-19 infections in the U.S. in a
worst-case scenario.
</p>
</div>
</dd>
<dt><a name="item208">[208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11352" title="Abstract">arXiv:2402.11352</a> [<a href="/pdf/2402.11352" title="Download PDF">pdf</a>, <a href="/format/2402.11352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unified Capacity Results for Free-Space Optical Communication Systems  Over Gamma-Gamma Atmospheric Turbulence Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Verma%2C+H">Himani Verma</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+K">Kamal Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In terrestrial free-space optical (FSO) communication systems, adaptive power
control at the optical laser transmitters is crucial not only to prolong the
life span of the laser sources, but more importantly to maintain robust and
spectrally efficient communication through atmospheric turbulence. However, a
comprehensive study of dynamic power adaptation in existing FSO systems is
lacking in the literature. In this paper, we investigate FSO communication
systems capable of adaptive laser power control with heterodyne detection (HD)
and direct detection (DD) based receivers operating under shot-noise-limited
conditions. Under these FSO systems considerations, we derive unified exact and
asymptotic formulas for the capacities of Gamma-Gamma atmospheric turbulence
channels with and without pointing errors; these novel closed-form capacity
expressions are much simpler and provide new insights into the impact of
varying turbulence conditions and pointing errors. Finally, the numerical
results highlight the intricate relations of atmospheric fading, pointing
error, and large-scale channel parameters in a typical terrestrial FSO channel
setting, followed up by an accurate assessment of the key parameters
determining the capacity performances of the aforementioned FSO systems
revealing several interesting characteristics.
</p>
</div>
</dd>
<dt><a name="item209">[209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11353" title="Abstract">arXiv:2402.11353</a> [<a href="/pdf/2402.11353" title="Download PDF">pdf</a>, <a href="/format/2402.11353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Impact of Long-Term Memory on Self-Disclosure with  Large Language Model-Driven Chatbots for Public Health Intervention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jo%2C+E">Eunkyung Jo</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+Y">Yuin Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+S">SoHyun Park</a>, 
<a href="/search/cs?searchtype=author&query=Epstein%2C+D+A">Daniel A. Epstein</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Young-Ho Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ACM CHI 2024 as a full paper
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Proceedings of the CHI Conference on Human Factors in Computing
  Systems (CHI '24), May 11-16, 2024, Honolulu, HI, USA. ACM, New York, NY, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Recent large language models (LLMs) offer the potential to support public
health monitoring by facilitating health disclosure through open-ended
conversations but rarely preserve the knowledge gained about individuals across
repeated interactions. Augmenting LLMs with long-term memory (LTM) presents an
opportunity to improve engagement and self-disclosure, but we lack an
understanding of how LTM impacts people's interaction with LLM-driven chatbots
in public health interventions. We examine the case of CareCall -- an
LLM-driven voice chatbot with LTM -- through the analysis of 1,252 call logs
and interviews with nine users. We found that LTM enhanced health disclosure
and fostered positive perceptions of the chatbot by offering familiarity.
However, we also observed challenges in promoting self-disclosure through LTM,
particularly around addressing chronic health conditions and privacy concerns.
We discuss considerations for LTM integration in LLM-driven chatbots for public
health monitoring, including carefully deciding what topics need to be
remembered in light of public health goals.
</p>
</div>
</dd>
<dt><a name="item210">[210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11354" title="Abstract">arXiv:2402.11354</a> [<a href="/pdf/2402.11354" title="Download PDF">pdf</a>, <a href="/format/2402.11354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Probabilistic Routing for Graph-Based Approximate Nearest Neighbor  Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+K">Kejing Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chuan Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Ishikawa%2C+Y">Yoshiharu Ishikawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Source code will be released at GitHub soon
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Databases (cs.DB); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Approximate nearest neighbor search (ANNS) in high-dimensional spaces is a
pivotal challenge in the field of machine learning. In recent years,
graph-based methods have emerged as the superior approach to ANNS, establishing
a new state of the art. Although various optimizations for graph-based ANNS
have been introduced, they predominantly rely on heuristic methods that lack
formal theoretical backing. This paper aims to enhance routing within
graph-based ANNS by introducing a method that offers a probabilistic guarantee
when exploring a node's neighbors in the graph. We formulate the problem as
probabilistic routing and develop two baseline strategies by incorporating
locality-sensitive techniques. Subsequently, we introduce PEOs, a novel
approach that efficiently identifies which neighbors in the graph should be
considered for exact distance computation, thus significantly improving
efficiency in practice. Our experiments demonstrate that equipping PEOs can
increase throughput on a commonly utilized graph index (HNSW) by a factor of
1.6 to 2.5, and its efficiency consistently outperforms the leading-edge
routing technique by 1.1 to 1.4 times.
</p>
</div>
</dd>
<dt><a name="item211">[211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11355" title="Abstract">arXiv:2402.11355</a> [<a href="/pdf/2402.11355" title="Download PDF">pdf</a>, <a href="/format/2402.11355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Changed? Converting Representational Interventions to Natural  Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Avitan%2C+M">Matan Avitan</a>, 
<a href="/search/cs?searchtype=author&query=Cotterell%2C+R">Ryan Cotterell</a>, 
<a href="/search/cs?searchtype=author&query=Goldberg%2C+Y">Yoav Goldberg</a>, 
<a href="/search/cs?searchtype=author&query=Ravfogel%2C+S">Shauli Ravfogel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">Interventions targeting the representation space of language models (LMs)
have emerged as effective means to influence model behavior. These methods are
employed, for example, to eliminate or alter the encoding of demographic
information such as gender within the model's representations, creating a
counterfactual representation. However, since the intervention operates within
the representation space, understanding precisely which features it modifies
poses a challenge. We show that representation-space counterfactuals can be
converted into natural language counterfactuals. We demonstrate that this
approach enables us to analyze the linguistic alterations corresponding to a
given representation-space intervention and to interpret the features utilized
for encoding a specific concept. Moreover, the resulting counterfactuals can be
used to mitigate bias in classification.
</p>
</div>
</dd>
<dt><a name="item212">[212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11359" title="Abstract">arXiv:2402.11359</a> [<a href="/pdf/2402.11359" title="Download PDF">pdf</a>, <a href="/format/2402.11359" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Language Model Agents without Modifying Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaokun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jieyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiale Liu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linxin Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Krishna%2C+R">Ranjay Krishna</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingyun Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Researchers and practitioners have recently reframed powerful Large Language
Models (LLMs) as agents, enabling them to automate complex tasks largely via
the use of specialized functions. To facilitate the development of LLM agents,
we present a novel paradigm of training LLM agents without modifying the LLM
weights, which is particularly useful when the LLMs are difficult or
inaccessible for modifications. Inspired by how humans continuously forge tools
to adapt to real-world tasks, rather than change our biological structure to
fit a static set of tools, we propose to progressively forge agent's functions
to better solve the downstream tasks instead of modifying the LLM weights. By
treating the functions as learnable `agent parameters' and leveraging the
fundamental idea of model training in artificial intelligence, we develop
AgentOptimizer that employs the LLM to update agents' functions and devise an
agent training algorithm with two strategies, roll-back, and early-stop, to
streamline the training process. With extensive experiments, we showcase that
the agent training paradigm could significantly improve the performance of
representative LLM agents in various downstream tasks. We also study the
behavior of the agent training regarding aspects like the learning curve and
domain transferability.
</p>
</div>
</dd>
<dt><a name="item213">[213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11361" title="Abstract">arXiv:2402.11361</a> [<a href="/pdf/2402.11361" title="Download PDF">pdf</a>, <a href="/format/2402.11361" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The matrix-free macro-element hybridized Discontinuous Galerkin method  for steady and unsteady compressible flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Badrkhani%2C+V">Vahid Badrkhani</a>, 
<a href="/search/cs?searchtype=author&query=Eikelder%2C+M+F+P+t">Marco F.P. ten Eikelder</a>, 
<a href="/search/cs?searchtype=author&query=Hiemstra%2C+R+R">Rene R. Hiemstra</a>, 
<a href="/search/cs?searchtype=author&query=Schillinger%2C+D">Dominik Schillinger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Engineering, Finance, and Science (cs.CE)</span>

</div>
<p class="mathjax">The macro-element variant of the hybridized discontinuous Galerkin (HDG)
method combines advantages of continuous and discontinuous finite element
discretization. In this paper, we investigate the performance of the
macro-element HDG method for the analysis of compressible flow problems at
moderate Reynolds numbers. To efficiently handle the corresponding large
systems of equations, we explore several strategies at the solver level. On the
one hand, we devise a second-layer static condensation approach that reduces
the size of the local system matrix in each macro-element and hence the
factorization time of the local solver. On the other hand, we employ a
multi-level preconditioner based on the FGMRES solver for the global system
that integrates well within a matrix-free implementation. In addition, we
integrate a standard diagonally implicit Runge-Kutta scheme for time
integration. We test the matrix-free macro-element HDG method for compressible
flow benchmarks, including Couette flow, flow past a sphere, and the
Taylor-Green vortex. Our results show that unlike standard HDG, the
macro-element HDG method can operate efficiently for moderate polynomial
degrees, as the local computational load can be flexibly increased via mesh
refinement within a macro-element. Our results also show that due to the
balance of local and global operations, the reduction in degrees of freedom,
and the reduction of the global problem size and the number of iterations for
its solution, the macro-element HDG method can be a competitive option for the
analysis of compressible flow problems.
</p>
</div>
</dd>
<dt><a name="item214">[214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11362" title="Abstract">arXiv:2402.11362</a> [<a href="/pdf/2402.11362" title="Download PDF">pdf</a>, <a href="/ps/2402.11362" title="Download PostScript">ps</a>, <a href="/format/2402.11362" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploiting T-norms for Deep Learning in Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stoian%2C+M+C">Mihaela C&#x103;t&#x103;lina Stoian</a>, 
<a href="/search/cs?searchtype=author&query=Giunchiglia%2C+E">Eleonora Giunchiglia</a>, 
<a href="/search/cs?searchtype=author&query=Lukasiewicz%2C+T">Thomas Lukasiewicz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published in Proceedings of the 17th International Workshop on Neural-Symbolic Learning and Reasoning, 2023 (NeSy 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Logic in Computer Science (cs.LO)

</div>
<p class="mathjax">Deep learning has been at the core of the autonomous driving field
development, due to the neural networks' success in finding patterns in raw
data and turning them into accurate predictions. Moreover, recent
neuro-symbolic works have shown that incorporating the available background
knowledge about the problem at hand in the loss function via t-norms can
further improve the deep learning models' performance. However, t-norm-based
losses may have very high memory requirements and, thus, they may be impossible
to apply in complex application domains like autonomous driving. In this paper,
we show how it is possible to define memory-efficient t-norm-based losses,
allowing for exploiting t-norms for the task of event detection in autonomous
driving. We conduct an extensive experimental analysis on the ROAD-R dataset
and show (i) that our proposal can be implemented and run on GPUs with less
than 25 GiB of available memory, while standard t-norm-based losses are
estimated to require more than 100 GiB, far exceeding the amount of memory
normally available, (ii) that t-norm-based losses improve performance,
especially when limited labelled data are available, and (iii) that
t-norm-based losses can further improve performance when exploited on both
labelled and unlabelled data.
</p>
</div>
</dd>
<dt><a name="item215">[215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11364" title="Abstract">arXiv:2402.11364</a> [<a href="/pdf/2402.11364" title="Download PDF">pdf</a>, <a href="/format/2402.11364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ironies of Generative AI: Understanding and mitigating productivity loss  in human-AI interactions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Simkute%2C+A">Auste Simkute</a>, 
<a href="/search/cs?searchtype=author&query=Tankelevitch%2C+L">Lev Tankelevitch</a>, 
<a href="/search/cs?searchtype=author&query=Kewenig%2C+V">Viktor Kewenig</a>, 
<a href="/search/cs?searchtype=author&query=Scott%2C+A+E">Ava Elizabeth Scott</a>, 
<a href="/search/cs?searchtype=author&query=Sellen%2C+A">Abigail Sellen</a>, 
<a href="/search/cs?searchtype=author&query=Rintel%2C+S">Sean Rintel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Generative AI (GenAI) systems offer opportunities to increase user
productivity in many tasks, such as programming and writing. However, while
they boost productivity in some studies, many others show that users are
working ineffectively with GenAI systems and losing productivity. Despite the
apparent novelty of these usability challenges, these 'ironies of automation'
have been observed for over three decades in Human Factors research on the
introduction of automation in domains such as aviation, automated driving, and
intelligence. We draw on this extensive research alongside recent GenAI user
studies to outline four key reasons for productivity loss with GenAI systems: a
shift in users' roles from production to evaluation, unhelpful restructuring of
workflows, interruptions, and a tendency for automation to make easy tasks
easier and hard tasks harder. We then suggest how Human Factors research can
also inform GenAI system design to mitigate productivity loss by using
approaches such as continuous feedback, system personalization, ecological
interface design, task stabilization, and clear task allocation. Thus, we
ground developments in GenAI system usability in decades of Human Factors
research, ensuring that the design of human-AI interactions in this rapidly
moving field learns from history instead of repeating it.
</p>
</div>
</dd>
<dt><a name="item216">[216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11365" title="Abstract">arXiv:2402.11365</a> [<a href="/pdf/2402.11365" title="Download PDF">pdf</a>, <a href="/format/2402.11365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data-Driven Stochastic AC-OPF using Gaussian Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mitrovic%2C+M">Mile Mitrovic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 112 pages, 29 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">The thesis focuses on developing a data-driven algorithm, based on machine
learning, to solve the stochastic alternating current (AC) chance-constrained
(CC) Optimal Power Flow (OPF) problem. Although the AC CC-OPF problem has been
successful in academic circles, it is highly nonlinear and computationally
demanding, which limits its practical impact. The proposed approach aims to
address this limitation and demonstrate its empirical efficiency through
applications to multiple IEEE test cases. To solve the non-convex and
computationally challenging CC AC-OPF problem, the proposed approach relies on
a machine learning Gaussian process regression (GPR) model. The full Gaussian
process (GP) approach is capable of learning a simple yet non-convex
data-driven approximation to the AC power flow equations that can incorporate
uncertain inputs. The proposed approach uses various approximations for
GP-uncertainty propagation. The full GP CC-OPF approach exhibits highly
competitive and promising results, outperforming the state-of-the-art
sample-based chance constraint approaches. To further improve the robustness
and complexity/accuracy trade-off of the full GP CC-OPF, a fast data-driven
setup is proposed. This setup relies on the sparse and hybrid Gaussian
processes (GP) framework to model the power flow equations with input
uncertainty.
</p>
</div>
</dd>
<dt><a name="item217">[217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11367" title="Abstract">arXiv:2402.11367</a> [<a href="/pdf/2402.11367" title="Download PDF">pdf</a>, <a href="/format/2402.11367" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi Task Inverse Reinforcement Learning for Common Sense Reward
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Glazer%2C+N">Neta Glazer</a>, 
<a href="/search/cs?searchtype=author&query=Navon%2C+A">Aviv Navon</a>, 
<a href="/search/cs?searchtype=author&query=Shamsian%2C+A">Aviv Shamsian</a>, 
<a href="/search/cs?searchtype=author&query=Fetaya%2C+E">Ethan Fetaya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">One of the challenges in applying reinforcement learning in a complex
real-world environment lies in providing the agent with a sufficiently detailed
reward function. Any misalignment between the reward and the desired behavior
can result in unwanted outcomes. This may lead to issues like "reward hacking"
where the agent maximizes rewards by unintended behavior. In this work, we
propose to disentangle the reward into two distinct parts. A simple
task-specific reward, outlining the particulars of the task at hand, and an
unknown common-sense reward, indicating the expected behavior of the agent
within the environment. We then explore how this common-sense reward can be
learned from expert demonstrations. We first show that inverse reinforcement
learning, even when it succeeds in training an agent, does not learn a useful
reward function. That is, training a new agent with the learned reward does not
impair the desired behaviors. We then demonstrate that this problem can be
solved by training simultaneously on multiple tasks. That is, multi-task
inverse reinforcement learning can be applied to learn a useful reward
function.
</p>
</div>
</dd>
<dt><a name="item218">[218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11370" title="Abstract">arXiv:2402.11370</a> [<a href="/pdf/2402.11370" title="Download PDF">pdf</a>, <a href="/format/2402.11370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stable Menus of Public Goods: A Matching Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fish%2C+S">Sara Fish</a>, 
<a href="/search/cs?searchtype=author&query=Gonczarowski%2C+Y+A">Yannai A. Gonczarowski</a>, 
<a href="/search/cs?searchtype=author&query=Hart%2C+S">Sergiu Hart</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Theoretical Economics (econ.TH); Combinatorics (math.CO)

</div>
<p class="mathjax">We study a matching problem between agents and public goods, in settings
without monetary transfers. Since goods are public, they have no capacity
constraints. There is no exogenously defined budget of goods to be provided.
Rather, each provided good must justify its cost, leading to strong
complementarities in the "preferences" of goods. Furthermore, goods that are in
high demand given other already-provided goods must also be provided. The
question of the existence of a stable solution (a menu of public goods to be
provided) exhibits a rich combinatorial structure. We uncover sufficient
conditions and necessary conditions for guaranteeing the existence of a stable
solution, and derive both positive and negative results for strategyproof
stable matching.
</p>
</div>
</dd>
<dt><a name="item219">[219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11382" title="Abstract">arXiv:2402.11382</a> [<a href="/pdf/2402.11382" title="Download PDF">pdf</a>, <a href="/format/2402.11382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Secure, Robust, and Energy-Efficient Authenticated Data Sharing in  UAV-Assisted 6G Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ejiyeh%2C+A+M">Atefeh Mohseni Ejiyeh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">This paper confronts the pressing challenges of sixth-generation (6G)
wireless communication networks by harnessing the unique capabilities of
Unmanned Aerial Vehicles (UAVs). With the ambitious promises of 6G, including
ultra-reliable 1 Tbps data delivery and ultra-low latency, the demand for
innovative solutions becomes imperative. Traditional terrestrial base stations,
though effective, exhibit limitations in scenarios requiring ubiquitous
connectivity, prompting the integration of UAVs. In response to these
challenges, we introduce a comprehensive solution. This involves UAVs
collaboratively downloading desired content from service providers, and
subsequently establishing secure connections with users for efficient content
exchange. Accordingly, we introduce two new protocols: a collaborative group
data downloading scheme among UAVs called SeGDS, and SeDDS for secure direct
data sharing through out-of-band autonomous Device-to-Device (D2D)
communication. Leveraging certificateless signcryption and certificateless
multi-receiver encryption, these protocols offer lightweight, certificate-free
solutions with features such as user revocation, non-repudiation, and mutual
authentication. Prioritizing high availability, the proposed protocols
effectively detect Denial of Service (DoS) and free riding attacks. A thorough
evaluation underscores the superiority of the proposed protocols in both
security and efficiency over existing models; SeDDS reduces overall computation
by 3x, imposing a lighter communication load on UAVs, while SeGDS meets swarm
UAV security requirements, reducing communication costs by 4x with low
computation cost.
</p>
</div>
</dd>
<dt><a name="item220">[220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11384" title="Abstract">arXiv:2402.11384</a> [<a href="/pdf/2402.11384" title="Download PDF">pdf</a>, <a href="/format/2402.11384" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement learning to maximise wind turbine energy generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soler%2C+D">Daniel Soler</a>, 
<a href="/search/cs?searchtype=author&query=Mari%C3%B1o%2C+O">Oscar Mari&#xf1;o</a>, 
<a href="/search/cs?searchtype=author&query=Huergo%2C+D">David Huergo</a>, 
<a href="/search/cs?searchtype=author&query=de+Frutos%2C+M">Mart&#xed;n de Frutos</a>, 
<a href="/search/cs?searchtype=author&query=Ferrer%2C+E">Esteban Ferrer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Mathematical Physics (math-ph); Optimization and Control (math.OC)

</div>
<p class="mathjax">We propose a reinforcement learning strategy to control wind turbine energy
generation by actively changing the rotor speed, the rotor yaw angle and the
blade pitch angle. A double deep Q-learning with a prioritized experience
replay agent is coupled with a blade element momentum model and is trained to
allow control for changing winds. The agent is trained to decide the best
control (speed, yaw, pitch) for simple steady winds and is subsequently
challenged with real dynamic turbulent winds, showing good performance. The
double deep Q- learning is compared with a classic value iteration
reinforcement learning control and both strategies outperform a classic PID
control in all environments. Furthermore, the reinforcement learning approach
is well suited to changing environments including turbulent/gusty winds,
showing great adaptability. Finally, we compare all control strategies with
real winds and compute the annual energy production. In this case, the double
deep Q-learning algorithm also outperforms classic methodologies.
</p>
</div>
</dd>
<dt><a name="item221">[221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11390" title="Abstract">arXiv:2402.11390</a> [<a href="/pdf/2402.11390" title="Download PDF">pdf</a>, <a href="/format/2402.11390" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wireless Distributed Matrix-Vector Multiplication using Over-the-Air  Computation and Analog Coding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jinho Choi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Systems and Control (eess.SY)

</div>
<p class="mathjax">In this paper, we propose an over-the-air (OTA)-based approach for
distributed matrix-vector multiplications in the context of distributed machine
learning (DML). Thanks to OTA computation, the column-wise partitioning of a
large matrix enables efficient workload distribution among workers (i.e., local
computing nodes) based on their computing capabilities. In addition, without
requiring additional bandwidth, it allows the system to remain scalable even as
the number of workers increases to mitigate the impact of slow workers, known
as stragglers. However, despite the improvements, there are still instances
where some workers experience deep fading and become stragglers, preventing
them from transmitting their results. By analyzing the mean squared error
(MSE), we demonstrate that incorporating more workers in the OTA-based approach
leads to MSE reduction without the need for additional radio resources.
Furthermore, we introduce an analog coding scheme to further enhance the
performance and compare it with conventional coded multiplication (CM) schemes.
Through simulations, it is shown that the OTA-based approach achieves
comparable performance to CM schemes while potentially requiring fewer radio
resources.
</p>
</div>
</dd>
<dt><a name="item222">[222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11397" title="Abstract">arXiv:2402.11397</a> [<a href="/pdf/2402.11397" title="Download PDF">pdf</a>, <a href="/ps/2402.11397" title="Download PostScript">ps</a>, <a href="/format/2402.11397" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Random Projection Neural Networks of Best Approximation: Convergence  theory and practical applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fabiani%2C+G">Gianluca Fabiani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">We investigate the concept of Best Approximation for Feedforward Neural
Networks (FNN) and explore their convergence properties through the lens of
Random Projection (RPNNs). RPNNs have predetermined and fixed, once and for
all, internal weights and biases, offering computational efficiency. We
demonstrate that there exists a choice of external weights, for any family of
such RPNNs, with non-polynomial infinitely differentiable activation functions,
that exhibit an exponential convergence rate when approximating any infinitely
differentiable function. For illustration purposes, we test the proposed
RPNN-based function approximation, with parsimoniously chosen basis functions,
across five benchmark function approximation problems. Results show that RPNNs
achieve comparable performance to established methods such as Legendre
Polynomials, highlighting their potential for efficient and accurate function
approximation.
</p>
</div>
</dd>
<dt><a name="item223">[223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11398" title="Abstract">arXiv:2402.11398</a> [<a href="/pdf/2402.11398" title="Download PDF">pdf</a>, <a href="/format/2402.11398" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics  for Domain Specialized Text Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+S">Shaochen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zihao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Huaqin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+P">Peng Shu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+W">Wenxiong Liao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Sheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Sikora%2C+A">Andrea Sikora</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tianming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this study, we leverage LLM to enhance the semantic analysis and develop
similarity metrics for texts, addressing the limitations of traditional
unsupervised NLP metrics like ROUGE and BLEU. We develop a framework where LLMs
such as GPT-4 are employed for zero-shot text identification and label
generation for radiology reports, where the labels are then used as
measurements for text similarity. By testing the proposed framework on the
MIMIC data, we find that GPT-4 generated labels can significantly improve the
semantic similarity assessment, with scores more closely aligned with clinical
ground truth than traditional NLP metrics. Our work demonstrates the
possibility of conducting semantic analysis of the text data using
semi-quantitative reasoning results by the LLMs for highly specialized domains.
While the framework is implemented for radiology report similarity analysis,
its concept can be extended to other specialized domains as well.
</p>
</div>
</dd>
<dt><a name="item224">[224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11399" title="Abstract">arXiv:2402.11399</a> [<a href="/pdf/2402.11399" title="Download PDF">pdf</a>, <a href="/format/2402.11399" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> k-SemStamp: A Clustering-Based Semantic Watermark for Detection of  Machine-Generated Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hou%2C+A+B">Abe Bohan Hou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yichen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Khashabi%2C+D">Daniel Khashabi</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tianxing He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR); Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent watermarked generation algorithms inject detectable signatures during
language generation to facilitate post-hoc detection. While token-level
watermarks are vulnerable to paraphrase attacks, SemStamp (Hou et al., 2023)
applies watermark on the semantic representation of sentences and demonstrates
promising robustness. SemStamp employs locality-sensitive hashing (LSH) to
partition the semantic space with arbitrary hyperplanes, which results in a
suboptimal tradeoff between robustness and speed. We propose k-SemStamp, a
simple yet effective enhancement of SemStamp, utilizing k-means clustering as
an alternative of LSH to partition the embedding space with awareness of
inherent semantic structure. Experimental results indicate that k-SemStamp
saliently improves its robustness and sampling efficiency while preserving the
generation quality, advancing a more effective tool for machine-generated text
detection.
</p>
</div>
</dd>
<dt><a name="item225">[225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11400" title="Abstract">arXiv:2402.11400</a> [<a href="/pdf/2402.11400" title="Download PDF">pdf</a>, <a href="/ps/2402.11400" title="Download PostScript">ps</a>, <a href="/format/2402.11400" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Text to Map: A System Dynamics Bot for Constructing Causal Loop  Diagrams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hosseinichimeh%2C+N">Niyousha Hosseinichimeh</a>, 
<a href="/search/cs?searchtype=author&query=Majumdar%2C+A">Aritra Majumdar</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+R">Ross Williams</a>, 
<a href="/search/cs?searchtype=author&query=Ghaffarzadegan%2C+N">Navid Ghaffarzadegan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 4 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">We introduce and test the System Dynamics Bot, a computer program leveraging
a large language model to automate the creation of causal loop diagrams from
textual data. To evaluate its performance, we ensembled two distinct databases.
The first dataset includes 20 causal loop diagrams and associated texts sourced
from the system dynamics literature. The second dataset comprises responses
from 30 participants to a vignette, along with causal loop diagrams coded by
three system dynamics modelers. The bot uses textual data and successfully
identifies approximately sixty percent of the links between variables and
feedback loops in both datasets. This paper outlines our approach, provides
examples, and presents evaluation results. We discuss encountered challenges
and implemented solutions in developing the System Dynamics Bot. The bot can
facilitate extracting mental models from textual data and improve model
building processes. Moreover, the two datasets can serve as a testbed for
similar programs.
</p>
</div>
</dd>
<dt><a name="item226">[226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11401" title="Abstract">arXiv:2402.11401</a> [<a href="/pdf/2402.11401" title="Download PDF">pdf</a>, <a href="/format/2402.11401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraphKD: Exploring Knowledge Distillation Towards Document Object  Detection with Structured Graph Creation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+A">Ayan Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+S">Sanket Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Llad%C3%B3s%2C+J">Josep Llad&#xf3;s</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+U">Umapada Pal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Object detection in documents is a key step to automate the structural
elements identification process in a digital or scanned document through
understanding the hierarchical structure and relationships between different
elements. Large and complex models, while achieving high accuracy, can be
computationally expensive and memory-intensive, making them impractical for
deployment on resource constrained devices. Knowledge distillation allows us to
create small and more efficient models that retain much of the performance of
their larger counterparts. Here we present a graph-based knowledge distillation
framework to correctly identify and localize the document objects in a document
image. Here, we design a structured graph with nodes containing proposal-level
features and edges representing the relationship between the different proposal
regions. Also, to reduce text bias an adaptive node sampling strategy is
designed to prune the weight distribution and put more weightage on non-text
nodes. We encode the complete graph as a knowledge representation and transfer
it from the teacher to the student through the proposed distillation loss by
effectively capturing both local and global information concurrently. Extensive
experimentation on competitive benchmarks demonstrates that the proposed
framework outperforms the current state-of-the-art approaches. The code will be
available at: https://github.com/ayanban011/GraphKD.
</p>
</div>
</dd>
<dt><a name="item227">[227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11403" title="Abstract">arXiv:2402.11403</a> [<a href="/pdf/2402.11403" title="Download PDF">pdf</a>, <a href="/format/2402.11403" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Evaluation of Neural and Neuro-symbolic Approaches to  Real-time Multimodal Complex Event Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+L">Liying Han</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+M+B">Mani B. Srivastava</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Robots and autonomous systems require an understanding of complex events
(CEs) from sensor data to interact with their environments and humans
effectively. Traditional end-to-end neural architectures, despite processing
sensor data efficiently, struggle with long-duration events due to limited
context sizes and reasoning capabilities. Recent advances in neuro-symbolic
methods, which integrate neural and symbolic models leveraging human knowledge,
promise improved performance with less data. This study addresses the gap in
understanding these approaches' effectiveness in complex event detection (CED),
especially in temporal reasoning. We investigate neural and neuro-symbolic
architectures' performance in a multimodal CED task, analyzing IMU and acoustic
data streams to recognize CE patterns. Our methodology includes (i) end-to-end
neural architectures for direct CE detection from sensor embeddings, (ii)
two-stage concept-based neural models mapping sensor embeddings to atomic
events (AEs) before CE detection, and (iii) a neuro-symbolic approach using a
symbolic finite-state machine for CE detection from AEs. Empirically, the
neuro-symbolic architecture significantly surpasses purely neural models,
demonstrating superior performance in CE recognition, even with extensive
training data and ample temporal context for neural approaches.
</p>
</div>
</dd>
<dt><a name="item228">[228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11404" title="Abstract">arXiv:2402.11404</a> [<a href="/pdf/2402.11404" title="Download PDF">pdf</a>, <a href="/ps/2402.11404" title="Download PostScript">ps</a>, <a href="/format/2402.11404" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating the Stability of Deep Learning Latent Feature Spaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mabadeje%2C+A+O">Ademide O. Mabadeje</a>, 
<a href="/search/cs?searchtype=author&query=Pyrcz%2C+M+J">Michael J. Pyrcz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 30 pages, 11 figures, submitted to Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">High-dimensional datasets present substantial challenges in statistical
modeling across various disciplines, necessitating effective dimensionality
reduction methods. Deep learning approaches, notable for their capacity to
distill essential features from complex data, facilitate modeling,
visualization, and compression through reduced dimensionality latent feature
spaces, have wide applications from bioinformatics to earth sciences. This
study introduces a novel workflow to evaluate the stability of these latent
spaces, ensuring consistency and reliability in subsequent analyses. Stability,
defined as the invariance of latent spaces to minor data, training
realizations, and parameter perturbations, is crucial yet often overlooked.
<br />Our proposed methodology delineates three stability types, sample,
structural, and inferential, within latent spaces, and introduces a suite of
metrics for comprehensive evaluation. We implement this workflow across 500
autoencoder realizations and three datasets, encompassing both synthetic and
real-world scenarios to explain latent space dynamics. Employing k-means
clustering and the modified Jonker-Volgenant algorithm for class alignment,
alongside anisotropy metrics and convex hull analysis, we introduce adjusted
stress and Jaccard dissimilarity as novel stability indicators.
<br />Our findings highlight inherent instabilities in latent feature spaces and
demonstrate the workflow's efficacy in quantifying and interpreting these
instabilities. This work advances the understanding of latent feature spaces,
promoting improved model interpretability and quality control for more informed
decision-making for diverse analytical workflows that leverage deep learning.
</p>
</div>
</dd>
<dt><a name="item229">[229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11406" title="Abstract">arXiv:2402.11406</a> [<a href="/pdf/2402.11406" title="Download PDF">pdf</a>, <a href="/format/2402.11406" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Don&#x27;t Go To Extremes: Revealing the Excessive Sensitivity and  Calibration Limitations of LLMs in Implicit Hate Speech Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+J">Jianfeng He</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+T">Taoran Ji</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chang-Tien Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The fairness and trustworthiness of Large Language Models (LLMs) are
receiving increasing attention. Implicit hate speech, which employs indirect
language to convey hateful intentions, occupies a significant portion of
practice. However, the extent to which LLMs effectively address this issue
remains insufficiently examined. This paper delves into the capability of LLMs
to detect implicit hate speech (Classification Task) and express confidence in
their responses (Calibration Task). Our evaluation meticulously considers
various prompt patterns and mainstream uncertainty estimation methods. Our
findings highlight that LLMs exhibit two extremes: (1) LLMs display excessive
sensitivity towards groups or topics that may cause fairness issues, resulting
in misclassifying benign statements as hate speech. (2) LLMs' confidence scores
for each method excessively concentrate on a fixed range, remaining unchanged
regardless of the dataset's complexity. Consequently, the calibration
performance is heavily reliant on primary classification accuracy. These
discoveries unveil new limitations of LLMs, underscoring the need for caution
when optimizing models to ensure they do not veer towards extremes. This serves
as a reminder to carefully consider sensitivity and confidence in the pursuit
of model fairness.
</p>
</div>
</dd>
<dt><a name="item230">[230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11409" title="Abstract">arXiv:2402.11409</a> [<a href="/pdf/2402.11409" title="Download PDF">pdf</a>, <a href="/format/2402.11409" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-dimensional Evaluation of Empathetic Dialog Responses
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhichao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jiepu Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Empathy is a critical element of effective and satisfactory conversational
communication, yet previous studies in measuring conversational empathy mostly
focus on expressed communicative intents -- in which way empathy is expressed,
ignoring the fact that conversation is also a collaborative practice involving
both speakers and listeners. In contrast, we propose a multi-dimensional
empathy evaluation framework that extends upon existing work to measure both
expressed intents from the speaker's perspective and perceived empathy from the
listener's perspective. Applying the proposed framework to analyzing our
internal customer-service dialogue shows that the two dimensions (expressed
intent types and perceived empathy) are inter-connected, while perceived
empathy has high correlation with the satisfactory level of dialogue sessions.
This proposed framework still requires subjective assessments from trained
annotators, which can be non-trivial to collect. To scale up evaluation without
excessive reliance on carefully annotated data, we explore different modeling
options to automatically measure conversational empathy with (1) prompting
frozen large language models (LLMs) and (2) training language model-based
classifiers. Extensive experiments on both internal and external dialogue
datasets show that measuring conversational empathy remains a challenging task
for prompting frozen LLMs, reflected by less satisfying performance of GPT-4
and Flan family models. On the other hand, our proposed instruction-finetuned
classifiers based on sequence-to-sequence (Seq2Seq) language models is able to
achieve the best performance compared to prior works and competitive baselines.
Finally, we perform comprehensive ablation studies on the performance of
proposed instruction-finetuned classifiers and give recommendations on
potentially adopting them as automatic conversational empathy evaluation
metrics.
</p>
</div>
</dd>
<dt><a name="item231">[231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11410" title="Abstract">arXiv:2402.11410</a> [<a href="/pdf/2402.11410" title="Download PDF">pdf</a>, <a href="/ps/2402.11410" title="Download PostScript">ps</a>, <a href="/format/2402.11410" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Elementary Predictor Obtaining $2\sqrt{T}$ Distance to Calibration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arunachaleswaran%2C+E+R">Eshwar Ram Arunachaleswaran</a>, 
<a href="/search/cs?searchtype=author&query=Collina%2C+N">Natalie Collina</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+A">Aaron Roth</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+M">Mirah Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Data Structures and Algorithms (cs.DS); Machine Learning (stat.ML)

</div>
<p class="mathjax">Blasiok et al. [2023] proposed distance to calibration as a natural measure
of calibration error that unlike expected calibration error (ECE) is
continuous. Recently, Qiao and Zheng [2024] gave a non-constructive argument
establishing the existence of an online predictor that can obtain $O(\sqrt{T})$
distance to calibration in the adversarial setting, which is known to be
impossible for ECE. They leave as an open problem finding an explicit,
efficient algorithm. We resolve this problem and give an extremely simple,
efficient, deterministic algorithm that obtains distance to calibration error
at most $2\sqrt{T}$.
</p>
</div>
</dd>
<dt><a name="item232">[232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11411" title="Abstract">arXiv:2402.11411</a> [<a href="/pdf/2402.11411" title="Download PDF">pdf</a>, <a href="/format/2402.11411" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Aligning Modalities in Vision Large Language Models via Preference  Fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yiyang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+C">Chenhang Cui</a>, 
<a href="/search/cs?searchtype=author&query=Rafailov%2C+R">Rafael Rafailov</a>, 
<a href="/search/cs?searchtype=author&query=Finn%2C+C">Chelsea Finn</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+H">Huaxiu Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Instruction-following Vision Large Language Models (VLLMs) have achieved
significant progress recently on a variety of tasks. These approaches merge
strong pre-trained vision models and large language models (LLMs). Since these
components are trained separately, the learned representations need to be
aligned with joint training on additional image-language pairs. This procedure
is not perfect and can cause the model to hallucinate - provide answers that do
not accurately reflect the image, even when the core LLM is highly factual and
the vision backbone has sufficiently complete representations. In this work, we
frame the hallucination problem as an alignment issue, tackle it with
preference tuning. Specifically, we propose POVID to generate feedback data
with AI models. We use ground-truth instructions as the preferred response and
a two-stage approach to generate dispreferred data. First, we prompt GPT-4V to
inject plausible hallucinations into the correct answer. Second, we distort the
image to trigger the inherent hallucination behavior of the VLLM. This is an
automated approach, which does not rely on human data generation or require a
perfect expert, which makes it easily scalable. Finally, both of these
generation strategies are integrated into an RLHF pipeline via Direct
Preference Optimization. In experiments across broad benchmarks, we show that
we can not only reduce hallucinations, but improve model performance across
standard benchmarks, outperforming prior approaches. Our data and code are
available at https://github.com/YiyangZhou/POVID.
</p>
</div>
</dd>
<dt><a name="item233">[233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11412" title="Abstract">arXiv:2402.11412</a> [<a href="/pdf/2402.11412" title="Download PDF">pdf</a>, <a href="/format/2402.11412" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting Maximum Permitted Process Forces for Object Grasping and  Manipulation Using a Deep Learning Regression Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wucherer%2C+S">S. Wucherer</a>, 
<a href="/search/cs?searchtype=author&query=McMurray%2C+R">R. McMurray</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+K+Y">K. Y. Ng</a>, 
<a href="/search/cs?searchtype=author&query=Kerber%2C+F">F. Kerber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 4 figures, 3 tables, to be submitted as a conference paper to IEEE CCTA2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">During the execution of handling processes in manufacturing, it is difficult
to measure the process forces with state-of-the-art gripper systems since they
usually lack integrated sensors. Thus, the exact state of the gripped object
and the actuating process forces during manipulation and handling are unknown.
This paper proposes a deep learning regression model to construct a continuous
stability metric to predict the maximum process forces on the gripped objects
using high-resolution optical tactile sensors. A pull experiment was developed
to obtain a valid dataset for training. Continuously force-based labeled pairs
of tactile images for varying grip positions of industrial gearbox parts were
acquired to train a novel neural network inspired by encoder-decoder
architectures. A ResNet-18 model was used for comparison. Both models can
predict the maximum process force for each object with a precision of less than
1 N. During validation, the generalization potential of the proposed
methodology with respect to previously unknown objects was demonstrated with an
accuracy of 0.4-2.1 N and precision of 1.7-3.4 N, respectively.
</p>
</div>
</dd>
<dt><a name="item234">[234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11413" title="Abstract">arXiv:2402.11413</a> [<a href="/pdf/2402.11413" title="Download PDF">pdf</a>, <a href="/ps/2402.11413" title="Download PostScript">ps</a>, <a href="/format/2402.11413" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multispectral Automated Transfer Technique (MATT) for machine-driven  image labeling utilizing the Segment Anything Model (SAM)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gallagher%2C+J+E">James E. Gallagher</a>, 
<a href="/search/cs?searchtype=author&query=Gogia%2C+A">Aryav Gogia</a>, 
<a href="/search/cs?searchtype=author&query=Oughton%2C+E+J">Edward J. Oughton</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Segment Anything Model (SAM) is drastically accelerating the speed and
accuracy of automatically segmenting and labeling large Red-Green-Blue (RGB)
imagery datasets. However, SAM is unable to segment and label images outside of
the visible light spectrum, for example, for multispectral or hyperspectral
imagery. Therefore, this paper outlines a method we call the Multispectral
Automated Transfer Technique (MATT). By transposing SAM segmentation masks from
RGB images we can automatically segment and label multispectral imagery with
high precision and efficiency. For example, the results demonstrate that
segmenting and labeling a 2,400-image dataset utilizing MATT achieves a time
reduction of 87.8% in developing a trained model, reducing roughly 20 hours of
manual labeling, to only 2.4 hours. This efficiency gain is associated with
only a 6.7% decrease in overall mean average precision (mAP) when training
multispectral models via MATT, compared to a manually labeled dataset. We
consider this an acceptable level of precision loss when considering the time
saved during training, especially for rapidly prototyping experimental modeling
methods. This research greatly contributes to the study of multispectral object
detection by providing a novel and open-source method to rapidly segment,
label, and train multispectral object detection models with minimal human
interaction. Future research needs to focus on applying these methods to (i)
space-based multispectral, and (ii) drone-based hyperspectral imagery.
</p>
</div>
</dd>
<dt><a name="item235">[235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11414" title="Abstract">arXiv:2402.11414</a> [<a href="/pdf/2402.11414" title="Download PDF">pdf</a>, <a href="/format/2402.11414" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fine-grained and Explainable Factuality Evaluation for Multimodal  Summarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jing%2C+L">Liqiang Jing</a>, 
<a href="/search/cs?searchtype=author&query=Zuo%2C+J">Jingxuan Zuo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Multimodal summarization aims to generate a concise summary based on the
input text and image. However, the existing methods potentially suffer from
unfactual output. To evaluate the factuality of multimodal summarization
models, we propose two fine-grained and explainable evaluation frameworks
(FALLACIOUS) for different application scenarios, i.e. reference-based
factuality evaluation framework and reference-free factuality evaluation
framework. Notably, the reference-free factuality evaluation framework doesn't
need ground truth and hence it has a wider application scenario. To evaluate
the effectiveness of the proposed frameworks, we compute the correlation
between our frameworks and the other metrics. The experimental results show the
effectiveness of our proposed method. We will release our code and dataset via
github.
</p>
</div>
</dd>
<dt><a name="item236">[236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11417" title="Abstract">arXiv:2402.11417</a> [<a href="/pdf/2402.11417" title="Download PDF">pdf</a>, <a href="/format/2402.11417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoRETTA: Low-Rank Economic Tensor-Train Adaptation for  Ultra-Low-Parameter Fine-Tuning of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yifan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiajun Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+N">Ngai Wong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Various parameter-efficient fine-tuning (PEFT) techniques have been proposed
to enable computationally efficient fine-tuning while maintaining model
performance. However, existing PEFT methods are still limited by the growing
number of trainable parameters with the rapid deployment of Large Language
Models (LLMs). To address this challenge, we present LoRETTA, an
ultra-parameter-efficient framework that significantly reduces trainable
parameters through tensor-train decomposition. Specifically, we propose two
methods, named {LoRETTA}$_{adp}$ and {LoRETTA}$_{rep}$. The former employs
tensorized adapters, offering a high-performance yet lightweight approach for
the fine-tuning of LLMs. The latter emphasizes fine-tuning via weight
parameterization with a set of small tensor factors. LoRETTA achieves
comparable or better performance than most widely used PEFT methods with up to
$100\times$ fewer parameters on the LLaMA-2-7B models. Furthermore, empirical
results demonstrate that the proposed method effectively improves training
efficiency, enjoys better multi-task learning performance, and enhances the
anti-overfitting capability. Plug-and-play codes built upon the Huggingface
framework and PEFT library will be released.
</p>
</div>
</dd>
<dt><a name="item237">[237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11420" title="Abstract">arXiv:2402.11420</a> [<a href="/pdf/2402.11420" title="Download PDF">pdf</a>, <a href="/format/2402.11420" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking the Roles of Large Language Models in Chinese Grammatical  Error Correction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yinghui Li</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+S">Shang Qin</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jingheng Ye</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shirong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yangning Li</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+L">Libo Qin</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xuming Hu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wenhao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Hai-Tao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recently, Large Language Models (LLMs) have been widely studied by
researchers for their roles in various downstream NLP tasks. As a fundamental
task in the NLP field, Chinese Grammatical Error Correction (CGEC) aims to
correct all potential grammatical errors in the input sentences. Previous
studies have shown that LLMs' performance as correctors on CGEC remains
unsatisfactory due to its challenging task focus. To promote the CGEC field to
better adapt to the era of LLMs, we rethink the roles of LLMs in the CGEC task
so that they can be better utilized and explored in CGEC. Considering the rich
grammatical knowledge stored in LLMs and their powerful semantic understanding
capabilities, we utilize LLMs as explainers to provide explanation information
for the CGEC small models during error correction to enhance performance. We
also use LLMs as evaluators to bring more reasonable CGEC evaluations, thus
alleviating the troubles caused by the subjectivity of the CGEC task. In
particular, our work is also an active exploration of how LLMs and small models
better collaborate in downstream tasks. Extensive experiments and detailed
analyses on widely used datasets verify the effectiveness of our thinking
intuition and the proposed methods.
</p>
</div>
</dd>
<dt><a name="item238">[238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11421" title="Abstract">arXiv:2402.11421</a> [<a href="/pdf/2402.11421" title="Download PDF">pdf</a>, <a href="/format/2402.11421" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Fatigue-Induced Compensatory Movements in Bicep Curls:  Gaining Insights for the Deployment of Wearable Sensors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chua%2C+M+X">Ming Xuan Chua</a>, 
<a href="/search/cs?searchtype=author&query=Okubo%2C+Y">Yoshiro Okubo</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+S">Shuhua Peng</a>, 
<a href="/search/cs?searchtype=author&query=Do%2C+T+N">Thanh Nho Do</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C+H">Chun Hui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Liao Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures, submitted
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">A common challenge in Bicep Curls rehabilitation is muscle compensation,
where patients adopt alternative movement patterns when the primary muscle
group cannot act due to injury or fatigue, significantly decreasing the
effectiveness of rehabilitation efforts. The problem is exacerbated by the
growing trend toward transitioning from in-clinic to home-based rehabilitation,
where constant monitoring and correction by physiotherapists are limited. To
address this challenge, developing wearable sensors capable of detecting muscle
compensation becomes crucial. This study aims to gain insights for the optimal
deployment of wearable sensors through a comprehensive study of muscle
compensation in Bicep Curls. We collect upper limb joint kinematics and surface
electromyography signals (sEMG) from eight muscles in 12 healthy subjects
during standard and fatigue stages. Two muscle synergies are derived from sEMG
signals and are analyzed comprehensively along with joint kinematics. Our
findings reveal a shift in the relative contribution of forearm muscles to
shoulder muscles, accompanied by a significant increase in activation amplitude
for both synergies. Additionally, more pronounced movement was observed at the
shoulder joint during fatigue. These results suggest focusing on the should
muscle activities and joint motions when deploying wearable sensors for
effective detection of compensatory movements.
</p>
</div>
</dd>
<dt><a name="item239">[239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11422" title="Abstract">arXiv:2402.11422</a> [<a href="/pdf/2402.11422" title="Download PDF">pdf</a>, <a href="/format/2402.11422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mitigating Catastrophic Forgetting in Multi-domain Chinese Spelling  Correction by Multi-stage Knowledge Transfer Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xing%2C+P">Peng Xing</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yinghui Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+S">Shirong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xinnian Liang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Haojing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yangning Li</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Hai-Tao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Wenhao Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Ying Shen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Chinese Spelling Correction (CSC) aims to detect and correct spelling errors
in given sentences. Recently, multi-domain CSC has gradually attracted the
attention of researchers because it is more practicable. In this paper, we
focus on the key flaw of the CSC model when adapting to multi-domain scenarios:
the tendency to forget previously acquired knowledge upon learning new
domain-specific knowledge (i.e., catastrophic forgetting). To address this, we
propose a novel model-agnostic Multi-stage Knowledge Transfer (MKT) framework,
which utilizes a continuously evolving teacher model for knowledge transfer in
each domain, rather than focusing solely on new domain knowledge. It deserves
to be mentioned that we are the first to apply continual learning methods to
the multi-domain CSC task. Experiments prove the effectiveness of our proposed
method, and further analyses demonstrate the importance of overcoming
catastrophic forgetting for improving the model performance.
</p>
</div>
</dd>
<dt><a name="item240">[240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11423" title="Abstract">arXiv:2402.11423</a> [<a href="/pdf/2402.11423" title="Download PDF">pdf</a>, <a href="/format/2402.11423" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VoltSchemer: Use Voltage Noise to Manipulate Your Wireless Charger
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhan%2C+Z">Zihao Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yirui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+H">Haoqi Shan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hanqiu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yier Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by the 33rd USENIX Security Symposium
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Wireless charging is becoming an increasingly popular charging solution in
portable electronic products for a more convenient and safer charging
experience than conventional wired charging. However, our research identified
new vulnerabilities in wireless charging systems, making them susceptible to
intentional electromagnetic interference. These vulnerabilities facilitate a
set of novel attack vectors, enabling adversaries to manipulate the charger and
perform a series of attacks.
<br />In this paper, we propose VoltSchemer, a set of innovative attacks that grant
attackers control over commercial-off-the-shelf wireless chargers merely by
modulating the voltage from the power supply. These attacks represent the first
of its kind, exploiting voltage noises from the power supply to manipulate
wireless chargers without necessitating any malicious modifications to the
chargers themselves. The significant threats imposed by VoltSchemer are
substantiated by three practical attacks, where a charger can be manipulated
to: control voice assistants via inaudible voice commands, damage devices being
charged through overcharging or overheating, and bypass Qi-standard specified
foreign-object-detection mechanism to damage valuable items exposed to intense
magnetic fields.
<br />We demonstrate the effectiveness and practicality of the VoltSchemer attacks
with successful attacks on 9 top-selling COTS wireless chargers. Furthermore,
we discuss the security implications of our findings and suggest possible
countermeasures to mitigate potential threats.
</p>
</div>
</dd>
<dt><a name="item241">[241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11424" title="Abstract">arXiv:2402.11424</a> [<a href="/pdf/2402.11424" title="Download PDF">pdf</a>, <a href="/format/2402.11424" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Data Distribution Distilled Generative Model for Generalized Zero-Shot  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yijie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+M">Mingjian Hong</a>, 
<a href="/search/cs?searchtype=author&query=Huangfu%2C+L">Luwen Huangfu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Sheng Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted as AAAI 2024 oral paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the realm of Zero-Shot Learning (ZSL), we address biases in Generalized
Zero-Shot Learning (GZSL) models, which favor seen data. To counter this, we
introduce an end-to-end generative GZSL framework called D$^3$GZSL. This
framework respects seen and synthesized unseen data as in-distribution and
out-of-distribution data, respectively, for a more balanced model. D$^3$GZSL
comprises two core modules: in-distribution dual space distillation (ID$^2$SD)
and out-of-distribution batch distillation (O$^2$DBD). ID$^2$SD aligns
teacher-student outcomes in embedding and label spaces, enhancing learning
coherence. O$^2$DBD introduces low-dimensional out-of-distribution
representations per batch sample, capturing shared structures between seen and
unseen categories. Our approach demonstrates its effectiveness across
established GZSL benchmarks, seamlessly integrating into mainstream generative
frameworks. Extensive experiments consistently showcase that D$^3$GZSL elevates
the performance of existing generative GZSL methods, underscoring its potential
to refine zero-shot learning practices.The code is available at:
https://github.com/PJBQ/D3GZSL.git
</p>
</div>
</dd>
<dt><a name="item242">[242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11426" title="Abstract">arXiv:2402.11426</a> [<a href="/pdf/2402.11426" title="Download PDF">pdf</a>, <a href="/ps/2402.11426" title="Download PostScript">ps</a>, <a href="/format/2402.11426" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximating Partition in Near-Linear Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+J">Jiayi Lian</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yuchen Mao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guochuan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in STOC2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">We propose an $\widetilde{O}(n + 1/\varepsilon)$-time FPTAS (Fully
Polynomial-Time Approximation Scheme) for the classical Partition problem. This
is the best possible (up to a logarithmic factor) assuming SETH (Strong
Exponential Time Hypothesis) [Abboud, Bringmann, Hermelin, and Shabtay'22].
Prior to our work, the best known FPTAS for Partition runs in $\widetilde{O}(n
+ 1/\varepsilon^{5/4})$ time [Deng, Jin and Mao'23, Wu and Chen'22]. Our result
is obtained by solving a more general problem of weakly approximating Subset
Sum.
</p>
</div>
</dd>
<dt><a name="item243">[243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11427" title="Abstract">arXiv:2402.11427</a> [<a href="/pdf/2402.11427" title="Download PDF">pdf</a>, <a href="/format/2402.11427" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OptEx: Expediting First-Order Optimization with Approximately  Parallelized Iterations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shu%2C+Y">Yao Shu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+J">Jiongfeng Fang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y+T">Ying Tiffany He</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F+R">Fei Richard Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">First-order optimization (FOO) algorithms are pivotal in numerous
computational domains such as machine learning and signal denoising. However,
their application to complex tasks like neural network training often entails
significant inefficiencies due to the need for many sequential iterations for
convergence. In response, we introduce first-order optimization expedited with
approximately parallelized iterations (OptEx), the first framework that
enhances the efficiency of FOO by leveraging parallel computing to mitigate its
iterative bottleneck. OptEx employs kernelized gradient estimation to make use
of gradient history for future gradient prediction, enabling parallelization of
iterations -- a strategy once considered impractical because of the inherent
iterative dependency in FOO. We provide theoretical guarantees for the
reliability of our kernelized gradient estimation and the iteration complexity
of SGD-based OptEx, confirming that estimation errors diminish to zero as
historical gradients accumulate and that SGD-based OptEx enjoys an effective
acceleration rate of $\Omega(\sqrt{N})$ over standard SGD given parallelism of
N. We also use extensive empirical studies, including synthetic functions,
reinforcement learning tasks, and neural network training across various
datasets, to underscore the substantial efficiency improvements achieved by
OptEx.
</p>
</div>
</dd>
<dt><a name="item244">[244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11429" title="Abstract">arXiv:2402.11429</a> [<a href="/pdf/2402.11429" title="Download PDF">pdf</a>, <a href="/format/2402.11429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deformable Object Manipulation With Constraints Using Path Set Planning  and Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jing Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+X">Xiangyu Chu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xin Ma</a>, 
<a href="/search/cs?searchtype=author&query=Au%2C+K+W+S">Kwok Wai Samuel Au</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 25 figures, journal
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Transactions on Robotics, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In robotic deformable object manipulation (DOM) applications, constraints
arise commonly from environments and task-specific requirements. Enabling DOM
with constraints is therefore crucial for its deployment in practice. However,
dealing with constraints turns out to be challenging due to many inherent
factors such as inaccessible deformation models of deformable objects (DOs) and
varying environmental setups. This article presents a systematic manipulation
framework for DOM subject to constraints by proposing a novel path set planning
and tracking scheme. First, constrained DOM tasks are formulated into a
versatile optimization formalism which enables dynamic constraint imposition.
Because of the lack of the local optimization objective and high state
dimensionality, the formulated problem is not analytically solvable. To address
this, planning of the path set, which collects paths of DO feedback points, is
proposed subsequently to offer feasible path and motion references for DO in
constrained setups. Both theoretical analyses and computationally efficient
algorithmic implementation of path set planning are discussed. Lastly, a
control architecture combining path set tracking and constraint handling is
designed for task execution. The effectiveness of our methods is validated in a
variety of DOM tasks with constrained experimental settings.
</p>
</div>
</dd>
<dt><a name="item245">[245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11430" title="Abstract">arXiv:2402.11430</a> [<a href="/pdf/2402.11430" title="Download PDF">pdf</a>, <a href="/format/2402.11430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EventRL: Enhancing Event Extraction with Outcome Supervision for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Huan Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Changlong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruifeng Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In this study, we present EventRL, a reinforcement learning approach
developed to enhance event extraction for large language models (LLMs). EventRL
utilizes outcome supervision with specific reward functions to tackle prevalent
challenges in LLMs, such as instruction following and hallucination, manifested
as the mismatch of event structure and the generation of undefined event types.
We evaluate EventRL against existing methods like Few-Shot Prompting (FSP)
(based on GPT4) and Supervised Fine-Tuning (SFT) across various LLMs, including
GPT-4, LLaMa, and CodeLLaMa models. Our findings show that EventRL
significantly outperforms these conventional approaches by improving the
performance in identifying and structuring events, particularly in handling
novel event types. The study emphasizes the critical role of reward function
selection and demonstrates the benefits of incorporating code data for better
event extraction. While increasing model size leads to higher accuracy,
maintaining the ability to generalize is essential to avoid overfitting.
</p>
</div>
</dd>
<dt><a name="item246">[246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11431" title="Abstract">arXiv:2402.11431</a> [<a href="/pdf/2402.11431" title="Download PDF">pdf</a>, <a href="/format/2402.11431" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Robust Error-Resistant View Selection Method for 3D Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaojie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yinghui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Nan%2C+B">Bin Nan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jinlong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+T">Tao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Liangyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingfeng Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">To address the issue of increased triangulation uncertainty caused by
selecting views with small camera baselines in Structure from Motion (SFM) view
selection, this paper proposes a robust error-resistant view selection method.
The method utilizes a triangulation-based computation to obtain an
error-resistant model, which is then used to construct an error-resistant
matrix. The sorting results of each row in the error-resistant matrix determine
the candidate view set for each view. By traversing the candidate view sets of
all views and completing the missing views based on the error-resistant matrix,
the integrity of 3D reconstruction is ensured. Experimental comparisons between
this method and the exhaustive method with the highest accuracy in the COLMAP
program are conducted in terms of average reprojection error and absolute
trajectory error in the reconstruction results. The proposed method
demonstrates an average reduction of 29.40% in reprojection error accuracy and
5.07% in absolute trajectory error on the TUM dataset and DTU dataset.
</p>
</div>
</dd>
<dt><a name="item247">[247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11432" title="Abstract">arXiv:2402.11432</a> [<a href="/pdf/2402.11432" title="Download PDF">pdf</a>, <a href="/format/2402.11432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark  for Deception Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+Z">Zheng Lian</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Haiyang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+J">Jianhua Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Deception detection has attracted increasing attention due to its importance
in many practical scenarios. Currently, data scarcity harms the development of
this field. On the one hand, it is costly to hire participants to simulate
deception scenarios. On the other hand, it is difficult to collect videos
containing deceptive behaviors on the Internet. To address data scarcity, this
paper proposes a new data collection pipeline. Specifically, we use GPT-4 to
simulate a role-play between a suspect and a police officer. During
interrogation, the suspect lies to the police officer to evade responsibility
for the crime, while the police officer uncovers the truth and gathers
evidence. Compared with previous datasets, this strategy reduces data
collection costs, providing a promising way to increase the dataset size.
Meanwhile, we extend the traditional deception detection task to deception
reasoning, further providing evidence for deceptive parts. This dataset can
also be used to evaluate the complex reasoning capability of current large
language models and serve as a reasoning benchmark for further research.
</p>
</div>
</dd>
<dt><a name="item248">[248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11433" title="Abstract">arXiv:2402.11433</a> [<a href="/pdf/2402.11433" title="Download PDF">pdf</a>, <a href="/ps/2402.11433" title="Download PostScript">ps</a>, <a href="/format/2402.11433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improved Indoor Localization with Machine Learning Techniques for IoT  applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maduranga%2C+M+W+P">M.W.P. Maduranga</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PhD thesis
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Networking and Internet Architecture (cs.NI); Signal Processing (eess.SP)

</div>
<p class="mathjax">The rise of the Internet of Things (IoT) and mobile internet applications has
spurred interest in location-based services (LBS) for commercial, military, and
social applications. While the global positioning system (GPS) dominates
outdoor localization, its efficacy wanes indoors due to signal challenges.
Indoor localization systems leverage wireless technologies like Wi-Fi, ZigBee,
Bluetooth, UWB, selecting based on context. Received signal strength indicator
(RSSI) technology, known for its accuracy and simplicity, is widely adopted.
This study employs machine learning algorithms in three phases: supervised
regressors, supervised classifiers, and ensemble methods for RSSI-based indoor
localization. Additionally, it introduces a weighted least squares technique
and pseudo-linear solution approach to address non-linear RSSI measurement
equations by approximating them with linear equations. An experimental testbed,
utilizing diverse wireless technologies and anchor nodes, is designed for data
collection, employing IoT cloud architectures. Pre-processing involves
investigating filters for data refinement before algorithm training. The study
employs machine learning models like linear regression, polynomial regression,
support vector regression, random forest regression, and decision tree
regressor across various wireless technologies. These models estimate the
geographical coordinates of a moving target node, and their performance is
evaluated using metrics such as accuracy, root mean square errors, precision,
recall, sensitivity, coefficient of determinant, and the f1-score. The
experiment's outcomes provide insights into the effectiveness of different
supervised machine learning techniques in terms of localization accuracy and
robustness in indoor environments.
</p>
</div>
</dd>
<dt><a name="item249">[249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11435" title="Abstract">arXiv:2402.11435</a> [<a href="/pdf/2402.11435" title="Download PDF">pdf</a>, <a href="/format/2402.11435" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Momentor: Advancing Video Large Language Model with Fine-Grained  Temporal Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+L">Long Qian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juncheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yaobo Ye</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+H">Hao Fei</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Y">Yueting Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Siliang Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) demonstrate remarkable proficiency in
comprehending and handling text-based tasks. Many efforts are being made to
transfer these attributes to video modality, which are termed Video-LLMs.
However, existing Video-LLMs can only capture the coarse-grained semantics and
are unable to effectively handle tasks related to comprehension or localization
of specific video segments. In light of these challenges, we propose Momentor,
a Video-LLM capable of accomplishing fine-grained temporal understanding tasks.
To support the training of Momentor, we design an automatic data generation
engine to construct Moment-10M, a large-scale video instruction dataset with
segment-level instruction data. We train Momentor on Moment-10M, enabling it to
perform segment-level reasoning and localization. Zero-shot evaluations on
several tasks demonstrate that Momentor excels in fine-grained temporally
grounded comprehension and localization.
</p>
</div>
</dd>
<dt><a name="item250">[250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11436" title="Abstract">arXiv:2402.11436</a> [<a href="/pdf/2402.11436" title="Download PDF">pdf</a>, <a href="/format/2402.11436" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wenda Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+G">Guanglei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+X">Xuandong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Liangming Pan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W+Y">William Yang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent studies show that self-feedback improves large language models (LLMs)
on certain tasks while worsens other tasks. We discovered that such a contrary
is due to LLM's bias towards their own output. In this paper, we formally
define LLM's self-bias -- the tendency to favor its own generation -- using two
statistics. We analyze six LLMs on translation, constrained text generation,
and mathematical reasoning tasks. We find that self-bias is prevalent in all
examined LLMs across multiple languages and tasks. Our analysis reveals that
while the self-refine pipeline improves the fluency and understandability of
model outputs, it further amplifies self-bias. To mitigate such biases, we
discover that larger model size and external feedback with accurate assessment
can significantly reduce bias in the self-refine pipeline, leading to actual
performance improvement in downstream tasks.
</p>
</div>
</dd>
<dt><a name="item251">[251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11437" title="Abstract">arXiv:2402.11437</a> [<a href="/pdf/2402.11437" title="Download PDF">pdf</a>, <a href="/format/2402.11437" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Assignment Game: New Mechanisms for Equitable Core Imputations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vazirani%2C+V+V">Vijay V. Vazirani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Theoretical Economics (econ.TH)

</div>
<p class="mathjax">The set of core imputations of the assignment game forms a (non-finite)
distributive lattice. So far, efficient algorithms were known for computing
only its two extreme imputations; however, each of them maximally favors one
side and disfavors the other side of the bipartition, leading to inequitable
profit sharing. Another issue is that a sub-coalition consisting of one player
(or a set of players from the same side of the bipartition) can make zero
profit, therefore a core imputation is not obliged to give them any profit.
Hence core imputations make no fairness guarantee at the level of individual
agents. This raises the question of computing {\em more equitable core
imputations}.
<br />In this paper, we give combinatorial (i.e., the mechanism does not invoke an
LP-solver) polynomial time mechanisms for computing the leximin and leximax
core imputations for the assignment game. These imputations achieve
``fairness'' in different ways: whereas leximin tries to make poor agents more
rich, leximax tries to make rich agents less rich. In general, the two
imputations are different.
<br />Our mechanisms were derived by a suitable adaptation of the classical
primal-dual paradigm from combinatorial optimization. The ``engine'' driving
them involves recent insights, obtained via complementarity, into core
imputations \cite{Va.New-characterizations} and the pristine combinatorial
structure of matching. We have identified other natural games which could
benefit from our approach.
</p>
</div>
</dd>
<dt><a name="item252">[252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11438" title="Abstract">arXiv:2402.11438</a> [<a href="/pdf/2402.11438" title="Download PDF">pdf</a>, <a href="/format/2402.11438" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NestedSGX: Bootstrapping Trust to Enclaves within Confidential VMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+L">Linke Song</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+B">Benshan Mei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shuang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shijun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shoumeng Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">XiaoFeng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+D">Dan Meng</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+R">Rui Hou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">Integrity is critical for maintaining system security, as it ensures that
only genuine software is loaded onto a machine. Although confidential virtual
machines (CVMs) function within isolated environments separate from the host,
it is important to recognize that users still encounter challenges in
maintaining control over the integrity of the code running within the trusted
execution environments (TEEs). The presence of a sophisticated operating system
(OS) raises the possibility of dynamically creating and executing any code,
making user applications within TEEs vulnerable to interference or tampering if
the guest OS is compromised. This paper introduces NestedSGX, which leverages
virtual machine privilege level (VMPL), a recent hardware feature available on
AMD SEV-SNP to enable the creation of hardware enclaves within the guest VM.
Similar to Intel SGX, NestedSGX considers the guest OS untrusted for loading
potentially malicious code. It ensures that only trusted and measured code
executed within the enclave can be remotely attested. To seamlessly protect
existing applications, NestedSGX aims for compatibility with Intel SGX by
simulating SGX leaf functions. We have also ported the SGX SDK to NestedSGX,
enabling the use of existing SGX toolchains and applications in the system.
Performance evaluations show that context switches in NestedSGX take about
35,000-37,000 cycles, approximately 2-3 times that of Intel SGX. NestedSGX
incurs minimal overhead in most real-world applications, with an average
overhead below 5% for most workloads and 22.7% for I/O intensive workloads.
</p>
</div>
</dd>
<dt><a name="item253">[253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11440" title="Abstract">arXiv:2402.11440</a> [<a href="/pdf/2402.11440" title="Download PDF">pdf</a>, <a href="/format/2402.11440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Simple Proof that Ricochet Robots is PSPACE-Complete
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balanza-Martinez%2C+J">Jose Balanza-Martinez</a>, 
<a href="/search/cs?searchtype=author&query=Cantu%2C+A+A">Angel A. Cantu</a>, 
<a href="/search/cs?searchtype=author&query=Schweller%2C+R">Robert Schweller</a>, 
<a href="/search/cs?searchtype=author&query=Wylie%2C+T">Tim Wylie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">In this paper, we seek to provide a simpler proof that the relocation problem
in Ricochet Robots (Lunar Lockout with fixed geometry) is PSPACE-complete via a
reduction from Finite Function Generation (FFG). Although this result was
originally proven in 2003, we give a simpler reduction by utilizing the FFG
problem, and put the result in context with recent publications showing that
relocation is also PSPACE-complete in related models.
</p>
</div>
</dd>
<dt><a name="item254">[254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11441" title="Abstract">arXiv:2402.11441</a> [<a href="/pdf/2402.11441" title="Download PDF">pdf</a>, <a href="/format/2402.11441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InfuserKI: Enhancing Large Language Models with Knowledge Graphs via  Infuser-Guided Knowledge Integration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fali Wang</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+R">Runxue Bao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Suhang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenchao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yanchi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haifeng Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Though Large Language Models (LLMs) have shown remarkable open-generation
capabilities across diverse domains, they struggle with knowledge-intensive
tasks. To alleviate this issue, knowledge integration methods have been
proposed to enhance LLMs with domain-specific knowledge graphs using external
modules. However, they suffer from data inefficiency as they require both known
and unknown knowledge for fine-tuning. Thus, we study a novel problem of
integrating unknown knowledge into LLMs efficiently without unnecessary overlap
of known knowledge. Injecting new knowledge poses the risk of forgetting
previously acquired knowledge. To tackle this, we propose a novel
Infuser-Guided Knowledge Integration (InfuserKI) framework that utilizes
transformer internal states to determine whether to enhance the original LLM
output with additional information, thereby effectively mitigating knowledge
forgetting. Evaluations on the UMLS-2.5k and MetaQA domain knowledge graphs
demonstrate that InfuserKI can effectively acquire new knowledge and outperform
state-of-the-art baselines by 9% and 6%, respectively, in reducing knowledge
forgetting.
</p>
</div>
</dd>
<dt><a name="item255">[255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11442" title="Abstract">arXiv:2402.11442</a> [<a href="/pdf/2402.11442" title="Download PDF">pdf</a>, <a href="/format/2402.11442" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and  Improving LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Siyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhongyu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+Y">Yejin Choi</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+X">Xiang Ren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have achieved impressive human-like performance
across various reasoning tasks. However, their mastery of underlying
inferential rules still falls short of human capabilities. To investigate this,
we propose a logic scaffolding inferential rule generation framework, to
construct an inferential rule base, ULogic, comprising both primitive and
compositional rules across five domains. Our analysis of GPT-series models over
a rule subset reveals significant gaps in LLMs' logic understanding compared to
human performance, especially in compositional and structural complex rules
with certain bias patterns. We further distill these rules into a smaller-scale
inference engine for flexible rule generation and enhancing downstream
reasoning. Through a multi-judger evaluation, our inference engine proves
effective in generating accurate, complex and abstract conclusions and
premises, and improve various commonsense reasoning tasks. Overall, our work
sheds light on LLMs' limitations in grasping inferential rule and suggests ways
to enhance their logical reasoning abilities~\footnote{Code and data are
available at \url{https://github.com/SiyuanWangw/ULogic}.}.
</p>
</div>
</dd>
<dt><a name="item256">[256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11443" title="Abstract">arXiv:2402.11443</a> [<a href="/pdf/2402.11443" title="Download PDF">pdf</a>, <a href="/format/2402.11443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM  Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Siyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+Z">Zhuohan Long</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+Z">Zhihao Fan</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhongyu Wei</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This paper presents a benchmark self-evolving framework to dynamically
evaluate rapidly advancing Large Language Models (LLMs), aiming for a more
accurate assessment of their capabilities and limitations. We utilize a
multi-agent system to manipulate the context or question of original instances,
reframing new evolving instances with high confidence that dynamically extend
existing benchmarks. Towards a more scalable, robust and fine-grained
evaluation, we implement six reframing operations to construct evolving
instances testing LLMs against diverse queries, data noise and probing their
problem-solving sub-abilities. With this framework, we extend benchmark
datasets of four tasks. Experimental results show a general performance decline
in most LLMs against their original results. This decline under our scalable
and robust evaluations, alongside our fine-grained evaluation, more accurately
reflect models' capabilities. Besides, our framework widens performance
discrepancies both between different models and within the same model across
various tasks, facilitating more informed model selection for specific tasks
(Code and data are available at
https://github.com/NanshineLoong/Self-Evolving-Benchmark).
</p>
</div>
</dd>
<dt><a name="item257">[257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11444" title="Abstract">arXiv:2402.11444</a> [<a href="/pdf/2402.11444" title="Download PDF">pdf</a>, <a href="/ps/2402.11444" title="Download PostScript">ps</a>, <a href="/format/2402.11444" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gauging Public Acceptance of Conditionally Automated Cars in the United  States
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saravanos%2C+A">Antonios Saravanos</a> (1) ((1) New York University)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this work we look at an element of smart cities, conditionally automated
cars (SAE Level 3), investigating the factors influencing public acceptance in
the United States. We apply an adaptation of the UTUAT2 model. Taking an
experimental approach study 358 participants in the US were presented with a
vignette outlining the L3 technology followed by a series of questions to
capture their perceptions of conditionally automated cars. PLS-SEM was used to
analyze the collected data. The results reveal that the acceptance of the
technology, in order of decreasing importance, was determined by social
influence, performance expectancy, hedonic motivation, facilitating conditions,
and effort expectancy. Furthermore, hedonic motivation, social influence,
facilitating conditions and effort expectancy all have a positive influence on
the perception of how useful the technology is; facilitating conditions,
hedonic motivation, and social influence all have a positive influence on
effort expectancy; social influence and facilitating conditions positively
influence hedonic motivation; and social influence positively influences
facilitating conditions. A moderating effect for gender was found, with the
effect of hedonic motivation influencing intention to adopt is more prominent
for men.
</p>
</div>
</dd>
<dt><a name="item258">[258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11445" title="Abstract">arXiv:2402.11445</a> [<a href="/pdf/2402.11445" title="Download PDF">pdf</a>, <a href="/format/2402.11445" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balanced Truncation of Linear Systems with Quadratic Outputs in Limited  Time and Frequency Intervals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Song%2C+Q">Qiu-Yan Song</a>, 
<a href="/search/eess?searchtype=author&query=Zulfiqar%2C+U">Umair Zulfiqar</a>, 
<a href="/search/eess?searchtype=author&query=Xiao%2C+Z">Zhi-Hua Xiao</a>, 
<a href="/search/eess?searchtype=author&query=Uddin%2C+M+M">Mohammad Monir Uddin</a>, 
<a href="/search/eess?searchtype=author&query=Sreeram%2C+V">Victor Sreeram</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Model order reduction involves constructing a reduced-order approximation of
a high-order model while retaining its essential characteristics. This
reduced-order model serves as a substitute for the original one in various
applications such as simulation, analysis, and design. Often, there's a need to
maintain high accuracy within a specific time or frequency interval, while
errors beyond this limit can be tolerated. This paper addresses time-limited
and frequency-limited model order reduction scenarios for linear systems with
quadratic outputs, by generalizing the recently introduced structure-preserving
balanced truncation algorithm. To that end, limited interval system Gramians
are defined, and the corresponding generalized Lyapunov equations governing
their computation are derived. Additionally, low-rank solutions for these
equations are investigated. Next, balanced truncation algorithms are proposed
for time-limited and frequency-limited scenarios, each utilizing its
corresponding limited-interval system Gramians. The proposed algorithms ensure
accurate results within specified time and frequency intervals while preserving
the quadratic-output structure. Two benchmark numerical examples are presented
to demonstrate the effectiveness of the algorithms, showcasing their ability to
achieve superior accuracy within the desired time or frequency interval.
</p>
</div>
</dd>
<dt><a name="item259">[259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11446" title="Abstract">arXiv:2402.11446</a> [<a href="/pdf/2402.11446" title="Download PDF">pdf</a>, <a href="/format/2402.11446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Penetration Vision through Virtual Reality Headsets: Identifying 360  Videos from Head Movements
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+A">Anh Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaokuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhisheng Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to USENIX Security '24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">In this paper, we present the first contactless side-channel attack for
identifying 360 videos being viewed in a Virtual Reality (VR) Head Mounted
Display (HMD). Although the video content is displayed inside the HMD without
any external exposure, we observe that user head movements are driven by the
video content, which creates a unique side channel that does not exist in
traditional 2D videos. By recording the user whose vision is blocked by the HMD
via a malicious camera, an attacker can analyze the correlation between the
user's head movements and the victim video to infer the video title.
<br />To exploit this new vulnerability, we present INTRUDE, a system for
identifying 360 videos from recordings of user head movements. INTRUDE is
empowered by an HMD-based head movement estimation scheme to extract a head
movement trace from the recording and a video saliency-based trace-fingerprint
matching framework to infer the video title. Evaluation results show that
INTRUDE achieves over 96% of accuracy for video identification and is robust
under different recording environments. Moreover, INTRUDE maintains its
effectiveness in the open-world identification scenario.
</p>
</div>
</dd>
<dt><a name="item260">[260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11447" title="Abstract">arXiv:2402.11447</a> [<a href="/pdf/2402.11447" title="Download PDF">pdf</a>, <a href="/format/2402.11447" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Example Ordering Guided by Label Distributions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhichao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+D">Daniel Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Srikumar%2C+V">Vivek Srikumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">By allowing models to predict without task-specific training, in-context
learning (ICL) with pretrained LLMs has enormous potential in NLP. However, a
number of problems persist in ICL. In particular, its performance is sensitive
to the choice and order of in-context examples. Given the same set of
in-context examples with different orderings, model performance may vary
between near random to near state-of-the-art. In this work, we formulate
in-context example ordering as an optimization problem. We examine three
problem settings that differ in the assumptions they make about what is known
about the task. Inspired by the idea of learning from label proportions, we
propose two principles for in-context example ordering guided by model's
probability predictions. We apply our proposed principles to thirteen text
classification datasets and nine different autoregressive LLMs with 700M to 13B
parameters. We demonstrate that our approach outperforms the baselines by
improving the classification accuracy, reducing model miscalibration, and also
by selecting better in-context examples.
</p>
</div>
</dd>
<dt><a name="item261">[261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11450" title="Abstract">arXiv:2402.11450</a> [<a href="/pdf/2402.11450" title="Download PDF">pdf</a>, <a href="/format/2402.11450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Learn Faster from Human Feedback with Language Model  Predictive Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jacky Liang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+F">Fei Xia</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+W">Wenhao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+A">Andy Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Arenas%2C+M+G">Montserrat Gonzalez Arenas</a>, 
<a href="/search/cs?searchtype=author&query=Attarian%2C+M">Maria Attarian</a>, 
<a href="/search/cs?searchtype=author&query=Bauza%2C+M">Maria Bauza</a>, 
<a href="/search/cs?searchtype=author&query=Bennice%2C+M">Matthew Bennice</a>, 
<a href="/search/cs?searchtype=author&query=Bewley%2C+A">Alex Bewley</a>, 
<a href="/search/cs?searchtype=author&query=Dostmohamed%2C+A">Adil Dostmohamed</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+C+K">Chuyuan Kelly Fu</a>, 
<a href="/search/cs?searchtype=author&query=Gileadi%2C+N">Nimrod Gileadi</a>, 
<a href="/search/cs?searchtype=author&query=Giustina%2C+M">Marissa Giustina</a>, 
<a href="/search/cs?searchtype=author&query=Gopalakrishnan%2C+K">Keerthana Gopalakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Hasenclever%2C+L">Leonard Hasenclever</a>, 
<a href="/search/cs?searchtype=author&query=Humplik%2C+J">Jan Humplik</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+J">Jasmine Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+N">Nikhil Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Jyenis%2C+B">Ben Jyenis</a>, 
<a href="/search/cs?searchtype=author&query=Kew%2C+C">Chase Kew</a>, 
<a href="/search/cs?searchtype=author&query=Kirmani%2C+S">Sean Kirmani</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+T+E">Tsang-Wei Edward Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kuang-Huei Lee</a>, 
<a href="/search/cs?searchtype=author&query=Michaely%2C+A+H">Assaf Hurwitz Michaely</a>, 
<a href="/search/cs?searchtype=author&query=Moore%2C+J">Joss Moore</a>, 
<a href="/search/cs?searchtype=author&query=Oslund%2C+K">Ken Oslund</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+D">Dushyant Rao</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+A">Allen Ren</a>, 
<a href="/search/cs?searchtype=author&query=Tabanpour%2C+B">Baruch Tabanpour</a>, 
<a href="/search/cs?searchtype=author&query=Vuong%2C+Q">Quan Vuong</a>, 
<a href="/search/cs?searchtype=author&query=Wahid%2C+A">Ayzaan Wahid</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+T">Ted Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Ying Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+V">Vincent Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+P">Peng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Frey%2C+E">Erik Frey</a>, 
<a href="/search/cs?searchtype=author&query=Caluwaerts%2C+K">Ken Caluwaerts</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tingnan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ichter%2C+B">Brian Ichter</a>, 
<a href="/search/cs?searchtype=author&query=Tompson%2C+J">Jonathan Tompson</a>, 
<a href="/search/cs?searchtype=author&query=Takayama%2C+L">Leila Takayama</a>, 
<a href="/search/cs?searchtype=author&query=Vanhoucke%2C+V">Vincent Vanhoucke</a>, 
<a href="/search/cs?searchtype=author&query=Shafran%2C+I">Izhak Shafran</a>, 
<a href="/search/cs?searchtype=author&query=Mataric%2C+M">Maja Mataric</a>, 
<a href="/search/cs?searchtype=author&query=Sadigh%2C+D">Dorsa Sadigh</a>, 
<a href="/search/cs?searchtype=author&query=Heess%2C+N">Nicolas Heess</a>, 
<a href="/search/cs?searchtype=author&query=Rao%2C+K">Kanishka Rao</a>, 
<a href="/search/cs?searchtype=author&query=Stewart%2C+N">Nik Stewart</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+J">Jie Tan</a>, 
<a href="/search/cs?searchtype=author&query=Parada%2C+C">Carolina Parada</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Large language models (LLMs) have been shown to exhibit a wide range of
capabilities, such as writing robot code from language commands -- enabling
non-experts to direct robot behaviors, modify them based on feedback, or
compose them to perform new tasks. However, these capabilities (driven by
in-context learning) are limited to short-term interactions, where users'
feedback remains relevant for only as long as it fits within the context size
of the LLM, and can be forgotten over longer interactions. In this work, we
investigate fine-tuning the robot code-writing LLMs, to remember their
in-context interactions and improve their teachability i.e., how efficiently
they adapt to human inputs (measured by average number of corrections before
the user considers the task successful). Our key observation is that when
human-robot interactions are formulated as a partially observable Markov
decision process (in which human language inputs are observations, and robot
code outputs are actions), then training an LLM to complete previous
interactions can be viewed as training a transition dynamics model -- that can
be combined with classic robotics techniques such as model predictive control
(MPC) to discover shorter paths to success. This gives rise to Language Model
Predictive Control (LMPC), a framework that fine-tunes PaLM 2 to improve its
teachability on 78 tasks across 5 robot embodiments -- improving non-expert
teaching success rates of unseen tasks by 26.9% while reducing the average
number of human corrections from 2.4 to 1.9. Experiments show that LMPC also
produces strong meta-learners, improving the success rate of in-context
learning new tasks on unseen robot embodiments and APIs by 31.5%. See videos,
code, and demos at: https://robot-teaching.github.io/.
</p>
</div>
</dd>
<dt><a name="item262">[262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11451" title="Abstract">arXiv:2402.11451</a> [<a href="/pdf/2402.11451" title="Download PDF">pdf</a>, <a href="/format/2402.11451" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SciAgent: Tool-augmented Language Models for Scientific Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Y">Yubo Ma</a>, 
<a href="/search/cs?searchtype=author&query=Gou%2C+Z">Zhibin Gou</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+J">Junheng Hao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Ruochen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuohang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Liangming Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yujiu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yixin Cao</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+A">Aixin Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Scientific reasoning poses an excessive challenge for even the most advanced
Large Language Models (LLMs). To make this task more practical and solvable for
LLMs, we introduce a new task setting named tool-augmented scientific
reasoning. This setting supplements LLMs with scalable toolsets, and shifts the
focus from pursuing an omniscient problem solver to a proficient tool-user. To
facilitate the research of such setting, we construct a tool-augmented training
corpus named MathFunc which encompasses over 30,000 samples and roughly 6,000
tools. Building on MathFunc, we develop SciAgent to retrieve, understand and,
if necessary, use tools for scientific problem solving. Additionally, we craft
a benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs'
abilities with tool assistance. Extensive experiments on SciToolBench confirm
the effectiveness of SciAgent. Notably, SciAgent-Mistral-7B surpasses other
LLMs with the same size by more than 13% in absolute accuracy. Furthermore,
SciAgent-DeepMath-7B shows much superior performance than ChatGPT.
</p>
</div>
</dd>
<dt><a name="item263">[263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11452" title="Abstract">arXiv:2402.11452</a> [<a href="/pdf/2402.11452" title="Download PDF">pdf</a>, <a href="/format/2402.11452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via  Controllable Question Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhaorun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhuokai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zhihong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruiqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+B">Bhiksha Raj</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+H">Huaxiu Yao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 4 figures, 11 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent advancements in large language models (LLMs) have shown promise in
multi-step reasoning tasks, yet their reliance on extensive manual labeling to
provide procedural feedback remains a significant impediment. To address this
challenge, in this paper, we propose a novel self-supervised framework AutoPRM
that efficiently enhances the fine-tuning of LLMs for intricate reasoning
challenges. Specifically, AutoPRM first decomposes complex problems into more
manageable subquestions with a controllable granularity switch, then
sequentially apply reinforcement learning to iteratively improve the
subquestion solver. Additionally, we propose context-guided-decoding to avoid
reward tampering and guide the subquestion solver towards the solution of the
holistic problem. Extensive experiments show that AutoPRM significantly
improves performance on mathematical and commonsense reasoning tasks over SOTA.
More encouragingly, AutoPRM can be easily integrated with other orthogonal
reasoning pipelines.
</p>
</div>
</dd>
<dt><a name="item264">[264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11453" title="Abstract">arXiv:2402.11453</a> [<a href="/pdf/2402.11453" title="Download PDF">pdf</a>, <a href="/format/2402.11453" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific  Data Visualization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhiyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zihan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cong%2C+X">Xin Cong</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yukun Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zhixing Tan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Pengyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+X">Xiaodong Shi</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Scientific data visualization plays a crucial role in research by enabling
the direct display of complex information and assisting researchers in
identifying implicit patterns. Despite its importance, the use of Large
Language Models (LLMs) for scientific data visualization remains rather
unexplored. In this study, we introduce MatPlotAgent, an efficient
model-agnostic LLM agent framework designed to automate scientific data
visualization tasks. Leveraging the capabilities of both code LLMs and
multi-modal LLMs, MatPlotAgent consists of three core modules: query
understanding, code generation with iterative debugging, and a visual feedback
mechanism for error correction. To address the lack of benchmarks in this
field, we present MatPlotBench, a high-quality benchmark consisting of 100
human-verified test cases. Additionally, we introduce a scoring approach that
utilizes GPT-4V for automatic evaluation. Experimental results demonstrate that
MatPlotAgent can improve the performance of various LLMs, including both
commercial and open-source models. Furthermore, the proposed evaluation method
shows a strong correlation with human-annotated scores.
</p>
</div>
</dd>
<dt><a name="item265">[265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11454" title="Abstract">arXiv:2402.11454</a> [<a href="/pdf/2402.11454" title="Download PDF">pdf</a>, <a href="/format/2402.11454" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Addressing Internally-Disconnected Communities in Leiden and Louvain  Community Detection Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sahu%2C+S">Subhajit Sahu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 11 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Community detection is the problem of identifying densely connected clusters
of nodes within a network. The Louvain algorithm is a widely used method for
this task, but it can produce communities that are internally disconnected. To
address this, the Leiden algorithm was introduced. However, our analysis and
empirical observations indicate that the Leiden algorithm still identifies
disconnected communities, albeit to a lesser extent. To mitigate this issue, we
propose two new parallel algorithms: GSP-Leiden and GSP-Louvain, based on the
Leiden and Louvain algorithms, respectively. On a system with two 16-core Intel
Xeon Gold 6226R processors, we demonstrate that GSP-Leiden/GSP-Louvain not only
address this issue, but also outperform the original Leiden, igraph Leiden, and
NetworKit Leiden by 373x/473x, 86x/186x, and 7.2x/17.2x respectively -
achieving a processing rate of 352M/652M edges/s on a 3.8B edge graph.
Furthermore, GSP-Leiden/GSP-Louvain improve performance at a rate of 1.6x/1.7x
for every doubling of threads.
</p>
</div>
</dd>
<dt><a name="item266">[266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11455" title="Abstract">arXiv:2402.11455</a> [<a href="/pdf/2402.11455" title="Download PDF">pdf</a>, <a href="/format/2402.11455" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative  Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hanqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ping%2C+B">Bowen Ping</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">LoRA employs lightweight modules to customize large language models (LLMs)
for each downstream task or domain, where different learned additional modules
represent diverse skills. Combining existing LoRAs to address new tasks can
enhance the reusability of learned LoRAs, particularly beneficial for tasks
with limited annotated data. Most prior works on LoRA combination primarily
rely on task-level weights for each involved LoRA, making different examples
and tokens share the same LoRA weights. However, in generative tasks, different
tokens may necessitate diverse skills to manage. Taking the Chinese math task
as an example, understanding the problem description may depend more on the
Chinese LoRA, while the calculation part may rely more on the math LoRA. To
this end, we propose LoRA-Flow, which utilizes dynamic weights to adjust the
impact of different LoRAs. The weights at each step are determined by a fusion
gate with extremely few parameters, which can be learned with only 200 training
examples. Experiments across six generative tasks demonstrate that our method
consistently outperforms baselines with task-level fusion weights. This
underscores the necessity of introducing dynamic fusion weights for LoRA
combination.
</p>
</div>
</dd>
<dt><a name="item267">[267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11456" title="Abstract">arXiv:2402.11456</a> [<a href="/pdf/2402.11456" title="Download PDF">pdf</a>, <a href="/format/2402.11456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FactPICO: Factuality Evaluation for Plain Language Summarization of  Medical Evidence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Joseph%2C+S+A">Sebastian Antony Joseph</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lily Chen</a>, 
<a href="/search/cs?searchtype=author&query=Trienes%2C+J">Jan Trienes</a>, 
<a href="/search/cs?searchtype=author&query=G%C3%B6ke%2C+H+L">Hannah Louisa G&#xf6;ke</a>, 
<a href="/search/cs?searchtype=author&query=Coers%2C+M">Monika Coers</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wallace%2C+B+C">Byron C Wallace</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J+J">Junyi Jessy Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Plain language summarization with LLMs can be useful for improving textual
accessibility of technical content. But how factual are these summaries in a
high-stakes domain like medicine? This paper presents FactPICO, a factuality
benchmark for plain language summarization of medical texts describing
randomized controlled trials (RCTs), which are the basis of evidence-based
medicine and can directly inform patient treatment. FactPICO consists of 345
plain language summaries of RCT abstracts generated from three LLMs (i.e.,
GPT-4, Llama-2, and Alpaca), with fine-grained evaluation and natural language
rationales from experts. We assess the factuality of critical elements of RCTs
in those summaries: Populations, Interventions, Comparators, Outcomes (PICO),
as well as the reported findings concerning these. We also evaluate the
correctness of the extra information (e.g., explanations) added by LLMs. Using
FactPICO, we benchmark a range of existing factuality metrics, including the
newly devised ones based on LLMs. We find that plain language summarization of
medical evidence is still challenging, especially when balancing between
simplicity and factuality, and that existing metrics correlate poorly with
expert judgments on the instance level.
</p>
</div>
</dd>
<dt><a name="item268">[268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11457" title="Abstract">arXiv:2402.11457</a> [<a href="/pdf/2402.11457" title="Download PDF">pdf</a>, <a href="/format/2402.11457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Do LLMs Need Retrieval Augmentation? Mitigating LLMs&#x27;  Overconfidence Helps Retrieval Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ni%2C+S">Shiyu Ni</a>, 
<a href="/search/cs?searchtype=author&query=Bi%2C+K">Keping Bi</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jiafeng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have been found to have difficulty knowing they
do not possess certain knowledge and tend to provide specious answers in such
cases. Retrieval Augmentation (RA) has been extensively studied to mitigate
LLMs' hallucinations. However, due to the extra overhead and unassured quality
of retrieval, it may not be optimal to conduct RA all the time. A
straightforward idea is to only conduct retrieval when LLMs are uncertain about
a question. This motivates us to enhance the LLMs' ability to perceive their
knowledge boundaries to help RA. In this paper, we first quantitatively measure
LLMs' such ability and confirm their overconfidence. Then, we study how LLMs'
certainty about a question correlates with their dependence on external
retrieved information. We propose several methods to enhance LLMs' perception
of knowledge boundaries and show that they are effective in reducing
overconfidence. Additionally, equipped with these methods, LLMs can achieve
comparable or even better performance of RA with much fewer retrieval calls.
</p>
</div>
</dd>
<dt><a name="item269">[269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11458" title="Abstract">arXiv:2402.11458</a> [<a href="/pdf/2402.11458" title="Download PDF">pdf</a>, <a href="/format/2402.11458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Key Patch Proposer: Key Patches Contain Rich Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jing Xu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+B">Beiwen Tian</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hao Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR 2024 Tiny Papers (notable)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we introduce a novel algorithm named Key Patch Proposer (KPP)
designed to select key patches in an image without additional training. Our
experiments showcase KPP's robust capacity to capture semantic information by
both reconstruction and classification tasks. The efficacy of KPP suggests its
potential application in active learning for semantic segmentation. Our source
code is publicly available at https://github.com/CA-TT-AC/key-patch-proposer.
</p>
</div>
</dd>
<dt><a name="item270">[270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11461" title="Abstract">arXiv:2402.11461</a> [<a href="/pdf/2402.11461" title="Download PDF">pdf</a>, <a href="/format/2402.11461" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FGeo-HyperGNet: Geometry Problem Solving Integrating Formal Symbolic  System and Hypergraph Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaokai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+N">Na Zhu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yiming He</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">Jia Zou</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+C">Cheng Qin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+Z">Zhenbing Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Leng%2C+T">Tuo Leng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Geometry problem solving has always been a long-standing challenge in the
fields of automated reasoning and artificial intelligence. This is the fifth
article in a series of our works, we built a neural-symbolic system to
automatically perform human-like geometric deductive reasoning. The symbolic
part is a formal system built on FormalGeo, which can automatically perform
geomertic relational reasoning and algebraic calculations and organize the
solving process into a solution hypertree with conditions as hypernodes and
theorems as hyperedges. The neural part, called HyperGNet, is a hypergraph
neural network based on the attention mechanism, including a encoder to
effectively encode the structural and semantic information of the hypertree,
and a solver to provide problem-solving guidance. The neural part predicts
theorems according to the hypertree, and the symbolic part applies theorems and
updates the hypertree, thus forming a Predict-Apply Cycle to ultimately achieve
readable and traceable automatic solving of geometric problems. Experiments
demonstrate the correctness and effectiveness of this neural-symbolic
architecture. We achieved a step-wised accuracy of 87.65% and an overall
accuracy of 85.53% on the formalgeo7k datasets. The code and data is available
at https://github.com/BitSecret/HyperGNet.
</p>
</div>
</dd>
<dt><a name="item271">[271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11462" title="Abstract">arXiv:2402.11462</a> [<a href="/pdf/2402.11462" title="Download PDF">pdf</a>, <a href="/ps/2402.11462" title="Download PostScript">ps</a>, <a href="/format/2402.11462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Age of $(k,n)$-Threshold Signature Scheme on a Gossip Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bayram%2C+E">Erkan Bayram</a>, 
<a href="/search/cs?searchtype=author&query=Bastopcu%2C+M">Melih Bastopcu</a>, 
<a href="/search/cs?searchtype=author&query=Belabbas%2C+M">Mohamed-Ali Belabbas</a>, 
<a href="/search/cs?searchtype=author&query=Ba%C5%9Far%2C+T">Tamer Ba&#x15f;ar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Networking and Internet Architecture (cs.NI); Systems and Control (eess.SY)

</div>
<p class="mathjax">We consider information update systems on a gossip network, which consists of
a single source and $n$ receiver nodes. The source encrypts the information
into $n$ distinct keys with version stamps, sending a unique key to each node.
For decryption in a $(k, n)$-Threshold Signature Scheme, each receiver node
requires at least $k+1$ different keys with the same version, shared over
peer-to-peer connections. We consider two different schemes: a memory scheme
(in which the nodes keep the source's current and previous encrypted messages)
and a memoryless scheme (in which the nodes are allowed to only keep the
source's current message). We measure the ''timeliness'' of information updates
by using the version age of information. Our work focuses on determining
closed-form expressions for the time average age of information in a
heterogeneous random graph. Our work not only allows to verify the expected
outcome that a memory scheme results in a lower average age compared to a
memoryless scheme, but also provides the quantitative difference between the
two. In our numerical results, we quantify the value of memory and demonstrate
that the advantages of memory diminish with infrequent source updates, frequent
gossipping between nodes, or a decrease in $k$ for a fixed number of nodes.
</p>
</div>
</dd>
<dt><a name="item272">[272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11463" title="Abstract">arXiv:2402.11463</a> [<a href="/pdf/2402.11463" title="Download PDF">pdf</a>, <a href="/format/2402.11463" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attractor Memory for Long-Term Time Series Forecasting: A Chaos  Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jiaxi Hu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yuehong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+M">Ming Jin</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+S">Shirui Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Q">Qingsong Wen</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yuxuan Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2008.07669">arXiv:2008.07669</a>, <a href="/abs/nlin/0307015">arXiv:nlin/0307015</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Chaotic Dynamics (nlin.CD)

</div>
<p class="mathjax">In long-term time series forecasting (LTSF) tasks, existing deep learning
models overlook the crucial characteristic that discrete time series originate
from underlying continuous dynamic systems, resulting in a lack of
extrapolation and evolution capabilities. Recognizing the chaotic nature of
real-world data, our model, \textbf{\textit{Attraos}}, incorporates chaos
theory into LTSF, perceiving real-world time series as observations from
unknown high-dimensional chaotic dynamic systems. Under the concept of
attractor invariance, Attraos utilizes the proposed multi-scale dynamic memory
unit to memorize historical dynamics structure and predicts by a
frequency-enhanced local evolution strategy. Detailed theoretical analysis and
abundant empirical evidence consistently show that Attraos outperforms various
LTSF methods on mainstream LTSF datasets and chaotic datasets.
</p>
</div>
</dd>
<dt><a name="item273">[273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11465" title="Abstract">arXiv:2402.11465</a> [<a href="/pdf/2402.11465" title="Download PDF">pdf</a>, <a href="/format/2402.11465" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Odd Cycle Transversal on $P_5$-free Graphs in Polynomial Time
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+A">Akanksha Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Lima%2C+P+T">Paloma T. Lima</a>, 
<a href="/search/cs?searchtype=author&query=Lokshtanov%2C+D">Daniel Lokshtanov</a>, 
<a href="/search/cs?searchtype=author&query=Rz%C4%85%C5%BCewski%2C+P">Pawel Rz&#x105;&#x17c;ewski</a>, 
<a href="/search/cs?searchtype=author&query=Saurabh%2C+S">Saket Saurabh</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R">Roohani Sharma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">An independent set in a graph G is a set of pairwise non-adjacent vertices. A
graph $G$ is bipartite if its vertex set can be partitioned into two
independent sets. In the Odd Cycle Transversal problem, the input is a graph
$G$ along with a weight function $w$ associating a rational weight with each
vertex, and the task is to find a smallest weight vertex subset $S$ in $G$ such
that $G - S$ is bipartite; the weight of $S$, $w(S) = \sum_{v\in S} w(v)$. We
show that Odd Cycle Transversal is polynomial-time solvable on graphs excluding
$P_5$ (a path on five vertices) as an induced subgraph. The problem was
previously known to be polynomial-time solvable on $P_4$-free graphs and
NP-hard on $P_6$-free graphs [Dabrowski, Feghali, Johnson, Paesani, Paulusma
and Rz\k{a}\.zewski, Algorithmica 2020]. Bonamy, Dabrowski, Feghali, Johnson
and Paulusma [Algorithmica 2019] posed the existence of a polynomial-time
algorithm on $P_5$-free graphs as an open problem, this was later re-stated by
Rz\k{a}\.zewski [Dagstuhl Reports, 9(6): 2019] and by Chudnovsky, King,
Pilipczuk, Rz\k{a}\.zewski, and Spirkl [SIDMA 2021], who gave an algorithm with
running time $n^{O(\sqrt{n})}$.
</p>
</div>
</dd>
<dt><a name="item274">[274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11467" title="Abstract">arXiv:2402.11467</a> [<a href="/pdf/2402.11467" title="Download PDF">pdf</a>, <a href="/ps/2402.11467" title="Download PostScript">ps</a>, <a href="/format/2402.11467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Decision-Making for Autonomous Vehicles: A Learning-Enhanced  Game-Theoretic Approach in Interactive Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heye Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jinxin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+G">Guanya Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shiyue Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Boqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianqiang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 24 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">This paper proposes an adaptive behavioral decision-making method for
autonomous vehicles (AVs) focusing on complex merging scenarios. Leveraging
principles from non-cooperative game theory, we develop a vehicle interaction
behavior model that defines key traffic elements and integrates a
multifactorial reward function. Maximum entropy inverse reinforcement learning
(IRL) is employed for behavior model parameter optimization. Optimal matching
parameters can be obtained using the interaction behavior feature vector and
the behavior probabilities output by the vehicle interaction model. Further, a
behavioral decision-making method adapted to dynamic environments is proposed.
By establishing a mapping model between multiple environmental variables and
model parameters, it enables parameters online learning and recognition, and
achieves to output interactive behavior probabilities of AVs. Quantitative
analysis employing naturalistic driving datasets (highD and exiD) and
real-vehicle test data validates the model's high consistency with human
decision-making. In 188 tested interaction scenarios, the average human-like
similarity rate is 81.73%, with a notable 83.12% in the highD dataset.
Furthermore, in 145 dynamic interactions, the method matches human decisions at
77.12%, with 6913 consistence instances. Moreover, in real-vehicle tests, a
72.73% similarity with 0% safety violations are obtained. Results demonstrate
the effectiveness of our proposed method in enabling AVs to make informed
adaptive behavior decisions in interactive environments.
</p>
</div>
</dd>
<dt><a name="item275">[275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11468" title="Abstract">arXiv:2402.11468</a> [<a href="/pdf/2402.11468" title="Download PDF">pdf</a>, <a href="/ps/2402.11468" title="Download PostScript">ps</a>, <a href="/format/2402.11468" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Physical Enhanced Residual Learning for Connected Autonomous  Vehicles Platoon Centralized Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hang Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heye Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Haotian Shi</a>, 
<a href="/search/cs?searchtype=author&query=Long%2C+K">Keke Long</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaopeng Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">This paper introduces an online physical enhanced residual learning (PERL)
framework for Connected Autonomous Vehicles (CAVs) platoon, aimed at addressing
the challenges posed by the dynamic and unpredictable nature of traffic
environments. The proposed framework synergistically combines a physical model,
represented by Model Predictive Control (MPC), with data-driven online
Q-learning. The MPC controller, enhanced for centralized CAV platoons, employs
vehicle velocity as a control input and focuses on multi-objective cooperative
optimization. The learning-based residual controller enriches the MPC with
prior knowledge and corrects residuals caused by traffic disturbances. The PERL
framework not only retains the interpretability and transparency of
physics-based models but also significantly improves computational efficiency
and control accuracy in real-world scenarios. The experimental results present
that the online Q-learning PERL controller, in comparison to the MPC controller
and PERL controller with a neural network, exhibits significantly reduced
position and velocity errors. Specifically, the PERL's cumulative absolute
position and velocity errors are, on average, 86.73% and 55.28% lower than the
MPC's, and 12.82% and 18.83% lower than the neural network-based PERL's, in
four tests with different reference trajectories and errors. The results
demonstrate our advanced framework's superior accuracy and quick convergence
capabilities, proving its effectiveness in maintaining platoon stability under
diverse conditions.
</p>
</div>
</dd>
<dt><a name="item276">[276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11469" title="Abstract">arXiv:2402.11469</a> [<a href="/pdf/2402.11469" title="Download PDF">pdf</a>, <a href="/format/2402.11469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Curious Case of Searching for the Correlation between Training Data  and Adversarial Robustness of Transformer Textual Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dang%2C+C">Cuong Dang</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+D+D">Dung D. Le</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+T">Thai Le</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Existing works have shown that fine-tuned textual transformer models achieve
state-of-the-art prediction performances but are also vulnerable to adversarial
text perturbations. Traditional adversarial evaluation is often done
\textit{only after} fine-tuning the models and ignoring the training data. In
this paper, we want to prove that there is also a strong correlation between
training data and model robustness. To this end, we extract 13 different
features representing a wide range of input fine-tuning corpora properties and
use them to predict the adversarial robustness of the fine-tuned models.
Focusing mostly on encoder-only transformer models BERT and RoBERTa with
additional results for BART, ELECTRA and GPT2, we provide diverse evidence to
support our argument. First, empirical analyses show that (a) extracted
features can be used with a lightweight classifier such as Random Forest to
effectively predict the attack success rate and (b) features with the most
influence on the model robustness have a clear correlation with the robustness.
Second, our framework can be used as a fast and effective additional tool for
robustness evaluation since it (a) saves 30x-193x runtime compared to the
traditional technique, (b) is transferable across models, (c) can be used under
adversarial training, and (d) robust to statistical randomness. Our code will
be publicly available.
</p>
</div>
</dd>
<dt><a name="item277">[277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11473" title="Abstract">arXiv:2402.11473</a> [<a href="/pdf/2402.11473" title="Download PDF">pdf</a>, <a href="/format/2402.11473" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jiawei Liang</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+S">Siyuan Liang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Aishan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xiaojun Jia</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+J">Junhao Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xiaochun Cao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 Spotlight
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The proliferation of face forgery techniques has raised significant concerns
within society, thereby motivating the development of face forgery detection
methods. These methods aim to distinguish forged faces from genuine ones and
have proven effective in practical applications. However, this paper introduces
a novel and previously unrecognized threat in face forgery detection scenarios
caused by backdoor attack. By embedding backdoors into models and incorporating
specific trigger patterns into the input, attackers can deceive detectors into
producing erroneous predictions for forged faces. To achieve this goal, this
paper proposes \emph{Poisoned Forgery Face} framework, which enables
clean-label backdoor attacks on face forgery detectors. Our approach involves
constructing a scalable trigger generator and utilizing a novel convolving
process to generate translation-sensitive trigger patterns. Moreover, we employ
a relative embedding method based on landmark-based regions to enhance the
stealthiness of the poisoned samples. Consequently, detectors trained on our
poisoned samples are embedded with backdoors. Notably, our approach surpasses
SoTA backdoor baselines with a significant improvement in attack success rate
(+16.39\% BD-AUC) and reduction in visibility (-12.65\% $L_\infty$).
Furthermore, our attack exhibits promising performance against backdoor
defenses. We anticipate that this paper will draw greater attention to the
potential threats posed by backdoor attacks in face forgery detection
scenarios. Our codes will be made available at
\url{https://github.com/JWLiang007/PFF}
</p>
</div>
</dd>
<dt><a name="item278">[278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11476" title="Abstract">arXiv:2402.11476</a> [<a href="/pdf/2402.11476" title="Download PDF">pdf</a>, <a href="/format/2402.11476" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EndoOOD: Uncertainty-aware Out-of-distribution Detection in Capsule  Endoscopy Diagnosis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+Q">Qiaozhi Tan</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+L">Long Bai</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guankun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+M">Mobarakol Islam</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+H">Hongliang Ren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in IEEE ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Wireless capsule endoscopy (WCE) is a non-invasive diagnostic procedure that
enables visualization of the gastrointestinal (GI) tract. Deep learning-based
methods have shown effectiveness in disease screening using WCE data,
alleviating the burden on healthcare professionals. However, existing capsule
endoscopy classification methods mostly rely on pre-defined categories, making
it challenging to identify and classify out-of-distribution (OOD) data, such as
undefined categories or anatomical landmarks. To address this issue, we propose
the Endoscopy Out-of-Distribution (EndoOOD) framework, which aims to
effectively handle the OOD detection challenge in WCE diagnosis. The proposed
framework focuses on improving the robustness and reliability of WCE diagnostic
capabilities by incorporating uncertainty-aware mixup training and long-tailed
in-distribution (ID) data calibration techniques. Additionally, virtual-logit
matching is employed to accurately distinguish between OOD and ID data while
minimizing information loss. To assess the performance of our proposed
solution, we conduct evaluations and comparisons with 12 state-of-the-art
(SOTA) methods using two publicly available datasets. The results demonstrate
the effectiveness of the proposed framework in enhancing diagnostic accuracy
and supporting clinical decision-making.
</p>
</div>
</dd>
<dt><a name="item279">[279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11477" title="Abstract">arXiv:2402.11477</a> [<a href="/pdf/2402.11477" title="Download PDF">pdf</a>, <a href="/format/2402.11477" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Studying Differential Mental Health Expressions in India
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shelat%2C+K">Khushi Shelat</a>, 
<a href="/search/cs?searchtype=author&query=Rai%2C+S">Sunny Rai</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+D+R">Devansh R Jain</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+Y+M">Young Min Cho</a>, 
<a href="/search/cs?searchtype=author&query=Redkar%2C+M">Maitreyi Redkar</a>, 
<a href="/search/cs?searchtype=author&query=Sawant%2C+S">Samindara Sawant</a>, 
<a href="/search/cs?searchtype=author&query=Guntuku%2C+S+C">Sharath Chandra Guntuku</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">Psychosocial stressors and the symptomatology of mental disorders are known
to vary with socio-cultural environment. Mental health expressions on social
media, however, are primarily informed by studies in the WEIRD (Western,
Educated, Industrial, Rich, and Democratic) contexts. In this paper, we analyze
mental health posts on Reddit made by individuals in India, to identify
variations in online depression language specific to the Indian context
compared to users from the Rest of the World (ROW). Unlike in Western samples,
mental health discussions in India additionally express sadness, use negation,
are present-focused, and are related to work and achievement. {Illness} is
exclusively correlated to India, reaffirming the link between somatic symptoms
and mental disorders in Indian patients. Two clinical psychologists validated
the findings from social media posts and found 95\% of the top-20 topics
associated with mental health discussions as {prevalent} in Indians.
Significant linguistic variations in online mental health-related language in
India compared to ROW, highlight the need for precision culturally-aware mental
health models. These findings have important implications for designing
culturally appropriate interventions to reduce the growing diagnosis and
treatment gap for mental disorders in India.
</p>
</div>
</dd>
<dt><a name="item280">[280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11478" title="Abstract">arXiv:2402.11478</a> [<a href="/pdf/2402.11478" title="Download PDF">pdf</a>, <a href="/format/2402.11478" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Reinforcement Learning for Uplink Centric Broadband  Communication Optimization over Unlicensed Spectrum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhou%2C+H">Hui Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Deng%2C+Y">Yansha Deng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">To provide Uplink Centric Broadband Communication (UCBC), New Radio
Unlicensed (NR-U) network has been standardized to exploit the unlicensed
spectrum using Listen Before Talk (LBT) scheme to fairly coexist with the
incumbent Wireless Fidelity (WiFi) network. Existing access schemes over
unlicensed spectrum are required to perform Clear Channel Assessment (CCA)
before transmissions, where fixed Energy Detection (ED) thresholds are adopted
to identify the channel as idle or busy. However, fixed ED thresholds setting
prevents devices from accessing the channel effectively and efficiently, which
leads to the hidden node (HN) and exposed node (EN) problems. In this paper, we
first develop a centralized double Deep Q-Network (DDQN) algorithm to optimize
the uplink system throughput, where the agent is deployed at the central server
to dynamically adjust the ED thresholds for NR-U and WiFi networks. Considering
that heterogeneous NR-U and WiFi networks, in practice, cannot share the raw
data with the central server directly, we then develop a federated DDQN
algorithm, where two agents are deployed in the NR-U and WiFi networks,
respectively. Our results have shown that the uplink system throughput
increases by over 100%, where cell throughput of NR-U network rises by 150%,
and cell throughput of WiFi network decreases by 30%. To guarantee the cell
throughput of WiFi network, we redesign the reward function to punish the agent
when the cell throughput of WiFi network is below the threshold, and our
revised design can still provide over 50% uplink system throughput gain.
</p>
</div>
</dd>
<dt><a name="item281">[281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11480" title="Abstract">arXiv:2402.11480</a> [<a href="/pdf/2402.11480" title="Download PDF">pdf</a>, <a href="/format/2402.11480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pattern-wise Transparent Sequential Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+K">Kun Ma</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Cong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zeyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">A transparent decision-making process is essential for developing reliable
and trustworthy recommender systems. For sequential recommendation, it means
that the model can identify critical items asthe justifications for its
recommendation results. However, achieving both model transparency and
recommendation performance simultaneously is challenging, especially for models
that take the entire sequence of items as input without screening. In this
paper,we propose an interpretable framework (named PTSR) that enables a
pattern-wise transparent decision-making process. It breaks the sequence of
items into multi-level patterns that serve as atomic units for the entire
recommendation process. The contribution of each pattern to the outcome is
quantified in the probability space. With a carefully designed pattern
weighting correction, the pattern contribution can be learned in the absence of
ground-truth critical patterns. The final recommended items are those items
that most critical patterns strongly endorse. Extensive experiments on four
public datasets demonstrate remarkable recommendation performance, while case
studies validate the model transparency. Our code is available at
https://anonymous.4open.science/r/PTSR-2237.
</p>
</div>
</dd>
<dt><a name="item282">[282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11481" title="Abstract">arXiv:2402.11481</a> [<a href="/pdf/2402.11481" title="Download PDF">pdf</a>, <a href="/format/2402.11481" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DictLLM: Harnessing Key-Value Data Structures with Large Language Models  for Enhanced Medical Diagnostics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">YiQiu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuchen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ya Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanfeng Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Structured data offers a sophisticated mechanism for the organization of
information. Existing methodologies for the text-serialization of structured
data in the context of large language models fail to adequately address the
heterogeneity inherent in key-value structured data. These methods are not
ideal and frequently result in larger input sizes and poor adaptability to
input changes. In this paper, we introduce DictLLM, an innovative framework
designed to improve the modeling of key-value structured data, like medical
laboratory reports, for generating medical diagnoses. DictLLM integrates three
key components: (1) group positional encoding to maintain permutation
invariance, (2) hierarchical attention bias to capture the inherent bias in
structured data, and (3) an optimal transport alignment layer that aligns the
embedding generated by the dictionary encoder with the LLM, thereby producing a
sequence of fixed-length virtual tokens. We carry out experiments using various
LLM models on a comprehensive real-world medical laboratory report dataset for
automatic diagnosis generation, our findings illustrate that DictLLM
significantly outperforms established baseline methods and few-shot GPT-4
implementations in terms of both Rouge-L and Knowledge F1 scores. Furthermore,
our evaluation of the framework's scalability and robustness, through a series
of experiments, underscores its exceptional capability in accurately modeling
the complex key-value data structure of medical dictionary data.
</p>
</div>
</dd>
<dt><a name="item283">[283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11483" title="Abstract">arXiv:2402.11483</a> [<a href="/pdf/2402.11483" title="Download PDF">pdf</a>, <a href="/format/2402.11483" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Fisher Information based Receding Horizon Control Method for Signal  Strength Model Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhu%2C+Y">Yancheng Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Andersson%2C+S+B">Sean B. Andersson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper considers the problem of localizing a set of nodes in a wireless
sensor network when both their positions and the parameters of the
communication model are unknown. We assume that a single agent moves through
the environment, taking measurements of the Received Signal Strength (RSS), and
seek a controller that optimizes a performance metric based on the Fisher
Information Matrix (FIM). We develop a receding horizon (RH) approach that
alternates between estimating the parameter values (using a maximum likelihood
estimator) and determining where to move so as to maximally inform the
estimation problem. The receding horizon controller solves a multi-stage look
ahead problem to determine the next control to be applied, executes the move,
collects the next measurement, and then re-estimates the parameters before
repeating the sequence. We consider both a Dynamic Programming (DP) approach to
solving the optimal control problem at each step, and a simplified heuristic
based on a pruning algorithm that significantly reduces the computational
complexity. We also consider a modified cost function that seeks to balance the
information acquired about each of the parameters to ensure the controller does
not focus on a single value in its optimization. These approaches are compared
against two baselines, one based on a purely random trajectory and one on a
greedy control solution. The simulations indicate our RH schemes outperform the
baselines, while the pruning algorithm produces significant reductions in
computation time with little effect on overall performance.
</p>
</div>
</dd>
<dt><a name="item284">[284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11485" title="Abstract">arXiv:2402.11485</a> [<a href="/pdf/2402.11485" title="Download PDF">pdf</a>, <a href="/format/2402.11485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models  with Entity-based Data Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yamada%2C+I">Ikuya Yamada</a>, 
<a href="/search/cs?searchtype=author&query=Ri%2C+R">Ryokan Ri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Adapting English-based large language models (LLMs) to other languages has
become increasingly popular due to the efficiency and potential of
cross-lingual transfer. However, existing language adaptation methods often
overlook the benefits of cross-lingual supervision. In this study, we introduce
LEIA, a language adaptation tuning method that utilizes Wikipedia entity names
aligned across languages. This method involves augmenting the target language
corpus with English entity names and training the model using left-to-right
language modeling. We assess LEIA on diverse question answering datasets using
7B-parameter LLMs, demonstrating significant performance gains across various
non-English languages. The source code is available at
https://github.com/studio-ousia/leia.
</p>
</div>
</dd>
<dt><a name="item285">[285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11487" title="Abstract">arXiv:2402.11487</a> [<a href="/pdf/2402.11487" title="Download PDF">pdf</a>, <a href="/format/2402.11487" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual Concept-driven Image Generation with Text-to-Image Diffusion  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rahman%2C+T">Tanzila Rahman</a>, 
<a href="/search/cs?searchtype=author&query=Mahajan%2C+S">Shweta Mahajan</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hsin-Ying Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jian Ren</a>, 
<a href="/search/cs?searchtype=author&query=Tulyakov%2C+S">Sergey Tulyakov</a>, 
<a href="/search/cs?searchtype=author&query=Sigal%2C+L">Leonid Sigal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 Figures, 8 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Text-to-image (TTI) diffusion models have demonstrated impressive results in
generating high-resolution images of complex and imaginative scenes. Recent
approaches have further extended these methods with personalization techniques
that allow them to integrate user-illustrated concepts (e.g., the user
him/herself) using a few sample image illustrations. However, the ability to
generate images with multiple interacting concepts, such as human subjects, as
well as concepts that may be entangled in one, or across multiple, image
illustrations remains illusive. In this work, we propose a concept-driven TTI
personalization framework that addresses these core challenges. We build on
existing works that learn custom tokens for user-illustrated concepts, allowing
those to interact with existing text tokens in the TTI model. However,
importantly, to disentangle and better learn the concepts in question, we
jointly learn (latent) segmentation masks that disentangle these concepts in
user-provided image illustrations. We do so by introducing an Expectation
Maximization (EM)-like optimization procedure where we alternate between
learning the custom tokens and estimating masks encompassing corresponding
concepts in user-supplied images. We obtain these masks based on
cross-attention, from within the U-Net parameterized latent diffusion model and
subsequent Dense CRF optimization. We illustrate that such joint alternating
refinement leads to the learning of better tokens for concepts and, as a
bi-product, latent masks. We illustrate the benefits of the proposed approach
qualitatively and quantitatively (through user studies) with a number of
examples and use cases that can combine up to three entangled concepts.
</p>
</div>
</dd>
<dt><a name="item286">[286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11488" title="Abstract">arXiv:2402.11488</a> [<a href="/pdf/2402.11488" title="Download PDF">pdf</a>, <a href="/format/2402.11488" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IRFundusSet: An Integrated Retinal Rundus Dataset with a Harmonized  Healthy Label
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Githinji%2C+P+B">P. Bilha Githinji</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+K">Keming Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiantao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+P">Peiwu Qin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Ocular conditions are a global concern and computational tools utilizing
retinal fundus color photographs can aid in routine screening and management.
Obtaining comprehensive and sufficiently sized datasets, however, is
non-trivial for the intricate retinal fundus, which exhibits heterogeneities
within pathologies, in addition to variations from demographics and
acquisition. Moreover, retinal fundus datasets in the public space suffer
fragmentation in the organization of data and definition of a healthy
observation. We present Integrated Retinal Fundus Set (IRFundusSet), a dataset
that consolidates, harmonizes and curates several public datasets, facilitating
their consumption as a unified whole and with a consistent is_normal label.
IRFundusSet comprises a Python package that automates harmonization and avails
a dataset object in line with the PyTorch approach. Moreover, images are
physically reviewed and a new is_normal label is annotated for a consistent
definition of a healthy observation. Ten public datasets are initially
considered with a total of 46064 images, of which 25406 are curated for a new
is_normal label and 3515 are deemed healthy across the sources.
</p>
</div>
</dd>
<dt><a name="item287">[287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11489" title="Abstract">arXiv:2402.11489</a> [<a href="/pdf/2402.11489" title="Download PDF">pdf</a>, <a href="/format/2402.11489" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What&#x27;s the Plan? Evaluating and Developing Planning-Aware Techniques for  LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hirsch%2C+E">Eran Hirsch</a>, 
<a href="/search/cs?searchtype=author&query=Uziel%2C+G">Guy Uziel</a>, 
<a href="/search/cs?searchtype=author&query=Anaby-Tavor%2C+A">Ateret Anaby-Tavor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages and an appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Planning is a fundamental task in artificial intelligence that involves
finding a sequence of actions that achieve a specified goal in a given
environment. Large language models (LLMs) are increasingly used for
applications that require planning capabilities, such as web or embodied
agents. In line with recent studies, we demonstrate through experimentation
that LLMs lack necessary skills required for planning. Based on these
observations, we advocate for the potential of a hybrid approach that combines
LLMs with classical planning methodology. Then, we introduce SimPlan, a novel
hybrid-method, and evaluate its performance in a new challenging setup. Our
extensive experiments across various planning domains demonstrate that SimPlan
significantly outperforms existing LLM-based planners.
</p>
</div>
</dd>
<dt><a name="item288">[288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11490" title="Abstract">arXiv:2402.11490</a> [<a href="/pdf/2402.11490" title="Download PDF">pdf</a>, <a href="/format/2402.11490" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Research status of the Mendeleev Periodic Table: a bibliometric analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+K">Kamna Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+D+K">Deepak Kumar Das</a>, 
<a href="/search/cs?searchtype=author&query=Ray%2C+S">Saibal Ray</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
<p class="mathjax">In this paper, we present a bibliometric analysis of the Mendeleev Periodic
Table. We have conducted a comprehensive analysis of the Scopus-based database
using the keyword "Mendeleev Periodic Table". Our findings suggest that the
Mendeleev Periodic Table is an influential topic in the field of Inorganic as
well as Organic Chemistry. Future researchers may focus on expanding our
analysis to include other bibliometric indicators to gain a more comprehensive
understanding of the impact of the Mendeleev Periodic Table in chemistry-based
scientific investigations and even in the field of astrochemistry.
</p>
</div>
</dd>
<dt><a name="item289">[289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11492" title="Abstract">arXiv:2402.11492</a> [<a href="/pdf/2402.11492" title="Download PDF">pdf</a>, <a href="/format/2402.11492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exponential Cluster Synchronization in Fast Switching Network  Topologies: A Pinning Control Approach with Necessary and Sufficient  Conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Du%2C+K">Ku Du</a>, 
<a href="/search/eess?searchtype=author&query=Kang%2C+Y">Yu Kang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This research investigates the intricate domain of synchronization problem
among multiple agents operating within a dynamic fast switching network
topology. We concentrate on cluster synchronization within coupled linear
system under pinning control, providing both necessary and sufficient
conditions. As a pivotal aspect, this paper aim to president the weakest
possible conditions to make the coupled linear system realize cluster
synchronization exponentially. Within the context of fast switching framework,
we initially examine the necessary conditions, commencing with the
transformation of the consensus problem into a stability problem, introducing a
new variable to make the coupled system achieve cluster synchronization if the
system is controllable; communication topology switching fast enough and the
coupling strength should be sufficiently robust. Then, by using the Lyapunov
theorem, we also present that the state matrix controllable is necessary for
cluster synchronization. Furthermore, this paper culminating in the
incorporation of contraction theory and an invariant manifold, demonstrating
that the switching topology has an average is imperative for achieving cluster
synchronization. Finally, we introduce three simulations to validate the
efficacy of the proposed approach.
</p>
</div>
</dd>
<dt><a name="item290">[290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11493" title="Abstract">arXiv:2402.11493</a> [<a href="/pdf/2402.11493" title="Download PDF">pdf</a>, <a href="/format/2402.11493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Benchmarking Knowledge Boundary for Large Language Model: A Different  Perspective on Model Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+X">Xunjian Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ruan%2C+J">Jie Ruan</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiaojun Wan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In recent years, substantial advancements have been made in the development
of large language models, achieving remarkable performance across diverse
tasks. To evaluate the knowledge ability of language models, previous studies
have proposed lots of benchmarks based on question-answering pairs. We argue
that it is not reliable and comprehensive to evaluate language models with a
fixed question or limited paraphrases as the query, since language models are
sensitive to prompt. Therefore, we introduce a novel concept named knowledge
boundary to encompass both prompt-agnostic and prompt-sensitive knowledge
within language models. Knowledge boundary avoids prompt sensitivity in
language model evaluations, rendering them more dependable and robust. To
explore the knowledge boundary for a given model, we propose projected gradient
descent method with semantic constraints, a new algorithm designed to identify
the optimal prompt for each piece of knowledge. Experiments demonstrate a
superior performance of our algorithm in computing the knowledge boundary
compared to existing methods. Furthermore, we evaluate the ability of multiple
language models in several domains with knowledge boundary.
</p>
</div>
</dd>
<dt><a name="item291">[291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11494" title="Abstract">arXiv:2402.11494</a> [<a href="/pdf/2402.11494" title="Download PDF">pdf</a>, <a href="/format/2402.11494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Out-of-Distribution Generalization via Causal Intervention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qitian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+F">Fan Nie</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chenxiao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Bao%2C+T">Tianyi Bao</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the research paper track of The Web Conference (WWW) 2024. The codes are available at <a href="https://github.com/fannie1208/CaNet">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Out-of-distribution (OOD) generalization has gained increasing attentions for
learning on graphs, as graph neural networks (GNNs) often exhibit performance
degradation with distribution shifts. The challenge is that distribution shifts
on graphs involve intricate interconnections between nodes, and the environment
labels are often absent in data. In this paper, we adopt a bottom-up
data-generative perspective and reveal a key observation through causal
analysis: the crux of GNNs' failure in OOD generalization lies in the latent
confounding bias from the environment. The latter misguides the model to
leverage environment-sensitive correlations between ego-graph features and
target nodes' labels, resulting in undesirable generalization on new unseen
nodes. Built upon this analysis, we introduce a conceptually simple yet
principled approach for training robust GNNs under node-level distribution
shifts, without prior knowledge of environment labels. Our method resorts to a
new learning objective derived from causal inference that coordinates an
environment estimator and a mixture-of-expert GNN predictor. The new approach
can counteract the confounding bias in training data and facilitate learning
generalizable predictive relations. Extensive experiment demonstrates that our
model can effectively enhance generalization with various types of distribution
shifts and yield up to 27.4\% accuracy improvement over state-of-the-arts on
graph OOD generalization benchmarks. Source codes are available at
https://github.com/fannie1208/CaNet.
</p>
</div>
</dd>
<dt><a name="item292">[292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11495" title="Abstract">arXiv:2402.11495</a> [<a href="/pdf/2402.11495" title="Download PDF">pdf</a>, <a href="/format/2402.11495" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> URLBERT:A Contrastive and Adversarial Pre-trained Model for URL  Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yujie Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanbin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Haitao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhenhao Guo</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zheng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">URLs play a crucial role in understanding and categorizing web content,
particularly in tasks related to security control and online recommendations.
While pre-trained models are currently dominating various fields, the domain of
URL analysis still lacks specialized pre-trained models. To address this gap,
this paper introduces URLBERT, the first pre-trained representation learning
model applied to a variety of URL classification or detection tasks. We first
train a URL tokenizer on a corpus of billions of URLs to address URL data
tokenization. Additionally, we propose two novel pre-training tasks: (1)
self-supervised contrastive learning tasks, which strengthen the model's
understanding of URL structure and the capture of category differences by
distinguishing different variants of the same URL; (2) virtual adversarial
training, aimed at improving the model's robustness in extracting semantic
features from URLs. Finally, our proposed methods are evaluated on tasks
including phishing URL detection, web page classification, and ad filtering,
achieving state-of-the-art performance. Importantly, we also explore multi-task
learning with URLBERT, and experimental results demonstrate that multi-task
learning model based on URLBERT exhibit equivalent effectiveness compared to
independently fine-tuned models, showing the simplicity of URLBERT in handling
complex task requirements. The code for our work is available at
https://github.com/Davidup1/URLBERT.
</p>
</div>
</dd>
<dt><a name="item293">[293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11496" title="Abstract">arXiv:2402.11496</a> [<a href="/pdf/2402.11496" title="Download PDF">pdf</a>, <a href="/format/2402.11496" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Point-Wise Vibration Pattern Production via a Sparse Actuator Array for  Surface Tactile Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaosa Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Runze Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C">Chengyue Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+X">Xiao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+W">Wenbo Ding</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Surface vibration tactile feedback is capable of conveying various semantic
information to humans via the handheld electronic devices, like smartphone,
touch panel,and game controller. However, covering the whole device contacting
surface with dense actuator arrangement can affect its normal use, how to
produce desired vibration patterns at any contact point with only several
sparse actuators deployed on the handled device surface remains a significant
challenge. In this work, we develop a tactile feedback board with only five
actuators in the size of a smartphone, and achieve the precise vibration
pattern production that can focus at any desired position all over the board.
Specifically, we investigate the vibration characteristics of single passive
coil actuator, and construct its vibration pattern model at any position on the
feedback board surface. Optimal phase and amplitude modulation, found with the
simulated annealing algorithm, is employed with five actuators in a sparse
array. And all actuators' vibration patterns are superimposed linearly to
synthetically generate different onboard vibration energy distribution for
tactile sensing. Experiments demonstrated that for point-wise vibration pattern
production on our tactile board achieved an average level of about 0.9 in the
Structural Similarity Index Measure (SSIM) evaluation, when compared to the
ideal single-point-focused target vibration pattern. The sparse actuator array
can be easily embedded into usual handheld electronic devices, which shows a
good significant implication for enriching their haptic interaction
functionalities.
</p>
</div>
</dd>
<dt><a name="item294">[294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11497" title="Abstract">arXiv:2402.11497</a> [<a href="/pdf/2402.11497" title="Download PDF">pdf</a>, <a href="/format/2402.11497" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thyroid ultrasound diagnosis improvement via multi-view self-supervised  learning and two-stage pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jian Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+X">Xiaohong Jia</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+W">Wufeng Xue</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Rusi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yanlin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiliang Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jianqiao Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+D">Dong Ni</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+N">Ning Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The article has been accepted by the journal of Computers in Biology and Medicine
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Thyroid nodule classification and segmentation in ultrasound images are
crucial for computer-aided diagnosis; however, they face limitations owing to
insufficient labeled data. In this study, we proposed a multi-view contrastive
self-supervised method to improve thyroid nodule classification and
segmentation performance with limited manual labels. Our method aligns the
transverse and longitudinal views of the same nodule, thereby enabling the
model to focus more on the nodule area. We designed an adaptive loss function
that eliminates the limitations of the paired data. Additionally, we adopted a
two-stage pre-training to exploit the pre-training on ImageNet and thyroid
ultrasound images. Extensive experiments were conducted on a large-scale
dataset collected from multiple centers. The results showed that the proposed
method significantly improves nodule classification and segmentation
performance with limited manual labels and outperforms state-of-the-art
self-supervised methods. The two-stage pre-training also significantly exceeded
ImageNet pre-training.
</p>
</div>
</dd>
<dt><a name="item295">[295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11498" title="Abstract">arXiv:2402.11498</a> [<a href="/pdf/2402.11498" title="Download PDF">pdf</a>, <a href="/format/2402.11498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Verifiably Following Complex Robot Instructions with Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Quartey%2C+B">Benedict Quartey</a>, 
<a href="/search/cs?searchtype=author&query=Rosen%2C+E">Eric Rosen</a>, 
<a href="/search/cs?searchtype=author&query=Tellex%2C+S">Stefanie Tellex</a>, 
<a href="/search/cs?searchtype=author&query=Konidaris%2C+G">George Konidaris</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Enabling robots to follow complex natural language instructions is an
important yet challenging problem. People want to flexibly express constraints,
refer to arbitrary landmarks and verify behavior when instructing robots.
Conversely, robots must disambiguate human instructions into specifications and
ground instruction referents in the real world. We propose Language Instruction
grounding for Motion Planning (LIMP), a system that leverages foundation models
and temporal logics to generate instruction-conditioned semantic maps that
enable robots to verifiably follow expressive and long-horizon instructions
with open vocabulary referents and complex spatiotemporal constraints. In
contrast to prior methods for using foundation models in robot task execution,
LIMP constructs an explainable instruction representation that reveals the
robot's alignment with an instructor's intended motives and affords the
synthesis of robot behaviors that are correct-by-construction. We demonstrate
LIMP in three real-world environments, across a set of 35 complex
spatiotemporal instructions, showing the generality of our approach and the
ease of deployment in novel unstructured domains. In our experiments, LIMP can
spatially ground open-vocabulary referents and synthesize constraint-satisfying
plans in 90% of object-goal navigation and 71% of mobile manipulation
instructions. See supplementary videos at https://robotlimp.github.io
</p>
</div>
</dd>
<dt><a name="item296">[296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11499" title="Abstract">arXiv:2402.11499</a> [<a href="/pdf/2402.11499" title="Download PDF">pdf</a>, <a href="/format/2402.11499" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Acousto-electric tomography by the convergence of Kaczamrz two-point  gradient-$&#x398;$ method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhu%2C+K">Kai Zhu</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+J">Jijun Liu</a>, 
<a href="/search/math?searchtype=author&query=Zhong%2C+M">Min Zhong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We study the numerical reconstruction problem in acousto-electric tomography
(AET) of recovering the conductivity distribution in a bounded domain from
multiple interior power density data. The Two-Point-Gradient-$\Theta$
(TPG-$\Theta$) in Kaczmarz type is proposed, with a general convex penalty term
$\Theta$, the algorithm can be utilized in AET problem for recovering sparse
and discontinuous conductivity distributions. We establish the convergence of
such iterative regularized method. Extensive numerical experiments are
presented to illustrate the feasibility and effectiveness of the proposed
approach.
</p>
</div>
</dd>
<dt><a name="item297">[297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11500" title="Abstract">arXiv:2402.11500</a> [<a href="/pdf/2402.11500" title="Download PDF">pdf</a>, <a href="/format/2402.11500" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Three-Party Repeated Coalition Formation Game for PLS in Wireless  Communications with IRSs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Haipeng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ruoyang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+C">Changyan Yi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juan Li</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+J">Jun Cai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to IEEE WCNC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">In this paper, a repeated coalition formation game (RCFG) with dynamic
decision-making for physical layer security (PLS) in wireless communications
with intelligent reflecting surfaces (IRSs) has been investigated. In the
considered system, one central legitimate transmitter (LT) aims to transmit
secret signals to a group of legitimate receivers (LRs) under the threat of a
proactive eavesdropper (EV), while there exist a number of third-party IRSs
(TIRSs) which can choose to form a coalition with either legitimate pairs (LPs)
or the EV to improve their respective performances in exchange for potential
benefits (e.g., payments). Unlike existing works that commonly restricted to
friendly IRSs or malicious IRSs only, we study the complicated dynamic
ally-adversary relationships among LPs, EV and TIRSs, under unpredictable
wireless channel conditions, and introduce a RCFG to model their long-term
strategic interactions. Particularly, we first analyze the existence of Nash
equilibrium (NE) in the formulated RCFG, and then propose a switch
operations-based coalition selection along with a deep reinforcement learning
(DRL)-based algorithm for obtaining such equilibrium. Simulations examine the
feasibility of the proposed algorithm and show its superiority over
counterparts.
</p>
</div>
</dd>
<dt><a name="item298">[298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11502" title="Abstract">arXiv:2402.11502</a> [<a href="/pdf/2402.11502" title="Download PDF">pdf</a>, <a href="/format/2402.11502" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GenAD: Generative End-to-End Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+W">Wenzhao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+R">Ruiqi Song</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xianda Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Long Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Directly producing planning results from raw sensors has been a long-desired
solution for autonomous driving and has attracted increasing attention
recently. Most existing end-to-end autonomous driving methods factorize this
problem into perception, motion prediction, and planning. However, we argue
that the conventional progressive pipeline still cannot comprehensively model
the entire traffic evolution process, e.g., the future interaction between the
ego car and other traffic participants and the structural trajectory prior. In
this paper, we explore a new paradigm for end-to-end autonomous driving, where
the key is to predict how the ego car and the surroundings evolve given past
scenes. We propose GenAD, a generative framework that casts autonomous driving
into a generative modeling problem. We propose an instance-centric scene
tokenizer that first transforms the surrounding scenes into map-aware instance
tokens. We then employ a variational autoencoder to learn the future trajectory
distribution in a structural latent space for trajectory prior modeling. We
further adopt a temporal model to capture the agent and ego movements in the
latent space to generate more effective future trajectories. GenAD finally
simultaneously performs motion prediction and planning by sampling
distributions in the learned structural latent space conditioned on the
instance tokens and using the learned temporal model to generate futures.
Extensive experiments on the widely used nuScenes benchmark show that the
proposed GenAD achieves state-of-the-art performance on vision-centric
end-to-end autonomous driving with high efficiency.
</p>
</div>
</dd>
<dt><a name="item299">[299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11504" title="Abstract">arXiv:2402.11504</a> [<a href="/pdf/2402.11504" title="Download PDF">pdf</a>, <a href="/ps/2402.11504" title="Download PostScript">ps</a>, <a href="/format/2402.11504" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> To use or not to use proprietary street view images in (health and  place) research? That is the question
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Helbich%2C+M">Marco Helbich</a>, 
<a href="/search/cs?searchtype=author&query=Danish%2C+M">Matthew Danish</a>, 
<a href="/search/cs?searchtype=author&query=Labib%2C+S">SM Labib</a>, 
<a href="/search/cs?searchtype=author&query=Ricker%2C+B">Britta Ricker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Applications (stat.AP)

</div>
<p class="mathjax">Computer vision-based analysis of street view imagery has transformative
impacts on environmental assessments. Interactive web services, particularly
Google Street View, play an ever-important role in making imagery data
ubiquitous. Despite the technical ease of harnessing millions of Google Street
View images, this article questions the current practices in using this
proprietary data source. Our concern lies with Google's terms of service, which
prohibit bulk image downloads and the generation of street view image-based
indices. To reconcile the challenge of advancing society through groundbreaking
research while maintaining data license agreements and legal integrity, it is
crucial to adhere to open data principles and utilize open image sources for
future research.
</p>
</div>
</dd>
<dt><a name="item300">[300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11505" title="Abstract">arXiv:2402.11505</a> [<a href="/pdf/2402.11505" title="Download PDF">pdf</a>, <a href="/format/2402.11505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Fine-tuning of Large Language Models under Heterogeneous  Language Tasks and Client Resources
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bai%2C+J">Jiamu Bai</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Daoyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+B">Bingchen Qian</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+L">Liuyi Yao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yaliang Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 8 figures, 8 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Federated Learning (FL) has recently been applied to the parameter-efficient
fine-tuning of Large Language Models (LLMs). While promising, it raises
significant challenges due to the heterogeneous resources and data
distributions of clients.This study introduces FlexLoRA, a simple yet effective
aggregation scheme for LLM fine-tuning, which mitigates the "buckets effect" in
traditional FL that restricts the potential of clients with ample resources by
tying them to the capabilities of the least-resourced participants. FlexLoRA
allows for dynamic adjustment of local LoRA ranks, fostering the development of
a global model imbued with broader, less task-specific knowledge. By
synthesizing a full-size LoRA weight from individual client contributions and
employing Singular Value Decomposition (SVD) for weight redistribution,
FlexLoRA fully leverages heterogeneous client resources. Involving over 1,600
clients performing diverse NLP tasks, our experiments validate the efficacy of
FlexLoRA, with the federated global model achieving up to a 3.1% average
improvement in downstream NLP task performance. FlexLoRA's practicality is
further underscored by its seamless integration with existing LoRA-based FL
methods and theoretical analysis, offering a path toward scalable,
privacy-preserving federated tuning for LLMs.
</p>
</div>
</dd>
<dt><a name="item301">[301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11507" title="Abstract">arXiv:2402.11507</a> [<a href="/pdf/2402.11507" title="Download PDF">pdf</a>, <a href="/format/2402.11507" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MAL: Motion-Aware Loss with Temporal and Distillation Hints for  Self-Supervised Depth Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yup-Jiang Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fang-Lue Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Song-Hai Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICRA 2024; Project homepage: <a href="https://yuejiangdong.github.io/MotionAwareLoss/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Depth perception is crucial for a wide range of robotic applications.
Multi-frame self-supervised depth estimation methods have gained research
interest due to their ability to leverage large-scale, unlabeled real-world
data. However, the self-supervised methods often rely on the assumption of a
static scene and their performance tends to degrade in dynamic environments. To
address this issue, we present Motion-Aware Loss, which leverages the temporal
relation among consecutive input frames and a novel distillation scheme between
the teacher and student networks in the multi-frame self-supervised depth
estimation methods. Specifically, we associate the spatial locations of moving
objects with the temporal order of input frames to eliminate errors induced by
object motion. Meanwhile, we enhance the original distillation scheme in
multi-frame methods to better exploit the knowledge from a teacher network. MAL
is a novel, plug-and-play module designed for seamless integration into
multi-frame self-supervised monocular depth estimation methods. Adding MAL into
previous state-of-the-art methods leads to a reduction in depth estimation
errors by up to 4.2% and 10.8% on KITTI and CityScapes benchmarks,
respectively.
</p>
</div>
</dd>
<dt><a name="item302">[302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11512" title="Abstract">arXiv:2402.11512</a> [<a href="/pdf/2402.11512" title="Download PDF">pdf</a>, <a href="/format/2402.11512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Prejudice to Parity: A New Approach to Debiasing Large Language  Model Word Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rakshit%2C+A">Aishik Rakshit</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Smriti Singh</a>, 
<a href="/search/cs?searchtype=author&query=Keshari%2C+S">Shuvam Keshari</a>, 
<a href="/search/cs?searchtype=author&query=Chowdhury%2C+A+G">Arijit Ghosh Chowdhury</a>, 
<a href="/search/cs?searchtype=author&query=Jain%2C+V">Vinija Jain</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">Embeddings play a pivotal role in the efficacy of Large Language Models. They
are the bedrock on which these models grasp contextual relationships and foster
a more nuanced understanding of language and consequently perform remarkably on
a plethora of complex tasks that require a fundamental understanding of human
language. Given that these embeddings themselves often reflect or exhibit bias,
it stands to reason that these models may also inadvertently learn this bias.
In this work, we build on the seminal previous work and propose DeepSoftDebias,
an algorithm that uses a neural network to perform `soft debiasing'. We
exhaustively evaluate this algorithm across a variety of SOTA datasets,
accuracy metrics, and challenging NLP tasks. We find that DeepSoftDebias
outperforms the current state-of-the-art methods at reducing bias across
gender, race, and religion.
</p>
</div>
</dd>
<dt><a name="item303">[303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11515" title="Abstract">arXiv:2402.11515</a> [<a href="/pdf/2402.11515" title="Download PDF">pdf</a>, <a href="/format/2402.11515" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Parallelization Strategies for Active Flow Control in Deep  Reinforcement Learning-Based Computational Fluid Dynamics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+W">Wang Jia</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hang Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">Deep Reinforcement Learning (DRL) has emerged as a promising approach for
handling highly dynamic and nonlinear Active Flow Control (AFC) problems.
However, the computational cost associated with training DRL models presents a
significant performance bottleneck. To address this challenge and enable
efficient scaling on high-performance computing architectures, this study
focuses on optimizing DRL-based algorithms in parallel settings. We validate an
existing state-of-the-art DRL framework used for AFC problems and discuss its
efficiency bottlenecks. Subsequently, by deconstructing the overall framework
and conducting extensive scalability benchmarks for individual components, we
investigate various hybrid parallelization configurations and propose efficient
parallelization strategies. Moreover, we refine input/output (I/O) operations
in multi-environment DRL training to tackle critical overhead associated with
data movement. Finally, we demonstrate the optimized framework for a typical
AFC problem where near-linear scaling can be obtained for the overall
framework. We achieve a significant boost in parallel efficiency from around
49% to approximately 78%, and the training process is accelerated by
approximately 47 times using 60 CPU cores. These findings are expected to
provide valuable insights for further advancements in DRL-based AFC studies.
</p>
</div>
</dd>
<dt><a name="item304">[304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11517" title="Abstract">arXiv:2402.11517</a> [<a href="/pdf/2402.11517" title="Download PDF">pdf</a>, <a href="/format/2402.11517" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+Z">Zijin Hong</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zheng Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qinggang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Feiran Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiao Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Generating accurate SQL for user queries (text-to-SQL) is a long-standing
problem since the generation of the SQL requires comprehending the query and
database and retrivale the accurate data from the database accordingly.
Existing models rely on the comprehensive ability of Large Language Models
(LLMs) to generate the SQL according to the database schema. However, there is
some necessary knowledge that is not explicitly included in the database schema
or has been learned by LLMs. Thus, the generated SQL of the
knowledge-insufficient queries may be inaccurate, which negatively impacts the
robustness of the text-to-SQL models. To deal with this situation, we propose
the Knowledge-to-SQL framework, which employs tailored Data Expert LLM (DELLM)
to provide helpful knowledge for all types of text-to-SQL models. Specifically,
we provide the detailed design of DELLM, in terms of table reading, and the
basic fine-tuning process. We further provide a Reinforcement Learning via
Database Feedback (RLDBF) training strategy to guide the DELLM to generate more
helpful knowledge for LLMs. Extensive experiments verify DELLM can enhance the
state-of-the-art LLMs on text-to-SQL tasks. The model structure and the
parameter weight of DELLM are released for further research.
</p>
</div>
</dd>
<dt><a name="item305">[305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11518" title="Abstract">arXiv:2402.11518</a> [<a href="/pdf/2402.11518" title="Download PDF">pdf</a>, <a href="/format/2402.11518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model-driven Meta-structure Discovery in Heterogeneous  Information Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+F">Fengli Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+N">Nian Li</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zhenyu Han</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Meng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong Li</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+P">Pan Hui</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Heterogeneous information networks (HIN) have gained increasing popularity
for being able to capture complex relations between nodes of diverse types.
Meta-structure was proposed to identify important patterns of relations on HIN,
which has been proven effective for extracting rich semantic information and
facilitating graph neural networks to learn expressive representations.
However, hand-crafted meta-structures pose challenges for scaling up, which
draws wide research attention for developing automatic meta-structure search
algorithms. Previous efforts concentrate on searching for meta-structures with
good empirical prediction performance, overlooking explainability. Thus, they
often produce meta-structures prone to overfitting and incomprehensible to
humans. To address this, we draw inspiration from the emergent reasoning
abilities of large language models (LLMs). We propose a novel REasoning
meta-STRUCTure search (ReStruct) framework that integrates LLM reasoning into
the evolutionary procedure. ReStruct uses a grammar translator to encode
meta-structures into natural language sentences, and leverages the reasoning
power of LLMs to evaluate semantically feasible meta-structures. ReStruct also
employs performance-oriented evolutionary operations. These two competing
forces jointly optimize for semantic explainability and empirical performance
of meta-structures. We also design a differential LLM explainer that can
produce natural language explanations for the discovered meta-structures, and
refine the explanation by reasoning through the search history. Experiments on
five datasets demonstrate ReStruct achieve SOTA performance in node
classification and link recommendation tasks. Additionally, a survey study
involving 73 graduate students shows that the meta-structures and natural
language explanations generated by ReStruct are substantially more
comprehensible.
</p>
</div>
</dd>
<dt><a name="item306">[306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11520" title="Abstract">arXiv:2402.11520</a> [<a href="/pdf/2402.11520" title="Download PDF">pdf</a>, <a href="/format/2402.11520" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cross-Attention Fusion of Visual and Geometric Features for Large  Vocabulary Arabic Lipreading
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Daou%2C+S">Samar Daou</a>, 
<a href="/search/cs?searchtype=author&query=Rekik%2C+A">Ahmed Rekik</a>, 
<a href="/search/cs?searchtype=author&query=Ben-Hamadou%2C+A">Achraf Ben-Hamadou</a>, 
<a href="/search/cs?searchtype=author&query=Kallel%2C+A">Abdelaziz Kallel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review, 24 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Lipreading involves using visual data to recognize spoken words by analyzing
the movements of the lips and surrounding area. It is a hot research topic with
many potential applications, such as human-machine interaction and enhancing
audio speech recognition. Recent deep-learning based works aim to integrate
visual features extracted from the mouth region with landmark points on the lip
contours. However, employing a simple combination method such as concatenation
may not be the most effective approach to get the optimal feature vector. To
address this challenge, firstly, we propose a cross-attention fusion-based
approach for large lexicon Arabic vocabulary to predict spoken words in videos.
Our method leverages the power of cross-attention networks to efficiently
integrate visual and geometric features computed on the mouth region. Secondly,
we introduce the first large-scale Lip Reading in the Wild for Arabic (LRW-AR)
dataset containing 20,000 videos for 100-word classes, uttered by 36 speakers.
The experimental results obtained on LRW-AR and ArabicVisual databases showed
the effectiveness and robustness of the proposed approach in recognizing Arabic
words. Our work provides insights into the feasibility and effectiveness of
applying lipreading techniques to the Arabic language, opening doors for
further research in this field. Link to the project page:
https://crns-smartvision.github.io/lrwar
</p>
</div>
</dd>
<dt><a name="item307">[307]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11522" title="Abstract">arXiv:2402.11522</a> [<a href="/pdf/2402.11522" title="Download PDF">pdf</a>, <a href="/format/2402.11522" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling the Secrets of Engaging Conversations: Factors that Keep Users  Hooked on Role-Playing Dialog Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Junwen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jia Yu</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+H">Huachuan Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yuming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Z">Zhenzhong Lan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">With the growing humanlike nature of dialog agents, people are now engaging
in extended conversations that can stretch from brief moments to substantial
periods of time. Understanding the factors that contribute to sustaining these
interactions is crucial, yet existing studies primarily focusing on short-term
simulations that rarely explore such prolonged and real conversations.
<br />In this paper, we investigate the factors influencing retention rates in real
interactions with roleplaying models. By analyzing a large dataset of
interactions between real users and thousands of characters, we systematically
examine multiple factors and assess their impact on user retention rate.
Surprisingly, we find that the degree to which the bot embodies the roles it
plays has limited influence on retention rates, while the length of each turn
it speaks significantly affects retention rates. This study sheds light on the
critical aspects of user engagement with role-playing models and provides
valuable insights for future improvements in the development of large language
models for role-playing purposes.
</p>
</div>
</dd>
<dt><a name="item308">[308]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11523" title="Abstract">arXiv:2402.11523</a> [<a href="/pdf/2402.11523" title="Download PDF">pdf</a>, <a href="/format/2402.11523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neighborhood-Enhanced Supervised Contrastive Learning for Collaborative  Filtering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+P">Peijie Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Le Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiangzhi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Meng Wang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE TKDE, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">While effective in recommendation tasks, collaborative filtering (CF)
techniques face the challenge of data sparsity. Researchers have begun
leveraging contrastive learning to introduce additional self-supervised signals
to address this. However, this approach often unintentionally distances the
target user/item from their collaborative neighbors, limiting its efficacy. In
response, we propose a solution that treats the collaborative neighbors of the
anchor node as positive samples within the final objective loss function. This
paper focuses on developing two unique supervised contrastive loss functions
that effectively combine supervision signals with contrastive loss. We analyze
our proposed loss functions through the gradient lens, demonstrating that
different positive samples simultaneously influence updating the anchor node's
embeddings. These samples' impact depends on their similarities to the anchor
node and the negative samples. Using the graph-based collaborative filtering
model as our backbone and following the same data augmentation methods as the
existing contrastive learning model SGL, we effectively enhance the performance
of the recommendation model. Our proposed Neighborhood-Enhanced Supervised
Contrastive Loss (NESCL) model substitutes the contrastive loss function in SGL
with our novel loss function, showing marked performance improvement. On three
real-world datasets, Yelp2018, Gowalla, and Amazon-Book, our model surpasses
the original SGL by 10.09%, 7.09%, and 35.36% on NDCG@20, respectively.
</p>
</div>
</dd>
<dt><a name="item309">[309]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11525" title="Abstract">arXiv:2402.11525</a> [<a href="/pdf/2402.11525" title="Download PDF">pdf</a>, <a href="/format/2402.11525" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Advancing Translation Preference Modeling with RLHF: A Step Towards  Cost-Effective Solution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+N">Nuo Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zu%2C+C">Can Zu</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Faithfulness, expressiveness, and elegance is the constant pursuit in machine
translation. However, traditional metrics like \textit{BLEU} do not strictly
align with human preference of translation quality. In this paper, we explore
leveraging reinforcement learning with human feedback (\textit{RLHF}) to
improve translation quality. It is non-trivial to collect a large high-quality
dataset of human comparisons between translations, especially for low-resource
languages. To address this issue, we propose a cost-effective preference
learning strategy, optimizing reward models by distinguishing between human and
machine translations. In this manner, the reward model learns the deficiencies
of machine translation compared to human and guides subsequent improvements in
machine translation. Experimental results demonstrate that \textit{RLHF} can
effectively enhance translation quality and this improvement benefits other
translation directions not trained with \textit{RLHF}. Further analysis
indicates that the model's language capabilities play a crucial role in
preference learning. A reward model with strong language capabilities can more
sensitively learn the subtle differences in translation quality and align
better with real human translation preferences.
</p>
</div>
</dd>
<dt><a name="item310">[310]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11526" title="Abstract">arXiv:2402.11526</a> [<a href="/pdf/2402.11526" title="Download PDF">pdf</a>, <a href="/format/2402.11526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring Privacy Loss in Distributed Spatio-Temporal Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koga%2C+T">Tatsuki Koga</a>, 
<a href="/search/cs?searchtype=author&query=Meehan%2C+C">Casey Meehan</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhuri%2C+K">Kamalika Chaudhuri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Chrome PDF viewer might not display Figures 3 and 4 properly
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Statistics about traffic flow and people's movement gathered from multiple
geographical locations in a distributed manner are the driving force powering
many applications, such as traffic prediction, demand prediction, and
restaurant occupancy reports. However, these statistics are often based on
sensitive location data of people, and hence privacy has to be preserved while
releasing them. The standard way to do this is via differential privacy, which
guarantees a form of rigorous, worst-case, person-level privacy. In this work,
motivated by several counter-intuitive features of differential privacy in
distributed location applications, we propose an alternative privacy loss
against location reconstruction attacks by an informed adversary. Our
experiments on real and synthetic data demonstrate that our privacy loss better
reflects our intuitions on individual privacy violation in the distributed
spatio-temporal setting.
</p>
</div>
</dd>
<dt><a name="item311">[311]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11528" title="Abstract">arXiv:2402.11528</a> [<a href="/pdf/2402.11528" title="Download PDF">pdf</a>, <a href="/ps/2402.11528" title="Download PostScript">ps</a>, <a href="/format/2402.11528" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Signed-Perturbed Sums Estimation of ARX Systems: Exact Coverage and  Strong Consistency (Extended Version)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Car%C3%A8%2C+A">Algo Car&#xe8;</a>, 
<a href="/search/eess?searchtype=author&query=Weyer%2C+E">Erik Weyer</a>, 
<a href="/search/eess?searchtype=author&query=Cs%C3%A1ji%2C+B+C">Bal&#xe1;zs Cs. Cs&#xe1;ji</a>, 
<a href="/search/eess?searchtype=author&query=Campi%2C+M+C">Marco C. Campi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Signal Processing (eess.SP); Statistics Theory (math.ST)

</div>
<p class="mathjax">Sign-Perturbed Sums (SPS) is a system identification method that constructs
confidence regions for the unknown system parameters. In this paper, we study
SPS for ARX systems, and establish that the confidence regions are guaranteed
to include the true model parameter with exact, user-chosen, probability under
mild statistical assumptions, a property that holds true for any finite number
of observed input-output data. Furthermore, we prove the strong consistency of
the method, that is, as the number of data points increases, the confidence
region gets smaller and smaller and will asymptotically almost surely exclude
any parameter value different from the true one. In addition, we also show
that, asymptotically, the SPS region is included in an ellipsoid which is
marginally larger than the confidence ellipsoid obtained from the asymptotic
theory of system identification. The results are theoretically proven and
illustrated in a simulation example.
</p>
</div>
</dd>
<dt><a name="item312">[312]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11530" title="Abstract">arXiv:2402.11530</a> [<a href="/pdf/2402.11530" title="Download PDF">pdf</a>, <a href="/format/2402.11530" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Multimodal Learning from Data-centric Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+M">Muyang He</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yexin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+B">Boya Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jianhao Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yueze Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tiejun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B">Bo Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Multimodal Large Language Models (MLLMs) have demonstrated notable
capabilities in general visual understanding and reasoning tasks. However,
their deployment is hindered by substantial computational costs in both
training and inference, limiting accessibility to the broader research and user
communities. A straightforward solution is to leverage smaller pre-trained
vision and language models, which inevitably causes significant performance
drop. In this paper, we demonstrate the possibility to beat the scaling law and
train a smaller but better MLLM by exploring more informative training data.
Specifically, we introduce Bunny, a family of lightweight MLLMs with flexible
vision and language backbones for efficient multimodal learning from condensed
training data. Remarkably, our Bunny-3B outperforms the state-of-the-art large
MLLMs, especially LLaVA-v1.5-13B, on multiple benchmarks. The code, models and
data can be found in https://github.com/BAAI-DCAI/Bunny.
</p>
</div>
</dd>
<dt><a name="item313">[313]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11531" title="Abstract">arXiv:2402.11531</a> [<a href="/pdf/2402.11531" title="Download PDF">pdf</a>, <a href="/format/2402.11531" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computational complexity of the Weisfeiler-Leman dimension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lichter%2C+M">Moritz Lichter</a>, 
<a href="/search/cs?searchtype=author&query=Ra%C3%9Fmann%2C+S">Simon Ra&#xdf;mann</a>, 
<a href="/search/cs?searchtype=author&query=Schweitzer%2C+P">Pascal Schweitzer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">The Weisfeiler-Leman dimension of a graph $G$ is the least number $k$ such
that the $k$-dimensional Weisfeiler-Leman algorithm distinguishes $G$ from
every other non-isomorphic graph. The dimension is a standard measure of the
descriptive complexity of a graph and recently finds various applications in
particular in the context of machine learning. In this paper, we study the
computational complexity of computing the Weisfeiler-Leman dimension. We
observe that in general the problem of deciding whether the Weisfeiler-Leman
dimension of $G$ is at most $k$ is NP-hard. This is also true for the more
restricted problem with graphs of color multiplicity at most 4. Therefore, we
study parameterized and approximate versions of the problem. We give, for each
fixed $k\geq 2$, a polynomial-time algorithm that decides whether the
Weisfeiler-Leman dimension of a given graph of color multiplicity at most $5$
is at most $k$. Moreover, we show that for these color multiplicities this is
optimal in the sense that this problem is P-hard under logspace-uniform
$\text{AC}_0$-reductions. Furthermore, for each larger bound $c$ on the color
multiplicity and each fixed $k \geq 2$, we provide a polynomial-time
approximation algorithm for the abelian case: given a relational structure with
abelian color classes of size at most $c$, the algorithm outputs either that
its Weisfeiler-Leman dimension is at most $(k+1)c$ or that it is larger than
$k$.
</p>
</div>
</dd>
<dt><a name="item314">[314]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11532" title="Abstract">arXiv:2402.11532</a> [<a href="/pdf/2402.11532" title="Download PDF">pdf</a>, <a href="/format/2402.11532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Chain-of-Instructions: Compositional Instruction Tuning on Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hayati%2C+S+A">Shirley Anugrah Hayati</a>, 
<a href="/search/cs?searchtype=author&query=Jung%2C+T">Taehee Jung</a>, 
<a href="/search/cs?searchtype=author&query=Bodding-Long%2C+T">Tristan Bodding-Long</a>, 
<a href="/search/cs?searchtype=author&query=Kar%2C+S">Sudipta Kar</a>, 
<a href="/search/cs?searchtype=author&query=Sethy%2C+A">Abhinav Sethy</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Joo-Kyung Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dongyeop Kang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Fine-tuning large language models (LLMs) with a collection of large and
diverse instructions has improved the model's generalization to different
tasks, even for unseen tasks. However, most existing instruction datasets
include only single instructions, and they struggle to follow complex
instructions composed of multiple subtasks (Wang et al., 2023a). In this work,
we propose a novel concept of compositional instructions called
chain-of-instructions (CoI), where the output of one instruction becomes an
input for the next like a chain. Unlike the conventional practice of solving
single instruction tasks, our proposed method encourages a model to solve each
subtask step by step until the final answer is reached. CoI-tuning (i.e.,
fine-tuning with CoI instructions) improves the model's ability to handle
instructions composed of multiple subtasks. CoI-tuned models also outperformed
baseline models on multilingual summarization, demonstrating the
generalizability of CoI models on unseen composite downstream tasks.
</p>
</div>
</dd>
<dt><a name="item315">[315]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11533" title="Abstract">arXiv:2402.11533</a> [<a href="/pdf/2402.11533" title="Download PDF">pdf</a>, <a href="/ps/2402.11533" title="Download PostScript">ps</a>, <a href="/format/2402.11533" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Randomness-Efficient Constructions of Capacity-Achieving List-Decodable  Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mosheiff%2C+J">Jonathan Mosheiff</a>, 
<a href="/search/cs?searchtype=author&query=Resch%2C+N">Nicolas Resch</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+K">Kuo Shang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+C">Chen Yuan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">In this work, we consider the task of generating list-decodable codes over
small (say, binary) alphabets using as little randomness as possible.
Specifically, we hope to generate codes achieving what we term the Elias Bound,
which means that they are $(\rho,L)$-list-decodable with rate $R \geq
1-h(\rho)-O(1/L)$. A long line of work shows that uniformly random linear codes
(RLCs) achieve the Elias bound: hence, we know $O(n^2)$ random bits suffice.
Prior works (Guruswami and Mosheiff, FOCS 2022; Putterman and Pyne, arXiv 2023)
demonstrate that just $O_L(n)$ random bits suffice.
<br />We provide two new constructions achieving the Elias bound consuming only
$O(nL)$ random bits. Compared to prior works, our constructions are
considerably simpler and more direct. Furthermore, our codes are designed in
such a way that their duals are also quite easy to analyze. Of particular note,
we construct with $O(nL)$ random bits a code where both the code \emph{and its
dual} achieve the Elias bound! As we discuss, properties of a dual code are
often crucial in applications of error-correcting codes in cryptography.
<br />In all of the above cases -- including the prior works achieving randomness
complexity $O_L(n)$ -- the codes are designed to ``approximate'' RLCs. More
precisely, for a given locality parameter $L$ we construct codes achieving the
same $L$-local properties as RLCs. This allows one to appeal to known
list-decodability results for RLCs and thereby conclude that the code
approximating an RLC also achieves the Elias bound (with high probability). As
a final contribution, we indicate that such a proof strategy is inherently
unable to generate list-decodable codes with $o(nL)$ bits of randomness.
</p>
</div>
</dd>
<dt><a name="item316">[316]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11534" title="Abstract">arXiv:2402.11534</a> [<a href="/pdf/2402.11534" title="Download PDF">pdf</a>, <a href="/format/2402.11534" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PreAct: Predicting Future in ReAct Enhances Agent&#x27;s Planning Ability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+D">Dayuan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jianzhao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Siyuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+G">Guanting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yejie Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+K">Keqing He</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weiran Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 6 gigures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Addressing the discrepancies between predictions and actual outcomes often
aids individuals in expanding their thought processes and engaging in
reflection, thereby facilitating reasoning in the correct direction. In this
paper, we introduce $\textbf{PreAct}$, an agent framework that integrates
$\textbf{pre}$diction with $\textbf{rea}$soning and $\textbf{act}$ion.
Leveraging the information provided by predictions, a large language model
(LLM) based agent can offer more diversified and strategically oriented
reasoning, which in turn leads to more effective actions that help the agent
complete complex tasks. Our experiments demonstrate that PreAct outperforms the
ReAct approach in accomplishing complex tasks and that PreAct can be
co-enhanced when combined with Reflexion methods. We prompt the model with
different numbers of historical predictions and find that historical
predictions have a sustained positive effect on LLM planning. The differences
in single-step reasoning between PreAct and ReAct show that PreAct indeed
offers advantages in terms of diversity and strategic directivity over ReAct.
</p>
</div>
</dd>
<dt><a name="item317">[317]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11537" title="Abstract">arXiv:2402.11537</a> [<a href="/pdf/2402.11537" title="Download PDF">pdf</a>, <a href="/format/2402.11537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deciphering the lmpact of Pretraining Data on Large Language Models  through Machine Unlearning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+L">Li Du</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+X">Xiao Ding</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+K">Kai Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhouhao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jun Shi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Ting Liu</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+B">Bing Qin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Through pretraining on a corpus with various sources, Large Language Models
(LLMs) have gained impressive performance. However, the impact of each
component of the pretraining corpus remains opaque. As a result, the
organization of the pretraining corpus is still empirical and may deviate from
the optimal. To address this issue, we systematically analyze the impact of 48
datasets from 5 major categories of pretraining data of LLMs and measure their
impacts on LLMs using benchmarks about nine major categories of model
capabilities. Our analyses provide empirical results about the contribution of
multiple corpora on the performances of LLMs, along with their joint impact
patterns, including complementary, orthogonal, and correlational relationships.
We also identify a set of ``high-impact data'' such as Books that is
significantly related to a set of model capabilities. These findings provide
insights into the organization of data to support more efficient pretraining of
LLMs.
</p>
</div>
</dd>
<dt><a name="item318">[318]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11540" title="Abstract">arXiv:2402.11540</a> [<a href="/pdf/2402.11540" title="Download PDF">pdf</a>, <a href="/format/2402.11540" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CPN: Complementary Proposal Network for Unconstrained Text Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+L">Longhuang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+S">Shangxuan Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Youxin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+P">Pengfei Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Existing methods for scene text detection can be divided into two paradigms:
segmentation-based and anchor-based. While Segmentation-based methods are
well-suited for irregular shapes, they struggle with compact or overlapping
layouts. Conversely, anchor-based approaches excel for complex layouts but
suffer from irregular shapes. To strengthen their merits and overcome their
respective demerits, we propose a Complementary Proposal Network (CPN) that
seamlessly and parallelly integrates semantic and geometric information for
superior performance. The CPN comprises two efficient networks for proposal
generation: the Deformable Morphology Semantic Network, which generates
semantic proposals employing an innovative deformable morphological operator,
and the Balanced Region Proposal Network, which produces geometric proposals
with pre-defined anchors. To further enhance the complementarity, we introduce
an Interleaved Feature Attention module that enables semantic and geometric
features to interact deeply before proposal generation. By leveraging both
complementary proposals and features, CPN outperforms state-of-the-art
approaches with significant margins under comparable computation cost.
Specifically, our approach achieves improvements of 3.6%, 1.3% and 1.0% on
challenging benchmarks ICDAR19-ArT, IC15, and MSRA-TD500, respectively. Code
for our method will be released.
</p>
</div>
</dd>
<dt><a name="item319">[319]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11541" title="Abstract">arXiv:2402.11541</a> [<a href="/pdf/2402.11541" title="Download PDF">pdf</a>, <a href="/format/2402.11541" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counter-intuitive: Large Language Models Can Better Understand Knowledge  Graphs Than We Thought
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xinbang Dai</a>, 
<a href="/search/cs?searchtype=author&query=Hua%2C+Y">Yuncheng Hua</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tongtong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+Y">Yang Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+G">Guilin Qi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Although the method of enhancing large language models' (LLMs') reasoning
ability and reducing their hallucinations through the use of knowledge graphs
(KGs) has received widespread attention, the exploration of how to enable LLMs
to integrate the structured knowledge in KGs on-the-fly remains inadequate.
Researchers often co-train KG embeddings and LLM parameters to equip LLMs with
the ability of comprehending KG knowledge. However, this resource-hungry
training paradigm significantly increases the model learning cost and is also
unsuitable for non-open-source, black-box LLMs. In this paper, we employ
complex question answering (CQA) as a task to assess the LLM's ability of
comprehending KG knowledge. We conducted a comprehensive comparison of KG
knowledge injection methods (from triples to natural language text), aiming to
explore the optimal prompting method for supplying KG knowledge to LLMs,
thereby enhancing their comprehension of KG. Contrary to our initial
expectations, our analysis revealed that LLMs effectively handle messy, noisy,
and linearized KG knowledge, outperforming methods that employ well-designed
natural language (NL) textual prompts. This counter-intuitive finding provides
substantial insights for future research on LLMs' comprehension of structured
knowledge.
</p>
</div>
</dd>
<dt><a name="item320">[320]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11542" title="Abstract">arXiv:2402.11542</a> [<a href="/pdf/2402.11542" title="Download PDF">pdf</a>, <a href="/format/2402.11542" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Question Answering Over Spatio-Temporal Knowledge Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+X">Xinbang Dai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Huiying Li</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+G">Guilin Qi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Spatio-temporal knowledge graphs (STKGs) extend the concept of knowledge
graphs (KGs) by incorporating time and location information. While the research
community's focus on Knowledge Graph Question Answering (KGQA), the field of
answering questions incorporating both spatio-temporal information based on
STKGs remains largely unexplored. Furthermore, a lack of comprehensive datasets
also has hindered progress in this area. To address this issue, we present
STQAD, a dataset comprising 10,000 natural language questions for
spatio-temporal knowledge graph question answering (STKGQA). Unfortunately,
various state-of-the-art KGQA approaches fall far short of achieving
satisfactory performance on our dataset. In response, we propose STCQA, a new
spatio-temporal KGQA approach that utilizes a novel STKG embedding method named
STComplEx. By extracting temporal and spatial information from a question, our
QA model can better comprehend the question and retrieve accurate answers from
the STKG. Through extensive experiments, we demonstrate the quality of our
dataset and the effectiveness of our STKGQA method.
</p>
</div>
</dd>
<dt><a name="item321">[321]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11543" title="Abstract">arXiv:2402.11543</a> [<a href="/pdf/2402.11543" title="Download PDF">pdf</a>, <a href="/ps/2402.11543" title="Download PostScript">ps</a>, <a href="/format/2402.11543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Energy Sector Resilience: Integrating Security by Design  Principles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shirtz%2C+D">Dov Shirtz</a>, 
<a href="/search/cs?searchtype=author&query=Koberman%2C+I">Inna Koberman</a>, 
<a href="/search/cs?searchtype=author&query=Elyashar%2C+A">Aviad Elyashar</a>, 
<a href="/search/cs?searchtype=author&query=Puzis%2C+R">Rami Puzis</a>, 
<a href="/search/cs?searchtype=author&query=Elovici%2C+Y">Yuval Elovici</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 66 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Security by design, Sbd is a concept for developing and maintaining systems
that are, to the greatest extent possible, free from security vulnerabilities
and impervious to security attacks. In addition to technical aspects, such as
how to develop a robust industrial control systems hardware, software,
communication product, etc., SbD includes also soft aspects, such as
organizational managerial attitude and behavior, and employee awareness. Under
the Sbd concept, systems, ICS in our context, will be considered more
trustworthy by users. User's trust in the systems will be derived from the
meticulous adherence to the SbD processes and policies. In accordance with the
SbD concept, security is considered. Security measures are implemented, at
every stage of the product and systems development life cycle, rather than
afterwards. This document presents the security requirements for the
implementation of the SbD in industrial control systems. The information
presented does not negate any existing security and cyber security standards,
etc. Instead, we strongly recommend that organizations should implement and
comply with those standards and best practices. Security by design is not a
one-time process. It starts at the very beginning of the products of the system
design and continues through all its lifecycle. Due to the benefits of the SbD,
higher level of security, and robustness to cyber attacks, all organizations
associated with the energy sector should strive to establish an ecosystem. The
requirements presented in this document may be perceived as burdensome by
organizations. However, strict compliance with the requirements and existing
security standards and best practices, including continuous monitoring, as
specified in this document, is essential to realize an ecosystem driven and
protected by the SbD
</p>
</div>
</dd>
<dt><a name="item322">[322]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11544" title="Abstract">arXiv:2402.11544</a> [<a href="/pdf/2402.11544" title="Download PDF">pdf</a>, <a href="/ps/2402.11544" title="Download PostScript">ps</a>, <a href="/format/2402.11544" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On efficient normal bases over binary fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sall%2C+M">Mohamadou Sall</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+M+A">M. Anwar Hasan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Binary field extensions are fundamental to many applications, such as
multivariate public key cryptography, code-based cryptography, and
error-correcting codes. Their implementation requires a foundation in number
theory and algebraic geometry and necessitates the utilization of efficient
bases. The continuous increase in the power of computation, and the design of
new (quantum) computers increase the threat to the security of systems and
impose increasingly demanding encryption standards with huge polynomial or
extension degrees. For cryptographic purposes or other common implementations
of finite fields arithmetic, it is essential to explore a wide range of
implementations with diverse bases. Unlike some bases, polynomial and Gaussian
normal bases are well-documented and widely employed. In this paper, we explore
other forms of bases of $\mathbb{F}_{2^n}$ over $\mathbb{F}_2$ to demonstrate
efficient implementation of operations within different ranges. To achieve
this, we leverage results on fast computations and elliptic periods introduced
by Couveignes and Lercier, and subsequently expanded upon by Ezome and Sall.
This leads to the establishment of new tables for efficient computation over
binary fields.
</p>
</div>
</dd>
<dt><a name="item323">[323]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11545" title="Abstract">arXiv:2402.11545</a> [<a href="/pdf/2402.11545" title="Download PDF">pdf</a>, <a href="/format/2402.11545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-order QMC nonconforming FEMs for nearly incompressible planar  stochastic elasticity equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dick%2C+J">J. Dick</a>, 
<a href="/search/math?searchtype=author&query=Gia%2C+T+L">T. Le Gia</a>, 
<a href="/search/math?searchtype=author&query=McLean%2C+W">W. McLean</a>, 
<a href="/search/math?searchtype=author&query=Mustapha%2C+K">K. Mustapha</a>, 
<a href="/search/math?searchtype=author&query=Tran%2C+T">T. Tran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In a recent work (Dick et al, <a href="/abs/2310.06187">arXiv:2310.06187</a>), we considered a linear
stochastic elasticity equation with random Lam\'e parameters which are
parameterized by a countably infinite number of terms in separate expansions.
We estimated the expected values over the infinite dimensional parametric space
of linear functionals ${\mathcal L}$ acting on the continuous solution $\vu$ of
the elasticity equation. This was achieved by truncating the expansions of the
random parameters, then using a high-order quasi-Monte Carlo (QMC) method to
approximate the high dimensional integral combined with the conforming Galerkin
finite element method (FEM) to approximate the displacement over the physical
domain $\Omega.$ In this work, as a further development of aforementioned
article, we focus on the case of a nearly incompressible linear stochastic
elasticity equation. To serve this purpose, in the presence of stochastic
inhomogeneous (variable Lam\'e parameters) nearly compressible material, we
develop a new locking-free symmetric nonconforming Galerkin FEM that handles
the inhomogeneity. In the case of nearly incompressible material, one known
important advantage of nonconforming approximations is that they yield optimal
order convergence rates that are uniform in the Poisson coefficient. Proving
the convergence of the nonconforming FEM leads to another challenge that is
summed up in showing the needed regularity properties of $\vu$. For the error
estimates from the high-order QMC method, which is needed to estimate the
expected value over the infinite dimensional parametric space of ${\mathcal
L}\vu,$ we %rely on (Dick et al. 2022). We are required here to show certain
regularity properties of $\vu$ with respect to the random coefficients. Some
numerical results are delivered at the end.
</p>
</div>
</dd>
<dt><a name="item324">[324]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11547" title="Abstract">arXiv:2402.11547</a> [<a href="/pdf/2402.11547" title="Download PDF">pdf</a>, <a href="/format/2402.11547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hybrid RIS With Sub-Connected Active Partitions: Performance Analysis  and Transmission Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ntougias%2C+K">Konstantinos Ntougias</a>, 
<a href="/search/cs?searchtype=author&query=Chatzinotas%2C+S">Symeon Chatzinotas</a>, 
<a href="/search/cs?searchtype=author&query=Krikidis%2C+I">Ioannis Krikidis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">The emerging reflecting intelligent surface (RIS) technology promises to
enhance the capacity of wireless communication systems via passive reflect
beamforming. However, the product path loss limits its performance gains.
Fully-connected (FC) active RIS, which integrates reflect-type power amplifiers
into the RIS elements, has been recently introduced in response to this issue.
Also, sub-connected (SC) active RIS and hybrid FC-active/passive RIS variants,
which employ a limited number of reflect-type power amplifiers, have been
proposed to provide energy savings. Nevertheless, their flexibility in
balancing diverse capacity requirements and power consumption constraints is
limited. In this direction, this study introduces novel hybrid RIS structures,
wherein at least one reflecting sub-surface (RS) adopts the SC-active RIS
design. The asymptotic signal-to-noise-ratio of the FC-active/passive and the
proposed hybrid RIS variants is analyzed in a single-user single-input
single-output setup. Furthermore, the transmit and RIS beamforming weights are
jointly optimized in each scenario to maximize the energy efficiency of a
hybrid RIS-aided multi-user multiple-input single-output downlink system
subject to the power consumption constraints of the base station and the active
RSs. Numerical simulation and analytic results highlight the performance gains
of the proposed RIS designs over benchmarks, unveil non-trivial trade-offs, and
provide valuable insights.
</p>
</div>
</dd>
<dt><a name="item325">[325]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11548" title="Abstract">arXiv:2402.11548</a> [<a href="/pdf/2402.11548" title="Download PDF">pdf</a>, <a href="/format/2402.11548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KMMLU: Measuring Massive Multitask Language Understanding in Korean
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Son%2C+G">Guijin Son</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">Hanwool Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungdong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seungone Kim</a>, 
<a href="/search/cs?searchtype=author&query=Muennighoff%2C+N">Niklas Muennighoff</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+T">Taekyoon Choi</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+C">Cheonbok Park</a>, 
<a href="/search/cs?searchtype=author&query=Yoo%2C+K+M">Kang Min Yoo</a>, 
<a href="/search/cs?searchtype=author&query=Biderman%2C+S">Stella Biderman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We propose KMMLU, a new Korean benchmark with 35,030 expert-level
multiple-choice questions across 45 subjects ranging from humanities to STEM.
Unlike previous Korean benchmarks that are translated from existing English
benchmarks, KMMLU is collected from original Korean exams, capturing linguistic
and cultural aspects of the Korean language. We test 26 publically available
and proprietary LLMs, identifying significant room for improvement. The best
publicly available model achieves 50.54% on KMMLU, far below the average human
performance of 62.6%. This model was primarily trained for English and Chinese,
not Korean. Current LLMs tailored to Korean, such as Polyglot-Ko, perform far
worse. Surprisingly, even the most capable proprietary LLMs, e.g., GPT-4 and
HyperCLOVA X, achieve 59.95% and 53.40%, respectively. This suggests that
further work is needed to improve Korean LLMs, and KMMLU offers the right tool
to track this progress. We make our dataset publicly available on the Hugging
Face Hub and integrate the benchmark into EleutherAI's Language Model
Evaluation Harness.
</p>
</div>
</dd>
<dt><a name="item326">[326]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11549" title="Abstract">arXiv:2402.11549</a> [<a href="/pdf/2402.11549" title="Download PDF">pdf</a>, <a href="/format/2402.11549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Syntactic Language Change in English and German: Metrics, Parsers, and  Convergences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yanran Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Breitbarth%2C+A">Anne Breitbarth</a>, 
<a href="/search/cs?searchtype=author&query=Stoeckel%2C+M">Manuel Stoeckel</a>, 
<a href="/search/cs?searchtype=author&query=Mehler%2C+A">Alexander Mehler</a>, 
<a href="/search/cs?searchtype=author&query=Eger%2C+S">Steffen Eger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Many studies have shown that human languages tend to optimize for lower
complexity and increased communication efficiency. Syntactic dependency
distance, which measures the linear distance between dependent words, is often
considered a key indicator of language processing difficulty and working memory
load. The current paper looks at diachronic trends in syntactic language change
in both English and German, using corpora of parliamentary debates from the
last c. 160 years. We base our observations on five dependency parsers,
including the widely used Stanford CoreNLP as well as 4 newer alternatives. Our
analysis of syntactic language change goes beyond linear dependency distance
and explores 15 metrics relevant to dependency distance minimization (DDM)
and/or based on tree graph properties, such as the tree height and degree
variance. Even though we have evidence that recent parsers trained on modern
treebanks are not heavily affected by data 'noise' such as spelling changes and
OCR errors in our historic data, we find that results of syntactic language
change are sensitive to the parsers involved, which is a caution against using
a single parser for evaluating syntactic language change as done in previous
work. We also show that syntactic language change over the time period
investigated is largely similar between English and German across the different
metrics explored: only 4% of cases we examine yield opposite conclusions
regarding upwards and downtrends of syntactic metrics across German and
English. We also show that changes in syntactic measures seem to be more
frequent at the tails of sentence length distributions. To our best knowledge,
ours is the most comprehensive analysis of syntactic language using modern NLP
technology in recent corpora of English and German.
</p>
</div>
</dd>
<dt><a name="item327">[327]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11550" title="Abstract">arXiv:2402.11550</a> [<a href="/pdf/2402.11550" title="Download PDF">pdf</a>, <a href="/format/2402.11550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LongAgent: Scaling Language Models to 128k Context through Multi-Agent  Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zu%2C+C">Can Zu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hao Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yi Lu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+W">Wei He</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yiwen Ding</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated impressive performance in
understanding language and executing complex reasoning tasks. However, LLMs
with long context windows have been notorious for their expensive training
costs and high inference latency. Even the most advanced models such as GPT-4
and Claude2 often make mistakes when processing inputs of over $100k$ tokens, a
phenomenon also known as \textit{lost in the middle}. In this paper, we propose
\textsc{LongAgent}, a method based on multi-agent collaboration, which scales
LLMs (e.g., LLaMA) to a context of 128K and demonstrates potential superiority
in long-text processing compared to GPT-4. In \textsc{LongAgent}, a leader is
responsible for understanding user intent and directing team members to acquire
information from documents. Due to members' hallucinations, it is non-trivial
for a leader to obtain accurate information from the responses of dozens to
hundreds of members. To address this, we develop an \textit{inter-member
communication} mechanism to resolve response conflicts caused by hallucinations
through information sharing. Our experimental results indicate that
\textsc{LongAgent} offers a promising alternative for long-text processing. The
agent team instantiated with LLaMA-7B achieves significant improvements in
tasks such as 128k-long text retrieval, multi-hop question answering, compared
to GPT-4.
</p>
</div>
</dd>
<dt><a name="item328">[328]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11553" title="Abstract">arXiv:2402.11553</a> [<a href="/pdf/2402.11553" title="Download PDF">pdf</a>, <a href="/format/2402.11553" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Limits of Information Spread by Memory-less Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=D%27Archivio%2C+N">Niccol&#xf2; D&#x27;Archivio</a> (1), 
<a href="/search/cs?searchtype=author&query=Vacus%2C+R">Robin Vacus</a> (2) ((1) Sapienza Universit&#xe0; di Roma, (2) Bocconi University)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">We address the self-stabilizing bit-dissemination problem, designed to
capture the challenges of spreading information and reaching consensus among
entities with minimal cognitive and communication capacities. Specifically, a
group of $n$ agents is required to adopt the correct opinion, initially held by
a single informed individual, choosing from two possible opinions. In order to
make decisions, agents are restricted to observing the opinions of a few
randomly sampled agents, and lack the ability to communicate further and to
identify the informed individual. Additionally, agents cannot retain any
information from one round to the next.
<br />According to a recent publication in SODA (2024), a logarithmic convergence
time without memory is achievable in the parallel setting (where agents are
updated simultaneously), as long as the number of samples is at least
$\Omega(\sqrt{n \log n})$. However, determining the minimal sample size for an
efficient protocol to exist remains a challenging open question. As a
preliminary step towards an answer, we establish the first lower bound for this
problem in the parallel setting. Specifically, we demonstrate that any protocol
with constant sample size requires asymptotically an almost-linear number of
rounds to converge, with high probability. This lower bound holds even when
agents are aware of both the exact value of $n$ and their own opinion, and
encompasses various simple existing dynamics designed to achieve consensus.
<br />Beyond the bit-dissemination problem, our result sheds light on the
convergence time of the "minority" dynamics, the counterpart of the well-known
majority rule, whose chaotic behavior is yet to be fully understood despite the
apparent simplicity of the algorithm.
</p>
</div>
</dd>
<dt><a name="item329">[329]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11558" title="Abstract">arXiv:2402.11558</a> [<a href="/pdf/2402.11558" title="Download PDF">pdf</a>, <a href="/format/2402.11558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Temporal Disentangled Contrastive Diffusion Model for Spatiotemporal  Imputation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yakun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+K">Kaize Shi</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhangkai Wu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Juan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xianzhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+G">Guandong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+S">Shui Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Spatiotemporal data analysis is pivotal across various domains, including
transportation, meteorology, and healthcare. However, the data collected in
real-world scenarios often suffers incompleteness due to sensor malfunctions
and network transmission errors. Spatiotemporal imputation endeavours to
predict missing values by exploiting the inherent spatial and temporal
dependencies present in the observed data. Traditional approaches, which rely
on classical statistical and machine learning techniques, are often inadequate,
particularly when the data fails to meet strict distributional assumptions. In
contrast, recent deep learning-based methods, leveraging graph and recurrent
neural networks, have demonstrated enhanced efficacy. Nonetheless, these
approaches are prone to error accumulation. Generative models have been
increasingly adopted to circumvent the reliance on potentially inaccurate
historical imputed values for future predictions. These models grapple with the
challenge of producing unstable results, a particular issue in diffusion-based
models. We aim to address these challenges by designing conditional features to
guide the generative process and expedite training. Specifically, we introduce
C$^2$TSD, a novel approach incorporating trend and seasonal information as
conditional features and employing contrastive learning to improve model
generalizability. The extensive experiments on three real-world datasets
demonstrate the superior performance of C$^2$TSD over various state-of-the-art
baselines.
</p>
</div>
</dd>
<dt><a name="item330">[330]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11565" title="Abstract">arXiv:2402.11565</a> [<a href="/pdf/2402.11565" title="Download PDF">pdf</a>, <a href="/format/2402.11565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Continual Learning on Graphs: Challenges, Solutions, and Opportunities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xikun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+D">Dongjin Song</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Continual learning on graph data has recently attracted paramount attention
for its aim to resolve the catastrophic forgetting problem on existing tasks
while adapting the sequentially updated model to newly emerged graph tasks.
While there have been efforts to summarize progress on continual learning
research over Euclidean data, e.g., images and texts, a systematic review of
progress in continual learning on graphs, a.k.a, continual graph learning (CGL)
or lifelong graph learning, is still demanding. Graph data are far more complex
in terms of data structures and application scenarios, making CGL task
settings, model designs, and applications extremely challenging. To bridge the
gap, we provide a comprehensive review of existing continual graph learning
(CGL) algorithms by elucidating the different task settings and categorizing
the existing methods based on their characteristics. We compare the CGL methods
with traditional continual learning techniques and analyze the applicability of
the traditional continual learning techniques to CGL tasks. Additionally, we
review the benchmark works that are crucial to CGL research. Finally, we
discuss the remaining challenges and propose several future directions. We will
maintain an up-to-date GitHub repository featuring a comprehensive list of CGL
algorithms, accessible at
https://github.com/UConn-DSIS/Survey-of-Continual-Learning-on-Graphs.
</p>
</div>
</dd>
<dt><a name="item331">[331]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11566" title="Abstract">arXiv:2402.11566</a> [<a href="/pdf/2402.11566" title="Download PDF">pdf</a>, <a href="/format/2402.11566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Boosting Semi-Supervised 2D Human Pose Estimation by Revisiting Data  Augmentation and Consistency Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Huayi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+M">Mukun Luo</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+F">Fei Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yue Ding</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Hongtao Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages. Semi-Supervised 2D Human Pose Estimation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The 2D human pose estimation is a basic visual problem. However, supervised
learning of a model requires massive labeled images, which is expensive and
labor-intensive. In this paper, we aim at boosting the accuracy of a pose
estimator by excavating extra unlabeled images in a semi-supervised learning
(SSL) way. Most previous consistency-based SSL methods strive to constraint the
model to predict consistent results for differently augmented images. Following
this consensus, we revisit two core aspects including advanced data
augmentation methods and concise consistency training frameworks. Specifically,
we heuristically dig various collaborative combinations of existing data
augmentations, and discover novel superior data augmentation schemes to more
effectively add noise on unlabeled samples. They can compose easy-hard
augmentation pairs with larger transformation difficulty gaps, which play a
crucial role in consistency-based SSL. Moreover, we propose to strongly augment
unlabeled images repeatedly with diverse augmentations, generate multi-path
predictions sequentially, and optimize corresponding unsupervised consistency
losses using one single network. This simple and compact design is on a par
with previous methods consisting of dual or triple networks. Furthermore, it
can also be integrated with multiple networks to produce better performance.
Comparing to state-of-the-art SSL approaches, our method brings substantial
improvements on public datasets. Code is released for academic use in
\url{https://github.com/hnuzhy/MultiAugs}.
</p>
</div>
</dd>
<dt><a name="item332">[332]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11568" title="Abstract">arXiv:2402.11568</a> [<a href="/pdf/2402.11568" title="Download PDF">pdf</a>, <a href="/format/2402.11568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A novel Fourier neural operator framework for classification of  multi-sized images: Application to 3D digital porous media
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kashefi%2C+A">Ali Kashefi</a>, 
<a href="/search/cs?searchtype=author&query=Mukerji%2C+T">Tapan Mukerji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Fourier neural operators (FNOs) are invariant with respect to the size of
input images, and thus images with any size can be fed into FNO-based
frameworks without any modification of network architectures, in contrast to
traditional convolutional neural networks (CNNs). Leveraging the advantage of
FNOs, we propose a novel deep-learning framework for classifying images with
varying sizes. Particularly, we simultaneously train the proposed network on
multi-sized images. As a practical application, we consider the problem of
predicting the label (e.g., permeability) of three-dimensional digital porous
media. To construct the framework, an intuitive approach is to connect FNO
layers to a classifier using adaptive max pooling. First, we show that this
approach is only effective for porous media with fixed sizes, whereas it fails
for porous media of varying sizes. To overcome this limitation, we introduce
our approach: instead of using adaptive max pooling, we use static max pooling
with the size of channel width of FNO layers. Since the channel width of the
FNO layers is independent of input image size, the introduced framework can
handle multi-sized images during training. We show the effectiveness of the
introduced framework and compare its performance with the intuitive approach
through the example of the classification of three-dimensional digital porous
media of varying sizes.
</p>
</div>
</dd>
<dt><a name="item333">[333]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11569" title="Abstract">arXiv:2402.11569</a> [<a href="/pdf/2402.11569" title="Download PDF">pdf</a>, <a href="/format/2402.11569" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Developing Autonomous Robot-Mediated Behavior Coaching Sessions with  Haru
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jel%C3%ADnek%2C+M">Matou&#x161; Jel&#xed;nek</a>, 
<a href="/search/cs?searchtype=author&query=Nichols%2C+E">Eric Nichols</a>, 
<a href="/search/cs?searchtype=author&query=Gomez%2C+R">Randy Gomez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as Late Breaking Report (LBR) at the 19th Annual ACM/IEEE International Conference on Human Robot Interaction (HRI '24)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> HRI '24 Companion, March 11-14, 2024, Boulder, CO, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">This study presents an empirical investigation into the design and impact of
autonomous dialogues in human-robot interaction for behavior change coaching.
We focus on the use of Haru, a tabletop social robot, and explore the
implementation of the Tiny Habits method for fostering positive behavior
change. The core of our study lies in developing a fully autonomous dialogue
system that maximizes Haru's emotional expressiveness and unique personality.
Our methodology involved iterative design and extensive testing of the dialogue
system, ensuring it effectively embodied the principles of the Tiny Habits
method while also incorporating strategies for trust-raising and
trust-dampening. The effectiveness of the final version of the dialogue was
evaluated in an experimental study with human participants (N=12). The results
indicated a significant improvement in perceptions of Haru's liveliness,
interactivity, and neutrality. Additionally, our study contributes to the
broader understanding of dialogue design in social robotics, offering practical
insights for future developments in the field.
</p>
</div>
</dd>
<dt><a name="item334">[334]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11570" title="Abstract">arXiv:2402.11570</a> [<a href="/pdf/2402.11570" title="Download PDF">pdf</a>, <a href="/format/2402.11570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Imitation Learning-Based Online Time-Optimal Control with  Multiple-Waypoint Constraints for Quadrotors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+J">Jiahao Mei</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+F">Fangguo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiming Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shuo Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Over the past decade, there has been a remarkable surge in utilizing
quadrotors for various purposes due to their simple structure and aggressive
maneuverability, such as search and rescue, delivery and autonomous drone
racing, etc. One of the key challenges preventing quadrotors from being widely
used in these scenarios is online waypoint-constrained time-optimal trajectory
generation and control technique. This letter proposes an imitation
learning-based online solution to efficiently navigate the quadrotor through
multiple waypoints with time-optimal performance. The neural networks
(WN&amp;CNets) are trained to learn the control law from the dataset generated by
the time-consuming CPC algorithm and then deployed to generate the optimal
control commands online to guide the quadrotors. To address the challenge of
limited training data and the hover maneuver at the final waypoint, we propose
a transition phase strategy that utilizes polynomials to help the quadrotor
'jump over' the stop-and-go maneuver when switching waypoints. Our method is
demonstrated in both simulation and real-world experiments, achieving a maximum
speed of 7 m/s while navigating through 7 waypoints in a confined space of 6.0
m * 4.0 m * 2.0 m. The results show that with a slight loss in optimality, the
WN&amp;CNets significantly reduce the processing time and enable online optimal
control for multiple-waypoint-constrained flight tasks.
</p>
</div>
</dd>
<dt><a name="item335">[335]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11571" title="Abstract">arXiv:2402.11571</a> [<a href="/pdf/2402.11571" title="Download PDF">pdf</a>, <a href="/format/2402.11571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ain&#x27;t Misbehavin&#x27; -- Using LLMs to Generate Expressive Robot Behavior in  Conversations with the Tabletop Robot Haru
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zining Wang</a>, 
<a href="/search/cs?searchtype=author&query=Reisert%2C+P">Paul Reisert</a>, 
<a href="/search/cs?searchtype=author&query=Nichols%2C+E">Eric Nichols</a>, 
<a href="/search/cs?searchtype=author&query=Gomez%2C+R">Randy Gomez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as Late Breaking Report (LBR) at the 19th Annual ACM/IEEE International Conference on Human Robot Interaction (HRI '24)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Companion of HRI '24, March 11-14, 2024, Boulder, CO, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Social robots aim to establish long-term bonds with humans through engaging
conversation. However, traditional conversational approaches, reliant on
scripted interactions, often fall short in maintaining engaging conversations.
This paper addresses this limitation by integrating large language models
(LLMs) into social robots to achieve more dynamic and expressive conversations.
We introduce a fully-automated conversation system that leverages LLMs to
generate robot responses with expressive behaviors, congruent with the robot's
personality. We incorporate robot behavior with two modalities: 1) a
text-to-speech (TTS) engine capable of various delivery styles, and 2) a
library of physical actions for the robot. We develop a custom,
state-of-the-art emotion recognition model to dynamically select the robot's
tone of voice and utilize emojis from LLM output as cues for generating robot
actions. A demo of our system is available here. To illuminate design and
implementation issues, we conduct a pilot study where volunteers chat with a
social robot using our proposed system, and we analyze their feedback,
conducting a rigorous error analysis of chat transcripts. Feedback was
overwhelmingly positive, with participants commenting on the robot's empathy,
helpfulness, naturalness, and entertainment. Most negative feedback was due to
automatic speech recognition (ASR) errors which had limited impact on
conversations. However, we observed a small class of errors, such as the LLM
repeating itself or hallucinating fictitious information and human responses,
that have the potential to derail conversations, raising important issues for
LLM application.
</p>
</div>
</dd>
<dt><a name="item336">[336]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11572" title="Abstract">arXiv:2402.11572</a> [<a href="/pdf/2402.11572" title="Download PDF">pdf</a>, <a href="/format/2402.11572" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cobra Effect in Reference-Free Image Captioning Metrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zheng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Changxin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+Y">Yawen Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+F">Fei Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianbing Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shujian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> pre-print version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Evaluating the compatibility between textual descriptions and corresponding
images represents a core endeavor within multi-modal research. In recent years,
a proliferation of reference-free methods, leveraging visual-language
pre-trained models (VLMs), has emerged. Empirical evidence has substantiated
that these innovative approaches exhibit a higher correlation with human
judgment, marking a significant advancement in the field. However, does a
higher correlation with human evaluations alone sufficiently denote the
complete of a metric? In response to this question, in this paper, we study if
there are any deficiencies in reference-free metrics. Specifically, inspired by
the Cobra Effect, we utilize metric scores as rewards to direct the captioning
model toward generating descriptions that closely align with the metric's
criteria. If a certain metric has flaws, it will be exploited by the model and
reflected in the generated sentences. Our findings reveal that descriptions
guided by these metrics contain significant flaws, e.g. incoherent statements
and excessive repetition. Subsequently, we propose a novel method termed
Self-Improving to rectify the identified shortcomings within these metrics. We
employ GPT-4V as an evaluative tool to assess generated sentences and the
result reveals that our approach achieves state-of-the-art (SOTA) performance.
In addition, we also introduce a challenging evaluation benchmark called Flaws
Caption to evaluate reference-free image captioning metrics comprehensively.
Our code is available at
https://github.com/aaronma2020/robust_captioning_metric
</p>
</div>
</dd>
<dt><a name="item337">[337]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11573" title="Abstract">arXiv:2402.11573</a> [<a href="/pdf/2402.11573" title="Download PDF">pdf</a>, <a href="/format/2402.11573" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval  Augmented Long-Context Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+K">Kun Luo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+S">Shitao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) call for extension of context to handle many
critical applications. However, the existing approaches are prone to expensive
costs and inferior quality of context extension. In this work, we
proposeExtensible Embedding, which realizes high-quality extension of LLM's
context with strong flexibility and cost-effectiveness. Extensible embedding
stand as an enhancement of typical token embedding, which represents the
information for an extensible scope of context instead of a single token. By
leveraging such compact input units of higher information density, the LLM can
access to a vast scope of context even with a small context window. Extensible
embedding is systematically optimized in architecture and training method,
which leads to multiple advantages. 1) High flexibility of context extension,
which flexibly supports ad-hoc extension of diverse context lengths. 2) Strong
sample efficiency of training, which enables the embedding model to be learned
in a cost-effective way. 3) Superior compatibility with the existing LLMs,
where the extensible embedding can be seamlessly introduced as a plug-in
component. Comprehensive evaluations on long-context language modeling and
understanding tasks verify extensible embedding as an effective, efficient,
flexible, and compatible method to extend the LLM's context.
</p>
</div>
</dd>
<dt><a name="item338">[338]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11574" title="Abstract">arXiv:2402.11574</a> [<a href="/pdf/2402.11574" title="Download PDF">pdf</a>, <a href="/format/2402.11574" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Visual In-Context Learning for Large Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yucheng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qianning Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+J">Jianbing Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">In Large Visual Language Models (LVLMs), the efficacy of In-Context Learning
(ICL) remains limited by challenges in cross-modal interactions and
representation disparities. To overcome these challenges, we introduce a novel
Visual In-Context Learning (VICL) method comprising Visual Demonstration
Retrieval, Intent-Oriented Image Summarization, and Intent-Oriented
Demonstration Composition. Our approach retrieves images via ''Retrieval &amp;
Rerank'' paradigm, summarises images with task intent and task-specific visual
parsing, and composes language-based demonstrations that reduce token count and
alleviate cross-modal interaction problem. Experimental evaluations on five
visual reasoning datasets demonstrate the effectiveness of our method.
Moreover, our extensive experiments leverage information flow analysis to
elucidate the effectiveness of our method, and investigate the impact of length
and position of demonstrations for LVLM. The use of in-context unlearning
further shows promise in resetting specific model knowledge without retraining.
</p>
</div>
</dd>
<dt><a name="item339">[339]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11576" title="Abstract">arXiv:2402.11576</a> [<a href="/pdf/2402.11576" title="Download PDF">pdf</a>, <a href="/ps/2402.11576" title="Download PostScript">ps</a>, <a href="/format/2402.11576" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Humanity-Centered Design without Hubris
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gorichanaz%2C+T">Tim Gorichanaz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper for the 2024 CHI Conference on Human Factors in Computing Systems, in the alt.chi session; see "CHI EA '24: Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems" (May 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Humanity-centered design is a concept of emerging interest in HCI, one
motivated by the limitations of human-centered design. As discussed to date,
humanity-centered design is compatible with but goes beyond human-centered
design in that it considers entire ecosystems and populations over the long
term and centers participatory design. Though the intentions of
humanity-centered design are laudable, current articulations of
humanity-centered design are incoherent in a number of ways, leading to
questions of how exactly it can or should be implemented. In this article, I
delineate four ways in which humanity-centered design is incoherent, which can
be boiled down to a tendency toward hubris, and propose a more fruitful way
forward, a humble approach to humanity-centered design. Rather than a
contradiction in terms, "humility" here refers to an organic, piecemeal,
patterns-based approach to design that will be good for our being on this
earth.
</p>
</div>
</dd>
<dt><a name="item340">[340]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11577" title="Abstract">arXiv:2402.11577</a> [<a href="/pdf/2402.11577" title="Download PDF">pdf</a>, <a href="/format/2402.11577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extensible Embedding: A Flexible Multipler For LLM&#x27;s Context Length
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+N">Ninglu Shao</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+S">Shitao Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peitian Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) call for extension of context to handle many
critical applications. However, the existing approaches are prone to expensive
costs and inferior quality of context extension. In this work, we propose
Extensible Embedding, which realizes high-quality extension of LLM's context
with strong flexibility and cost-effectiveness. Extensible embedding stand as
an enhancement of typical token embedding, which represents the information for
an extensible scope of context instead of a single token. By leveraging such
compact input units of higher information density, the LLM can access to a vast
scope of context even with a small context window. Extensible embedding is
systematically optimized in architecture and training method, which leads to
multiple advantages. 1) High flexibility of context extension, which flexibly
supports ad-hoc extension of diverse context lengths. 2) Strong sample
efficiency of training, which enables the embedding model to be learned in a
cost-effective way. 3) Superior compatibility with the existing LLMs, where the
extensible embedding can be seamlessly introduced as a plug-in component.
Comprehensive evaluations on long-context language modeling and understanding
tasks verify extensible embedding as an effective, efficient, flexible, and
compatible method to extend the LLM's context.
</p>
</div>
</dd>
<dt><a name="item341">[341]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11581" title="Abstract">arXiv:2402.11581</a> [<a href="/pdf/2402.11581" title="Download PDF">pdf</a>, <a href="/ps/2402.11581" title="Download PostScript">ps</a>, <a href="/format/2402.11581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using rule engine in self-healing systems and MAPE model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yazdanparast%2C+Z">Zahra Yazdanparast</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Software malfunction presents a significant hurdle within the computing
domain, carrying substantial risks for systems, enterprises, and users
universally. To produce software with high reliability and quality, effective
debugging is essential. Program debugging is an activity to reduce software
maintenance costs. In this study, a failure repair method that uses a rule
engine is presented. The simulation on mRUBIS showed that the proposed method
could be efficient in the operational environment. Through a thorough grasp of
software failure and the adoption of efficient mitigation strategies,
stakeholders can bolster the dependability, security, and adaptability of
software systems. This, in turn, reduces the repercussions of failures and
cultivates increased confidence in digital technologies.
</p>
</div>
</dd>
<dt><a name="item342">[342]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11582" title="Abstract">arXiv:2402.11582</a> [<a href="/pdf/2402.11582" title="Download PDF">pdf</a>, <a href="/format/2402.11582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Publicly auditable privacy-preserving electoral rolls
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Agrawal%2C+P">Prashant Agrawal</a>, 
<a href="/search/cs?searchtype=author&query=Jhanwar%2C+M+P">Mahabir Prasad Jhanwar</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+S+V">Subodh Vishnu Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+S">Subhashis Banerjee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">While existing literature on electronic voting has extensively addressed
verifiability of voting protocols, the vulnerability of electoral rolls in
large public elections remains a critical concern. To ensure integrity of
electoral rolls, the current practice is to either make electoral rolls public
or share them with the political parties. However, this enables construction of
detailed voter profiles and selective targeting and manipulation of voters,
thereby undermining the fundamental principle of free and fair elections. In
this paper, we study the problem of designing publicly auditable yet
privacy-preserving electoral rolls. We first formulate a threat model and
provide formal security definitions. We then present a protocol for creation
and maintenance of electoral rolls that mitigates the threats. Eligible voters
can verify their inclusion, whereas political parties and auditors can
statistically audit the electoral roll. The entire electoral roll is never
revealed, which prevents any large-scale systematic voter targeting and
manipulation.
</p>
</div>
</dd>
<dt><a name="item343">[343]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11585" title="Abstract">arXiv:2402.11585</a> [<a href="/pdf/2402.11585" title="Download PDF">pdf</a>, <a href="/format/2402.11585" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PolypNextLSTM: A lightweight and fast polyp video segmentation network  using ConvNext and ConvLSTM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+D">Debayan Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Reuter%2C+K">Konrad Reuter</a>, 
<a href="/search/cs?searchtype=author&query=Behrendnt%2C+F">Finn Behrendnt</a>, 
<a href="/search/cs?searchtype=author&query=Maack%2C+L">Lennart Maack</a>, 
<a href="/search/cs?searchtype=author&query=Grube%2C+S">Sarah Grube</a>, 
<a href="/search/cs?searchtype=author&query=Schlaefer%2C+A">Alexander Schlaefer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Commonly employed in polyp segmentation, single image UNet architectures lack
the temporal insight clinicians gain from video data in diagnosing polyps. To
mirror clinical practices more faithfully, our proposed solution,
PolypNextLSTM, leverages video-based deep learning, harnessing temporal
information for superior segmentation performance with the least parameter
overhead, making it possibly suitable for edge devices. PolypNextLSTM employs a
UNet-like structure with ConvNext-Tiny as its backbone, strategically omitting
the last two layers to reduce parameter overhead. Our temporal fusion module, a
Convolutional Long Short Term Memory (ConvLSTM), effectively exploits temporal
features. Our primary novelty lies in PolypNextLSTM, which stands out as the
leanest in parameters and the fastest model, surpassing the performance of five
state-of-the-art image and video-based deep learning models. The evaluation of
the SUN-SEG dataset spans easy-to-detect and hard-to-detect polyp scenarios,
along with videos containing challenging artefacts like fast motion and
occlusion.
</p>
</div>
</dd>
<dt><a name="item344">[344]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11588" title="Abstract">arXiv:2402.11588</a> [<a href="/pdf/2402.11588" title="Download PDF">pdf</a>, <a href="/format/2402.11588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SDiT: Spiking Diffusion Model with Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Shu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Hanzhi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chengting Yu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">Aili Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+E">Er-Ping Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Spiking neural networks (SNNs) have low power consumption and
bio-interpretable characteristics, and are considered to have tremendous
potential for energy-efficient computing. However, the exploration of SNNs on
image generation tasks remains very limited, and a unified and effective
structure for SNN-based generative models has yet to be proposed. In this
paper, we explore a novel diffusion model architecture within spiking neural
networks. We utilize transformer to replace the commonly used U-net structure
in mainstream diffusion models. It can generate higher quality images with
relatively lower computational cost and shorter sampling time. It aims to
provide an empirical baseline for research of generative models based on SNNs.
Experiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets demonstrate that our
work is highly competitive compared to existing SNN generative models.
</p>
</div>
</dd>
<dt><a name="item345">[345]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11590" title="Abstract">arXiv:2402.11590</a> [<a href="/pdf/2402.11590" title="Download PDF">pdf</a>, <a href="/format/2402.11590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Designing interactive data visualizations representing recovery progress  for patients after stroke
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ouskine%2C+A">Alicia Ouskine</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+A+D+C">Adrian D. C. Chan</a>, 
<a href="/search/cs?searchtype=author&query=Rajabiyazdi%2C+F">Fateme Rajabiyazdi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Stroke is one of the leading causes of disability worldwide. The efficacy of
recovery is determined by a variety of factors, including patient adherence to
rehabilitation programs. One way to increase patient adherence to their
rehabilitation program is to show patients their progress that is visualized in
a simple and intuitive way. We begin to gather preliminary information on
Functional Capacity, Motor Function, and Mood/cognition from occupational
Therapists at the Bruyere Hospital to gain a better understanding of how stroke
recovery data is collected within in-patient stroke rehabilitation centers. The
future aim is to design, develop, and evaluate a data visualization tool
representing progress made by patients recovering from stroke.
</p>
</div>
</dd>
<dt><a name="item346">[346]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11592" title="Abstract">arXiv:2402.11592</a> [<a href="/pdf/2402.11592" title="Download PDF">pdf</a>, <a href="/format/2402.11592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Zeroth-Order Optimization for Memory-Efficient LLM  Fine-Tuning: A Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yihua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pingzhi Li</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+J">Junyuan Hong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaxiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yimeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+W">Wenqing Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+P">Pin-Yu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+D">Jason D. Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+W">Wotao Yin</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+M">Mingyi Hong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sijia Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianlong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">In the evolving landscape of natural language processing (NLP), fine-tuning
pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like
SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial
memory overhead from back-propagation (BP) for FO gradient computation presents
a significant challenge. Addressing this issue is crucial, especially for
applications like on-device training where memory efficiency is paramount. This
paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a
solution for reducing memory costs during LLM fine-tuning, building on the
initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work
expands the exploration to a wider array of ZO optimization techniques, through
a comprehensive, first-of-its-kind benchmarking study across five LLM families
(Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five
fine-tuning schemes. Our study unveils previously overlooked optimization
principles, highlighting the importance of task alignment, the role of the
forward gradient method, and the balance between algorithm complexity and
fine-tuning performance. We further introduce novel enhancements to ZO
optimization, including block-wise descent, hybrid training, and gradient
sparsity. Our study offers a promising direction for achieving further
memory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at
https://github.com/ZO-Bench/ZO-LLM .
</p>
</div>
</dd>
<dt><a name="item347">[347]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11594" title="Abstract">arXiv:2402.11594</a> [<a href="/pdf/2402.11594" title="Download PDF">pdf</a>, <a href="/format/2402.11594" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simplifying Hyperparameter Tuning in Online Machine Learning -- The  spotRiverGUI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bartz-Beielstein%2C+T">Thomas Bartz-Beielstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Batch Machine Learning (BML) reaches its limits when dealing with very large
amounts of streaming data. This is especially true for available memory,
handling drift in data streams, and processing new, unknown data. Online
Machine Learning (OML) is an alternative to BML that overcomes the limitations
of BML. OML is able to process data in a sequential manner, which is especially
useful for data streams. The `river` package is a Python OML-library, which
provides a variety of online learning algorithms for classification,
regression, clustering, anomaly detection, and more. The `spotRiver` package
provides a framework for hyperparameter tuning of OML models. The
`spotRiverGUI` is a graphical user interface for the `spotRiver` package. The
`spotRiverGUI` releases the user from the burden of manually searching for the
optimal hyperparameter setting. After the data is provided, users can compare
different OML algorithms from the powerful `river` package in a convenient way
and tune the selected algorithms very efficiently.
</p>
</div>
</dd>
<dt><a name="item348">[348]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11596" title="Abstract">arXiv:2402.11596</a> [<a href="/pdf/2402.11596" title="Download PDF">pdf</a>, <a href="/ps/2402.11596" title="Download PostScript">ps</a>, <a href="/format/2402.11596" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faster algorithms on linear delta-matroids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koana%2C+T">Tomohiro Koana</a>, 
<a href="/search/cs?searchtype=author&query=Wahlstr%C3%B6m%2C+M">Magnus Wahlstr&#xf6;m</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
<p class="mathjax">We show new algorithms and constructions over linear delta-matroids. We
observe an alternative representation for linear delta-matroids, as a
contraction representation over a skew-symmetric matrix. This is equivalent to
the more standard "twist representation" up to $O(n^\omega)$-time
transformations, but is much more convenient for algorithmic tasks. For
instance, the problem of finding a max-weight feasible set now reduces directly
to the problem of finding a max-weight basis in a linear matroid. Supported by
this representation, we provide new algorithms and constructions over linear
delta-matroids. We show that the union and delta-sum of linear delta-matroids
define linear delta-matroids, and a representation for the resulting
delta-matroid can be constructed in randomized time $O(n^\omega)$. Previously,
it was only known that these operations define delta-matroids. We also note
that every projected linear delta-matroid can be represented as an elementary
projection. This implies that several optimization problems over (projected)
linear delta-matroids, including the coverage, delta-coverage, and parity
problems, reduce (in their decision versions) to a single $O(n^{\omega})$-time
matrix rank computation. Using the methods of Harvey, previously used by
Cheung, Lao and Leung for linear matroid parity, we furthermore show how to
solve the search versions in the same time. This improves on the $O(n^4)$-time
augmenting path algorithm of Geelen, Iwata and Murota. Finally, we consider the
maximum-cardinality delta-matroid intersection problem. Using Storjohann's
algorithms for symbolic determinants, we show that such a solution can be found
in $O(n^{\omega+1})$ time. This is the first polynomial-time algorithm for the
problem, solving an open question of Kakimura and Takamatsu.
</p>
</div>
</dd>
<dt><a name="item349">[349]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11597" title="Abstract">arXiv:2402.11597</a> [<a href="/pdf/2402.11597" title="Download PDF">pdf</a>, <a href="/format/2402.11597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-Task Inference: Can Large Language Models Follow Multiple  Instructions at Once?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Son%2C+G">Guijin Son</a>, 
<a href="/search/cs?searchtype=author&query=Baek%2C+S">Sangwon Baek</a>, 
<a href="/search/cs?searchtype=author&query=Nam%2C+S">Sangdae Nam</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+I">Ilgyun Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seungone Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) are typically prompted to follow a single
instruction per inference call. In this work, we analyze whether LLMs also hold
the capability to handle multiple instructions simultaneously, denoted as
Multi-Task Inference. For this purpose, we introduce the MTI Bench(Multi-Task
Inference Benchmark), a comprehensive evaluation benchmark encompassing 5,000
instances across 25 tasks. Each task in the MTI Bench involves 2 to 3
sub-tasks. As expected, we first demonstrate that Multi-Task Inference reduces
the total inference time by 1.46 times in average since it does not require
multiple inference calls. Interestingly, contrary to the expectation that LLMs
would perform better when tasks are divided, we find that state-of-the-art
LLMs, such as Llama-2-Chat-70B and GPT-4, show up to 7.3% and 12.4% improved
performance with Multi-Task Inference compared to Single-Task Inference on the
MTI Bench. We release the MTI Bench dataset and our code at this link
https://github.com/guijinSON/MTI-Bench.
</p>
</div>
</dd>
<dt><a name="item350">[350]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11601" title="Abstract">arXiv:2402.11601</a> [<a href="/pdf/2402.11601" title="Download PDF">pdf</a>, <a href="/ps/2402.11601" title="Download PostScript">ps</a>, <a href="/format/2402.11601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Smooth Path Planning with Subharmonic Artificial Potential Field
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+B">Bo Peng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lingke Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+R">Rong Xiong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper is submitted to ICARM2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">When a mobile robot plans its path in an environment with obstacles using
Artificial Potential Field (APF) strategy, it may fall into the local minimum
point and fail to reach the goal. Also, the derivatives of APF will explode
close to obstacles causing poor planning performance. To solve the problems,
exponential functions are used to modify potential fields' formulas. The
potential functions can be subharmonic when the distance between the robot and
obstacles is above a predefined threshold. Subharmonic functions do not have
local minimum and the derivatives of exponential functions increase mildly when
the robot is close to obstacles, thus eliminate the problems in theory.
Circular sampling technique is used to keep the robot outside a danger distance
to obstacles and support the construction of subharmonic functions. Through
simulations, it is proven that mobile robots can bypass local minimum points
and construct a smooth path to reach the goal successfully by the proposed
methods.
</p>
</div>
</dd>
<dt><a name="item351">[351]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11604" title="Abstract">arXiv:2402.11604</a> [<a href="/pdf/2402.11604" title="Download PDF">pdf</a>, <a href="/format/2402.11604" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-evolving Autoencoder Embedded Q-Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Senthilnath%2C+J">J. Senthilnath</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Bangjian Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+Z+W">Zhen Wei Ng</a>, 
<a href="/search/cs?searchtype=author&query=Aggarwal%2C+D">Deeksha Aggarwal</a>, 
<a href="/search/cs?searchtype=author&query=Dutta%2C+R">Rajdeep Dutta</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+J+W">Ji Wei Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Aung%2C+A+P+P">Aye Phyu Phyu Aung</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+K">Keyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Min Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoli Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 9 figures, 3 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">In the realm of sequential decision-making tasks, the exploration capability
of a reinforcement learning (RL) agent is paramount for achieving high rewards
through interactions with the environment. To enhance this crucial ability, we
propose SAQN, a novel approach wherein a self-evolving autoencoder (SA) is
embedded with a Q-Network (QN). In SAQN, the self-evolving autoencoder
architecture adapts and evolves as the agent explores the environment. This
evolution enables the autoencoder to capture a diverse range of raw
observations and represent them effectively in its latent space. By leveraging
the disentangled states extracted from the encoder generated latent space, the
QN is trained to determine optimal actions that improve rewards. During the
evolution of the autoencoder architecture, a bias-variance regulatory strategy
is employed to elicit the optimal response from the RL agent. This strategy
involves two key components: (i) fostering the growth of nodes to retain
previously acquired knowledge, ensuring a rich representation of the
environment, and (ii) pruning the least contributing nodes to maintain a more
manageable and tractable latent space. Extensive experimental evaluations
conducted on three distinct benchmark environments and a real-world molecular
environment demonstrate that the proposed SAQN significantly outperforms
state-of-the-art counterparts. The results highlight the effectiveness of the
self-evolving autoencoder and its collaboration with the Q-Network in tackling
sequential decision-making tasks.
</p>
</div>
</dd>
<dt><a name="item352">[352]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11605" title="Abstract">arXiv:2402.11605</a> [<a href="/pdf/2402.11605" title="Download PDF">pdf</a>, <a href="/format/2402.11605" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradient-enhanced crystal plasticity coupled with phase-field fracture  modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Auth%2C+K+L">Kim Louisa Auth</a>, 
<a href="/search/math?searchtype=author&query=Brouzoulis%2C+J">Jim Brouzoulis</a>, 
<a href="/search/math?searchtype=author&query=Ekh%2C+M">Magnus Ekh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">This study addresses ductile fracture of single grains in metals by modeling
of the formation and propagation of transgranular cracks. A proposed model
integrates gradient extended hardening, phase-field modeling for fracture, and
crystal plasticity. It is presented in a thermodynamical framework in large
deformation kinematics and accounts for damage irreversibility. A micromorphic
approach for variationally and thermodynamically consistent damage
irreversibility is adopted. The main objective of this work is to analyze the
capability of the proposed model to predict transgranular crack propagation.
Further, the micromorphic approach for damage irreversibility is evaluated in
the context of the presented ductile phase-field model. This is done by
analyzing the impact of gradient-enhanced hardening considering micro-free and
micro-hard boundary conditions, studying the effect of the micromorphic
regularization parameter, evaluating the performance of the model in ratcheting
loading and and testing its capability to predict three-dimensional crack
propagation. In order to solve the fully coupled global and local equation
systems, a staggered solution scheme that extends to the local level is
presented.
</p>
</div>
</dd>
<dt><a name="item353">[353]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11608" title="Abstract">arXiv:2402.11608</a> [<a href="/pdf/2402.11608" title="Download PDF">pdf</a>, <a href="/format/2402.11608" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Metric-Learning Encoding Models Identify Processing Profiles of  Linguistic Features in BERT&#x27;s Representations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jalouzot%2C+L">Louis Jalouzot</a>, 
<a href="/search/cs?searchtype=author&query=Sobczyk%2C+R">Robin Sobczyk</a>, 
<a href="/search/cs?searchtype=author&query=Lhopitallier%2C+B">Bastien Lhopitallier</a>, 
<a href="/search/cs?searchtype=author&query=Salle%2C+J">Jeanne Salle</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+N">Nur Lan</a>, 
<a href="/search/cs?searchtype=author&query=Chemla%2C+E">Emmanuel Chemla</a>, 
<a href="/search/cs?searchtype=author&query=Lakretz%2C+Y">Yair Lakretz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We introduce Metric-Learning Encoding Models (MLEMs) as a new approach to
understand how neural systems represent the theoretical features of the objects
they process. As a proof-of-concept, we apply MLEMs to neural representations
extracted from BERT, and track a wide variety of linguistic features (e.g.,
tense, subject person, clause type, clause embedding). We find that: (1)
linguistic features are ordered: they separate representations of sentences to
different degrees in different layers; (2) neural representations are organized
hierarchically: in some layers, we find clusters of representations nested
within larger clusters, following successively important linguistic features;
(3) linguistic features are disentangled in middle layers: distinct, selective
units are activated by distinct linguistic features. Methodologically, MLEMs
are superior (4) to multivariate decoding methods, being more robust to type-I
errors, and (5) to univariate encoding methods, in being able to predict both
local and distributed representations. Together, this demonstrates the utility
of Metric-Learning Encoding Methods for studying how linguistic features are
neurally encoded in language models and the advantage of MLEMs over traditional
methods. MLEMs can be extended to other domains (e.g. vision) and to other
neural systems, such as the human brain.
</p>
</div>
</dd>
<dt><a name="item354">[354]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11617" title="Abstract">arXiv:2402.11617</a> [<a href="/pdf/2402.11617" title="Download PDF">pdf</a>, <a href="/ps/2402.11617" title="Download PostScript">ps</a>, <a href="/format/2402.11617" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A high order, block finite difference, error inhibiting scheme for the  transport equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ditkowski%2C+A">Adi Ditkowski</a>, 
<a href="/search/math?searchtype=author&query=Blanc%2C+A+L">Anne Le Blanc</a>, 
<a href="/search/math?searchtype=author&query=Shu%2C+C">Chi-Wang Shu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We propose a block finite difference, error inhibiting scheme that is
fourth-order accurate for short to moderate times and has a six-order
convergence rate for long times. This scheme outperforms the standard
fourth-order Finite Difference scheme. We also demonstrate that the proposed
scheme is a particular type of nodal-based Discontinuous Galerkin method with
$p=1$.
</p>
</div>
</dd>
<dt><a name="item355">[355]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11618" title="Abstract">arXiv:2402.11618</a> [<a href="/pdf/2402.11618" title="Download PDF">pdf</a>, <a href="/ps/2402.11618" title="Download PostScript">ps</a>, <a href="/format/2402.11618" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Network Design and Planning 2.0 for Optical-computing-enabled  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hai%2C+D+T">Dao Thanh Hai</a>, 
<a href="/search/cs?searchtype=author&query=Woungang%2C+I">Isaac Woungang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures, 2 tables, accepted version to The 38th International Conference on Advanced Information Networking and Applications (AINA-2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">In accommodating the continued explosive growth in Internet traffic, optical
core networks have been evolving accordingly thanks to numerous technological
and architectural innovations. From an architectural perspective, the adoption
of optical-bypass networking in the last two decades has resulted in
substantial cost savings, owning to the elimination of massive
optical-electrical optical interfaces. In optical-bypass framework, the basic
functions of optical nodes include adding (dropping) and cross-connecting
transitional lightpaths. Moreover, in the process of cross-connecting
transiting lightpaths through an intermediate node, these lightpaths must be
separated from each other in either time, frequency or spatial domain, to avoid
unwanted interference which deems to deteriorate the signal qualities. In light
of recently enormous advances in photonic signal processing / computing
technologies enabling the precisely controlled interference of optical channels
for various computing functions, we propose a new architectural paradigm for
future optical networks, namely, optical-computing-enabled networks. Our
proposal is defined by the added capability of optical nodes permitting the
superposition of transitional lightpaths for computing purposes to achieve
greater capacity efficiency. Specifically, we present two illustrative examples
highlighting the potential benefits of bringing about in-network optical
computing functions which are relied on optical aggregation and optical XOR
gate. The new optical computing capabilities armed at optical nodes therefore
call for a radical change in formulating networking problems and designing
accompanying algorithms, which are collectively referred to as optical network
design and planning 2.0 so that the capital and operational efficiency could be
fully unlocked.
</p>
</div>
</dd>
<dt><a name="item356">[356]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11621" title="Abstract">arXiv:2402.11621</a> [<a href="/pdf/2402.11621" title="Download PDF">pdf</a>, <a href="/ps/2402.11621" title="Download PostScript">ps</a>, <a href="/format/2402.11621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoding News Narratives: A Critical Analysis of Large Language Models  in Framing Bias Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pastorino%2C+V">Valeria Pastorino</a>, 
<a href="/search/cs?searchtype=author&query=Sivakumar%2C+J+A">Jasivan A. Sivakumar</a>, 
<a href="/search/cs?searchtype=author&query=Moosavi%2C+N+S">Nafise Sadat Moosavi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">This work contributes to the expanding research on the applicability of LLMs
in social sciences by examining the performance of GPT-3.5 Turbo, GPT-4, and
Flan-T5 models in detecting framing bias in news headlines through zero-shot,
few-shot, and explainable prompting methods. A key insight from our evaluation
is the notable efficacy of explainable prompting in enhancing the reliability
of these models, highlighting the importance of explainable settings for social
science research on framing bias. GPT-4, in particular, demonstrated enhanced
performance in few-shot scenarios when presented with a range of relevant,
in-domain examples. FLAN-T5's poor performance indicates that smaller models
may require additional task-specific fine-tuning for identifying framing bias
detection. Our study also found that models, particularly GPT-4, often
misinterpret emotional language as an indicator of framing bias, underscoring
the challenge of distinguishing between reporting genuine emotional expression
and intentionally use framing bias in news headlines. We further evaluated the
models on two subsets of headlines where the presence or absence of framing
bias was either clear-cut or more contested, with the results suggesting that
these models' can be useful in flagging potential annotation inaccuracies
within existing or new datasets. Finally, the study evaluates the models in
real-world conditions ("in the wild"), moving beyond the initial dataset
focused on U.S. Gun Violence, assessing the models' performance on framed
headlines covering a broad range of topics.
</p>
</div>
</dd>
<dt><a name="item357">[357]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11622" title="Abstract">arXiv:2402.11622</a> [<a href="/pdf/2402.11622" title="Download PDF">pdf</a>, <a href="/format/2402.11622" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Logical Closed Loop: Uncovering Object Hallucinations in Large  Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Junfei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qiang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Ding Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jinghao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Liang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+T">Tieniu Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Object hallucination has been an Achilles' heel which hinders the broader
applications of large vision-language models (LVLMs). Object hallucination
refers to the phenomenon that the LVLMs claim non-existent objects in the
image. To mitigate the object hallucinations, instruction tuning and external
model-based detection methods have been proposed, which either require
large-scare computational resources or depend on the detection result of
external models. However, there remains an under-explored field to utilize the
LVLM itself to alleviate object hallucinations. In this work, we adopt the
intuition that the LVLM tends to respond logically consistently for existent
objects but inconsistently for hallucinated objects. Therefore, we propose a
Logical Closed Loop-based framework for Object Hallucination Detection and
Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency
probing to raise questions with logical correlations, inquiring about
attributes from objects and vice versa. Whether their responses can form a
logical closed loop serves as an indicator of object hallucination. As a
plug-and-play method, it can be seamlessly applied to all existing LVLMs.
Comprehensive experiments conducted on three benchmarks across four LVLMs have
demonstrated significant improvements brought by our method, indicating its
effectiveness and generality.
</p>
</div>
</dd>
<dt><a name="item358">[358]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11625" title="Abstract">arXiv:2402.11625</a> [<a href="/pdf/2402.11625" title="Download PDF">pdf</a>, <a href="/format/2402.11625" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpeCrawler: Generating OpenAPI Specifications from API Documentation  Using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lazar%2C+K">Koren Lazar</a>, 
<a href="/search/cs?searchtype=author&query=Vetzler%2C+M">Matan Vetzler</a>, 
<a href="/search/cs?searchtype=author&query=Uziel%2C+G">Guy Uziel</a>, 
<a href="/search/cs?searchtype=author&query=Boaz%2C+D">David Boaz</a>, 
<a href="/search/cs?searchtype=author&query=Goldbraich%2C+E">Esther Goldbraich</a>, 
<a href="/search/cs?searchtype=author&query=Amid%2C+D">David Amid</a>, 
<a href="/search/cs?searchtype=author&query=Anaby-Tavor%2C+A">Ateret Anaby-Tavor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under Review for KDD 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In the digital era, the widespread use of APIs is evident. However, scalable
utilization of APIs poses a challenge due to structure divergence observed in
online API documentation. This underscores the need for automatic tools to
facilitate API consumption. A viable approach involves the conversion of
documentation into an API Specification format. While previous attempts have
been made using rule-based methods, these approaches encountered difficulties
in generalizing across diverse documentation. In this paper we introduce
SpeCrawler, a comprehensive system that utilizes large language models (LLMs)
to generate OpenAPI Specifications from diverse API documentation through a
carefully crafted pipeline. By creating a standardized format for numerous
APIs, SpeCrawler aids in streamlining integration processes within API
orchestrating systems and facilitating the incorporation of tools into LLMs.
The paper explores SpeCrawler's methodology, supported by empirical evidence
and case studies, demonstrating its efficacy through LLM capabilities.
</p>
</div>
</dd>
<dt><a name="item359">[359]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11626" title="Abstract">arXiv:2402.11626</a> [<a href="/pdf/2402.11626" title="Download PDF">pdf</a>, <a href="/format/2402.11626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Metacognitive Retrieval-Augmented Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yujia Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+J">Jiajie Jin</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+J">Jian-Yun Nie</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Z">Zhicheng Dou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Retrieval-augmented generation have become central in natural language
processing due to their efficacy in generating factual content. While
traditional methods employ single-time retrieval, more recent approaches have
shifted towards multi-time retrieval for multi-hop reasoning tasks. However,
these strategies are bound by predefined reasoning steps, potentially leading
to inaccuracies in response generation. This paper introduces MetaRAG, an
approach that combines the retrieval-augmented generation process with
metacognition. Drawing from cognitive psychology, metacognition allows an
entity to self-reflect and critically evaluate its cognitive processes. By
integrating this, MetaRAG enables the model to monitor, evaluate, and plan its
response strategies, enhancing its introspective reasoning abilities. Through a
three-step metacognitive regulation pipeline, the model can identify
inadequacies in initial cognitive responses and fixes them. Empirical
evaluations show that MetaRAG significantly outperforms existing methods.
</p>
</div>
</dd>
<dt><a name="item360">[360]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11627" title="Abstract">arXiv:2402.11627</a> [<a href="/pdf/2402.11627" title="Download PDF">pdf</a>, <a href="/format/2402.11627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interactive Garment Recommendation with User in the Loop
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Becattini%2C+F">Federico Becattini</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaolin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Puccia%2C+A">Andrea Puccia</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+H">Haokun Wen</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xuemeng Song</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+L">Liqiang Nie</a>, 
<a href="/search/cs?searchtype=author&query=Del+Bimbo%2C+A">Alberto Del Bimbo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Recommending fashion items often leverages rich user profiles and makes
targeted suggestions based on past history and previous purchases. In this
paper, we work under the assumption that no prior knowledge is given about a
user. We propose to build a user profile on the fly by integrating user
reactions as we recommend complementary items to compose an outfit. We present
a reinforcement learning agent capable of suggesting appropriate garments and
ingesting user feedback so to improve its recommendations and maximize user
satisfaction. To train such a model, we resort to a proxy model to be able to
simulate having user feedback in the training loop. We experiment on the
IQON3000 fashion dataset and we find that a reinforcement learning-based agent
becomes capable of improving its recommendations by taking into account
personal preferences. Furthermore, such task demonstrated to be hard for
non-reinforcement models, that cannot exploit exploration during training.
</p>
</div>
</dd>
<dt><a name="item361">[361]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11628" title="Abstract">arXiv:2402.11628</a> [<a href="/pdf/2402.11628" title="Download PDF">pdf</a>, <a href="/ps/2402.11628" title="Download PostScript">ps</a>, <a href="/format/2402.11628" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discrete Neural Algorithmic Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rodionov%2C+G">Gleb Rodionov</a>, 
<a href="/search/cs?searchtype=author&query=Prokhorenkova%2C+L">Liudmila Prokhorenkova</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Neural algorithmic reasoning aims to capture computations with neural
networks via learning the models to imitate the execution of classical
algorithms. While common architectures are expressive enough to contain the
correct model in the weights space, current neural reasoners are struggling to
generalize well on out-of-distribution data. On the other hand, classical
computations are not affected by distribution shifts as they can be described
as transitions between discrete computational states. In this work, we propose
to force neural reasoners to maintain the execution trajectory as a combination
of finite predefined states. Trained with supervision on the algorithm's state
transitions, such models are able to perfectly align with the original
algorithm. To show this, we evaluate our approach on the SALSA-CLRS benchmark,
where we get perfect test scores for all tasks. Moreover, the proposed
architectural choice allows us to prove the correctness of the learned
algorithms for any test data.
</p>
</div>
</dd>
<dt><a name="item362">[362]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11631" title="Abstract">arXiv:2402.11631</a> [<a href="/pdf/2402.11631" title="Download PDF">pdf</a>, <a href="/format/2402.11631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Neuromorphic Face Analysis: a Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Becattini%2C+F">Federico Becattini</a>, 
<a href="/search/cs?searchtype=author&query=Berlincioni%2C+L">Lorenzo Berlincioni</a>, 
<a href="/search/cs?searchtype=author&query=Cultrera%2C+L">Luca Cultrera</a>, 
<a href="/search/cs?searchtype=author&query=Del+Bimbo%2C+A">Alberto Del Bimbo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Neuromorphic sensors, also known as event cameras, are a class of imaging
devices mimicking the function of biological visual systems. Unlike traditional
frame-based cameras, which capture fixed images at discrete intervals,
neuromorphic sensors continuously generate events that represent changes in
light intensity or motion in the visual field with high temporal resolution and
low latency. These properties have proven to be interesting in modeling human
faces, both from an effectiveness and a privacy-preserving point of view.
Neuromorphic face analysis however is still a raw and unstructured field of
research, with several attempts at addressing different tasks with no clear
standard or benchmark. This survey paper presents a comprehensive overview of
capabilities, challenges and emerging applications in the domain of
neuromorphic face analysis, to outline promising directions and open issues.
After discussing the fundamental working principles of neuromorphic vision and
presenting an in-depth overview of the related research, we explore the current
state of available data, standard data representations, emerging challenges,
and limitations that require further investigation. This paper aims to
highlight the recent process in this evolving field to provide to both
experienced and newly come researchers an all-encompassing analysis of the
state of the art along with its problems and shortcomings.
</p>
</div>
</dd>
<dt><a name="item363">[363]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11633" title="Abstract">arXiv:2402.11633</a> [<a href="/pdf/2402.11633" title="Download PDF">pdf</a>, <a href="/format/2402.11633" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-seeding and Multi-intent Self-instructing LLMs for Generating  Intent-aware Information-Seeking dialogs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Askari%2C+A">Arian Askari</a>, 
<a href="/search/cs?searchtype=author&query=Petcu%2C+R">Roxana Petcu</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+C">Chuan Meng</a>, 
<a href="/search/cs?searchtype=author&query=Aliannejadi%2C+M">Mohammad Aliannejadi</a>, 
<a href="/search/cs?searchtype=author&query=Abolghasemi%2C+A">Amin Abolghasemi</a>, 
<a href="/search/cs?searchtype=author&query=Kanoulas%2C+E">Evangelos Kanoulas</a>, 
<a href="/search/cs?searchtype=author&query=Verberne%2C+S">Suzan Verberne</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Identifying user intents in information-seeking dialogs is crucial for a
system to meet user's information needs. Intent prediction (IP) is challenging
and demands sufficient dialogs with human-labeled intents for training.
However, manually annotating intents is resource-intensive. While large
language models (LLMs) have been shown to be effective in generating synthetic
data, there is no study on using LLMs to generate intent-aware
information-seeking dialogs. In this paper, we focus on leveraging LLMs for
zero-shot generation of large-scale, open-domain, and intent-aware
information-seeking dialogs. We propose SOLID, which has novel self-seeding and
multi-intent self-instructing schemes. The former improves the generation
quality by using the LLM's own knowledge scope to initiate dialog generation;
the latter prompts the LLM to generate utterances sequentially, and mitigates
the need for manual prompt design by asking the LLM to autonomously adapt its
prompt instruction when generating complex multi-intent utterances.
Furthermore, we propose SOLID-RL, which is further trained to generate a dialog
in one step on the data generated by SOLID. We propose a length-based quality
estimation mechanism to assign varying weights to SOLID-generated dialogs based
on their quality during the training process of SOLID-RL. We use SOLID and
SOLID-RL to generate more than 300k intent-aware dialogs, surpassing the size
of existing datasets. Experiments show that IP methods trained on dialogs
generated by SOLID and SOLID-RL achieve better IP quality than ones trained on
human-generated dialogs.
</p>
</div>
</dd>
<dt><a name="item364">[364]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11635" title="Abstract">arXiv:2402.11635</a> [<a href="/pdf/2402.11635" title="Download PDF">pdf</a>, <a href="/format/2402.11635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tool-Augmented LLMs as a Universal Interface for IDEs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zharov%2C+Y">Yaroslav Zharov</a>, 
<a href="/search/cs?searchtype=author&query=Khudyakov%2C+Y">Yury Khudyakov</a>, 
<a href="/search/cs?searchtype=author&query=Fedotova%2C+E">Evgeniia Fedotova</a>, 
<a href="/search/cs?searchtype=author&query=Grigorenko%2C+E">Evgeny Grigorenko</a>, 
<a href="/search/cs?searchtype=author&query=Bogomolov%2C+E">Egor Bogomolov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First IDE Workshop, ICSE'24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Modern-day Integrated Development Environments (IDEs) have come a long way
from the early text editing utilities to the complex programs encompassing
thousands of functions to help developers. However, with the increasing number
of efficiency-enhancing tools incorporated, IDEs gradually became sophisticated
software with a steep learning curve. The rise of the Large Language Models
(LLMs) capable of both natural language dialogue and code generation leads to a
discourse on the obsolescence of the concept of IDE. In this work, we offer a
view on the place of the LLMs in the IDEs as the universal interface wrapping
the IDE facilities. We envision a model that is able to perform complex actions
involving multiple IDE features upon user command, stripping the user
experience of the tedious work involved in searching through options and
actions. For the practical part of the work, we engage with the works exploring
the ability of LLMs to call for external tools to expedite a given task
execution. We showcase a proof-of-concept of such a tool.
</p>
</div>
</dd>
<dt><a name="item365">[365]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11637" title="Abstract">arXiv:2402.11637</a> [<a href="/pdf/2402.11637" title="Download PDF">pdf</a>, <a href="/format/2402.11637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Poisoning Federated Recommender Systems with Fake Users
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+M">Ming Yin</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yichang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+M">Minghong Fang</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+N+Z">Neil Zhenqiang Gong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in The Web Conference 2024 (WWW '24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Information Retrieval (cs.IR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Federated recommendation is a prominent use case within federated learning,
yet it remains susceptible to various attacks, from user to server-side
vulnerabilities. Poisoning attacks are particularly notable among user-side
attacks, as participants upload malicious model updates to deceive the global
model, often intending to promote or demote specific targeted items. This study
investigates strategies for executing promotion attacks in federated
recommender systems.
<br />Current poisoning attacks on federated recommender systems often rely on
additional information, such as the local training data of genuine users or
item popularity. However, such information is challenging for the potential
attacker to obtain. Thus, there is a need to develop an attack that requires no
extra information apart from item embeddings obtained from the server. In this
paper, we introduce a novel fake user based poisoning attack named PoisonFRS to
promote the attacker-chosen targeted item in federated recommender systems
without requiring knowledge about user-item rating data, user attributes, or
the aggregation rule used by the server. Extensive experiments on multiple
real-world datasets demonstrate that PoisonFRS can effectively promote the
attacker-chosen targeted item to a large portion of genuine users and
outperform current benchmarks that rely on additional information about the
system. We further observe that the model updates from both genuine and fake
users are indistinguishable within the latent space.
</p>
</div>
</dd>
<dt><a name="item366">[366]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11638" title="Abstract">arXiv:2402.11638</a> [<a href="/pdf/2402.11638" title="Download PDF">pdf</a>, <a href="/format/2402.11638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stumbling Blocks: Stress Testing the Robustness of Machine-Generated  Text Detectors Under Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yichen Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shangbin Feng</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+A+B">Abe Bohan Hou</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+X">Xiao Pu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tsvetkov%2C+Y">Yulia Tsvetkov</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+T">Tianxing He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The widespread use of large language models (LLMs) is increasing the demand
for methods that detect machine-generated text to prevent misuse. The goal of
our study is to stress test the detectors' robustness to malicious attacks
under realistic scenarios. We comprehensively study the robustness of popular
machine-generated text detectors under attacks from diverse categories:
editing, paraphrasing, prompting, and co-generating. Our attacks assume limited
access to the generator LLMs, and we compare the performance of detectors on
different attacks under different budget levels. Our experiments reveal that
almost none of the existing detectors remain robust under all the attacks, and
all detectors exhibit different loopholes. Averaging all detectors, the
performance drops by 35% across all attacks. Further, we investigate the
reasons behind these defects and propose initial out-of-the-box patches to
improve robustness.
</p>
</div>
</dd>
<dt><a name="item367">[367]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11639" title="Abstract">arXiv:2402.11639</a> [<a href="/pdf/2402.11639" title="Download PDF">pdf</a>, <a href="/format/2402.11639" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Learning with Transformers: Softmax Attention Adapts to  Function Lipschitzness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Collins%2C+L">Liam Collins</a>, 
<a href="/search/cs?searchtype=author&query=Parulekar%2C+A">Advait Parulekar</a>, 
<a href="/search/cs?searchtype=author&query=Mokhtari%2C+A">Aryan Mokhtari</a>, 
<a href="/search/cs?searchtype=author&query=Sanghavi%2C+S">Sujay Sanghavi</a>, 
<a href="/search/cs?searchtype=author&query=Shakkottai%2C+S">Sanjay Shakkottai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">A striking property of transformers is their ability to perform in-context
learning (ICL), a machine learning framework in which the learner is presented
with a novel context during inference implicitly through some data, and tasked
with making a prediction in that context. As such that learner must adapt to
the context without additional training. We explore the role of softmax
attention in an ICL setting where each context encodes a regression task. We
show that an attention unit learns a window that it uses to implement a
nearest-neighbors predictor adapted to the landscape of the pretraining tasks.
Specifically, we show that this window widens with decreasing Lipschitzness and
increasing label noise in the pretraining tasks. We also show that on low-rank,
linear problems, the attention unit learns to project onto the appropriate
subspace before inference. Further, we show that this adaptivity relies
crucially on the softmax activation and thus cannot be replicated by the linear
activation often studied in prior theoretical analyses.
</p>
</div>
</dd>
<dt><a name="item368">[368]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11641" title="Abstract">arXiv:2402.11641</a> [<a href="/pdf/2402.11641" title="Download PDF">pdf</a>, <a href="/format/2402.11641" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Versatile Graph Learning Approach: from the Perspective of Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+L">Lanning Wei</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jun Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Huan Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Graph-structured data are the commonly used and have wide application
scenarios in the real world. For these diverse applications, the vast variety
of learning tasks, graph domains, and complex graph learning procedures present
challenges for human experts when designing versatile graph learning
approaches. Facing these challenges, large language models (LLMs) offer a
potential solution due to the extensive knowledge and the human-like
intelligence. This paper proposes a novel conceptual prototype for designing
versatile graph learning methods with LLMs, with a particular focus on the
``where'' and ``how'' perspectives. From the ``where'' perspective, we
summarize four key graph learning procedures, including task definition, graph
data feature engineering, model selection and optimization, deployment and
serving. We then explore the application scenarios of LLMs in these procedures
across a wider spectrum. In the ``how'' perspective, we align the abilities of
LLMs with the requirements of each procedure. Finally, we point out the
promising directions that could better leverage the strength of LLMs towards
versatile graph learning methods.
</p>
</div>
</dd>
<dt><a name="item369">[369]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11647" title="Abstract">arXiv:2402.11647</a> [<a href="/pdf/2402.11647" title="Download PDF">pdf</a>, <a href="/format/2402.11647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spectral Independence Beyond Uniqueness with. the topological method --  An extended view
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Efthymiou%2C+C">Charilaos Efthymiou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The manuscript also includes the results from <a href="/abs/2211.03753">arXiv:2211.03753</a> with shorter proofs
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>

</div>
<p class="mathjax">We present novel results for fast mixing of Glauber dynamics using the newly
introduced and powerful Spectral Independence method from [Anari, Liu,
Oveis-Gharan: FOCS 2020]. We mainly focus on the Hard-core model and the Ising
model. We obtain bounds for fast mixing with the parameters expressed in terms
of the spectral radius of the adjacency matrix, improving on the seminal work
in [Hayes: FOCS 2006]. Furthermore, we go beyond the adjacency matrix and
establish -- for the first time -- rapid mixing results for Glauber dynamics
expressed in terms of the spectral radius of the Hashimoto non-backtracking
matrix of the underlying graph $G$. Working with the non-backtracking spectrum
is extremely challenging, but also more desirable. Its eigenvalues are less
correlated with the high-degree vertices than those of the adjacency matrix and
express more accurately invariants of the graph such as the growth rate. Our
results require ``weak normality" from the Hashimoto matrix. This condition is
mild and allows us to obtain very interesting bound. We study the pairwise
influence matrix ${I}^{\Lambda,\tau}_{G}$ by exploiting the connection between
the matrix and the trees of self-avoiding walks, however, we go beyond the
standard treatment of the distributional recursions. The common framework that
underlies our techniques we call the topological method. Our approach is novel
and gives new insights into how to establish Spectral Independence for Gibbs
distributions. More importantly, it allows us to derive new -- improved --
rapid mixing bounds for Glauber dynamics on distributions such as the Hard-core
model and the Ising model for graphs that the spectral radius is smaller than
the maximum degree.
</p>
</div>
</dd>
<dt><a name="item370">[370]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11648" title="Abstract">arXiv:2402.11648</a> [<a href="/pdf/2402.11648" title="Download PDF">pdf</a>, <a href="/format/2402.11648" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Iterative Linear Quadratic Regulator With Variational Equation-Based  Discretization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Shigematsu%2C+K">Katsuya Shigematsu</a>, 
<a href="/search/eess?searchtype=author&query=Hoshino%2C+H">Hikaru Hoshino</a>, 
<a href="/search/eess?searchtype=author&query=Furutani%2C+E">Eiko Furutani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper discusses discretization methods for implementing nonlinear model
predictive controllers using Iterative Linear Quadratic Regulator (ILQR).
Finite-difference approximations are mostly used to derive a discrete-time
state equation from the original continuous-time model. However, the timestep
of the discretization is sometimes restricted to be small to suppress the
approximation error. In this paper, we propose to use the variational equation
for deriving linearizations of the discretized system required in ILQR
algorithms, which allows accurate computation regardless of the timestep.
Numerical simulations of the swing-up control of an inverted pendulum
demonstrate the effectiveness of this method. By the relaxing stringent
requirement for the size of the timestep, the use of the variational equation
can improve control performance by increasing the number of ILQR iterations
possible at each timestep in the realtime computation.
</p>
</div>
</dd>
<dt><a name="item371">[371]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11650" title="Abstract">arXiv:2402.11650</a> [<a href="/pdf/2402.11650" title="Download PDF">pdf</a>, <a href="/format/2402.11650" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Theoretical foundations for programmatic reinforcement learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shabadi%2C+G">Guruprerana Shabadi</a>, 
<a href="/search/cs?searchtype=author&query=Fijalkow%2C+N">Nathana&#xeb;l Fijalkow</a>, 
<a href="/search/cs?searchtype=author&query=Matricon%2C+T">Th&#xe9;o Matricon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Logic in Computer Science (cs.LO); Programming Languages (cs.PL)

</div>
<p class="mathjax">The field of Reinforcement Learning (RL) is concerned with algorithms for
learning optimal policies in unknown stochastic environments. Programmatic RL
studies representations of policies as programs, meaning involving higher order
constructs such as control loops. Despite attracting a lot of attention at the
intersection of the machine learning and formal methods communities, very
little is known on the theoretical front about programmatic RL: what are good
classes of programmatic policies? How large are optimal programmatic policies?
How can we learn them? The goal of this paper is to give first answers to these
questions, initiating a theoretical study of programmatic RL.
</p>
</div>
</dd>
<dt><a name="item372">[372]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11651" title="Abstract">arXiv:2402.11651</a> [<a href="/pdf/2402.11651" title="Download PDF">pdf</a>, <a href="/format/2402.11651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning From Failure: Integrating Negative Examples when Fine-tuning  Large Language Models as Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Renxi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haonan Li</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xudong Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yixuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Baldwin%2C+T">Timothy Baldwin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Agent, LLM, Large Language Model
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have achieved success in acting as agents, which
interact with environments through tools like search engines. However, LLMs are
not optimized specifically for tool use during training or alignment, limiting
their effectiveness as agents. To resolve this problem, previous work has
collected interaction trajectories between GPT-4 and environments, and
fine-tuned smaller models with them. As part of this, the standard approach has
been to simply discard trajectories that do not finish the task successfully,
which, on the one hand, leads to a significant waste of data and resources, and
on the other hand, has the potential to limit the possible optimization paths
during fine-tuning. In this paper, we contend that large language models can
learn from failures through appropriate data cleaning and fine-tuning
strategies. We conduct experiments on mathematical reasoning, multi-hop
question answering, and strategic question answering tasks. Experimental
results demonstrate that compared to solely using positive examples,
incorporating negative examples enhances model performance by a large margin.
</p>
</div>
</dd>
<dt><a name="item373">[373]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11653" title="Abstract">arXiv:2402.11653</a> [<a href="/pdf/2402.11653" title="Download PDF">pdf</a>, <a href="/format/2402.11653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combinatorial Client-Master Multiagent Deep Reinforcement Learning for  Task Offloading in Mobile Edge Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gebrekidan%2C+T+Z">Tesfay Zemuy Gebrekidan</a>, 
<a href="/search/cs?searchtype=author&query=Stein%2C+S">Sebastian Stein</a>, 
<a href="/search/cs?searchtype=author&query=Norman%2C+T+J">Timothy J.Norman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 5 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">Recently, there has been an explosion of mobile applications that perform
computationally intensive tasks such as video streaming, data mining, virtual
reality, augmented reality, image processing, video processing, face
recognition, and online gaming. However, user devices (UDs), such as tablets
and smartphones, have a limited ability to perform the computation needs of the
tasks. Mobile edge computing (MEC) has emerged as a promising technology to
meet the increasing computing demands of UDs. Task offloading in MEC is a
strategy that meets the demands of UDs by distributing tasks between UDs and
MEC servers. Deep reinforcement learning (DRL) is gaining attention in
task-offloading problems because it can adapt to dynamic changes and minimize
online computational complexity. However, the various types of continuous and
discrete resource constraints on UDs and MEC servers pose challenges to the
design of an efficient DRL-based task-offloading strategy. Existing DRL-based
task-offloading algorithms focus on the constraints of the UDs, assuming the
availability of enough storage resources on the server. Moreover, existing
multiagent DRL (MADRL)--based task-offloading algorithms are homogeneous agents
and consider homogeneous constraints as a penalty in their reward function. We
proposed a novel combinatorial client-master MADRL (CCM\_MADRL) algorithm for
task offloading in MEC (CCM\_MADRL\_MEC) that enables UDs to decide their
resource requirements and the server to make a combinatorial decision based on
the requirements of the UDs. CCM\_MADRL\_MEC is the first MADRL in task
offloading to consider server storage capacity in addition to the constraints
in the UDs. By taking advantage of the combinatorial action selection,
CCM\_MADRL\_MEC has shown superior convergence over existing MADDPG and
heuristic algorithms.
</p>
</div>
</dd>
<dt><a name="item374">[374]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11655" title="Abstract">arXiv:2402.11655</a> [<a href="/pdf/2402.11655" title="Download PDF">pdf</a>, <a href="/format/2402.11655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Competition of Mechanisms: Tracing How Language Models Handle Facts and  Counterfactuals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ortu%2C+F">Francesco Ortu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zhijing Jin</a>, 
<a href="/search/cs?searchtype=author&query=Doimo%2C+D">Diego Doimo</a>, 
<a href="/search/cs?searchtype=author&query=Sachan%2C+M">Mrinmaya Sachan</a>, 
<a href="/search/cs?searchtype=author&query=Cazzaniga%2C+A">Alberto Cazzaniga</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%B6lkopf%2C+B">Bernhard Sch&#xf6;lkopf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Interpretability research aims to bridge the gap between the empirical
success and our scientific understanding of the inner workings of large
language models (LLMs). However, most existing research in this area focused on
analyzing a single mechanism, such as how models copy or recall factual
knowledge. In this work, we propose the formulation of competition of
mechanisms, which instead of individual mechanisms focuses on the interplay of
multiple mechanisms, and traces how one of them becomes dominant in the final
prediction. We uncover how and where the competition of mechanisms happens
within LLMs using two interpretability methods, logit inspection and attention
modification. Our findings show traces of the mechanisms and their competition
across various model components, and reveal attention positions that
effectively control the strength of certain mechanisms. Our code and data are
at https://github.com/francescortu/Competition_of_Mechanisms.
</p>
</div>
</dd>
<dt><a name="item375">[375]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11656" title="Abstract">arXiv:2402.11656</a> [<a href="/pdf/2402.11656" title="Download PDF">pdf</a>, <a href="/ps/2402.11656" title="Download PostScript">ps</a>, <a href="/format/2402.11656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Pre-Trained Language Model with Physical Layer  Communications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Ju-Hyung Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Dong-Ho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Joohan Lee</a>, 
<a href="/search/cs?searchtype=author&query=Pujara%2C+J">Jay Pujara</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
<p class="mathjax">The burgeoning field of on-device AI communication, where devices exchange
information directly through embedded foundation models, such as language
models (LMs), requires robust, efficient, and generalizable communication
frameworks. However, integrating these frameworks with existing wireless
systems and effectively managing noise and bit errors pose significant
challenges. In this work, we introduce a practical on-device AI communication
framework, integrated with physical layer (PHY) communication functions,
demonstrated through its performance on a link-level simulator. Our framework
incorporates end-to-end training with channel noise to enhance resilience,
incorporates vector quantized variational autoencoders (VQ-VAE) for efficient
and robust communication, and utilizes pre-trained encoder-decoder transformers
for improved generalization capabilities. Simulations, across various
communication scenarios, reveal that our framework achieves a 50% reduction in
transmission size while demonstrating substantial generalization ability and
noise robustness under standardized 3GPP channel models.
</p>
</div>
</dd>
<dt><a name="item376">[376]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11658" title="Abstract">arXiv:2402.11658</a> [<a href="/pdf/2402.11658" title="Download PDF">pdf</a>, <a href="/format/2402.11658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic planning in hierarchical active inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Priorelli%2C+M">Matteo Priorelli</a>, 
<a href="/search/cs?searchtype=author&query=Stoianov%2C+I+P">Ivilin Peev Stoianov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Robotics (cs.RO)

</div>
<p class="mathjax">By dynamic planning, we refer to the ability of the human brain to infer and
impose motor trajectories related to cognitive decisions. A recent paradigm,
active inference, brings fundamental insights into the adaptation of biological
organisms, constantly striving to minimize prediction errors to restrict
themselves to life-compatible states. Over the past years, many studies have
shown how human and animal behavior could be explained in terms of an active
inferential process -- either as discrete decision-making or continuous motor
control -- inspiring innovative solutions in robotics and artificial
intelligence. Still, the literature lacks a comprehensive outlook on how to
effectively plan actions in changing environments. Setting ourselves the goal
of modeling tool use, we delve into the topic of dynamic planning in active
inference, keeping in mind two crucial aspects of biological goal-directed
behavior: the capacity to understand and exploit affordances for object
manipulation, and to learn the hierarchical interactions between the self and
the environment, including other agents. We start from a simple unit and
gradually describe more advanced structures, comparing recently proposed design
choices and providing basic examples for each section. This study distances
itself from traditional views centered on neural networks and reinforcement
learning, and points toward a yet unexplored direction in active inference:
hybrid representations in hierarchical models.
</p>
</div>
</dd>
<dt><a name="item377">[377]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11662" title="Abstract">arXiv:2402.11662</a> [<a href="/pdf/2402.11662" title="Download PDF">pdf</a>, <a href="/format/2402.11662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TDE-3: An improved prior for optical flow computation in spiking neural  networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yedutenko%2C+M">Matthew Yedutenko</a>, 
<a href="/search/cs?searchtype=author&query=Paredes-Valles%2C+F">Federico Paredes-Valles</a>, 
<a href="/search/cs?searchtype=author&query=Khacef%2C+L">Lyes Khacef</a>, 
<a href="/search/cs?searchtype=author&query=De+Croon%2C+G+C+H+E">Guido C.H.E. De Croon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
<p class="mathjax">Motion detection is a primary task required for robotic systems to perceive
and navigate in their environment. Proposed in the literature bioinspired
neuromorphic Time-Difference Encoder (TDE-2) combines event-based sensors and
processors with spiking neural networks to provide real-time and
energy-efficient motion detection through extracting temporal correlations
between two points in space. However, on the algorithmic level, this design
leads to loss of direction-selectivity of individual TDEs in textured
environments. Here we propose an augmented 3-point TDE (TDE-3) with additional
inhibitory input that makes TDE-3 direction-selectivity robust in textured
environments. We developed a procedure to train the new TDE-3 using
backpropagation through time and surrogate gradients to linearly map input
velocities into an output spike count or an Inter-Spike Interval (ISI). Our
work is the first instance of training a spiking neuron to have a specific ISI.
Using synthetic data we compared training and inference with spike count and
ISI with respect to changes in stimuli dynamic range, spatial frequency, and
level of noise. ISI turns out to be more robust towards variation in spatial
frequency, whereas the spike count is a more reliable training signal in the
presence of noise. We performed the first in-depth quantitative investigation
of optical flow coding with TDE and compared TDE-2 vs TDE-3 in terms of
energy-efficiency and coding precision. Results show that on the network level
both detectors show similar precision (20 degree angular error, 88% correlation
with ground truth). Yet, due to the more robust direction-selectivity of
individual TDEs, TDE-3 based network spike less and hence is more
energy-efficient. Reported precision is on par with model-based methods but the
spike-based processing of the TDEs provides allows more energy-efficient
inference with neuromorphic hardware.
</p>
</div>
</dd>
<dt><a name="item378">[378]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11664" title="Abstract">arXiv:2402.11664</a> [<a href="/pdf/2402.11664" title="Download PDF">pdf</a>, <a href="/format/2402.11664" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Short-Term Load Forecasting via Multi-Scale Temporal  Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuqi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yan Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yize Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to 23rd Power Systems Computation Conference (PSCC); cross referenced in Electric Power Systems Research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Rapid progress in machine learning and deep learning has enabled a wide range
of applications in the electricity load forecasting of power systems, for
instance, univariate and multivariate short-term load forecasting. Though the
strong capabilities of learning the non-linearity of the load patterns and the
high prediction accuracy have been achieved, the interpretability of typical
deep learning models for electricity load forecasting is less studied. This
paper proposes an interpretable deep learning method, which learns a linear
combination of neural networks that each attends to an input time feature. We
also proposed a multi-scale time series decomposition method to deal with the
complex time patterns. Case studies have been carried out on the Belgium
central grid load dataset and the proposed model demonstrated better accuracy
compared to the frequently applied baseline model. Specifically, the proposed
multi-scale temporal decomposition achieves the best MSE, MAE and RMSE of 0.52,
0.57 and 0.72 respectively. As for interpretability, on one hand, the proposed
method displays generalization capability. On the other hand, it can
demonstrate not only the feature but also the temporal interpretability
compared to other baseline methods. Besides, the global time feature
interpretabilities are also obtained. Obtaining global feature
interpretabilities allows us to catch the overall patterns, trends, and
cyclicality in load data while also revealing the significance of various
time-related features in forming the final outputs.
</p>
</div>
</dd>
<dt><a name="item379">[379]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11666" title="Abstract">arXiv:2402.11666</a> [<a href="/pdf/2402.11666" title="Download PDF">pdf</a>, <a href="/format/2402.11666" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Specifying and Analyzing Networked and Layered Control Systems Operating  on Multiple Clocks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Incer%2C+I">Inigo Incer</a>, 
<a href="/search/eess?searchtype=author&query=Csomay-Shanklin%2C+N">Noel Csomay-Shanklin</a>, 
<a href="/search/eess?searchtype=author&query=Ames%2C+A">Aaron Ames</a>, 
<a href="/search/eess?searchtype=author&query=Murray%2C+R+M">Richard M. Murray</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">We consider the problem of reasoning about networked and layered control
systems using assume-guarantee specifications. As these systems are formed by
the interconnection of components that operate under various clocks, we
introduce a new logic, Multiclock Logic (MCL), to be able to express the
requirements of components form the point of view of their local clocks.
Specifying components locally promotes independent design and component reuse.
We carry out a contract-based analysis of a control system implemented via two
control algorithms (model predictive control and feedback linearization)
running on their own processors and clocks. Then we implement each of the
contracts to build a system. The system performs as desired when the
requirements derived from our system-level analysis are respected. Violating
the constraints required by the contract-based analysis of the system leads to
error.
</p>
</div>
</dd>
<dt><a name="item380">[380]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11670" title="Abstract">arXiv:2402.11670</a> [<a href="/pdf/2402.11670" title="Download PDF">pdf</a>, <a href="/format/2402.11670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Challenging the Black Box: A Comprehensive Evaluation of Attribution  Maps of CNN Applications in Agriculture and Forestry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nieradzik%2C+L">Lars Nieradzik</a>, 
<a href="/search/cs?searchtype=author&query=Stephani%2C+H">Henrike Stephani</a>, 
<a href="/search/cs?searchtype=author&query=Sieburg-Rockel%2C+J">J&#xf6;rdis Sieburg-Rockel</a>, 
<a href="/search/cs?searchtype=author&query=Helmling%2C+S">Stephanie Helmling</a>, 
<a href="/search/cs?searchtype=author&query=Olbrich%2C+A">Andrea Olbrich</a>, 
<a href="/search/cs?searchtype=author&query=Keuper%2C+J">Janis Keuper</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 19th International Joint Conference on Computer
  Vision, Imaging and Computer Graphics Theory and Applications - Volume 2:
  VISAPP, 2024, pp. 483-492
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this study, we explore the explainability of neural networks in
agriculture and forestry, specifically in fertilizer treatment classification
and wood identification. The opaque nature of these models, often considered
'black boxes', is addressed through an extensive evaluation of state-of-the-art
Attribution Maps (AMs), also known as class activation maps (CAMs) or saliency
maps. Our comprehensive qualitative and quantitative analysis of these AMs
uncovers critical practical limitations. Findings reveal that AMs frequently
fail to consistently highlight crucial features and often misalign with the
features considered important by domain experts. These discrepancies raise
substantial questions about the utility of AMs in understanding the
decision-making process of neural networks. Our study provides critical
insights into the trustworthiness and practicality of AMs within the
agriculture and forestry sectors, thus facilitating a better understanding of
neural networks in these application areas.
</p>
</div>
</dd>
<dt><a name="item381">[381]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11671" title="Abstract">arXiv:2402.11671</a> [<a href="/pdf/2402.11671" title="Download PDF">pdf</a>, <a href="/ps/2402.11671" title="Download PostScript">ps</a>, <a href="/format/2402.11671" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autocorrect for Estonian texts: final report from project EKTB25
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luhtaru%2C+A">Agnes Luhtaru</a>, 
<a href="/search/cs?searchtype=author&query=Vainikko%2C+M">Martin Vainikko</a>, 
<a href="/search/cs?searchtype=author&query=Liin%2C+K">Krista Liin</a>, 
<a href="/search/cs?searchtype=author&query=Allkivi-Metsoja%2C+K">Kais Allkivi-Metsoja</a>, 
<a href="/search/cs?searchtype=author&query=Kippar%2C+J">Jaagup Kippar</a>, 
<a href="/search/cs?searchtype=author&query=Eslon%2C+P">Pille Eslon</a>, 
<a href="/search/cs?searchtype=author&query=Fishel%2C+M">Mark Fishel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> in Estonian language
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The project was funded in 2021-2023 by the National Programme of Estonian
Language Technology. Its main aim was to develop spelling and grammar
correction tools for the Estonian language. The main challenge was the very
small amount of available error correction data needed for such development. To
mitigate this, (1) we annotated more correction data for model training and
testing, (2) we tested transfer-learning, i.e. retraining machine learning
models created for other tasks, so as not to depend solely on correction data,
(3) we compared the developed method and model with alternatives, including
large language models. We also developed automatic evaluation, which can
calculate the accuracy and yield of corrections by error category, so that the
effectiveness of different methods can be compared in detail.
<br />There has been a breakthrough in large language models during the project:
GPT4, a commercial language model with Estonian-language support, has been
created. We took into account the existence of the model when adjusting plans
and in the report we present a comparison with the ability of GPT4 to improve
the Estonian language text.
<br />The final results show that the approach we have developed provides better
scores than GPT4 and the result is usable but not entirely reliable yet. The
report also contains ideas on how GPT4 and other major language models can be
implemented in the future, focusing on open-source solutions.
<br />All results of this project are open-data/open-source, with licenses that
allow them to be used for purposes including commercial ones.
</p>
</div>
</dd>
<dt><a name="item382">[382]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11674" title="Abstract">arXiv:2402.11674</a> [<a href="/pdf/2402.11674" title="Download PDF">pdf</a>, <a href="/format/2402.11674" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Fast Algorithm to Simulate Nonlinear Resistive Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Scellier%2C+B">Benjamin Scellier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Emerging Technologies (cs.ET)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG)

</div>
<p class="mathjax">In the quest for energy-efficient artificial intelligence systems, resistor
networks are attracting interest as an alternative to conventional GPU-based
neural networks. These networks leverage the physics of electrical circuits for
inference and can be optimized with local training techniques such as
equilibrium propagation. Despite their potential advantage in terms of power
consumption, the challenge of efficiently simulating these resistor networks
has been a significant bottleneck to assess their scalability, with current
methods either being limited to linear networks or relying on realistic, yet
slow circuit simulators like SPICE. Assuming ideal circuit elements, we
introduce a novel approach for the simulation of nonlinear resistive networks,
which we frame as a quadratic programming problem with linear inequality
constraints, and which we solve using a fast, exact coordinate descent
algorithm. Our simulation methodology significantly outperforms existing
SPICE-based simulations, enabling the training of networks up to 325 times
larger at speeds 150 times faster, resulting in a 50,000-fold improvement in
the ratio of network size to epoch duration. Our approach, adaptable to other
electrical components, can foster more rapid progress in the simulations of
nonlinear electrical networks.
</p>
</div>
</dd>
<dt><a name="item383">[383]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11676" title="Abstract">arXiv:2402.11676</a> [<a href="/pdf/2402.11676" title="Download PDF">pdf</a>, <a href="/format/2402.11676" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multi-Aspect Framework for Counter Narrative Evaluation using Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jones%2C+J">Jaylen Jones</a>, 
<a href="/search/cs?searchtype=author&query=Mo%2C+L">Lingbo Mo</a>, 
<a href="/search/cs?searchtype=author&query=Fosler-Lussier%2C+E">Eric Fosler-Lussier</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Huan Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Counter narratives - informed responses to hate speech contexts designed to
refute hateful claims and de-escalate encounters - have emerged as an effective
hate speech intervention strategy. While previous work has proposed automatic
counter narrative generation methods to aid manual interventions, the
evaluation of these approaches remains underdeveloped. Previous automatic
metrics for counter narrative evaluation lack alignment with human judgment as
they rely on superficial reference comparisons instead of incorporating key
aspects of counter narrative quality as evaluation criteria. To address prior
evaluation limitations, we propose a novel evaluation framework prompting LLMs
to provide scores and feedback for generated counter narrative candidates using
5 defined aspects derived from guidelines from counter narrative specialized
NGOs. We found that LLM evaluators achieve strong alignment to human-annotated
scores and feedback and outperform alternative metrics, indicating their
potential as multi-aspect, reference-free and interpretable evaluators for
counter narrative evaluation.
</p>
</div>
</dd>
<dt><a name="item384">[384]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11677" title="Abstract">arXiv:2402.11677</a> [<a href="/pdf/2402.11677" title="Download PDF">pdf</a>, <a href="/format/2402.11677" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MultiCorrupt: A Multi-Modal Robustness Dataset and Benchmark of  LiDAR-Camera Fusion for 3D Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beemelmanns%2C+T">Till Beemelmanns</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Quan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Eckstein%2C+L">Lutz Eckstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code: <a href="https://github.com/ika-rwth-aachen/MultiCorrupt">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Multi-modal 3D object detection models for automated driving have
demonstrated exceptional performance on computer vision benchmarks like
nuScenes. However, their reliance on densely sampled LiDAR point clouds and
meticulously calibrated sensor arrays poses challenges for real-world
applications. Issues such as sensor misalignment, miscalibration, and disparate
sampling frequencies lead to spatial and temporal misalignment in data from
LiDAR and cameras. Additionally, the integrity of LiDAR and camera data is
often compromised by adverse environmental conditions such as inclement
weather, leading to occlusions and noise interference. To address this
challenge, we introduce MultiCorrupt, a comprehensive benchmark designed to
evaluate the robustness of multi-modal 3D object detectors against ten distinct
types of corruptions. We evaluate five state-of-the-art multi-modal detectors
on MultiCorrupt and analyze their performance in terms of their resistance
ability. Our results show that existing methods exhibit varying degrees of
robustness depending on the type of corruption and their fusion strategy. We
provide insights into which multi-modal design choices make such models robust
against certain perturbations. The dataset generation code and benchmark are
open-sourced at https://github.com/ika-rwth-aachen/MultiCorrupt.
</p>
</div>
</dd>
<dt><a name="item385">[385]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11680" title="Abstract">arXiv:2402.11680</a> [<a href="/pdf/2402.11680" title="Download PDF">pdf</a>, <a href="/format/2402.11680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D Point Cloud Compression with Recurrent Neural Network and Image  Compression Methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beemelmanns%2C+T">Till Beemelmanns</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+Y">Yuchen Tao</a>, 
<a href="/search/cs?searchtype=author&query=Lampe%2C+B">Bastian Lampe</a>, 
<a href="/search/cs?searchtype=author&query=Reiher%2C+L">Lennart Reiher</a>, 
<a href="/search/cs?searchtype=author&query=van+Kempen%2C+R">Raphael van Kempen</a>, 
<a href="/search/cs?searchtype=author&query=Woopen%2C+T">Timo Woopen</a>, 
<a href="/search/cs?searchtype=author&query=Eckstein%2C+L">Lutz Eckstein</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code: <a href="https://github.com/ika-rwth-aachen/Point-Cloud-Compression">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2022 IEEE Intelligent Vehicles Symposium (IV)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">Storing and transmitting LiDAR point cloud data is essential for many AV
applications, such as training data collection, remote control, cloud services
or SLAM. However, due to the sparsity and unordered structure of the data, it
is difficult to compress point cloud data to a low volume. Transforming the raw
point cloud data into a dense 2D matrix structure is a promising way for
applying compression algorithms. We propose a new lossless and calibrated
3D-to-2D transformation which allows compression algorithms to efficiently
exploit spatial correlations within the 2D representation. To compress the
structured representation, we use common image compression methods and also a
self-supervised deep compression approach using a recurrent neural network. We
also rearrange the LiDAR's intensity measurements to a dense 2D representation
and propose a new metric to evaluate the compression performance of the
intensity. Compared to approaches that are based on generic octree point cloud
compression or based on raw point cloud data compression, our approach achieves
the best quantitative and visual performance. Source code and dataset are
available at https://github.com/ika-rwth-aachen/Point-Cloud-Compression.
</p>
</div>
</dd>
<dt><a name="item386">[386]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11681" title="Abstract">arXiv:2402.11681</a> [<a href="/pdf/2402.11681" title="Download PDF">pdf</a>, <a href="/format/2402.11681" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Opening the black box of language acquisition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Michaud%2C+J">J&#xe9;r&#xf4;me Michaud</a>, 
<a href="/search/cs?searchtype=author&query=Jon-and%2C+A">Anna Jon-and</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">Recent advances in large language models using deep learning techniques have
renewed interest on how languages can be learned from data. However, it is
unclear whether or how these models represent grammatical information from the
learned languages. In addition, the models must be pre-trained on large corpora
before they can be used. In this work, we propose an alternative, more
transparent and cognitively plausible architecture for learning language.
Instead of using deep learning, our approach uses a minimal cognitive
architecture based on sequence memory and chunking. The learning mechanism is
based on the principles of reinforcement learning. We test our architecture on
a number of natural-like toy languages. Results show that the model can learn
these artificial languages from scratch and extract grammatical information
that supports learning. Our study demonstrates the power of this simple
architecture and stresses the importance of sequence memory as a key component
of the language learning process. Since other animals do not seem to have a
faithful sequence memory, this may explain why only humans have developed
complex languages.
</p>
</div>
</dd>
<dt><a name="item387">[387]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11682" title="Abstract">arXiv:2402.11682</a> [<a href="/pdf/2402.11682" title="Download PDF">pdf</a>, <a href="/format/2402.11682" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Conditional Invariances through Non-Commutativity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chaudhuri%2C+A">Abhra Chaudhuri</a>, 
<a href="/search/cs?searchtype=author&query=Georgescu%2C+S">Serban Georgescu</a>, 
<a href="/search/cs?searchtype=author&query=Dutta%2C+A">Anjan Dutta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> International Conference on Learning Representations (ICLR) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Invariance learning algorithms that conditionally filter out domain-specific
random variables as distractors, do so based only on the data semantics, and
not the target domain under evaluation. We show that a provably optimal and
sample-efficient way of learning conditional invariances is by relaxing the
invariance criterion to be non-commutatively directed towards the target
domain. Under domain asymmetry, i.e., when the target domain contains
semantically relevant information absent in the source, the risk of the encoder
$\varphi^*$ that is optimal on average across domains is strictly lower-bounded
by the risk of the target-specific optimal encoder $\Phi^*_\tau$. We prove that
non-commutativity steers the optimization towards $\Phi^*_\tau$ instead of
$\varphi^*$, bringing the $\mathcal{H}$-divergence between domains down to
zero, leading to a stricter bound on the target risk. Both our theory and
experiments demonstrate that non-commutative invariance (NCI) can leverage
source domain samples to meet the sample complexity needs of learning
$\Phi^*_\tau$, surpassing SOTA invariance learning algorithms for domain
adaptation, at times by over $2\%$, approaching the performance of an oracle.
Implementation is available at https://github.com/abhrac/nci.
</p>
</div>
</dd>
<dt><a name="item388">[388]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11683" title="Abstract">arXiv:2402.11683</a> [<a href="/pdf/2402.11683" title="Download PDF">pdf</a>, <a href="/format/2402.11683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Siledar%2C+T">Tejpalsingh Siledar</a>, 
<a href="/search/cs?searchtype=author&query=Nath%2C+S">Swaroop Nath</a>, 
<a href="/search/cs?searchtype=author&query=Muddu%2C+S+S+R+R">Sankara Sri Raghava Ravindra Muddu</a>, 
<a href="/search/cs?searchtype=author&query=Rangaraju%2C+R">Rupasai Rangaraju</a>, 
<a href="/search/cs?searchtype=author&query=Nath%2C+S">Swaprava Nath</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharyya%2C+P">Pushpak Bhattacharyya</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+S">Suman Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Patil%2C+A">Amey Patil</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S+S">Sudhanshu Shekhar Singh</a>, 
<a href="/search/cs?searchtype=author&query=Chelliah%2C+M">Muthusamy Chelliah</a>, 
<a href="/search/cs?searchtype=author&query=Garera%2C+N">Nikesh Garera</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Evaluation of opinion summaries using conventional reference-based metrics
rarely provides a holistic evaluation and has been shown to have a relatively
low correlation with human judgments. Recent studies suggest using Large
Language Models (LLMs) as reference-free metrics for NLG evaluation, however,
they remain unexplored for opinion summary evaluation. Moreover, limited
opinion summary evaluation datasets inhibit progress. To address this, we
release the SUMMEVAL-OP dataset covering 7 dimensions related to the evaluation
of opinion summaries: fluency, coherence, relevance, faithfulness, aspect
coverage, sentiment consistency, and specificity. We investigate Op-I-Prompt a
dimension-independent prompt, and Op-Prompts, a dimension-dependent set of
prompts for opinion summary evaluation. Experiments indicate that Op-I-Prompt
emerges as a good alternative for evaluating opinion summaries achieving an
average Spearman correlation of 0.70 with humans, outperforming all previous
approaches. To the best of our knowledge, we are the first to investigate LLMs
as evaluators on both closed-source and open-source models in the opinion
summarization domain.
</p>
</div>
</dd>
<dt><a name="item389">[389]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11684" title="Abstract">arXiv:2402.11684</a> [<a href="/pdf/2402.11684" title="Download PDF">pdf</a>, <a href="/format/2402.11684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language  Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+G+H">Guiming Hardy Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shunian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruifei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Junying Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiangbo Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhihong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianquan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiang Wan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Benyou Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advancements in Large Vision-Language Models (LVLMs) have enabled
processing of multimodal inputs in language models but require significant
computational resources for deployment, especially in edge devices. This study
aims to bridge the performance gap between traditional-scale LVLMs and
resource-friendly lite versions by adopting high-quality training data. To do
this, a synthetic dataset is created by leveraging GPT-4V's ability to generate
detailed captions, complex reasoning instructions and detailed answers from
images. The resulted model trained with our data, ALLaVA, achieves competitive
performance on 12 benchmarks up to 3B LVLMs. This work highlights the
feasibility of adopting high-quality data in crafting more efficient LVLMs. Our
online demo is available at \url{https://allava.freedomai.cn}.
</p>
</div>
</dd>
<dt><a name="item390">[390]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11685" title="Abstract">arXiv:2402.11685</a> [<a href="/pdf/2402.11685" title="Download PDF">pdf</a>, <a href="/ps/2402.11685" title="Download PostScript">ps</a>, <a href="/format/2402.11685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variability-Aware Noise-Induced Dynamic Instability of Ultra-Low-Voltage  SRAM Bitcells
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Van+Brandt%2C+L">L&#xe9;opold Van Brandt</a>, 
<a href="/search/cs?searchtype=author&query=Delvenne%2C+J">Jean-Charles Delvenne</a>, 
<a href="/search/cs?searchtype=author&query=Flandre%2C+D">Denis Flandre</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted proceeding of IEEE LASCAS 2024, Punta del Este, Uruguay
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">Stability of ultra-low-voltage SRAM bitcells in retention mode is threatened
by two types of uncertainty: process variability and intrinsic noise. While
variability dominates the failure probability, noise-induced bit flips in
weakened bitcells lead to dynamic instability. We study both effects jointly in
a unified SPICE simulation framework. Starting from a synthetic representation
of process variations introduced in a previous work, we identify the cases of
poor noise immunity that require thorough noise analyses. Relying on a rigorous
and systematic methodology, we simulate them in the time domain so as to
emulate a true data retention operation. Short times to failure, unacceptable
for a practical ultra-low-power memory system application, are recorded. The
transient bit-flip mechanism is analysed and a dynamic failure criterion
involving the unstable point is established. We conclude that, beyond static
variability, the dynamic noise inflates defectiveness among SRAM bitcells. We
also discuss the limits of existing analytical formulas from the literature,
which rely on a linear near-equilibrium approximation of the SRAM dynamics to,
inaccurately, predict the mean time to failure.
</p>
</div>
</dd>
<dt><a name="item391">[391]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11686" title="Abstract">arXiv:2402.11686</a> [<a href="/pdf/2402.11686" title="Download PDF">pdf</a>, <a href="/format/2402.11686" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning the Topology and Behavior of Discrete Dynamical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Z">Zirou Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Adiga%2C+A">Abhijin Adiga</a>, 
<a href="/search/cs?searchtype=author&query=Marathe%2C+M+V">Madhav V. Marathe</a>, 
<a href="/search/cs?searchtype=author&query=Ravi%2C+S+S">S. S. Ravi</a>, 
<a href="/search/cs?searchtype=author&query=Rosenkrantz%2C+D+J">Daniel J. Rosenkrantz</a>, 
<a href="/search/cs?searchtype=author&query=Stearns%2C+R+E">Richard E. Stearns</a>, 
<a href="/search/cs?searchtype=author&query=Vullikanti%2C+A">Anil Vullikanti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAAI-24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Discrete dynamical systems are commonly used to model the spread of
contagions on real-world networks. Under the PAC framework, existing research
has studied the problem of learning the behavior of a system, assuming that the
underlying network is known. In this work, we focus on a more challenging
setting: to learn both the behavior and the underlying topology of a black-box
system. We show that, in general, this learning problem is computationally
intractable. On the positive side, we present efficient learning methods under
the PAC model when the underlying graph of the dynamical system belongs to some
classes. Further, we examine a relaxed setting where the topology of an unknown
system is partially observed. For this case, we develop an efficient PAC
learner to infer the system and establish the sample complexity. Lastly, we
present a formal analysis of the expressive power of the hypothesis class of
dynamical systems where both the topology and behavior are unknown, using the
well-known formalism of the Natarajan dimension. Our results provide a
theoretical foundation for learning both the behavior and topology of discrete
dynamical systems.
</p>
</div>
</dd>
<dt><a name="item392">[392]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11690" title="Abstract">arXiv:2402.11690</a> [<a href="/pdf/2402.11690" title="Download PDF">pdf</a>, <a href="/format/2402.11690" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhiyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Chao Feng</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+R">Rulin Shao</a>, 
<a href="/search/cs?searchtype=author&query=Ashby%2C+T">Trevor Ashby</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Ying Shen</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+D">Di Jin</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yu Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qifan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Lifu Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 Pages, visual instruction tuning
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Despite vision-language models' (VLMs) remarkable capabilities as versatile
visual assistants, two substantial challenges persist within the existing VLM
frameworks: (1) lacking task diversity in pretraining and visual instruction
tuning, and (2) annotation error and bias in GPT-4 synthesized instruction
tuning data. Both challenges lead to issues such as poor generalizability,
hallucination, and catastrophic forgetting. To address these challenges, we
construct Vision-Flan, the most diverse publicly available visual instruction
tuning dataset to date, comprising 187 diverse tasks and 1,664,261 instances
sourced from academic datasets, and each task is accompanied by an
expert-written instruction. In addition, we propose a two-stage instruction
tuning framework, in which VLMs are firstly finetuned on Vision-Flan and
further tuned on GPT-4 synthesized data. We find this two-stage tuning
framework significantly outperforms the traditional single-stage visual
instruction tuning framework and achieves the state-of-the-art performance
across a wide range of multi-modal evaluation benchmarks. Finally, we conduct
in-depth analyses to understand visual instruction tuning and our findings
reveal that: (1) GPT-4 synthesized data does not substantially enhance VLMs'
capabilities but rather modulates the model's responses to human-preferred
formats; (2) A minimal quantity (e.g., 1,000) of GPT-4 synthesized data can
effectively align VLM responses with human-preference; (3) Visual instruction
tuning mainly helps large-language models (LLMs) to understand visual features.
</p>
</div>
</dd>
<dt><a name="item393">[393]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11691" title="Abstract">arXiv:2402.11691</a> [<a href="/pdf/2402.11691" title="Download PDF">pdf</a>, <a href="/ps/2402.11691" title="Download PostScript">ps</a>, <a href="/format/2402.11691" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Nonlinear Dynamical Modelling of SRAM Bitcells in Retention  Mode
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Van+Brandt%2C+L">L&#xe9;opold Van Brandt</a>, 
<a href="/search/cs?searchtype=author&query=Flandre%2C+D">Denis Flandre</a>, 
<a href="/search/cs?searchtype=author&query=Delvenne%2C+J">Jean-Charles Delvenne</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted proceeding for invited talk at IEEE EDTM 2024, Bangalore, India
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
<p class="mathjax">SRAM bitcells in retention mode behave as autonomous stochastic nonlinear
dynamical systems. From observation of variability-aware transient noise
simulations, we provide an unidimensional model, fully characterizable by
conventional deterministic SPICE simulations, insightfully explaining the
mechanism of intrinsic noise-induced bit flips. The proposed model is exploited
to, first, explain the reported inaccuracy of existing closed-form
near-equilibrium formulas aimed at predicting the mean time to failure and,
secondly, to propose a closer estimate attractive in terms of CPU time.
</p>
</div>
</dd>
<dt><a name="item394">[394]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11695" title="Abstract">arXiv:2402.11695</a> [<a href="/pdf/2402.11695" title="Download PDF">pdf</a>, <a href="/ps/2402.11695" title="Download PostScript">ps</a>, <a href="/format/2402.11695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enabling Software Defined Optical Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Panchal%2C+D">Deven Panchal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Computers and Society (cs.CY); Emerging Technologies (cs.ET)

</div>
<p class="mathjax">This paper gives an overview of Software Defined Optical Networks or SDONs
and how they can be implemented. It traces the evolution of Optical networks
upto GMPLS and traces the idea of SDN and builds upto OpenFlow. The paper
explores the need for SDONs and explains what a SDON solution could look like,
including the hardware. It also seeks to explain how OpenFlow could be used as
a part of this solution to overcome the limitations of GMPLS.
</p>
</div>
</dd>
<dt><a name="item395">[395]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11698" title="Abstract">arXiv:2402.11698</a> [<a href="/pdf/2402.11698" title="Download PDF">pdf</a>, <a href="/ps/2402.11698" title="Download PostScript">ps</a>, <a href="/format/2402.11698" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 5G Cellular -- An Energy Efficiency Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Panchal%2C+D">Deven Panchal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Computers and Society (cs.CY); Performance (cs.PF)

</div>
<p class="mathjax">While the 5G technology of cellular communications promises great capacity
and coverage to access information anywhere and anytime, it is feared to have
huge power consumption. Significant research been has been directed towards
solving this problem which exists both on the subscribers side as well as the
operators side. There have been efforts like predicting traffic, modifying the
physical layer etc. towards making the 5G technology more energy efficient. The
aim of this study is to see the technology enablers for 5G from an energy
efficiency perspective. Efforts will be made to point out specific areas in 5G
cellular where improvements or modifications could make 5G cellular more energy
efficient.
</p>
</div>
</dd>
<dt><a name="item396">[396]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11700" title="Abstract">arXiv:2402.11700</a> [<a href="/pdf/2402.11700" title="Download PDF">pdf</a>, <a href="/format/2402.11700" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Why Lift so Heavy? Slimming Large Language Models by Cutting Off the  Layers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Shuzhou Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+E">Ercong Nie</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+B">Bolei Ma</a>, 
<a href="/search/cs?searchtype=author&query=F%C3%A4rber%2C+M">Michael F&#xe4;rber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) possess outstanding capabilities in addressing
various natural language processing (NLP) tasks. However, the sheer size of
these models poses challenges in terms of storage, training and inference due
to the inclusion of billions of parameters through layer stacking. While
traditional approaches such as model pruning or distillation offer ways for
reducing model size, they often come at the expense of performance retention.
In our investigation, we systematically explore the approach of reducing the
number of layers in LLMs. Surprisingly, we observe that even with fewer layers,
LLMs maintain similar or better performance levels, particularly in
prompt-based fine-tuning for text classification tasks. Remarkably, in certain
cases, models with a single layer outperform their fully layered counterparts.
These findings offer valuable insights for future work aimed at mitigating the
size constraints of LLMs while preserving their performance, thereby opening
avenues for significantly more efficient use of LLMs.
</p>
</div>
</dd>
<dt><a name="item397">[397]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11702" title="Abstract">arXiv:2402.11702</a> [<a href="/pdf/2402.11702" title="Download PDF">pdf</a>, <a href="/format/2402.11702" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can ChatGPT Support Developers? An Empirical Evaluation of Large  Language Models for Code Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+K">Kailun Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chung-Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+H+V">Hung Viet Pham</a>, 
<a href="/search/cs?searchtype=author&query=Hemmati%2C+H">Hadi Hemmati</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4 pages, 3 figures, 21st International Conference on Mining Software Repositories (MSR '24), April 15-16, 2024, Lisbon, Portugal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large language models (LLMs) have demonstrated notable proficiency in code
generation, with numerous prior studies showing their promising capabilities in
various development scenarios. However, these studies mainly provide
evaluations in research settings, which leaves a significant gap in
understanding how effectively LLMs can support developers in real-world. To
address this, we conducted an empirical analysis of conversations in DevGPT, a
dataset collected from developers' conversations with ChatGPT (captured with
the Share Link feature on platforms such as GitHub). Our empirical findings
indicate that the current practice of using LLM-generated code is typically
limited to either demonstrating high-level concepts or providing examples in
documentation, rather than to be used as production-ready code. These findings
indicate that there is much future work needed to improve LLMs in code
generation before they can be integral parts of modern software development.
</p>
</div>
</dd>
<dt><a name="item398">[398]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11707" title="Abstract">arXiv:2402.11707</a> [<a href="/pdf/2402.11707" title="Download PDF">pdf</a>, <a href="/format/2402.11707" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Search Engines Post-ChatGPT: How Generative Artificial Intelligence  Could Make Search Less Reliable
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Memon%2C+S+A">Shahan Ali Memon</a>, 
<a href="/search/cs?searchtype=author&query=West%2C+J+D">Jevin D. West</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">In this commentary, we discuss the evolving nature of search engines, as they
begin to generate, index, and distribute content created by generative
artificial intelligence (GenAI). Our discussion highlights challenges in the
early stages of GenAI integration, particularly around factual inconsistencies
and biases. We discuss how output from GenAI carries an unwarranted sense of
credibility, while decreasing transparency and sourcing ability. Furthermore,
search engines are already answering queries with error-laden, generated
content, further blurring the provenance of information and impacting the
integrity of the information ecosystem. We argue how all these factors could
reduce the reliability of search engines. Finally, we summarize some of the
active research directions and open questions.
</p>
</div>
</dd>
<dt><a name="item399">[399]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11709" title="Abstract">arXiv:2402.11709</a> [<a href="/pdf/2402.11709" title="Download PDF">pdf</a>, <a href="/format/2402.11709" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GNNavi: Navigating the Information Flow in Large Language Models by  Graph Neural Network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Shuzhou Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+E">Ercong Nie</a>, 
<a href="/search/cs?searchtype=author&query=F%C3%A4rber%2C+M">Michael F&#xe4;rber</a>, 
<a href="/search/cs?searchtype=author&query=Schmid%2C+H">Helmut Schmid</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%BCtze%2C+H">Hinrich Sch&#xfc;tze</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large Language Models (LLMs) exhibit strong In-Context Learning (ICL)
capabilities when prompts with demonstrations are applied to them. However,
fine-tuning still remains crucial to further enhance their adaptability.
Prompt-based fine-tuning proves to be an effective fine-tuning method in
low-data scenarios, but high demands on computing resources limit its
practicality. We address this issue by introducing a prompt-based
parameter-efficient fine-tuning (PEFT) approach. GNNavi leverages insights into
ICL's information flow dynamics, which indicates that label words act in
prompts as anchors for information propagation. GNNavi employs a Graph Neural
Network (GNN) layer to precisely guide the aggregation and distribution of
information flow during the processing of prompts by hardwiring the desired
information flow into the GNN. Our experiments on text classification tasks
with GPT-2 and Llama2 shows GNNavi surpasses standard prompt-based fine-tuning
methods in few-shot settings by updating just 0.2% to 0.5% of parameters. We
compare GNNavi with prevalent PEFT approaches, such as prefix tuning, LoRA and
Adapter in terms of performance and efficiency. Our analysis reveals that
GNNavi enhances information flow and ensures a clear aggregation process.
</p>
</div>
</dd>
<dt><a name="item400">[400]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11710" title="Abstract">arXiv:2402.11710</a> [<a href="/pdf/2402.11710" title="Download PDF">pdf</a>, <a href="/format/2402.11710" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Note on Bias to Complete
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jia Xu</a>, 
<a href="/search/cs?searchtype=author&query=Diab%2C+M">Mona Diab</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Minimizing social bias strengthens societal bonds, promoting shared
understanding and better decision-making. We revisit the definition of bias by
discovering new bias types (e.g., societal status) in dynamic environments and
describe them relative to context, such as culture, region, time, and personal
background. Our framework includes eight hypotheses about bias and a minimizing
bias strategy for each assumption as well as five methods as proposed solutions
in LLM. The realization of the framework is yet to be completed.
</p>
</div>
</dd>
<dt><a name="item401">[401]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11711" title="Abstract">arXiv:2402.11711</a> [<a href="/pdf/2402.11711" title="Download PDF">pdf</a>, <a href="/format/2402.11711" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement  Learning for Discrete Prompt Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jafari%2C+Y">Yasaman Jafari</a>, 
<a href="/search/cs?searchtype=author&query=Mekala%2C+D">Dheeraj Mekala</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+R">Rose Yu</a>, 
<a href="/search/cs?searchtype=author&query=Berg-Kirkpatrick%2C+T">Taylor Berg-Kirkpatrick</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">RL-based techniques can be used to search for prompts that when fed into a
target language model maximize a set of user-specified reward functions.
However, in many target applications, the natural reward functions are in
tension with one another -- for example, content preservation vs. style
matching in style transfer tasks. Current techniques focus on maximizing the
average of reward functions, which does not necessarily lead to prompts that
achieve balance across rewards -- an issue that has been well-studied in the
multi-objective and robust optimization literature. In this paper, we adapt
several techniques for multi-objective optimization to RL-based discrete prompt
optimization -- two that consider volume of the Pareto reward surface, and
another that chooses an update direction that benefits all rewards
simultaneously. We conduct an empirical analysis of these methods on two NLP
tasks: style transfer and machine translation, each using three competing
reward functions. Our experiments demonstrate that multi-objective methods that
directly optimize volume perform better and achieve a better balance of all
rewards than those that attempt to find monotonic update directions.
</p>
</div>
</dd>
<dt><a name="item402">[402]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11712" title="Abstract">arXiv:2402.11712</a> [<a href="/pdf/2402.11712" title="Download PDF">pdf</a>, <a href="/format/2402.11712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modelling Political Coalition Negotiations Using LLM-based Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Moghimifar%2C+F">Farhad Moghimifar</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuan-Fang Li</a>, 
<a href="/search/cs?searchtype=author&query=Thomson%2C+R">Robert Thomson</a>, 
<a href="/search/cs?searchtype=author&query=Haffari%2C+G">Gholamreza Haffari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Coalition negotiations are a cornerstone of parliamentary democracies,
characterised by complex interactions and strategic communications among
political parties. Despite its significance, the modelling of these
negotiations has remained unexplored with the domain of Natural Language
Processing (NLP), mostly due to lack of proper data. In this paper, we
introduce coalition negotiations as a novel NLP task, and model it as a
negotiation between large language model-based agents. We introduce a
multilingual dataset, POLCA, comprising manifestos of European political
parties and coalition agreements over a number of elections in these countries.
This dataset addresses the challenge of the current scope limitations in
political negotiation modelling by providing a diverse, real-world basis for
simulation. Additionally, we propose a hierarchical Markov decision process
designed to simulate the process of coalition negotiation between political
parties and predict the outcomes. We evaluate the performance of
state-of-the-art large language models (LLMs) as agents in handling coalition
negotiations, offering insights into their capabilities and paving the way for
future advancements in political modelling.
</p>
</div>
</dd>
<dt><a name="item403">[403]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11718" title="Abstract">arXiv:2402.11718</a> [<a href="/pdf/2402.11718" title="Download PDF">pdf</a>, <a href="/ps/2402.11718" title="Download PostScript">ps</a>, <a href="/format/2402.11718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deployment of Microcells by integrating LTE-U with LTE
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Panchal%2C+D">Deven Panchal</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">With increasing data requirements of users, cellular operators are finding
new ways to fulfil these requirements. These attempts involve the practice of
deploying Wi-Fi access points nearer to the user and backhauling it to the
nearest eNB (in case of LTE and LTE-A). The paper studies LTE-U, an extension
of LTE which works in the unlicensed spectrum, as a potential solution to this
problem. It is based on the idea of densification. Network deployments
incorporating LTE-U will be able to better cater to the growing data rate
demand of voice and video, thus reducing the load on eNB. Further we explore
the possibility of LTE-U as an alternative to Wi-Fi or co-existing with Wi-Fi
deployments and issues revolving around this idea. We show that LTE-U
deployment solves the problem of capacity in both cases.
</p>
</div>
</dd>
<dt><a name="item404">[404]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11719" title="Abstract">arXiv:2402.11719</a> [<a href="/pdf/2402.11719" title="Download PDF">pdf</a>, <a href="/format/2402.11719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixed material point method formulation, stabilization, and validation  for a unified analysis of free-surface and seepage flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chandra%2C+B">Bodhinanda Chandra</a>, 
<a href="/search/math?searchtype=author&query=Hashimoto%2C+R">Ryota Hashimoto</a>, 
<a href="/search/math?searchtype=author&query=Kamrin%2C+K">Ken Kamrin</a>, 
<a href="/search/math?searchtype=author&query=Soga%2C+K">Kenichi Soga</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 48 pages, 29 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">This paper presents a novel stabilized mixed material point method (MPM)
designed for the unified modeling of free-surface and seepage flow. The unified
formulation integrates the Navier-Stokes equation with the
Darcy-Brinkman-Forchheimer equation, effectively capturing flows in both
non-porous and porous domains. In contrast to the conventional Eulerian
computational fluid dynamics (CFD) solver, which solves the velocity and
pressure fields as unknown variables, the proposed method employs a monolithic
displacement-pressure formulation adopted from the mixed-form
updated-Lagrangian finite element method (FEM). To satisfy the discrete inf-sup
stability condition, a stabilization strategy based on the variational
multiscale method (VMS) is derived and integrated into the proposed
formulation. Another distinctive feature is the implementation of blurred
interfaces, which facilitate a seamless and stable transition of flows between
free and porous domains, as well as across two distinct porous media. The
efficacy of the proposed formulation is verified and validated through several
benchmark cases in 1D, 2D, and 3D scenarios. Conducted numerical examples
demonstrate enhanced accuracy and stability compared to analytical,
experimental, and other numerical solutions.
</p>
</div>
</dd>
<dt><a name="item405">[405]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11722" title="Abstract">arXiv:2402.11722</a> [<a href="/pdf/2402.11722" title="Download PDF">pdf</a>, <a href="/format/2402.11722" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Invertible Fourier Neural Operators for Tackling Both Forward and  Inverse Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Long%2C+D">Da Long</a>, 
<a href="/search/cs?searchtype=author&query=Zhe%2C+S">Shandian Zhe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Fourier Neural Operator (FNO) is a popular operator learning method, which
has demonstrated state-of-the-art performance across many tasks. However, FNO
is mainly used in forward prediction, yet a large family of applications rely
on solving inverse problems. In this paper, we propose an invertible Fourier
Neural Operator (iFNO) that tackles both the forward and inverse problems. We
designed a series of invertible Fourier blocks in the latent channel space to
share the model parameters, efficiently exchange the information, and mutually
regularize the learning for the bi-directional tasks. We integrated a
variational auto-encoder to capture the intrinsic structures within the input
space and to enable posterior inference so as to overcome challenges of
illposedness, data shortage, noises, etc. We developed a three-step process for
pre-training and fine tuning for efficient training. The evaluations on five
benchmark problems have demonstrated the effectiveness of our approach.
</p>
</div>
</dd>
<dt><a name="item406">[406]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11723" title="Abstract">arXiv:2402.11723</a> [<a href="/pdf/2402.11723" title="Download PDF">pdf</a>, <a href="/format/2402.11723" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shaping Human-AI Collaboration: Varied Scaffolding Levels in Co-writing  with Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dhillon%2C+P+S">Paramveer S. Dhillon</a>, 
<a href="/search/cs?searchtype=author&query=Molaei%2C+S">Somayeh Molaei</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiaqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Golub%2C+M">Maximilian Golub</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Shaochun Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Robert%2C+L+P">Lionel P. Robert</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Appearing at CHI 2024 (Honolulu, HI)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Advances in language modeling have paved the way for novel human-AI
co-writing experiences. This paper explores how varying levels of scaffolding
from large language models (LLMs) shape the co-writing process. Employing a
within-subjects field experiment with a Latin square design, we asked
participants (N=131) to respond to argumentative writing prompts under three
randomly sequenced conditions: no AI assistance (control), next-sentence
suggestions (low scaffolding), and next-paragraph suggestions (high
scaffolding). Our findings reveal a U-shaped impact of scaffolding on writing
quality and productivity (words/time). While low scaffolding did not
significantly improve writing quality or productivity, high scaffolding led to
significant improvements, especially benefiting non-regular writers and less
tech-savvy users. No significant cognitive burden was observed while using the
scaffolded writing tools, but a moderate decrease in text ownership and
satisfaction was noted. Our results have broad implications for the design of
AI-powered writing tools, including the need for personalized scaffolding
mechanisms.
</p>
</div>
</dd>
<dt><a name="item407">[407]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11724" title="Abstract">arXiv:2402.11724</a> [<a href="/pdf/2402.11724" title="Download PDF">pdf</a>, <a href="/format/2402.11724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models as Data Augmenters for Cold-Start Item  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jianling Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Haokai Lu</a>, 
<a href="/search/cs?searchtype=author&query=Caverlee%2C+J">James Caverlee</a>, 
<a href="/search/cs?searchtype=author&query=Chi%2C+E">Ed Chi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Minmin Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">The reasoning and generalization capabilities of LLMs can help us better
understand user preferences and item characteristics, offering exciting
prospects to enhance recommendation systems. Though effective while user-item
interactions are abundant, conventional recommendation systems struggle to
recommend cold-start items without historical interactions. To address this, we
propose utilizing LLMs as data augmenters to bridge the knowledge gap on
cold-start items during training. We employ LLMs to infer user preferences for
cold-start items based on textual description of user historical behaviors and
new item descriptions. The augmented training signals are then incorporated
into learning the downstream recommendation models through an auxiliary
pairwise loss. Through experiments on public Amazon datasets, we demonstrate
that LLMs can effectively augment the training signals for cold-start items,
leading to significant improvements in cold-start item recommendation for
various recommendation models.
</p>
</div>
</dd>
<dt><a name="item408">[408]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11725" title="Abstract">arXiv:2402.11725</a> [<a href="/pdf/2402.11725" title="Download PDF">pdf</a>, <a href="/format/2402.11725" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Susceptible are Large Language Models to Ideological Manipulation?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zihao He</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jun Yan</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+T">Taiwei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Lerman%2C+K">Kristina Lerman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR); Computers and Society (cs.CY)

</div>
<p class="mathjax">Large Language Models (LLMs) possess the potential to exert substantial
influence on public perceptions and interactions with information. This raises
concerns about the societal impact that could arise if the ideologies within
these models can be easily manipulated. In this work, we investigate how
effectively LLMs can learn and generalize ideological biases from their
instruction-tuning data. Our findings reveal a concerning vulnerability:
exposure to only a small amount of ideologically driven samples significantly
alters the ideology of LLMs. Notably, LLMs demonstrate a startling ability to
absorb ideology from one topic and generalize it to even unrelated ones. The
ease with which LLMs' ideologies can be skewed underscores the risks associated
with intentionally poisoned training data by malicious actors or inadvertently
introduced biases by data annotators. It also emphasizes the imperative for
robust safeguards to mitigate the influence of ideological manipulations on
LLMs.
</p>
</div>
</dd>
<dt><a name="item409">[409]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11727" title="Abstract">arXiv:2402.11727</a> [<a href="/pdf/2402.11727" title="Download PDF">pdf</a>, <a href="/format/2402.11727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Cartesian Closed Category for Random Variables
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Di+Gianantonio%2C+P">Pietro Di Gianantonio</a>, 
<a href="/search/cs?searchtype=author&query=Edalat%2C+A">Abbas Edalat</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">We present a novel, yet rather simple construction within the traditional
framework of Scott domains to provide semantics to probabilistic programming,
thus obtaining a solution to a long-standing open problem in this area. Unlike
current main approaches that employ some probability measures or continuous
valuations on non-standard or rather complex structures, we use the Scott
domain of random variables from a standard sample space -- the unit interval or
the Cantor space -- to any given Scott domain. The map taking any such random
variable to its corresponding probability distribution provides an effectively
given, Scott continuous surjection onto the probabilistic power domain of the
underlying Scott domain, establishing a new basic result in classical domain
theory. We obtain a Cartesian closed category by enriching the category of
Scott domains to capture the equivalence of random variables on these domains.
The construction of the domain of random variables on this enriched category
forms a strong commutative monad, which is suitable for defining the semantics
of probabilistic programming.
</p>
</div>
</dd>
<dt><a name="item410">[410]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11728" title="Abstract">arXiv:2402.11728</a> [<a href="/pdf/2402.11728" title="Download PDF">pdf</a>, <a href="/format/2402.11728" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Numerical Claim Detection in Finance: A New Financial Dataset,  Weak-Supervision Model, and Market Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shah%2C+A">Agam Shah</a>, 
<a href="/search/cs?searchtype=author&query=Hiray%2C+A">Arnav Hiray</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+P">Pratvi Shah</a>, 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+A">Arkaprabha Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+A">Anushka Singh</a>, 
<a href="/search/cs?searchtype=author&query=Eidnani%2C+D">Dheeraj Eidnani</a>, 
<a href="/search/cs?searchtype=author&query=Chaudhury%2C+B">Bhaskar Chaudhury</a>, 
<a href="/search/cs?searchtype=author&query=Chava%2C+S">Sudheer Chava</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG); Computational Finance (q-fin.CP)

</div>
<p class="mathjax">In this paper, we investigate the influence of claims in analyst reports and
earnings calls on financial market returns, considering them as significant
quarterly events for publicly traded companies. To facilitate a comprehensive
analysis, we construct a new financial dataset for the claim detection task in
the financial domain. We benchmark various language models on this dataset and
propose a novel weak-supervision model that incorporates the knowledge of
subject matter experts (SMEs) in the aggregation function, outperforming
existing approaches. Furthermore, we demonstrate the practical utility of our
proposed model by constructing a novel measure ``optimism". Furthermore, we
observed the dependence of earnings surprise and return on our optimism
measure. Our dataset, models, and code will be made publicly (under CC BY 4.0
license) available on GitHub and Hugging Face.
</p>
</div>
</dd>
<dt><a name="item411">[411]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11729" title="Abstract">arXiv:2402.11729</a> [<a href="/pdf/2402.11729" title="Download PDF">pdf</a>, <a href="/format/2402.11729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prospector Heads: Generalized Feature Attribution for Large Models &amp;  Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Machiraju%2C+G">Gautam Machiraju</a>, 
<a href="/search/cs?searchtype=author&query=Derry%2C+A">Alexander Derry</a>, 
<a href="/search/cs?searchtype=author&query=Desai%2C+A">Arjun Desai</a>, 
<a href="/search/cs?searchtype=author&query=Guha%2C+N">Neel Guha</a>, 
<a href="/search/cs?searchtype=author&query=Karimi%2C+A">Amir-Hossein Karimi</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">James Zou</a>, 
<a href="/search/cs?searchtype=author&query=Altman%2C+R">Russ Altman</a>, 
<a href="/search/cs?searchtype=author&query=R%C3%A9%2C+C">Christopher R&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Mallick%2C+P">Parag Mallick</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 12 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">Feature attribution, the ability to localize regions of the input data that
are relevant for classification, is an important capability for machine
learning models in scientific and biomedical domains. Current methods for
feature attribution, which rely on "explaining" the predictions of end-to-end
classifiers, suffer from imprecise feature localization and are inadequate for
use with small sample sizes and high-dimensional datasets due to computational
challenges. We introduce prospector heads, an efficient and interpretable
alternative to explanation-based methods for feature attribution that can be
applied to any encoder and any data modality. Prospector heads generalize
across modalities through experiments on sequences (text), images (pathology),
and graphs (protein structures), outperforming baseline attribution methods by
up to 49 points in mean localization AUPRC. We also demonstrate how prospector
heads enable improved interpretation and discovery of class-specific patterns
in the input data. Through their high performance, flexibility, and
generalizability, prospectors provide a framework for improving trust and
transparency for machine learning models in complex domains.
</p>
</div>
</dd>
<dt><a name="item412">[412]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11733" title="Abstract">arXiv:2402.11733</a> [<a href="/pdf/2402.11733" title="Download PDF">pdf</a>, <a href="/format/2402.11733" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Effectiveness of Random Forgetting for Robust Generalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramkumar%2C+V+R+T">Vijaya Raghavan T Ramkumar</a>, 
<a href="/search/cs?searchtype=author&query=Zonooz%2C+B">Bahram Zonooz</a>, 
<a href="/search/cs?searchtype=author&query=Arani%2C+E">Elahe Arani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Deep neural networks are susceptible to adversarial attacks, which can
compromise their performance and accuracy. Adversarial Training (AT) has
emerged as a popular approach for protecting neural networks against such
attacks. However, a key challenge of AT is robust overfitting, where the
network's robust performance on test data deteriorates with further training,
thus hindering generalization. Motivated by the concept of active forgetting in
the brain, we introduce a novel learning paradigm called "Forget to Mitigate
Overfitting (FOMO)". FOMO alternates between the forgetting phase, which
randomly forgets a subset of weights and regulates the model's information
through weight reinitialization, and the relearning phase, which emphasizes
learning generalizable features. Our experiments on benchmark datasets and
adversarial attacks show that FOMO alleviates robust overfitting by
significantly reducing the gap between the best and last robust test accuracy
while improving the state-of-the-art robustness. Furthermore, FOMO provides a
better trade-off between standard and robust accuracy, outperforming baseline
adversarial methods. Finally, our framework is robust to AutoAttacks and
increases generalization in many real-world scenarios.
</p>
</div>
</dd>
<dt><a name="item413">[413]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11734" title="Abstract">arXiv:2402.11734</a> [<a href="/pdf/2402.11734" title="Download PDF">pdf</a>, <a href="/format/2402.11734" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Solving Data-centric Tasks using Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barke%2C+S">Shraddha Barke</a>, 
<a href="/search/cs?searchtype=author&query=Poelitz%2C+C">Christian Poelitz</a>, 
<a href="/search/cs?searchtype=author&query=Negreanu%2C+C+S">Carina Suzana Negreanu</a>, 
<a href="/search/cs?searchtype=author&query=Zorn%2C+B">Benjamin Zorn</a>, 
<a href="/search/cs?searchtype=author&query=Cambronero%2C+J">Jos&#xe9; Cambronero</a>, 
<a href="/search/cs?searchtype=author&query=Gordon%2C+A+D">Andrew D. Gordon</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+V">Vu Le</a>, 
<a href="/search/cs?searchtype=author&query=Nouri%2C+E">Elnaz Nouri</a>, 
<a href="/search/cs?searchtype=author&query=Polikarpova%2C+N">Nadia Polikarpova</a>, 
<a href="/search/cs?searchtype=author&query=Sarkar%2C+A">Advait Sarkar</a>, 
<a href="/search/cs?searchtype=author&query=Slininger%2C+B">Brian Slininger</a>, 
<a href="/search/cs?searchtype=author&query=Toronto%2C+N">Neil Toronto</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+J">Jack Williams</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Artificial Intelligence (cs.AI); Software Engineering (cs.SE)

</div>
<p class="mathjax">Large language models (LLMs) are rapidly replacing help forums like
StackOverflow, and are especially helpful for non-professional programmers and
end users. These users are often interested in data-centric tasks, such as
spreadsheet manipulation and data wrangling, which are hard to solve if the
intent is only communicated using a natural-language description, without
including the data. But how do we decide how much data and which data to
include in the prompt? This paper makes two contributions towards answering
this question. First, we create a dataset of real-world NL-to-code tasks
manipulating tabular data, mined from StackOverflow posts. Second, we introduce
a cluster-then-select prompting technique, which adds the most representative
rows from the input data to the LLM prompt. Our experiments show that LLM
performance is indeed sensitive to the amount of data passed in the prompt, and
that for tasks with a lot of syntactic variation in the input table, our
cluster-then-select technique outperforms a random selection baseline.
</p>
</div>
</dd>
<dt><a name="item414">[414]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11735" title="Abstract">arXiv:2402.11735</a> [<a href="/pdf/2402.11735" title="Download PDF">pdf</a>, <a href="/format/2402.11735" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LiRaFusion: Deep Adaptive LiDAR-Radar Fusion for 3D Object Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jingyu Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Lingjun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Skinner%2C+K+A">Katherine A. Skinner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">We propose LiRaFusion to tackle LiDAR-radar fusion for 3D object detection to
fill the performance gap of existing LiDAR-radar detectors. To improve the
feature extraction capabilities from these two modalities, we design an early
fusion module for joint voxel feature encoding, and a middle fusion module to
adaptively fuse feature maps via a gated network. We perform extensive
evaluation on nuScenes to demonstrate that LiRaFusion leverages the
complementary information of LiDAR and radar effectively and achieves notable
improvement over existing methods.
</p>
</div>
</dd>
<dt><a name="item415">[415]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11736" title="Abstract">arXiv:2402.11736</a> [<a href="/pdf/2402.11736" title="Download PDF">pdf</a>, <a href="/format/2402.11736" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Monte Carlo with kernel-based Gibbs measures: Guarantees for  probabilistic herding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rouault%2C+M">Martin Rouault</a>, 
<a href="/search/cs?searchtype=author&query=Bardenet%2C+R">R&#xe9;mi Bardenet</a>, 
<a href="/search/cs?searchtype=author&query=Ma%C3%AFda%2C+M">Myl&#xe8;ne Ma&#xef;da</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 2 figures. Comments are welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Probability (math.PR); Machine Learning (stat.ML)

</div>
<p class="mathjax">Kernel herding belongs to a family of deterministic quadratures that seek to
minimize the worst-case integration error over a reproducing kernel Hilbert
space (RKHS). In spite of strong experimental support, it has revealed
difficult to prove that this worst-case error decreases at a faster rate than
the standard square root of the number of quadrature nodes, at least in the
usual case where the RKHS is infinite-dimensional. In this theoretical paper,
we study a joint probability distribution over quadrature nodes, whose support
tends to minimize the same worst-case error as kernel herding. We prove that it
does outperform i.i.d. Monte Carlo, in the sense of coming with a tighter
concentration inequality on the worst-case integration error. While not
improving the rate yet, this demonstrates that the mathematical tools of the
study of Gibbs measures can help understand to what extent kernel herding and
its variants improve on computationally cheaper methods. Moreover, we provide
early experimental evidence that a faster rate of convergence, though not
worst-case, is likely.
</p>
</div>
</dd>
<dt><a name="item416">[416]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11737" title="Abstract">arXiv:2402.11737</a> [<a href="/pdf/2402.11737" title="Download PDF">pdf</a>, <a href="/format/2402.11737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compression Repair for Feedforward Neural Networks Based on Model  Equivalence Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mo%2C+Z">Zihao Mo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yejiang Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shuaizheng Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+W">Weiming Xiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper, we propose a method of repairing compressed Feedforward Neural
Networks (FNNs) based on equivalence evaluation of two neural networks. In the
repairing framework, a novel neural network equivalence evaluation method is
developed to compute the output discrepancy between two neural networks. The
output discrepancy can quantitatively characterize the output difference
produced by compression procedures. Based on the computed output discrepancy,
the repairing method first initializes a new training set for the compressed
networks to narrow down the discrepancy between the two neural networks and
improve the performance of the compressed network. Then, we repair the
compressed FNN by re-training based on the training set. We apply our developed
method to the MNIST dataset to demonstrate the effectiveness and advantages of
our proposed repair method.
</p>
</div>
</dd>
<dt><a name="item417">[417]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11739" title="Abstract">arXiv:2402.11739</a> [<a href="/pdf/2402.11739" title="Download PDF">pdf</a>, <a href="/format/2402.11739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Transition System Abstraction Framework for Neural Network Dynamical  System Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Yang%2C+Y">Yejiang Yang</a>, 
<a href="/search/eess?searchtype=author&query=Mo%2C+Z">Zihao Mo</a>, 
<a href="/search/eess?searchtype=author&query=Tran%2C+H">Hoang-Dung Tran</a>, 
<a href="/search/eess?searchtype=author&query=Xiang%2C+W">Weiming Xiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ACC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper proposes a transition system abstraction framework for neural
network dynamical system models to enhance the model interpretability, with
applications to complex dynamical systems such as human behavior learning and
verification. To begin with, the localized working zone will be segmented into
multiple localized partitions under the data-driven Maximum Entropy (ME)
partitioning method. Then, the transition matrix will be obtained based on the
set-valued reachability analysis of neural networks. Finally, applications to
human handwriting dynamics learning and verification are given to validate our
proposed abstraction framework, which demonstrates the advantages of enhancing
the interpretability of the black-box model, i.e., our proposed framework is
able to abstract a data-driven neural network model into a transition system,
making the neural network model interpretable through verifying specifications
described in Computational Tree Logic (CTL) languages.
</p>
</div>
</dd>
<dt><a name="item418">[418]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11740" title="Abstract">arXiv:2402.11740</a> [<a href="/pdf/2402.11740" title="Download PDF">pdf</a>, <a href="/ps/2402.11740" title="Download PostScript">ps</a>, <a href="/format/2402.11740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extraction of nonlinearity in neural networks and model compression with  Koopman operator
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sugishita%2C+N">Naoki Sugishita</a>, 
<a href="/search/cs?searchtype=author&query=Kinjo%2C+K">Kayo Kinjo</a>, 
<a href="/search/cs?searchtype=author&query=Ohkubo%2C+J">Jun Ohkubo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Nonlinearity plays a crucial role in deep neural networks. In this paper, we
first investigate the degree to which the nonlinearity of the neural network is
essential. For this purpose, we employ the Koopman operator, extended dynamic
mode decomposition, and the tensor-train format. The results imply that
restricted nonlinearity is enough for the classification of handwritten
numbers. Then, we propose a model compression method for deep neural networks,
which could be beneficial to handling large networks in resource-constrained
environments. Leveraging the Koopman operator, the proposed method enables us
to use linear algebra in the internal processing of neural networks. We
numerically show that the proposed method performs comparably or better than
conventional methods in highly compressed model settings for the handwritten
number recognition task.
</p>
</div>
</dd>
<dt><a name="item419">[419]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11741" title="Abstract">arXiv:2402.11741</a> [<a href="/pdf/2402.11741" title="Download PDF">pdf</a>, <a href="/format/2402.11741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> To Store or Not to Store: a graph theoretical approach for Dataset  Versioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+A">Anxin Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jingwei Li</a>, 
<a href="/search/cs?searchtype=author&query=Sukprasert%2C+P">Pattara Sukprasert</a>, 
<a href="/search/cs?searchtype=author&query=Khuller%2C+S">Samir Khuller</a>, 
<a href="/search/cs?searchtype=author&query=Deshpande%2C+A">Amol Deshpande</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+K">Koyel Mukherjee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IPDPS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Computational Complexity (cs.CC); Databases (cs.DB); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">In this work, we study the cost efficient data versioning problem, where the
goal is to optimize the storage and reconstruction (retrieval) costs of data
versions, given a graph of datasets as nodes and edges capturing edit/delta
information. One central variant we study is MinSum Retrieval (MSR) where the
goal is to minimize the total retrieval costs, while keeping the storage costs
bounded. This problem (along with its variants) was introduced by Bhattacherjee
et al. [VLDB'15]. While such problems are frequently encountered in
collaborative tools (e.g., version control systems and data analysis
pipelines), to the best of our knowledge, no existing research studies the
theoretical aspects of these problems.
<br />We establish that the currently best-known heuristic, LMG, can perform
arbitrarily badly in a simple worst case. Moreover, we show that it is hard to
get $o(n)$-approximation for MSR on general graphs even if we relax the storage
constraints by an $O(\log n)$ factor. Similar hardness results are shown for
other variants. Meanwhile, we propose poly-time approximation schemes for
tree-like graphs, motivated by the fact that the graphs arising in practice
from typical edit operations are often not arbitrary. As version graphs
typically have low treewidth, we further develop new algorithms for bounded
treewidth graphs.
<br />Furthermore, we propose two new heuristics and evaluate them empirically.
First, we extend LMG by considering more potential ``moves'', to propose a new
heuristic LMG-All. LMG-All consistently outperforms LMG while having comparable
run time on a wide variety of datasets, i.e., version graphs. Secondly, we
apply our tree algorithms on the minimum-storage arborescence of an instance,
yielding algorithms that are qualitatively better than all previous heuristics
for MSR, as well as for another variant BoundedMin Retrieval (BMR).
</p>
</div>
</dd>
<dt><a name="item420">[420]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11742" title="Abstract">arXiv:2402.11742</a> [<a href="/pdf/2402.11742" title="Download PDF">pdf</a>, <a href="/format/2402.11742" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with  Spectral Imbalance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaushik%2C+C">Chiraag Kaushik</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+R">Ran Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chi-Heng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Khera%2C+A">Amrit Khera</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+M+Y">Matthew Y Jin</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+W">Wenrui Ma</a>, 
<a href="/search/cs?searchtype=author&query=Muthukumar%2C+V">Vidya Muthukumar</a>, 
<a href="/search/cs?searchtype=author&query=Dyer%2C+E+L">Eva L Dyer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Classification models are expected to perform equally well for different
classes, yet in practice, there are often large gaps in their performance. This
issue of class bias is widely studied in cases of datasets with sample
imbalance, but is relatively overlooked in balanced datasets. In this work, we
introduce the concept of spectral imbalance in features as a potential source
for class disparities and study the connections between spectral imbalance and
class bias in both theory and practice. To build the connection between
spectral imbalance and class gap, we develop a theoretical framework for
studying class disparities and derive exact expressions for the per-class error
in a high-dimensional mixture model setting. We then study this phenomenon in
11 different state-of-the-art pretrained encoders and show how our proposed
framework can be used to compare the quality of encoders, as well as evaluate
and combine data augmentation strategies to mitigate the issue. Our work sheds
light on the class-dependent effects of learning, and provides new insights
into how state-of-the-art pretrained features may have unknown biases that can
be diagnosed through their spectra.
</p>
</div>
</dd>
<dt><a name="item421">[421]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11744" title="Abstract">arXiv:2402.11744</a> [<a href="/pdf/2402.11744" title="Download PDF">pdf</a>, <a href="/format/2402.11744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Machine-generated Text Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhongping Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+W">Wenda Qin</a>, 
<a href="/search/cs?searchtype=author&query=Plummer%2C+B+A">Bryan A. Plummer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Machine-Generated Text (MGT) detection aims to identify a piece of text as
machine or human written. Prior work has primarily formulated MGT as a binary
classification task over an entire document, with limited work exploring cases
where only part of a document is machine generated. This paper provides the
first in-depth study of MGT that localizes the portions of a document that were
machine generated. Thus, if a bad actor were to change a key portion of a news
article to spread misinformation, whole document MGT detection may fail since
the vast majority is human written, but our approach can succeed due to its
granular approach. A key challenge in our MGT localization task is that short
spans of text, e.g., a single sentence, provides little information indicating
if it is machine generated due to its short length. To address this, we
leverage contextual information, where we predict whether multiple sentences
are machine or human written at once. This enables our approach to identify
changes in style or content to boost performance. A gain of 4-13% mean Average
Precision (mAP) over prior work demonstrates the effectiveness of approach on
five diverse datasets: GoodNews, VisualNews, WikiText, Essay, and WP. We
release our implementation at
\href{https://github.com/Zhongping-Zhang/MGT_Localization}{this http URL}.
</p>
</div>
</dd>
<dt><a name="item422">[422]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11746" title="Abstract">arXiv:2402.11746</a> [<a href="/pdf/2402.11746" title="Download PDF">pdf</a>, <a href="/format/2402.11746" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned  Language Models through Task Arithmetic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhardwaj%2C+R">Rishabh Bhardwaj</a>, 
<a href="/search/cs?searchtype=author&query=Anh%2C+D+D">Do Duc Anh</a>, 
<a href="/search/cs?searchtype=author&query=Poria%2C+S">Soujanya Poria</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Aligned language models face a significant limitation as their fine-tuning
often results in compromised safety. To tackle this, we propose a simple method
RESTA that performs LLM safety realignment. RESTA stands for REstoring Safety
through Task Arithmetic. At its core, it involves a simple arithmetic addition
of a safety vector to the weights of the compromised model. We demonstrate the
effectiveness of RESTA in both parameter-efficient and full fine-tuning,
covering a wide range of downstream tasks, including instruction following in
Chinese, English, and Hindi, as well as problem-solving capabilities in Code
and Math. We also showcase the generalizability of RESTA on three existing
safety evaluation benchmarks and a multilingual benchmark dataset proposed as a
part of this work, consisting of 550 harmful questions covering 11 categories,
each with 5 sub-categories of harm. Overall, RESTA decreases the harmfulness of
the compromised model from 18.6% to 5.1% and from 9.2% to 1.5% in
parameter-efficient and full fine-tuning, respectively, while maintaining most
of the model's performance on the task. We release the source codes at:
https://github.com/declare-lab/resta.
</p>
</div>
</dd>
<dt><a name="item423">[423]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11748" title="Abstract">arXiv:2402.11748</a> [<a href="/pdf/2402.11748" title="Download PDF">pdf</a>, <a href="/format/2402.11748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Low-power SNN-based audio source localisation using a Hilbert Transform  spike encoding scheme
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haghighatshoar%2C+S">Saeid Haghighatshoar</a>, 
<a href="/search/cs?searchtype=author&query=Muir%2C+D+R">Dylan R Muir</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Neural and Evolutionary Computing (cs.NE); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Sound source localisation is used in many consumer electronics devices, to
help isolate audio from individual speakers and to reject noise. Localization
is frequently accomplished by "beamforming" algorithms, which combine
microphone audio streams to improve received signal power from particular
incident source directions. Beamforming algorithms generally use knowledge of
the frequency components of the audio source, along with the known microphone
array geometry, to analytically phase-shift microphone streams before combining
them. A dense set of band-pass filters is often used to obtain known-frequency
"narrowband" components from wide-band audio streams. These approaches achieve
high accuracy, but state of the art narrowband beamforming algorithms are
computationally demanding, and are therefore difficult to integrate into
low-power IoT devices. We demonstrate a novel method for sound source
localisation in arbitrary microphone arrays, designed for efficient
implementation in ultra-low-power spiking neural networks (SNNs). We use a
novel short-time Hilbert transform (STHT) to remove the need for demanding
band-pass filtering of audio, and introduce a new accompanying method for audio
encoding with spiking events. Our beamforming and localisation approach
achieves state-of-the-art accuracy for SNN methods, and comparable with
traditional non-SNN super-resolution approaches. We deploy our method to
low-power SNN audio inference hardware, and achieve much lower power
consumption compared with super-resolution methods. We demonstrate that signal
processing approaches can be co-designed with spiking neural network
implementations to achieve high levels of power efficiency. Our new
Hilbert-transform-based method for beamforming promises to also improve the
efficiency of traditional DSP-based signal processing.
</p>
</div>
</dd>
<dt><a name="item424">[424]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11750" title="Abstract">arXiv:2402.11750</a> [<a href="/pdf/2402.11750" title="Download PDF">pdf</a>, <a href="/format/2402.11750" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> In-Context Learning Demonstration Selection via Influence Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=S.%2C+V+M">Vinay M.S.</a>, 
<a href="/search/cs?searchtype=author&query=Van%2C+M">Minh-Hao Van</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xintao Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 1 figure, and 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) have demonstrated their In-Context Learning
(ICL) capabilities which provides an opportunity to perform few shot learning
without any gradient update. Despite its multiple benefits, ICL generalization
performance is sensitive to the selected demonstrations. Selecting effective
demonstrations for ICL is still an open research challenge. To address this
challenge, we propose a demonstration selection method called InfICL which
analyzes influences of training samples through influence functions.
Identifying highly influential training samples can potentially aid in
uplifting the ICL generalization performance. To limit the running cost of
InfICL, we only employ the LLM to generate sample embeddings, and don't perform
any costly fine tuning. We perform empirical study on multiple real-world
datasets and show merits of our InfICL against state-of-the-art baselines.
</p>
</div>
</dd>
<dt><a name="item425">[425]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11752" title="Abstract">arXiv:2402.11752</a> [<a href="/pdf/2402.11752" title="Download PDF">pdf</a>, <a href="/ps/2402.11752" title="Download PostScript">ps</a>, <a href="/format/2402.11752" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diagonalisation SGD: Fast &amp; Convergent SGD for Non-Differentiable Models  via Reparameterisation and Smoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wagner%2C+D">Dominik Wagner</a>, 
<a href="/search/cs?searchtype=author&query=Khajwal%2C+B">Basim Khajwal</a>, 
<a href="/search/cs?searchtype=author&query=Ong%2C+C+-+L">C.-H. Luke Ong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Optimization and Control (math.OC)

</div>
<p class="mathjax">It is well-known that the reparameterisation gradient estimator, which
exhibits low variance in practice, is biased for non-differentiable models.
This may compromise correctness of gradient-based optimisation methods such as
stochastic gradient descent (SGD). We introduce a simple syntactic framework to
define non-differentiable functions piecewisely and present a systematic
approach to obtain smoothings for which the reparameterisation gradient
estimator is unbiased. Our main contribution is a novel variant of SGD,
Diagonalisation Stochastic Gradient Descent, which progressively enhances the
accuracy of the smoothed approximation during optimisation, and we prove
convergence to stationary points of the unsmoothed (original) objective. Our
empirical evaluation reveals benefits over the state of the art: our approach
is simple, fast, stable and attains orders of magnitude reduction in
work-normalised variance.
</p>
</div>
</dd>
<dt><a name="item426">[426]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11753" title="Abstract">arXiv:2402.11753</a> [<a href="/pdf/2402.11753" title="Download PDF">pdf</a>, <a href="/format/2402.11753" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+F">Fengqing Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zhangchen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+L">Luyao Niu</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Z">Zhen Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Ramasubramanian%2C+B">Bhaskar Ramasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bo Li</a>, 
<a href="/search/cs?searchtype=author&query=Poovendran%2C+R">Radha Poovendran</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Safety is critical to the usage of large language models (LLMs). Multiple
techniques such as data filtering and supervised fine-tuning have been
developed to strengthen LLM safety. However, currently known techniques presume
that corpora used for safety alignment of LLMs are solely interpreted by
semantics. This assumption, however, does not hold in real-world applications,
which leads to severe vulnerabilities in LLMs. For example, users of forums
often use ASCII art, a form of text-based art, to convey image information. In
this paper, we propose a novel ASCII art-based jailbreak attack and introduce a
comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the
capabilities of LLMs in recognizing prompts that cannot be solely interpreted
by semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and
Llama2) struggle to recognize prompts provided in the form of ASCII art. Based
on this observation, we develop the jailbreak attack ArtPrompt, which leverages
the poor performance of LLMs in recognizing ASCII art to bypass safety measures
and elicit undesired behaviors from LLMs. ArtPrompt only requires black-box
access to the victim LLMs, making it a practical attack. We evaluate ArtPrompt
on five SOTA LLMs, and show that ArtPrompt can effectively and efficiently
induce undesired behaviors from all five LLMs.
</p>
</div>
</dd>
<dt><a name="item427">[427]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11755" title="Abstract">arXiv:2402.11755</a> [<a href="/pdf/2402.11755" title="Download PDF">pdf</a>, <a href="/format/2402.11755" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SPML: A DSL for Defending Language Models Against Prompt Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+R+K">Reshabh K Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+V">Vinayak Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Grossman%2C+D">Dan Grossman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Cryptography and Security (cs.CR); Programming Languages (cs.PL)

</div>
<p class="mathjax">Large language models (LLMs) have profoundly transformed natural language
applications, with a growing reliance on instruction-based definitions for
designing chatbots. However, post-deployment the chatbot definitions are fixed
and are vulnerable to attacks by malicious users, emphasizing the need to
prevent unethical applications and financial losses. Existing studies explore
user prompts' impact on LLM-based chatbots, yet practical methods to contain
attacks on application-specific chatbots remain unexplored. This paper presents
System Prompt Meta Language (SPML), a domain-specific language for refining
prompts and monitoring the inputs to the LLM-based chatbots. SPML actively
checks attack prompts, ensuring user inputs align with chatbot definitions to
prevent malicious execution on the LLM backbone, optimizing costs. It also
streamlines chatbot definition crafting with programming language capabilities,
overcoming natural language design challenges. Additionally, we introduce a
groundbreaking benchmark with 1.8k system prompts and 20k user inputs, offering
the inaugural language and benchmark for chatbot definition evaluation.
Experiments across datasets demonstrate SPML's proficiency in understanding
attacker prompts, surpassing models like GPT-4, GPT-3.5, and LLAMA. Our data
and codes are publicly available at: https://prompt-compiler.github.io/SPML/.
</p>
</div>
</dd>
<dt><a name="item428">[428]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11756" title="Abstract">arXiv:2402.11756</a> [<a href="/pdf/2402.11756" title="Download PDF">pdf</a>, <a href="/format/2402.11756" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in  Generative LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bakman%2C+Y+F">Yavuz Faruk Bakman</a>, 
<a href="/search/cs?searchtype=author&query=Yaldiz%2C+D+N">Duygu Nur Yaldiz</a>, 
<a href="/search/cs?searchtype=author&query=Buyukates%2C+B">Baturalp Buyukates</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+C">Chenyang Tao</a>, 
<a href="/search/cs?searchtype=author&query=Dimitriadis%2C+D">Dimitrios Dimitriadis</a>, 
<a href="/search/cs?searchtype=author&query=Avestimehr%2C+S">Salman Avestimehr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Generative Large Language Models (LLMs) are widely utilized for their
excellence in various tasks. However, their tendency to produce inaccurate or
misleading outputs poses a potential risk, particularly in high-stakes
environments. Therefore, estimating the correctness of generative LLM outputs
is an important task for enhanced reliability. Uncertainty Estimation (UE) in
generative LLMs is an evolving domain, where SOTA probability-based methods
commonly employ length-normalized scoring. In this work, we propose
Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized
scoring for UE methods. MARS is a novel scoring function that considers the
semantic contribution of each token in the generated sequence in the context of
the question. We demonstrate that integrating MARS into UE methods results in a
universal and significant improvement in UE performance. We conduct experiments
using three distinct closed-book question-answering datasets across five
popular pre-trained LLMs. Lastly, we validate the efficacy of MARS on a Medical
QA dataset. Code can be found
https://anonymous.4open.science/r/LLM_Uncertainity-309B.
</p>
</div>
</dd>
<dt><a name="item429">[429]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11757" title="Abstract">arXiv:2402.11757</a> [<a href="/pdf/2402.11757" title="Download PDF">pdf</a>, <a href="/format/2402.11757" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Models for Stemming: Promises, Pitfalls and Failures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+S">Shengyao Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Zuccon%2C+G">Guido Zuccon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Text stemming is a natural language processing technique that is used to
reduce words to their base form, also known as the root form. The use of
stemming in IR has been shown to often improve the effectiveness of
keyword-matching models such as BM25. However, traditional stemming methods,
focusing solely on individual terms, overlook the richness of contextual
information. Recognizing this gap, in this paper, we investigate the promising
idea of using large language models (LLMs) to stem words by leveraging its
capability of context understanding. With this respect, we identify three
avenues, each characterised by different trade-offs in terms of computational
cost, effectiveness and robustness : (1) use LLMs to stem the vocabulary for a
collection, i.e., the set of unique words that appear in the collection
(vocabulary stemming), (2) use LLMs to stem each document separately
(contextual stemming), and (3) use LLMs to extract from each document entities
that should not be stemmed, then use vocabulary stemming to stem the rest of
the terms (entity-based contextual stemming). Through a series of empirical
experiments, we compare the use of LLMs for stemming with that of traditional
lexical stemmers such as Porter and Krovetz for English text. We find that
while vocabulary stemming and contextual stemming fail to achieve higher
effectiveness than traditional stemmers, entity-based contextual stemming can
achieve a higher effectiveness than using Porter stemmer alone, under specific
conditions.
</p>
</div>
</dd>
<dt><a name="item430">[430]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11760" title="Abstract">arXiv:2402.11760</a> [<a href="/pdf/2402.11760" title="Download PDF">pdf</a>, <a href="/format/2402.11760" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning as a Parsimonious Alternative to Prediction  Cascades: A Case Study on Image Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Srikishan%2C+B">Bharat Srikishan</a>, 
<a href="/search/cs?searchtype=author&query=Tabassum%2C+A">Anika Tabassum</a>, 
<a href="/search/cs?searchtype=author&query=Allu%2C+S">Srikanth Allu</a>, 
<a href="/search/cs?searchtype=author&query=Kannan%2C+R">Ramakrishnan Kannan</a>, 
<a href="/search/cs?searchtype=author&query=Muralidhar%2C+N">Nikhil Muralidhar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Deep learning architectures have achieved state-of-the-art (SOTA) performance
on computer vision tasks such as object detection and image segmentation. This
may be attributed to the use of over-parameterized, monolithic deep learning
architectures executed on large datasets. Although such architectures lead to
increased accuracy, this is usually accompanied by a large increase in
computation and memory requirements during inference. While this is a non-issue
in traditional machine learning pipelines, the recent confluence of machine
learning and fields like the Internet of Things has rendered such large
architectures infeasible for execution in low-resource settings. In such
settings, previous efforts have proposed decision cascades where inputs are
passed through models of increasing complexity until desired performance is
achieved. However, we argue that cascaded prediction leads to increased
computational cost due to wasteful intermediate computations. To address this,
we propose PaSeR (Parsimonious Segmentation with Reinforcement Learning) a
non-cascading, cost-aware learning pipeline as an alternative to cascaded
architectures. Through experimental evaluation on real-world and standard
datasets, we demonstrate that PaSeR achieves better accuracy while minimizing
computational cost relative to cascaded models. Further, we introduce a new
metric IoU/GigaFlop to evaluate the balance between cost and performance. On
the real-world task of battery material phase segmentation, PaSeR yields a
minimum performance improvement of 174% on the IoU/GigaFlop metric with respect
to baselines. We also demonstrate PaSeR's adaptability to complementary models
trained on a noisy MNIST dataset, where it achieved a minimum performance
improvement on IoU/GigaFlop of 13.4% over SOTA models. Code and data are
available at https://github.com/scailab/paser .
</p>
</div>
</dd>
<dt><a name="item431">[431]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11763" title="Abstract">arXiv:2402.11763</a> [<a href="/pdf/2402.11763" title="Download PDF">pdf</a>, <a href="/format/2402.11763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autonomous Hyperspectral Characterisation Station: Robotically Assisted  Characterisation of Polymer Degradation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Azizi%2C+S">Shayan Azizi</a>, 
<a href="/search/eess?searchtype=author&query=Asadi%2C+E">Ehsan Asadi</a>, 
<a href="/search/eess?searchtype=author&query=Howard%2C+S">Shaun Howard</a>, 
<a href="/search/eess?searchtype=author&query=Muir%2C+B+W">Benjamin W. Muir</a>, 
<a href="/search/eess?searchtype=author&query=O%27Shea%2C+R">Riley O&#x27;Shea</a>, 
<a href="/search/eess?searchtype=author&query=Bab-Hadiashar%2C+A">Alireza Bab-Hadiashar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">This paper addresses the gap between the capabilities and utilisation of
robotics and automation in laboratory settings and builds upon the concept of
Self Driving Labs (SDL). %to significantly impact laboratory operations. We
introduce an innovative approach to the temporal characterisation of materials.
The article discusses the challenges posed by manual methods involving
established laboratory equipment and presents an automated hyperspectral
characterisation station. This station integrates robot-aided hyperspectral
imaging, complex material characterisation modeling, and automated data
analysis, offering a non-destructive and comprehensive approach. This work
explains how the proposed assembly can automatically measure the half-life of
biodegradable polymers with higher throughput and accuracy than manual methods.
The investigation explores the effect of pH, number of average molecular weight
(Mn), end groups, and blends on the degradation rate of polylactic acid (PLA).
The contributions of the paper lie in introducing an adaptable classification
station for novel characterisation methods and presenting an innovative
methodology for polymer degradation rate measurement. The proposed system has
the potential to accelerate the development of high-throughput screening and
characterisation methods in material and chemistry laboratories.
</p>
</div>
</dd>
<dt><a name="item432">[432]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11764" title="Abstract">arXiv:2402.11764</a> [<a href="/pdf/2402.11764" title="Download PDF">pdf</a>, <a href="/format/2402.11764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatGPT Based Data Augmentation for Improved Parameter-Efficient  Debiasing of LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+P">Pengrui Han</a>, 
<a href="/search/cs?searchtype=author&query=Kocielnik%2C+R">Rafal Kocielnik</a>, 
<a href="/search/cs?searchtype=author&query=Saravanan%2C+A">Adhithya Saravanan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+R">Roy Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Sharir%2C+O">Or Sharir</a>, 
<a href="/search/cs?searchtype=author&query=Anandkumar%2C+A">Anima Anandkumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024 Workshop on Language Technology for Equality, Diversity, Inclusion (LT-EDI-2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Large Language models (LLMs), while powerful, exhibit harmful social biases.
Debiasing is often challenging due to computational costs, data constraints,
and potential degradation of multi-task language capabilities. This work
introduces a novel approach utilizing ChatGPT to generate synthetic training
data, aiming to enhance the debiasing of LLMs. We propose two strategies:
Targeted Prompting, which provides effective debiasing for known biases but
necessitates prior specification of bias in question; and General Prompting,
which, while slightly less effective, offers debiasing across various
categories. We leverage resource-efficient LLM debiasing using adapter tuning
and compare the effectiveness of our synthetic data to existing debiasing
datasets. Our results reveal that: (1) ChatGPT can efficiently produce
high-quality training data for debiasing other LLMs; (2) data produced via our
approach surpasses existing datasets in debiasing performance while also
preserving internal knowledge of a pre-trained LLM; and (3) synthetic data
exhibits generalizability across categories, effectively mitigating various
biases, including intersectional ones. These findings underscore the potential
of synthetic data in advancing the fairness of LLMs with minimal retraining
cost.
</p>
</div>
</dd>
<dt><a name="item433">[433]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11765" title="Abstract">arXiv:2402.11765</a> [<a href="/pdf/2402.11765" title="Download PDF">pdf</a>, <a href="/format/2402.11765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guide to Numerical Experiments on Elections in Computational Social  Choice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boehmer%2C+N">Niclas Boehmer</a>, 
<a href="/search/cs?searchtype=author&query=Faliszewski%2C+P">Piotr Faliszewski</a>, 
<a href="/search/cs?searchtype=author&query=Janeczko%2C+%C5%81">&#x141;ukasz Janeczko</a>, 
<a href="/search/cs?searchtype=author&query=Kaczmarczyk%2C+A">Andrzej Kaczmarczyk</a>, 
<a href="/search/cs?searchtype=author&query=Lisowski%2C+G">Grzegorz Lisowski</a>, 
<a href="/search/cs?searchtype=author&query=Pierczy%C5%84ski%2C+G">Grzegorz Pierczy&#x144;ski</a>, 
<a href="/search/cs?searchtype=author&query=Rey%2C+S">Simon Rey</a>, 
<a href="/search/cs?searchtype=author&query=Stolicki%2C+D">Dariusz Stolicki</a>, 
<a href="/search/cs?searchtype=author&query=Szufa%2C+S">Stanis&#x142;aw Szufa</a>, 
<a href="/search/cs?searchtype=author&query=W%C4%85s%2C+T">Tomasz W&#x105;s</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We analyze how numerical experiments regarding elections were conducted
within the computational social choice literature (focusing on papers published
in the IJCAI, AAAI, and AAMAS conferences). We analyze the sizes of the studied
elections and the methods used for generating preference data, thereby making
previously hidden standards and practices explicit. In particular, we survey a
number of statistical cultures for generating elections and their commonly used
parameters.
</p>
</div>
</dd>
<dt><a name="item434">[434]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11766" title="Abstract">arXiv:2402.11766</a> [<a href="/pdf/2402.11766" title="Download PDF">pdf</a>, <a href="/format/2402.11766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Well-Connected Set and Its Application to Multi-Robot Path Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+T">Teng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jingjin Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Parking lots and autonomous warehouses for accommodating many vehicles/robots
adopt designs in which the underlying graphs are \emph{well-connected} to
simplify planning and reduce congestion. In this study, we formulate and delve
into the \emph{largest well-connected set} (LWCS) problem and explore its
applications in layout design for multi-robot path planning. Roughly speaking,
a well-connected set over a connected graph is a set of vertices such that
there is a path on the graph connecting any pair of vertices in the set without
passing through any additional vertices of the set. Identifying an LWCS has
many potential high-utility applications, e.g., for determining parking garage
layout and capacity, as prioritized planning can be shown to be complete when
start/goal configurations belong to an LWCS. In this work, we establish that
computing an LWCS is NP-complete. We further develop optimal and near-optimal
LWCS algorithms, with the near-optimal algorithm targeting large maps. A
complete prioritized planning method is given for planning paths for multiple
robots residing on an LWCS.
</p>
</div>
</dd>
<dt><a name="item435">[435]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11767" title="Abstract">arXiv:2402.11767</a> [<a href="/pdf/2402.11767" title="Download PDF">pdf</a>, <a href="/format/2402.11767" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized Lifelong Path Planning for Multiple Ackerman Car-Like  Robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+T">Teng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jingjin Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Path planning for multiple non-holonomic robots in continuous domains
constitutes a difficult robotics challenge with many applications. Despite
significant recent progress on the topic, computationally efficient and
high-quality solutions are lacking, especially in lifelong settings where
robots must continuously take on new tasks. In this work, we make it possible
to extend key ideas enabling state-of-the-art (SOTA) methods for multi-robot
planning in discrete domains to the motion planning of multiple Ackerman
(car-like) robots in lifelong settings, yielding high-performance centralized
and decentralized planners. Our planners compute trajectories that allow the
robots to reach precise $SE(2)$ goal poses. The effectiveness of our methods is
thoroughly evaluated and confirmed using both simulation and real-world
experiments.
</p>
</div>
</dd>
<dt><a name="item436">[436]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11768" title="Abstract">arXiv:2402.11768</a> [<a href="/pdf/2402.11768" title="Download PDF">pdf</a>, <a href="/format/2402.11768" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Targeted Parallelization of Conflict-Based Search for Multi-Robot Path  Planning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+T">Teng Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+J">Jingjin Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IROS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Multi-Robot Path Planning (MRPP) on graphs, equivalently known as Multi-Agent
Path Finding (MAPF), is a well-established NP-hard problem with critically
important applications. As serial computation in (near)-optimally solving MRPP
approaches the computation efficiency limit, parallelization offers a promising
route to push the limit further, especially in handling hard or large MRPP
instances. In this study, we initiated a \emph{targeted} parallelization effort
to boost the performance of conflict-based search for MRPP. Specifically, when
instances are relatively small but robots are densely packed with strong
interactions, we apply a decentralized parallel algorithm that concurrently
explores multiple branches that leads to markedly enhanced solution discovery.
On the other hand, when instances are large with sparse robot-robot
interactions, we prioritize node expansion and conflict resolution. Our
innovative multi-threaded approach to parallelizing bounded-suboptimal conflict
search-based algorithms demonstrates significant improvements over baseline
serial methods in success rate or runtime. Our contribution further pushes the
understanding of MRPP and charts a promising path for elevating solution
quality and computational efficiency through parallel algorithmic strategies.
</p>
</div>
</dd>
<dt><a name="item437">[437]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11769" title="Abstract">arXiv:2402.11769</a> [<a href="/pdf/2402.11769" title="Download PDF">pdf</a>, <a href="/format/2402.11769" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Connection-Aware P2P Trading: Simultaneous Trading and Peer Selection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Feng%2C+C">Cheng Feng</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+K">Kedi Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Shan%2C+L">Lanqing Shan</a>, 
<a href="/search/eess?searchtype=author&query=Alers%2C+H">Hani Alers</a>, 
<a href="/search/eess?searchtype=author&query=Stergioulas%2C+L">Lampros Stergioulas</a>, 
<a href="/search/eess?searchtype=author&query=Guo%2C+H">Hongye Guo</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Q">Qixin Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE PES Transactions
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Computer Science and Game Theory (cs.GT); Optimization and Control (math.OC)

</div>
<p class="mathjax">Peer-to-peer (P2P) trading is seen as a viable solution to handle the growing
number of distributed energy resources in distribution networks. However, when
dealing with large-scale consumers, there are several challenges that must be
addressed. One of these challenges is limited communication capabilities.
Additionally, prosumers may have specific preferences when it comes to trading.
Both can result in serious asynchrony in peer-to-peer trading, potentially
impacting the effectiveness of negotiations and hindering convergence before
the market closes. This paper introduces a connection-aware P2P trading
algorithm designed for extensive prosumer trading. The algorithm facilitates
asynchronous trading while respecting prosumer's autonomy in trading peer
selection, an often overlooked aspect in traditional models. In addition, to
optimize the use of limited connection opportunities, a smart trading peer
connection selection strategy is developed to guide consumers to communicate
strategically to accelerate convergence. A theoretical convergence guarantee is
provided for the connection-aware P2P trading algorithm, which further details
how smart selection strategies enhance convergence efficiency. Numerical
studies are carried out to validate the effectiveness of the connection-aware
algorithm and the performance of smart selection strategies in reducing the
overall convergence time.
</p>
</div>
</dd>
<dt><a name="item438">[438]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11770" title="Abstract">arXiv:2402.11770</a> [<a href="/pdf/2402.11770" title="Download PDF">pdf</a>, <a href="/format/2402.11770" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structured Chain-of-Thought Prompting for Few-Shot Generation of  Content-Grounded QA Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sultan%2C+M+A">Md Arafat Sultan</a>, 
<a href="/search/cs?searchtype=author&query=Ganhotra%2C+J">Jatin Ganhotra</a>, 
<a href="/search/cs?searchtype=author&query=Astudillo%2C+R+F">Ram&#xf3;n Fernandez Astudillo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We introduce a structured chain-of-thought (SCoT) prompting approach to
generating content-grounded multi-turn question-answer conversations using a
pre-trained large language model (LLM). At the core of our proposal is a
structured breakdown of the complex task into a number of states in a state
machine, so that actions corresponding to various subtasks, e.g., content
reading and utterance generation, can be executed in their own dedicated
states. Each state leverages a unique set of resources including prompts and
(optionally) additional tools to augment the generation process. Our
experimental results show that SCoT prompting with designated states for
hallucination mitigation increases agent faithfulness to grounding documents by
up to 16.8%. When used as training data, our open-domain conversations
synthesized from only 6 Wikipedia-based seed demonstrations train strong
conversational QA agents; in out-of-domain evaluation, for example, we observe
improvements of up to 13.9% over target domain gold data when the latter is
augmented with our generated examples.
</p>
</div>
</dd>
<dt><a name="item439">[439]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11771" title="Abstract">arXiv:2402.11771</a> [<a href="/pdf/2402.11771" title="Download PDF">pdf</a>, <a href="/format/2402.11771" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating the Effectiveness of Index-Based Treatment Allocation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boehmer%2C+N">Niclas Boehmer</a>, 
<a href="/search/cs?searchtype=author&query=Nair%2C+Y">Yash Nair</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+S">Sanket Shah</a>, 
<a href="/search/cs?searchtype=author&query=Janson%2C+L">Lucas Janson</a>, 
<a href="/search/cs?searchtype=author&query=Taneja%2C+A">Aparna Taneja</a>, 
<a href="/search/cs?searchtype=author&query=Tambe%2C+M">Milind Tambe</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">When resources are scarce, an allocation policy is needed to decide who
receives a resource. This problem occurs, for instance, when allocating scarce
medical resources and is often solved using modern ML methods. This paper
introduces methods to evaluate index-based allocation policies -- that allocate
a fixed number of resources to those who need them the most -- by using data
from a randomized control trial. Such policies create dependencies between
agents, which render the assumptions behind standard statistical tests invalid
and limit the effectiveness of estimators. Addressing these challenges, we
translate and extend recent ideas from the statistics literature to present an
efficient estimator and methods for computing asymptotically correct confidence
intervals. This enables us to effectively draw valid statistical conclusions, a
critical gap in previous work. Our extensive experiments validate our
methodology in practical settings, while also showcasing its statistical power.
We conclude by proposing and empirically verifying extensions of our
methodology that enable us to reevaluate a past randomized control trial to
evaluate different ML allocation policies in the context of a mHealth program,
drawing previously invisible conclusions.
</p>
</div>
</dd>
<dt><a name="item440">[440]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11773" title="Abstract">arXiv:2402.11773</a> [<a href="/pdf/2402.11773" title="Download PDF">pdf</a>, <a href="/format/2402.11773" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Multi-Network Mining of Tensor Time Series
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Obata%2C+K">Kohei Obata</a>, 
<a href="/search/cs?searchtype=author&query=Kawabata%2C+K">Koki Kawabata</a>, 
<a href="/search/cs?searchtype=author&query=Matsubara%2C+Y">Yasuko Matsubara</a>, 
<a href="/search/cs?searchtype=author&query=Sakurai%2C+Y">Yasushi Sakurai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT)

</div>
<p class="mathjax">Subsequence clustering of time series is an essential task in data mining,
and interpreting the resulting clusters is also crucial since we generally do
not have prior knowledge of the data. Thus, given a large collection of tensor
time series consisting of multiple modes, including timestamps, how can we
achieve subsequence clustering for tensor time series and provide interpretable
insights? In this paper, we propose a new method, Dynamic Multi-network Mining
(DMM), that converts a tensor time series into a set of segment groups of
various lengths (i.e., clusters) characterized by a dependency network
constrained with l1-norm. Our method has the following properties. (a)
Interpretable: it characterizes the cluster with multiple networks, each of
which is a sparse dependency network of a corresponding non-temporal mode, and
thus provides visible and interpretable insights into the key relationships.
(b) Accurate: it discovers the clusters with distinct networks from tensor time
series according to the minimum description length (MDL). (c) Scalable: it
scales linearly in terms of the input data size when solving a non-convex
problem to optimize the number of segments and clusters, and thus it is
applicable to long-range and high-dimensional tensors. Extensive experiments
with synthetic datasets confirm that our method outperforms the
state-of-the-art methods in terms of clustering accuracy. We then use real
datasets to demonstrate that DMM is useful for providing interpretable insights
from tensor time series.
</p>
</div>
</dd>
<dt><a name="item441">[441]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11777" title="Abstract">arXiv:2402.11777</a> [<a href="/pdf/2402.11777" title="Download PDF">pdf</a>, <a href="/format/2402.11777" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncovering Latent Human Wellbeing in Language Model Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Freire%2C+P">Pedro Freire</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+C">ChengCheng Tan</a>, 
<a href="/search/cs?searchtype=author&query=Gleave%2C+A">Adam Gleave</a>, 
<a href="/search/cs?searchtype=author&query=Hendrycks%2C+D">Dan Hendrycks</a>, 
<a href="/search/cs?searchtype=author&query=Emmons%2C+S">Scott Emmons</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Do language models implicitly learn a concept of human wellbeing? We explore
this through the ETHICS Utilitarianism task, assessing if scaling enhances
pretrained models' representations. Our initial finding reveals that, without
any prompt engineering or finetuning, the leading principal component from
OpenAI's text-embedding-ada-002 achieves 73.9% accuracy. This closely matches
the 74.6% of BERT-large finetuned on the entire ETHICS dataset, suggesting
pretraining conveys some understanding about human wellbeing. Next, we consider
four language model families, observing how Utilitarianism accuracy varies with
increased parameters. We find performance is nondecreasing with increased model
size when using sufficient numbers of principal components.
</p>
</div>
</dd>
<dt><a name="item442">[442]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11778" title="Abstract">arXiv:2402.11778</a> [<a href="/pdf/2402.11778" title="Download PDF">pdf</a>, <a href="/format/2402.11778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Theoretical Understandings of Self-Consuming Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+S">Shi Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yingjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+X">Xinmei Tian</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper tackles the emerging challenge of training generative models
within a self-consuming loop, wherein successive generations of models are
recursively trained on mixtures of real and synthetic data from previous
generations. We construct a theoretical framework to rigorously evaluate how
this training regimen impacts the data distributions learned by future models.
Specifically, we derive bounds on the total variation (TV) distance between the
synthetic data distributions produced by future models and the original real
data distribution under various mixed training scenarios. Our analysis
demonstrates that this distance can be effectively controlled under the
condition that mixed training dataset sizes or proportions of real data are
large enough. Interestingly, we further unveil a phase transition induced by
expanding synthetic data amounts, proving theoretically that while the TV
distance exhibits an initial ascent, it declines beyond a threshold point.
Finally, we specialize our general results to diffusion models, delivering
nuanced insights such as the efficacy of optimal early stopping within the
self-consuming loop.
</p>
</div>
</dd>
<dt><a name="item443">[443]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11780" title="Abstract">arXiv:2402.11780</a> [<a href="/pdf/2402.11780" title="Download PDF">pdf</a>, <a href="/format/2402.11780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Joint Optimization for DNN Architecture and Configuration for  Compute-In-Memory Hardware
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kundu%2C+S">Souvik Kundu</a>, 
<a href="/search/cs?searchtype=author&query=Sarah%2C+A">Anthony Sarah</a>, 
<a href="/search/cs?searchtype=author&query=Joshi%2C+V">Vinay Joshi</a>, 
<a href="/search/cs?searchtype=author&query=Omer%2C+O+J">Om J Omer</a>, 
<a href="/search/cs?searchtype=author&query=Subramoney%2C+S">Sreenivas Subramoney</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 4 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the recent growth in demand for large-scale deep neural networks,
compute in-memory (CiM) has come up as a prominent solution to alleviate
bandwidth and on-chip interconnect bottlenecks that constrain Von-Neuman
architectures. However, the construction of CiM hardware poses a challenge as
any specific memory hierarchy in terms of cache sizes and memory bandwidth at
different interfaces may not be ideally matched to any neural network's
attributes such as tensor dimension and arithmetic intensity, thus leading to
suboptimal and under-performing systems. Despite the success of neural
architecture search (NAS) techniques in yielding efficient sub-networks for a
given hardware metric budget (e.g., DNN execution time or latency), it assumes
the hardware configuration to be frozen, often yielding sub-optimal
sub-networks for a given budget. In this paper, we present CiMNet, a framework
that jointly searches for optimal sub-networks and hardware configurations for
CiM architectures creating a Pareto optimal frontier of downstream task
accuracy and execution metrics (e.g., latency). The proposed framework can
comprehend the complex interplay between a sub-network's performance and the
CiM hardware configuration choices including bandwidth, processing element
size, and memory size. Exhaustive experiments on different model architectures
from both CNN and Transformer families demonstrate the efficacy of the CiMNet
in finding co-optimized sub-networks and CiM hardware configurations.
Specifically, for similar ImageNet classification accuracy as baseline ViT-B,
optimizing only the model architecture increases performance (or reduces
workload execution time) by 1.7x while optimizing for both the model
architecture and hardware configuration increases it by 3.1x.
</p>
</div>
</dd>
<dt><a name="item444">[444]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11782" title="Abstract">arXiv:2402.11782</a> [<a href="/pdf/2402.11782" title="Download PDF">pdf</a>, <a href="/format/2402.11782" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Evidence Do Language Models Find Convincing?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+A">Alexander Wan</a>, 
<a href="/search/cs?searchtype=author&query=Wallace%2C+E">Eric Wallace</a>, 
<a href="/search/cs?searchtype=author&query=Klein%2C+D">Dan Klein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Retrieval-augmented language models are being increasingly tasked with
subjective, contentious, and conflicting queries such as "is aspartame linked
to cancer". To resolve these ambiguous queries, one must search through a large
range of websites and consider "which, if any, of this evidence do I find
convincing?". In this work, we study how LLMs answer this question. In
particular, we construct ConflictingQA, a dataset that pairs controversial
queries with a series of real-world evidence documents that contain different
facts (e.g., quantitative results), argument styles (e.g., appeals to
authority), and answers (Yes or No). We use this dataset to perform sensitivity
and counterfactual analyses to explore which text features most affect LLM
predictions. Overall, we find that current models rely heavily on the relevance
of a website to the query, while largely ignoring stylistic features that
humans find important such as whether a text contains scientific references or
is written with a neutral tone. Taken together, these results highlight the
importance of RAG corpus quality (e.g., the need to filter misinformation), and
possibly even a shift in how LLMs are trained to better align with human
judgements.
</p>
</div>
</dd>
<dt><a name="item445">[445]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11788" title="Abstract">arXiv:2402.11788</a> [<a href="/pdf/2402.11788" title="Download PDF">pdf</a>, <a href="/format/2402.11788" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MM-SurvNet: Deep Learning-Based Survival Risk Stratification in Breast  Cancer Through Multimodal Data Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mondol%2C+R+K">Raktim Kumar Mondol</a>, 
<a href="/search/cs?searchtype=author&query=Millar%2C+E+K+A">Ewan K.A. Millar</a>, 
<a href="/search/cs?searchtype=author&query=Sowmya%2C+A">Arcot Sowmya</a>, 
<a href="/search/cs?searchtype=author&query=Meijering%2C+E">Erik Meijering</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Keywords: Multimodal Fusion, Breast Cancer, Whole Slide Images, Survival Prediction
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Survival risk stratification is an important step in clinical decision making
for breast cancer management. We propose a novel deep learning approach for
this purpose by integrating histopathological imaging, genetic and clinical
data. It employs vision transformers, specifically the MaxViT model, for image
feature extraction, and self-attention to capture intricate image relationships
at the patient level. A dual cross-attention mechanism fuses these features
with genetic data, while clinical data is incorporated at the final layer to
enhance predictive accuracy. Experiments on the public TCGA-BRCA dataset show
that our model, trained using the negative log likelihood loss function, can
achieve superior performance with a mean C-index of 0.64, surpassing existing
methods. This advancement facilitates tailored treatment strategies,
potentially leading to improved patient outcomes.
</p>
</div>
</dd>
<dt><a name="item446">[446]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11790" title="Abstract">arXiv:2402.11790</a> [<a href="/pdf/2402.11790" title="Download PDF">pdf</a>, <a href="/format/2402.11790" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoLRIO: LiDAR-Ranging-Inertial Centralized State Estimation for Robotic  Swarms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+S">Shipeng Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hongbo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Y">Yuhua Qi</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+D">Dapeng Feng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhiqiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+W">Weisong Wen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+M">Ming Liu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> published in ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Collaborative state estimation using different heterogeneous sensors is a
fundamental prerequisite for robotic swarms operating in GPS-denied
environments, posing a significant research challenge. In this paper, we
introduce a centralized system to facilitate collaborative
LiDAR-ranging-inertial state estimation, enabling robotic swarms to operate
without the need for anchor deployment. The system efficiently distributes
computationally intensive tasks to a central server, thereby reducing the
computational burden on individual robots for local odometry calculations. The
server back-end establishes a global reference by leveraging shared data and
refining joint pose graph optimization through place recognition, global
optimization techniques, and removal of outlier data to ensure precise and
robust collaborative state estimation. Extensive evaluations of our system,
utilizing both publicly available datasets and our custom datasets, demonstrate
significant enhancements in the accuracy of collaborative SLAM estimates.
Moreover, our system exhibits remarkable proficiency in large-scale missions,
seamlessly enabling ten robots to collaborate effectively in performing SLAM
tasks. In order to contribute to the research community, we will make our code
open-source and accessible at \url{https://github.com/PengYu-team/Co-LRIO}.
</p>
</div>
</dd>
<dt><a name="item447">[447]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11791" title="Abstract">arXiv:2402.11791</a> [<a href="/pdf/2402.11791" title="Download PDF">pdf</a>, <a href="/format/2402.11791" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SDGE: Stereo Guided Depth Estimation for 360&#xb0; Camera Sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jialei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xianming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Junjun Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+X">Xiangyang Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Depth estimation is a critical technology in autonomous driving, and
multi-camera systems are often used to achieve a 360{\deg} perception. These
360{\deg} camera sets often have limited or low-quality overlap regions, making
multi-view stereo methods infeasible for the entire image. Alternatively,
monocular methods may not produce consistent cross-view predictions. To address
these issues, we propose the Stereo Guided Depth Estimation (SGDE) method,
which enhances depth estimation of the full image by explicitly utilizing
multi-view stereo results on the overlap. We suggest building virtual pinhole
cameras to resolve the distortion problem of fisheye cameras and unify the
processing for the two types of 360{\deg} cameras. For handling the varying
noise on camera poses caused by unstable movement, the approach employs a
self-calibration method to obtain highly accurate relative poses of the
adjacent cameras with minor overlap. These enable the use of robust stereo
methods to obtain high-quality depth prior in the overlap region. This prior
serves not only as an additional input but also as pseudo-labels that enhance
the accuracy of depth estimation methods and improve cross-view prediction
consistency. The effectiveness of SGDE is evaluated on one fisheye camera
dataset, Synthetic Urban, and two pinhole camera datasets, DDAD and nuScenes.
Our experiments demonstrate that SGDE is effective for both supervised and
self-supervised depth estimation, and highlight the potential of our method for
advancing downstream autonomous driving technologies, such as 3D object
detection and occupancy prediction.
</p>
</div>
</dd>
<dt><a name="item448">[448]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11792" title="Abstract">arXiv:2402.11792</a> [<a href="/pdf/2402.11792" title="Download PDF">pdf</a>, <a href="/format/2402.11792" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SInViG: A Self-Evolving Interactive Visual Agent for Human-Robot  Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanbo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xinghang Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huaping Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+X">Xuguang Lan</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+T">Tao Kong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Linguistic ambiguity is ubiquitous in our daily lives. Previous works adopted
interaction between robots and humans for language disambiguation.
Nevertheless, when interactive robots are deployed in daily environments, there
are significant challenges for natural human-robot interaction, stemming from
complex and unpredictable visual inputs, open-ended interaction, and diverse
user demands. In this paper, we present SInViG, which is a self-evolving
interactive visual agent for human-robot interaction based on natural
languages, aiming to resolve language ambiguity, if any, through multi-turn
visual-language dialogues. It continuously and automatically learns from
unlabeled images and large language models, without human intervention, to be
more robust against visual and linguistic complexity. Benefiting from
self-evolving, it sets new state-of-the-art on several interactive visual
grounding benchmarks. Moreover, our human-robot interaction experiments show
that the evolved models consistently acquire more and more preferences from
human users. Besides, we also deployed our model on a Franka robot for
interactive manipulation tasks. Results demonstrate that our model can follow
diverse user instructions and interact naturally with humans in natural
language, despite the complexity and disturbance of the environment.
</p>
</div>
</dd>
<dt><a name="item449">[449]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11793" title="Abstract">arXiv:2402.11793</a> [<a href="/pdf/2402.11793" title="Download PDF">pdf</a>, <a href="/format/2402.11793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Kaleidoscopic Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shrivastava%2C+H">Harsh Shrivastava</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">We discovered that the Deep ReLU networks (or Multilayer Perceptron
architecture) demonstrate an 'over-generalization' phenomenon. That is, the
output values for the inputs that were not seen during training are mapped
close to the output range that were observed during the learning process. In
other words, the MLP learns a many-to-one mapping and this effect is more
prominent as we increase the number of layers or depth of the MLP. We utilize
this property of Deep ReLU networks to design a dataset kaleidoscope, termed as
'Generative Kaleidoscopic Networks'. Briefly, if we learn a MLP to map from
input $x\in\mathbb{R}^D$ to itself $f_\mathcal{N}(x)\rightarrow x$, the
'Kaleidoscopic sampling' procedure starts with a random input noise
$z\in\mathbb{R}^D$ and recursively applies $f_\mathcal{N}(\cdots
f_\mathcal{N}(z)\cdots )$. After a burn-in period duration, we start observing
samples from the input distribution and we found that deeper the MLP, higher is
the quality of samples recovered. Scope: We observed this phenomenon to various
degrees for the other deep learning architectures like CNNs, Transformers &amp;
U-Nets and we are currently investigating them further.
</p>
</div>
</dd>
<dt><a name="item450">[450]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11794" title="Abstract">arXiv:2402.11794</a> [<a href="/pdf/2402.11794" title="Download PDF">pdf</a>, <a href="/format/2402.11794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling the Magic: Investigating Attention Distillation in  Retrieval-augmented Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zizhong Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Haopeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiawei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Retrieval-augmented generation framework can address the limitations of large
language models by enabling real-time knowledge updates for more accurate
answers. An efficient way in the training phase of retrieval-augmented models
is attention distillation, which uses attention scores as a supervision signal
instead of manually annotated query-document pairs. Despite its growing
popularity, the detailed mechanisms behind the success of attention
distillation remain unexplored, particularly the specific patterns it leverages
to benefit training. In this paper, we address this gap by conducting a
comprehensive review of attention distillation workflow and identifying key
factors influencing the learning quality of retrieval-augmented language
models. We further propose indicators for optimizing models' training methods
and avoiding ineffective training.
</p>
</div>
</dd>
<dt><a name="item451">[451]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11797" title="Abstract">arXiv:2402.11797</a> [<a href="/pdf/2402.11797" title="Download PDF">pdf</a>, <a href="/format/2402.11797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Suspended Magnetometer Survey for Mineral Data Acquisition with Vertical  Take-off and Landing Fixed-wing Aircraft
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Efrem%2C+R">Robel Efrem</a>, 
<a href="/search/cs?searchtype=author&query=Coutu%2C+A">Alex Coutu</a>, 
<a href="/search/cs?searchtype=author&query=Saeedi%2C+S">Sajad Saeedi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Multirotor Unmanned Aerial Vehicles (UAV)s have recently become an important
instrument for collecting mineral data, enabling more effective and accurate
geological investigations. This paper explores the difficulties in mounting
high-sensitivity sensors on a UAV platform, including electromagnetic
interference, payload dynamics, and maintaining stable sensor performance while
in flight. It is highlighted how the specific solutions provided to deal with
these problems have the potential to alter the collection of mineral data
assisted by UAVs. The work also shows experimental findings that demonstrate
the creative potential of these solutions in UAV-based mineral data collecting,
leading to improvements in effective mineral exploration through careful
design, testing, and assessment of these systems. These innovations resulted in
a platform that is quickly deployable in remote areas and able to operate more
efficiently compared to traditional multirotor UAVs while still producing equal
or higher quality mineral data. This allows for much higher efficiency and
lower operating costs for high-production UAV-based mineral data acquisition.
</p>
</div>
</dd>
<dt><a name="item452">[452]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11799" title="Abstract">arXiv:2402.11799</a> [<a href="/pdf/2402.11799" title="Download PDF">pdf</a>, <a href="/format/2402.11799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized Multi-Robot Navigation for Autonomous Surface Vehicles  with Distributional Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yewei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Fanfei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Englot%2C+B">Brendan Englot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The 2024 IEEE International Conference on Robotics and Automation (ICRA 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Collision avoidance algorithms for Autonomous Surface Vehicles (ASV) that
follow the Convention on the International Regulations for Preventing
Collisions at Sea (COLREGs) have been proposed in recent years. However, it may
be difficult and unsafe to follow COLREGs in congested waters, where multiple
ASVs are navigating in the presence of static obstacles and strong currents,
due to the complex interactions. To address this problem, we propose a
decentralized multi-ASV collision avoidance policy based on Distributional
Reinforcement Learning, which considers the interactions among ASVs as well as
with static obstacles and current flows. We evaluate the performance of the
proposed Distributional RL based policy against a traditional RL-based policy
and two classical methods, Artificial Potential Fields (APF) and Reciprocal
Velocity Obstacles (RVO), in simulation experiments, which show that the
proposed policy achieves superior performance in navigation safety, while
requiring minimal travel time and energy. A variant of our framework that
automatically adapts its risk sensitivity is also demonstrated to improve ASV
safety in highly congested environments.
</p>
</div>
</dd>
<dt><a name="item453">[453]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11800" title="Abstract">arXiv:2402.11800</a> [<a href="/pdf/2402.11800" title="Download PDF">pdf</a>, <a href="/format/2402.11800" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Approximation with Delayed Updates: Finite-Time Rates under  Markovian Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adibi%2C+A">Arman Adibi</a>, 
<a href="/search/cs?searchtype=author&query=Fabbro%2C+N+D">Nicolo Dal Fabbro</a>, 
<a href="/search/cs?searchtype=author&query=Schenato%2C+L">Luca Schenato</a>, 
<a href="/search/cs?searchtype=author&query=Kulkarni%2C+S">Sanjeev Kulkarni</a>, 
<a href="/search/cs?searchtype=author&query=Poor%2C+H+V">H. Vincent Poor</a>, 
<a href="/search/cs?searchtype=author&query=Pappas%2C+G+J">George J. Pappas</a>, 
<a href="/search/cs?searchtype=author&query=Hassani%2C+H">Hamed Hassani</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+A">Aritra Mitra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA); Systems and Control (eess.SY); Optimization and Control (math.OC)

</div>
<p class="mathjax">Motivated by applications in large-scale and multi-agent reinforcement
learning, we study the non-asymptotic performance of stochastic approximation
(SA) schemes with delayed updates under Markovian sampling. While the effect of
delays has been extensively studied for optimization, the manner in which they
interact with the underlying Markov process to shape the finite-time
performance of SA remains poorly understood. In this context, our first main
contribution is to show that under time-varying bounded delays, the delayed SA
update rule guarantees exponentially fast convergence of the \emph{last
iterate} to a ball around the SA operator's fixed point. Notably, our bound is
\emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the
mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel
inductive proof technique that, unlike various existing delayed-optimization
analyses, relies on establishing uniform boundedness of the iterates. As such,
our proof may be of independent interest. Next, to mitigate the impact of the
maximum delay on the convergence rate, we provide the first finite-time
analysis of a delay-adaptive SA scheme under Markovian sampling. In particular,
we show that the exponent of convergence of this scheme gets scaled down by
$\tau_{avg}$, as opposed to $\tau_{max}$ for the vanilla delayed SA rule; here,
$\tau_{avg}$ denotes the average delay across all iterations. Moreover, the
adaptive scheme requires no prior knowledge of the delay sequence for step-size
tuning. Our theoretical findings shed light on the finite-time effects of
delays for a broad class of algorithms, including TD learning, Q-learning, and
stochastic gradient descent under Markovian sampling.
</p>
</div>
</dd>
<dt><a name="item454">[454]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11801" title="Abstract">arXiv:2402.11801</a> [<a href="/pdf/2402.11801" title="Download PDF">pdf</a>, <a href="/format/2402.11801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Empathetic Response Generation by Augmenting LLMs with  Small-scale Empathetic Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhou Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhaochun Ren</a>, 
<a href="/search/cs?searchtype=author&query=Yufeng%2C+W">Wang Yufeng</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+S">Shizhong Peng</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Haizhou Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xiaofei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+X">Xiangwen Liao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Empathetic response generation is increasingly significant in AI,
necessitating nuanced emotional and cognitive understanding coupled with
articulate response expression. Current large language models (LLMs) excel in
response expression; however, they lack the ability to deeply understand
emotional and cognitive nuances, particularly in pinpointing fine-grained
emotions and their triggers. Conversely, small-scale empathetic models (SEMs)
offer strength in fine-grained emotion detection and detailed emotion cause
identification. To harness the complementary strengths of both LLMs and SEMs,
we introduce a Hybrid Empathetic Framework (HEF). HEF regards SEMs as flexible
plugins to improve LLM's nuanced emotional and cognitive understanding.
Regarding emotional understanding, HEF implements a two-stage emotion
prediction strategy, encouraging LLMs to prioritize primary emotions emphasized
by SEMs, followed by other categories, substantially alleviates the
difficulties for LLMs in fine-grained emotion detection. Regarding cognitive
understanding, HEF employs an emotion cause perception strategy, prompting LLMs
to focus on crucial emotion-eliciting words identified by SEMs, thus boosting
LLMs' capabilities in identifying emotion causes. This collaborative approach
enables LLMs to discern emotions more precisely and formulate empathetic
responses. We validate HEF on the Empathetic-Dialogue dataset, and the findings
indicate that our framework enhances the refined understanding of LLMs and
their ability to convey empathetic responses.
</p>
</div>
</dd>
<dt><a name="item455">[455]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11804" title="Abstract">arXiv:2402.11804</a> [<a href="/pdf/2402.11804" title="Download PDF">pdf</a>, <a href="/format/2402.11804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge  Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuwei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhiyong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Siqiang Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts
from new KGs that are not seen during training, has been widely adopted in
various applications. One critical challenge of KG inductive reasoning is
handling low-resource scenarios with scarcity in both textual and structural
aspects. In this paper, we attempt to address this challenge with Large
Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to
generate a graph-structural prompt to enhance the pre-trained Graph Neural
Networks (GNNs), which brings us new methodological insights into the KG
inductive reasoning methods, as well as high generalizability in practice. On
the methodological side, we introduce a novel pretraining and prompting
framework ProLINK, designed for low-resource inductive reasoning across
arbitrary KGs without requiring additional training. On the practical side, we
experimentally evaluate our approach on 36 low-resource KG datasets and find
that ProLINK outperforms previous methods in three-shot, one-shot, and
zero-shot reasoning tasks, exhibiting average performance improvements by 20%,
45%, and 147%, respectively. Furthermore, ProLINK demonstrates strong
robustness for various LLM promptings as well as full-shot scenarios.
</p>
</div>
</dd>
<dt><a name="item456">[456]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11807" title="Abstract">arXiv:2402.11807</a> [<a href="/pdf/2402.11807" title="Download PDF">pdf</a>, <a href="/ps/2402.11807" title="Download PostScript">ps</a>, <a href="/format/2402.11807" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Density estimation for elliptic PDE with random input by preintegration  and quasi-Monte Carlo methods
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gilbert%2C+A+D">Alexander D. Gilbert</a>, 
<a href="/search/math?searchtype=author&query=Kuo%2C+F+Y">Frances Y. Kuo</a>, 
<a href="/search/math?searchtype=author&query=Srikumar%2C+A">Abirami Srikumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we apply quasi-Monte Carlo (QMC) methods with an initial
preintegration step to estimate cumulative distribution functions and
probability density functions in uncertainty quantification (UQ). The
distribution and density functions correspond to a quantity of interest
involving the solution to an elliptic partial differential equation (PDE) with
a lognormally distributed coefficient and a normally distributed source term.
There is extensive previous work on using QMC to compute expected values in UQ,
which have proven very successful in tackling a range of different PDE
problems. However, the use of QMC for density estimation applied to UQ problems
will be explored here for the first time. Density estimation presents a more
difficult challenge compared to computing the expected value due to
discontinuities present in the integral formulations of both the distribution
and density. Our strategy is to use preintegration to eliminate the
discontinuity by integrating out a carefully selected random parameter, so that
QMC can be used to approximate the remaining integral. First, we establish
regularity results for the PDE quantity of interest that are required for
smoothing by preintegration to be effective. We then show that an $N$-point
lattice rule can be constructed for the integrands corresponding to the
distribution and density, such that after preintegration the QMC error is of
order $\mathcal{O}(N^{-1+\epsilon})$ for arbitrarily small $\epsilon&gt;0$. This
is the same rate achieved for computing the expected value of the quantity of
interest. Numerical results are presented to reaffirm our theory.
</p>
</div>
</dd>
<dt><a name="item457">[457]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11809" title="Abstract">arXiv:2402.11809</a> [<a href="/pdf/2402.11809" title="Download PDF">pdf</a>, <a href="/format/2402.11809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generation Meets Verification: Accelerating Large Language Model  Inference with Smart Parallel Auto-Correct Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yi%2C+H">Hanling Yi</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+F">Feng Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongbin Li</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+P">Peiyang Ning</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+X">Xiaotian Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+R">Rong Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This research aims to accelerate the inference speed of large language models
(LLMs) with billions of parameters. We propose \textbf{S}mart \textbf{P}arallel
\textbf{A}uto-\textbf{C}orrect d\textbf{E}coding (SPACE), an innovative
approach designed for achieving lossless acceleration of LLMs. By integrating
semi-autoregressive inference and speculative decoding capabilities, SPACE
uniquely enables autoregressive LLMs to parallelize token generation and
verification. This is realized through a specialized semi-autoregressive
supervised fine-tuning process that equips existing LLMs with the ability to
simultaneously predict multiple tokens. Additionally, an auto-correct decoding
algorithm facilitates the simultaneous generation and verification of token
sequences within a single model invocation. Through extensive experiments on a
range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x
on HumanEval-X while maintaining output quality.
</p>
</div>
</dd>
<dt><a name="item458">[458]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11810" title="Abstract">arXiv:2402.11810</a> [<a href="/pdf/2402.11810" title="Download PDF">pdf</a>, <a href="/format/2402.11810" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sensor Integration and Performance Optimizations for Mineral Exploration  using Large-scale Hybrid Multirotor UAVs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Efrem%2C+R">Robel Efrem</a>, 
<a href="/search/cs?searchtype=author&query=Coutu%2C+A">Alex Coutu</a>, 
<a href="/search/cs?searchtype=author&query=Saeedi%2C+S">Sajad Saeedi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In this paper, the focus is on improving the efficiency and precision of
mineral data collection using UAVs by addressing key challenges associated with
sensor integration. These challenges include mitigating electromagnetic
interference, reducing vibration noise, and ensuring consistent sensor
performance during flight. The paper demonstrates how innovative approaches to
these issues can significantly transform UAV-assisted mineral data collection.
Through meticulous design, testing, and evaluation, the study presents
experimental evidence of the efficacy of these methods in collecting mineral
data via UAVs. The advancements achieved in this research enable the UAV
platform to remain airborne up to 6$\times$ longer than standard
battery-powered multirotors, while still gathering high-quality mineral data.
This leads to increased operational efficiency and reduced costs in UAV-based
mineral data-gathering processes
</p>
</div>
</dd>
<dt><a name="item459">[459]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11811" title="Abstract">arXiv:2402.11811</a> [<a href="/pdf/2402.11811" title="Download PDF">pdf</a>, <a href="/format/2402.11811" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FIPO: Free-form Instruction-oriented Prompt Optimization with Preference  Dataset and Modular Fine-tuning Schema
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Junru Lu</a>, 
<a href="/search/cs?searchtype=author&query=An%2C+S">Siyu An</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yulan He</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Di Yin</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xing Sun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">In the quest to facilitate the deep intelligence of Large Language Models
(LLMs) accessible in final-end user-bot interactions, the art of prompt
crafting emerges as a critical yet complex task for the average user. Contrast
to previous model-oriented yet instruction-agnostic Automatic Prompt
Optimization methodologies, yielding polished results for predefined target
models while suffering rapid degradation with out-of-box models, we present
Free-form Instruction-oriented Prompt Optimization (FIPO). This approach is
supported by our large-scale prompt preference dataset and employs a modular
fine-tuning schema. The FIPO schema reimagines the optimization process into
manageable modules, anchored by a meta prompt that dynamically adapts content.
This allows for the flexible integration of the raw task instruction, the
optional instruction response, and the optional ground truth to produce finely
optimized task prompts. The FIPO preference dataset is meticulously constructed
using the optimal and suboptimal LLMs, undergoing rigorous cross-verification
by human experts and analytical models. Applying the insights from the data
with Tulu2 models and fine-tuning strategies, we validate the efficacy of FIPO
schema across five public benchmarks. Codes, data and scripts are here:
https://github.com/LuJunru/FIPO_Project.
</p>
</div>
</dd>
<dt><a name="item460">[460]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11812" title="Abstract">arXiv:2402.11812</a> [<a href="/pdf/2402.11812" title="Download PDF">pdf</a>, <a href="/format/2402.11812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Embedding for Ad-hoc Video Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jiaxin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Ngo%2C+C">Chong-Wah Ngo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted in ACMMM 2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
<p class="mathjax">Answering query with semantic concepts has long been the mainstream approach
for video search. Until recently, its performance is surpassed by concept-free
approach, which embeds queries in a joint space as videos. Nevertheless, the
embedded features as well as search results are not interpretable, hindering
subsequent steps in video browsing and query reformulation. This paper
integrates feature embedding and concept interpretation into a neural network
for unified dual-task learning. In this way, an embedding is associated with a
list of semantic concepts as an interpretation of video content. This paper
empirically demonstrates that, by using either the embedding features or
concepts, considerable search improvement is attainable on TRECVid benchmarked
datasets. Concepts are not only effective in pruning false positive videos, but
also highly complementary to concept-free search, leading to large margin of
improvement compared to state-of-the-art approaches.
</p>
</div>
</dd>
<dt><a name="item461">[461]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11813" title="Abstract">arXiv:2402.11813</a> [<a href="/pdf/2402.11813" title="Download PDF">pdf</a>, <a href="/format/2402.11813" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A novel framework for adaptive stress testing of autonomous vehicles in  highways
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Trinh%2C+L">Linh Trinh</a>, 
<a href="/search/cs?searchtype=author&query=Luu%2C+Q">Quang-Hung Luu</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+M">Thai M. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+H+L">Hai L. Vu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Guaranteeing the safe operations of autonomous vehicles (AVs) is crucial for
their widespread adoption and public acceptance. It is thus of a great
significance to not only assess the AV against the standard safety tests, but
also discover potential corner cases of the AV under test that could lead to
unsafe behaviour or scenario. In this paper, we propose a novel framework to
systematically explore corner cases that can result in safety concerns in a
highway traffic scenario. The framework is based on an adaptive stress testing
(AST) approach, an emerging validation method that leverages a Markov decision
process to formulate the scenarios and deep reinforcement learning (DRL) to
discover the desirable patterns representing corner cases. To this end, we
develop a new reward function for DRL to guide the AST in identifying crash
scenarios based on the collision probability estimate between the AV under test
(i.e., the ego vehicle) and the trajectory of other vehicles on the highway.
The proposed framework is further integrated with a new driving model enabling
us to create more realistic traffic scenarios capturing both the longitudinal
and lateral movements of vehicles on the highway. In our experiment, we
calibrate our model using real-world crash statistics involving automated
vehicles in California, and then we analyze the characteristics of the AV and
the framework. Quantitative and qualitative analyses of our experimental
results demonstrate that our framework outperforms other existing AST schemes.
The study can help discover crash scenarios of AV that are unknown or absent in
human driving, thereby enhancing the safety and trustworthiness of AV
technology.
</p>
</div>
</dd>
<dt><a name="item462">[462]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11814" title="Abstract">arXiv:2402.11814</a> [<a href="/pdf/2402.11814" title="Download PDF">pdf</a>, <a href="/format/2402.11814" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Evaluation of LLMs for Solving Offensive Security  Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+M">Minghao Shao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Boyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jancheska%2C+S">Sofija Jancheska</a>, 
<a href="/search/cs?searchtype=author&query=Dolan-Gavitt%2C+B">Brendan Dolan-Gavitt</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+S">Siddharth Garg</a>, 
<a href="/search/cs?searchtype=author&query=Karri%2C+R">Ramesh Karri</a>, 
<a href="/search/cs?searchtype=author&query=Shafique%2C+M">Muhammad Shafique</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
<p class="mathjax">Capture The Flag (CTF) challenges are puzzles related to computer security
scenarios. With the advent of large language models (LLMs), more and more CTF
participants are using LLMs to understand and solve the challenges. However, so
far no work has evaluated the effectiveness of LLMs in solving CTF challenges
with a fully automated workflow. We develop two CTF-solving workflows,
human-in-the-loop (HITL) and fully-automated, to examine the LLMs' ability to
solve a selected set of CTF challenges, prompted with information about the
question. We collect human contestants' results on the same set of questions,
and find that LLMs achieve higher success rate than an average human
participant. This work provides a comprehensive evaluation of the capability of
LLMs in solving real world CTF challenges, from real competition to fully
automated workflow. Our results provide references for applying LLMs in
cybersecurity education and pave the way for systematic evaluation of offensive
cybersecurity capabilities in LLMs.
</p>
</div>
</dd>
<dt><a name="item463">[463]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11815" title="Abstract">arXiv:2402.11815</a> [<a href="/pdf/2402.11815" title="Download PDF">pdf</a>, <a href="/format/2402.11815" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to  Detect Machine-Generated Text?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dipta%2C+S+R">Shubhashis Roy Dipta</a>, 
<a href="/search/cs?searchtype=author&query=Shahriar%2C+S">Sadat Shahriar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to SemEval 2024 (Colocated with NAACL 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper describes our system developed for SemEval-2024 Task 8,
"Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text
Detection." Machine-generated texts have been one of the main concerns due to
the use of large language models (LLM) in fake text generation, phishing,
cheating in exams, or even plagiarizing copyright materials. A lot of systems
have been developed to detect machine-generated text. Nonetheless, the majority
of these systems rely on the text-generating model, a limitation that is
impractical in real-world scenarios, as it's often impossible to know which
specific model the user has used for text generation. In this work, we propose
a single model based on contrastive learning, which uses ~40% of the baseline's
parameters (149M vs. 355M) but shows a comparable performance on the test
dataset (21st out of 137 participants). Our key finding is that even without an
ensemble of multiple models, a single base model can have comparable
performance with the help of data augmentation and contrastive learning.
</p>
</div>
</dd>
<dt><a name="item464">[464]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11816" title="Abstract">arXiv:2402.11816</a> [<a href="/pdf/2402.11816" title="Download PDF">pdf</a>, <a href="/format/2402.11816" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Avoiding Feature Suppression in Contrastive Learning: Learning What Has  Not Been Learned Before
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jihai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+X">Xiang Lan</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+X">Xiaoye Qu</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yu Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+M">Mengling Feng</a>, 
<a href="/search/cs?searchtype=author&query=Hooi%2C+B">Bryan Hooi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A preprint version of an ongoing work
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Self-Supervised contrastive learning has emerged as a powerful method for
obtaining high-quality representations from unlabeled data. However, feature
suppression has recently been identified in standard contrastive learning
($e.g.$, SimCLR, CLIP): in a single end-to-end training stage, the contrastive
model captures only parts of the shared information across contrasting views,
while ignore the other potentially useful information. With feature
suppression, contrastive models often fail to learn sufficient representations
capable for various downstream tasks. To mitigate the feature suppression
problem and ensure the contrastive model to learn comprehensive
representations, we develop a novel Multistage Contrastive Learning (MCL)
framework. Unlike standard contrastive learning that often result in feature
suppression, MCL progressively learn new features that have not been explored
in the previous stage, while maintaining the well-learned features. Extensive
experiments conducted on various publicly available benchmarks validate the
effectiveness of our proposed framework. In addition, we demonstrate that the
proposed MCL can be adapted to a variety of popular contrastive learning
backbones and boost their performance by learning features that could not be
gained from standard contrastive learning procedures.
</p>
</div>
</dd>
<dt><a name="item465">[465]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11818" title="Abstract">arXiv:2402.11818</a> [<a href="/pdf/2402.11818" title="Download PDF">pdf</a>, <a href="/format/2402.11818" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Where It Really Matters: Few-Shot Environmental Conservation Media  Monitoring for Low-Resource Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jain%2C+S">Sameer Jain</a>, 
<a href="/search/cs?searchtype=author&query=Keh%2C+S+S">Sedrick Scott Keh</a>, 
<a href="/search/cs?searchtype=author&query=Chettri%2C+S">Shova Chettri</a>, 
<a href="/search/cs?searchtype=author&query=Dewan%2C+K">Karun Dewan</a>, 
<a href="/search/cs?searchtype=author&query=Izquierdo%2C+P">Pablo Izquierdo</a>, 
<a href="/search/cs?searchtype=author&query=Prussman%2C+J">Johanna Prussman</a>, 
<a href="/search/cs?searchtype=author&query=Shreshtha%2C+P">Pooja Shreshtha</a>, 
<a href="/search/cs?searchtype=author&query=Suarez%2C+C">Cesar Suarez</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z+R">Zheyuan Ryan Shi</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+F">Fei Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 2024: AI for Social Impact Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Environmental conservation organizations routinely monitor news content on
conservation in protected areas to maintain situational awareness of
developments that can have an environmental impact. Existing automated media
monitoring systems require large amounts of data labeled by domain experts,
which is only feasible at scale for high-resource languages like English.
However, such tools are most needed in the global south where news of interest
is mainly in local low-resource languages, and far fewer experts are available
to annotate datasets sustainably. In this paper, we propose NewsSerow, a method
to automatically recognize environmental conservation content in low-resource
languages. NewsSerow is a pipeline of summarization, in-context few-shot
classification, and self-reflection using large language models (LLMs). Using
at most 10 demonstration example news articles in Nepali, NewsSerow
significantly outperforms other few-shot methods and achieves comparable
performance with models fully fine-tuned using thousands of examples. The World
Wide Fund for Nature (WWF) has deployed NewsSerow for media monitoring in
Nepal, significantly reducing their operational burden, and ensuring that AI
tools for conservation actually reach the communities that need them the most.
NewsSerow has also been deployed for countries with other languages like
Colombia.
</p>
</div>
</dd>
<dt><a name="item466">[466]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11819" title="Abstract">arXiv:2402.11819</a> [<a href="/pdf/2402.11819" title="Download PDF">pdf</a>, <a href="/format/2402.11819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Head-wise Shareable Attention for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zouying Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yifei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hai Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large Language Models (LLMs) suffer from huge number of parameters, which
restricts their deployment on edge devices. Weight sharing is one promising
solution that encourages weight reuse, effectively reducing memory usage with
less performance drop. However, current weight sharing techniques primarily
focus on small-scale models like BERT and employ coarse-grained sharing rules,
e.g., layer-wise. This becomes limiting given the prevalence of LLMs and
sharing an entire layer or block obviously diminishes the flexibility of weight
sharing. In this paper, we present a perspective on $\textit{$\textbf{head-wise
shareable attention for large language models}$}$. We further propose two
memory-efficient methods that share parameters across attention heads, with a
specific focus on LLMs. Both of them use the same dynamic strategy to select
the shared weight matrices. The first method directly reuses the pre-trained
weights without retraining, denoted as $\textbf{DirectShare}$. The second
method first post-trains with constraint on weight matrix similarity and then
shares, denoted as $\textbf{PostShare}$. Experimental results reveal our
head-wise shared models still maintain satisfactory capabilities, demonstrating
the feasibility of fine-grained weight sharing applied to LLMs.
</p>
</div>
</dd>
<dt><a name="item467">[467]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11820" title="Abstract">arXiv:2402.11820</a> [<a href="/pdf/2402.11820" title="Download PDF">pdf</a>, <a href="/ps/2402.11820" title="Download PostScript">ps</a>, <a href="/format/2402.11820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A critical analysis of cognitive load measurement methods for evaluating  the usability of different types of interfaces: guidelines and framework for  Human-Computer Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Darejeh%2C+A">Ali Darejeh</a>, 
<a href="/search/cs?searchtype=author&query=Marcusa%2C+N">Nadine Marcusa</a>, 
<a href="/search/cs?searchtype=author&query=Mohammadi%2C+G">Gelareh Mohammadi</a>, 
<a href="/search/cs?searchtype=author&query=Sweller%2C+J">John Sweller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Usability testing is an essential part of product design, particularly for
user interfaces. To enhance the reliability of usability evaluations, employing
cognitive load measurement methods can be highly effective in assessing the
mental effort required to complete tasks during user testing. This review aims
to provide an overview of the most suitable cognitive load measurement methods
for evaluating various types of user interfaces, serving as a valuable resource
for guiding usability assessments. To bridge the existing gap in the
literature, a systematic review was conducted, analyzing 76 articles with
experimental study designs that met the eligibility criteria. The review
encompasses different methods of measuring cognitive load applicable to
assessing the usability of diverse user interfaces, including computer
software, information systems, video games, web and mobile applications,
robotics, and virtual reality applications. The results highlight the most
widely utilized cognitive load measurement methods in software usability, their
respective usage percentages, and their application in evaluating the usability
of each user interface type. Additionally, the advantages and disadvantages of
each method are discussed. Furthermore, the review proposes a framework to
assist usability testers in selecting an appropriate cognitive load measurement
method for conducting accurate usability evaluations.
</p>
</div>
</dd>
<dt><a name="item468">[468]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11821" title="Abstract">arXiv:2402.11821</a> [<a href="/pdf/2402.11821" title="Download PDF">pdf</a>, <a href="/format/2402.11821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Microstructures and Accuracy of Graph Recall by Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanbang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+H">Hejie Cui</a>, 
<a href="/search/cs?searchtype=author&query=Kleinberg%2C+J">Jon Kleinberg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 7 tables, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL); Information Retrieval (cs.IR); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Graphs data is crucial for many applications, and much of it exists in the
relations described in textual format. As a result, being able to accurately
recall and encode a graph described in earlier text is a basic yet pivotal
ability that LLMs need to demonstrate if they are to perform reasoning tasks
that involve graph-structured information. Human performance at graph recall by
has been studied by cognitive scientists for decades, and has been found to
often exhibit certain structural patterns of bias that align with human
handling of social relationships. To date, however, we know little about how
LLMs behave in analogous graph recall tasks: do their recalled graphs also
exhibit certain biased patterns, and if so, how do they compare with humans and
affect other graph reasoning tasks? In this work, we perform the first
systematical study of graph recall by LLMs, investigating the accuracy and
biased microstructures (local structural patterns) in their recall. We find
that LLMs not only underperform often in graph recall, but also tend to favor
more triangles and alternating 2-paths. Moreover, we find that more advanced
LLMs have a striking dependence on the domain that a real-world graph comes
from -- by yielding the best recall accuracy when the graph is narrated in a
language style consistent with its original domain.
</p>
</div>
</dd>
<dt><a name="item469">[469]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11823" title="Abstract">arXiv:2402.11823</a> [<a href="/pdf/2402.11823" title="Download PDF">pdf</a>, <a href="/format/2402.11823" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying Periods of Cyclical Stress in University Students Using  Wearables In-the-Wild
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Neigel%2C+P">Peter Neigel</a>, 
<a href="/search/cs?searchtype=author&query=Vargo%2C+A">Andrew Vargo</a>, 
<a href="/search/cs?searchtype=author&query=Tag%2C+B">Benjamin Tag</a>, 
<a href="/search/cs?searchtype=author&query=Kise%2C+K">Koichi Kise</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 6 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
<p class="mathjax">University students encounter various forms of stress during their academic
journey, including cyclical stress associated with final exams. Supporting
their well-being means helping them manage their stress levels. In this study,
we used a wearable health-tracking ring on a cohort of 103 Japanese university
students for up to 28 months in the wild. The study aimed to investigate
whether group-wide biomarkers of stress can be identified in a sample having
similar daily schedules and whether these occurrences can be pinpointed to
specific periods of the academic year. We found population-wide increased
stress markers during exams, New Year's, and job hunting season, a Japanese job
market peculiarity. Our results highlight the available potential of
unobtrusive, in-situ detection of the current mental state of university
student populations using off-the-shelf wearables from noisy data, with
significant implications for the well-being of the users. Our approach and
method of analysis allows for monitoring the student body's stress level
without singling out individuals and therefore represents a privacy-preserving
method. This way, new and sudden stress increases can be recognized, which can
help identify the stressor and inform the design and introduction of counter
measures.
</p>
</div>
</dd>
<dt><a name="item470">[470]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11826" title="Abstract">arXiv:2402.11826</a> [<a href="/pdf/2402.11826" title="Download PDF">pdf</a>, <a href="/format/2402.11826" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unveiling the Depths: A Multi-Modal Fusion Framework for Challenging  Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jialei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xianming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Junjun Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+K">Kui Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Rui Li</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+K">Kai Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+X">Xiangyang Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Monocular depth estimation from RGB images plays a pivotal role in 3D vision.
However, its accuracy can deteriorate in challenging environments such as
nighttime or adverse weather conditions. While long-wave infrared cameras offer
stable imaging in such challenging conditions, they are inherently
low-resolution, lacking rich texture and semantics as delivered by the RGB
image. Current methods focus solely on a single modality due to the
difficulties to identify and integrate faithful depth cues from both sources.
To address these issues, this paper presents a novel approach that identifies
and integrates dominant cross-modality depth features with a learning-based
framework. Concretely, we independently compute the coarse depth maps with
separate networks by fully utilizing the individual depth cues from each
modality. As the advantageous depth spreads across both modalities, we propose
a novel confidence loss steering a confidence predictor network to yield a
confidence map specifying latent potential depth areas. With the resulting
confidence map, we propose a multi-modal fusion network that fuses the final
depth in an end-to-end manner. Harnessing the proposed pipeline, our method
demonstrates the ability of robust depth estimation in a variety of difficult
scenarios. Experimental results on the challenging MS$^2$ and ViViD++ datasets
demonstrate the effectiveness and robustness of our method.
</p>
</div>
</dd>
<dt><a name="item471">[471]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11827" title="Abstract">arXiv:2402.11827</a> [<a href="/pdf/2402.11827" title="Download PDF">pdf</a>, <a href="/format/2402.11827" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ask Optimal Questions: Aligning Large Language Models with Retriever&#x27;s  Preference in Conversational Search
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yoon%2C+C">Chanwoong Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+G">Gangwoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Jeon%2C+B">Byeongguk Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sungdong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Jo%2C+Y">Yohan Jo</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jaewoo Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Conversational search, unlike single-turn retrieval tasks, requires
understanding the current question within a dialogue context. The common
approach of rewrite-then-retrieve aims to decontextualize questions to be
self-sufficient for off-the-shelf retrievers, but most existing methods produce
sub-optimal query rewrites due to the limited ability to incorporate signals
from the retrieval results. To overcome this limitation, we present a novel
framework RetPO (Retriever's Preference Optimization), which is designed to
optimize a language model (LM) for reformulating search queries in line with
the preferences of the target retrieval systems. The process begins by
prompting a large LM to produce various potential rewrites and then collects
retrieval performance for these rewrites as the retrievers' preferences.
Through the process, we construct a large-scale dataset called RF collection,
containing Retrievers' Feedback on over 410K query rewrites across 12K
conversations. Furthermore, we fine-tune a smaller LM using this dataset to
align it with the retrievers' preferences as feedback. The resulting model
achieves state-of-the-art performance on two recent conversational search
benchmarks, significantly outperforming existing baselines, including GPT-3.5.
</p>
</div>
</dd>
<dt><a name="item472">[472]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11829" title="Abstract">arXiv:2402.11829</a> [<a href="/pdf/2402.11829" title="Download PDF">pdf</a>, <a href="/ps/2402.11829" title="Download PostScript">ps</a>, <a href="/format/2402.11829" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deployment of Advanced and Intelligent Logistics Vehicles with Enhanced  Tracking and Security Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Siddique%2C+I+M">Iqtiar Md Siddique</a>, 
<a href="/search/cs?searchtype=author&query=Molla%2C+S">Selim Molla</a>, 
<a href="/search/cs?searchtype=author&query=Hasan%2C+M+R">MD Rakib Hasan</a>, 
<a href="/search/cs?searchtype=author&query=Siddique%2C+A+A">Anamika Ahmed Siddique</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of IoT and Machine Learning,2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">This study focuses on the implementation of modern and intelligent logistics
vehicles equipped with advanced tracking and security features. In response to
the evolving landscape of logistics management, the proposed system integrates
cutting edge technologies to enhance efficiency and ensure the security of the
entire logistics process. The core component of this implementation is the
incorporation of state-of-the art tracking mechanisms, enabling real-time
monitoring of vehicle locations and movements. Furthermore, the system
addresses the paramount concern of security by introducing advanced security
measures. Through the utilization of sophisticated tracking technologies and
security protocols, the proposed logistics vehicles aim to safeguard both
customer and provider data. The implementation includes the integration of QR
code concepts, creating a binary image system that conceals sensitive
information and ensures access only to authorized users. In addition to
tracking and security, the study delves into the realm of information mining,
employing techniques such as classification, clustering, and recommendation to
extract meaningful patterns from vast datasets. Collaborative filtering
techniques are incorporated to enhance customer experience by recommending
services based on user preferences and historical data. This abstract
encapsulates the comprehensive approach of deploying modern logistics vehicles,
emphasizing their intelligence through advanced tracking, robust security
measures, and data-driven insights. The proposed system aims to revolutionize
logistics management, providing a seamless and secure experience for both
customers and service providers in the dynamic logistics landscape.
</p>
</div>
</dd>
<dt><a name="item473">[473]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11831" title="Abstract">arXiv:2402.11831</a> [<a href="/pdf/2402.11831" title="Download PDF">pdf</a>, <a href="/format/2402.11831" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rock Classification Based on Residual Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhoubian%2C+S">Sining Zhoubian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Z">Zhihuan Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Rock Classification is an essential geological problem since it provides
important formation information. However, exploration on this problem using
convolutional neural networks is not sufficient. To tackle this problem, we
propose two approaches using residual neural networks. We first adopt data
augmentation methods to enlarge our dataset. By modifying kernel sizes,
normalization methods and composition based on ResNet34, we achieve an accuracy
of 70.1% on the test dataset, with an increase of 3.5% compared to regular
Resnet34. Furthermore, using a similar backbone like BoTNet that incorporates
multihead self attention, we additionally use internal residual connections in
our model. This boosts the model's performance, achieving an accuracy of 73.7%
on the test dataset. We also explore how the number of bottleneck transformer
blocks may influence model performance. We discover that models with more than
one bottleneck transformer block may not further improve performance. Finally,
we believe that our approach can inspire future work related to this problem
and our model design can facilitate the development of new residual model
architectures.
</p>
</div>
</dd>
<dt><a name="item474">[474]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11834" title="Abstract">arXiv:2402.11834</a> [<a href="/pdf/2402.11834" title="Download PDF">pdf</a>, <a href="/ps/2402.11834" title="Download PostScript">ps</a>, <a href="/format/2402.11834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Terahertz User-Centric Clustering in the Presence of Beam Misalignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Humadi%2C+K">Khaled Humadi</a>, 
<a href="/search/eess?searchtype=author&query=Trigui%2C+I">Imene Trigui</a>, 
<a href="/search/eess?searchtype=author&query=Zhu%2C+W">Wei-Ping Zhu</a>, 
<a href="/search/eess?searchtype=author&query=Ajib%2C+W">Wessam Ajib</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">Beam misalignment is one of the main challenges for the design of reliable
wireless systems in terahertz (THz) bands. This paper investigates how to apply
user-centric base station (BS) clustering as a valuable add-on in THz networks.
In particular, to reduce the impact of beam misalignment, a user-centric BS
clustering design that provides multi-connectivity via BS cooperation is
investigated. The coverage probability is derived by leveraging an accurate
approximation of the aggregate interference distribution that captures the
effect of beam misalignment and THz fading. The numerical results reveal the
impact of beam misalignment with respect to crucial link parameters, such as
the transmitter's beam width and the serving cluster size, demonstrating that
user-centric BS clustering is a promising enabler of THz networks.
</p>
</div>
</dd>
<dt><a name="item475">[475]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11835" title="Abstract">arXiv:2402.11835</a> [<a href="/pdf/2402.11835" title="Download PDF">pdf</a>, <a href="/format/2402.11835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Easy as ABCs: Unifying Boltzmann Q-Learning and Counterfactual Regret  Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=D%27Amico-Wong%2C+L">Luca D&#x27;Amico-Wong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hugh Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lanctot%2C+M">Marc Lanctot</a>, 
<a href="/search/cs?searchtype=author&query=Parkes%2C+D+C">David C. Parkes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Science and Game Theory (cs.GT); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">We propose ABCs (Adaptive Branching through Child stationarity), a
best-of-both-worlds algorithm combining Boltzmann Q-learning (BQL), a classic
reinforcement learning algorithm for single-agent domains, and counterfactual
regret minimization (CFR), a central algorithm for learning in multi-agent
domains. ABCs adaptively chooses what fraction of the environment to explore
each iteration by measuring the stationarity of the environment's reward and
transition dynamics. In Markov decision processes, ABCs converges to the
optimal policy with at most an O(A) factor slowdown compared to BQL, where A is
the number of actions in the environment. In two-player zero-sum games, ABCs is
guaranteed to converge to a Nash equilibrium (assuming access to a perfect
oracle for detecting stationarity), while BQL has no such guarantees.
Empirically, ABCs demonstrates strong performance when benchmarked across
environments drawn from the OpenSpiel game library and OpenAI Gym and exceeds
all prior methods in environments which are neither fully stationary nor fully
nonstationary.
</p>
</div>
</dd>
<dt><a name="item476">[476]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11836" title="Abstract">arXiv:2402.11836</a> [<a href="/pdf/2402.11836" title="Download PDF">pdf</a>, <a href="/format/2402.11836" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DIO: Dataset of 3D Mesh Models of Indoor Objects for Robotics and  Computer Vision Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nimal%2C+N">Nillan Nimal</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Wenbin Li</a>, 
<a href="/search/cs?searchtype=author&query=Clark%2C+R">Ronald Clark</a>, 
<a href="/search/cs?searchtype=author&query=Saeedi%2C+S">Sajad Saeedi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">The creation of accurate virtual models of real-world objects is imperative
to robotic simulations and applications such as computer vision, artificial
intelligence, and machine learning. This paper documents the different methods
employed for generating a database of mesh models of real-world objects. These
methods address the tedious and time-intensive process of manually generating
the models using CAD software. Essentially, DSLR/phone cameras were employed to
acquire images of target objects. These images were processed using a
photogrammetry software known as Meshroom to generate a dense surface
reconstruction of the scene. The result produced by Meshroom was edited and
simplified using MeshLab, a mesh-editing software to produce the final model.
Based on the obtained models, this process was effective in modelling the
geometry and texture of real-world objects with high fidelity. An active 3D
scanner was also utilized to accelerate the process for large objects. All
generated models and captured images are made available on the website of the
project.
</p>
</div>
</dd>
<dt><a name="item477">[477]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11837" title="Abstract">arXiv:2402.11837</a> [<a href="/pdf/2402.11837" title="Download PDF">pdf</a>, <a href="/format/2402.11837" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Guided Robust Graph Structure Refinement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=In%2C+Y">Yeonjun In</a>, 
<a href="/search/cs?searchtype=author&query=Yoon%2C+K">Kanghoon Yoon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kibum Kim</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+K">Kijung Shin</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+C">Chanyoung Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by TheWebConf 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Recent studies have revealed that GNNs are vulnerable to adversarial attacks.
To defend against such attacks, robust graph structure refinement (GSR) methods
aim at minimizing the effect of adversarial edges based on node features, graph
structure, or external information. However, we have discovered that existing
GSR methods are limited by narrowassumptions, such as assuming clean node
features, moderate structural attacks, and the availability of external clean
graphs, resulting in the restricted applicability in real-world scenarios. In
this paper, we propose a self-guided GSR framework (SG-GSR), which utilizes a
clean sub-graph found within the given attacked graph itself. Furthermore, we
propose a novel graph augmentation and a group-training strategy to handle the
two technical challenges in the clean sub-graph extraction: 1) loss of
structural information, and 2) imbalanced node degree distribution. Extensive
experiments demonstrate the effectiveness of SG-GSR under various scenarios
including non-targeted attacks, targeted attacks, feature attacks, e-commerce
fraud, and noisy node labels. Our code is available at
https://github.com/yeonjun-in/torch-SG-GSR.
</p>
</div>
</dd>
<dt><a name="item478">[478]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11838" title="Abstract">arXiv:2402.11838</a> [<a href="/pdf/2402.11838" title="Download PDF">pdf</a>, <a href="/format/2402.11838" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yuan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+J">Jingtao Ding</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jie Feng</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+D">Depeng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Urban spatio-temporal prediction is crucial for informed decision-making,
such as transportation management, resource optimization, and urban planning.
Although pretrained foundation models for natural languages have experienced
remarkable breakthroughs, wherein one general-purpose model can tackle multiple
tasks across various domains, urban spatio-temporal modeling lags behind.
Existing approaches for urban prediction are usually tailored for specific
spatio-temporal scenarios, requiring task-specific model designs and extensive
in-domain training data. In this work, we propose a universal model, UniST, for
urban spatio-temporal prediction. Drawing inspiration from large language
models, UniST achieves success through: (i) flexibility towards diverse
spatio-temporal data characteristics, (ii) effective generative pre-training
with elaborated masking strategies to capture complex spatio-temporal
relationships, (iii) spatio-temporal knowledge-guided prompts that align and
leverage intrinsic and shared knowledge across scenarios. These designs
together unlock the potential of a one-for-all model for spatio-temporal
prediction with powerful generalization capability. Extensive experiments on 15
cities and 6 domains demonstrate the universality of UniST in advancing
state-of-the-art prediction performance, especially in few-shot and zero-shot
scenarios.
</p>
</div>
</dd>
<dt><a name="item479">[479]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11839" title="Abstract">arXiv:2402.11839</a> [<a href="/pdf/2402.11839" title="Download PDF">pdf</a>, <a href="/ps/2402.11839" title="Download PostScript">ps</a>, <a href="/format/2402.11839" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An enhanced Teaching-Learning-Based Optimization (TLBO) with Grey Wolf  Optimizer (GWO) for text feature selection and clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azarshab%2C+M">Mahsa Azarshab</a>, 
<a href="/search/cs?searchtype=author&query=Fathian%2C+M">Mohammad Fathian</a>, 
<a href="/search/cs?searchtype=author&query=Amiri%2C+B">Babak Amiri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
<p class="mathjax">Text document clustering can play a vital role in organizing and handling the
everincreasing number of text documents. Uninformative and redundant features
included in large text documents reduce the effectiveness of the clustering
algorithm. Feature selection (FS) is a well-known technique for removing these
features. Since FS can be formulated as an optimization problem, various
meta-heuristic algorithms have been employed to solve it.
Teaching-Learning-Based Optimization (TLBO) is a novel meta-heuristic algorithm
that benefits from the low number of parameters and fast convergence. A hybrid
method can simultaneously benefit from the advantages of TLBO and tackle the
possible entrapment in the local optimum. By proposing a hybrid of TLBO, Grey
Wolf Optimizer (GWO), and Genetic Algorithm (GA) operators, this paper suggests
a filter-based FS algorithm (TLBO-GWO). Six benchmark datasets are selected,
and TLBO-GWO is compared with three recently proposed FS algorithms with
similar approaches, the main TLBO and GWO. The comparison is conducted based on
clustering evaluation measures, convergence behavior, and dimension reduction,
and is validated using statistical tests. The results reveal that TLBO-GWO can
significantly enhance the effectiveness of the text clustering technique
(K-means).
</p>
</div>
</dd>
<dt><a name="item480">[480]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11840" title="Abstract">arXiv:2402.11840</a> [<a href="/pdf/2402.11840" title="Download PDF">pdf</a>, <a href="/format/2402.11840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Endoscopic Chisel: Intraoperative Imaging Carves 3D Anatomical Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mangulabnan%2C+J+E">Jan Emily Mangulabnan</a>, 
<a href="/search/cs?searchtype=author&query=Soberanis-Mukul%2C+R+D">Roger D. Soberanis-Mukul</a>, 
<a href="/search/cs?searchtype=author&query=Teufel%2C+T">Timo Teufel</a>, 
<a href="/search/cs?searchtype=author&query=Sahu%2C+M">Manish Sahu</a>, 
<a href="/search/cs?searchtype=author&query=Porras%2C+J+L">Jose L. Porras</a>, 
<a href="/search/cs?searchtype=author&query=Vedula%2C+S+S">S. Swaroop Vedula</a>, 
<a href="/search/cs?searchtype=author&query=Ishii%2C+M">Masaru Ishii</a>, 
<a href="/search/cs?searchtype=author&query=Hager%2C+G">Gregory Hager</a>, 
<a href="/search/cs?searchtype=author&query=Taylor%2C+R+H">Russell H. Taylor</a>, 
<a href="/search/cs?searchtype=author&query=Unberath%2C+M">Mathias Unberath</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Purpose: Preoperative imaging plays a pivotal role in sinus surgery where CTs
offer patient-specific insights of complex anatomy, enabling real-time
intraoperative navigation to complement endoscopy imaging. However, surgery
elicits anatomical changes not represented in the preoperative model,
generating an inaccurate basis for navigation during surgery progression.
<br />Methods: We propose a first vision-based approach to update the preoperative
3D anatomical model leveraging intraoperative endoscopic video for navigated
sinus surgery where relative camera poses are known. We rely on comparisons of
intraoperative monocular depth estimates and preoperative depth renders to
identify modified regions. The new depths are integrated in these regions
through volumetric fusion in a truncated signed distance function
representation to generate an intraoperative 3D model that reflects tissue
manipulation.
<br />Results: We quantitatively evaluate our approach by sequentially updating
models for a five-step surgical progression in an ex vivo specimen. We compute
the error between correspondences from the updated model and ground-truth
intraoperative CT in the region of anatomical modification. The resulting
models show a decrease in error during surgical progression as opposed to
increasing when no update is employed.
<br />Conclusion: Our findings suggest that preoperative 3D anatomical models can
be updated using intraoperative endoscopy video in navigated sinus surgery.
Future work will investigate improvements to monocular depth estimation as well
as removing the need for external navigation systems. The resulting ability to
continuously update the patient model may provide surgeons with a more precise
understanding of the current anatomical state and paves the way toward a
digital twin paradigm for sinus surgery.
</p>
</div>
</dd>
<dt><a name="item481">[481]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11841" title="Abstract">arXiv:2402.11841</a> [<a href="/pdf/2402.11841" title="Download PDF">pdf</a>, <a href="/format/2402.11841" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ASGNet: Adaptive Semantic Gate Networks for Log-Based Anomaly Diagnosis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Haitian Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+D">Degang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yanshu Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weiqing Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Logs are widely used in the development and maintenance of software systems.
Logs can help engineers understand the runtime behavior of systems and diagnose
system failures. For anomaly diagnosis, existing methods generally use log
event data extracted from historical logs to build diagnostic models. However,
we find that existing methods do not make full use of two types of features,
(1) statistical features: some inherent statistical features in log data, such
as word frequency and abnormal label distribution, are not well exploited.
Compared with log raw data, statistical features are deterministic and
naturally compatible with corresponding tasks. (2) semantic features: Logs
contain the execution logic behind software systems, thus log statements share
deep semantic relationships. How to effectively combine statistical features
and semantic features in log data to improve the performance of log anomaly
diagnosis is the key point of this paper. In this paper, we propose an adaptive
semantic gate networks (ASGNet) that combines statistical features and semantic
features to selectively use statistical features to consolidate log text
semantic representation. Specifically, ASGNet encodes statistical features via
a variational encoding module and fuses useful information through a
well-designed adaptive semantic threshold mechanism. The threshold mechanism
introduces the information flow into the classifier based on the confidence of
the semantic features in the decision, which is conducive to training a robust
classifier and can solve the overfitting problem caused by the use of
statistical features. The experimental results on the real data set show that
our method proposed is superior to all baseline methods in terms of various
performance indicators.
</p>
</div>
</dd>
<dt><a name="item482">[482]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11842" title="Abstract">arXiv:2402.11842</a> [<a href="/pdf/2402.11842" title="Download PDF">pdf</a>, <a href="/format/2402.11842" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CodeArt: Better Code Models by Attention Regularization When Symbols Are  Lacking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+Z">Zian Su</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiangzhe Xu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Ziyang Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yapeng Ye</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jianjun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiangyu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Transformer based code models have impressive performance in many software
engineering tasks. However, their effectiveness degrades when symbols are
missing or not informative. The reason is that the model may not learn to pay
attention to the right correlations/contexts without the help of symbols. We
propose a new method to pre-train general code models when symbols are lacking.
We observe that in such cases, programs degenerate to something written in a
very primitive language. We hence propose to use program analysis to extract
contexts a priori (instead of relying on symbols and masked language modeling
as in vanilla models). We then leverage a novel attention masking method to
only allow the model attending to these contexts, e.g., bi-directional program
dependence transitive closures and token co-occurrences. In the meantime, the
inherent self-attention mechanism is utilized to learn which of the allowed
attentions are more important compared to others. To realize the idea, we
enhance the vanilla tokenization and model architecture of a BERT model,
construct and utilize attention masks, and introduce a new pre-training
algorithm. We pre-train this BERT-like model from scratch, using a dataset of
26 million stripped binary functions with explicit program dependence
information extracted by our tool. We apply the model in three downstream
tasks: binary similarity, type inference, and malware family classification.
Our pre-trained model can improve the SOTAs in these tasks from 53% to 64%, 49%
to 60%, and 74% to 94%, respectively. It also substantially outperforms other
general pre-training techniques of code understanding models.
</p>
</div>
</dd>
<dt><a name="item483">[483]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11843" title="Abstract">arXiv:2402.11843</a> [<a href="/pdf/2402.11843" title="Download PDF">pdf</a>, <a href="/format/2402.11843" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WildFake: A Large-scale Challenging Dataset for AI-Generated Images  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+Y">Yan Hong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianfu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The extraordinary ability of generative models enabled the generation of
images with such high quality that human beings cannot distinguish Artificial
Intelligence (AI) generated images from real-life photographs. The development
of generation techniques opened up new opportunities but concurrently
introduced potential risks to privacy, authenticity, and security. Therefore,
the task of detecting AI-generated imagery is of paramount importance to
prevent illegal activities. To assess the generalizability and robustness of
AI-generated image detection, we present a large-scale dataset, referred to as
WildFake, comprising state-of-the-art generators, diverse object categories,
and real-world applications. WildFake dataset has the following advantages: 1)
Rich Content with Wild collection: WildFake collects fake images from the
open-source community, enriching its diversity with a broad range of image
classes and image styles. 2) Hierarchical structure: WildFake contains fake
images synthesized by different types of generators from GANs, diffusion
models, to other generative models. These key strengths enhance the
generalization and robustness of detectors trained on WildFake, thereby
demonstrating WildFake's considerable relevance and effectiveness for
AI-generated detectors in real-world scenarios. Moreover, our extensive
evaluation experiments are tailored to yield profound insights into the
capabilities of different levels of generative models, a distinctive advantage
afforded by WildFake's unique hierarchical structure.
</p>
</div>
</dd>
<dt><a name="item484">[484]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11845" title="Abstract">arXiv:2402.11845</a> [<a href="/pdf/2402.11845" title="Download PDF">pdf</a>, <a href="/format/2402.11845" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modularized Networks for Few-shot Hateful Meme Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+R">Rui Cao</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+R+K">Roy Ka-Wei Lee</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jing Jiang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> camera-ready for WWW, 2024, Web4Good
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">In this paper, we address the challenge of detecting hateful memes in the
low-resource setting where only a few labeled examples are available. Our
approach leverages the compositionality of Low-rank adaptation (LoRA), a widely
used parameter-efficient tuning technique. We commence by fine-tuning large
language models (LLMs) with LoRA on selected tasks pertinent to hateful meme
detection, thereby generating a suite of LoRA modules. These modules are
capable of essential reasoning skills for hateful meme detection. We then use
the few available annotated samples to train a module composer, which assigns
weights to the LoRA modules based on their relevance. The model's learnable
parameters are directly proportional to the number of LoRA modules. This
modularized network, underpinned by LLMs and augmented with LoRA modules,
exhibits enhanced generalization in the context of hateful meme detection. Our
evaluation spans three datasets designed for hateful meme detection in a
few-shot learning context. The proposed method demonstrates superior
performance to traditional in-context learning, which is also more
computationally intensive during inference.We then use the few available
annotated samples to train a module composer, which assigns weights to the LoRA
modules based on their relevance. The model's learnable parameters are directly
proportional to the number of LoRA modules. This modularized network,
underpinned by LLMs and augmented with LoRA modules, exhibits enhanced
generalization in the context of hateful meme detection. Our evaluation spans
three datasets designed for hateful meme detection in a few-shot learning
context. The proposed method demonstrates superior performance to traditional
in-context learning, which is also more computationally intensive during
inference.
</p>
</div>
</dd>
<dt><a name="item485">[485]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11846" title="Abstract">arXiv:2402.11846</a> [<a href="/pdf/2402.11846" title="Download PDF">pdf</a>, <a href="/format/2402.11846" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UnlearnCanvas: A Stylized Image Dataset to Benchmark Machine Unlearning  for Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yihua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yimeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuguang Yao</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+J">Jinghan Jia</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiancheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xiaoming Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sijia Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The rapid advancement of diffusion models (DMs) has not only transformed
various real-world industries but has also introduced negative societal
concerns, including the generation of harmful content, copyright disputes, and
the rise of stereotypes and biases. To mitigate these issues, machine
unlearning (MU) has emerged as a potential solution, demonstrating its ability
to remove undesired generative capabilities of DMs in various applications.
However, by examining existing MU evaluation methods, we uncover several key
challenges that can result in incomplete, inaccurate, or biased evaluations for
MU in DMs. To address them, we enhance the evaluation metrics for MU, including
the introduction of an often-overlooked retainability measurement for DMs
post-unlearning. Additionally, we introduce UnlearnCanvas, a comprehensive
high-resolution stylized image dataset that facilitates us to evaluate the
unlearning of artistic painting styles in conjunction with associated image
objects. We show that this dataset plays a pivotal role in establishing a
standardized and automated evaluation framework for MU techniques on DMs,
featuring 7 quantitative metrics to address various aspects of unlearning
effectiveness. Through extensive experiments, we benchmark 5 state-of-the-art
MU methods, revealing novel insights into their pros and cons, and the
underlying unlearning mechanisms. Furthermore, we demonstrate the potential of
UnlearnCanvas to benchmark other generative modeling tasks, such as style
transfer. The UnlearnCanvas dataset, benchmark, and the codes to reproduce all
the results in this work can be found at
https://github.com/OPTML-Group/UnlearnCanvas.
</p>
</div>
</dd>
<dt><a name="item486">[486]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11849" title="Abstract">arXiv:2402.11849</a> [<a href="/pdf/2402.11849" title="Download PDF">pdf</a>, <a href="/format/2402.11849" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ComFusion: Personalized Subject Generation in Multiple Specific Scenes  From Single Image
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+Y">Yan Hong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianfu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recent advancements in personalizing text-to-image (T2I) diffusion models
have shown the capability to generate images based on personalized visual
concepts using a limited number of user-provided examples. However, these
models often struggle with maintaining high visual fidelity, particularly in
manipulating scenes as defined by textual inputs. Addressing this, we introduce
ComFusion, a novel approach that leverages pretrained models generating
composition of a few user-provided subject images and predefined-text scenes,
effectively fusing visual-subject instances with textual-specific scenes,
resulting in the generation of high-fidelity instances within diverse scenes.
ComFusion integrates a class-scene prior preservation regularization, which
leverages composites the subject class and scene-specific knowledge from
pretrained models to enhance generation fidelity. Additionally, ComFusion uses
coarse generated images, ensuring they align effectively with both the instance
image and scene texts. Consequently, ComFusion maintains a delicate balance
between capturing the essence of the subject and maintaining scene
fidelity.Extensive evaluations of ComFusion against various baselines in T2I
personalization have demonstrated its qualitative and quantitative superiority.
</p>
</div>
</dd>
<dt><a name="item487">[487]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11853" title="Abstract">arXiv:2402.11853</a> [<a href="/pdf/2402.11853" title="Download PDF">pdf</a>, <a href="/format/2402.11853" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Voice Assistants: Exploring Advantages and Risks of an In-Car  Social Robot in Real Driving Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yuanchao Li</a>, 
<a href="/search/cs?searchtype=author&query=Urquhart%2C+L">Lachlan Urquhart</a>, 
<a href="/search/cs?searchtype=author&query=Karatas%2C+N">Nihan Karatas</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+S">Shun Shao</a>, 
<a href="/search/cs?searchtype=author&query=Ishiguro%2C+H">Hiroshi Ishiguro</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+X">Xun Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ACM Transactions on Computer-Human Interaction
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computers and Society (cs.CY); Robotics (cs.RO)

</div>
<p class="mathjax">In-car Voice Assistants (VAs) play an increasingly critical role in
automotive user interface design. However, existing VAs primarily perform
simple 'query-answer' tasks, limiting their ability to sustain drivers'
long-term attention. In this study, we investigate the effectiveness of an
in-car Robot Assistant (RA) that offers functionalities beyond voice
interaction. We aim to answer the question: How does the presence of a social
robot impact user experience in real driving scenarios? Our study begins with a
user survey to understand perspectives on in-car VAs and their influence on
driving experiences. We then conduct non-driving and on-road experiments with
selected participants to assess user experiences with an RA. Additionally, we
conduct subjective ratings to evaluate user perceptions of the RA's
personality, which is crucial for robot design. We also explore potential
concerns regarding ethical risks. Finally, we provide a comprehensive
discussion and recommendations for the future development of in-car RAs.
</p>
</div>
</dd>
<dt><a name="item488">[488]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11855" title="Abstract">arXiv:2402.11855</a> [<a href="/pdf/2402.11855" title="Download PDF">pdf</a>, <a href="/format/2402.11855" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TriSampler: A Better Negative Sampling Principle for Dense Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Z">Zhou Shao</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yuxiao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jie Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">Negative sampling stands as a pivotal technique in dense retrieval, essential
for training effective retrieval models and significantly impacting retrieval
performance. While existing negative sampling methods have made commendable
progress by leveraging hard negatives, a comprehensive guiding principle for
constructing negative candidates and designing negative sampling distributions
is still lacking. To bridge this gap, we embark on a theoretical analysis of
negative sampling in dense retrieval. This exploration culminates in the
unveiling of the quasi-triangular principle, a novel framework that elucidates
the triangular-like interplay between query, positive document, and negative
document. Fueled by this guiding principle, we introduce TriSampler, a
straightforward yet highly effective negative sampling method. The keypoint of
TriSampler lies in its ability to selectively sample more informative negatives
within a prescribed constrained region. Experimental evaluation show that
TriSampler consistently attains superior retrieval performance across a diverse
of representative retrieval models.
</p>
</div>
</dd>
<dt><a name="item489">[489]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11857" title="Abstract">arXiv:2402.11857</a> [<a href="/pdf/2402.11857" title="Download PDF">pdf</a>, <a href="/format/2402.11857" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Communication-Efficient Distributed Learning with Local Immediate Error  Compensation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yifei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Linli Xu</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+X">Xun Qian</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Shiwei Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yiming Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Enhong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Gradient compression with error compensation has attracted significant
attention with the target of reducing the heavy communication overhead in
distributed learning. However, existing compression methods either perform only
unidirectional compression in one iteration with higher communication cost, or
bidirectional compression with slower convergence rate. In this work, we
propose the Local Immediate Error Compensated SGD (LIEC-SGD) optimization
algorithm to break the above bottlenecks based on bidirectional compression and
carefully designed compensation approaches. Specifically, the bidirectional
compression technique is to reduce the communication cost, and the compensation
technique compensates the local compression error to the model update
immediately while only maintaining the global error variable on the server
throughout the iterations to boost its efficacy. Theoretically, we prove that
LIEC-SGD is superior to previous works in either the convergence rate or the
communication cost, which indicates that LIEC-SGD could inherit the dual
advantages from unidirectional compression and bidirectional compression.
Finally, experiments of training deep neural networks validate the
effectiveness of the proposed LIEC-SGD algorithm.
</p>
</div>
</dd>
<dt><a name="item490">[490]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11862" title="Abstract">arXiv:2402.11862</a> [<a href="/pdf/2402.11862" title="Download PDF">pdf</a>, <a href="/format/2402.11862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Effects of Group Discussion and Role-playing Training on  Self-efficacy, Support-seeking, and Reporting Phishing Emails: Evidence from  a Mixed-design Experiment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiaowei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sacr%C3%A9%2C+M">Margault Sacr&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Lenzini%2C+G">Gabriele Lenzini</a>, 
<a href="/search/cs?searchtype=author&query=Greiff%2C+S">Samuel Greiff</a>, 
<a href="/search/cs?searchtype=author&query=Distler%2C+V">Verena Distler</a>, 
<a href="/search/cs?searchtype=author&query=Sergeeva%2C+A">Anastasia Sergeeva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper is conditionally accepted in ACM CHI Conference 2024
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Proceedings of the CHI Conference on Human Factors in Computing
  Systems (CHI 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Organizations rely on phishing interventions to enhance employees' vigilance
and safe responses to phishing emails that bypass technical solutions. While
various resources are available to counteract phishing, studies emphasize the
need for interactive and practical training approaches. To investigate the
effectiveness of such an approach, we developed and delivered two anti-phishing
trainings, group discussion and role-playing, at a European university. We
conducted a pre-registered experiment (N = 105), incorporating repeated
measures at three time points, a control group, and three in-situ phishing
tests. Both trainings enhanced employees' anti-phishing self-efficacy and
support-seeking intention in within-group analyses. Only the role-playing
training significantly improved support-seeking intention when compared to the
control group. Participants in both trainings reported more phishing tests and
demonstrated heightened vigilance to phishing attacks compared to the control
group. We discuss practical implications for evaluating and improving phishing
interventions and promoting safe responses to phishing threats within
organizations.
</p>
</div>
</dd>
<dt><a name="item491">[491]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11863" title="Abstract">arXiv:2402.11863</a> [<a href="/pdf/2402.11863" title="Download PDF">pdf</a>, <a href="/format/2402.11863" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Interpretable are Reasoning Explanations from Prompting Large  Language Models?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jie%2C+Y+W">Yeo Wei Jie</a>, 
<a href="/search/cs?searchtype=author&query=Satapathy%2C+R">Ranjan Satapathy</a>, 
<a href="/search/cs?searchtype=author&query=Mong%2C+G+S">Goh Siow Mong</a>, 
<a href="/search/cs?searchtype=author&query=Rick">Rick</a>, 
<a href="/search/cs?searchtype=author&query=Cambria%2C+E">Erik Cambria</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Prompt Engineering has garnered significant attention for enhancing the
performance of large language models across a multitude of tasks. Techniques
such as the Chain-of-Thought not only bolster task performance but also
delineate a clear trajectory of reasoning steps, offering a tangible form of
explanation for the audience. Prior works on interpretability assess the
reasoning chains yielded by Chain-of-Thought solely along a singular axis,
namely faithfulness. We present a comprehensive and multifaceted evaluation of
interpretability, examining not only faithfulness but also robustness and
utility across multiple commonsense reasoning benchmarks. Likewise, our
investigation is not confined to a single prompting technique; it expansively
covers a multitude of prevalent prompting techniques employed in large language
models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition,
we introduce a simple interpretability alignment technique, termed
Self-Entailment-Alignment Chain-of-thought, that yields more than 70\%
improvements across multiple dimensions of interpretability. Code is available
at https://github.com/wj210/CoT_interpretability
</p>
</div>
</dd>
<dt><a name="item492">[492]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11866" title="Abstract">arXiv:2402.11866</a> [<a href="/pdf/2402.11866" title="Download PDF">pdf</a>, <a href="/format/2402.11866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Two Online Map Matching Algorithms Based on Analytic Hierarchy Process  and Fuzzy Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+J+J">Jeremy J. Lin</a>, 
<a href="/search/cs?searchtype=author&query=Mochida%2C+T">Tomoro Mochida</a>, 
<a href="/search/cs?searchtype=author&query=O%27Neill%2C+R+C+W">Riley C. W. O&#x27;Neill</a>, 
<a href="/search/cs?searchtype=author&query=Yoshida%2C+A">Atsuro Yoshida</a>, 
<a href="/search/cs?searchtype=author&query=Yamazaki%2C+M">Masashi Yamazaki</a>, 
<a href="/search/cs?searchtype=author&query=Sasada%2C+A">Akinobu Sasada</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 27 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Our aim of this paper is to develop new map matching algorithms and to
improve upon previous work. We address two key approaches: Analytic Hierarchy
Process (AHP) map matching and fuzzy logic map matching. AHP is a
decision-making method that combines mathematical analysis with human judgment,
and fuzzy logic is an approach to computing based on the degree of truth and
aims at modeling the imprecise modes of reasoning from 0 to 1 rather than the
usual boolean logic. Of these algorithms, the way of our applying AHP to map
matching is newly developed in this paper, meanwhile, our application of fuzzy
logic to map matching is mostly the same as existing research except for some
small changes. Because of the common characteristic that both methods are
designed to handle imprecise information and simplicity for implementation, we
decided to use these methods.
</p>
</div>
</dd>
<dt><a name="item493">[493]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11867" title="Abstract">arXiv:2402.11867</a> [<a href="/pdf/2402.11867" title="Download PDF">pdf</a>, <a href="/format/2402.11867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoRA Training in the NTK Regime has No Spurious Local Minima
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jang%2C+U">Uijeong Jang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+J+D">Jason D. Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ryu%2C+E+K">Ernest K. Ryu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">Low-rank adaptation (LoRA) has become the standard approach for
parameter-efficient fine-tuning of large language models (LLM), but our
theoretical understanding of LoRA has been limited. In this work, we
theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK)
regime with $N$ data points, showing: (i) full fine-tuning (without LoRA)
admits a low-rank solution of rank $r\lesssim \sqrt{N}$; (ii) using LoRA with
rank $r\gtrsim \sqrt{N}$ eliminates spurious local minima, allowing gradient
descent to find the low-rank solutions; (iii) the low-rank solution found using
LoRA generalizes well.
</p>
</div>
</dd>
<dt><a name="item494">[494]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11870" title="Abstract">arXiv:2402.11870</a> [<a href="/pdf/2402.11870" title="Download PDF">pdf</a>, <a href="/ps/2402.11870" title="Download PostScript">ps</a>, <a href="/format/2402.11870" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cooperative Backscatter Communications with Reconfigurable Intelligent  Surfaces: An APSK Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qiang Li</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yehuai Feng</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+M">Miaowen Wen</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Jinming Wen</a>, 
<a href="/search/cs?searchtype=author&query=Alexandropoulos%2C+G+C">George C. Alexandropoulos</a>, 
<a href="/search/cs?searchtype=author&query=Basar%2C+E">Ertugrul Basar</a>, 
<a href="/search/cs?searchtype=author&query=Poor%2C+H+V">H. Vincent Poor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 9 figures, submitted to an IEEE Transactions Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">In this paper, a novel amplitude phase shift keying (APSK) modulation scheme
for cooperative backscatter communications aided by a reconfigurable
intelligent surface (RIS-CBC) is presented, according to which the RIS is
configured to modulate backscatter information onto unmodulated or
PSK-modulated signals impinging on its surface via APSK. We consider both
passive and active RISs, with the latter including an amplification unit at
each reflecting element. In the passive (resp. active) RIS-CBC-APSK,
backscatter information is conveyed through the number of RIS reflecting
elements being on the ON state (resp. active mode) and their phase shift
values. By using the optimal APSK constellation to ensure that reflected
signals from the RIS undergo APSK modulation, a bit-mapping mechanism is
presented. Assuming maximum-likelihood detection, we also present closed-form
upper bounds for the symbol error rate (SER) performance for both passive and
active RIS-CBC-APSK schemes over Rician fading channels. In addition, we devise
a low-complexity detector that can achieve flexible trade-offs between
performance and complexity. Finally, we extend RIS-CBC-APSK to multiple-input
single-output scenarios and present an alternating optimization approach for
the joint design of transmit beamforming and RIS reflection. Our extensive
simulation results on the SER performance corroborate our conducted performance
analysis and showcase the superiority of the proposed RIS-CBC-APSK schemes over
the state-of-the-art RIS-CBC benchmarks.
</p>
</div>
</dd>
<dt><a name="item495">[495]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11871" title="Abstract">arXiv:2402.11871</a> [<a href="/pdf/2402.11871" title="Download PDF">pdf</a>, <a href="/format/2402.11871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Reals to Logic and Back: Inventing Symbolic Vocabularies, Actions  and Models for Planning from Raw Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shah%2C+N">Naman Shah</a>, 
<a href="/search/cs?searchtype=author&query=Nagpal%2C+J">Jayesh Nagpal</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+P">Pulkit Verma</a>, 
<a href="/search/cs?searchtype=author&query=Srivastava%2C+S">Siddharth Srivastava</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Hand-crafted, logic-based state and action representations have been widely
used to overcome the intractable computational complexity of long-horizon robot
planning problems, including task and motion planning problems. However,
creating such representations requires experts with strong intuitions and
detailed knowledge about the robot and the tasks it may need to accomplish in a
given setting. Removing this dependency on human intuition is a highly active
research area.
<br />This paper presents the first approach for autonomously learning
generalizable, logic-based relational representations for abstract states and
actions starting from unannotated high-dimensional, real-valued robot
trajectories. The learned representations constitute auto-invented PDDL-like
domain models. Empirical results in deterministic settings show that powerful
abstract representations can be learned from just a handful of robot
trajectories; the learned relational representations include but go beyond
classical, intuitive notions of high-level actions; and that the learned models
allow planning algorithms to scale to tasks that were previously beyond the
scope of planning without hand-crafted abstractions.
</p>
</div>
</dd>
<dt><a name="item496">[496]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11872" title="Abstract">arXiv:2402.11872</a> [<a href="/pdf/2402.11872" title="Download PDF">pdf</a>, <a href="/format/2402.11872" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-time 3D Semantic Scene Perception for Egocentric Robots with  Binocular Vision
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+K">K. Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Dang%2C+T">T. Dang</a>, 
<a href="/search/cs?searchtype=author&query=Huber%2C+M">M. Huber</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Perceiving a three-dimensional (3D) scene with multiple objects while moving
indoors is essential for vision-based mobile cobots, especially for enhancing
their manipulation tasks. In this work, we present an end-to-end pipeline with
instance segmentation, feature matching, and point-set registration for
egocentric robots with binocular vision, and demonstrate the robot's grasping
capability through the proposed pipeline. First, we design an RGB image-based
segmentation approach for single-view 3D semantic scene segmentation,
leveraging common object classes in 2D datasets to encapsulate 3D points into
point clouds of object instances through corresponding depth maps. Next, 3D
correspondences of two consecutive segmented point clouds are extracted based
on matched keypoints between objects of interest in RGB images from the prior
step. In addition, to be aware of spatial changes in 3D feature distribution,
we also weigh each 3D point pair based on the estimated distribution using
kernel density estimation (KDE), which subsequently gives robustness with less
central correspondences while solving for rigid transformations between point
clouds. Finally, we test our proposed pipeline on the 7-DOF dual-arm Baxter
robot with a mounted Intel RealSense D435i RGB-D camera. The result shows that
our robot can segment objects of interest, register multiple views while
moving, and grasp the target object. The source code is available at
https://github.com/mkhangg/semantic_scene_perception.
</p>
</div>
</dd>
<dt><a name="item497">[497]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11874" title="Abstract">arXiv:2402.11874</a> [<a href="/pdf/2402.11874" title="Download PDF">pdf</a>, <a href="/format/2402.11874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language-guided Image Reflection Separation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+H">Haofeng Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+Y">Yuchen Hong</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+S">Shuchen Weng</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jinxiu Liang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+B">Boxin Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">This paper studies the problem of language-guided reflection separation,
which aims at addressing the ill-posed reflection separation problem by
introducing language descriptions to provide layer content. We propose a
unified framework to solve this problem, which leverages the cross-attention
mechanism with contrastive learning strategies to construct the correspondence
between language descriptions and image layers. A gated network design and a
randomized training strategy are employed to tackle the recognizable layer
ambiguity. The effectiveness of the proposed method is validated by the
significant performance advantage over existing reflection separation methods
on both quantitative and qualitative comparisons.
</p>
</div>
</dd>
<dt><a name="item498">[498]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11875" title="Abstract">arXiv:2402.11875</a> [<a href="/pdf/2402.11875" title="Download PDF">pdf</a>, <a href="/format/2402.11875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> M2K-VDG: Model-Adaptive Multimodal Knowledge Anchor Enhanced  Video-grounded Dialogue Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Hongcheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pingjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanfeng Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Video-grounded dialogue generation (VDG) requires the system to generate a
fluent and accurate answer based on multimodal knowledge. However, the
difficulty in multimodal knowledge utilization brings serious hallucinations to
VDG models in practice. Although previous works mitigate the hallucination in a
variety of ways, they hardly take notice of the importance of the multimodal
knowledge anchor answer tokens. In this paper, we reveal via perplexity that
different VDG models experience varying hallucinations and exhibit diverse
anchor tokens. Based on this observation, we propose M2K-VDG, a model-adaptive
multimodal knowledge anchor enhancement framework for hallucination reduction.
Furthermore, we introduce the counterfactual effect for more accurate anchor
token detection. The experimental results on three popular benchmarks exhibit
the superiority of our approach over state-of-the-art methods, demonstrating
its effectiveness in reducing hallucinations.
</p>
</div>
</dd>
<dt><a name="item499">[499]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11877" title="Abstract">arXiv:2402.11877</a> [<a href="/pdf/2402.11877" title="Download PDF">pdf</a>, <a href="/format/2402.11877" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finite-Time Error Analysis of Online Model-Based Q-Learning with a  Relaxed Sampling Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lim%2C+H">Han-Dong Lim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H">HyeAnn Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+D">Donghwan Lee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Reinforcement learning has witnessed significant advancements, particularly
with the emergence of model-based approaches. Among these, $Q$-learning has
proven to be a powerful algorithm in model-free settings. However, the
extension of $Q$-learning to a model-based framework remains relatively
unexplored. In this paper, we delve into the sample complexity of $Q$-learning
when integrated with a model-based approach. Through theoretical analyses and
empirical evaluations, we seek to elucidate the conditions under which
model-based $Q$-learning excels in terms of sample efficiency compared to its
model-free counterpart.
</p>
</div>
</dd>
<dt><a name="item500">[500]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11879" title="Abstract">arXiv:2402.11879</a> [<a href="/pdf/2402.11879" title="Download PDF">pdf</a>, <a href="/format/2402.11879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Incipient Slip Detection by Vibration Injection into Soft Sensor
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Komeno%2C+N">Naoto Komeno</a>, 
<a href="/search/cs?searchtype=author&query=Matsubara%2C+T">Takamitsu Matsubara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, Accepted by Robotics and Automation Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">In robotic manipulation, preventing objects from slipping and establishing a
secure grip on them is critical. Successful manipulation requires tactile
sensors that detect the microscopic incipient slip phenomenon at the contact
surface. Unfortunately, the tiny signals generated by incipient slip are
quickly buried by environmental noise, and precise stress-distribution
measurement requires an extensive optical system and integrated circuits. In
this study, we focus on the macroscopic deformation of the entire fingertip's
soft structure instead of directly observing the contact surface and its role
as a vibration medium for sensing. The proposed method compresses the stick
ratio's information into a one-dimensional pressure signal using the change in
the propagation characteristics by vibration injection into the soft structure,
which magnifies the microscopic incipient slip phenomena into the entire
deformation. This mechanism allows a tactile sensor to use just a single
vibration sensor. In the implemented system, a biomimetic tactile sensor is
vibrated using a white signal from a PZT motor and utilizes frequency spectrum
change of the propagated vibration as features. We investigated the proposed
method's effectiveness on stick-ratio estimation and \red{stick-ratio
stabilization} control during incipient slip. Our estimation error and the
control performance results significantly outperformed the conventional
methods.
</p>
</div>
</dd>
<dt><a name="item501">[501]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11882" title="Abstract">arXiv:2402.11882</a> [<a href="/pdf/2402.11882" title="Download PDF">pdf</a>, <a href="/format/2402.11882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NOTE: Notable generation Of patient Text summaries through Efficient  approach based on direct preference optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ahn%2C+I">Imjin Ahn</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Gwon%2C+H">Hansle Gwon</a> (1 and 2), 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Young-Hak Kim</a> (1 and 3), 
<a href="/search/cs?searchtype=author&query=Jun%2C+T+J">Tae Joon Jun</a> (1 and 3), 
<a href="/search/cs?searchtype=author&query=Park%2C+S">Sanghyun Park</a> (2) ((1) INMED DATA, Seoul, Republic of Korea, (2) Yonsei University, Seoul, Republic of Korea (3) Asan Medical Center, Seoul, Republic of Korea)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 3 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">The discharge summary is a one of critical documents in the patient journey,
encompassing all events experienced during hospitalization, including multiple
visits, medications, tests, surgery/procedures, and admissions/discharge.
Providing a summary of the patient's progress is crucial, as it significantly
influences future care and planning. Consequently, clinicians face the
laborious and resource-intensive task of manually collecting, organizing, and
combining all the necessary data for a discharge summary. Therefore, we propose
"NOTE", which stands for "Notable generation Of patient Text summaries through
an Efficient approach based on direct preference optimization". NOTE is based
on Medical Information Mart for Intensive Care- III dataset and summarizes a
single hospitalization of a patient. Patient events are sequentially combined
and used to generate a discharge summary for each hospitalization. In the
present circumstances, large language models' application programming
interfaces (LLMs' APIs) are widely available, but importing and exporting
medical data presents significant challenges due to privacy protection policies
in healthcare institutions. Moreover, to ensure optimal performance, it is
essential to implement a lightweight model for internal server or program
within the hospital. Therefore, we utilized DPO and parameter efficient fine
tuning (PEFT) techniques to apply a fine-tuning method that guarantees superior
performance. To demonstrate the practical application of the developed NOTE, we
provide a webpage-based demonstration software. In the future, we will aim to
deploy the software available for actual use by clinicians in hospital. NOTE
can be utilized to generate various summaries not only discharge summaries but
also throughout a patient's journey, thereby alleviating the labor-intensive
workload of clinicians and aiming for increased efficiency.
</p>
</div>
</dd>
<dt><a name="item502">[502]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11883" title="Abstract">arXiv:2402.11883</a> [<a href="/pdf/2402.11883" title="Download PDF">pdf</a>, <a href="/format/2402.11883" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InMD-X: Large Language Models for Internal Medicine Doctors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gwon%2C+H">Hansle Gwon</a> (1), 
<a href="/search/cs?searchtype=author&query=Ahn%2C+I">Imjin Ahn</a> (1), 
<a href="/search/cs?searchtype=author&query=Jung%2C+H">Hyoje Jung</a> (2), 
<a href="/search/cs?searchtype=author&query=Kim%2C+B">Byeolhee Kim</a> (2), 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Young-Hak Kim</a> (3), 
<a href="/search/cs?searchtype=author&query=Jun%2C+T+J">Tae Joon Jun</a> (4) ((1) INMED DATA, Seoul, Republic of Korea (2) Department of Information Medicine, Asan Medical Center, Seoul, Republic of Korea (3) Division of Cardiology, Department of Information Medicine, Asan Medical Center, University of Ulsan College of Medicine, Seoul, Republic of Korea (4) Big Data Research Center, Asan Institute for Life Sciences, Asan Medical Center, Seoul, Republic of Korea)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we introduce InMD-X, a collection of multiple large language
models specifically designed to cater to the unique characteristics and demands
of Internal Medicine Doctors (IMD). InMD-X represents a groundbreaking
development in natural language processing, offering a suite of language models
fine-tuned for various aspects of the internal medicine field. These models
encompass a wide range of medical sub-specialties, enabling IMDs to perform
more efficient and accurate research, diagnosis, and documentation. InMD-X's
versatility and adaptability make it a valuable tool for improving the
healthcare industry, enhancing communication between healthcare professionals,
and advancing medical research. Each model within InMD-X is meticulously
tailored to address specific challenges faced by IMDs, ensuring the highest
level of precision and comprehensiveness in clinical text analysis and decision
support. This paper provides an overview of the design, development, and
evaluation of InMD-X, showcasing its potential to revolutionize the way
internal medicine practitioners interact with medical data and information. We
present results from extensive testing, demonstrating the effectiveness and
practical utility of InMD-X in real-world medical scenarios.
</p>
</div>
</dd>
<dt><a name="item503">[503]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11885" title="Abstract">arXiv:2402.11885</a> [<a href="/pdf/2402.11885" title="Download PDF">pdf</a>, <a href="/ps/2402.11885" title="Download PostScript">ps</a>, <a href="/format/2402.11885" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Error estimates of the cubic interpolated pseudo-particle scheme for  one-dimensional advection equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kashiwabara%2C+T">Takahito Kashiwabara</a>, 
<a href="/search/math?searchtype=author&query=Takemura%2C+H">Haruki Takemura</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, no figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Error estimates of cubic interpolated pseudo-particle scheme (CIP scheme) for
the one-dimensional advection equation with periodic boundary conditions are
presented. The CIP scheme is a semi-Lagrangian method involving the piecewise
cubic Hermite interpolation. Although it is numerically known that the
space-time accuracy of the scheme is third order, its rigorous proof remains an
open problem. In this paper, denoting the spatial and temporal mesh sizes by $
h $ and $ \Delta t $ respectively, we prove an error estimate $ O(\Delta t^3 +
\frac{h^4}{\Delta t}) $ in $ L^2 $ norm theoretically, which justifies the
above-mentioned prediction if $ h = O(\Delta t) $. The proof is based on
properties of the interpolation operator; the most important one is that it
behaves as the $ L^2 $ projection for the second-order derivatives. We remark
that the same strategy perfectly works as well to address an error estimate for
the semi-Lagrangian method with the cubic spline interpolation.
</p>
</div>
</dd>
<dt><a name="item504">[504]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11886" title="Abstract">arXiv:2402.11886</a> [<a href="/pdf/2402.11886" title="Download PDF">pdf</a>, <a href="/format/2402.11886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional  Supporters for Queer Youth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lissak%2C+S">Shir Lissak</a>, 
<a href="/search/cs?searchtype=author&query=Calderon%2C+N">Nitay Calderon</a>, 
<a href="/search/cs?searchtype=author&query=Shenkman%2C+G">Geva Shenkman</a>, 
<a href="/search/cs?searchtype=author&query=Ophir%2C+Y">Yaakov Ophir</a>, 
<a href="/search/cs?searchtype=author&query=Fruchter%2C+E">Eyal Fruchter</a>, 
<a href="/search/cs?searchtype=author&query=Klomek%2C+A+B">Anat Brunstein Klomek</a>, 
<a href="/search/cs?searchtype=author&query=Reichart%2C+R">Roi Reichart</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Queer youth face increased mental health risks, such as depression, anxiety,
and suicidal ideation. Hindered by negative stigma, they often avoid seeking
help and rely on online resources, which may provide incompatible information.
Although access to a supportive environment and reliable information is
invaluable, many queer youth worldwide have no access to such support. However,
this could soon change due to the rapid adoption of Large Language Models
(LLMs) such as ChatGPT. This paper aims to comprehensively explore the
potential of LLMs to revolutionize emotional support for queers. To this end,
we conduct a qualitative and quantitative analysis of LLM's interactions with
queer-related content. To evaluate response quality, we develop a novel
ten-question scale that is inspired by psychological standards and expert
input. We apply this scale to score several LLMs and human comments to posts
where queer youth seek advice and share experiences. We find that LLM responses
are supportive and inclusive, outscoring humans. However, they tend to be
generic, not empathetic enough, and lack personalization, resulting in
nonreliable and potentially harmful advice. We discuss these challenges,
demonstrate that a dedicated prompt can improve the performance, and propose a
blueprint of an LLM-supporter that actively (but sensitively) seeks user
context to provide personalized, empathetic, and reliable responses. Our
annotated dataset is available for further research.
</p>
</div>
</dd>
<dt><a name="item505">[505]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11887" title="Abstract">arXiv:2402.11887</a> [<a href="/pdf/2402.11887" title="Download PDF">pdf</a>, <a href="/format/2402.11887" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative Semi-supervised Graph Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiao%2C+H">Hezhe Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+Q">Qingsong Wen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoli Li</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+E">Ee-Peng Lim</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+G">Guansong Pang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This work considers a practical semi-supervised graph anomaly detection (GAD)
scenario, where part of the nodes in a graph are known to be normal,
contrasting to the unsupervised setting in most GAD studies with a fully
unlabeled graph. As expected, we find that having access to these normal nodes
helps enhance the detection performance of existing unsupervised GAD methods
when they are adapted to the semi-supervised setting. However, their
utilization of these normal nodes is limited. In this paper, we propose a novel
Generative GAD approach (GGAD) for the semi-supervised scenario to better
exploit the normal nodes. The key idea is to generate outlier nodes that
assimilate anomaly nodes in both local structure and node representations for
providing effective negative node samples in training a discriminative
one-class classifier. There have been many generative anomaly detection
approaches, but they are designed for non-graph data, and as a result, they
fail to take account of the graph structure information. Our approach tackles
this problem by generating graph structure-aware outlier nodes that have
asymmetric affinity separability from normal nodes while being enforced to
achieve egocentric closeness to normal nodes in the node representation space.
Comprehensive experiments on four real-world datasets are performed to
establish a benchmark for semi-supervised GAD and show that GGAD substantially
outperforms state-of-the-art unsupervised and semi-supervised GAD methods with
varying numbers of training normal nodes. Code will be made available at
https://github.com/mala-lab/GGAD.
</p>
</div>
</dd>
<dt><a name="item506">[506]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11889" title="Abstract">arXiv:2402.11889</a> [<a href="/pdf/2402.11889" title="Download PDF">pdf</a>, <a href="/format/2402.11889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ROSE Doesn&#x27;t Do That: Boosting the Safety of Instruction-Tuned Large  Language Models with Reverse Prompt Contrastive Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Q">Qihuang Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Juhua Liu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+B">Bo Du</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">With the development of instruction-tuned large language models (LLMs),
improving the safety of LLMs has become more critical. However, the current
approaches for aligning the LLMs output with expected safety usually require
substantial training efforts, e.g., high-quality safety data and expensive
computational resources, which are costly and inefficient. To this end, we
present reverse prompt contrastive decoding (ROSE), a simple-yet-effective
method to directly boost the safety of existing instruction-tuned LLMs without
any additional training. The principle of ROSE is to improve the probability of
desired safe output via suppressing the undesired output induced by the
carefully-designed reverse prompts. Experiments on 6 safety and 2
general-purpose tasks show that, our ROSE not only brings consistent and
significant safety improvements (up to +13.8% safety score) upon 5 types of
instruction-tuned LLMs, but also benefits the general-purpose ability of LLMs.
In-depth analyses explore the underlying mechanism of ROSE, and reveal when and
where to use it.
</p>
</div>
</dd>
<dt><a name="item507">[507]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11890" title="Abstract">arXiv:2402.11890</a> [<a href="/pdf/2402.11890" title="Download PDF">pdf</a>, <a href="/format/2402.11890" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Knowledge Distillation for Autoregressive Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Q">Qihuang Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Li Shen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Juhua Liu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+B">Bo Du</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Knowledge distillation (KD) is a common approach to compress a teacher model
to reduce its inference cost and memory footprint, by training a smaller
student model. However, in the context of autoregressive language models (LMs),
we empirically find that larger teacher LMs might dramatically result in a
poorer student. In response to this problem, we conduct a series of analyses
and reveal that different tokens have different teaching modes, neglecting
which will lead to performance degradation. Motivated by this, we propose a
simple yet effective adaptive teaching approach (ATKD) to improve the KD. The
core of ATKD is to reduce rote learning and make teaching more diverse and
flexible. Extensive experiments on 8 LM tasks show that, with the help of ATKD,
various baseline KD methods can achieve consistent and significant performance
gains (up to +3.04% average score) across all model types and sizes. More
encouragingly, ATKD can improve the student model generalization effectively.
</p>
</div>
</dd>
<dt><a name="item508">[508]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11891" title="Abstract">arXiv:2402.11891</a> [<a href="/pdf/2402.11891" title="Download PDF">pdf</a>, <a href="/format/2402.11891" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FeB4RAG: Evaluating Federated Search in the Context of Retrieval  Augmented Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Khramtsova%2C+E">Ekaterina Khramtsova</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+S">Shengyao Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Zuccon%2C+G">Guido Zuccon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Federated search systems aggregate results from multiple search engines,
selecting appropriate sources to enhance result quality and align with user
intent. With the increasing uptake of Retrieval-Augmented Generation (RAG)
pipelines, federated search can play a pivotal role in sourcing relevant
information across heterogeneous data sources to generate informed responses.
However, existing datasets, such as those developed in the past TREC FedWeb
tracks, predate the RAG paradigm shift and lack representation of modern
information retrieval challenges. To bridge this gap, we present FeB4RAG, a
novel dataset specifically designed for federated search within RAG frameworks.
This dataset, derived from 16 sub-collections of the widely used \beir
benchmarking collection, includes 790 information requests (akin to
conversational queries) tailored for chatbot applications, along with top
results returned by each resource and associated LLM-derived relevance
judgements. Additionally, to support the need for this collection, we
demonstrate the impact on response generation of a high quality federated
search system for RAG compared to a naive approach to federated search. We do
so by comparing answers generated through the RAG pipeline through a
qualitative side-by-side comparison. Our collection fosters and supports the
development and evaluation of new federated search methods, especially in the
context of RAG pipelines.
</p>
</div>
</dd>
<dt><a name="item509">[509]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11892" title="Abstract">arXiv:2402.11892</a> [<a href="/pdf/2402.11892" title="Download PDF">pdf</a>, <a href="/format/2402.11892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Program Repair with Semantic-Preserving Transformations: A  Naturalness Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Le-Cong%2C+T">Thanh Le-Cong</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+D">Dat Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+B">Bach Le</a>, 
<a href="/search/cs?searchtype=author&query=Murray%2C+T">Toby Murray</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this paper, we investigate the naturalness of semantic-preserving
transformations and their impacts on the evaluation of NPR. To achieve this, we
conduct a two-stage human study, including (1) interviews with senior software
developers to establish the first concrete criteria for assessing the
naturalness of code transformations and (2) a survey involving 10 developers to
assess the naturalness of 1178 transformations, i.e., pairs of original and
transformed programs, applied to 225 real-world bugs. Our findings reveal that
nearly 60% and 20% of these transformations are considered natural and
unnatural with substantially high agreement among human annotators.
Furthermore, the unnatural code transformations introduce a 25.2% false alarm
rate on robustness of five well-known NPR systems. Additionally, the
performance of the NPR systems drops notably when evaluated using natural
transformations, i.e., a drop of up to 22.9% and 23.6% in terms of the numbers
of correct and plausible patches generated by these systems. These results
highlight the importance of robustness testing by considering naturalness of
code transformations, which unveils true effectiveness of NPR systems. Finally,
we conduct an exploration study on automating the assessment of naturalness of
code transformations by deriving a new naturalness metric based on
Cross-Entropy. Based on our naturalness metric, we can effectively assess
naturalness for code transformations automatically with an AUC of 0.7.
</p>
</div>
</dd>
<dt><a name="item510">[510]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11893" title="Abstract">arXiv:2402.11893</a> [<a href="/pdf/2402.11893" title="Download PDF">pdf</a>, <a href="/format/2402.11893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discerning and Resolving Knowledge Conflicts through Adaptive Decoding  with Contextual Information-Entropy Constraint
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xiaowei Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yequan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shengping Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Large language models internalize enormous parametric knowledge during
pre-training. Concurrently, realistic applications necessitate external
contextual knowledge to aid models on the underlying tasks. This raises a
crucial dilemma known as knowledge conflicts, where the contextual knowledge
clashes with the However, existing decoding works are specialized in resolving
knowledge conflicts and could inadvertently deteriorate performance in absence
of conflicts. In this paper, we propose an adaptive decoding method, termed as
contextual information-entropy constraint decoding (COIECD), to discern whether
the knowledge conflicts occur and resolve them. It can improve the model's
faithfulness to conflicting context, and simultaneously maintain high
performance among non- Our experiments show that COIECD exhibits strong
performance and robustness over knowledge conflicts in realistic datasets. Code
is available.
</p>
</div>
</dd>
<dt><a name="item511">[511]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11894" title="Abstract">arXiv:2402.11894</a> [<a href="/pdf/2402.11894" title="Download PDF">pdf</a>, <a href="/format/2402.11894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Have Seen Me Before? Automating Dataset Updates Towards Reliable and  Timely Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ying%2C+J">Jiahao Ying</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yixin Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Bo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+W">Wei Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yizhe Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shuicheng Yan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Due to the expanding capabilities and pre-training data, Large Language
Models (LLMs) are facing increasingly serious evaluation challenges. On one
hand, the data leakage issue cause over-estimation on existing benchmarks. On
the other hand, periodically curating datasets manually is costly. In this
paper, we propose to automate dataset updates for reliable and timely
evaluation. The basic idea is to generate unseen and high-quality testing
samples based on existing ones to mitigate leakage issues. In specific, we
propose two strategies with systematically verification. First, the mimicking
strategy employs LLMs to create new samples resembling existing ones, to the
maximum extent preserving the stylistic of the original dataset. Our
experiments demonstrate its evaluation stability across multiple instantiations
and its effectiveness in dealing with data leakage issues in most cases.
Second, for the cases that mimicking dataset works poorly, we design an
extending strategy that adjusts the difficulty of the generated samples
according to varying cognitive levels. This not only makes our evaluation more
systematic, but also, with a balanced difficulty, even discern model
capabilities better at fine-grained levels.
</p>
</div>
</dd>
<dt><a name="item512">[512]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11895" title="Abstract">arXiv:2402.11895</a> [<a href="/pdf/2402.11895" title="Download PDF">pdf</a>, <a href="/format/2402.11895" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging or Breaking: Impact of Intergroup Interactions on Religious  Polarization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chaturvedi%2C+R">Rochana Chaturvedi</a>, 
<a href="/search/cs?searchtype=author&query=Chaturvedi%2C+S">Sugat Chaturvedi</a>, 
<a href="/search/cs?searchtype=author&query=Zheleva%2C+E">Elena Zheleva</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computation and Language (cs.CL); Physics and Society (physics.soc-ph)

</div>
<p class="mathjax">While exposure to diverse viewpoints may reduce polarization, it can also
have a backfire effect and exacerbate polarization when the discussion is
adversarial. Here, we examine the question whether intergroup interactions
around important events affect polarization between majority and minority
groups in social networks. We compile data on the religious identity of nearly
700,000 Indian Twitter users engaging in COVID-19-related discourse during
2020. We introduce a new measure for an individual's group conformity based on
contextualized embeddings of tweet text, which helps us assess polarization
between religious groups. We then use a meta-learning framework to examine
heterogeneous treatment effects of intergroup interactions on an individual's
group conformity in the light of communal, political, and socio-economic
events. We find that for political and social events, intergroup interactions
reduce polarization. This decline is weaker for individuals at the extreme who
already exhibit high conformity to their group. In contrast, during communal
events, intergroup interactions can increase group conformity. Finally, we
decompose the differential effects across religious groups in terms of emotions
and topics of discussion. The results show that the dynamics of religious
polarization are sensitive to the context and have important implications for
understanding the role of intergroup interactions.
</p>
</div>
</dd>
<dt><a name="item513">[513]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11896" title="Abstract">arXiv:2402.11896</a> [<a href="/pdf/2402.11896" title="Download PDF">pdf</a>, <a href="/format/2402.11896" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wen%2C+Z">Zhihao Wen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yuan Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Fine-tuning all parameters of large language models (LLMs) necessitates
substantial computational power and extended time. Latest advancements in
parameter-efficient fine-tuning (PEFT) techniques, such as Adapter tuning and
LoRA, allow for adjustments to only a minor fraction of the parameters of these
LLMs. Concurrently, it has been noted that the issue of over-smoothing
diminishes the effectiveness of these Transformer-based LLMs, resulting in
suboptimal performances in downstream tasks. In this paper, we present SIBO,
which is a SImple BOoster to enhance PEFT, by injecting an initial residual.
SIBO is straight-forward and readily extensible to a range of state-of-the-art
PEFT techniques to alleviate over-smoothing and enhance performance. Extensive
experiments on 22 benchmark datasets demonstrate that SIBO significantly
enhances the performance of various strong baselines, achieving up to 15.7% and
23.5% improvement over existing PEFT methods on the arithmetic and commonsense
reasoning tasks, respectively.
</p>
</div>
</dd>
<dt><a name="item514">[514]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11897" title="Abstract">arXiv:2402.11897</a> [<a href="/pdf/2402.11897" title="Download PDF">pdf</a>, <a href="/ps/2402.11897" title="Download PostScript">ps</a>, <a href="/format/2402.11897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Power Prediction of Photovoltaic Systems: Leveraging Dynamic  Physical Model for Irradiance-to-Power Conversion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+B">Baojie Li</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+X">Xin Chen</a>, 
<a href="/search/eess?searchtype=author&query=Jain%2C+A">Anubhav Jain</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Power prediction is crucial to the efficiency and reliability of Photovoltaic
(PV) systems. For the model-chain-based (also named indirect or physical) power
prediction, the conversion of ground environmental data (plane-of-array
irradiance and module temperature) to the output power is a fundamental step,
commonly accomplished through physical modeling. The core of the physical model
lies in the parameters. However, traditional parameter estimation either relies
on datasheet information that cannot reflect the system's current health status
or necessitates additional I-V characterization of the entire array, which is
not commonly available. To address this, our paper introduces PVPro, a dynamic
physical modeling method for irradiance-to-power conversion. It extracts model
parameters from the recent production data without requiring I-V curve
measurements. This dynamic model, periodically-updated (as short as daily), can
closely capture the actual health status, enabling precise power estimation. To
evaluate the performance, PVPro is compared with the smart persistence, nominal
physical, and various machine learning models for day-ahead power prediction.
The results indicate that PVPro achieves an outstanding power estimation
performance with the average nMAE =1.4% across four field PV systems, reducing
the error by 17.6% compared to the best of other techniques. Furthermore, PVPro
demonstrates robustness across different seasons and weather conditions. More
importantly, PVPro can also perform well with a limited amount of historical
production data (3 days), rendering it applicable for new PV systems. The tool
is available as a Python package at: https://github.com/DuraMAT/pvpro.
</p>
</div>
</dd>
<dt><a name="item515">[515]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11900" title="Abstract">arXiv:2402.11900</a> [<a href="/pdf/2402.11900" title="Download PDF">pdf</a>, <a href="/format/2402.11900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ju%2C+T">Tianjie Ju</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yijin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+X">Xinwei Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuosheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+W">Wei Du</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yubin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Gongshen Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Working in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent work has showcased the powerful capability of large language models
(LLMs) in recalling knowledge and reasoning. However, the reliability of LLMs
in combining these two capabilities into reasoning through multi-hop facts has
not been widely explored. This paper systematically investigates the
possibilities for LLMs to utilize shortcuts based on direct connections between
the initial and terminal entities of multi-hop knowledge. We first explore the
existence of factual shortcuts through Knowledge Neurons, revealing that: (i)
the strength of factual shortcuts is highly correlated with the frequency of
co-occurrence of initial and terminal entities in the pre-training corpora;
(ii) few-shot prompting leverage more shortcuts in answering multi-hop
questions compared to chain-of-thought prompting. Then, we analyze the risks
posed by factual shortcuts from the perspective of multi-hop knowledge editing.
Analysis shows that approximately 20% of the failures are attributed to
shortcuts, and the initial and terminal entities in these failure instances
usually have higher co-occurrences in the pre-training corpus. Finally, we
propose erasing shortcut neurons to mitigate the associated risks and find that
this approach significantly reduces failures in multiple-hop knowledge editing
caused by shortcuts.
</p>
</div>
</dd>
<dt><a name="item516">[516]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11901" title="Abstract">arXiv:2402.11901</a> [<a href="/pdf/2402.11901" title="Download PDF">pdf</a>, <a href="/format/2402.11901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-World Planning with PDDL+ and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piotrowski%2C+W">Wiktor Piotrowski</a>, 
<a href="/search/cs?searchtype=author&query=Perez%2C+A">Alexandre Perez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Real-world applications of AI Planning often require a highly expressive
modeling language to accurately capture important intricacies of target
systems. Hybrid systems are ubiquitous in the real-world, and PDDL+ is the
standardized modeling language for capturing such systems as planning domains.
PDDL+ enables accurate encoding of mixed discrete-continuous system dynamics,
exogenous activity, and many other interesting features exhibited in realistic
scenarios. However, the uptake in usage of PDDL+ has been slow and
apprehensive, largely due to a general shortage of PDDL+ planning software, and
rigid limitations of the few existing planners. To overcome this chasm, we
present Nyx, a novel PDDL+ planner built to emphasize lightness, simplicity,
and, most importantly, adaptability. The planner is designed to be effortlessly
customizable to expand its capabilities well beyond the scope of PDDL+. As a
result, Nyx can be tailored to virtually any potential real-world application
requiring some form of AI Planning, paving the way for wider adoption of
planning methods for solving real-world problems.
</p>
</div>
</dd>
<dt><a name="item517">[517]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11903" title="Abstract">arXiv:2402.11903</a> [<a href="/pdf/2402.11903" title="Download PDF">pdf</a>, <a href="/format/2402.11903" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhen%2C+H">Hui-Ling Zhen</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+Z">Zehua Pei</a>, 
<a href="/search/cs?searchtype=author&query=Lian%2C+Y">Yingzhao Lian</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+L">Lihao Yin</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+M">Mingxuan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bei Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Considering the challenges faced by large language models (LLMs) on logical
reasoning, prior efforts have sought to transform problem-solving through tool
learning. While progress has been made on small-scale problems, solving
industrial cases remains difficult due to their large scale and intricate
expressions. In this paper, we propose a novel solver-layer adaptation (SoLA)
method, where we introduce a solver as a new layer of the LLM to differentially
guide solutions towards satisfiability. In SoLA, LLM aims to comprehend the
search space described in natural language and identify local solutions of the
highest quality, while the solver layer focuses solely on constraints not
satisfied by the initial solution. Leveraging MaxSAT as a bridge, we define
forward and backward transfer gradients, enabling the final model to converge
to a satisfied solution or prove unsatisfiability. The backdoor theory ensures
that SoLA can obtain accurate solutions within polynomial loops. We evaluate
the performance of SoLA on various datasets and empirically demonstrate its
consistent outperformance against existing symbolic solvers (including Z3 and
Kissat) and tool-learning methods in terms of efficiency in large-scale
problem-solving.
</p>
</div>
</dd>
<dt><a name="item518">[518]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11904" title="Abstract">arXiv:2402.11904</a> [<a href="/pdf/2402.11904" title="Download PDF">pdf</a>, <a href="/format/2402.11904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scalable Virtual Valuations Combinatorial Auction Design by Combining  Zeroth-Order and First-Order Optimization Method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+Z">Zhijian Duan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Haoran Sun</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yichong Xia</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Siqiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhilin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chuan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jian Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+B">Bo Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+X">Xiaotie Deng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Automated auction design seeks to discover empirically high-revenue and
incentive-compatible mechanisms using machine learning. Ensuring dominant
strategy incentive compatibility (DSIC) is crucial, and the most effective
approach is to confine the mechanism to Affine Maximizer Auctions (AMAs).
Nevertheless, existing AMA-based approaches encounter challenges such as
scalability issues (arising from combinatorial candidate allocations) and the
non-differentiability of revenue. In this paper, to achieve a scalable
AMA-based method, we further restrict the auction mechanism to Virtual
Valuations Combinatorial Auctions (VVCAs), a subset of AMAs with significantly
fewer parameters. Initially, we employ a parallelizable dynamic programming
algorithm to compute the winning allocation of a VVCA. Subsequently, we propose
a novel optimization method that combines both zeroth-order and first-order
techniques to optimize the VVCA parameters. Extensive experiments demonstrate
the efficacy and scalability of our proposed approach, termed Zeroth-order and
First-order Optimization of VVCAs (ZFO-VVCA), particularly when applied to
large-scale auctions.
</p>
</div>
</dd>
<dt><a name="item519">[519]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11905" title="Abstract">arXiv:2402.11905</a> [<a href="/pdf/2402.11905" title="Download PDF">pdf</a>, <a href="/format/2402.11905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Edit: Aligning LLMs with Knowledge Editing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yuxin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yufei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chuhan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+W">Wanjun Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xingshan Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+J">Jiahui Gao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liangyou Li</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+L">Lifeng Shang</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+R">Ruiming Tang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 8 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Knowledge editing techniques, aiming to efficiently modify a minor proportion
of knowledge in large language models (LLMs) without negatively impacting
performance across other inputs, have garnered widespread attention. However,
existing methods predominantly rely on memorizing the updated knowledge,
impeding LLMs from effectively combining the new knowledge with their inherent
knowledge when answering questions. To this end, we propose a Learning to Edit
(LTE) framework, focusing on teaching LLMs to apply updated knowledge into
input questions, inspired by the philosophy of "Teach a man to fish." LTE
features a two-phase process: (i) the Alignment Phase, which fine-tunes LLMs on
a meticulously curated parallel dataset to make reliable, in-scope edits while
preserving out-of-scope information and linguistic proficiency; and (ii) the
Inference Phase, which employs a retrieval-based mechanism for real-time and
mass knowledge editing. By comparing our approach with seven advanced baselines
across four popular knowledge editing benchmarks and two LLM architectures, we
demonstrate LTE's superiority in knowledge editing performance, robustness in
both batch and sequential editing, minimal interference on general tasks, and
rapid editing speeds. The data and code are available at
https://github.com/YJiangcm/LTE.
</p>
</div>
</dd>
<dt><a name="item520">[520]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11907" title="Abstract">arXiv:2402.11907</a> [<a href="/pdf/2402.11907" title="Download PDF">pdf</a>, <a href="/format/2402.11907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct Large Language Model Alignment Through Self-Rewarding Contrastive  Prompt Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+A">Aiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+H">Haoping Bai</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhiyun Lu</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+X">Xiang Kong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Simon Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+J">Jiulong Shan</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+M">Meng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+L">Lijie Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 5 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Aligning large language models (LLMs) with human expectations without
human-annotated preference data is an important problem. In this paper, we
propose a method to evaluate the response preference by using the output
probabilities of response pairs under contrastive prompt pairs, which could
achieve better performance on LLaMA2-7B and LLaMA2-13B compared to RLAIF. Based
on this, we propose an automatic alignment method, Direct Large Model Alignment
(DLMA). First, we use contrastive prompt pairs to automatically generate
preference data. Then, we continue to evaluate the generated preference data
using contrastive prompt pairs and calculate a self-rewarding score. Finally,
we use the DPO algorithm to effectively align LLMs by combining this
self-rewarding score. In the experimental stage, our DLMA method could surpass
the \texttt{RLHF} method without relying on human-annotated preference data.
</p>
</div>
</dd>
<dt><a name="item521">[521]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11908" title="Abstract">arXiv:2402.11908</a> [<a href="/pdf/2402.11908" title="Download PDF">pdf</a>, <a href="/format/2402.11908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semantic Textual Similarity Assessment in Chest X-ray Reports Using a  Domain-Specific Cosine-Based Metric
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Picha%2C+S+G">Sayeh Gholipour Picha</a>, 
<a href="/search/cs?searchtype=author&query=Chanti%2C+D+A">Dawood Al Chanti</a>, 
<a href="/search/cs?searchtype=author&query=Caplier%2C+A">Alice Caplier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Medical language processing and deep learning techniques have emerged as
critical tools for improving healthcare, particularly in the analysis of
medical imaging and medical text data. These multimodal data fusion techniques
help to improve the interpretation of medical imaging and lead to increased
diagnostic accuracy, informed clinical decisions, and improved patient
outcomes. The success of these models relies on the ability to extract and
consolidate semantic information from clinical text. This paper addresses the
need for more robust methods to evaluate the semantic content of medical
reports. Conventional natural language processing approaches and metrics are
initially designed for considering the semantic context in the natural language
domain and machine translation, often failing to capture the complex semantic
meanings inherent in medical content. In this study, we introduce a novel
approach designed specifically for assessing the semantic similarity between
generated medical reports and the ground truth. Our approach is validated,
demonstrating its efficiency in assessing domain-specific semantic similarity
within medical contexts. By applying our metric to state-of-the-art Chest X-ray
report generation models, we obtain results that not only align with
conventional metrics but also provide more contextually meaningful scores in
the considered medical domain.
</p>
</div>
</dd>
<dt><a name="item522">[522]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11909" title="Abstract">arXiv:2402.11909</a> [<a href="/pdf/2402.11909" title="Download PDF">pdf</a>, <a href="/format/2402.11909" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> One2Avatar: Generative Implicit Head Avatar For Few-shot User Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zhixuan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Z">Ziqian Bai</a>, 
<a href="/search/cs?searchtype=author&query=Meka%2C+A">Abhimitra Meka</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+F">Feitong Tan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Q">Qiangeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Pandey%2C+R">Rohit Pandey</a>, 
<a href="/search/cs?searchtype=author&query=Fanello%2C+S">Sean Fanello</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+H+S">Hyun Soo Park</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yinda Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Traditional methods for constructing high-quality, personalized head avatars
from monocular videos demand extensive face captures and training time, posing
a significant challenge for scalability. This paper introduces a novel approach
to create high quality head avatar utilizing only a single or a few images per
user. We learn a generative model for 3D animatable photo-realistic head avatar
from a multi-view dataset of expressions from 2407 subjects, and leverage it as
a prior for creating personalized avatar from few-shot images. Different from
previous 3D-aware face generative models, our prior is built with a
3DMM-anchored neural radiance field backbone, which we show to be more
effective for avatar creation through auto-decoding based on few-shot inputs.
We also handle unstable 3DMM fitting by jointly optimizing the 3DMM fitting and
camera calibration that leads to better few-shot adaptation. Our method
demonstrates compelling results and outperforms existing state-of-the-art
methods for few-shot avatar adaptation, paving the way for more efficient and
personalized avatar creation.
</p>
</div>
</dd>
<dt><a name="item523">[523]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11910" title="Abstract">arXiv:2402.11910</a> [<a href="/pdf/2402.11910" title="Download PDF">pdf</a>, <a href="/format/2402.11910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Large Language Models for Text-to-Testcase Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alagarsamy%2C+S">Saranya Alagarsamy</a>, 
<a href="/search/cs?searchtype=author&query=Tantithamthavorn%2C+C">Chakkrit Tantithamthavorn</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+C">Chetan Arora</a>, 
<a href="/search/cs?searchtype=author&query=Aleti%2C+A">Aldeida Aleti</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Context: Test-driven development (TDD) is a widely employed software
development practice that involves developing test cases based on requirements
prior to writing the code. Although various methods for automated test case
generation have been proposed, they are not specifically tailored for TDD,
where requirements instead of code serve as input. Objective: In this paper, we
introduce a text-to-testcase generation approach based on a large language
model (GPT-3.5) that is fine-tuned on our curated dataset with an effective
prompt design. Method: Our approach involves enhancing the capabilities of
basic GPT-3.5 for text-to-testcase generation task that is fine-tuned on our
curated dataset with an effective prompting design. We evaluated the
effectiveness of our approach using a span of five large-scale open-source
software projects. Results: Our approach generated 7k test cases for open
source projects, achieving 78.5% syntactic correctness, 67.09% requirement
alignment, and 61.7% code coverage, which substantially outperforms all other
LLMs (basic GPT-3.5, Bloom, and CodeT5). In addition, our ablation study
demonstrates the substantial performance improvement of the fine-tuning and
prompting components of the GPT-3.5 model. Conclusions: These findings lead us
to conclude that fine-tuning and prompting should be considered in the future
when building a language model for the text-to-testcase generation task
</p>
</div>
</dd>
<dt><a name="item524">[524]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11913" title="Abstract">arXiv:2402.11913</a> [<a href="/pdf/2402.11913" title="Download PDF">pdf</a>, <a href="/format/2402.11913" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PhySU-Net: Long Temporal Context Transformer for rPPG with  Self-Supervised Pre-training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Savic%2C+M">Marko Savic</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+G">Guoying Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Remote photoplethysmography (rPPG) is a promising technology that consists of
contactless measuring of cardiac activity from facial videos. Most recent
approaches utilize convolutional networks with limited temporal modeling
capability or ignore long temporal context. Supervised rPPG methods are also
severely limited by scarce data availability. In this work, we propose
PhySU-Net, the first long spatial-temporal map rPPG transformer network and a
self-supervised pre-training strategy that exploits unlabeled data to improve
our model. Our strategy leverages traditional methods and image masking to
provide pseudo-labels for self-supervised pre-training. Our model is tested on
two public datasets (OBF and VIPL-HR) and shows superior performance in
supervised training. Furthermore, we demonstrate that our self-supervised
pre-training strategy further improves our model's performance by leveraging
representations learned from unlabeled data.
</p>
</div>
</dd>
<dt><a name="item525">[525]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11915" title="Abstract">arXiv:2402.11915</a> [<a href="/pdf/2402.11915" title="Download PDF">pdf</a>, <a href="/ps/2402.11915" title="Download PostScript">ps</a>, <a href="/format/2402.11915" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Pseudorandom Generators for Low-Degree Polynomials Over  Moderately Large Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dwivedi%2C+A">Ashish Dwivedi</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zeyu Guo</a>, 
<a href="/search/cs?searchtype=author&query=Volk%2C+B+L">Ben Lee Volk</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Symbolic Computation (cs.SC)

</div>
<p class="mathjax">We construct explicit pseudorandom generators that fool $n$-variate
polynomials of degree at most $d$ over a finite field $\mathbb{F}_q$. The seed
length of our generators is $O(d \log n + \log q)$, over fields of size
exponential in $d$ and characteristic at least $d(d-1)+1$. Previous
constructions such as Bogdanov's (STOC 2005) and Derksen and Viola's (FOCS
2022) had either suboptimal seed length or required the field size to depend on
$n$.
<br />Our approach follows Bogdanov's paradigm while incorporating techniques from
Lecerf's factorization algorithm (J. Symb. Comput. 2007) and insights from the
construction of Derksen and Viola regarding the role of indecomposability of
polynomials.
</p>
</div>
</dd>
<dt><a name="item526">[526]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11917" title="Abstract">arXiv:2402.11917</a> [<a href="/pdf/2402.11917" title="Download PDF">pdf</a>, <a href="/format/2402.11917" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step  Reasoning Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brinkmann%2C+J">Jannik Brinkmann</a>, 
<a href="/search/cs?searchtype=author&query=Sheshadri%2C+A">Abhay Sheshadri</a>, 
<a href="/search/cs?searchtype=author&query=Levoso%2C+V">Victor Levoso</a>, 
<a href="/search/cs?searchtype=author&query=Swoboda%2C+P">Paul Swoboda</a>, 
<a href="/search/cs?searchtype=author&query=Bartelt%2C+C">Christian Bartelt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Transformers demonstrate impressive performance on a range of reasoning
benchmarks. To evaluate the degree to which these abilities are a result of
actual reasoning, existing work has focused on developing sophisticated
benchmarks for behavioral studies. However, these studies do not provide
insights into the internal mechanisms driving the observed capabilities. To
improve our understanding of the internal mechanisms of transformers, we
present a comprehensive mechanistic analysis of a transformer trained on a
synthetic reasoning task. We identify a set of interpretable mechanisms the
model uses to solve the task, and validate our findings using correlational and
causal evidence. Our results suggest that it implements a depth-bounded
recurrent mechanisms that operates in parallel and stores intermediate results
in selected token positions. We anticipate that the motifs we identified in our
synthetic setting can provide valuable insights into the broader operating
principles of transformers and thus provide a basis for understanding more
complex models.
</p>
</div>
</dd>
<dt><a name="item527">[527]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11918" title="Abstract">arXiv:2402.11918</a> [<a href="/pdf/2402.11918" title="Download PDF">pdf</a>, <a href="/ps/2402.11918" title="Download PostScript">ps</a>, <a href="/format/2402.11918" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modifying an Instance of the Super-Stable Matching Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamiyama%2C+N">Naoyuki Kamiyama</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Super-stability is one of the stability concepts in the stable matching
problem with ties. It is known that there may not exist a super-stable
matching, and the existence of a super-stable matching can be checked in
polynomial time. In this paper, we consider the problem of modifying an
instance of the super-stable matching problem by deleting some bounded number
of agents in such a way that there exists a super-stable matching in the
modified instance. First, we prove that if we are allowed to delete agents on
only one side, then our problem can be solved in polynomial time.
Interestingly, this result is obtained by carefully observing the existing
algorithm for checking the existence of a super-stable matching. In addition,
we prove that if we are allowed to delete agents on both sides, then our
problem is NP-complete.
</p>
</div>
</dd>
<dt><a name="item528">[528]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11919" title="Abstract">arXiv:2402.11919</a> [<a href="/pdf/2402.11919" title="Download PDF">pdf</a>, <a href="/format/2402.11919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unraveling Complex Data Diversity in Underwater Acoustic Target  Recognition through Convolution-based Mixture of Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yuan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jiawei Ren</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Ji Xu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Expert Systems with Applications (2024): 123431
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Underwater acoustic target recognition is a difficult task owing to the
intricate nature of underwater acoustic signals. The complex underwater
environments, unpredictable transmission channels, and dynamic motion states
greatly impact the real-world underwater acoustic signals, and may even obscure
the intrinsic characteristics related to targets. Consequently, the data
distribution of underwater acoustic signals exhibits high intra-class
diversity, thereby compromising the accuracy and robustness of recognition
systems.To address these issues, this work proposes a convolution-based mixture
of experts (CMoE) that recognizes underwater targets in a fine-grained manner.
The proposed technique introduces multiple expert layers as independent
learners, along with a routing layer that determines the assignment of experts
according to the characteristics of inputs. This design allows the model to
utilize independent parameter spaces, facilitating the learning of complex
underwater signals with high intra-class diversity. Furthermore, this work
optimizes the CMoE structure by balancing regularization and an optional
residual module. To validate the efficacy of our proposed techniques, we
conducted detailed experiments and visualization analyses on three underwater
acoustic databases across several acoustic features. The experimental results
demonstrate that our CMoE consistently achieves significant performance
improvements, delivering superior recognition accuracy when compared to
existing advanced methods.
</p>
</div>
</dd>
<dt><a name="item529">[529]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11922" title="Abstract">arXiv:2402.11922</a> [<a href="/pdf/2402.11922" title="Download PDF">pdf</a>, <a href="/format/2402.11922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Y">Yuan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+C">Chenyang Shao</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+J">Jingtao Ding</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+D">Depeng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yong Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Spatio-temporal graph (STG) learning is foundational for smart city
applications, yet it is often hindered by data scarcity in many cities and
regions. To bridge this gap, we propose a novel generative pre-training
framework, GPDiff, for STG transfer learning. Unlike conventional approaches
that heavily rely on common feature extraction or intricate transfer learning
designs, our solution takes a novel approach by performing generative
pre-training on a collection of model parameters optimized with data from
source cities. We recast STG transfer learning as pre-training a generative
hypernetwork, which generates tailored model parameters guided by prompts,
allowing for adaptability to diverse data distributions and city-specific
characteristics. GPDiff employs a diffusion model with a transformer-based
denoising network, which is model-agnostic to integrate with powerful STG
models. By addressing challenges arising from data gaps and the complexity of
generalizing knowledge across cities, our framework consistently outperforms
state-of-the-art baselines on multiple real-world datasets for tasks such as
traffic speed prediction and crowd flow prediction. The implementation of our
approach is available: https://github.com/PLUTO-SCY/GPDiff.
</p>
</div>
</dd>
<dt><a name="item530">[530]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11924" title="Abstract">arXiv:2402.11924</a> [<a href="/pdf/2402.11924" title="Download PDF">pdf</a>, <a href="/format/2402.11924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jian Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Linyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Okumura%2C+M">Manabu Okumura</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yue Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Although Large Language Models (LLMs) have shown strong performance in
Multi-hop Question Answering (MHQA) tasks, their real reasoning ability remains
exploration. Current LLM QA evaluation benchmarks have shown limitations,
including 1) data contamination, the evaluation data are potentially exposed to
LLMs during the pretraining stage; and 2) ignoration of the reasoning chain
evaluation. Thus we introduce an LLM MHQA evaluation benchmark, the first QA
benchmark based on the new, unprecedented knowledge by editing the
off-the-shelf HotpotQA dataset; Besides, we also annotate and evaluate the
reasoning chain in the form of sub-questions and intermediate answers
corresponding to the multi-hop questions. Specifically, based on the
observation, 1) LLMs show a performance gap between the original HotpotQA and
our edited data, deeming that current MHQA benchmarks have the potential risk
of data contamination that hard to evaluate LLMs' performance objectively and
scientifically; 2) LLMs only get a small percentage of the right reasoning
chain, e.g. GPT-4 only gets 36.3\% right reasoning chain. We believe this new
Multi-hop QA evaluation benchmark and novel evaluation methods will facilitate
the development of trustworthy LLM evaluation on the MHQA task.
</p>
</div>
</dd>
<dt><a name="item531">[531]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11925" title="Abstract">arXiv:2402.11925</a> [<a href="/pdf/2402.11925" title="Download PDF">pdf</a>, <a href="/format/2402.11925" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Energy-Efficient Edge Learning via Joint Data Deepening-and-Prefetching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kook%2C+S">Sujin Kook</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+W">Won-Yong Shin</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Seong-Lyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Ko%2C+S">Seung-Woo Ko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted for publication in IEEE Transactions on Wireless Communications. arXiv admin note: text overlap with <a href="/abs/2211.07146">arXiv:2211.07146</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT)

</div>
<p class="mathjax">The vision of pervasive artificial intelligence (AI) services can be realized
by training an AI model on time using real-time data collected by internet of
things (IoT) devices. To this end, IoT devices require offloading their data to
an edge server in proximity. However, transmitting high-dimensional and
voluminous data from energy-constrained IoT devices poses a significant
challenge. To address this limitation, we propose a novel offloading
architecture, called joint data deepening-and-prefetching (JD2P), which is
feature-by-feature offloading comprising two key techniques. The first one is
data deepening, where each data sample's features are sequentially offloaded in
the order of importance determined by the data embedding technique such as
principle component analysis (PCA). Offloading is terminated once the already
transmitted features are sufficient for accurate data classification, resulting
in a reduction in the amount of transmitted data. The criteria to offload data
are derived for binary and multi-class classifiers, which are designed based on
support vector machine (SVM) and deep neural network (DNN), respectively. The
second one is data prefetching, where some features potentially required in the
future are offloaded in advance, thus achieving high efficiency via precise
prediction and parameter optimization. We evaluate the effectiveness of JD2P
through experiments using the MNIST dataset, and the results demonstrate its
significant reduction in expected energy consumption compared to several
benchmarks without degrading learning accuracy.
</p>
</div>
</dd>
<dt><a name="item532">[532]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11926" title="Abstract">arXiv:2402.11926</a> [<a href="/pdf/2402.11926" title="Download PDF">pdf</a>, <a href="/format/2402.11926" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lax-Wendroff Flux Reconstruction on adaptive curvilinear meshes with  error based time stepping for hyperbolic conservation laws
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Babbar%2C+A">Arpit Babbar</a>, 
<a href="/search/math?searchtype=author&query=Chandrashekar%2C+P">Praveen Chandrashekar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 15 figures (with some figures), 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Lax-Wendroff Flux Reconstruction (LWFR) is a single-stage, high order,
quadrature free method for solving hyperbolic conservation laws. This work
extends the LWFR scheme to solve conservation laws on curvilinear meshes with
adaptive mesh refinement (AMR). The scheme uses a subcell based blending
limiter to perform shock capturing and exploits the same subcell structure to
obtain admissibility preservation on curvilinear meshes. It is proven that the
proposed extension of LWFR scheme to curvilinear grids preserves constant
solution (free stream preservation) under the standard metric identities. For
curvilinear meshes, linear Fourier stability analysis cannot be used to obtain
an optimal CFL number. Thus, an embedded-error based time step computation
method is proposed for LWFR method which reduces fine-tuning process required
to select a stable CFL number using the wave speed based time step computation.
The developments are tested on compressible Euler's equations, validating the
blending limiter, admissibility preservation, AMR algorithm, curvilinear meshes
and error based time stepping.
</p>
</div>
</dd>
<dt><a name="item533">[533]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11928" title="Abstract">arXiv:2402.11928</a> [<a href="/pdf/2402.11928" title="Download PDF">pdf</a>, <a href="/format/2402.11928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Separating common from salient patterns with Contrastive Representation  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Louiset%2C+R">Robin Louiset</a>, 
<a href="/search/cs?searchtype=author&query=Duchesnay%2C+E">Edouard Duchesnay</a>, 
<a href="/search/cs?searchtype=author&query=Grigis%2C+A">Antoine Grigis</a>, 
<a href="/search/cs?searchtype=author&query=Gori%2C+P">Pietro Gori</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Contrastive Analysis is a sub-field of Representation Learning that aims at
separating common factors of variation between two datasets, a background
(i.e., healthy subjects) and a target (i.e., diseased subjects), from the
salient factors of variation, only present in the target dataset. Despite their
relevance, current models based on Variational Auto-Encoders have shown poor
performance in learning semantically-expressive representations. On the other
hand, Contrastive Representation Learning has shown tremendous performance
leaps in various applications (classification, clustering, etc.). In this work,
we propose to leverage the ability of Contrastive Learning to learn
semantically expressive representations well adapted for Contrastive Analysis.
We reformulate it under the lens of the InfoMax Principle and identify two
Mutual Information terms to maximize and one to minimize. We decompose the
first two terms into an Alignment and a Uniformity term, as commonly done in
Contrastive Learning. Then, we motivate a novel Mutual Information minimization
strategy to prevent information leakage between common and salient
distributions. We validate our method, called SepCLR, on three visual datasets
and three medical datasets, specifically conceived to assess the pattern
separation capability in Contrastive Analysis. Code available at
https://github.com/neurospin-projects/2024_rlouiset_sep_clr.
</p>
</div>
</dd>
<dt><a name="item534">[534]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11929" title="Abstract">arXiv:2402.11929</a> [<a href="/pdf/2402.11929" title="Download PDF">pdf</a>, <a href="/format/2402.11929" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DiLightNet: Fine-grained Lighting Control for Diffusion-based Image  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+C">Chong Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yue Dong</a>, 
<a href="/search/cs?searchtype=author&query=Peers%2C+P">Pieter Peers</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+Y">Youkang Kong</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hongzhi Wu</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+X">Xin Tong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
<p class="mathjax">This paper presents a novel method for exerting fine-grained lighting control
during text-driven diffusion-based image generation. While existing diffusion
models already have the ability to generate images under any lighting
condition, without additional guidance these models tend to correlate image
content and lighting. Moreover, text prompts lack the necessary expressional
power to describe detailed lighting setups. To provide the content creator with
fine-grained control over the lighting during image generation, we augment the
text-prompt with detailed lighting information in the form of radiance hints,
i.e., visualizations of the scene geometry with a homogeneous canonical
material under the target lighting. However, the scene geometry needed to
produce the radiance hints is unknown. Our key observation is that we only need
to guide the diffusion process, hence exact radiance hints are not necessary;
we only need to point the diffusion model in the right direction. Based on this
observation, we introduce a three stage method for controlling the lighting
during image generation. In the first stage, we leverage a standard pretrained
diffusion model to generate a provisional image under uncontrolled lighting.
Next, in the second stage, we resynthesize and refine the foreground object in
the generated image by passing the target lighting to a refined diffusion
model, named DiLightNet, using radiance hints computed on a coarse shape of the
foreground object inferred from the provisional image. To retain the texture
details, we multiply the radiance hints with a neural encoding of the
provisional synthesized image before passing it to DiLightNet. Finally, in the
third stage, we resynthesize the background to be consistent with the lighting
on the foreground object. We demonstrate and validate our lighting controlled
diffusion model on a variety of text prompts and lighting conditions.
</p>
</div>
</dd>
<dt><a name="item535">[535]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11931" title="Abstract">arXiv:2402.11931</a> [<a href="/pdf/2402.11931" title="Download PDF">pdf</a>, <a href="/format/2402.11931" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Soft-Weighted CrossEntropy Loss for Continous Alzheimer&#x27;s Disease  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaohui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+W">Wenjie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+M">Mangui Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Alzheimer's disease is a common cognitive disorder in the elderly. Early and
accurate diagnosis of Alzheimer's disease (AD) has a major impact on the
progress of research on dementia. At present, researchers have used machine
learning methods to detect Alzheimer's disease from the speech of participants.
However, the recognition accuracy of current methods is unsatisfactory, and
most of them focus on using low-dimensional handcrafted features to extract
relevant information from audios. This paper proposes an Alzheimer's disease
detection system based on the pre-trained framework Wav2vec 2.0 (Wav2vec2). In
addition, by replacing the loss function with the Soft-Weighted CrossEntropy
loss function, we achieved 85.45\% recognition accuracy on the same test
dataset.
</p>
</div>
</dd>
<dt><a name="item536">[536]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11933" title="Abstract">arXiv:2402.11933</a> [<a href="/pdf/2402.11933" title="Download PDF">pdf</a>, <a href="/format/2402.11933" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SLADE: Detecting Dynamic Anomalies in Edge Streams without Labels via  Self-Supervised Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+J">Jongha Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sunwoo Kim</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+K">Kijung Shin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Social and Information Networks (cs.SI)

</div>
<p class="mathjax">To detect anomalies in real-world graphs, such as social, email, and
financial networks, various approaches have been developed. While they
typically assume static input graphs, most real-world graphs grow over time,
naturally represented as edge streams. In this context, we aim to achieve three
goals: (a) instantly detecting anomalies as they occur, (b) adapting to
dynamically changing states, and (c) handling the scarcity of dynamic anomaly
labels. In this paper, we propose SLADE (Self-supervised Learning for Anomaly
Detection in Edge Streams) for rapid detection of dynamic anomalies in edge
streams, without relying on labels. SLADE detects the shifts of nodes into
abnormal states by observing deviations in their interaction patterns over
time. To this end, it trains a deep neural network to perform two
self-supervised tasks: (a) minimizing drift in node representations and (b)
generating long-term interaction patterns from short-term ones. Failure in
these tasks for a node signals its deviation from the norm. Notably, the neural
network and tasks are carefully designed so that all required operations can be
performed in constant time (w.r.t. the graph size) in response to each new edge
in the input stream. In dynamic anomaly detection across four real-world
datasets, SLADE outperforms nine competing methods, even those leveraging label
supervision.
</p>
</div>
</dd>
<dt><a name="item537">[537]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11934" title="Abstract">arXiv:2402.11934</a> [<a href="/pdf/2402.11934" title="Download PDF">pdf</a>, <a href="/format/2402.11934" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Team QUST at SemEval-2024 Task 8: A Comprehensive Study of Monolingual  and Multilingual Approaches for Detecting AI-generated Text
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaoman Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiangrun Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Taihang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+J">Jianxiang Tian</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Ye Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper presents the participation of team QUST in Task 8 SemEval 2024. We
first performed data augmentation and cleaning on the dataset to enhance model
training efficiency and accuracy. In the monolingual task, we evaluated
traditional deep-learning methods, multiscale positive-unlabeled framework
(MPU), fine-tuning, adapters and ensemble methods. Then, we selected the
top-performing models based on their accuracy from the monolingual models and
evaluated them in subtasks A and B. The final model construction employed a
stacking ensemble that combined fine-tuning with MPU. Our system achieved 8th
(scored 8th in terms of accuracy, officially ranked 13th) place in the official
test set in multilingual settings of subtask A. We release our system code
at:https://github.com/warmth27/SemEval2024_QUST
</p>
</div>
</dd>
<dt><a name="item538">[538]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11938" title="Abstract">arXiv:2402.11938</a> [<a href="/pdf/2402.11938" title="Download PDF">pdf</a>, <a href="/format/2402.11938" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parallel Program Analysis on Path Ranges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Haltermanna%2C+J">Jan Haltermanna</a>, 
<a href="/search/cs?searchtype=author&query=Jakobs%2C+M">Marie-Christine Jakobs</a>, 
<a href="/search/cs?searchtype=author&query=Richter%2C+C">Cedric Richter</a>, 
<a href="/search/cs?searchtype=author&query=Wehrheim%2C+H">Heike Wehrheim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> under submission
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Symbolic execution is a software verification technique symbolically running
programs and thereby checking for bugs. Ranged symbolic execution performs
symbolic execution on program parts, so called path ranges, in parallel. Due to
the parallelism, verification is accelerated and hence scales to larger
programs.
<br />In this paper, we discuss a generalization of ranged symbolic execution to
arbitrary program analyses. More specifically, we present a verification
approach that splits programs into path ranges and then runs arbitrary analyses
on the ranges in parallel. Our approach in particular allows to run different
analyses on different program parts. We have implemented this generalization on
top of the tool CPAchecker and evaluated it on programs from the SV-COMP
benchmark. Our evaluation shows that verification can benefit from the
parallelisation of the verification task, but also needs a form of work
stealing (between analyses) as to become efficient
</p>
</div>
</dd>
<dt><a name="item539">[539]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11940" title="Abstract">arXiv:2402.11940</a> [<a href="/pdf/2402.11940" title="Download PDF">pdf</a>, <a href="/format/2402.11940" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AICAttack: Adversarial Image Captioning Attack with Attention-Based  Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiyao Li</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+M">Mingze Ni</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yifei Dong</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Tianqing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wei Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent advances in deep learning research have shown remarkable achievements
across many tasks in computer vision (CV) and natural language processing
(NLP). At the intersection of CV and NLP is the problem of image captioning,
where the related models' robustness against adversarial attacks has not been
well studied. In this paper, we present a novel adversarial attack strategy,
which we call AICAttack (Attention-based Image Captioning Attack), designed to
attack image captioning models through subtle perturbations on images.
Operating within a black-box attack scenario, our algorithm requires no access
to the target model's architecture, parameters, or gradient information. We
introduce an attention-based candidate selection mechanism that identifies the
optimal pixels to attack, followed by Differential Evolution (DE) for
perturbing pixels' RGB values. We demonstrate AICAttack's effectiveness through
extensive experiments on benchmark datasets with multiple victim models. The
experimental results demonstrate that our method surpasses current leading-edge
techniques by effectively distributing the alignment and semantics of words in
the output.
</p>
</div>
</dd>
<dt><a name="item540">[540]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11941" title="Abstract">arXiv:2402.11941</a> [<a href="/pdf/2402.11941" title="Download PDF">pdf</a>, <a href="/format/2402.11941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comprehensive Cognitive LLM Agent for Smartphone GUI Automation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xinbei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuosheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hai Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) have shown remarkable potential as human-like
autonomous language agents to interact with real-world environments, especially
for graphical user interface (GUI) automation. However, those GUI agents
require comprehensive cognition ability including exhaustive perception and
reliable action response. We propose \underline{Co}mprehensive
\underline{Co}gnitive LLM \underline{Agent}, CoCo-Agent, with two novel
approaches, comprehensive environment perception (CEP) and conditional action
prediction (CAP), to systematically improve the GUI automation performance.
First, CEP facilitates the GUI perception through different aspects and
granularity, including screenshots and complementary detailed layouts for the
visual channel and historical actions for the textual channel. Second, CAP
decomposes the action prediction into sub-problems: action type prediction and
action target conditioned on the action type. With our technical design, our
agent achieves new state-of-the-art performance on AITW and META-GUI
benchmarks, showing promising abilities in realistic scenarios.
</p>
</div>
</dd>
<dt><a name="item541">[541]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11942" title="Abstract">arXiv:2402.11942</a> [<a href="/pdf/2402.11942" title="Download PDF">pdf</a>, <a href="/format/2402.11942" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The effect of Leaky ReLUs on the training and generalization of  overparameterized networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yinglong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shaohan Li</a>, 
<a href="/search/cs?searchtype=author&query=Lerman%2C+G">Gilad Lerman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We investigate the training and generalization errors of overparameterized
neural networks (NNs) with a wide class of leaky rectified linear unit (ReLU)
functions. More specifically, we carefully upper bound both the convergence
rate of the training error and the generalization error of such NNs and
investigate the dependence of these bounds on the Leaky ReLU parameter,
$\alpha$. We show that $\alpha =-1$, which corresponds to the absolute value
activation function, is optimal for the training error bound. Furthermore, in
special settings, it is also optimal for the generalization error bound.
Numerical experiments empirically support the practical choices guided by the
theory.
</p>
</div>
</dd>
<dt><a name="item542">[542]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11943" title="Abstract">arXiv:2402.11943</a> [<a href="/pdf/2402.11943" title="Download PDF">pdf</a>, <a href="/format/2402.11943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with  External Knowledge Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xuan%2C+K">Keyang Xuan</a>, 
<a href="/search/cs?searchtype=author&query=Yi%2C+L">Li Yi</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+F">Fan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Ruochen Wu</a>, 
<a href="/search/cs?searchtype=author&query=Fung%2C+Y+R">Yi R. Fung</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The rise of multimodal misinformation on social platforms poses significant
challenges for individuals and societies. Its increased credibility and broader
impact compared to textual misinformation make detection complex, requiring
robust reasoning across diverse media types and profound knowledge for accurate
verification. The emergence of Large Vision Language Model (LVLM) offers a
potential solution to this problem. Leveraging their proficiency in processing
visual and textual information, LVLM demonstrates promising capabilities in
recognizing complex information and exhibiting strong reasoning skills. In this
paper, we first investigate the potential of LVLM on multimodal misinformation
detection. We find that even though LVLM has a superior performance compared to
LLMs, its profound reasoning may present limited power with a lack of evidence.
Based on these observations, we propose LEMMA: LVLM-Enhanced Multimodal
Misinformation Detection with External Knowledge Augmentation. LEMMA leverages
LVLM intuition and reasoning capabilities while augmenting them with external
knowledge to enhance the accuracy of misinformation detection. Our method
improves the accuracy over the top baseline LVLM by 7% and 13% on Twitter and
Fakeddit datasets respectively.
</p>
</div>
</dd>
<dt><a name="item543">[543]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11948" title="Abstract">arXiv:2402.11948</a> [<a href="/pdf/2402.11948" title="Download PDF">pdf</a>, <a href="/ps/2402.11948" title="Download PostScript">ps</a>, <a href="/format/2402.11948" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mini-Hes: A Parallelizable Second-order Latent Factor Analysis Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jialiang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+W">Weiling Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+Y">Yurong Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+X">Xin Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Interactions among large number of entities is naturally high-dimensional and
incomplete (HDI) in many big data related tasks. Behavioral characteristics of
users are hidden in these interactions, hence, effective representation of the
HDI data is a fundamental task for understanding user behaviors. Latent factor
analysis (LFA) model has proven to be effective in representing HDI data. The
performance of an LFA model relies heavily on its training process, which is a
non-convex optimization. It has been proven that incorporating local curvature
and preprocessing gradients during its training process can lead to superior
performance compared to LFA models built with first-order family methods.
However, with the escalation of data volume, the feasibility of second-order
algorithms encounters challenges. To address this pivotal issue, this paper
proposes a mini-block diagonal hessian-free (Mini-Hes) optimization for
building an LFA model. It leverages the dominant diagonal blocks in the
generalized Gauss-Newton matrix based on the analysis of the Hessian matrix of
LFA model and serves as an intermediary strategy bridging the gap between
first-order and second-order optimization methods. Experiment results indicate
that, with Mini-Hes, the LFA model outperforms several state-of-the-art models
in addressing missing data estimation task on multiple real HDI datasets from
recommender system. (The source code of Mini-Hes is available at
https://github.com/Goallow/Mini-Hes)
</p>
</div>
</dd>
<dt><a name="item544">[544]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11953" title="Abstract">arXiv:2402.11953</a> [<a href="/pdf/2402.11953" title="Download PDF">pdf</a>, <a href="/format/2402.11953" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stealing the Invisible: Unveiling Pre-Trained CNN Models through  Adversarial Examples and Timing Side-Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shukla%2C+S">Shubhi Shukla</a>, 
<a href="/search/cs?searchtype=author&query=Alam%2C+M">Manaar Alam</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+P">Pabitra Mitra</a>, 
<a href="/search/cs?searchtype=author&query=Mukhopadhyay%2C+D">Debdeep Mukhopadhyay</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Machine learning, with its myriad applications, has become an integral
component of numerous technological systems. A common practice in this domain
is the use of transfer learning, where a pre-trained model's architecture,
readily available to the public, is fine-tuned to suit specific tasks. As
Machine Learning as a Service (MLaaS) platforms increasingly use pre-trained
models in their backends, it's crucial to safeguard these architectures and
understand their vulnerabilities. In this work, we present an approach based on
the observation that the classification patterns of adversarial images can be
used as a means to steal the models. Furthermore, the adversarial image
classifications in conjunction with timing side channels can lead to a model
stealing method. Our approach, designed for typical user-level access in remote
MLaaS environments exploits varying misclassifications of adversarial images
across different models to fingerprint several renowned Convolutional Neural
Network (CNN) and Vision Transformer (ViT) architectures. We utilize the
profiling of remote model inference times to reduce the necessary adversarial
images, subsequently decreasing the number of queries required. We have
presented our results over 27 pre-trained models of different CNN and ViT
architectures using CIFAR-10 dataset and demonstrate a high accuracy of 88.8%
while keeping the query budget under 20.
</p>
</div>
</dd>
<dt><a name="item545">[545]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11954" title="Abstract">arXiv:2402.11954</a> [<a href="/pdf/2402.11954" title="Download PDF">pdf</a>, <a href="/format/2402.11954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Emotion Recognition from Raw Audio with Sinc-convolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaohui Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+W">Wenjie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+M">Mangui Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Multimedia (cs.MM); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">Speech Emotion Recognition (SER) is still a complex task for computers with
average recall rates usually about 70% on the most realistic datasets. Most SER
systems use hand-crafted features extracted from audio signal such as energy,
zero crossing rate, spectral information, prosodic, mel frequency cepstral
coefficient (MFCC), and so on. More recently, using raw waveform for training
neural network is becoming an emerging trend. This approach is advantageous as
it eliminates the feature extraction pipeline. Learning from time-domain signal
has shown good results for tasks such as speech recognition, speaker
verification etc. In this paper, we utilize Sinc-convolution layer, which is an
efficient architecture for preprocessing raw speech waveform for emotion
recognition, to extract acoustic features from raw audio signals followed by a
long short-term memory (LSTM). We also incorporate linguistic features and
append a dialogical emotion decoding (DED) strategy. Our approach achieves a
weighted accuracy of 85.1\% in four class emotion on the Interactive Emotional
Dyadic Motion Capture (IEMOCAP) dataset.
</p>
</div>
</dd>
<dt><a name="item546">[546]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11955" title="Abstract">arXiv:2402.11955</a> [<a href="/pdf/2402.11955" title="Download PDF">pdf</a>, <a href="/format/2402.11955" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Multidomain Abstractive Summarization Using Salience  Allocation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rehman%2C+T">Tohida Rehman</a>, 
<a href="/search/cs?searchtype=author&query=Bose%2C+R">Raghubir Bose</a>, 
<a href="/search/cs?searchtype=author&query=Dey%2C+S">Soumik Dey</a>, 
<a href="/search/cs?searchtype=author&query=Chattopadhyay%2C+S">Samiran Chattopadhyay</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 1 figure, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">This paper explores the realm of abstractive text summarization through the
lens of the SEASON (Salience Allocation as Guidance for Abstractive
SummarizatiON) technique, a model designed to enhance summarization by
leveraging salience allocation techniques. The study evaluates SEASON's
efficacy by comparing it with prominent models like BART, PEGASUS, and
ProphetNet, all fine-tuned for various text summarization tasks. The assessment
is conducted using diverse datasets including CNN/Dailymail, SAMSum, and
Financial-news based Event-Driven Trading (EDT), with a specific focus on a
financial dataset containing a substantial volume of news articles from
2020/03/01 to 2021/05/06. This paper employs various evaluation metrics such as
ROUGE, METEOR, BERTScore, and MoverScore to evaluate the performance of these
models fine-tuned for generating abstractive summaries. The analysis of these
metrics offers a thorough insight into the strengths and weaknesses
demonstrated by each model in summarizing news dataset, dialogue dataset and
financial text dataset. The results presented in this paper not only contribute
to the evaluation of the SEASON model's effectiveness but also illuminate the
intricacies of salience allocation techniques across various types of datasets.
</p>
</div>
</dd>
<dt><a name="item547">[547]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11957" title="Abstract">arXiv:2402.11957</a> [<a href="/pdf/2402.11957" title="Download PDF">pdf</a>, <a href="/format/2402.11957" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Event-Based Motion Magnification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yutian Chen</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+S">Shi Guo</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+F">Fangzheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Feng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jinwei Gu</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+T">Tianfan Xue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://openimaginglab.github.io/emm/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Detecting and magnifying imperceptible high-frequency motions in real-world
scenarios has substantial implications for industrial and medical applications.
These motions are characterized by small amplitudes and high frequencies.
Traditional motion magnification methods rely on costly high-speed cameras or
active light sources, which limit the scope of their applications. In this
work, we propose a dual-camera system consisting of an event camera and a
conventional RGB camera for video motion magnification, containing
temporally-dense information from the event stream and spatially-dense data
from the RGB images. This innovative combination enables a broad and
cost-effective amplification of high-frequency motions. By revisiting the
physical camera model, we observe that estimating motion direction and
magnitude necessitates the integration of event streams with additional image
features. On this basis, we propose a novel deep network for event-based video
motion magnification that addresses two primary challenges: firstly, the high
frequency of motion induces a large number of interpolated frames (up to 80),
which our network mitigates with a Second-order Recurrent Propagation module
for better handling of long-term frame interpolations; and secondly, magnifying
subtle motions is sensitive to noise, which we address by utilizing a temporal
filter to amplify motion at specific frequencies and reduce noise impact. We
demonstrate the effectiveness and accuracy of our dual-camera system and
network through extensive experiments in magnifying small-amplitude,
high-frequency motions, offering a cost-effective and flexible solution for
motion detection and magnification.
</p>
</div>
</dd>
<dt><a name="item548">[548]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11958" title="Abstract">arXiv:2402.11958</a> [<a href="/pdf/2402.11958" title="Download PDF">pdf</a>, <a href="/format/2402.11958" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automatic Evaluation for Mental Health Counseling using LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+A">Anqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+N">Nirui Song</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lizhi Ma</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+Z">Zhenzhong Lan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">High-quality psychological counseling is crucial for mental health worldwide,
and timely evaluation is vital for ensuring its effectiveness. However,
obtaining professional evaluation for each counseling session is expensive and
challenging. Existing methods that rely on self or third-party manual reports
to assess the quality of counseling suffer from subjective biases and
limitations of time-consuming.
<br />To address above challenges, this paper proposes an innovative and efficient
automatic approach using large language models (LLMs) to evaluate the working
alliance in counseling conversations. We collected a comprehensive counseling
dataset and conducted multiple third-party evaluations based on therapeutic
relationship theory. Our LLM-based evaluation, combined with our guidelines,
shows high agreement with human evaluations and provides valuable insights into
counseling scripts. This highlights the potential of LLMs as supervisory tools
for psychotherapists. By integrating LLMs into the evaluation process, our
approach offers a cost-effective and dependable means of assessing counseling
quality, enhancing overall effectiveness.
</p>
</div>
</dd>
<dt><a name="item549">[549]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11960" title="Abstract">arXiv:2402.11960</a> [<a href="/pdf/2402.11960" title="Download PDF">pdf</a>, <a href="/format/2402.11960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DB-LLM: Accurate Dual-Binarization for Efficient LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+C">Chengtao Lv</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+H">Haotong Qin</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xiabin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Y">Yifu Ding</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuebo Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Min Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+J">Jinyang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xianglong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Large language models (LLMs) have significantly advanced the field of natural
language processing, while the expensive memory and computation consumption
impede their practical deployment. Quantization emerges as one of the most
effective methods for improving the computational efficiency of LLMs. However,
existing ultra-low-bit quantization always causes severe accuracy drops. In
this paper, we empirically relieve the micro and macro characteristics of
ultra-low bit quantization and present a novel Dual-Binarization method for
LLMs, namely DB-LLM. For the micro-level, we take both the accuracy advantage
of 2-bit-width and the efficiency advantage of binarization into account,
introducing Flexible Dual Binarization (FDB). By splitting 2-bit quantized
weights into two independent sets of binaries, FDB ensures the accuracy of
representations and introduces flexibility, utilizing the efficient bitwise
operations of binarization while retaining the inherent high sparsity of
ultra-low bit quantization. For the macro-level, we find the distortion that
exists in the prediction of LLM after quantization, which is specified as the
deviations related to the ambiguity of samples. We propose the Deviation-Aware
Distillation (DAD) method, enabling the model to focus differently on various
samples. Comprehensive experiments show that our DB-LLM not only significantly
surpasses the current State-of-The-Art (SoTA) in ultra-low bit quantization
(eg, perplexity decreased from 9.64 to 7.23), but also achieves an additional
20\% reduction in computational consumption compared to the SOTA method under
the same bit-width. Our code will be released soon.
</p>
</div>
</dd>
<dt><a name="item550">[550]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11963" title="Abstract">arXiv:2402.11963</a> [<a href="/pdf/2402.11963" title="Download PDF">pdf</a>, <a href="/format/2402.11963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Imbalance in Regression Datasets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kowatsch%2C+D">Daniel Kowatsch</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+N+M">Nicolas M. M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Tscharke%2C+K">Kilian Tscharke</a>, 
<a href="/search/cs?searchtype=author&query=Sperl%2C+P">Philip Sperl</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%B6tinger%2C+K">Konstantin B&#xf6;tinger</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">For classification, the problem of class imbalance is well known and has been
extensively studied. In this paper, we argue that imbalance in regression is an
equally important problem which has so far been overlooked: Due to under- and
over-representations in a data set's target distribution, regressors are prone
to degenerate to naive models, systematically neglecting uncommon training data
and over-representing targets seen often during training. We analyse this
problem theoretically and use resulting insights to develop a first definition
of imbalance in regression, which we show to be a generalisation of the
commonly employed imbalance measure in classification. With this, we hope to
turn the spotlight on the overlooked problem of imbalance in regression and to
provide common ground for future research.
</p>
</div>
</dd>
<dt><a name="item551">[551]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11968" title="Abstract">arXiv:2402.11968</a> [<a href="/pdf/2402.11968" title="Download PDF">pdf</a>, <a href="/format/2402.11968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> What Do Dialect Speakers Want? A Survey of Attitudes Towards Language  Technology for German Dialects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Blaschke%2C+V">Verena Blaschke</a>, 
<a href="/search/cs?searchtype=author&query=Purschke%2C+C">Christoph Purschke</a>, 
<a href="/search/cs?searchtype=author&query=Sch%C3%BCtze%2C+H">Hinrich Sch&#xfc;tze</a>, 
<a href="/search/cs?searchtype=author&query=Plank%2C+B">Barbara Plank</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Natural language processing (NLP) has largely focused on modelling
standardized languages. More recently, attention has increasingly shifted to
local, non-standardized languages and dialects. However, the relevant speaker
populations' needs and wishes with respect to NLP tools are largely unknown. In
this paper, we focus on dialects and regional languages related to German -- a
group of varieties that is heterogeneous in terms of prestige and
standardization. We survey speakers of these varieties (N=327) and present
their opinions on hypothetical language technologies for their dialects.
Although attitudes vary among subgroups of our respondents, we find that
respondents are especially in favour of potential NLP tools that work with
dialectal input (especially audio input) such as virtual assistants, and less
so for applications that produce dialectal output such as machine translation
or spellcheckers.
</p>
</div>
</dd>
<dt><a name="item552">[552]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11973" title="Abstract">arXiv:2402.11973</a> [<a href="/pdf/2402.11973" title="Download PDF">pdf</a>, <a href="/format/2402.11973" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Active Learning for Censored Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=H%C3%BCttel%2C+F+B">Frederik Boe H&#xfc;ttel</a>, 
<a href="/search/cs?searchtype=author&query=Riis%2C+C">Christoffer Riis</a>, 
<a href="/search/cs?searchtype=author&query=Rodrigues%2C+F">Filipe Rodrigues</a>, 
<a href="/search/cs?searchtype=author&query=Pereira%2C+F+C">Francisco C&#xe2;mara Pereira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Bayesian active learning is based on information theoretical approaches that
focus on maximising the information that new observations provide to the model
parameters. This is commonly done by maximising the Bayesian Active Learning by
Disagreement (BALD) acquisitions function. However, we highlight that it is
challenging to estimate BALD when the new data points are subject to
censorship, where only clipped values of the targets are observed. To address
this, we derive the entropy and the mutual information for censored
distributions and derive the BALD objective for active learning in censored
regression ($\mathcal{C}$-BALD). We propose a novel modelling approach to
estimate the $\mathcal{C}$-BALD objective and use it for active learning in the
censored setting. Across a wide range of datasets and models, we demonstrate
that $\mathcal{C}$-BALD outperforms other Bayesian active learning methods in
censored regression.
</p>
</div>
</dd>
<dt><a name="item553">[553]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11975" title="Abstract">arXiv:2402.11975</a> [<a href="/pdf/2402.11975" title="Download PDF">pdf</a>, <a href="/format/2402.11975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compress to Impress: Unleashing the Potential of Compressive Memory in  Real-World Long-Term Conversations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+N">Nuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hongguang Li</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Juhua Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Baoyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jia Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Existing retrieval-based methods have made significant strides in maintaining
long-term conversations. However, these approaches face challenges in memory
database management and accurate memory retrieval, hindering their efficacy in
dynamic, real-world interactions. This study introduces a novel framework,
COmpressive Memory-Enhanced Dialogue sYstems (COMEDY), which eschews
traditional retrieval modules and memory databases. Instead, COMEDY adopts a
''One-for-All'' approach, utilizing a single language model to manage memory
generation, compression, and response generation. Central to this framework is
the concept of compressive memory, which intergrates session-specific
summaries, user-bot dynamics, and past events into a concise memory format. To
support COMEDY, we curated a large-scale Chinese instruction-tuning dataset,
Dolphin, derived from real user-chatbot interactions. Comparative evaluations
demonstrate COMEDY's superiority over traditional retrieval-based methods in
producing more nuanced and human-like conversational experiences. Our codes are
available at https://github.com/nuochenpku/COMEDY.
</p>
</div>
</dd>
<dt><a name="item554">[554]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11980" title="Abstract">arXiv:2402.11980</a> [<a href="/pdf/2402.11980" title="Download PDF">pdf</a>, <a href="/format/2402.11980" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Buffered Streaming Edge Partitioning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chhabra%2C+A">Adil Chhabra</a>, 
<a href="/search/cs?searchtype=author&query=Faraj%2C+M+F">Marcelo Fonseca Faraj</a>, 
<a href="/search/cs?searchtype=author&query=Schulz%2C+C">Christian Schulz</a>, 
<a href="/search/cs?searchtype=author&query=Seemaier%2C+D">Daniel Seemaier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Addressing the challenges of processing massive graphs, which are prevalent
in diverse fields such as social, biological, and technical networks, we
introduce HeiStreamE and FreightE, two innovative (buffered) streaming
algorithms designed for efficient edge partitioning of large-scale graphs.
HeiStreamE utilizes an adapted Split-and-Connect graph model and a Fennel-based
multilevel partitioning scheme, while FreightE partitions a hypergraph
representation of the input graph. Besides ensuring superior solution quality,
these approaches also overcome the limitations of existing algorithms by
maintaining linear dependency on the graph size in both time and memory
complexity with no dependence on the number of blocks of partition. Our
comprehensive experimental analysis demonstrates that HeiStreamE outperforms
current streaming algorithms and the re-streaming algorithm 2PS in partitioning
quality (replication factor), and is more memory-efficient for real-world
networks where the number of edges is far greater than the number of vertices.
Further, FreightE is shown to produce fast and efficient partitions,
particularly for higher numbers of partition blocks.
</p>
</div>
</dd>
<dt><a name="item555">[555]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11984" title="Abstract">arXiv:2402.11984</a> [<a href="/pdf/2402.11984" title="Download PDF">pdf</a>, <a href="/format/2402.11984" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hebbian Learning based Orthogonal Projection for Continual Learning of  Spiking Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiao%2C+M">Mingqing Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+Q">Qingyan Meng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zongpeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+D">Di He</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zhouchen Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Neuromorphic computing with spiking neural networks is promising for
energy-efficient artificial intelligence (AI) applications. However, different
from humans who continually learn different tasks in a lifetime, neural network
models suffer from catastrophic forgetting. How could neuronal operations solve
this problem is an important question for AI and neuroscience. Many previous
studies draw inspiration from observed neuroscience phenomena and propose
episodic replay or synaptic metaplasticity, but they are not guaranteed to
explicitly preserve knowledge for neuron populations. Other works focus on
machine learning methods with more mathematical grounding, e.g., orthogonal
projection on high dimensional spaces, but there is no neural correspondence
for neuromorphic computing. In this work, we develop a new method with neuronal
operations based on lateral connections and Hebbian learning, which can protect
knowledge by projecting activity traces of neurons into an orthogonal subspace
so that synaptic weight update will not interfere with old tasks. We show that
Hebbian and anti-Hebbian learning on recurrent lateral connections can
effectively extract the principal subspace of neural activities and enable
orthogonal projection. This provides new insights into how neural circuits and
Hebbian learning can help continual learning, and also how the concept of
orthogonal projection can be realized in neuronal systems. Our method is also
flexible to utilize arbitrary training methods based on presynaptic
activities/traces. Experiments show that our method consistently solves
forgetting for spiking neural networks with nearly zero forgetting under
various supervised training methods with different error propagation
approaches, and outperforms previous approaches under various settings. Our
method can pave a solid path for building continual neuromorphic computing
systems.
</p>
</div>
</dd>
<dt><a name="item556">[556]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11985" title="Abstract">arXiv:2402.11985</a> [<a href="/pdf/2402.11985" title="Download PDF">pdf</a>, <a href="/format/2402.11985" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weakly Supervised Object Detection in Chest X-Rays with Differentiable  ROI Proposal Networks and Soft ROI Pooling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=M%C3%BCller%2C+P">Philip M&#xfc;ller</a>, 
<a href="/search/cs?searchtype=author&query=Meissen%2C+F">Felix Meissen</a>, 
<a href="/search/cs?searchtype=author&query=Kaissis%2C+G">Georgios Kaissis</a>, 
<a href="/search/cs?searchtype=author&query=Rueckert%2C+D">Daniel Rueckert</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Weakly supervised object detection (WSup-OD) increases the usefulness and
interpretability of image classification algorithms without requiring
additional supervision. The successes of multiple instance learning in this
task for natural images, however, do not translate well to medical images due
to the very different characteristics of their objects (i.e. pathologies). In
this work, we propose Weakly Supervised ROI Proposal Networks (WSRPN), a new
method for generating bounding box proposals on the fly using a specialized
region of interest-attention (ROI-attention) module. WSRPN integrates well with
classic backbone-head classification algorithms and is end-to-end trainable
with only image-label supervision. We experimentally demonstrate that our new
method outperforms existing methods in the challenging task of disease
localization in chest X-ray images. Code:
https://github.com/philip-mueller/wsrpn
</p>
</div>
</dd>
<dt><a name="item557">[557]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11987" title="Abstract">arXiv:2402.11987</a> [<a href="/pdf/2402.11987" title="Download PDF">pdf</a>, <a href="/format/2402.11987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Type Isomorphisms for Multiplicative-Additive Linear Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Di+Guardia%2C+R">R&#xe9;mi Di Guardia</a> (LIP, PLUME), 
<a href="/search/cs?searchtype=author&query=Laurent%2C+O">Olivier Laurent</a> (LIP, PLUME)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">We characterize type isomorphisms in the multiplicative-additive fragment of
linear logic (MALL), and thus in *-autonomous categories with finite products,
extending a result for the multiplicative fragment by Balat and Di Cosmo. This
yields a much richer equational theory involving distributivity and
cancellation laws. The unit-free case is obtained by relying on the proof-net
syntax introduced by Hughes and Van Glabbeek. We use the sequent calculus to
extend our results to full MALL, including all units, thanks to a study of
cut-elimination and rule commutations.
</p>
</div>
</dd>
<dt><a name="item558">[558]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11989" title="Abstract">arXiv:2402.11989</a> [<a href="/pdf/2402.11989" title="Download PDF">pdf</a>, <a href="/format/2402.11989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+Z">Zihao Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xilie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Feng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Koh%2C+Y+S">Yun Sing Koh</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Di Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jingfeng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Low-rank adaptation (LoRA) is an efficient strategy for adapting latent
diffusion models (LDMs) on a training dataset to generate specific objects by
minimizing the adaptation loss. However, adapted LDMs via LoRA are vulnerable
to membership inference (MI) attacks that can judge whether a particular data
point belongs to private training datasets, thus facing severe risks of privacy
leakage. To defend against MI attacks, we make the first effort to propose a
straightforward solution: privacy-preserving LoRA (PrivateLoRA). PrivateLoRA is
formulated as a min-max optimization problem where a proxy attack model is
trained by maximizing its MI gain while the LDM is adapted by minimizing the
sum of the adaptation loss and the proxy attack model's MI gain. However, we
empirically disclose that PrivateLoRA has the issue of unstable optimization
due to the large fluctuation of the gradient scale which impedes adaptation. To
mitigate this issue, we propose Stable PrivateLoRA that adapts the LDM by
minimizing the ratio of the adaptation loss to the MI gain, which implicitly
rescales the gradient and thus stabilizes the optimization. Our comprehensive
empirical results corroborate that adapted LDMs via Stable PrivateLoRA can
effectively defend against MI attacks while generating high-quality images. Our
code is available at https://github.com/WilliamLUO0/StablePrivateLoRA.
</p>
</div>
</dd>
<dt><a name="item559">[559]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11990" title="Abstract">arXiv:2402.11990</a> [<a href="/pdf/2402.11990" title="Download PDF">pdf</a>, <a href="/ps/2402.11990" title="Download PostScript">ps</a>, <a href="/format/2402.11990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gaussian Broadcast on Grids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiradilok%2C+P">Pakawut Jiradilok</a>, 
<a href="/search/cs?searchtype=author&query=Mossel%2C+E">Elchanan Mossel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 1 figure. Comments are very welcome!
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Probability (math.PR); Statistics Theory (math.ST)

</div>
<p class="mathjax">Motivated by the classical work on finite noisy automata (Gray 1982, G\'{a}cs
2001, Gray 2001) and by the recent work on broadcasting on grids (Makur,
Mossel, and Polyanskiy 2022), we introduce Gaussian variants of these models.
These models are defined on graded posets. At time $0$, all nodes begin with
$X_0$. At time $k\ge 1$, each node on layer $k$ computes a combination of its
inputs at layer $k-1$ with independent Gaussian noise added. When is it
possible to recover $X_0$ with non-vanishing correlation? We consider different
notions of recovery including recovery from a single node, recovery from a
bounded window, and recovery from an unbounded window.
<br />Our main interest is in two models defined on grids:
<br />In the infinite model, layer $k$ is the vertices of $\mathbb{Z}^{d+1}$ whose
sum of entries is $k$ and for a vertex $v$ at layer $k \ge 1$, $X_v=\alpha\sum
(X_u + W_{u,v})$, summed over all $u$ on layer $k-1$ that differ from $v$
exactly in one coordinate, and $W_{u,v}$ are i.i.d. $\mathcal{N}(0,1)$. We show
that when $\alpha&lt;1/(d+1)$, the correlation between $X_v$ and $X_0$ decays
exponentially, and when $\alpha&gt;1/(d+1)$, the correlation is bounded away from
$0$. The critical case when $\alpha=1/(d+1)$ exhibits a phase transition in
dimension, where $X_v$ has non-vanishing correlation with $X_0$ if and only if
$d\ge 3$. The same results hold for any bounded window.
<br />In the finite model, layer $k$ is the vertices of $\mathbb{Z}^{d+1}$ with
nonnegative entries with sum $k$. We identify the sub-critical and the
super-critical regimes. In the sub-critical regime, the correlation decays to
$0$ for unbounded windows. In the super-critical regime, there exists for every
$t$ a convex combination of $X_u$ on layer $t$ whose correlation is bounded
away from $0$. We find that for the critical parameters, the correlation is
vanishing in all dimensions and for unbounded window sizes.
</p>
</div>
</dd>
<dt><a name="item560">[560]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11993" title="Abstract">arXiv:2402.11993</a> [<a href="/pdf/2402.11993" title="Download PDF">pdf</a>, <a href="/ps/2402.11993" title="Download PostScript">ps</a>, <a href="/format/2402.11993" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Energy Efficient RAN: From Industry Standards to Trending  Practice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kundu%2C+L">Lopamudra Kundu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+X">Xingqin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Gadiyar%2C+R">Rajesh Gadiyar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">As 5G deployments continue throughout the world, concerns regarding its
energy consumption have gained significant traction. This article focuses on
radio access networks (RANs) which account for a major portion of the network
energy use. Firstly, we introduce the state-of-the-art 3GPP and O-RAN
standardization work on enhancing RAN energy efficiency. Then we highlight
three unique ways for enabling energy optimization in telecommunication
networks, including full stack acceleration, network functions consolidation,
and shared infrastructure between communication and artificial intelligence.
These network design strategies not only allow for considerable overall
reduction in the energy footprint, but also deliver several added benefits
including improved throughput, reduced cost of ownership, and increased revenue
opportunities for telcos.
</p>
</div>
</dd>
<dt><a name="item561">[561]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11995" title="Abstract">arXiv:2402.11995</a> [<a href="/pdf/2402.11995" title="Download PDF">pdf</a>, <a href="/format/2402.11995" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Network Inversion of Binarised Neural Nets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Suhail%2C+P">Pirzada Suhail</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Supratik Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Sethi%2C+A">Amit Sethi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">While the deployment of neural networks, yielding impressive results, becomes
more prevalent in various applications, their interpretability and
understanding remain a critical challenge. Network inversion, a technique that
aims to reconstruct the input space from the model's learned internal
representations, plays a pivotal role in unraveling the black-box nature of
input to output mappings in neural networks. In safety-critical scenarios,
where model outputs may influence pivotal decisions, the integrity of the
corresponding input space is paramount, necessitating the elimination of any
extraneous "garbage" to ensure the trustworthiness of the network. Binarised
Neural Networks (BNNs), characterized by binary weights and activations, offer
computational efficiency and reduced memory requirements, making them suitable
for resource-constrained environments. This paper introduces a novel approach
to invert a trained BNN by encoding it into a CNF formula that captures the
network's structure, allowing for both inference and inversion.
</p>
</div>
</dd>
<dt><a name="item562">[562]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11996" title="Abstract">arXiv:2402.11996</a> [<a href="/pdf/2402.11996" title="Download PDF">pdf</a>, <a href="/format/2402.11996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ISCUTE: Instance Segmentation of Cables Using Text Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kozlovsky%2C+S">Shir Kozlovsky</a>, 
<a href="/search/cs?searchtype=author&query=Joglekar%2C+O">Omkar Joglekar</a>, 
<a href="/search/cs?searchtype=author&query=Di+Castro%2C+D">Dotan Di Castro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In the field of robotics and automation, conventional object recognition and
instance segmentation methods face a formidable challenge when it comes to
perceiving Deformable Linear Objects (DLOs) like wires, cables, and flexible
tubes. This challenge arises primarily from the lack of distinct attributes
such as shape, color, and texture, which calls for tailored solutions to
achieve precise identification. In this work, we propose a foundation
model-based DLO instance segmentation technique that is text-promptable and
user-friendly. Specifically, our approach combines the text-conditioned
semantic segmentation capabilities of CLIPSeg model with the zero-shot
generalization capabilities of Segment Anything Model (SAM). We show that our
method exceeds SOTA performance on DLO instance segmentation, achieving a mIoU
of $91.21\%$. We also introduce a rich and diverse DLO-specific dataset for
instance segmentation.
</p>
</div>
</dd>
<dt><a name="item563">[563]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11997" title="Abstract">arXiv:2402.11997</a> [<a href="/pdf/2402.11997" title="Download PDF">pdf</a>, <a href="/format/2402.11997" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Remember This Event That Year? Assessing Temporal Information and  Reasoning in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beniwal%2C+H">Himanshu Beniwal</a>, 
<a href="/search/cs?searchtype=author&query=D%2C+K+N">Kowsik Nandagopan D</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+M">Mayank Singh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Large Language Models (LLMs) are increasingly becoming ubiquitous, yet their
ability to reason about and retain temporal information remains limited. This
hinders their application in real-world scenarios where understanding the
sequential nature of events is crucial. This paper experiments with
state-of-the-art models on a novel, large-scale temporal dataset,
\textbf{TempUN}, to reveal significant limitations in temporal retention and
reasoning abilities. Interestingly, closed-source models indicate knowledge
gaps more frequently, potentially suggesting a trade-off between uncertainty
awareness and incorrect responses. Further, exploring various fine-tuning
approaches yielded no major performance improvements. The associated dataset
and code are available at the following URL
(https://github.com/lingoiitgn/TempUN).
</p>
</div>
</dd>
<dt><a name="item564">[564]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12000" title="Abstract">arXiv:2402.12000</a> [<a href="/pdf/2402.12000" title="Download PDF">pdf</a>, <a href="/ps/2402.12000" title="Download PostScript">ps</a>, <a href="/format/2402.12000" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Thinking Outside the Black Box: Insights from a Digital Exhibition in  the Humanities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Barzaghi%2C+S">Sebastian Barzaghi</a>, 
<a href="/search/cs?searchtype=author&query=Bordignon%2C+A">Alice Bordignon</a>, 
<a href="/search/cs?searchtype=author&query=Gualandi%2C+B">Bianca Gualandi</a>, 
<a href="/search/cs?searchtype=author&query=Peroni%2C+S">Silvio Peroni</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Sumbitted to the AIUCD2024 Conference: <a href="https://aiucd2024.unict.it/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>

</div>
<p class="mathjax">One of the main goals of Open Science is to make research more reproducible.
There is no consensus, however, on what exactly "reproducibility" is, as
opposed for example to "replicability", and how it applies to different
research fields. After a short review of the literature on
reproducibility/replicability with a focus on the humanities, we describe how
the creation of the digital twin of the temporary exhibition "The Other
Renaissance" has been documented throughout, with different methods, but with
constant attention to research transparency, openness and accountability. A
careful documentation of the study design, data collection and analysis
techniques helps reflect and make all possible influencing factors explicit,
and is a fundamental tool for reliability and rigour and for opening the "black
box" of research.
</p>
</div>
</dd>
<dt><a name="item565">[565]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12001" title="Abstract">arXiv:2402.12001</a> [<a href="/pdf/2402.12001" title="Download PDF">pdf</a>, <a href="/format/2402.12001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Extractive Knowledge Graph Summarization: Applications,  Approaches, Evaluation, and Future Directions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaxia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+G">Gong Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 13 figures, submitted to the IJCAI 2024 Survey Track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Databases (cs.DB); Information Retrieval (cs.IR); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">With the continuous growth of large Knowledge Graphs (KGs), extractive KG
summarization becomes a trending task. Aiming at distilling a compact subgraph
with condensed information, it facilitates various downstream KG-based tasks.
In this survey paper, we are among the first to provide a systematic overview
of its applications and define a taxonomy for existing methods from its
interdisciplinary studies. Future directions are also laid out based on our
extensive and comparative review.
</p>
</div>
</dd>
<dt><a name="item566">[566]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12002" title="Abstract">arXiv:2402.12002</a> [<a href="/pdf/2402.12002" title="Download PDF">pdf</a>, <a href="/ps/2402.12002" title="Download PostScript">ps</a>, <a href="/format/2402.12002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixed-Reality-Guided Teleoperation of a Collaborative Robot for Surgical  Procedures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rus%2C+G">Gabriela Rus</a>, 
<a href="/search/cs?searchtype=author&query=Hajjar%2C+N+A">Nadim Al Hajjar</a> (UMP), 
<a href="/search/cs?searchtype=author&query=Tucan%2C+P">Paul Tucan</a>, 
<a href="/search/cs?searchtype=author&query=Ciocan%2C+A">Andra Ciocan</a> (UMP), 
<a href="/search/cs?searchtype=author&query=Vaida%2C+C">Calin Vaida</a>, 
<a href="/search/cs?searchtype=author&query=Radu%2C+C">Corina Radu</a> (UMP), 
<a href="/search/cs?searchtype=author&query=Chablat%2C+D">Damien Chablat</a> (LS2N-&#xe9;quipe RoMas), 
<a href="/search/cs?searchtype=author&query=Pisla%2C+D">Doina Pisla</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 33rd International Conference on Robotics in Alpe-Adria-Danube
  Region, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">The development of advanced surgical systems embedding the Master-Slave
control strategy introduced the possibility of remote interaction between the
surgeon and the patient, also known as teleoperation. The present paper aims to
integrate innovative technologies into the teleoperation process to enhance
workflow during surgeries. The proposed system incorporates a collaborative
robot, Kuka IIWA LBR, and Hololens 2 (an augmented reality device), allowing
the user to control the robot in an expansive environment that integrates
actual (real data) with additional digital information imported via Hololens 2.
Experimental data demonstrate the user's ability to control the Kuka IIWA using
various gestures to position it with respect to real or digital objects. Thus,
this system offers a novel solution to manipulate robots used in surgeries in a
more intuitive manner, contributing to the reduction of the learning curve for
surgeons. Calibration and testing in multiple scenarios demonstrate the
efficiency of the system in providing seamless movements.
</p>
</div>
</dd>
<dt><a name="item567">[567]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12004" title="Abstract">arXiv:2402.12004</a> [<a href="/pdf/2402.12004" title="Download PDF">pdf</a>, <a href="/format/2402.12004" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Direct Consistency Optimization for Compositional Text-to-Image  Personalization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kyungmin Lee</a>, 
<a href="/search/cs?searchtype=author&query=Kwak%2C+S">Sangkyung Kwak</a>, 
<a href="/search/cs?searchtype=author&query=Sohn%2C+K">Kihyuk Sohn</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+J">Jinwoo Shin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint. See our project page (<a href="https://dco-t2i.github.io/">this https URL</a>) for more examples and codes
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Text-to-image (T2I) diffusion models, when fine-tuned on a few personal
images, are able to generate visuals with a high degree of consistency.
However, they still lack in synthesizing images of different scenarios or
styles that are possible in the original pretrained models. To address this, we
propose to fine-tune the T2I model by maximizing consistency to reference
images, while penalizing the deviation from the pretrained model. We devise a
novel training objective for T2I diffusion models that minimally fine-tunes the
pretrained model to achieve consistency. Our method, dubbed \emph{Direct
Consistency Optimization}, is as simple as regular diffusion loss, while
significantly enhancing the compositionality of personalized T2I models. Also,
our approach induces a new sampling method that controls the tradeoff between
image fidelity and prompt fidelity. Lastly, we emphasize the necessity of using
a comprehensive caption for reference images to further enhance the image-text
alignment. We show the efficacy of the proposed method on the T2I
personalization for subject, style, or both. In particular, our method results
in a superior Pareto frontier to the baselines. Generated examples and codes
are in our project page( https://dco-t2i.github.io/).
</p>
</div>
</dd>
<dt><a name="item568">[568]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12008" title="Abstract">arXiv:2402.12008</a> [<a href="/pdf/2402.12008" title="Download PDF">pdf</a>, <a href="/format/2402.12008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cluster Metric Sensitivity to Irrelevant Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McCrory%2C+M">Miles McCrory</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+S+A">Spencer A. Thomas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Clustering algorithms are used extensively in data analysis for data
exploration and discovery. Technological advancements lead to continually
growth of data in terms of volume, dimensionality and complexity. This provides
great opportunities in data analytics as the data can be interrogated for many
different purposes. This however leads challenges, such as identification of
relevant features for a given task. In supervised tasks, one can utilise a
number of methods to optimise the input features for the task objective (e.g.
classification accuracy). In unsupervised problems, such tools are not readily
available, in part due to an inability to quantify feature relevance in
unlabeled tasks. In this paper, we investigate the sensitivity of clustering
performance noisy uncorrelated variables iteratively added to baseline datasets
with well defined clusters. We show how different types of irrelevant variables
can impact the outcome of a clustering result from $k$-means in different ways.
We observe a resilience to very high proportions of irrelevant features for
adjusted rand index (ARI) and normalised mutual information (NMI) when the
irrelevant features are Gaussian distributed. For Uniformly distributed
irrelevant features, we notice the resilience of ARI and NMI is dependent on
the dimensionality of the data and exhibits tipping points between high scores
and near zero. Our results show that the Silhouette Coefficient and the
Davies-Bouldin score are the most sensitive to irrelevant added features
exhibiting large changes in score for comparably low proportions of irrelevant
features regardless of underlying distribution or data scaling. As such the
Silhouette Coefficient and the Davies-Bouldin score are good candidates for
optimising feature selection in unsupervised clustering tasks.
</p>
</div>
</dd>
<dt><a name="item569">[569]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12010" title="Abstract">arXiv:2402.12010</a> [<a href="/pdf/2402.12010" title="Download PDF">pdf</a>, <a href="/format/2402.12010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training Green AI Models Using Elite Samples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alswaitti%2C+M">Mohammed Alswaitti</a>, 
<a href="/search/cs?searchtype=author&query=Verdecchia%2C+R">Roberto Verdecchia</a>, 
<a href="/search/cs?searchtype=author&query=Danoy%2C+G">Gr&#xe9;goire Danoy</a>, 
<a href="/search/cs?searchtype=author&query=Bouvry%2C+P">Pascal Bouvry</a>, 
<a href="/search/cs?searchtype=author&query=Pecero%2C+J">Johnatan Pecero</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">The substantial increase in AI model training has considerable environmental
implications, mandating more energy-efficient and sustainable AI practices. On
the one hand, data-centric approaches show great potential towards training
energy-efficient AI models. On the other hand, instance selection methods
demonstrate the capability of training AI models with minimised training sets
and negligible performance degradation. Despite the growing interest in both
topics, the impact of data-centric training set selection on energy efficiency
remains to date unexplored. This paper presents an evolutionary-based sampling
framework aimed at (i) identifying elite training samples tailored for datasets
and model pairs, (ii) comparing model performance and energy efficiency gains
against typical model training practice, and (iii) investigating the
feasibility of this framework for fostering sustainable model training
practices. To evaluate the proposed framework, we conducted an empirical
experiment including 8 commonly used AI classification models and 25 publicly
available datasets. The results showcase that by considering 10% elite training
samples, the models' performance can show a 50% improvement and remarkable
energy savings of 98% compared to the common training practice.
</p>
</div>
</dd>
<dt><a name="item570">[570]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12011" title="Abstract">arXiv:2402.12011</a> [<a href="/pdf/2402.12011" title="Download PDF">pdf</a>, <a href="/format/2402.12011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Systematic Comparison of Contextualized Word Embeddings for Lexical  Semantic Change
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Periti%2C+F">Francesco Periti</a>, 
<a href="/search/cs?searchtype=author&query=Tahmasebi%2C+N">Nina Tahmasebi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to NAACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Contextualized embeddings are the preferred tool for modeling Lexical
Semantic Change (LSC). Current evaluations typically focus on a specific task
known as Graded Change Detection (GCD). However, performance comparison across
work are often misleading due to their reliance on diverse settings. In this
paper, we evaluate state-of-the-art models and approaches for GCD under equal
conditions. We further break the LSC problem into Word-in-Context (WiC) and
Word Sense Induction (WSI) tasks, and compare models across these different
levels. Our evaluation is performed across different languages on eight
available benchmarks for LSC, and shows that (i) APD outperforms other
approaches for GCD; (ii) XL-LEXEME outperforms other contextualized models for
WiC, WSI, and GCD, while being comparable to GPT-4; (iii) there is a clear need
for improving the modeling of word meanings, as well as focus on how, when, and
why these meanings change, rather than solely focusing on the extent of
semantic change.
</p>
</div>
</dd>
<dt><a name="item571">[571]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12015" title="Abstract">arXiv:2402.12015</a> [<a href="/pdf/2402.12015" title="Download PDF">pdf</a>, <a href="/format/2402.12015" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Index Policy Based on Sarsa and Q-learning for Heterogeneous Smart  Target Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hao%2C+Y">Yuhang Hao</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Z">Zengfu Wang</a>, 
<a href="/search/eess?searchtype=author&query=Fu%2C+J">Jing Fu</a>, 
<a href="/search/eess?searchtype=author&query=Pan%2C+Q">Quan Pan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In solving the non-myopic radar scheduling for multiple smart target tracking
within an active and passive radar network, we need to consider both short-term
enhanced tracking performance and a higher probability of target maneuvering in
the future with active tracking. Acquiring the long-term tracking performance
while scheduling the beam resources of active and passive radars poses a
challenge. To address this challenge, we model this problem as a Markov
decision process consisting of parallel restless bandit processes. Each bandit
process is associated with a smart target, of which the estimation state
evolves according to different discrete dynamic models for different actions -
whether or not the target is being tracked. The discrete state is defined by
the dynamic mode. The problem exhibits the curse of dimensionality, where
optimal solutions are in general intractable. We resort to heuristics through
the famous restless multi-armed bandit techniques. It follows with efficient
scheduling policies based on the indices that are real numbers representing the
marginal rewards of taking different actions. For the inevitable practical case
with unknown transition matrices, we propose a new method that utilizes the
forward Sarsa and backward Q-learning to approximate the indices through
adapting the state-action value functions, or equivalently the Q-functions, and
propose a new policy, namely ISQ, aiming to maximize the long-term tracking
rewards. Numerical results demonstrate that the proposed ISQ policy outperforms
conventional Q-learning-based methods and rapidly converges to the well-known
Whittle index policy with revealed state transition models, which is considered
the benchmark.
</p>
</div>
</dd>
<dt><a name="item572">[572]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12017" title="Abstract">arXiv:2402.12017</a> [<a href="/pdf/2402.12017" title="Download PDF">pdf</a>, <a href="/ps/2402.12017" title="Download PostScript">ps</a>, <a href="/format/2402.12017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Private Interdependent Valuations: New Bounds for Single-Item Auctions  and Matroids
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eden%2C+A">Alon Eden</a>, 
<a href="/search/cs?searchtype=author&query=Feldman%2C+M">Michal Feldman</a>, 
<a href="/search/cs?searchtype=author&query=Mauras%2C+S">Simon Mauras</a>, 
<a href="/search/cs?searchtype=author&query=Mohan%2C+D">Divyarthi Mohan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">We study auction design within the widely acclaimed model of interdependent
values, introduced by Milgrom and Weber [1982]. In this model, every bidder $i$
has a private signal $s_i$ for the item for sale, and a public valuation
function $v_i(s_1,\ldots,s_n)$ which maps every vector of private signals (of
all bidders) into a real value. A recent line of work established the existence
of approximately-optimal mechanisms within this framework, even in the more
challenging scenario where each bidder's valuation function $v_i$ is also
private. This body of work has primarily focused on single-item auctions with
two natural classes of valuations: those exhibiting submodularity over signals
(SOS) and $d$-critical valuations.
<br />In this work we advance the state of the art on interdependent values with
private valuation functions, with respect to both SOS and $d$-critical
valuations. For SOS valuations, we devise a new mechanism that gives an
improved approximation bound of $5$ for single-item auctions. This mechanism
employs a novel variant of an "eating mechanism", leveraging LP-duality to
achieve feasibility with reduced welfare loss. For $d$-critical valuations, we
broaden the scope of existing results beyond single-item auctions, introducing
a mechanism that gives a $(d+1)$-approximation for any environment with matroid
feasibility constraints on the set of agents that can be simultaneously served.
Notably, this approximation bound is tight, even with respect to single-item
auctions.
</p>
</div>
</dd>
<dt><a name="item573">[573]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12018" title="Abstract">arXiv:2402.12018</a> [<a href="/pdf/2402.12018" title="Download PDF">pdf</a>, <a href="/format/2402.12018" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Even-Cycle Detection in the Randomized and Quantum CONGEST Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fraigniaud%2C+P">Pierre Fraigniaud</a>, 
<a href="/search/cs?searchtype=author&query=Luce%2C+M">Mael Luce</a>, 
<a href="/search/cs?searchtype=author&query=Magniez%2C+F">Frederic Magniez</a>, 
<a href="/search/cs?searchtype=author&query=Todinca%2C+I">Ioan Todinca</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Quantum Physics (quant-ph)

</div>
<p class="mathjax">We show that, for every $k\geq 2$, $C_{2k}$-freeness can be decided in
$O(n^{1-1/k})$ rounds in the \CONGEST{} model by a randomized Monte-Carlo
distributed algorithm with one-sided error probability $1/3$. This matches the
best round-complexities of previously known algorithms for $k\in\{2,3,4,5\}$ by
Drucker et al. [PODC'14] and Censor-Hillel et al. [DISC'20], but improves the
complexities of the known algorithms for $k&gt;5$ by Eden et al. [DISC'19], which
were essentially of the form $\tilde O(n^{1-2/k^2})$. Our algorithm uses
colored BFS-explorations with threshold, but with an original \emph{global}
approach that enables to overcome a recent impossibility result by Fraigniaud
et al. [SIROCCO'23] about using colored BFS-exploration with \emph{local}
threshold for detecting cycles.
<br />We also show how to quantize our algorithm for achieving a round-complexity
$\tilde O(n^{\frac{1}{2}-\frac{1}{2k}})$ in the quantum setting for deciding
$C_{2k}$ freeness. Furthermore, this allows us to improve the known quantum
complexities of the simpler problem of detecting cycles of length \emph{at
most}~$2k$ by van Apeldoorn and de Vos [PODC'22]. Our quantization is in two
steps. First, the congestion of our randomized algorithm is reduced, to the
cost of reducing its success probability too. Second, the success probability
is boosted using a new quantum framework derived from sequential algorithms,
namely Monte-Carlo quantum amplification.
</p>
</div>
</dd>
<dt><a name="item574">[574]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12019" title="Abstract">arXiv:2402.12019</a> [<a href="/pdf/2402.12019" title="Download PDF">pdf</a>, <a href="/format/2402.12019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Collision-Free Robot Scheduling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Adamson%2C+D">Duncan Adamson</a>, 
<a href="/search/cs?searchtype=author&query=Flaherty%2C+N">Nathan Flaherty</a>, 
<a href="/search/cs?searchtype=author&query=Potapov%2C+I">Igor Potapov</a>, 
<a href="/search/cs?searchtype=author&query=Spirakis%2C+P">Paul Spirakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Robots are becoming an increasingly common part of scientific work within
laboratory environments. In this paper, we investigate the problem of designing
\emph{schedules} for completing a set of tasks at fixed locations with multiple
robots in a laboratory. We represent the laboratory as a graph with tasks
placed on fixed vertices and robots represented as agents, with the constraint
that no two robots may occupy the same vertex at any given timestep. Each
schedule is partitioned into a set of timesteps, corresponding to a walk
through the graph (allowing for a robot to wait at a vertex to complete a
task), with each timestep taking time equal to the time for a robot to move
from one vertex to another and each task taking some given number of timesteps
during the completion of which a robot must stay at the vertex containing the
task. The goal is to determine a set of schedules, with one schedule for each
robot, minimising the number of timesteps taken by the schedule taking the
greatest number of timesteps within the set of schedules.
<br />We show that this problem is NP-complete for many simple classes of graphs,
the problem of determining the fastest schedule, defined by the number of time
steps required for a robot to visit every vertex in the schedule and complete
every task assigned in its assigned schedule. Explicitly, we provide this
result for complete graphs, bipartite graphs, star graphs, and planar graphs.
Finally, we provide positive results for line graphs, showing that we can find
an optimal set of schedules for $k$ robots completing $m$ tasks of equal length
of a path of length $n$ in $O(kmn)$ time, and a $k$-approximation when the
length of the tasks is unbounded.
</p>
</div>
</dd>
<dt><a name="item575">[575]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12021" title="Abstract">arXiv:2402.12021</a> [<a href="/pdf/2402.12021" title="Download PDF">pdf</a>, <a href="/ps/2402.12021" title="Download PostScript">ps</a>, <a href="/format/2402.12021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Projected Block Coordinate Descent for sparse spike estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=B%C3%A9nard%2C+P">Pierre-Jean B&#xe9;nard</a> (IMB), 
<a href="/search/math?searchtype=author&query=Traonmilin%2C+Y">Yann Traonmilin</a> (IMB), 
<a href="/search/math?searchtype=author&query=Aujol%2C+J+F">Jean Fran&#xe7;ois Aujol</a> (IMB)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Optimization and Control (math.OC)

</div>
<p class="mathjax">We consider the problem of recovering off-the-grid spikes from linear
measurements. The state of the art Over-Parametrized Continuous Orthogonal
Matching Pursuit (OP-COMP) with Projected Gradient Descent (PGD) successfully
recovers those signals. In most cases, the main computational cost lies in a
unique global descent on all parameters (positions and amplitudes). In this
paper, we propose to improve this algorithm by accelerating this descent step.
We introduce a new algorithm, based on Block Coordinate Descent, that takes
advantages of the sparse structure of the problem. Based on qualitative
theoretical results, this algorithm shows improvement in calculation times in
realistic synthetic microscopy experiments.
</p>
</div>
</dd>
<dt><a name="item576">[576]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12022" title="Abstract">arXiv:2402.12022</a> [<a href="/pdf/2402.12022" title="Download PDF">pdf</a>, <a href="/format/2402.12022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distilling Large Language Models for Text-Attributed Graph Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+B">Bo Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+Y">Yuntong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+L">Liang Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Text-Attributed Graphs (TAGs) are graphs of connected textual documents.
Graph models can efficiently learn TAGs, but their training heavily relies on
human-annotated labels, which are scarce or even unavailable in many
applications. Large language models (LLMs) have recently demonstrated
remarkable capabilities in few-shot and zero-shot TAG learning, but they suffer
from scalability, cost, and privacy issues. Therefore, in this work, we focus
on synergizing LLMs and graph models with their complementary strengths by
distilling the power of LLMs to a local graph model on TAG learning. To address
the inherent gaps between LLMs (generative models for texts) and graph models
(discriminative models for graphs), we propose first to let LLMs teach an
interpreter with rich textual rationale and then let a student model mimic the
interpreter's reasoning without LLMs' textual rationale. Extensive experiments
validate the efficacy of our proposed framework.
</p>
</div>
</dd>
<dt><a name="item577">[577]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12023" title="Abstract">arXiv:2402.12023</a> [<a href="/pdf/2402.12023" title="Download PDF">pdf</a>, <a href="/format/2402.12023" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluation of ChatGPT&#x27;s Smart Contract Auditing Capabilities Based on  Chain of Thought
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yuying Du</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xueyan Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Smart contracts, as a key component of blockchain technology, play a crucial
role in ensuring the automation of transactions and adherence to protocol
rules. However, smart contracts are susceptible to security vulnerabilities,
which, if exploited, can lead to significant asset losses. This study explores
the potential of enhancing smart contract security audits using the GPT-4
model. We utilized a dataset of 35 smart contracts from the SolidiFI-benchmark
vulnerability library, containing 732 vulnerabilities, and compared it with
five other vulnerability detection tools to evaluate GPT-4's ability to
identify seven common types of vulnerabilities. Moreover, we assessed GPT-4's
performance in code parsing and vulnerability capture by simulating a
professional auditor's auditing process using CoT(Chain of Thought) prompts
based on the audit reports of eight groups of smart contracts. We also
evaluated GPT-4's ability to write Solidity Proof of Concepts (PoCs). Through
experimentation, we found that GPT-4 performed poorly in detecting smart
contract vulnerabilities, with a high Precision of 96.6%, but a low Recall of
37.8%, and an F1-score of 41.1%, indicating a tendency to miss vulnerabilities
during detection. Meanwhile, it demonstrated good contract code parsing
capabilities, with an average comprehensive score of 6.5, capable of
identifying the background information and functional relationships of smart
contracts; in 60% of the cases, it could write usable PoCs, suggesting GPT-4
has significant potential application in PoC writing. These experimental
results indicate that GPT-4 lacks the ability to detect smart contract
vulnerabilities effectively, but its performance in contract code parsing and
PoC writing demonstrates its significant potential as an auxiliary tool in
enhancing the efficiency and effectiveness of smart contract security audits.
</p>
</div>
</dd>
<dt><a name="item578">[578]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12024" title="Abstract">arXiv:2402.12024</a> [<a href="/pdf/2402.12024" title="Download PDF">pdf</a>, <a href="/format/2402.12024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lightweight Syntactic API Usage Analysis with UCov
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Monce%2C+G">Gustave Monce</a> (LaBRI), 
<a href="/search/cs?searchtype=author&query=Couturou%2C+T">Thomas Couturou</a> (LaBRI), 
<a href="/search/cs?searchtype=author&query=Hamdaoui%2C+Y">Yasmine Hamdaoui</a> (LaBRI), 
<a href="/search/cs?searchtype=author&query=Degueule%2C+T">Thomas Degueule</a> (LaBRI), 
<a href="/search/cs?searchtype=author&query=Falleri%2C+J">Jean-R&#xe9;my Falleri</a> (LaBRI, IUF)
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 32nd IEEE/ACM International Conference on Program Comprehension
  (ICPC 2024), Apr 2024, Lisboa, Portugal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Designing an effective API is essential for library developers as it is the
lens through which clients will judge its usability and benefits, as well as
the main friction point when the library evolves. Despite its importance,
defining the boundaries of an API is a challenging task, mainly due to the
diverse mechanisms provided by programming languages that have non-trivial
interplays. In this paper, we present a novel conceptual framework designed to
assist library maintainers in understanding the interactions allowed by their
APIs via the use of syntactic usage models. These customizable models enable
library maintainers to improve their design ahead of release, reducing friction
during evolution. The complementary syntactic usage footprints and coverage
scores, inferred from client code using the API (e.g., documentation samples,
tests, third-party clients), enable developers to understand in-the-wild uses
of their APIs and to reflect on the adequacy of their tests and documentation.
We implement these models for Java libraries in a new tool UCov and demonstrate
its capabilities on three libraries exhibiting diverse styles of interaction:
jsoup, commons-cli, and spark. Our exploratory case study shows that UCov
provides valuable information regarding API design and fine-grained analysis of
client code to identify under-tested and under-documented library code.
</p>
</div>
</dd>
<dt><a name="item579">[579]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12025" title="Abstract">arXiv:2402.12025</a> [<a href="/pdf/2402.12025" title="Download PDF">pdf</a>, <a href="/format/2402.12025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Speech Translation with Speech Foundation Models and Large Language  Models: What is There and What is Missing?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gaido%2C+M">Marco Gaido</a>, 
<a href="/search/cs?searchtype=author&query=Papi%2C+S">Sara Papi</a>, 
<a href="/search/cs?searchtype=author&query=Negri%2C+M">Matteo Negri</a>, 
<a href="/search/cs?searchtype=author&query=Bentivogli%2C+L">Luisa Bentivogli</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The field of natural language processing (NLP) has recently witnessed a
transformative shift with the emergence of foundation models, particularly
Large Language Models (LLMs) that have revolutionized text-based NLP. This
paradigm has extended to other modalities, including speech, where researchers
are actively exploring the combination of Speech Foundation Models (SFMs) and
LLMs into single, unified models capable of addressing multimodal tasks. Among
such tasks, this paper focuses on speech-to-text translation (ST). By examining
the published papers on the topic, we propose a unified view of the
architectural solutions and training strategies presented so far, highlighting
similarities and differences among them. Based on this examination, we not only
organize the lessons learned but also show how diverse settings and evaluation
approaches hinder the identification of the best-performing solution for each
architectural building block and training choice. Lastly, we outline
recommendations for future works on the topic aimed at better understanding the
strengths and weaknesses of the SFM+LLM solutions for ST.
</p>
</div>
</dd>
<dt><a name="item580">[580]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12026" title="Abstract">arXiv:2402.12026</a> [<a href="/pdf/2402.12026" title="Download PDF">pdf</a>, <a href="/format/2402.12026" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Acquiring Clean Language Models from Backdoor Poisoned Datasets by  Downscaling Frequency Space
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zongru Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuosheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+P">Pengzhou Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Gongshen Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Despite the notable success of language models (LMs) in various natural
language processing (NLP) tasks, the reliability of LMs is susceptible to
backdoor attacks. Prior research attempts to mitigate backdoor learning while
training the LMs on the poisoned dataset, yet struggles against complex
backdoor attacks in real-world scenarios. In this paper, we investigate the
learning mechanisms of backdoor LMs in the frequency space by Fourier analysis.
Our findings indicate that the backdoor mapping presented on the poisoned
datasets exhibits a more discernible inclination towards lower frequency
compared to clean mapping, resulting in the faster convergence of backdoor
mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation
(MuScleLoRA), which deploys multiple radial scalings in the frequency space
with low-rank adaptation to the target model and further aligns the gradients
when updating parameters. Through downscaling in the frequency space,
MuScleLoRA encourages the model to prioritize the learning of relatively
high-frequency clean mapping, consequently mitigating backdoor learning.
Experimental results demonstrate that MuScleLoRA outperforms baselines
significantly. Notably, MuScleLoRA reduces the average success rate of diverse
backdoor attacks to below 15\% across multiple datasets and generalizes to
various backbone LMs, including BERT, RoBERTa, and Llama2. The codes are
available at https://github.com/ZrW00/MuScleLoRA.
</p>
</div>
</dd>
<dt><a name="item581">[581]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12028" title="Abstract">arXiv:2402.12028</a> [<a href="/pdf/2402.12028" title="Download PDF">pdf</a>, <a href="/format/2402.12028" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact solutions to the Weighted Region Problem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Berg%2C+S">Sarita de Berg</a>, 
<a href="/search/cs?searchtype=author&query=Esteban%2C+G">Guillermo Esteban</a>, 
<a href="/search/cs?searchtype=author&query=Silveira%2C+R+I">Rodrigo I. Silveira</a>, 
<a href="/search/cs?searchtype=author&query=Staals%2C+F">Frank Staals</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">In this paper, we consider the Weighted Region Problem. In the Weighted
Region Problem, the length of a path is defined as the sum of the weights of
the subpaths within each region, where the weight of a subpath is its Euclidean
length multiplied by a weight $ \alpha \geq 0 $ depending on the region. We
study a restricted version of the problem of determining shortest paths through
a single weighted rectangular region. We prove that even this very restricted
version of the problem is unsolvable within the Algebraic Computation Model
over the Rational Numbers (ACMQ). On the positive side, we provide the
equations for the shortest paths that are computable within the ACMQ.
Additionally, we provide equations for the bisectors between regions of the
Shortest Path Map for a source point on the boundary of (or inside) the
rectangular region.
</p>
</div>
</dd>
<dt><a name="item582">[582]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12030" title="Abstract">arXiv:2402.12030</a> [<a href="/pdf/2402.12030" title="Download PDF">pdf</a>, <a href="/format/2402.12030" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Cross-Tokenizer Distillation: the Universal Logit Distillation  Loss for LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Boizard%2C+N">Nicolas Boizard</a>, 
<a href="/search/cs?searchtype=author&query=El-Haddad%2C+K">Kevin El-Haddad</a>, 
<a href="/search/cs?searchtype=author&query=Hudelot%2C+C">C&#xe9;line Hudelot</a>, 
<a href="/search/cs?searchtype=author&query=Colombo%2C+P">Pierre Colombo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, Under Review - ACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Deploying large language models (LLMs) of several billion parameters can be
impractical in most industrial use cases due to constraints such as cost,
latency limitations, and hardware accessibility. Knowledge distillation (KD)
offers a solution by compressing knowledge from resource-intensive large models
to smaller ones. Various strategies exist, some relying on the text generated
by the teacher model and optionally utilizing his logits to enhance learning.
However, these methods based on logits often require both teacher and student
models to share the same tokenizer, limiting their applicability across
different LLM families. In this paper, we introduce Universal Logit
Distillation (ULD) loss, grounded in optimal transport, to address this
limitation. Our experimental results demonstrate the effectiveness of ULD loss
in enabling distillation across models with different architectures and
tokenizers, paving the way to a more widespread use of distillation techniques.
</p>
</div>
</dd>
<dt><a name="item583">[583]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12032" title="Abstract">arXiv:2402.12032</a> [<a href="/pdf/2402.12032" title="Download PDF">pdf</a>, <a href="/ps/2402.12032" title="Download PostScript">ps</a>, <a href="/format/2402.12032" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flexible Robust Optimal Bidding of Renewable Virtual Power Plants in  Sequential Markets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Nemati%2C+H">Hadi Nemati</a>, 
<a href="/search/eess?searchtype=author&query=S%C3%A1nchez-Mart%C3%ADn%2C+P">Pedro S&#xe1;nchez-Mart&#xed;n</a>, 
<a href="/search/eess?searchtype=author&query=Ortega%2C+%C3%81">&#xc1;lvaro Ortega</a>, 
<a href="/search/eess?searchtype=author&query=Sigrist%2C+L">Lukas Sigrist</a>, 
<a href="/search/eess?searchtype=author&query=Lobato%2C+E">Enrique Lobato</a>, 
<a href="/search/eess?searchtype=author&query=Rouco%2C+L">Luis Rouco</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">In this paper, a novel approach to define the optimal bidding of
renewable-only virtual power plants (RVPPs) in the day-ahead, secondary
reserve, and intra-day markets is proposed. To this aim, a robust optimization
algorithm is developed to account for the asymmetric nature of the
uncertainties that characterize the market prices, as well as the energy
production of the RVPP stochastic sources and flexible demand consumption.
Simulation results show increased RVPP benefits compared to other existing
solutions and demonstrate the potential of renewable sources to further
increase their economic competitiveness. The simplicity of the implementation,
the computational efficiency, and the flexible robustness are also verified.
</p>
</div>
</dd>
<dt><a name="item584">[584]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12035" title="Abstract">arXiv:2402.12035</a> [<a href="/pdf/2402.12035" title="Download PDF">pdf</a>, <a href="/format/2402.12035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Class-incremental Learning for Time Series: Benchmark and Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Z">Zhongzheng Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+Q">Quang Pham</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Z">Zhen Cao</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+H+H">Hoang H Le</a>, 
<a href="/search/cs?searchtype=author&query=Suganthan%2C+P+N">P.N.Suganthan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xudong Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Savitha%2C+R">Ramasamy Savitha</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Currently under review for KDD 2024 (ADS track)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Real-world environments are inherently non-stationary, frequently introducing
new classes over time. This is especially common in time series classification,
such as the emergence of new disease classification in healthcare or the
addition of new activities in human activity recognition. In such cases, a
learning system is required to assimilate novel classes effectively while
avoiding catastrophic forgetting of the old ones, which gives rise to the
Class-incremental Learning (CIL) problem. However, despite the encouraging
progress in the image and language domains, CIL for time series data remains
relatively understudied. Existing studies suffer from inconsistent experimental
designs, necessitating a comprehensive evaluation and benchmarking of methods
across a wide range of datasets. To this end, we first present an overview of
the Time Series Class-incremental Learning (TSCIL) problem, highlight its
unique challenges, and cover the advanced methodologies. Further, based on
standardized settings, we develop a unified experimental framework that
supports the rapid development of new algorithms, easy integration of new
datasets, and standardization of the evaluation process. Using this framework,
we conduct a comprehensive evaluation of various generic and
time-series-specific CIL methods in both standard and privacy-sensitive
scenarios. Our extensive experiments not only provide a standard baseline to
support future research but also shed light on the impact of various design
factors such as normalization layers or memory budget thresholds. Codes are
available at https://github.com/zqiao11/TSCIL.
</p>
</div>
</dd>
<dt><a name="item585">[585]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12036" title="Abstract">arXiv:2402.12036</a> [<a href="/pdf/2402.12036" title="Download PDF">pdf</a>, <a href="/format/2402.12036" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Model Adaptation to Specialized Domains through Selective  Masking based on Genre and Topical Characteristics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Belfathi%2C+A">Anas Belfathi</a>, 
<a href="/search/cs?searchtype=author&query=Gallina%2C+Y">Ygor Gallina</a>, 
<a href="/search/cs?searchtype=author&query=Hernandez%2C+N">Nicolas Hernandez</a>, 
<a href="/search/cs?searchtype=author&query=Dufour%2C+R">Richard Dufour</a>, 
<a href="/search/cs?searchtype=author&query=Monceaux%2C+L">Laura Monceaux</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Recent advances in pre-trained language modeling have facilitated significant
progress across various natural language processing (NLP) tasks. Word masking
during model training constitutes a pivotal component of language modeling in
architectures like BERT. However, the prevalent method of word masking relies
on random selection, potentially disregarding domain-specific linguistic
attributes. In this article, we introduce an innovative masking approach
leveraging genre and topicality information to tailor language models to
specialized domains. Our method incorporates a ranking process that prioritizes
words based on their significance, subsequently guiding the masking procedure.
Experiments conducted using continual pre-training within the legal domain have
underscored the efficacy of our approach on the LegalGLUE benchmark in the
English language. Pre-trained language models and code are freely available for
use.
</p>
</div>
</dd>
<dt><a name="item586">[586]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12038" title="Abstract">arXiv:2402.12038</a> [<a href="/pdf/2402.12038" title="Download PDF">pdf</a>, <a href="/format/2402.12038" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-AMPLIFY: Improving Small Language Models with Self Post Hoc  Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhan%2C+M">Milan Bhan</a>, 
<a href="/search/cs?searchtype=author&query=Vittaut%2C+J">Jean-Noel Vittaut</a>, 
<a href="/search/cs?searchtype=author&query=Chesneau%2C+N">Nicolas Chesneau</a>, 
<a href="/search/cs?searchtype=author&query=Lesot%2C+M">Marie-Jeanne Lesot</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Incorporating natural language rationales in the prompt and In-Context
Learning (ICL) has led to a significant improvement of Large Language Models
(LLMs) performance. However, rationales currently require human-annotation or
the use of auxiliary proxy models to target promising samples or generate
high-quality rationales. In this work, we propose Self-AMPLIFY to generate
automatically rationales from post hoc explanation methods applied to Small
Language Models (SLMs) to improve their own performance. Self-AMPLIFY is a
3-step method that targets samples, generates rationales and builds a final
prompt to leverage ICL. Self-AMPLIFY performance is evaluated on two SLMs and
two datasets requiring reasoning abilities: these experiments show that
Self-AMPLIFY achieves good results against competitors. Self-AMPLIFY is the
first method to apply post hoc explanation methods to SLM to generate
rationales to improve their own performance in a fully automated manner.
</p>
</div>
</dd>
<dt><a name="item587">[587]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12040" title="Abstract">arXiv:2402.12040</a> [<a href="/pdf/2402.12040" title="Download PDF">pdf</a>, <a href="/ps/2402.12040" title="Download PostScript">ps</a>, <a href="/format/2402.12040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Attack Tree Generation via Process Mining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Konsta%2C+A">Alyzia-Maria Konsta</a>, 
<a href="/search/cs?searchtype=author&query=Di+Federico%2C+G">Gemma Di Federico</a>, 
<a href="/search/cs?searchtype=author&query=Lafuente%2C+A+L">Alberto Lluch Lafuente</a>, 
<a href="/search/cs?searchtype=author&query=Burattin%2C+A">Andrea Burattin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">Attack Trees are a graphical model of security used to study threat
scenarios. While visually appealing and supported by solid theories and
effective tools, one of their main drawbacks remains the amount of effort
required by security experts to design them from scratch. This work aims to
remedy this by providing a method for the automatic generation of Attack Trees
from attack logs. The main original feature of our approach w.r.t existing ones
is the use of Process Mining algorithms to synthesize Attack Trees, which allow
users to customize the way a set of logs are summarized as an Attack Tree, for
example by discarding statistically irrelevant events. Our approach is
supported by a prototype that, apart from the derivation and translation of the
model, provides the user with an Attack Tree in the RisQFLan format, a tool
used for quantitative risk modeling and analysis with Attack Trees. We
illustrate our approach with the case study of attacks on a communication
protocol, produced by a state-of-the-art protocol analyzer.
</p>
</div>
</dd>
<dt><a name="item588">[588]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12041" title="Abstract">arXiv:2402.12041</a> [<a href="/pdf/2402.12041" title="Download PDF">pdf</a>, <a href="/format/2402.12041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Surround-View Fisheye Optics in Computer Vision and Simulation: Survey  and Challenge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jakab%2C+D">Daniel Jakab</a>, 
<a href="/search/cs?searchtype=author&query=Deegan%2C+B+M">Brian Michael Deegan</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+S">Sushil Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Grua%2C+E+M">Eoin Martino Grua</a>, 
<a href="/search/cs?searchtype=author&query=Horgan%2C+J">Jonathan Horgan</a>, 
<a href="/search/cs?searchtype=author&query=Ward%2C+E">Enda Ward</a>, 
<a href="/search/cs?searchtype=author&query=Van+De+Ven%2C+P">Pepijn Van De Ven</a>, 
<a href="/search/cs?searchtype=author&query=Scanlan%2C+A">Anthony Scanlan</a>, 
<a href="/search/cs?searchtype=author&query=Eising%2C+C">Ciaran Eising</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 19 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
<p class="mathjax">In this paper, we provide a survey on automotive surround-view fisheye
optics, with an emphasis on the impact of optical artifacts on computer vision
tasks in autonomous driving and ADAS. The automotive industry has advanced in
applying state-of-the-art computer vision to enhance road safety and provide
automated driving functionality. When using camera systems on vehicles, there
is a particular need for a wide field of view to capture the entire vehicle's
surroundings, in areas such as low-speed maneuvering, automated parking, and
cocoon sensing. However, one crucial challenge in surround-view cameras is the
strong optical aberrations of the fisheye camera, which is an area that has
received little attention in the literature. Additionally, a comprehensive
dataset is needed for testing safety-critical scenarios in vehicle automation.
The industry has turned to simulation as a cost-effective strategy for creating
synthetic datasets with surround-view camera imagery. We examine different
simulation methods (such as model-driven and data-driven simulations) and
discuss the simulators' ability (or lack thereof) to model real-world optical
performance. Overall, this paper highlights the optical aberrations in
automotive fisheye datasets, and the limitations of optical reality in
simulated fisheye datasets, with a focus on computer vision in surround-view
optical systems.
</p>
</div>
</dd>
<dt><a name="item589">[589]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12042" title="Abstract">arXiv:2402.12042</a> [<a href="/pdf/2402.12042" title="Download PDF">pdf</a>, <a href="/format/2402.12042" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Linear bandits with polylogarithmic minimax regret
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lumbreras%2C+J">Josep Lumbreras</a>, 
<a href="/search/cs?searchtype=author&query=Tomamichel%2C+M">Marco Tomamichel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 39 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">We study a noise model for linear stochastic bandits for which the
subgaussian noise parameter vanishes linearly as we select actions on the unit
sphere closer and closer to the unknown vector. We introduce an algorithm for
this problem that exhibits a minimax regret scaling as $\log^3(T)$ in the time
horizon $T$, in stark contrast the square root scaling of this regret for
typical bandit algorithms. Our strategy, based on weighted least-squares
estimation, achieves the eigenvalue relation $\lambda_{\min} ( V_t ) = \Omega
(\sqrt{\lambda_{\max}(V_t ) })$ for the design matrix $V_t$ at each time step
$t$ through geometrical arguments that are independent of the noise model and
might be of independent interest. This allows us to tightly control the
expected regret in each time step to be of the order $O(\frac1{t})$, leading to
the logarithmic scaling of the cumulative regret.
</p>
</div>
</dd>
<dt><a name="item590">[590]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12043" title="Abstract">arXiv:2402.12043</a> [<a href="/pdf/2402.12043" title="Download PDF">pdf</a>, <a href="/format/2402.12043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Lightweight Parallel Framework for Blind Image Quality Assessment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Q">Qunyue Huang</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+B">Bin Fang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Existing blind image quality assessment (BIQA) methods focus on designing
complicated networks based on convolutional neural networks (CNNs) or
transformer. In addition, some BIQA methods enhance the performance of the
model in a two-stage training manner. Despite the significant advancements,
these methods remarkably raise the parameter count of the model, thus requiring
more training time and computational resources. To tackle the above issues, we
propose a lightweight parallel framework (LPF) for BIQA. First, we extract the
visual features using a pre-trained feature extraction network. Furthermore, we
construct a simple yet effective feature embedding network (FEN) to transform
the visual features, aiming to generate the latent representations that contain
salient distortion information. To improve the robustness of the latent
representations, we present two novel self-supervised subtasks, including a
sample-level category prediction task and a batch-level quality comparison
task. The sample-level category prediction task is presented to help the model
with coarse-grained distortion perception. The batch-level quality comparison
task is formulated to enhance the training data and thus improve the robustness
of the latent representations. Finally, the latent representations are fed into
a distortion-aware quality regression network (DaQRN), which simulates the
human vision system (HVS) and thus generates accurate quality scores.
Experimental results on multiple benchmark datasets demonstrate that the
proposed method achieves superior performance over state-of-the-art approaches.
Moreover, extensive analyses prove that the proposed method has lower
computational complexity and faster convergence speed.
</p>
</div>
</dd>
<dt><a name="item591">[591]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12046" title="Abstract">arXiv:2402.12046</a> [<a href="/pdf/2402.12046" title="Download PDF">pdf</a>, <a href="/format/2402.12046" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Citation Amnesia: NLP and Other Academic Fields Are in a Citation Age  Recession
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wahle%2C+J+P">Jan Philip Wahle</a>, 
<a href="/search/cs?searchtype=author&query=Ruas%2C+T">Terry Ruas</a>, 
<a href="/search/cs?searchtype=author&query=Abdalla%2C+M">Mohamed Abdalla</a>, 
<a href="/search/cs?searchtype=author&query=Gipp%2C+B">Bela Gipp</a>, 
<a href="/search/cs?searchtype=author&query=Mohammad%2C+S+M">Saif M. Mohammad</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Digital Libraries (cs.DL)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">This study examines the tendency to cite older work across 20 fields of study
over 43 years (1980--2023). We put NLP's propensity to cite older work in the
context of these 20 other fields to analyze whether NLP shows similar temporal
citation patterns to these other fields over time or whether differences can be
observed. Our analysis, based on a dataset of approximately 240 million papers,
reveals a broader scientific trend: many fields have markedly declined in
citing older works (e.g., psychology, computer science). We term this decline a
'citation age recession', analogous to how economists define periods of reduced
economic activity. The trend is strongest in NLP and ML research (-12.8% and
-5.5% in citation age from previous peaks). Our results suggest that citing
more recent works is not directly driven by the growth in publication rates
(-3.4% across fields; -5.2% in humanities; -5.5% in formal sciences) -- even
when controlling for an increase in the volume of papers. Our findings raise
questions about the scientific community's engagement with past literature,
particularly for NLP, and the potential consequences of neglecting older but
relevant research. The data and a demo showcasing our results are publicly
available.
</p>
</div>
</dd>
<dt><a name="item592">[592]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12048" title="Abstract">arXiv:2402.12048</a> [<a href="/pdf/2402.12048" title="Download PDF">pdf</a>, <a href="/format/2402.12048" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+D">Didi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhongyi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zexi Li</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+T">Tao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+K">Ke Yan</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+S">Shouhong Ding</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+K">Kun Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chao Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Catastrophic forgetting emerges as a critical challenge when fine-tuning
multi-modal large language models (MLLMs), where improving performance on
unseen tasks often leads to a significant performance drop on the original
tasks. This paper presents a comprehensive analysis of catastrophic forgetting
in MLLMs and introduces a post-training adjustment method called Model Tailor.
Our method primarily preserves the pre-trained parameters while replacing a
small number ($\leq$ 10\%) of fine-tuned parameters, maintaining $\sim$ 99\%
effectiveness on original tasks versus pre-training, and achieving $\sim$ 97\%
on new tasks compared to standard fine-tuning. Specifically, we derive a sparse
mask to identify the "model patch", based on a fusion strategy that integrates
salience and sensitivity analysis. Subsequently, a compensation mechanism is
introduced to "decorate the patch", enhancing the model's performance on both
target and original tasks. Additionally, our method is adaptable to multi-task
scenarios. Through extensive experiments on InstructBLIP and LLaVA-1.5 in both
image captioning and visual question answering tasks, our approach demonstrates
significant task adaptability while preserving inherent pre-trained
capabilities.
</p>
</div>
</dd>
<dt><a name="item593">[593]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12052" title="Abstract">arXiv:2402.12052</a> [<a href="/pdf/2402.12052" title="Download PDF">pdf</a>, <a href="/format/2402.12052" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When  and What to Retrieve for LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+J">Jiejun Tan</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Z">Zhicheng Dou</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yutao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+P">Peidong Guo</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+K">Kun Fang</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Ji-Rong Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The integration of large language models (LLMs) and search engines represents
a significant evolution in knowledge acquisition methodologies. However,
determining the knowledge that an LLM already possesses and the knowledge that
requires the help of a search engine remains an unresolved issue. Most existing
methods solve this problem through the results of preliminary answers or
reasoning done by the LLM itself, but this incurs excessively high
computational costs. This paper introduces a novel collaborative approach,
namely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model,
to enhance the LLM's knowledge acquisition process. We employ a proxy model
which has far fewer parameters, and take its answers as heuristic answers.
Heuristic answers are then utilized to predict the knowledge required to answer
the user question, as well as the known and unknown knowledge within the LLM.
We only conduct retrieval for the missing knowledge in questions that the LLM
does not know. Extensive experimental results on five datasets with two LLMs
demonstrate a notable improvement in the end-to-end performance of LLMs in
question-answering tasks, achieving or surpassing current state-of-the-art
models with lower LLM inference costs.
</p>
</div>
</dd>
<dt><a name="item594">[594]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12055" title="Abstract">arXiv:2402.12055</a> [<a href="/pdf/2402.12055" title="Download PDF">pdf</a>, <a href="/format/2402.12055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Are LLM-based Evaluators Confusing NLG Quality Criteria?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xinyu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+M">Mingqi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Sen Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yicheng Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Teng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+X">Xiaojun Wan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Some prior work has shown that LLMs perform well in NLG evaluation for
different tasks. However, we discover that LLMs seem to confuse different
evaluation criteria, which reduces their reliability. For further verification,
we first consider avoiding issues of inconsistent conceptualization and vague
expression in existing NLG quality criteria themselves. So we summarize a clear
hierarchical classification system for 11 common aspects with corresponding
different criteria from previous studies involved. Inspired by behavioral
testing, we elaborately design 18 types of aspect-targeted perturbation attacks
for fine-grained analysis of the evaluation behaviors of different LLMs. We
also conduct human annotations beyond the guidance of the classification system
to validate the impact of the perturbations. Our experimental results reveal
confusion issues inherent in LLMs, as well as other noteworthy phenomena, and
necessitate further research and improvements for LLM-based evaluation.
</p>
</div>
</dd>
<dt><a name="item595">[595]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12058" title="Abstract">arXiv:2402.12058</a> [<a href="/pdf/2402.12058" title="Download PDF">pdf</a>, <a href="/format/2402.12058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scaffolding Coordinates to Promote Vision-Language Coordination in Large  Multi-Modal Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+X">Xuanyu Lei</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zonghan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xinrui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peng Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">State-of-the-art Large Multi-Modal Models (LMMs) have demonstrated
exceptional capabilities in vision-language tasks. Despite their advanced
functionalities, the performances of LMMs are still limited in challenging
scenarios that require complex reasoning with multiple levels of visual
information. Existing prompting techniques for LMMs focus on either improving
textual reasoning or leveraging tools for image preprocessing, lacking a simple
and general visual prompting scheme to promote vision-language coordination in
LMMs. In this work, we propose Scaffold prompting that scaffolds coordinates to
promote vision-language coordination. Specifically, Scaffold overlays a dot
matrix within the image as visual information anchors and leverages
multi-dimensional coordinates as textual positional references. Extensive
experiments on a wide range of challenging vision-language tasks demonstrate
the superiority of Scaffold over GPT-4V with the textual CoT prompting. Our
code is released in https://github.com/leixy20/Scaffold.
</p>
</div>
</dd>
<dt><a name="item596">[596]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12059" title="Abstract">arXiv:2402.12059</a> [<a href="/pdf/2402.12059" title="Download PDF">pdf</a>, <a href="/ps/2402.12059" title="Download PostScript">ps</a>, <a href="/format/2402.12059" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flipped structured matrix-sequences in image deblurring with general  boundary conditions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ferrari%2C+P">Paola Ferrari</a>, 
<a href="/search/math?searchtype=author&query=Furci%2C+I">Isabella Furci</a>, 
<a href="/search/math?searchtype=author&query=Serra-Capizzano%2C+S">Stefano Serra-Capizzano</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Motivated by a recent work on a preconditioned MINRES for flipped linear
systems in imaging, in this note we extend the scope of that research for
including more precise boundary conditions such as reflective and
anti-reflective ones. We prove spectral results for the matrix-sequences
associated to the original problem, which justify the use of the MINRES in the
current setting. The theoretical spectral analysis is supported by a wide
variety of numerical experiments, concerning the visualization of the spectra
of the original matrices in various ways. We also report numerical tests
regarding the convergence speed and regularization features of the associated
GMRES and MINRES methods. Conclusions and open problems end the present study.
</p>
</div>
</dd>
<dt><a name="item597">[597]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12060" title="Abstract">arXiv:2402.12060</a> [<a href="/pdf/2402.12060" title="Download PDF">pdf</a>, <a href="/format/2402.12060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design and evaluation of a multi-finger skin-stretch tactile interface  for hand rehabilitation robots
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ratschat%2C+A+L">Alexandre L. Ratschat</a> (1, 2), 
<a href="/search/cs?searchtype=author&query=Mart%C3%ADn-Rodr%C3%ADguez%2C+R">Rub&#xe9;n Mart&#xed;n-Rodr&#xed;guez</a> (1), 
<a href="/search/cs?searchtype=author&query=Vardar%2C+Y">Yasemin Vardar</a> (1), 
<a href="/search/cs?searchtype=author&query=Ribbers%2C+G+M">Gerard M. Ribbers</a> (2), 
<a href="/search/cs?searchtype=author&query=Marchal-Crespo%2C+L">Laura Marchal-Crespo</a> (1, 2, 3) ((1) Delft University of Technology, Delft, The Netherlands, (2) Erasmus MC, Rotterdam, The Netherlands, (3) University of Bern, Bern, Switzerland)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 3 figures, submitted to BioRob 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Object properties perceived through the tactile sense, such as weight,
friction, and slip, greatly influence motor control during manipulation tasks.
However, the provision of tactile information during robotic training in
neurorehabilitation has not been well explored. Therefore, we designed and
evaluated a tactile interface based on a two-degrees-of-freedom moving platform
mounted on a hand rehabilitation robot that provides skin stretch at four
fingertips, from the index through the little finger. To accurately control the
rendered forces, we included a custom magnetic-based force sensor to control
the tactile interface in a closed loop. The technical evaluation showed that
our custom force sensor achieved measurable shear forces of +-8N with
accuracies of 95.2-98.4% influenced by hysteresis, viscoelastic creep, and
torsional deformation. The tactile interface accurately rendered forces with a
step response steady-state accuracy of 97.5-99.4% and a frequency response in
the range of most activities of daily living. Our sensor showed the highest
measurement-range-to-size ratio and comparable accuracy to sensors of its kind.
These characteristics enabled the closed-loop force control of the tactile
interface for precise rendering of multi-finger two-dimensional skin stretch.
The proposed system is a first step towards more realistic and rich haptic
feedback during robotic sensorimotor rehabilitation, potentially improving
therapy outcomes.
</p>
</div>
</dd>
<dt><a name="item598">[598]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12061" title="Abstract">arXiv:2402.12061</a> [<a href="/pdf/2402.12061" title="Download PDF">pdf</a>, <a href="/format/2402.12061" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> All Language Models Large and Small
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhixun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yali Du</a>, 
<a href="/search/cs?searchtype=author&query=Mguni%2C+D">David Mguni</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Many leading language models (LMs) use high-intensity computational resources
both during training and execution. This poses the challenge of lowering
resource costs for deployment and faster execution of decision-making tasks
among others. We introduce a novel plug-and-play LM framework named Language
Optimising Network Distribution (LONDI) framework. LONDI learns to selectively
employ large LMs only where complex decision-making and reasoning are required
while using low-resource LMs everywhere else. LONDI consists of a system of two
(off-)policy networks, an LM, a large LM (LLM), and a reinforcement learning
module that uses switching controls to quickly learn which system states to
call the LLM. We then introduce a variant of LONDI that maintains budget
constraints on LLM calls and hence its resource usage. Theoretically, we prove
LONDI learns the subset of system states to activate the LLM required to solve
the task. We then prove that LONDI converges to optimal solutions while also
preserving budgetary constraints on LLM calls almost surely enabling it to
solve various tasks while significantly lowering computational costs. We test
LONDI's performance in a range of tasks in ScienceWorld and BabyAI-Text and
demonstrate that LONDI can solve tasks only solvable by resource-intensive LLMs
while reducing GPU usage by up to 30%.
</p>
</div>
</dd>
<dt><a name="item599">[599]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12062" title="Abstract">arXiv:2402.12062</a> [<a href="/pdf/2402.12062" title="Download PDF">pdf</a>, <a href="/ps/2402.12062" title="Download PostScript">ps</a>, <a href="/format/2402.12062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Causal Equal Protection as Algorithmic Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Di+Bello%2C+M">Marcello Di Bello</a>, 
<a href="/search/cs?searchtype=author&query=Cangiotti%2C+N">Nicol&#xf2; Cangiotti</a>, 
<a href="/search/cs?searchtype=author&query=Loi%2C+M">Michele Loi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI); Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)

</div>
<p class="mathjax">Over the last ten years the literature in computer science and philosophy has
formulated different criteria of algorithmic fairness. One of the most
discussed, classification parity, requires that the erroneous classifications
of a predictive algorithm occur with equal frequency for groups picked out by
protected characteristics. Despite its intuitive appeal, classification parity
has come under attack. Multiple scenarios can be imagined in which -
intuitively - a predictive algorithm does not treat any individual unfairly,
and yet classification parity is violated. To make progress, we turn to a
related principle, equal protection, originally developed in the context of
criminal justice. Key to equal protection is equalizing the risks of erroneous
classifications (in a sense to be specified) as opposed to equalizing the rates
of erroneous classifications. We show that equal protection avoids many of the
counterexamples to classification parity, but also fails to model our moral
intuitions in a number of common scenarios, for example, when the predictor is
causally downstream relative to the protected characteristic. To address these
difficulties, we defend a novel principle, causal equal protection, that models
the fair allocation of the risks of erroneous classification through the lenses
of causality.
</p>
</div>
</dd>
<dt><a name="item600">[600]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12065" title="Abstract">arXiv:2402.12065</a> [<a href="/pdf/2402.12065" title="Download PDF">pdf</a>, <a href="/format/2402.12065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WKVQuant: Quantizing Weight and Key/Value Cache for Large Language  Models Gains More
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yue%2C+Y">Yuxuan Yue</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+Z">Zhihang Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Duanmu%2C+H">Haojie Duanmu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+S">Sifan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jianlong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Nie%2C+L">Liqiang Nie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Frist work to exclusively quantize weights and Key/Value cache for large language models
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Large Language Models (LLMs) face significant deployment challenges due to
their substantial memory requirements and the computational demands of
auto-regressive text generation process. This paper addresses these challenges
by focusing on the quantization of LLMs, a technique that reduces memory
consumption by converting model parameters and activations into low-bit
integers. We critically analyze the existing quantization approaches,
identifying their limitations in balancing the accuracy and efficiency of the
quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ
framework especially designed for quantizing weights and the key/value (KV)
cache of LLMs. Specifically, we incorporates past-only quantization to improve
the computation of attention. Additionally, we introduce two-dimensional
quantization strategy to handle the distribution of KV cache, along with a
cross-block reconstruction regularization for parameter optimization.
Experiments show that WKVQuant achieves almost comparable memory savings to
weight-activation quantization, while also approaching the performance of
weight-only quantization.
</p>
</div>
</dd>
<dt><a name="item601">[601]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12066" title="Abstract">arXiv:2402.12066</a> [<a href="/pdf/2402.12066" title="Download PDF">pdf</a>, <a href="/format/2402.12066" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Max-Min Fairness for Uplink Rate-Splitting Multiple Access with Finite  Blocklength
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiawei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yijie Mao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">In this letter, we investigate the performance of Max Minimum Fairness (MMF)
for uplink Rate-Splitting Multiple Access (RSMA) in short-packet
communications. Specifically, considering a Single-Input Single-Output (SISO)
Multiple Access Channel (MAC), we optimize the transmit power allocation
between the splitting user messages to maximize the minimum rate among users
with Finite Blocklength (FBL) constraints. To tackle this problem, we propose a
Successive Convex Approximation (SCA)-based approach. Additionally, we
introduce a low-complexity scheme to design the decoding order at the receiver.
Numerical results show that RSMA outperforms conventional transmission schemes
such as Non-orthogonal Multiple Access (NOMA) in terms of MMF.
</p>
</div>
</dd>
<dt><a name="item602">[602]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12067" title="Abstract">arXiv:2402.12067</a> [<a href="/pdf/2402.12067" title="Download PDF">pdf</a>, <a href="/format/2402.12067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretable Brain-Inspired Representations Improve RL Performance on  Visual Navigation Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lange%2C+M">Moritz Lange</a>, 
<a href="/search/cs?searchtype=author&query=Engelhardt%2C+R+C">Raphael C. Engelhardt</a>, 
<a href="/search/cs?searchtype=author&query=Konen%2C+W">Wolfgang Konen</a>, 
<a href="/search/cs?searchtype=author&query=Wiskott%2C+L">Laurenz Wiskott</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at XAI4DRL workshop at AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE); Robotics (cs.RO)

</div>
<p class="mathjax">Visual navigation requires a whole range of capabilities. A crucial one of
these is the ability of an agent to determine its own location and heading in
an environment. Prior works commonly assume this information as given, or use
methods which lack a suitable inductive bias and accumulate error over time. In
this work, we show how the method of slow feature analysis (SFA), inspired by
neuroscience research, overcomes both limitations by generating interpretable
representations of visual data that encode location and heading of an agent. We
employ SFA in a modern reinforcement learning context, analyse and compare
representations and illustrate where hierarchical SFA can outperform other
feature extractors on navigation tasks.
</p>
</div>
</dd>
<dt><a name="item603">[603]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12068" title="Abstract">arXiv:2402.12068</a> [<a href="/pdf/2402.12068" title="Download PDF">pdf</a>, <a href="/format/2402.12068" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Computation of Equilibria in Discrete First-Price Auctions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Filos-Ratsikas%2C+A">Aris Filos-Ratsikas</a>, 
<a href="/search/cs?searchtype=author&query=Giannakopoulos%2C+Y">Yiannis Giannakopoulos</a>, 
<a href="/search/cs?searchtype=author&query=Hollender%2C+A">Alexandros Hollender</a>, 
<a href="/search/cs?searchtype=author&query=Kokkalis%2C+C">Charalampos Kokkalis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 54 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">We study the computational complexity of computing Bayes-Nash equilibria in
first-price auctions with discrete value distributions and discrete bidding
space, under general subjective beliefs. It is known that such auctions do not
always have pure equilibria. In this paper we prove that the problem of
deciding their existence is NP-complete, even for approximate equilibria. On
the other hand, it can be shown that mixed equilibria are guaranteed to exist;
however, their computational complexity has not been studied before. We
establish the PPAD-completeness of computing a mixed equilibrium and we
complement this by an efficient algorithm for finding symmetric approximate
equilibria in the special case of iid priors. En route to these results, we
develop a computational equivalence framework between continuous and discrete
first-price auctions, which can be of independent interest, and which allows us
to transfer existing positive and negative results from one setting to the
other. Finally, we show that correlated equilibria of the auction can be
computed in polynomial time.
</p>
</div>
</dd>
<dt><a name="item604">[604]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12071" title="Abstract">arXiv:2402.12071</a> [<a href="/pdf/2402.12071" title="Download PDF">pdf</a>, <a href="/format/2402.12071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EmoBench: Evaluating the Emotional Intelligence of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sabour%2C+S">Sahand Sabour</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Siyang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zheyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J+M">June M. Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jinfeng Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sunaryo%2C+A+S">Alvionna S. Sunaryo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Juanzi Li</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+T+M+C">Tatia M.C. Lee</a>, 
<a href="/search/cs?searchtype=author&query=Mihalcea%2C+R">Rada Mihalcea</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+M">Minlie Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recent advances in Large Language Models (LLMs) have highlighted the need for
robust, comprehensive, and challenging benchmarks. Yet, research on evaluating
their Emotional Intelligence (EI) is considerably limited. Existing benchmarks
have two major shortcomings: first, they mainly focus on emotion recognition,
neglecting essential EI capabilities such as emotion regulation and thought
facilitation through emotion understanding; second, they are primarily
constructed from existing datasets, which include frequent patterns, explicit
information, and annotation errors, leading to unreliable evaluation. We
propose EmoBench, a benchmark that draws upon established psychological
theories and proposes a comprehensive definition for machine EI, including
Emotional Understanding and Emotional Application. EmoBench includes a set of
400 hand-crafted questions in English and Chinese, which are meticulously
designed to require thorough reasoning and understanding. Our findings reveal a
considerable gap between the EI of existing LLMs and the average human,
highlighting a promising direction for future research. Our code and data will
be publicly available from https://github.com/Sahandfer/EmoBench.
</p>
</div>
</dd>
<dt><a name="item605">[605]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12074" title="Abstract">arXiv:2402.12074</a> [<a href="/pdf/2402.12074" title="Download PDF">pdf</a>, <a href="/format/2402.12074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HIP Network: Historical Information Passing Network for Extrapolation  Reasoning on Temporal Knowledge Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Y">Yongquan He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Luchen Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Q">Qi Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chuang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 3 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IJCAI (2021) 1915-1921
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">In recent years, temporal knowledge graph (TKG) reasoning has received
significant attention. Most existing methods assume that all timestamps and
corresponding graphs are available during training, which makes it difficult to
predict future events. To address this issue, recent works learn to infer
future events based on historical information. However, these methods do not
comprehensively consider the latent patterns behind temporal changes, to pass
historical information selectively, update representations appropriately and
predict events accurately. In this paper, we propose the Historical Information
Passing (HIP) network to predict future events. HIP network passes information
from temporal, structural and repetitive perspectives, which are used to model
the temporal evolution of events, the interactions of events at the same time
step, and the known events respectively. In particular, our method considers
the updating of relation representations and adopts three scoring functions
corresponding to the above dimensions. Experimental results on five benchmark
datasets show the superiority of HIP network, and the significant improvements
on Hits@1 prove that our method can more accurately predict what is going to
happen.
</p>
</div>
</dd>
<dt><a name="item606">[606]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12076" title="Abstract">arXiv:2402.12076</a> [<a href="/pdf/2402.12076" title="Download PDF">pdf</a>, <a href="/format/2402.12076" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Periodic Implicit Representation, Design and Optimization of Porous  Structures Using Periodic B-splines
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Depeng%2C+G">Gao Depeng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Gao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Hongwei%2C+L">Lin Hongwei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">Porous structures are intricate solid materials with numerous small pores,
extensively used in fields like medicine, chemical engineering, and aerospace.
However, the design of such structures using computer-aided tools is a
time-consuming and tedious process.In this study, we propose a novel
representation method and design approach for porous units that can be
infinitely spliced to form a porous structure. We use periodic B-spline
functions to represent periodic or symmetric porous units. Starting from a
voxel representation of a porous sample, the discrete distance field is
computed. To fit the discrete distance field with a periodic B-spline, we
introduce the constrained least squares progressive-iterative approximation
algorithm, which results in an implicit porous unit. This unit can be subject
to optimization to enhance connectivity and utilized for topology optimization,
thereby improving the model's stiffness while maintaining periodicity or
symmetry. The experimental results demonstrate the potential of the designed
complex porous units in enhancing the mechanical performance of the model.
Consequently, this study has the potential to incorporate remarkable structures
derived from artificial design or nature into the design of high-performing
models, showing the promise for biomimetic applications.
</p>
</div>
</dd>
<dt><a name="item607">[607]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12077" title="Abstract">arXiv:2402.12077</a> [<a href="/pdf/2402.12077" title="Download PDF">pdf</a>, <a href="/ps/2402.12077" title="Download PostScript">ps</a>, <a href="/format/2402.12077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Single and Multi-Objective Real-Time Optimisation of an Industrial  Injection Moulding Process via a Bayesian Adaptive Design of Experiment  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Kariminejad%2C+M">Mandana Kariminejad</a>, 
<a href="/search/eess?searchtype=author&query=Tormey%2C+D">David Tormey</a>, 
<a href="/search/eess?searchtype=author&query=Ryan%2C+C">Caitr&#xed;ona Ryan</a>, 
<a href="/search/eess?searchtype=author&query=O%27Hara%2C+C">Christopher O&#x27;Hara</a>, 
<a href="/search/eess?searchtype=author&query=Weinert%2C+A">Albert Weinert</a>, 
<a href="/search/eess?searchtype=author&query=McAfee%2C+M">Marion McAfee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
<p class="mathjax">Minimising cycle time without inducing quality defects is a major challenge
in the injection moulding (IM). Design of Experiment methods (DoE) have been
widely studied for optimisation of the IM, however existing methods have
limitations, including the need for a large number of experiments and a
pre-determined search space. Bayesian adaptive design of experiment (ADoE) is
an iterative process where the results of the previous experiments are used to
make an informed selection for the next design. In this study, for the first
time, an experimental ADoE approach, based on Bayesian optimisation, was
developed in injection moulding using process and sensor data to optimise the
quality and cycle time in real-time. A novel approach for the real-time
characterisation of post-production shrinkage was introduced, utilising
in-mould sensor data on temperature differential during part cooling. This
characterisation approach was verified by post-production metrology results.
<br />A single and multi-objective optimisation of the cycle time and temperature
differential in an injection moulded component is proposed. The multi-objective
optimisation techniques, composite desirability function and Nondominated
Sorting Genetic Algorithm (NSGA-II) using Response Surface Methodology (RSM)
model, are compared with the real-time novel ADoE approach. ADoE achieved
almost a 50% reduction in the number of experiments required for the single
optimisation of temperature differential, and an almost 30% decrease for the
optimisation of temperature differential and cycle time together compared to
composite desirability function and NSGA-II. Also, the optimal settings
identified by ADoE for multiobjective optimisation were similar to the selected
Pareto optimal solution found by the NSGA-II.
</p>
</div>
</dd>
<dt><a name="item608">[608]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12078" title="Abstract">arXiv:2402.12078</a> [<a href="/pdf/2402.12078" title="Download PDF">pdf</a>, <a href="/format/2402.12078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mirroring Call-by-Need, or Values Acting Silly
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Accattoli%2C+B">Beniamino Accattoli</a>, 
<a href="/search/cs?searchtype=author&query=Lancelot%2C+A">Adrienne Lancelot</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to FSCD 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">Call-by-need evaluation for the lambda-calculus can be seen as merging the
best of call-by-name and call-by-value, namely the wise erasing behaviour of
the former and the wise duplicating behaviour of the latter. To better
understand how duplication and erasure can be combined, we design a degenerated
calculus, dubbed call-by-silly, that is symmetric to call-by-need in that it
merges the worst of call-by-name and call-by-value, namely silly duplications
by-name and silly erasures by-value.
<br />We validate the design of the call-by-silly calculus via rewriting properties
and multi types. In particular, we mirror the main theorem about call-by-need
-- that is, its operational equivalence with call-by-name -- showing that
call-by-silly and call-by-value induce the same contextual equivalence. This
fact shows the blindness with respect to efficiency of call-by-value contextual
equivalence. We also define a call-by-silly strategy and measure its length via
tight multi types. Lastly, we prove that the call-by-silly strategy computes
evaluation sequences of maximal length in the calculus.
</p>
</div>
</dd>
<dt><a name="item609">[609]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12079" title="Abstract">arXiv:2402.12079</a> [<a href="/pdf/2402.12079" title="Download PDF">pdf</a>, <a href="/format/2402.12079" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LVCHAT: Facilitating Long Video Comprehension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zeyuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=McAuley%2C+J">Julian McAuley</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zexue He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages; 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">Enabling large language models (LLMs) to read videos is vital for multimodal
LLMs. Existing works show promise on short videos whereas long video (longer
than e.g.~1 minute) comprehension remains challenging. The major problem lies
in the over-compression of videos, i.e., the encoded video representations are
not enough to represent the whole video. To address this issue, we propose Long
Video Chat (LVChat), where Frame-Scalable Encoding (FSE) is introduced to
dynamically adjust the number of embeddings in alignment with the duration of
the video to ensure long videos are not overly compressed into a few
embeddings. To deal with long videos whose length is beyond videos seen during
training, we propose Interleaved Frame Encoding (IFE), repeating positional
embedding and interleaving multiple groups of videos to enable long video
input, avoiding performance degradation due to overly long videos. Experimental
results show that LVChat significantly outperforms existing methods by up to
27\% in accuracy on long-video QA datasets and long-video captioning
benchmarks. Our code is published at https://github.com/wangyu-ustc/LVChat.
</p>
</div>
</dd>
<dt><a name="item610">[610]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12080" title="Abstract">arXiv:2402.12080</a> [<a href="/pdf/2402.12080" title="Download PDF">pdf</a>, <a href="/format/2402.12080" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can LLMs Compute with Reasons?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sandilya%2C+H">Harshit Sandilya</a>, 
<a href="/search/cs?searchtype=author&query=Raj%2C+P">Peehu Raj</a>, 
<a href="/search/cs?searchtype=author&query=Bafna%2C+J+S">Jainit Sushil Bafna</a>, 
<a href="/search/cs?searchtype=author&query=Mukhopadhyay%2C+S">Srija Mukhopadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+S">Shivansh Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+E">Ellwil Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Arastu Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Trivedi%2C+N">Neeta Trivedi</a>, 
<a href="/search/cs?searchtype=author&query=Shrivastava%2C+M">Manish Shrivastava</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+R">Rajesh Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) often struggle with complex mathematical tasks,
prone to "hallucinating" incorrect answers due to their reliance on statistical
patterns. This limitation is further amplified in average Small LangSLMs with
limited context and training data. To address this challenge, we propose an
"Inductive Learning" approach utilizing a distributed network of SLMs. This
network leverages error-based learning and hint incorporation to refine the
reasoning capabilities of SLMs. Our goal is to provide a framework that
empowers SLMs to approach the level of logic-based applications achieved by
high-parameter models, potentially benefiting any language model. Ultimately,
this novel concept paves the way for bridging the logical gap between humans
and LLMs across various fields.
</p>
</div>
</dd>
<dt><a name="item611">[611]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12086" title="Abstract">arXiv:2402.12086</a> [<a href="/pdf/2402.12086" title="Download PDF">pdf</a>, <a href="/ps/2402.12086" title="Download PostScript">ps</a>, <a href="/format/2402.12086" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Navigating simplicity and complexity of social-ecological systems  through a dialog between dynamical systems and agent-based models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Radosavljevic%2C+S">Sonja Radosavljevic</a>, 
<a href="/search/cs?searchtype=author&query=Sanga%2C+U">Udita Sanga</a>, 
<a href="/search/cs?searchtype=author&query=Schl%C3%BCter%2C+M">Maja Schl&#xfc;ter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multiagent Systems (cs.MA)</span>

</div>
<p class="mathjax">Social-ecological systems (SES) research aims to understand the nature of
social-ecological phenomena, to find effective ways to foster or manage
conditions under which desirable phenomena, such as sustainable resource use,
occur or to change conditions or reduce the negative consequences of
undesirable phenomena, such as poverty traps. Challenges such as these are
often addressed using dynamical systems models (DSM) or agent-based models
(ABM). Both modeling approaches have strengths and weaknesses. DSM are praised
for their analytical tractability and efficient exploration of asymptotic
dynamics and bifurcation, which are enabled by reduced number and heterogeneity
of system components. ABM allows representing heterogeneity, agency, learning
and interactions of diverse agents within SES, but this also comes at a price
such as inefficiency to explore asymptotic dynamics or bifurcations. In this
paper we combine DSM and ABM to leverage strengths of each modeling technique
and gain deeper insights into dynamics of a system. We start with an ABM and
research questions that the ABM was not able to answer. Using results of the
ABM analysis as inputs for DSM, we create a DSM. Stability and bifurcation
analysis of the DSM gives partial answers to the research questions and direct
attention to where additional details are needed. This informs further ABM
analysis, prevents burdening the ABM with less important details and reveals
new insights about system dynamics. The iterative process and dialogue between
the ABM and DSM leads to more complete answers to research questions and
surpasses insights provided by each of the models separately. We illustrate the
procedure with the example of the emergence of poverty traps in an agricultural
system with endogenously driven innovation.
</p>
</div>
</dd>
<dt><a name="item612">[612]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12088" title="Abstract">arXiv:2402.12088</a> [<a href="/pdf/2402.12088" title="Download PDF">pdf</a>, <a href="/format/2402.12088" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uniqueness, stability and algorithm for an inverse wave-number-dependent  source problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zhao%2C+M">Mengjie Zhao</a>, 
<a href="/search/math?searchtype=author&query=Si%2C+S">Suliang Si</a>, 
<a href="/search/math?searchtype=author&query=Hu%2C+G">Guanghui Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 35 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
<p class="mathjax">This paper is concerned with an inverse
wave-number-dependent/frequency-dependent source problem for the Helmholtz
equation. In d-dimensions (d = 2,3), the unknown source term is supposed to be
compactly supported in spatial variables but independent on x_d. The dependance
of the source function on k is supposed to be unknown. Based on the
Dirichlet-Laplacian method and the Fourier-Transform method, we develop two
effcient non-iterative numerical algorithms to recover the
wave-number-dependent source. Uniqueness and increasing stability analysis are
proved. Numerical experiments are conducted to illustrate the effctiveness and
effciency of the proposed method.
</p>
</div>
</dd>
<dt><a name="item613">[613]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12089" title="Abstract">arXiv:2402.12089</a> [<a href="/pdf/2402.12089" title="Download PDF">pdf</a>, <a href="/format/2402.12089" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strengths and Weaknesses of the ETSI Adaptive DCC Algorithm: A Proposal  for Improvement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Soto%2C+I">Ignacio Soto</a>, 
<a href="/search/cs?searchtype=author&query=Amador%2C+O">Oscar Amador</a>, 
<a href="/search/cs?searchtype=author&query=Urue%C3%B1a%2C+M">Manuel Urue&#xf1;a</a>, 
<a href="/search/cs?searchtype=author&query=Calderon%2C+M">Maria Calderon</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Communications Letters, 23(5), pp. 802 - 805. 2019
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
<p class="mathjax">This letter studies the adaptive Decentralized Congestion Control (DCC)
algorithm defined in the ETSI TS 102 687 V1.2.1 specification. We provide
insights on the parameters used in the algorithm and explore the impact of
those parameters on its performance. We show how the algorithm achieves good
average medium utilization while protecting against congestion, but we also
show how the chosen parameters can result in slow speed of convergence and long
periods of unfairness in transitory situations. Finally, we propose a
modification to the algorithm which results in significant improvements in
speed of convergence and fairness.
</p>
</div>
</dd>
<dt><a name="item614">[614]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12091" title="Abstract">arXiv:2402.12091</a> [<a href="/pdf/2402.12091" title="Download PDF">pdf</a>, <a href="/format/2402.12091" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do Large Language Models Understand Logic or Just Mimick Context?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junbing Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Over the past few years, the abilities of large language models (LLMs) have
received extensive attention, which have performed exceptionally well in
complicated scenarios such as logical reasoning and symbolic inference. A
significant factor contributing to this progress is the benefit of in-context
learning and few-shot prompting. However, the reasons behind the success of
such models using contextual reasoning have not been fully explored. Do LLMs
have understand logical rules to draw inferences, or do they ``guess'' the
answers by learning a type of probabilistic mapping through context? This paper
investigates the reasoning capabilities of LLMs on two logical reasoning
datasets by using counterfactual methods to replace context text and modify
logical concepts. Based on our analysis, it is found that LLMs do not truly
understand logical rules; rather, in-context learning has simply enhanced the
likelihood of these models arriving at the correct answers. If one alters
certain words in the context text or changes the concepts of logical terms, the
outputs of LLMs can be significantly disrupted, leading to counter-intuitive
responses. This work provides critical insights into the limitations of LLMs,
underscoring the need for more robust mechanisms to ensure reliable logical
reasoning in LLMs.
</p>
</div>
</dd>
<dt><a name="item615">[615]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12095" title="Abstract">arXiv:2402.12095</a> [<a href="/pdf/2402.12095" title="Download PDF">pdf</a>, <a href="/format/2402.12095" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Major TOM: Expandable Datasets for Earth Observation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Francis%2C+A">Alistair Francis</a>, 
<a href="/search/cs?searchtype=author&query=Czerkawski%2C+M">Mikolaj Czerkawski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Databases (cs.DB)

</div>
<p class="mathjax">Deep learning models are increasingly data-hungry, requiring significant
resources to collect and compile the datasets needed to train them, with Earth
Observation (EO) models being no exception. However, the landscape of datasets
in EO is relatively atomised, with interoperability made difficult by diverse
formats and data structures. If ever larger datasets are to be built, and
duplication of effort minimised, then a shared framework that allows users to
combine and access multiple datasets is needed. Here, Major TOM (Terrestrial
Observation Metaset) is proposed as this extensible framework. Primarily, it
consists of a geographical indexing system based on a set of grid points and a
metadata structure that allows multiple datasets with different sources to be
merged. Besides the specification of Major TOM as a framework, this work also
presents a large, open-access dataset, MajorTOM-Core, which covers the vast
majority of the Earth's land surface. This dataset provides the community with
both an immediately useful resource, as well as acting as a template for future
additions to the Major TOM ecosystem. Access: https://huggingface.co/Major-TOM
</p>
</div>
</dd>
<dt><a name="item616">[616]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12098" title="Abstract">arXiv:2402.12098</a> [<a href="/pdf/2402.12098" title="Download PDF">pdf</a>, <a href="/format/2402.12098" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Explainable LiDAR Point Cloud Semantic Segmentation via Gradient  Based Target Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuriyal%2C+A">Abhishek Kuriyal</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+V">Vaibhav Kumar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Semantic Segmentation (SS) of LiDAR point clouds is essential for many
applications, such as urban planning and autonomous driving. While much
progress has been made in interpreting SS predictions for images, interpreting
point cloud SS predictions remains a challenge. This paper introduces pGS-CAM,
a novel gradient-based method for generating saliency maps in neural network
activation layers. Inspired by Grad-CAM, which uses gradients to highlight
local importance, pGS-CAM is robust and effective on a variety of datasets
(SemanticKITTI, Paris-Lille3D, DALES) and 3D deep learning architectures
(KPConv, RandLANet). Our experiments show that pGS-CAM effectively accentuates
the feature learning in intermediate activations of SS architectures by
highlighting the contribution of each point. This allows us to better
understand how SS models make their predictions and identify potential areas
for improvement. Relevant codes are available at
https://github.com/geoai4cities/pGS-CAM.
</p>
</div>
</dd>
<dt><a name="item617">[617]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12099" title="Abstract">arXiv:2402.12099</a> [<a href="/pdf/2402.12099" title="Download PDF">pdf</a>, <a href="/format/2402.12099" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human Video Translation via Query Warping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Haiming Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yangyang Xu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shengfeng He</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">In this paper, we present QueryWarp, a novel framework for temporally
coherent human motion video translation. Existing diffusion-based video editing
approaches that rely solely on key and value tokens to ensure temporal
consistency, which scarifies the preservation of local and structural regions.
In contrast, we aim to consider complementary query priors by constructing the
temporal correlations among query tokens from different frames. Initially, we
extract appearance flows from source poses to capture continuous human
foreground motion. Subsequently, during the denoising process of the diffusion
model, we employ appearance flows to warp the previous frame's query token,
aligning it with the current frame's query. This query warping imposes explicit
constraints on the outputs of self-attention layers, effectively guaranteeing
temporally coherent translation. We perform experiments on various human motion
video translation tasks, and the results demonstrate that our QueryWarp
framework surpasses state-of-the-art methods both qualitatively and
quantitatively.
</p>
</div>
</dd>
<dt><a name="item618">[618]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12100" title="Abstract">arXiv:2402.12100</a> [<a href="/pdf/2402.12100" title="Download PDF">pdf</a>, <a href="/format/2402.12100" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Groot: Adversarial Testing for Generative Text-to-Image Models with  Tree-based Semantic Transformation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+G">Guowei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+G">Gelei Deng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Feiyue Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yuqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+L">Ling Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tianwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Software Engineering (cs.SE)

</div>
<p class="mathjax">With the prevalence of text-to-image generative models, their safety becomes
a critical concern. adversarial testing techniques have been developed to probe
whether such models can be prompted to produce Not-Safe-For-Work (NSFW)
content. However, existing solutions face several challenges, including low
success rate and inefficiency. We introduce Groot, the first automated
framework leveraging tree-based semantic transformation for adversarial testing
of text-to-image models. Groot employs semantic decomposition and sensitive
element drowning strategies in conjunction with LLMs to systematically refine
adversarial prompts. Our comprehensive evaluation confirms the efficacy of
Groot, which not only exceeds the performance of current state-of-the-art
approaches but also achieves a remarkable success rate (93.66%) on leading
text-to-image models such as DALL-E 3 and Midjourney.
</p>
</div>
</dd>
<dt><a name="item619">[619]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12101" title="Abstract">arXiv:2402.12101</a> [<a href="/pdf/2402.12101" title="Download PDF">pdf</a>, <a href="/format/2402.12101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Design and Performance of Enhanced Spread Spectrum Aloha for Unsourced  Multiple Access
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schiavone%2C+R">Riccardo Schiavone</a>, 
<a href="/search/cs?searchtype=author&query=Liva%2C+G">Gianluigi Liva</a>, 
<a href="/search/cs?searchtype=author&query=Garello%2C+R">Roberto Garello</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print version. Submitted to IEEE Communications Letter, currently under review process
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
<p class="mathjax">We analyze the performance of enhanced spread spectrum Aloha (E-SSA) in the
framework of unsourced multiple access (UMAC). The asynchronous, unframed
transmission of E-SSA is modified to enable a direct comparison with framed
UMAC schemes, as well as with the Polyanskiy's achievability bound. The design
of E-SSA is tailored to the peculiarities of the UMAC setting, resorting to
short polar codes and the use of a timing channel to improve the energy
efficiency of the protocol. We assess the impact of the preamble length and of
the spreading factor on the system efficiency. The resulting scheme exhibits
simplicity at the transmitter and linear complexity with respect to the number
of active users at the receiver, approaching the UMAC achievability bound in
close competition with the best known UMAC schemes.
</p>
</div>
</dd>
<dt><a name="item620">[620]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12102" title="Abstract">arXiv:2402.12102</a> [<a href="/pdf/2402.12102" title="Download PDF">pdf</a>, <a href="/format/2402.12102" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is It a Free Lunch for Removing Outliers during Pretraining?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liao%2C+B">Baohao Liao</a>, 
<a href="/search/cs?searchtype=author&query=Monz%2C+C">Christof Monz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 3 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">With the growing size of large language models, the role of quantization
becomes increasingly significant. However, outliers present in weights or
activations notably influence the performance of quantized models. Recently,
\citet{qtransformer} introduced a novel softmax function aimed at pretraining
models in an outlier-free manner, thereby enhancing their suitability for
quantization. Interestingly, we observed that such an approach leads to
performance degradation in full precision. Building on this insight, we enhance
the method by ensuring its normalization is invariant to sequence length, a
crucial factor for bridging the gap between pretraining and fine-tuning.
Moreover, this improved method also facilitates successful pretraining of
causal language models.
</p>
</div>
</dd>
<dt><a name="item621">[621]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12108" title="Abstract">arXiv:2402.12108</a> [<a href="/pdf/2402.12108" title="Download PDF">pdf</a>, <a href="/ps/2402.12108" title="Download PostScript">ps</a>, <a href="/format/2402.12108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weak-Linear Types
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gramaglia%2C+H">Hector Gramaglia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>

</div>
<p class="mathjax">Computational interpretations of linear logic allow static control of memory
resources: the data produced by the program are endowed through its type with
attributes that determine its life cycle, and guarantee safe deallocation. The
use of linear types encounters limitations in practice, since linear data, in
the traditional sense, do not so often appear in actual programs. Several
alternatives have been proposed in the attempt to relax the condition of
linearity, adding coercions to the language to allow linear objects to be
temporarily aliased. In this work we propose a new alternative, whose virtue is
to preserve the simplicity and elegance of the original system.
</p>
</div>
</dd>
<dt><a name="item622">[622]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12109" title="Abstract">arXiv:2402.12109</a> [<a href="/pdf/2402.12109" title="Download PDF">pdf</a>, <a href="/format/2402.12109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Persistent Homology-Driven Optimization of Effective Relative Density  Range for Triply Periodic Minimal Surface
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Depeng%2C+G">Gao Depeng</a>, 
<a href="/search/cs?searchtype=author&query=Yuanzhi%2C+Z">Zhang Yuanzhi</a>, 
<a href="/search/cs?searchtype=author&query=Hongwei%2C+L">Lin Hongwei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 33 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Graphics (cs.GR)</span>

</div>
<p class="mathjax">Triply periodic minimal surfaces (TPMSs) play a vital role in the design of
porous structures, with applications in bone tissue engineering, chemical
engineering, and the creation of lightweight models. However, fabrication of
TPMSs via additive manufacturing is feasible only within a specific range of
relative densities, termed the effective relative density range (EDR), outside
of which TPMSs exhibit unmanufacturable features. In this study, the persistent
homology is applied to theoretically calculate and extend the EDRs of TPMSs.
The TPMSs with extended EDRs are referred to as extended TPMSs. To achieve
this, TPMSs are converted into implicit B-spline representation through
fitting. By analyzing the symmetry of TPMSs, a partial fitting method is
utilized to preserve the symmetry and enhance fitting precision. A topological
objective function is modeled based on the understanding of topological
features, resulting in extended TPMSs that possess extended EDRs while
maintaining a high degree of similarity to the original TPMSs. Experimental
validation confirms the effectiveness of the approach in extending the EDRs of
TPMSs. Furthermore, the extended TPMSs demonstrate superior performance in
porous model design and topology optimization compared to their original
counterparts. The extended TPMSs with increased EDRs hold promise for replacing
traditional TPMSs in applications that require porous structures with varying
densities.
</p>
</div>
</dd>
<dt><a name="item623">[623]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12110" title="Abstract">arXiv:2402.12110</a> [<a href="/pdf/2402.12110" title="Download PDF">pdf</a>, <a href="/format/2402.12110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Complexity of Geodesic Spanners using Steiner Points
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=de+Berg%2C+S">Sarita de Berg</a>, 
<a href="/search/cs?searchtype=author&query=Ophelders%2C+T">Tim Ophelders</a>, 
<a href="/search/cs?searchtype=author&query=Parada%2C+I">Irene Parada</a>, 
<a href="/search/cs?searchtype=author&query=Staals%2C+F">Frank Staals</a>, 
<a href="/search/cs?searchtype=author&query=Wulms%2C+J">Jules Wulms</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">A geometric $t$-spanner $\mathcal{G}$ on a set $S$ of $n$ point sites in a
metric space $P$ is a subgraph of the complete graph on $S$ such that for every
pair of sites $p,q$ the distance in $\mathcal{G}$ is a most $t$ times the
distance $d(p,q)$ in $P$. We call a connection between two sites in the spanner
a link. In some settings, such as when $P$ is a simple polygon with $m$
vertices and a link is a shortest path in $P$, links can consist of $\Theta
(m)$ segments and thus have non-constant complexity. The total spanner
complexity is a recently-introduced measure of how compact a spanner is. In
this paper, we study what happens if we are allowed to introduce $k$ Steiner
points to reduce the spanner complexity. We study such Steiner spanners in
simple polygons, polygonal domains, and edge-weighted trees.
<br />Surprisingly, we show that Steiner points have only limited utility. For a
spanner that uses $k$ Steiner points, we provide an $\Omega(nm/k)$ lower bound
on the worst-case complexity of any $(3-\varepsilon)$-spanner, and an
$\Omega(mn^{1/(t+1)}/k^{1/(t+1)})$ lower bound on the worst-case complexity of
any $(t-\varepsilon)$-spanner, for any constant $\varepsilon\in (0,1)$ and
integer constant $t \geq 2$. These lower bounds hold in all settings.
Additionally, we show NP-hardness for the problem of deciding whether a set of
sites in a polygonal domain admits a $3$-spanner with a given maximum
complexity using $k$ Steiner points.
<br />On the positive side, for trees we show how to build a $2t$-spanner that uses
$k$ Steiner points and of complexity $O(mn^{1/t}/k^{1/t} + n \log (n/k))$, for
any integer $t \geq 1$. We generalize this result to forests, and apply it to
obtain a $2\sqrt{2}t$-spanner in a simple polygon or a $6t$-spanner in a
polygonal domain, with total complexity $O(mn^{1/t}(\log k)^{1+1/t}/k^{1/t} +
n\log^2 n)$.
</p>
</div>
</dd>
<dt><a name="item624">[624]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12111" title="Abstract">arXiv:2402.12111</a> [<a href="/pdf/2402.12111" title="Download PDF">pdf</a>, <a href="/format/2402.12111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Versal AI Engines for option price discovery in market risk  analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Klaisoongnoen%2C+M">Mark Klaisoongnoen</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+N">Nick Brown</a>, 
<a href="/search/cs?searchtype=author&query=Dykes%2C+T">Tim Dykes</a>, 
<a href="/search/cs?searchtype=author&query=Jones%2C+J+R">Jessica R. Jones</a>, 
<a href="/search/cs?searchtype=author&query=Haus%2C+U">Utz-Uwe Haus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Author accepted version of paper accepted to the 32nd ACM/SIGDA International Symposium on Field-Programmable Gate Arrays
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">Whilst Field-Programmable Gate Arrays (FPGAs) have been popular in
accelerating high-frequency financial workload for many years, their
application in quantitative finance, the utilisation of mathematical models to
analyse financial markets and securities, is less mature. Nevertheless, recent
work has demonstrated the benefits that FPGAs can deliver to quantitative
workloads, and in this paper, we study whether the Versal ACAP and its AI
Engines (AIEs) can also deliver improved performance. We focus specifically on
the industry standard Strategic Technology Analysis Center's (STAC) derivatives
risk analysis benchmark STAC-A2. Porting a purely FPGA-based accelerator
STAC-A2 inspired market risk (SIMR) benchmark to the Versal ACAP device by
combining Programmable Logic (PL) and AIEs, we explore the development approach
and techniques, before comparing performance across PL and AIEs. Ultimately, we
found that our AIE approach is slower than a highly optimised existing PL-only
version due to limits on both the AIE and PL that we explore and describe.
</p>
</div>
</dd>
<dt><a name="item625">[625]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12118" title="Abstract">arXiv:2402.12118</a> [<a href="/pdf/2402.12118" title="Download PDF">pdf</a>, <a href="/format/2402.12118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DualView: Data Attribution from the Dual Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yolcu%2C+G+%C3%9C">Galip &#xdc;mit Yolcu</a>, 
<a href="/search/cs?searchtype=author&query=Wiegand%2C+T">Thomas Wiegand</a>, 
<a href="/search/cs?searchtype=author&query=Samek%2C+W">Wojciech Samek</a>, 
<a href="/search/cs?searchtype=author&query=Lapuschkin%2C+S">Sebastian Lapuschkin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Local data attribution (or influence estimation) techniques aim at estimating
the impact that individual data points seen during training have on particular
predictions of an already trained Machine Learning model during test time.
Previous methods either do not perform well consistently across different
evaluation criteria from literature, are characterized by a high computational
demand, or suffer from both. In this work we present DualView, a novel method
for post-hoc data attribution based on surrogate modelling, demonstrating both
high computational efficiency, as well as good evaluation results. With a focus
on neural networks, we evaluate our proposed technique using suitable
quantitative evaluation strategies from the literature against related
principal local data attribution methods. We find that DualView requires
considerably lower computational resources than other methods, while
demonstrating comparable performance to competing approaches across evaluation
metrics. Futhermore, our proposed method produces sparse explanations, where
sparseness can be tuned via a hyperparameter. Finally, we showcase that with
DualView, we can now render explanations from local data attributions
compatible with established local feature attribution methods: For each
prediction on (test) data points explained in terms of impactful samples from
the training set, we are able to compute and visualize how the prediction on
(test) sample relates to each influential training sample in terms of features
recognized and by the model. We provide an Open Source implementation of
DualView online, together with implementations for all other local data
attribution methods we compare against, as well as the metrics reported here,
for full reproducibility.
</p>
</div>
</dd>
<dt><a name="item626">[626]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12121" title="Abstract">arXiv:2402.12121</a> [<a href="/pdf/2402.12121" title="Download PDF">pdf</a>, <a href="/format/2402.12121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Image Review Ability of Vision Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saito%2C+S">Shigeki Saito</a>, 
<a href="/search/cs?searchtype=author&query=Hayashi%2C+K">Kazuki Hayashi</a>, 
<a href="/search/cs?searchtype=author&query=Ide%2C+Y">Yusuke Ide</a>, 
<a href="/search/cs?searchtype=author&query=Sakai%2C+Y">Yusuke Sakai</a>, 
<a href="/search/cs?searchtype=author&query=Onishi%2C+K">Kazuma Onishi</a>, 
<a href="/search/cs?searchtype=author&query=Suzuki%2C+T">Toma Suzuki</a>, 
<a href="/search/cs?searchtype=author&query=Gobara%2C+S">Seiji Gobara</a>, 
<a href="/search/cs?searchtype=author&query=Kamigaito%2C+H">Hidetaka Kamigaito</a>, 
<a href="/search/cs?searchtype=author&query=Hayashi%2C+K">Katsuhiko Hayashi</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+T">Taro Watanabe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9pages, under reviewing
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)

</div>
<p class="mathjax">Large-scale vision language models (LVLMs) are language models that are
capable of processing images and text inputs by a single model. This paper
explores the use of LVLMs to generate review texts for images. The ability of
LVLMs to review images is not fully understood, highlighting the need for a
methodical evaluation of their review abilities. Unlike image captions, review
texts can be written from various perspectives such as image composition and
exposure. This diversity of review perspectives makes it difficult to uniquely
determine a single correct review for an image. To address this challenge, we
introduce an evaluation method based on rank correlation analysis, in which
review texts are ranked by humans and LVLMs, then, measures the correlation
between these rankings. We further validate this approach by creating a
benchmark dataset aimed at assessing the image review ability of recent LVLMs.
Our experiments with the dataset reveal that LVLMs, particularly those with
proven superiority in other evaluative contexts, excel at distinguishing
between high-quality and substandard image reviews.
</p>
</div>
</dd>
<dt><a name="item627">[627]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12122" title="Abstract">arXiv:2402.12122</a> [<a href="/pdf/2402.12122" title="Download PDF">pdf</a>, <a href="/ps/2402.12122" title="Download PostScript">ps</a>, <a href="/format/2402.12122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Almost sure convergence rates of adaptive increasingly rare Markov chain  Monte Carlo
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hofstadler%2C+J">Julian Hofstadler</a>, 
<a href="/search/math?searchtype=author&query=Latuszynski%2C+K">Krzysztof Latuszynski</a>, 
<a href="/search/math?searchtype=author&query=Roberts%2C+G+O">Gareth O. Roberts</a>, 
<a href="/search/math?searchtype=author&query=Rudolf%2C+D">Daniel Rudolf</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Probability (math.PR); Statistics Theory (math.ST)

</div>
<p class="mathjax">We consider adaptive increasingly rare Markov chain Monte Carlo (AIR MCMC),
which is an adaptive MCMC method, where the adaptation concerning the past
happens less and less frequently over time. Under a contraction assumption for
a Wasserstein-like function we deduce upper bounds of the convergence rate of
Monte Carlo sums taking a renormalisation factor into account that is close to
the one that appears in a law of the iterated logarithm. We demonstrate the
applicability of our results by considering different settings, among which are
those of simultaneous geometric and uniform ergodicity. All proofs are carried
out on an augmented state space, including the classical non-augmented setting
as a special case. In contrast to other adaptive MCMC limit theory, some
technical assumptions, like diminishing adaptation, are not needed.
</p>
</div>
</dd>
<dt><a name="item628">[628]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12123" title="Abstract">arXiv:2402.12123</a> [<a href="/pdf/2402.12123" title="Download PDF">pdf</a>, <a href="/format/2402.12123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polylogarithmic Time Algorithms for Shortest Path Forests in  Programmable Matter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Padalkin%2C+A">Andreas Padalkin</a>, 
<a href="/search/cs?searchtype=author&query=Scheideler%2C+C">Christian Scheideler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">In this paper, we study the computation of shortest paths within the
\emph{geometric amoebot model}, a commonly used model for programmable matter.
Shortest paths are essential for various tasks and therefore have been heavily
investigated in many different contexts. For example, in the programmable
matter context, which is the focus of this paper, Kostitsyna et al. have
utilized shortest path trees to transform one amoebot structure into another
[DISC, 2023]. We consider the \emph{reconfigurable circuit extension} of the
model where this amoebot structure is able to interconnect amoebots by
so-called circuits. These circuits permit the instantaneous transmission of
simple signals between connected amoebots.
<br />We propose two distributed algorithms for the \emph{shortest path forest
problem} where, given a set of $k$ sources and a set of $\ell$ destinations,
the amoebot structure has to compute a forest that connects each destination to
its closest source on a shortest path. For hole-free structures, the first
algorithm constructs a shortest path tree for a single source within $O(\log
\ell)$ rounds, and the second algorithm a shortest path forest for an arbitrary
number of sources within $O(\log n \log^2 k)$ rounds. The former algorithm also
provides an $O(1)$ rounds solution for the \emph{single pair shortest path
problem} (SPSP) and an $O(\log n)$ rounds solution for the \emph{single source
shortest path problem} (SSSP) since these problems are special cases of the
considered problem.
</p>
</div>
</dd>
<dt><a name="item629">[629]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12127" title="Abstract">arXiv:2402.12127</a> [<a href="/pdf/2402.12127" title="Download PDF">pdf</a>, <a href="/format/2402.12127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rate-Splitting Multiple Access for Transmissive Reconfigurable  Intelligent Surface Transceiver Empowered ISAC System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Q">Qingqing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+J">Jinhong Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shanshan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhendong Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jun Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
<p class="mathjax">In this paper, a novel transmissive reconfigurable intelligent surface (TRIS)
transceiver empowered integrated sensing and communications (ISAC) system is
proposed for future multi-demand terminals. To address interference management,
we implement rate-splitting multiple access (RSMA), where the common stream is
independently designed for the sensing service. We introduce the sensing
quality of service (QoS) criteria based on this structure and construct an
optimization problem with the sensing QoS criteria as the objective function to
optimize the sensing stream precoding matrix and the communication stream
precoding matrix. Due to the coupling of optimization variables, the formulated
problem is a non-convex optimization problem that cannot be solved directly. To
tackle the above-mentioned challenging problem, alternating optimization (AO)
is utilized to decouple the optimization variables. Specifically, the problem
is decoupled into three subproblems about the sensing stream precoding matrix,
the communication stream precoding matrix, and the auxiliary variables, which
is solved alternatively through AO until the convergence is reached. For
solving the problem, successive convex approximation (SCA) is applied to deal
with the sum-rate threshold constraints on communications, and
difference-of-convex (DC) programming is utilized to solve rank-one non-convex
constraints. Numerical simulation results verify the superiority of the
proposed scheme in terms of improving the communication and sensing QoS.
</p>
</div>
</dd>
<dt><a name="item630">[630]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12128" title="Abstract">arXiv:2402.12128</a> [<a href="/pdf/2402.12128" title="Download PDF">pdf</a>, <a href="/format/2402.12128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D Vascular Segmentation Supervised by 2D Annotation of Maximum  Intensity Projection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhanqiang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+Z">Zimeng Tan</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+J">Jianjiang Feng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jie Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Vascular structure segmentation plays a crucial role in medical analysis and
clinical applications. The practical adoption of fully supervised segmentation
models is impeded by the intricacy and time-consuming nature of annotating
vessels in the 3D space. This has spurred the exploration of weakly-supervised
approaches that reduce reliance on expensive segmentation annotations. Despite
this, existing weakly supervised methods employed in organ segmentation, which
encompass points, bounding boxes, or graffiti, have exhibited suboptimal
performance when handling sparse vascular structure. To alleviate this issue,
we employ maximum intensity projection (MIP) to decrease the dimensionality of
3D volume to 2D image for efficient annotation, and the 2D labels are utilized
to provide guidance and oversight for training 3D vessel segmentation model.
Initially, we generate pseudo-labels for 3D blood vessels using the annotations
of 2D projections. Subsequently, taking into account the acquisition method of
the 2D labels, we introduce a weakly-supervised network that fuses 2D-3D deep
features via MIP to further improve segmentation performance. Furthermore, we
integrate confidence learning and uncertainty estimation to refine the
generated pseudo-labels, followed by fine-tuning the segmentation network. Our
method is validated on five datasets (including cerebral vessel, aorta and
coronary artery), demonstrating highly competitive performance in segmenting
vessels and the potential to significantly reduce the time and effort required
for vessel annotation. Our code is available at:
https://github.com/gzq17/Weakly-Supervised-by-MIP.
</p>
</div>
</dd>
<dt><a name="item631">[631]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12129" title="Abstract">arXiv:2402.12129</a> [<a href="/pdf/2402.12129" title="Download PDF">pdf</a>, <a href="/format/2402.12129" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modified RRT* for Path Planning in Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=T%2C+S">Sugirtha T</a>, 
<a href="/search/cs?searchtype=author&query=S%2C+P">Pranav S</a>, 
<a href="/search/cs?searchtype=author&query=Dasiah%2C+N+B">Nitin Benjamin Dasiah</a>, 
<a href="/search/cs?searchtype=author&query=M%2C+S">Sridevi M</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at International Conference on Applied Artificial Intelligence 2024 (AICNOF'24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">Essential tasks in autonomous driving includes environment perception,
detection and tracking, path planning and action control. This paper focus on
path planning, which is one of the challenging task as it needs to find optimal
path in highly complex and dynamic environments. Usually, a driving scenario
has large number of obstacles in their route. In this paper, we propose a
two-stage path planning algorithm named Angle-based Directed Rapidly exploring
Random Trees (AD-RRT*) to address the problem of optimal path in complex
environment. The proposed algorithm uses A* algorithm for global path planning
and modifies RRT* to bound the samples using angle. The efficiency of the
proposed algorithm is evaluated through experiments in different scenarios
based on the location and number of obstacles. The proposed algorithm showed
higher rate of convergence with reduced time and less number of nodes than the
base RRT* algorithm.
</p>
</div>
</dd>
<dt><a name="item632">[632]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12130" title="Abstract">arXiv:2402.12130</a> [<a href="/pdf/2402.12130" title="Download PDF">pdf</a>, <a href="/ps/2402.12130" title="Download PostScript">ps</a>, <a href="/format/2402.12130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Factor Machine: Mixed-signal Architecture for Fine-Grained Graph-Based  Computing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dudek%2C+P">Pitr Dudek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> An essay in contribution to the Festschrift for Professor Steve Furber, Manchester, 12 January 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>

</div>
<p class="mathjax">This paper proposes the design and implementation strategy of a novel
computing architecture, the Factor Machine. The work is a step towards a
general-purpose parallel system operating in a non-sequential manner,
exploiting processing/memory co-integration and replacing the traditional
Turing/von Neumann model of a computer system with a framework based on
"factorised computation". This architecture is inspired by neural information
processing principles and aims to progress the development of brain-like
machine intelligence systems, through providing a computing substrate designed
from the ground up to enable efficient implementations of algorithms based on
relational networks. The paper provides a rationale for such machine, in the
context of the history of computing, and more recent developments in
neuromorphic hardware, reviews its general features, and proposes a
mixed-signal hardware implementation, based on using analogue circuits to carry
out computation and localised and sparse communication between the compute
units.
</p>
</div>
</dd>
<dt><a name="item633">[633]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12132" title="Abstract">arXiv:2402.12132</a> [<a href="/pdf/2402.12132" title="Download PDF">pdf</a>, <a href="/format/2402.12132" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SSTKG: Simple Spatio-Temporal Knowledge Graph for Intepretable and  Versatile Dynamic Information Embedding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Ruiyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Salim%2C+F+D">Flora D. Salim</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+H">Hao Xue</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> for Web conf 2024. 8 pages context
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">Knowledge graphs (KGs) have been increasingly employed for link prediction
and recommendation using real-world datasets. However, the majority of current
methods rely on static data, neglecting the dynamic nature and the hidden
spatio-temporal attributes of real-world scenarios. This often results in
suboptimal predictions and recommendations. Although there are effective
spatio-temporal inference methods, they face challenges such as scalability
with large datasets and inadequate semantic understanding, which impede their
performance. To address these limitations, this paper introduces a novel
framework - Simple Spatio-Temporal Knowledge Graph (SSTKG), for constructing
and exploring spatio-temporal KGs. To integrate spatial and temporal data into
KGs, our framework exploited through a new 3-step embedding method. Output
embeddings can be used for future temporal sequence prediction and spatial
information recommendation, providing valuable insights for various
applications such as retail sales forecasting and traffic volume prediction.
Our framework offers a simple but comprehensive way to understand the
underlying patterns and trends in dynamic KG, thereby enhancing the accuracy of
predictions and the relevance of recommendations. This work paves the way for
more effective utilization of spatio-temporal data in KGs, with potential
impacts across a wide range of sectors.
</p>
</div>
</dd>
<dt><a name="item634">[634]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12138" title="Abstract">arXiv:2402.12138</a> [<a href="/pdf/2402.12138" title="Download PDF">pdf</a>, <a href="/format/2402.12138" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Perceiving Longer Sequences With Bi-Directional Cross-Attention  Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hiller%2C+M">Markus Hiller</a>, 
<a href="/search/cs?searchtype=author&query=Ehinger%2C+K+A">Krista A. Ehinger</a>, 
<a href="/search/cs?searchtype=author&query=Drummond%2C+T">Tom Drummond</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">We present a novel bi-directional Transformer architecture (BiXT) which
scales linearly with input size in terms of computational cost and memory
consumption, but does not suffer the drop in performance or limitation to only
one input modality seen with other efficient Transformer-based approaches. BiXT
is inspired by the Perceiver architectures but replaces iterative attention
with an efficient bi-directional cross-attention module in which input tokens
and latent variables attend to each other simultaneously, leveraging a
naturally emerging attention-symmetry between the two. This approach unlocks a
key bottleneck experienced by Perceiver-like architectures and enables the
processing and interpretation of both semantics (`what') and location (`where')
to develop alongside each other over multiple layers -- allowing its direct
application to dense and instance-based tasks alike. By combining efficiency
with the generality and performance of a full Transformer architecture, BiXT
can process longer sequences like point clouds or images at higher feature
resolutions and achieves competitive performance across a range of tasks like
point cloud part segmentation, semantic image segmentation and image
classification.
</p>
</div>
</dd>
<dt><a name="item635">[635]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12140" title="Abstract">arXiv:2402.12140</a> [<a href="/pdf/2402.12140" title="Download PDF">pdf</a>, <a href="/format/2402.12140" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Many-Stage Optimal Stabilized Runge-Kutta Methods for Hyperbolic Partial  Differential Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Doehring%2C+D">Daniel Doehring</a>, 
<a href="/search/math?searchtype=author&query=Gassner%2C+G+J">Gregor J. Gassner</a>, 
<a href="/search/math?searchtype=author&query=Torrilhon%2C+M">Manuel Torrilhon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph); Classical Analysis and ODEs (math.CA)

</div>
<p class="mathjax">A novel optimization procedure for the generation of stability polynomials of
stabilized explicit Runge-Kutta method is devised. Intended for
semidiscretizations of hyperbolic partial differential equations, the herein
developed approach allows the optimization of stability polynomials with more
than hundred stages. A potential application of these high degree stability
polynomials are problems with locally varying characteristic speeds as found in
non-uniformly refined meshes and different wave speeds.
<br />To demonstrate the applicability of the stability polynomials we construct 2N
storage many-stage Runge-Kutta methods that match their designed second order
of accuracy when applied to a range of linear and nonlinear hyperbolic PDEs
with smooth solutions. The methods are constructed to reduce the amplification
of round off errors which becomes a significant concern for these many-stage
methods.
</p>
</div>
</dd>
<dt><a name="item636">[636]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12141" title="Abstract">arXiv:2402.12141</a> [<a href="/pdf/2402.12141" title="Download PDF">pdf</a>, <a href="/format/2402.12141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fast deep learning based reconstruction for limited angle tomography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Salomonsson%2C+K">Knut Salomonsson</a>, 
<a href="/search/math?searchtype=author&query=Oldgren%2C+E">Eric Oldgren</a>, 
<a href="/search/math?searchtype=author&query=Str%C3%B6m%2C+E">Emanuel Str&#xf6;m</a>, 
<a href="/search/math?searchtype=author&query=%C3%96ktem%2C+O">Ozan &#xd6;ktem</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">A major challenge in computed tomography is reconstructing objects from
incomplete data. An increasingly popular solution for these problems is to
incorporate deep learning models into reconstruction algorithms. This study
introduces a novel approach by integrating a Fourier neural operator (FNO) into
the Filtered Backprojection (FBP) reconstruction method, yielding the FNO back
projection (FNO-BP) network. We employ moment conditions for sinogram
extrapolation to assist the model in mitigating artefacts from limited data.
Notably, our deep learning architecture maintains a runtime comparable to
classical filtered back projection (FBP) reconstructions, ensuring swift
performance during both inference and training. We assess our reconstruction
method in the context of the Helsinki Tomography Challenge 2022 and also
compare it against regular FBP methods.
</p>
</div>
</dd>
<dt><a name="item637">[637]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12142" title="Abstract">arXiv:2402.12142</a> [<a href="/pdf/2402.12142" title="Download PDF">pdf</a>, <a href="/format/2402.12142" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Bayesian Network Ensembles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Daalen%2C+F">Florian van Daalen</a>, 
<a href="/search/cs?searchtype=author&query=Ippel%2C+L">Lianne Ippel</a>, 
<a href="/search/cs?searchtype=author&query=Dekker%2C+A">Andre Dekker</a>, 
<a href="/search/cs?searchtype=author&query=Bermejo%2C+I">Inigo Bermejo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been accepted and published at FLTA 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">Federated learning allows us to run machine learning algorithms on
decentralized data when data sharing is not permitted due to privacy concerns.
Ensemble-based learning works by training multiple (weak) classifiers whose
output is aggregated. Federated ensembles are ensembles applied to a federated
setting, where each classifier in the ensemble is trained on one data location.
<br />In this article, we explore the use of federated ensembles of Bayesian
networks (FBNE) in a range of experiments and compare their performance with
locally trained models and models trained with VertiBayes, a federated learning
algorithm to train Bayesian networks from decentralized data. Our results show
that FBNE outperforms local models and provides a significant increase in
training speed compared with VertiBayes while maintaining a similar performance
in most settings, among other advantages. We show that FBNE is a potentially
useful tool within the federated learning toolbox, especially when local
populations are heavily biased, or there is a strong imbalance in population
size across parties. We discuss the advantages and disadvantages of this
approach in terms of time complexity, model accuracy, privacy protection, and
model interpretability.
</p>
</div>
</dd>
<dt><a name="item638">[638]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12144" title="Abstract">arXiv:2402.12144</a> [<a href="/pdf/2402.12144" title="Download PDF">pdf</a>, <a href="/format/2402.12144" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Connectivity Labeling in Faulty Colored Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Petruschka%2C+A">Asaf Petruschka</a>, 
<a href="/search/cs?searchtype=author&query=Sapir%2C+S">Shay Sapir</a>, 
<a href="/search/cs?searchtype=author&query=Tzalik%2C+E">Elad Tzalik</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> shortened abstract for arxiv
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
<p class="mathjax">Fault-tolerant connectivity labelings are schemes that, given an $n$-vertex
graph $G=(V,E)$ and $f\geq 1$, produce succinct yet informative labels for the
elements of the graph. Given only the labels of two vertices $u,v$ and of the
elements in a faulty-set $F$ with $|F|\leq f$, one can determine if $u,v$ are
connected in $G-F$, the surviving graph after removing $F$. For the edge or
vertex faults models, i.e., $F\subseteq E$ or $F\subseteq V$, a sequence of
recent work established schemes with $poly(f,\log n)$-bit labels. This paper
considers the color faults model, recently introduced in the context of
spanners [Petruschka, Sapir and Tzalik, ITCS'24], which accounts for known
correlations between failures. Here, the edges (or vertices) of the input $G$
are arbitrarily colored, and the faulty elements in $F$ are colors; a failing
color causes all edges (vertices) of that color to crash.
<br />Our main contribution is settling the label length complexity for
connectivity under one color fault ($f=1$). The existing implicit solution, by
applying the state-of-the-art scheme for edge faults of [Dory and Parter,
PODC'21], might yield labels of $\Omega(n)$ bits. We provide a deterministic
scheme with labels of $\tilde{O}(\sqrt{n})$ bits in the worst case, and a
matching lower bound. Moreover, our scheme is universally optimal: even schemes
tailored to handle only colorings of one specific graph topology cannot produce
asymptotically smaller labels. We extend our labeling approach to yield a
routing scheme avoiding a single forbidden color. We also consider the
centralized setting, and show an $\tilde{O}(n)$-space oracle, answering
connectivity queries under one color fault in $\tilde{O}(1)$ time. Turning to
$f\geq 2$ color faults, we give a randomized labeling scheme with
$\tilde{O}(n^{1-1/2^f})$-bit labels, along with a lower bound of
$\Omega(n^{1-1/(f+1)})$ bits.
</p>
</div>
</dd>
<dt><a name="item639">[639]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12146" title="Abstract">arXiv:2402.12146</a> [<a href="/pdf/2402.12146" title="Download PDF">pdf</a>, <a href="/format/2402.12146" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta Ranking: Less Capable Language Models are Capable for Single  Response Judgement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zijun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kou%2C+B">Boqun Kou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peng Li</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Ming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Ji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint, under review. 25 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Although Large Language Models (LLMs) have demonstrated strong performance on
a wide range of tasks, they still face reliability challenges such as
hallucination. Previous studies reveal that highly capable LLMs like GPT-4 are
effective in judging the reliability of individual responses, while less
capable ones are often tuned to evaluate the relative reliability of responses
to the same query. To enable less capable LLMs to effectively judge the
reliability of individual responses, we propose a novel method named
$\textit{Meta}$ $\textit{Ranking}$ (MR). Unlike previous methods, which assess
the response directly, we achieve the judgement by comparing the target
query-response pair with reference query-response pairs. We found its
remarkable effectiveness in error detection for LLM responses on reasoning
tasks, where less capable LLMs could outperform strong baselines, even without
fine-tuning. We further demonstrate that MR can be used to enhance the
performance of LLMs in two practical applications: query routing and iterative
training data filtering. The former achieves GPT-4-turbo comparable performance
with less than half the token consumption, while the latter makes the
instruction-tuned LLaMA-7B and Phi-2, a 2.7B model, significantly surpass
Alpaca-13B over fewer training samples, underscoring the high potential of our
proposed method.
</p>
</div>
</dd>
<dt><a name="item640">[640]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12147" title="Abstract">arXiv:2402.12147</a> [<a href="/pdf/2402.12147" title="Download PDF">pdf</a>, <a href="/format/2402.12147" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-end multilingual fact-checking at scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Setty%2C+V">Vinay Setty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In this article, we describe how you can perform end-to-end fact-checking in
over 100 languages using Factiverse AI models. We also show through an
experimental benchmark that fine-tuned models tailored for fact-checking tasks
outperform Large Language Models such as GPT-4, GPT-3.5-Turbo, and Mistral-7b.
</p>
</div>
</dd>
<dt><a name="item641">[641]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12148" title="Abstract">arXiv:2402.12148</a> [<a href="/pdf/2402.12148" title="Download PDF">pdf</a>, <a href="/format/2402.12148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Local certification of forbidden subgraphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bousquet%2C+N">Nicolas Bousquet</a>, 
<a href="/search/cs?searchtype=author&query=Cook%2C+L">Linda Cook</a>, 
<a href="/search/cs?searchtype=author&query=Feuilloley%2C+L">Laurent Feuilloley</a>, 
<a href="/search/cs?searchtype=author&query=Pierron%2C+T">Th&#xe9;o Pierron</a>, 
<a href="/search/cs?searchtype=author&query=Zeitoun%2C+S">S&#xe9;bastien Zeitoun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Detecting specific structures in a network has been a very active theme of
research in distributed computing for at least a decade. In this paper, we
start the study of subgraph detection from the perspective of local
certification. Remember that a local certification is a distributed mechanism
enabling the nodes of a network to check the correctness of the current
configuration, thanks to small pieces of information called certificates. Our
main question is: For a given graph $H$, what is the minimum certificate size
that allows checking that the network does not contain $H$ as a (possibly
induced) subgraph?
<br />We show a variety of lower and upper bounds, uncovering an interesting
interplay between the optimal certificate size, the size of the forbidden
subgraph, and the locality of the verification. Along the way we introduce
several new technical tools, in particular what we call the \emph{layered map},
which is not specific to forbidden subgraphs and that we expect to be useful
for certifying many other properties.
</p>
</div>
</dd>
<dt><a name="item642">[642]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12149" title="Abstract">arXiv:2402.12149</a> [<a href="/pdf/2402.12149" title="Download PDF">pdf</a>, <a href="/ps/2402.12149" title="Download PostScript">ps</a>, <a href="/format/2402.12149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MLFEF: Machine Learning Fusion Model with Empirical Formula to Explore  the Momentum in Competitive Sports
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+R">Ruixin Peng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Ziqing Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Tennis is so popular that coaches and players are curious about factors other
than skill, such as momentum. This article will try to define and quantify
momentum, providing a basis for real-time analysis of tennis matches. Based on
the tennis Grand Slam men's singles match data in recent years, we built two
models, one is to build a model based on data-driven, and the other is to build
a model based on empirical formulas. For the data-driven model, we first found
a large amount of public data including public data on tennis matches in the
past five years and personal information data of players. Then the data is
preprocessed, and feature engineered, and a fusion model of SVM, Random Forrest
algorithm and XGBoost was established. For the mechanism analysis model,
important features were selected based on the suggestions of many tennis
players and enthusiasts, the sliding window algorithm was used to calculate the
weight, and different methods were used to visualize the momentum. For further
analysis of the momentum fluctuation, it is based on the popular CUMSUM
algorithm in the industry as well as the RUN Test, and the result shows the
momentum is not random and the trend might be random. At last, the robustness
of the fusion model is analyzed by Monte Carlo simulation.
</p>
</div>
</dd>
<dt><a name="item643">[643]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12150" title="Abstract">arXiv:2402.12150</a> [<a href="/pdf/2402.12150" title="Download PDF">pdf</a>, <a href="/format/2402.12150" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Your Large Language Model is Secretly a Fairness Proponent and You  Should Prompt it Like One
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianlin Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xiaoyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+C">Chao Du</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+T">Tianyu Pang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Q">Qing Guo</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chao Shen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">The widespread adoption of large language models (LLMs) underscores the
urgent need to ensure their fairness. However, LLMs frequently present dominant
viewpoints while ignoring alternative perspectives from minority parties,
resulting in potential biases. We hypothesize that these fairness-violating
behaviors occur because LLMs express their viewpoints using a human personality
that represents the majority of training data. In response to this, we validate
that prompting LLMs with specific roles can allow LLMs to express diverse
viewpoints. Building on this insight and observation, we develop FairThinking,
a pipeline designed to automatically generate roles that enable LLMs to
articulate diverse perspectives for fair expressions. To evaluate FairThinking,
we create a dataset with a thousand items covering three fairness-related
topics and conduct experiments on GPT-3.5, GPT-4, Llama2, and Mistral to
demonstrate its superior performance.
</p>
</div>
</dd>
<dt><a name="item644">[644]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12151" title="Abstract">arXiv:2402.12151</a> [<a href="/pdf/2402.12151" title="Download PDF">pdf</a>, <a href="/format/2402.12151" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformer-based Causal Language Models Perform Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xinbo Wu</a>, 
<a href="/search/cs?searchtype=author&query=Varshney%2C+L+R">Lav R. Varshney</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Even though large language models (LLMs) have demonstrated remarkable
capability in solving various natural language tasks, the capability of an LLM
to follow human instructions is still a concern. Recent works have shown great
improvements in the instruction-following capability via additional training
for instruction-following tasks. However, the mechanisms responsible for
effective instruction-following capabilities remain inadequately understood.
Here, we introduce a simplified instruction-following task and use synthetic
datasets to analyze a Transformer-based causal language model. Our findings
suggest that the model learns task-specific information by clustering data
within its hidden space, with this clustering process evolving dynamically
during learning. We also demonstrate how this phenomenon assists the model in
handling unseen instances and validate our results in a more realistic setting.
</p>
</div>
</dd>
<dt><a name="item645">[645]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12152" title="Abstract">arXiv:2402.12152</a> [<a href="/pdf/2402.12152" title="Download PDF">pdf</a>, <a href="/format/2402.12152" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerated gradient flows for large bending deformations of nonlinear  plates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dong%2C+G">Guozhi Dong</a>, 
<a href="/search/math?searchtype=author&query=Guo%2C+H">Hailong Guo</a>, 
<a href="/search/math?searchtype=author&query=Yang%2C+S">Shuo Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">In this paper, we propose and analyze a series of novel algorithms based on
projection-free accelerated gradient flows to minimize bending energies for
nonlinear plates with non-convex metric constraints. We discuss the stability
and constraint consistency in a semi-discrete setting for both bilayer and
prestrained plates. Our proposed algorithms demonstrate substantial
improvements, in both efficiency and accuracy, over current state-of-the-art
methods based on gradient flows.
</p>
</div>
</dd>
<dt><a name="item646">[646]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12161" title="Abstract">arXiv:2402.12161</a> [<a href="/pdf/2402.12161" title="Download PDF">pdf</a>, <a href="/format/2402.12161" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Endowing Pre-trained Graph Models with Provable Fairness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhongjian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengmei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiawei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chuan Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Social and Information Networks (cs.SI)

</div>
<p class="mathjax">Pre-trained graph models (PGMs) aim to capture transferable inherent
structural properties and apply them to different downstream tasks. Similar to
pre-trained language models, PGMs also inherit biases from human society,
resulting in discriminatory behavior in downstream applications. The debiasing
process of existing fair methods is generally coupled with parameter
optimization of GNNs. However, different downstream tasks may be associated
with different sensitive attributes in reality, directly employing existing
methods to improve the fairness of PGMs is inflexible and inefficient.
Moreover, most of them lack a theoretical guarantee, i.e., provable lower
bounds on the fairness of model predictions, which directly provides assurance
in a practical scenario. To overcome these limitations, we propose a novel
adapter-tuning framework that endows pre-trained \textbf{Graph} models with
\textbf{P}rovable f\textbf{A}i\textbf{R}ness (called GraphPAR). GraphPAR
freezes the parameters of PGMs and trains a parameter-efficient adapter to
flexibly improve the fairness of PGMs in downstream tasks. Specifically, we
design a sensitive semantic augmenter on node representations, to extend the
node representations with different sensitive attribute semantics for each
node. The extended representations will be used to further train an adapter, to
prevent the propagation of sensitive attribute semantics from PGMs to task
predictions. Furthermore, with GraphPAR, we quantify whether the fairness of
each node is provable, i.e., predictions are always fair within a certain range
of sensitive attribute semantics. Experimental evaluations on real-world
datasets demonstrate that GraphPAR achieves state-of-the-art prediction
performance and fairness on node classification task. Furthermore, based on our
GraphPAR, around 90\% nodes have provable fairness.
</p>
</div>
</dd>
<dt><a name="item647">[647]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12162" title="Abstract">arXiv:2402.12162</a> [<a href="/pdf/2402.12162" title="Download PDF">pdf</a>, <a href="/format/2402.12162" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SCARF: Securing Chips with a Robust Framework against Fabrication-time  Hardware Trojans
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eslami%2C+M">Mohammad Eslami</a>, 
<a href="/search/cs?searchtype=author&query=Ghasempouri%2C+T">Tara Ghasempouri</a>, 
<a href="/search/cs?searchtype=author&query=Pagliarini%2C+S">Samuel Pagliarini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Hardware Architecture (cs.AR)

</div>
<p class="mathjax">The globalization of the semiconductor industry has introduced security
challenges to Integrated Circuits (ICs), particularly those related to the
threat of Hardware Trojans (HTs) - malicious logic that can be introduced
during IC fabrication. While significant efforts are directed towards verifying
the correctness and reliability of ICs, their security is often overlooked. In
this paper, we propose a comprehensive approach to enhance IC security from the
front-end to back-end stages of design. Initially, we outline a systematic
method to transform existing verification assets into potent security checkers
by repurposing verification assertions. To further improve security, we
introduce an innovative technique for integrating online monitors during
physical synthesis - a back-end insertion providing an additional layer of
defense. Experimental results demonstrate a significant increase in security,
measured by our introduced metric, Security Coverage (SC), with a marginal rise
in area and power consumption, typically under 20\%. The insertion of online
monitors during physical synthesis enhances security metrics by up to 33.5\%.
This holistic approach offers a comprehensive and resilient defense mechanism
across the entire spectrum of IC design.
</p>
</div>
</dd>
<dt><a name="item648">[648]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12164" title="Abstract">arXiv:2402.12164</a> [<a href="/pdf/2402.12164" title="Download PDF">pdf</a>, <a href="/format/2402.12164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Dynamic Weighted Approach with Fictitious Play and Pure  Counterfactual Regret Minimization for Equilibrium Finding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ju%2C+Q">Qi Ju</a>, 
<a href="/search/cs?searchtype=author&query=Hei%2C+F">Falin Hei</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+Z">Zhemei Fang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yunfeng Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">Developing efficient algorithms to converge to Nash Equilibrium is a key
focus in game theory. The use of dynamic weighting has been especially
advantageous in normal-form games, enhancing the rate of convergence. For
instance, the Greedy Regret Minimization (RM) algorithm has markedly
outperformed earlier techniques. Nonetheless, its dependency on mixed
strategies throughout the iterative process introduces complexity to dynamic
weighting, which in turn restricts its use in extensive-form games.
<br />In this study, we introduce two novel dynamic weighting algorithms: Dynamic
Weighted Fictitious Play (DW-FP) and Dynamic Weighted Pure Counterfactual
Regret Minimization (DW-PCFR). These algorithms, utilizing pure strategies in
each iteration, offer key benefits: (i) Addressing the complexity of dynamic
weight computation in Greedy RM, thereby facilitating application in
extensive-form games; (ii) Incorporating the low-memory usage and ease-of-use
features of FP and CFR; (iii) They guarantee a convergence lower bound of
$\mathcal{O}\left(T^{-\frac{1}{2}}\right)$, with a tendency to achieve a
convergence rate of $\mathcal{O}(T^{-1})$ as runtime increases. This research
not only theoretically affirms the convergence capabilities of these algorithms
but also empirically demonstrates their superiority over existing leading
algorithms across all our tests.
</p>
</div>
</dd>
<dt><a name="item649">[649]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12168" title="Abstract">arXiv:2402.12168</a> [<a href="/pdf/2402.12168" title="Download PDF">pdf</a>, <a href="/format/2402.12168" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Defending Against Weight-Poisoning Backdoor Attacks for  Parameter-Efficient Fine-Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+S">Shuai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+L">Leilei Gan</a>, 
<a href="/search/cs?searchtype=author&query=Tuan%2C+L+A">Luu Anh Tuan</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+L">Lingjuan Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+M">Meihuizi Jia</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Jinming Wen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recently, various parameter-efficient fine-tuning (PEFT) strategies for
application to language models have been proposed and successfully implemented.
However, this raises the question of whether PEFT, which only updates a limited
set of model parameters, constitutes security vulnerabilities when confronted
with weight-poisoning backdoor attacks. In this study, we show that PEFT is
more susceptible to weight-poisoning backdoor attacks compared to the
full-parameter fine-tuning method, with pre-defined triggers remaining
exploitable and pre-defined targets maintaining high confidence, even after
fine-tuning. Motivated by this insight, we developed a Poisoned Sample
Identification Module (PSIM) leveraging PEFT, which identifies poisoned samples
through confidence, providing robust defense against weight-poisoning backdoor
attacks. Specifically, we leverage PEFT to train the PSIM with randomly reset
sample labels. During the inference process, extreme confidence serves as an
indicator for poisoned samples, while others are clean. We conduct experiments
on text classification tasks, five fine-tuning strategies, and three
weight-poisoning backdoor attack methods. Experiments show near 100% success
rates for weight-poisoning backdoor attacks when utilizing PEFT. Furthermore,
our defensive approach exhibits overall competitive performance in mitigating
weight-poisoning backdoor attacks.
</p>
</div>
</dd>
<dt><a name="item650">[650]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12169" title="Abstract">arXiv:2402.12169</a> [<a href="/pdf/2402.12169" title="Download PDF">pdf</a>, <a href="/format/2402.12169" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Automating Boundary Filling in Cubical Agda
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dor%C3%A9%2C+M">Maximilian Dor&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Cavallo%2C+E">Evan Cavallo</a>, 
<a href="/search/cs?searchtype=author&query=M%C3%B6rtberg%2C+A">Anders M&#xf6;rtberg</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
<p class="mathjax">When working in a proof assistant, automation is key to discharging routine
proof goals such as equations between algebraic expressions. Homotopy Type
Theory allows the user to reason about higher structures, such as topological
spaces, using higher inductive types (HITs) and univalence. Cubical Agda is an
extension of Agda with computational support for HITs and univalence. A
difficulty when working in Cubical Agda is dealing with the complex
combinatorics of higher structures, an infinite-dimensional generalisation of
equational reasoning. To solve these higher-dimensional equations consists in
constructing cubes with specified boundaries.
<br />We develop a simplified cubical language in which we isolate and study two
automation problems: contortion solving, where we attempt to "contort" a cube
to fit a given boundary, and the more general Kan solving, where we search for
solutions that involve pasting multiple cubes together. Both problems are
difficult in the general case - Kan solving is even undecidable - so we focus
on heuristics that perform well on practical examples. We provide a solver for
the contortion problem using a reformulation of contortions in terms of poset
maps, while we solve Kan problems using constraint satisfaction programming. We
have implemented our algorithms in an experimental Haskell solver that can be
used to automatically solve goals presented by Cubical Agda. We illustrate this
with a case study establishing the Eckmann-Hilton theorem using our solver, as
well as various benchmarks - providing the ground for further study of proof
automation in cubical type theories.
</p>
</div>
</dd>
<dt><a name="item651">[651]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12170" title="Abstract">arXiv:2402.12170</a> [<a href="/pdf/2402.12170" title="Download PDF">pdf</a>, <a href="/format/2402.12170" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unsupervised LLM Adaptation for Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Saito%2C+K">Kuniaki Saito</a>, 
<a href="/search/cs?searchtype=author&query=Sohn%2C+K">Kihyuk Sohn</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+C">Chen-Yu Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ushiku%2C+Y">Yoshitaka Ushiku</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLM) learn diverse knowledge present in the
large-scale training dataset via self-supervised training. Followed by
instruction-tuning, LLM acquires the ability to return correct information for
diverse questions. However, adapting these pre-trained LLMs to new target
domains, such as different organizations or periods, for the question-answering
(QA) task incurs a substantial annotation cost. To tackle this challenge, we
propose a novel task, unsupervised LLM adaptation for question answering. In
this task, we leverage a pre-trained LLM, a publicly available QA dataset
(source data), and unlabeled documents from the target domain. Our goal is to
learn LLM that can answer questions about the target domain. We introduce one
synthetic and two real datasets to evaluate models fine-tuned on the source and
target data, and reveal intriguing insights; (i) fine-tuned models exhibit the
ability to provide correct answers for questions about the target domain even
though they do not see any questions about the information described in the
unlabeled documents, but (ii) they have difficulties in accessing information
located in the middle or at the end of documents, and (iii) this challenge can
be partially mitigated by replacing input tokens with random ones during
adaptation.
</p>
</div>
</dd>
<dt><a name="item652">[652]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12173" title="Abstract">arXiv:2402.12173</a> [<a href="/pdf/2402.12173" title="Download PDF">pdf</a>, <a href="/ps/2402.12173" title="Download PostScript">ps</a>, <a href="/format/2402.12173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Second-order flows for approaching stationary points of a class of  non-convex energies via convex-splitting schemes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+H">Haifan Chen</a>, 
<a href="/search/math?searchtype=author&query=Dong%2C+G">Guozhi Dong</a>, 
<a href="/search/math?searchtype=author&query=Iglesias%2C+J+A">Jos&#xe9; A. Iglesias</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+W">Wei Liu</a>, 
<a href="/search/math?searchtype=author&query=Xie%2C+Z">Ziqing Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">The use of accelerated gradient flows is an emerging field in optimization,
scientific computing and beyond. This paper contributes to the theoretical
underpinnings of a recently-introduced computational paradigm known as
second-order flows, which demonstrate significant performance particularly for
the minimization of non-convex energy functionals defined on Sobolev spaces,
and are characterized by novel dissipative hyperbolic partial differential
equations. Our approach hinges upon convex-splitting schemes, a tool which is
not only pivotal for clarifying the well-posedness of second-order flows, but
also yields a versatile array of robust numerical schemes through temporal and
spatial discretization. We prove the convergence to stationary points of such
schemes in the semi-discrete setting. Further, we establish their convergence
to time-continuous solutions as the time-step tends to zero, and perform a
comprehensive error analysis in the fully discrete case. Finally, these
algorithms undergo thorough testing and validation in approaching stationary
points of non-convex variational models in applied sciences, such as the
Ginzburg-Landau energy in phase-field modeling and a specific case of the
Landau-de Gennes energy of the Q-tensor model for liquid crystals.
</p>
</div>
</dd>
<dt><a name="item653">[653]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12174" title="Abstract">arXiv:2402.12174</a> [<a href="/pdf/2402.12174" title="Download PDF">pdf</a>, <a href="/format/2402.12174" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BIDER: Bridging Knowledge Inconsistency for Efficient  Retrieval-Augmented LLMs via Key Supporting Evidence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+J">Jiajie Jin</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yutao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yujia Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Z">Zhicheng Dou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Retrieval-augmented large language models (LLMs) have demonstrated efficacy
in knowledge-intensive tasks such as open-domain QA, addressing inherent
challenges in knowledge update and factual inadequacy. However, inconsistencies
between retrieval knowledge and the necessary knowledge for LLMs, leading to a
decline in LLM's answer quality. This paper introduces BIDER, an approach that
refines retrieval documents into Key Supporting Evidence (KSE) through
knowledge synthesis, supervised fine-tuning (SFT), and preference alignment. We
train BIDER by learning from crafting KSE, while maximizing its output to align
with LLM's information acquisition preferences through reinforcement learning.
Evaluations across five datasets show BIDER boosts LLMs' answer quality by 7%
while reducing input content length in retrieval documents by 80%,
outperforming existing methods. The proposed KSE simulation effectively equips
LLMs with essential information for accurate question answering.
</p>
</div>
</dd>
<dt><a name="item654">[654]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12175" title="Abstract">arXiv:2402.12175</a> [<a href="/pdf/2402.12175" title="Download PDF">pdf</a>, <a href="/format/2402.12175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Discretized Bayesian Networks with GOMEA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ha%2C+D+M+F">Damy M.F. Ha</a>, 
<a href="/search/cs?searchtype=author&query=Alderliesten%2C+T">Tanja Alderliesten</a>, 
<a href="/search/cs?searchtype=author&query=Bosman%2C+P+A+N">Peter A.N. Bosman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The code is available at: <a href="https://github.com/damyha/dbn_gomea">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Bayesian networks model relationships between random variables under
uncertainty and can be used to predict the likelihood of events and outcomes
while incorporating observed evidence. From an eXplainable AI (XAI)
perspective, such models are interesting as they tend to be compact. Moreover,
captured relations can be directly inspected by domain experts. In practice,
data is often real-valued. Unless assumptions of normality can be made,
discretization is often required. The optimal discretization, however, depends
on the relations modelled between the variables. This complicates learning
Bayesian networks from data. For this reason, most literature focuses on
learning conditional dependencies between sets of variables, called structure
learning. In this work, we extend an existing state-of-the-art structure
learning approach based on the Gene-pool Optimal Mixing Evolutionary Algorithm
(GOMEA) to jointly learn variable discretizations. The proposed Discretized
Bayesian Network GOMEA (DBN-GOMEA) obtains similar or better results than the
current state-of-the-art when tasked to retrieve randomly generated
ground-truth networks. Moreover, leveraging a key strength of evolutionary
algorithms, we can straightforwardly perform DBN learning multi-objectively. We
show how this enables incorporating expert knowledge in a uniquely insightful
fashion, finding multiple DBNs that trade-off complexity, accuracy, and the
difference with a pre-determined expert network.
</p>
</div>
</dd>
<dt><a name="item655">[655]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12177" title="Abstract">arXiv:2402.12177</a> [<a href="/pdf/2402.12177" title="Download PDF">pdf</a>, <a href="/ps/2402.12177" title="Download PostScript">ps</a>, <a href="/format/2402.12177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingtian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+S">Shawn Lan</a>, 
<a href="/search/cs?searchtype=author&query=Hayes%2C+P">Peter Hayes</a>, 
<a href="/search/cs?searchtype=author&query=Barber%2C+D">David Barber</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Retrieval Augmented Generation (RAG) has emerged as an effective solution for
mitigating hallucinations in Large Language Models (LLMs). The retrieval stage
in RAG typically involves a pre-trained embedding model, which converts queries
and passages into vectors to capture their semantics. However, a standard
pre-trained embedding model may exhibit sub-optimal performance when applied to
specific domain knowledge, necessitating fine-tuning. This paper addresses
scenarios where the embeddings are only available from a black-box model. We
introduce Model augmented fine-tuning (Mafin) -- a novel approach for
fine-tuning a black-box embedding model by augmenting it with a trainable
embedding model. Our results demonstrate that Mafin significantly enhances the
performance of the black-box embeddings by only requiring the training of a
small augmented model. We validate the effectiveness of our method on both
labeled and unlabeled datasets, illustrating its broad applicability and
efficiency.
</p>
</div>
</dd>
<dt><a name="item656">[656]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12179" title="Abstract">arXiv:2402.12179</a> [<a href="/pdf/2402.12179" title="Download PDF">pdf</a>, <a href="/format/2402.12179" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Examining Monitoring System: Detecting Abnormal Behavior In Online  Examinations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ngo%2C+D+A">Dinh An Ngo</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+D">Thanh Dat Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Dang%2C+T+L+C">Thi Le Chi Dang</a>, 
<a href="/search/cs?searchtype=author&query=Le%2C+H+H">Huy Hoan Le</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+T+B">Ton Bao Ho</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+V+T+K">Vo Thanh Khang Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+T+T+H">Truong Thanh Hung Nguyen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">Cheating in online exams has become a prevalent issue over the past decade,
especially during the COVID-19 pandemic. To address this issue of academic
dishonesty, our "Exam Monitoring System: Detecting Abnormal Behavior in Online
Examinations" is designed to assist proctors in identifying unusual student
behavior. Our system demonstrates high accuracy and speed in detecting cheating
in real-time scenarios, providing valuable information, and aiding proctors in
decision-making. This article outlines our methodology and the effectiveness of
our system in mitigating the widespread problem of cheating in online exams.
</p>
</div>
</dd>
<dt><a name="item657">[657]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12181" title="Abstract">arXiv:2402.12181</a> [<a href="/pdf/2402.12181" title="Download PDF">pdf</a>, <a href="/format/2402.12181" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Revisiting Data Augmentation in Deep Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jianshu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yunpeng Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+P">Paul Weng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Various data augmentation techniques have been recently proposed in
image-based deep reinforcement learning (DRL). Although they empirically
demonstrate the effectiveness of data augmentation for improving sample
efficiency or generalization, which technique should be preferred is not always
clear. To tackle this question, we analyze existing methods to better
understand them and to uncover how they are connected. Notably, by expressing
the variance of the Q-targets and that of the empirical actor/critic losses of
these methods, we can analyze the effects of their different components and
compare them. We furthermore formulate an explanation about how these methods
may be affected by choosing different data augmentation transformations in
calculating the target Q-values. This analysis suggests recommendations on how
to exploit data augmentation in a more principled way. In addition, we include
a regularization term called tangent prop, previously proposed in computer
vision, but whose adaptation to DRL is novel to the best of our knowledge. We
evaluate our proposition and validate our analysis in several domains. Compared
to different relevant baselines, we demonstrate that it achieves
state-of-the-art performance in most environments and shows higher sample
efficiency and better generalization ability in some complex environments.
</p>
</div>
</dd>
<dt><a name="item658">[658]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12183" title="Abstract">arXiv:2402.12183</a> [<a href="/pdf/2402.12183" title="Download PDF">pdf</a>, <a href="/format/2402.12183" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MultiFIX: An XAI-friendly feature inducing approach to building models  from multimodal data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Malafaia%2C+M">Mafalda Malafaia</a>, 
<a href="/search/cs?searchtype=author&query=Schlender%2C+T">Thalea Schlender</a>, 
<a href="/search/cs?searchtype=author&query=Bosman%2C+P+A+N">Peter A. N. Bosman</a>, 
<a href="/search/cs?searchtype=author&query=Alderliesten%2C+T">Tanja Alderliesten</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
<p class="mathjax">In the health domain, decisions are often based on different data modalities.
Thus, when creating prediction models, multimodal fusion approaches that can
extract and combine relevant features from different data modalities, can be
highly beneficial. Furthermore, it is important to understand how each modality
impacts the final prediction, especially in high-stake domains, so that these
models can be used in a trustworthy and responsible manner. We propose
MultiFIX: a new interpretability-focused multimodal data fusion pipeline that
explicitly induces separate features from different data types that can
subsequently be combined to make a final prediction. An end-to-end deep
learning architecture is used to train a predictive model and extract
representative features of each modality. Each part of the model is then
explained using explainable artificial intelligence techniques. Attention maps
are used to highlight important regions in image inputs. Inherently
interpretable symbolic expressions, learned with GP-GOMEA, are used to describe
the contribution of tabular inputs. The fusion of the extracted features to
predict the target label is also replaced by a symbolic expression, learned
with GP-GOMEA. Results on synthetic problems demonstrate the strengths and
limitations of MultiFIX. Lastly, we apply MultiFIX to a publicly available
dataset for the detection of malignant skin lesions.
</p>
</div>
</dd>
<dt><a name="item659">[659]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12184" title="Abstract">arXiv:2402.12184</a> [<a href="/pdf/2402.12184" title="Download PDF">pdf</a>, <a href="/format/2402.12184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Colorizing Monochromatic Radiance Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Y">Yean Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+R">Renjie Wan</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+S">Shuchen Weng</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chengxuan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yakun Chang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+B">Boxin Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Though Neural Radiance Fields (NeRF) can produce colorful 3D representations
of the world by using a set of 2D images, such ability becomes non-existent
when only monochromatic images are provided. Since color is necessary in
representing the world, reproducing color from monochromatic radiance fields
becomes crucial. To achieve this goal, instead of manipulating the
monochromatic radiance fields directly, we consider it as a
representation-prediction task in the Lab color space. By first constructing
the luminance and density representation using monochromatic images, our
prediction stage can recreate color representation on the basis of an image
colorization module. We then reproduce a colorful implicit model through the
representation of luminance, density, and color. Extensive experiments have
been conducted to validate the effectiveness of our approaches. Our project
page: https://liquidammonia.github.io/color-nerf.
</p>
</div>
</dd>
<dt><a name="item660">[660]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12185" title="Abstract">arXiv:2402.12185</a> [<a href="/pdf/2402.12185" title="Download PDF">pdf</a>, <a href="/format/2402.12185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChartX &amp; ChartVLM: A Versatile Benchmark and Foundation Model for  Complicated Chart Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+R">Renqiu Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Hancheng Ye</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xiangchao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hongbin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zijun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+M">Min Dou</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+B">Botian Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code and dataset are available for downloading at: <a href="https://github.com/UniModal4Reasoning/ChartVLM">this https URL</a> 22 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Recently, many versatile Multi-modal Large Language Models (MLLMs) have
emerged continuously. However, their capacity to query information depicted in
visual charts and engage in reasoning based on the queried contents remains
under-explored. In this paper, to comprehensively and rigorously benchmark the
ability of the off-the-shelf MLLMs in the chart domain, we construct ChartX, a
multi-modal evaluation set covering 18 chart types, 7 chart tasks, 22
disciplinary topics, and high-quality chart data. Besides, we develop ChartVLM
to offer a new perspective on handling multi-modal tasks that strongly depend
on interpretable patterns, such as reasoning tasks in the field of charts or
geometric images. We evaluate the chart-related ability of mainstream MLLMs and
our ChartVLM on the proposed ChartX evaluation set. Extensive experiments
demonstrate that ChartVLM surpasses both versatile and chart-related large
models, achieving results comparable to GPT-4V. We believe that our study can
pave the way for further exploration in creating a more comprehensive chart
evaluation set and developing more interpretable multi-modal models. Both
ChartX and ChartVLM are available at:
https://github.com/UniModal4Reasoning/ChartVLM
</p>
</div>
</dd>
<dt><a name="item661">[661]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12187" title="Abstract">arXiv:2402.12187</a> [<a href="/pdf/2402.12187" title="Download PDF">pdf</a>, <a href="/format/2402.12187" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Feature Alignment: Balancing Robustness and Accuracy in Deep  Learning via Adversarial Training
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+L+H">Leo Hyun Park</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jaeuk Kim</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+M+G">Myung Gyo Oh</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+J">Jaewoo Park</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+T">Taekyoung Kwon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 5 figures, 16 tables, 2 algorithms
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Deep learning models continue to advance in accuracy, yet they remain
vulnerable to adversarial attacks, which often lead to the misclassification of
adversarial examples. Adversarial training is used to mitigate this problem by
increasing robustness against these attacks. However, this approach typically
reduces a model's standard accuracy on clean, non-adversarial samples. The
necessity for deep learning models to balance both robustness and accuracy for
security is obvious, but achieving this balance remains challenging, and the
underlying reasons are yet to be clarified. This paper proposes a novel
adversarial training method called Adversarial Feature Alignment (AFA), to
address these problems. Our research unveils an intriguing insight:
misalignment within the feature space often leads to misclassification,
regardless of whether the samples are benign or adversarial. AFA mitigates this
risk by employing a novel optimization algorithm based on contrastive learning
to alleviate potential feature misalignment. Through our evaluations, we
demonstrate the superior performance of AFA. The baseline AFA delivers higher
robust accuracy than previous adversarial contrastive learning methods while
minimizing the drop in clean accuracy to 1.86% and 8.91% on CIFAR10 and
CIFAR100, respectively, in comparison to cross-entropy. We also show that joint
optimization of AFA and TRADES, accompanied by data augmentation using a recent
diffusion model, achieves state-of-the-art accuracy and robustness.
</p>
</div>
</dd>
<dt><a name="item662">[662]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12189" title="Abstract">arXiv:2402.12189</a> [<a href="/pdf/2402.12189" title="Download PDF">pdf</a>, <a href="/format/2402.12189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Amplifying Training Data Exposure through Fine-Tuning with  Pseudo-Labeled Memberships
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oh%2C+M+G">Myung Gyo Oh</a>, 
<a href="/search/cs?searchtype=author&query=Ahn%2C+H+E">Hong Eun Ahn</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+L+H">Leo Hyun Park</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+T">Taekyoung Kwon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 6 figures, 15 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Neural language models (LMs) are vulnerable to training data extraction
attacks due to data memorization. This paper introduces a novel attack scenario
wherein an attacker adversarially fine-tunes pre-trained LMs to amplify the
exposure of the original training data. This strategy differs from prior
studies by aiming to intensify the LM's retention of its pre-training dataset.
To achieve this, the attacker needs to collect generated texts that are closely
aligned with the pre-training data. However, without knowledge of the actual
dataset, quantifying the amount of pre-training data within generated texts is
challenging. To address this, we propose the use of pseudo-labels for these
generated texts, leveraging membership approximations indicated by
machine-generated probabilities from the target LM. We subsequently fine-tune
the LM to favor generations with higher likelihoods of originating from the
pre-training data, based on their membership probabilities. Our empirical
findings indicate a remarkable outcome: LMs with over 1B parameters exhibit a
four to eight-fold increase in training data exposure. We discuss potential
mitigations and suggest future research directions.
</p>
</div>
</dd>
<dt><a name="item663">[663]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12192" title="Abstract">arXiv:2402.12192</a> [<a href="/pdf/2402.12192" title="Download PDF">pdf</a>, <a href="/format/2402.12192" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pan-Mamba: Effective pan-sharpening with State Space Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xuanhua He</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+K">Ke Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+K">Keyu Yan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Rui Li</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+C">Chengjun Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Man Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Pan-sharpening involves integrating information from lowresolution
multi-spectral and high-resolution panchromatic images to generate
high-resolution multi-spectral counterparts. While recent advancements in the
state space model, particularly the efficient long-range dependency modeling
achieved by Mamba, have revolutionized computer vision community, its untapped
potential in pan-sharpening motivates our exploration. Our contribution,
Pan-Mamba, represents a novel pansharpening network that leverages the
efficiency of the Mamba model in global information modeling. In Pan-Mamba, we
customize two core components: channel swapping Mamba and cross-modal Mamba,
strategically designed for efficient cross-modal information exchange and
fusion. The former initiates a lightweight cross-modal interaction through the
exchange of partial panchromatic and multispectral channels, while the latter
facilities the information representation capability by exploiting inherent
cross-modal relationships. Through extensive experiments across diverse
datasets, our proposed approach surpasses state-of-theart methods, showcasing
superior fusion results in pan-sharpening. To the best of our knowledge, this
work is the first attempt in exploring the potential of the Mamba model and
establishes a new frontier in the pan-sharpening techniques. The source code is
available at https://github.com/alexhe101/Pan-Mamba .
</p>
</div>
</dd>
<dt><a name="item664">[664]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12193" title="Abstract">arXiv:2402.12193</a> [<a href="/pdf/2402.12193" title="Download PDF">pdf</a>, <a href="/format/2402.12193" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Chinese Dataset for Evaluating the Safeguards in Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuxia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+Z">Zenan Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haonan Li</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xudong Han</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Lizhi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhenxuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jingru Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Nakov%2C+P">Preslav Nakov</a>, 
<a href="/search/cs?searchtype=author&query=Baldwin%2C+T">Timothy Baldwin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Many studies have demonstrated that large language models (LLMs) can produce
harmful responses, exposing users to unexpected risks when LLMs are deployed.
Previous studies have proposed comprehensive taxonomies of the risks posed by
LLMs, as well as corresponding prompts that can be used to examine the safety
mechanisms of LLMs. However, the focus has been almost exclusively on English,
and little has been explored for other languages. Here we aim to bridge this
gap. We first introduce a dataset for the safety evaluation of Chinese LLMs,
and then extend it to two other scenarios that can be used to better identify
false negative and false positive examples in terms of risky prompt rejections.
We further present a set of fine-grained safety assessment criteria for each
risk type, facilitating both manual annotation and automatic evaluation in
terms of LLM response harmfulness. Our experiments on five LLMs show that
region-specific risks are the prevalent type of risk, presenting the major
issue with all Chinese LLMs we experimented with. Warning: this paper contains
example data that may be offensive, harmful, or biased.
</p>
</div>
</dd>
<dt><a name="item665">[665]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12195" title="Abstract">arXiv:2402.12195</a> [<a href="/pdf/2402.12195" title="Download PDF">pdf</a>, <a href="/format/2402.12195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Browse and Concentrate: Comprehending Multimodal Content via prior-LLM  Context Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yiqi Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+F">Fuwen Luo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peng Li</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+M">Ming Yan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Ji Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+F">Fei Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">With the bloom of Large Language Models (LLMs), Multimodal Large Language
Models (MLLMs) that incorporate LLMs with pre-trained vision models have
recently demonstrated impressive performance across diverse vision-language
tasks. However, they fall short to comprehend context involving multiple
images. A primary reason for this shortcoming is that the visual features for
each images are encoded individually by frozen encoders before feeding into the
LLM backbone, lacking awareness of other images and the multimodal
instructions. We term this issue as prior-LLM modality isolation and propose a
two phase paradigm, browse-and-concentrate, to enable in-depth multimodal
context fusion prior to feeding the features into LLMs. This paradigm initially
"browses" through the inputs for essential insights, and then revisits the
inputs to "concentrate" on crucial details, guided by these insights, to
achieve a more comprehensive understanding of the multimodal inputs.
Additionally, we develop training strategies specifically to enhance the
understanding of multi-image inputs. Our method markedly boosts the performance
on 7 multi-image scenarios, contributing to increments on average accuracy by
2.13% and 7.60% against strong MLLMs baselines with 3B and 11B LLMs,
respectively.
</p>
</div>
</dd>
<dt><a name="item666">[666]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12198" title="Abstract">arXiv:2402.12198</a> [<a href="/pdf/2402.12198" title="Download PDF">pdf</a>, <a href="/format/2402.12198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Zero shot VLMs for hate meme detection: Are we there yet?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rizwan%2C+N">Naquee Rizwan</a>, 
<a href="/search/cs?searchtype=author&query=Bhaskar%2C+P">Paramananda Bhaskar</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+M">Mithun Das</a>, 
<a href="/search/cs?searchtype=author&query=Majhi%2C+S+S">Swadhin Satyaprakash Majhi</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+P">Punyajoy Saha</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+A">Animesh Mukherjee</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Multimedia content on social media is rapidly evolving, with memes gaining
prominence as a distinctive form. Unfortunately, some malicious users exploit
memes to target individuals or vulnerable communities, making it imperative to
identify and address such instances of hateful memes. Extensive research has
been conducted to address this issue by developing hate meme detection models.
However, a notable limitation of traditional machine/deep learning models is
the requirement for labeled datasets for accurate classification. Recently, the
research community has witnessed the emergence of several visual language
models that have exhibited outstanding performance across various tasks. In
this study, we aim to investigate the efficacy of these visual language models
in handling intricate tasks such as hate meme detection. We use various prompt
settings to focus on zero-shot classification of hateful/harmful memes. Through
our analysis, we observe that large VLMs are still vulnerable for zero-shot
hate meme detection.
</p>
</div>
</dd>
<dt><a name="item667">[667]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12201" title="Abstract">arXiv:2402.12201</a> [<a href="/pdf/2402.12201" title="Download PDF">pdf</a>, <a href="/format/2402.12201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic  Interpretability: A Case Study on Othello-GPT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhengfu He</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+X">Xuyang Ge</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Q">Qiong Tang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tianxiang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Q">Qinyuan Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 13 figures. Not final version. Better dictionary training in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Sparse dictionary learning has been a rapidly growing technique in
mechanistic interpretability to attack superposition and extract more
human-understandable features from model activations. We ask a further question
based on the extracted more monosemantic features: How do we recognize circuits
connecting the enormous amount of dictionary features? We propose a circuit
discovery framework alternative to activation patching. Our framework suffers
less from out-of-distribution and proves to be more efficient in terms of
asymptotic complexity. The basic unit in our framework is dictionary features
decomposed from all modules writing to the residual stream, including
embedding, attention output and MLP output. Starting from any logit, dictionary
feature or attention score, we manage to trace down to lower-level dictionary
features of all tokens and compute their contribution to these more
interpretable and local model behaviors. We dig in a small transformer trained
on a synthetic task named Othello and find a number of human-understandable
fine-grained circuits inside of it.
</p>
</div>
</dd>
<dt><a name="item668">[668]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12202" title="Abstract">arXiv:2402.12202</a> [<a href="/pdf/2402.12202" title="Download PDF">pdf</a>, <a href="/format/2402.12202" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Heterogeneity-aware Cross-school Electives Recommendation: a Hybrid  Federated Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ju%2C+C">Chengyi Ju</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jiannong Cao</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhen-Qun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+H+M">Ho Man Lee</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE International Conference on Data Mining Workshops
  (ICDMW)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In the era of modern education, addressing cross-school learner diversity is
crucial, especially in personalized recommender systems for elective course
selection. However, privacy concerns often limit cross-school data sharing,
which hinders existing methods' ability to model sparse data and address
heterogeneity effectively, ultimately leading to suboptimal recommendations. In
response, we propose HFRec, a heterogeneity-aware hybrid federated recommender
system designed for cross-school elective course recommendations. The proposed
model constructs heterogeneous graphs for each school, incorporating various
interactions and historical behaviors between students to integrate context and
content information. We design an attention mechanism to capture
heterogeneity-aware representations. Moreover, under a federated scheme, we
train individual school-based models with adaptive learning settings to
recommend tailored electives. Our HFRec model demonstrates its effectiveness in
providing personalized elective recommendations while maintaining privacy, as
it outperforms state-of-the-art models on both open-source and real-world
datasets.
</p>
</div>
</dd>
<dt><a name="item669">[669]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12203" title="Abstract">arXiv:2402.12203</a> [<a href="/pdf/2402.12203" title="Download PDF">pdf</a>, <a href="/format/2402.12203" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MPI Implementation Profiling for Better Application Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shipley%2C+R">Riley Shipley</a>, 
<a href="/search/cs?searchtype=author&query=Hooten%2C+G">Garrett Hooten</a>, 
<a href="/search/cs?searchtype=author&query=Boehme%2C+D">David Boehme</a>, 
<a href="/search/cs?searchtype=author&query=Schafer%2C+D">Derek Schafer</a>, 
<a href="/search/cs?searchtype=author&query=Skjellum%2C+A">Anthony Skjellum</a>, 
<a href="/search/cs?searchtype=author&query=Pearce%2C+O">Olga Pearce</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Performance (cs.PF); Software Engineering (cs.SE)

</div>
<p class="mathjax">While application profiling has been a mainstay in the HPC community for
years, profiling of MPI and other communication middleware has not received the
same degree of exploration. This paper adds to the discussion of MPI profiling,
contributing two general-purpose profiling methods as well as practical
applications of these methods to an existing implementation. The ability to
detect performance defects in MPI codes using these methods increases the
potential of further research and development in communication optimization.
</p>
</div>
</dd>
<dt><a name="item670">[670]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12204" title="Abstract">arXiv:2402.12204</a> [<a href="/pdf/2402.12204" title="Download PDF">pdf</a>, <a href="/format/2402.12204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Multilingual Capabilities of Large Language Models through  Self-Distillation from Resource-Rich Languages
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuanchi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yile Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zijun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaolong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Peng Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">While large language models (LLMs) have been pre-trained on multilingual
corpora, their performance still lags behind in most languages compared to a
few resource-rich languages. One common approach to mitigate this issue is to
translate training data from resource-rich languages into other languages and
then continue training. However, using the data obtained solely relying on
translation while ignoring the original capabilities of LLMs across languages
is not always effective, which we show will limit the performance of
cross-lingual knowledge transfer. In this work, we propose SDRRL, a method
based on Self-Distillation from Resource-Rich Languages that effectively
improve multilingual performance by leveraging the internal capabilities of
LLMs on resource-rich languages. We evaluate on different LLMs (LLaMA-2 and
SeaLLM) and source languages across various comprehension and generation tasks,
experimental results demonstrate that SDRRL can significantly enhance
multilingual capabilities while minimizing the impact on original performance
in resource-rich languages.
</p>
</div>
</dd>
<dt><a name="item671">[671]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12212" title="Abstract">arXiv:2402.12212</a> [<a href="/pdf/2402.12212" title="Download PDF">pdf</a>, <a href="/format/2402.12212" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polarization of Autonomous Generative AI Agents Under Echo Chambers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ohagi%2C+M">Masaya Ohagi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Online social networks often create echo chambers where people only hear
opinions reinforcing their beliefs. An echo chamber often generates
polarization, leading to conflicts caused by people with radical opinions, such
as the January 6, 2021, attack on the US Capitol. The echo chamber has been
viewed as a human-specific problem, but this implicit assumption is becoming
less reasonable as large language models, such as ChatGPT, acquire social
abilities. In response to this situation, we investigated the potential for
polarization to occur among a group of autonomous AI agents based on generative
language models in an echo chamber environment. We had AI agents discuss
specific topics and analyzed how the group's opinions changed as the discussion
progressed. As a result, we found that the group of agents based on ChatGPT
tended to become polarized in echo chamber environments. The analysis of
opinion transitions shows that this result is caused by ChatGPT's high prompt
understanding ability to update its opinion by considering its own and
surrounding agents' opinions. We conducted additional experiments to
investigate under what specific conditions AI agents tended to polarize. As a
result, we identified factors that strongly influence polarization, such as the
agent's persona. These factors should be monitored to prevent the polarization
of AI agents.
</p>
</div>
</dd>
<dt><a name="item672">[672]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12214" title="Abstract">arXiv:2402.12214</a> [<a href="/pdf/2402.12214" title="Download PDF">pdf</a>, <a href="/format/2402.12214" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SlopeSeeker: A Search Tool for Exploring a Dataset of Quantifiable  Trends
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bendeck%2C+A">Alexander Bendeck</a>, 
<a href="/search/cs?searchtype=author&query=Bromley%2C+D">Dennis Bromley</a>, 
<a href="/search/cs?searchtype=author&query=Setlur%2C+V">Vidya Setlur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 19 figures. Intelligent User Interfaces (IUI) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
<p class="mathjax">Natural language and search interfaces intuitively facilitate data
exploration and provide visualization responses to diverse analytical queries
based on the underlying datasets. However, these interfaces often fail to
interpret more complex analytical intents, such as discerning subtleties and
quantifiable differences between terms like "bump" and "spike" in the context
of COVID cases, for example. We address this gap by extending the capabilities
of a data exploration search interface for interpreting semantic concepts in
time series trends. We first create a comprehensive dataset of semantic
concepts by mapping quantifiable univariate data trends such as slope and angle
to crowdsourced, semantically meaningful trend labels. The dataset contains
quantifiable properties that capture the slope-scalar effect of semantic
modifiers like "sharply" and "gradually," as well as multi-line trends (e.g.,
"peak," "valley"). We demonstrate the utility of this dataset in SlopeSeeker, a
tool that supports natural language querying of quantifiable trends, such as
"show me stocks that tanked in 2010." The tool incorporates novel scoring and
ranking techniques based on semantic relevance and visual prominence to present
relevant trend chart responses containing these semantic trend concepts. In
addition, SlopeSeeker provides a faceted search interface for users to navigate
a semantic hierarchy of concepts from general trends (e.g., "increase") to more
specific ones (e.g., "sharp increase"). A preliminary user evaluation of the
tool demonstrates that the search interface supports greater expressivity of
queries containing concepts that describe data trends. We identify potential
future directions for leveraging our publicly available quantitative semantics
dataset in other data domains and for novel visual analytics interfaces.
</p>
</div>
</dd>
<dt><a name="item673">[673]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12216" title="Abstract">arXiv:2402.12216</a> [<a href="/pdf/2402.12216" title="Download PDF">pdf</a>, <a href="/format/2402.12216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Copyleft for Alleviating AIGC Copyright Dilemma: What-if Analysis,  Public Perception and Implications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xinwei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yujun Li</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yafeng Peng</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+X">Xuetao Wei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">As AIGC has impacted our society profoundly in the past years, ethical issues
have received tremendous attention. The most urgent one is the AIGC copyright
dilemma, which can immensely stifle the development of AIGC and greatly cost
the entire society. Given the complexity of AIGC copyright governance and the
fact that no perfect solution currently exists, previous work advocated
copyleft on AI governance but without substantive analysis. In this paper, we
take a step further to explore the feasibility of copyleft to alleviate the
AIGC copyright dilemma. We conduct a mixed-methods study from two aspects:
qualitatively, we use a formal what-if analysis to clarify the dilemma and
provide case studies to show the feasibility of copyleft; quantitatively, we
perform a carefully designed survey to find out how the public feels about
copylefting AIGC. The key findings include: a) people generally perceive the
dilemma, b) they prefer to use authorized AIGC under loose restriction, and c)
they are positive to copyleft in AIGC and willing to use it in the future.
</p>
</div>
</dd>
<dt><a name="item674">[674]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12219" title="Abstract">arXiv:2402.12219</a> [<a href="/pdf/2402.12219" title="Download PDF">pdf</a>, <a href="/format/2402.12219" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reformatted Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+R">Run-Ze Fan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuefeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+H">Haoyang Zou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Junlong Li</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shwai He</a>, 
<a href="/search/cs?searchtype=author&query=Chern%2C+E">Ethan Chern</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jiewen Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Pengfei Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Homepage: <a href="https://gair-nlp.github.io/ReAlign/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The quality of finetuning data is crucial for aligning large language models
(LLMs) with human values. Current methods to improve data quality are either
labor-intensive or prone to factual errors caused by LLM hallucinations. This
paper explores elevating the quality of existing instruction data to better
align with human values, introducing a simple and effective approach named
ReAlign, which reformats the responses of instruction data into a format that
better aligns with pre-established criteria and the collated evidence. This
approach minimizes human annotation, hallucination, and the difficulty in
scaling, remaining orthogonal to existing alignment techniques. Experimentally,
ReAlign significantly boosts the general alignment ability, math reasoning,
factuality, and readability of the LLMs.
<br />Encouragingly, without introducing any additional data or advanced training
techniques, and merely by reformatting the response, LLaMA-2-13B's mathematical
reasoning ability on GSM8K can be improved from 46.77% to 56.63% in accuracy.
Additionally, a mere 5% of ReAlign data yields a 67% boost in general alignment
ability measured by the Alpaca dataset. This work highlights the need for
further research into the science and mechanistic interpretability of LLMs. We
have made the associated code and data publicly accessible to support future
studies at https://github.com/GAIR-NLP/ReAlign.
</p>
</div>
</dd>
<dt><a name="item675">[675]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12222" title="Abstract">arXiv:2402.12222</a> [<a href="/pdf/2402.12222" title="Download PDF">pdf</a>, <a href="/format/2402.12222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement  Learning for LLM-based Mutation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eom%2C+J">Jueon Eom</a>, 
<a href="/search/cs?searchtype=author&query=Jeong%2C+S">Seyeon Jeong</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+T">Taekyoung Kwon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 4 figures, 9 tables, 2 listings
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Software Engineering (cs.SE)

</div>
<p class="mathjax">Fuzzing is an effective bug-finding technique but it struggles with complex
systems like JavaScript engines that demand precise grammatical input.
Recently, researchers have adopted language models for context-aware mutation
in fuzzing to address this problem. However, existing techniques are limited in
utilizing coverage guidance for fuzzing, which is rather performed in a
black-box manner. This paper presents a novel technique called CovRL
(Coverage-guided Reinforcement Learning) that combines Large Language Models
(LLMs) with reinforcement learning from coverage feedback. Our fuzzer,
CovRL-Fuzz, integrates coverage feedback directly into the LLM by leveraging
the Term Frequency-Inverse Document Frequency (TF-IDF) method to construct a
weighted coverage map. This map is key in calculating the fuzzing reward, which
is then applied to the LLM-based mutator through reinforcement learning.
CovRL-Fuzz, through this approach, enables the generation of test cases that
are more likely to discover new coverage areas, thus improving vulnerability
detection while minimizing syntax and semantic errors, all without needing
extra post-processing. Our evaluation results indicate that CovRL-Fuzz
outperforms the state-of-the-art fuzzers in terms of code coverage and
bug-finding capabilities: CovRL-Fuzz identified 48 real-world security-related
bugs in the latest JavaScript engines, including 39 previously unknown
vulnerabilities and 11 CVEs.
</p>
</div>
</dd>
<dt><a name="item676">[676]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12225" title="Abstract">arXiv:2402.12225</a> [<a href="/pdf/2402.12225" title="Download PDF">pdf</a>, <a href="/format/2402.12225" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pushing Auto-regressive Models for 3D Shape Generation at Capacity and  Scalability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qian%2C+X">Xuelin Qian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+S">Simian Luo</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yinda Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tai%2C+Y">Ying Tai</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhenyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chengjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+X">Xiangyang Xue</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B">Bo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tiejun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yunsheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yanwei Fu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://argus-3d.github.io/">this https URL</a> . Datasets: <a href="https://huggingface.co/datasets/BAAI/Objaverse-MIX.">this https URL</a> arXiv admin note: substantial text overlap with <a href="/abs/2303.14700">arXiv:2303.14700</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Auto-regressive models have achieved impressive results in 2D image
generation by modeling joint distributions in grid space. In this paper, we
extend auto-regressive models to 3D domains, and seek a stronger ability of 3D
shape generation by improving auto-regressive models at capacity and
scalability simultaneously. Firstly, we leverage an ensemble of publicly
available 3D datasets to facilitate the training of large-scale models. It
consists of a comprehensive collection of approximately 900,000 objects, with
multiple properties of meshes, points, voxels, rendered images, and text
captions. This diverse labeled dataset, termed Objaverse-Mix, empowers our
model to learn from a wide range of object variations. However, directly
applying 3D auto-regression encounters critical challenges of high
computational demands on volumetric grids and ambiguous auto-regressive order
along grid dimensions, resulting in inferior quality of 3D shapes. To this end,
we then present a novel framework Argus3D in terms of capacity. Concretely, our
approach introduces discrete representation learning based on a latent vector
instead of volumetric grids, which not only reduces computational costs but
also preserves essential geometric details by learning the joint distributions
in a more tractable order. The capacity of conditional generation can thus be
realized by simply concatenating various conditioning inputs to the latent
vector, such as point clouds, categories, images, and texts. In addition,
thanks to the simplicity of our model architecture, we naturally scale up our
approach to a larger model with an impressive 3.6 billion parameters, further
enhancing the quality of versatile 3D generation. Extensive experiments on four
generation tasks demonstrate that Argus3D can synthesize diverse and faithful
shapes across multiple categories, achieving remarkable performance.
</p>
</div>
</dd>
<dt><a name="item677">[677]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12226" title="Abstract">arXiv:2402.12226</a> [<a href="/pdf/2402.12226" title="Download PDF">pdf</a>, <a href="/format/2402.12226" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhan%2C+J">Jun Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+J">Junqi Dai</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J">Jiasheng Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yunhua Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhigeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+R">Ruibin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Ge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Linyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+H">Hang Yan</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tianxiang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+Y">Yugang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xipeng Qiu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 16 figures, under review, work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce AnyGPT, an any-to-any multimodal language model that utilizes
discrete representations for the unified processing of various modalities,
including speech, text, images, and music. AnyGPT can be trained stably without
any alterations to the current large language model (LLM) architecture or
training paradigms. Instead, it relies exclusively on data-level preprocessing,
facilitating the seamless integration of new modalities into LLMs, akin to the
incorporation of new languages. We build a multimodal text-centric dataset for
multimodal alignment pre-training. Utilizing generative models, we synthesize
the first large-scale any-to-any multimodal instruction dataset. It consists of
108k samples of multi-turn conversations that intricately interweave various
modalities, thus equipping the model to handle arbitrary combinations of
multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is
capable of facilitating any-to-any multimodal conversation while achieving
performance comparable to specialized models across all modalities, proving
that discrete representations can effectively and conveniently unify multiple
modalities within a language model. Demos are shown in
https://junzhan2000.github.io/AnyGPT.github.io/
</p>
</div>
</dd>
<dt><a name="item678">[678]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12231" title="Abstract">arXiv:2402.12231</a> [<a href="/pdf/2402.12231" title="Download PDF">pdf</a>, <a href="/format/2402.12231" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Tempering Improves Parameter Estimation with Probabilistic  Integrators for Ordinary Differential Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beck%2C+J">Jonas Beck</a>, 
<a href="/search/cs?searchtype=author&query=Bosch%2C+N">Nathanael Bosch</a>, 
<a href="/search/cs?searchtype=author&query=Deistler%2C+M">Michael Deistler</a>, 
<a href="/search/cs?searchtype=author&query=Kadhim%2C+K+L">Kyra L. Kadhim</a>, 
<a href="/search/cs?searchtype=author&query=Macke%2C+J+H">Jakob H. Macke</a>, 
<a href="/search/cs?searchtype=author&query=Hennig%2C+P">Philipp Hennig</a>, 
<a href="/search/cs?searchtype=author&query=Berens%2C+P">Philipp Berens</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Ordinary differential equations (ODEs) are widely used to describe dynamical
systems in science, but identifying parameters that explain experimental
measurements is challenging. In particular, although ODEs are differentiable
and would allow for gradient-based parameter optimization, the nonlinear
dynamics of ODEs often lead to many local minima and extreme sensitivity to
initial conditions. We therefore propose diffusion tempering, a novel
regularization technique for probabilistic numerical methods which improves
convergence of gradient-based parameter optimization in ODEs. By iteratively
reducing a noise parameter of the probabilistic integrator, the proposed method
converges more reliably to the true parameters. We demonstrate that our method
is effective for dynamical systems of different complexity and show that it
obtains reliable parameter estimates for a Hodgkin-Huxley model with a
practically relevant number of parameters.
</p>
</div>
</dd>
<dt><a name="item679">[679]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12233" title="Abstract">arXiv:2402.12233</a> [<a href="/pdf/2402.12233" title="Download PDF">pdf</a>, <a href="/format/2402.12233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empirical Study on Updating Key-Value Memories in Transformer  Feed-forward Layers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Z">Zihan Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zeyu Huang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Youcheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Tiny Paper @ ICLR 2024. Codes available at this $\href{<a href="https://github.com/qiuzh20/Tuning-keys-v.s.-values">this https URL</a>}{this\,repo}$
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The feed-forward networks (FFNs) in transformers are recognized as a group of
key-value neural memories to restore abstract high-level knowledge. In this
work, we conduct an empirical ablation study on updating keys (the 1st layer in
the FFNs layer) or values (the 2nd layer in the FFNs layer). We compare those
two methods in various knowledge editing and fine-tuning tasks of large
language models to draw insights to understand FFNs further. Code is available
at $\href{https://github.com/qiuzh20/Tuning-keys-v.s.-values}{this\,repo}$.
</p>
</div>
</dd>
<dt><a name="item680">[680]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12234" title="Abstract">arXiv:2402.12234</a> [<a href="/pdf/2402.12234" title="Download PDF">pdf</a>, <a href="/ps/2402.12234" title="Download PostScript">ps</a>, <a href="/format/2402.12234" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Task-Oriented Dialogue with In-Context Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bocklisch%2C+T">Tom Bocklisch</a>, 
<a href="/search/cs?searchtype=author&query=Werkmeister%2C+T">Thomas Werkmeister</a>, 
<a href="/search/cs?searchtype=author&query=Varshneya%2C+D">Daksh Varshneya</a>, 
<a href="/search/cs?searchtype=author&query=Nichol%2C+A">Alan Nichol</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We describe a system for building task-oriented dialogue systems combining
the in-context learning abilities of large language models (LLMs) with the
deterministic execution of business logic. LLMs are used to translate between
the surface form of the conversation and a domain-specific language (DSL) which
is used to progress the business logic. We compare our approach to the
intent-based NLU approach predominantly used in industry today. Our experiments
show that developing chatbots with our system requires significantly less
effort than established approaches, that these chatbots can successfully
navigate complex dialogues which are extremely challenging for NLU-based
systems, and that our system has desirable properties for scaling task-oriented
dialogue systems to a large number of tasks. We make our implementation
available for use and further study.
</p>
</div>
</dd>
<dt><a name="item681">[681]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12235" title="Abstract">arXiv:2402.12235</a> [<a href="/pdf/2402.12235" title="Download PDF">pdf</a>, <a href="/format/2402.12235" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Fundamental Limits of Least-Privilege Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stadler%2C+T">Theresa Stadler</a>, 
<a href="/search/cs?searchtype=author&query=Kulynych%2C+B">Bogdan Kulynych</a>, 
<a href="/search/cs?searchtype=author&query=Papernot%2C+N">Nicoals Papernot</a>, 
<a href="/search/cs?searchtype=author&query=Gastpar%2C+M">Michael Gastpar</a>, 
<a href="/search/cs?searchtype=author&query=Troncoso%2C+C">Carmela Troncoso</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The promise of least-privilege learning -- to find feature representations
that are useful for a learning task but prevent inference of any sensitive
information unrelated to this task -- is highly appealing. However, so far this
concept has only been stated informally. It thus remains an open question
whether and how we can achieve this goal. In this work, we provide the first
formalisation of the least-privilege principle for machine learning and
characterise its feasibility. We prove that there is a fundamental trade-off
between a representation's utility for a given task and its leakage beyond the
intended task: it is not possible to learn representations that have high
utility for the intended task but, at the same time prevent inference of any
attribute other than the task label itself. This trade-off holds regardless of
the technique used to learn the feature mappings that produce these
representations. We empirically validate this result for a wide range of
learning techniques, model architectures, and datasets.
</p>
</div>
</dd>
<dt><a name="item682">[682]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12237" title="Abstract">arXiv:2402.12237</a> [<a href="/pdf/2402.12237" title="Download PDF">pdf</a>, <a href="/format/2402.12237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Defer in Content Moderation: The Human-AI Interplay
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lykouris%2C+T">Thodoris Lykouris</a>, 
<a href="/search/cs?searchtype=author&query=Weng%2C+W">Wentao Weng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT); Human-Computer Interaction (cs.HC); Performance (cs.PF)

</div>
<p class="mathjax">Successful content moderation in online platforms relies on a human-AI
collaboration approach. A typical heuristic estimates the expected harmfulness
of a post and uses fixed thresholds to decide whether to remove it and whether
to send it for human review. This disregards the prediction uncertainty, the
time-varying element of human review capacity and post arrivals, and the
selective sampling in the dataset (humans only review posts filtered by the
admission algorithm).
<br />In this paper, we introduce a model to capture the human-AI interplay in
content moderation. The algorithm observes contextual information for incoming
posts, makes classification and admission decisions, and schedules posts for
human review. Only admitted posts receive human reviews on their harmfulness.
These reviews help educate the machine-learning algorithms but are delayed due
to congestion in the human review system. The classical learning-theoretic way
to capture this human-AI interplay is via the framework of learning to defer,
where the algorithm has the option to defer a classification task to humans for
a fixed cost and immediately receive feedback. Our model contributes to this
literature by introducing congestion in the human review system. Moreover,
unlike work on online learning with delayed feedback where the delay in the
feedback is exogenous to the algorithm's decisions, the delay in our model is
endogenous to both the admission and the scheduling decisions.
<br />We propose a near-optimal learning algorithm that carefully balances the
classification loss from a selectively sampled dataset, the idiosyncratic loss
of non-reviewed posts, and the delay loss of having congestion in the human
review system. To the best of our knowledge, this is the first result for
online learning in contextual queueing systems and hence our analytical
framework may be of independent interest.
</p>
</div>
</dd>
<dt><a name="item683">[683]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12238" title="Abstract">arXiv:2402.12238</a> [<a href="/pdf/2402.12238" title="Download PDF">pdf</a>, <a href="/format/2402.12238" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Mixed Gaussian Flow for Diverse Trajectory Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiahe Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jinkun Cao</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+D">Dahua Lin</a>, 
<a href="/search/cs?searchtype=author&query=Kitani%2C+K">Kris Kitani</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+J">Jiangmiao Pang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Existing trajectory prediction studies intensively leverage generative
models. Normalizing flow is one of the genres with the advantage of being
invertible to derive the probability density of predicted trajectories.
However, mapping from a standard Gaussian by a flow-based model hurts the
capacity to capture complicated patterns of trajectories, ignoring the
under-represented motion intentions in the training data. To solve the problem,
we propose a flow-based model to transform a mixed Gaussian prior into the
future trajectory manifold. The model shows a better capacity for generating
diverse trajectory patterns. Also, by associating each sub-Gaussian with a
certain subspace of trajectories, we can generate future trajectories with
controllable motion intentions. In such a fashion, the flow-based model is not
encouraged to simply seek the most likelihood of the intended manifold anymore
but a family of controlled manifolds with explicit interpretability. Our
proposed method is demonstrated to show state-of-the-art performance in the
quantitative evaluation of sampling well-aligned trajectories in top-M
generated candidates. We also demonstrate that it can generate diverse,
controllable, and out-of-distribution trajectories. Code is available at
https://github.com/mulplue/MGF.
</p>
</div>
</dd>
<dt><a name="item684">[684]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12240" title="Abstract">arXiv:2402.12240</a> [<a href="/pdf/2402.12240" title="Download PDF">pdf</a>, <a href="/format/2402.12240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BEARS Make Neuro-Symbolic Models Aware of their Reasoning Shortcuts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marconato%2C+E">Emanuele Marconato</a>, 
<a href="/search/cs?searchtype=author&query=Bortolotti%2C+S">Samuele Bortolotti</a>, 
<a href="/search/cs?searchtype=author&query=van+Krieken%2C+E">Emile van Krieken</a>, 
<a href="/search/cs?searchtype=author&query=Vergari%2C+A">Antonio Vergari</a>, 
<a href="/search/cs?searchtype=author&query=Passerini%2C+A">Andrea Passerini</a>, 
<a href="/search/cs?searchtype=author&query=Teso%2C+S">Stefano Teso</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Neuro-Symbolic (NeSy) predictors that conform to symbolic knowledge -
encoding, e.g., safety constraints - can be affected by Reasoning Shortcuts
(RSs): They learn concepts consistent with the symbolic knowledge by exploiting
unintended semantics. RSs compromise reliability and generalization and, as we
show in this paper, they are linked to NeSy models being overconfident about
the predicted concepts. Unfortunately, the only trustworthy mitigation strategy
requires collecting costly dense supervision over the concepts. Rather than
attempting to avoid RSs altogether, we propose to ensure NeSy models are aware
of the semantic ambiguity of the concepts they learn, thus enabling their users
to identify and distrust low-quality concepts. Starting from three simple
desiderata, we derive bears (BE Aware of Reasoning Shortcuts), an ensembling
technique that calibrates the model's concept-level confidence without
compromising prediction accuracy, thus encouraging NeSy architectures to be
uncertain about concepts affected by RSs. We show empirically that bears
improves RS-awareness of several state-of-the-art NeSy models, and also
facilitates acquiring informative dense annotations for mitigation purposes.
</p>
</div>
</dd>
<dt><a name="item685">[685]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12241" title="Abstract">arXiv:2402.12241</a> [<a href="/pdf/2402.12241" title="Download PDF">pdf</a>, <a href="/format/2402.12241" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence of Gradient Descent for Recurrent Neural Networks: A  Nonasymptotic Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cayci%2C+S">Semih Cayci</a>, 
<a href="/search/cs?searchtype=author&query=Eryilmaz%2C+A">Atilla Eryilmaz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
<p class="mathjax">We analyze recurrent neural networks trained with gradient descent in the
supervised learning setting for dynamical systems, and prove that gradient
descent can achieve optimality \emph{without} massive overparameterization. Our
in-depth nonasymptotic analysis (i) provides sharp bounds on the network size
$m$ and iteration complexity $\tau$ in terms of the sequence length $T$, sample
size $n$ and ambient dimension $d$, and (ii) identifies the significant impact
of long-term dependencies in the dynamical system on the convergence and
network width bounds characterized by a cutoff point that depends on the
Lipschitz continuity of the activation function. Remarkably, this analysis
reveals that an appropriately-initialized recurrent neural network trained with
$n$ samples can achieve optimality with a network size $m$ that scales only
logarithmically with $n$. This sharply contrasts with the prior works that
require high-order polynomial dependency of $m$ on $n$ to establish strong
regularity conditions. Our results are based on an explicit characterization of
the class of dynamical systems that can be approximated and learned by
recurrent neural networks via norm-constrained transportation mappings, and
establishing local smoothness properties of the hidden state with respect to
the learnable parameters.
</p>
</div>
</dd>
<dt><a name="item686">[686]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12242" title="Abstract">arXiv:2402.12242</a> [<a href="/pdf/2402.12242" title="Download PDF">pdf</a>, <a href="/format/2402.12242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthetic location trajectory generation using categorical diffusion  models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dirmeier%2C+S">Simon Dirmeier</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+Y">Ye Hong</a>, 
<a href="/search/cs?searchtype=author&query=Perez-Cruz%2C+F">Fernando Perez-Cruz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">Diffusion probabilistic models (DPMs) have rapidly evolved to be one of the
predominant generative models for the simulation of synthetic data, for
instance, for computer vision, audio, natural language processing, or
biomolecule generation. Here, we propose using DPMs for the generation of
synthetic individual location trajectories (ILTs) which are sequences of
variables representing physical locations visited by individuals. ILTs are of
major importance in mobility research to understand the mobility behavior of
populations and to ultimately inform political decision-making. We represent
ILTs as multi-dimensional categorical random variables and propose to model
their joint distribution using a continuous DPM by first applying the diffusion
process in a continuous unconstrained space and then mapping the continuous
variables into a discrete space. We demonstrate that our model can synthesize
realistic ILPs by comparing conditionally and unconditionally generated
sequences to real-world ILPs from a GNSS tracking data set which suggests the
potential use of our model for synthetic data generation, for example, for
benchmarking models used in mobility research.
</p>
</div>
</dd>
<dt><a name="item687">[687]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12243" title="Abstract">arXiv:2402.12243</a> [<a href="/pdf/2402.12243" title="Download PDF">pdf</a>, <a href="/format/2402.12243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Effects of Noise in Text-to-SQL: An Examination of the  BIRD-Bench Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wretblad%2C+N">Niklas Wretblad</a>, 
<a href="/search/cs?searchtype=author&query=Riseby%2C+F+G">Fredrik Gordh Riseby</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+R">Rahul Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Ahmadi%2C+A">Amin Ahmadi</a>, 
<a href="/search/cs?searchtype=author&query=Holmstr%C3%B6m%2C+O">Oskar Holmstr&#xf6;m</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Text-to-SQL, which involves translating natural language into Structured
Query Language (SQL), is crucial for enabling broad access to structured
databases without expert knowledge. However, designing models for such tasks is
challenging due to numerous factors, including the presence of 'noise,' such as
ambiguous questions and syntactical errors. This study provides an in-depth
analysis of the distribution and types of noise in the widely used BIRD-Bench
benchmark and the impact of noise on models. While BIRD-Bench was created to
model dirty and noisy database values, it was not created to contain noise and
errors in the questions and gold queries. We found that noise in questions and
gold queries are prevalent in the dataset, with varying amounts across domains,
and with an uneven distribution between noise types. The presence of incorrect
gold SQL queries, which then generate incorrect gold answers, has a significant
impact on the benchmark's reliability. Surprisingly, when evaluating models on
corrected SQL queries, zero-shot baselines surpassed the performance of
state-of-the-art prompting methods. We conclude that informative noise labels
and reliable benchmarks are crucial to developing new Text-to-SQL methods that
can handle varying types of noise.
</p>
</div>
</dd>
<dt><a name="item688">[688]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12245" title="Abstract">arXiv:2402.12245</a> [<a href="/pdf/2402.12245" title="Download PDF">pdf</a>, <a href="/format/2402.12245" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constrained Boundary Labeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Depian%2C+T">Thomas Depian</a>, 
<a href="/search/cs?searchtype=author&query=N%C3%B6llenburg%2C+M">Martin N&#xf6;llenburg</a>, 
<a href="/search/cs?searchtype=author&query=Terziadis%2C+S">Soeren Terziadis</a>, 
<a href="/search/cs?searchtype=author&query=Wallinger%2C+M">Markus Wallinger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">Boundary labeling is a technique used to label dense sets of feature points
in an illustration. It involves placing labels along a rectangular boundary box
and connecting each label with its corresponding feature using non-crossing
leader lines. Although boundary labeling is well-studied, semantic constraints
on the labels have not been investigated thoroughly. In this paper, we consider
grouping and ordering constraints for boundary labeling: Grouping constraints
enforce that all labels in a group are placed consecutively on the boundary,
and ordering constraints enforce a partial order over the labels. We show that
finding an admissible labeling for labels of uniform size that can be placed on
fixed candidate positions on two opposite sides of the boundary is NP-complete.
Furthermore, we show that it is also weakly NP-hard to find an admissible
labeling for non-uniform labels that can slide along one side of the boundary.
However, we obtain polynomial-time algorithms in the one-sided setting for
either fixed candidate positions or uniform-height labels. Finally, we
experimentally confirm that our approach has also practical relevance.
</p>
</div>
</dd>
<dt><a name="item689">[689]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12248" title="Abstract">arXiv:2402.12248</a> [<a href="/pdf/2402.12248" title="Download PDF">pdf</a>, <a href="/format/2402.12248" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A high-order, fully well-balanced, unconditionally positivity-preserving  finite volume framework for flood simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ciallella%2C+M">Mirco Ciallella</a>, 
<a href="/search/math?searchtype=author&query=Micalizzi%2C+L">Lorenzo Micalizzi</a>, 
<a href="/search/math?searchtype=author&query=Michel-Dansac%2C+V">Victor Michel-Dansac</a>, 
<a href="/search/math?searchtype=author&query=%C3%96ffner%2C+P">Philipp &#xd6;ffner</a>, 
<a href="/search/math?searchtype=author&query=Torlo%2C+D">Davide Torlo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">In this work, we present a high-order finite volume framework for the
numerical simulation of shallow water flows. The method is designed to
accurately capture complex dynamics inherent in shallow water systems,
particularly suited for applications such as tsunami simulations. The
arbitrarily high-order framework ensures precise representation of flow
behaviors, crucial for simulating phenomena characterized by rapid changes and
fine-scale features. Thanks to an {\it ad-hoc} reformulation in terms of
production-destruction terms, the time integration ensures positivity
preservation without any time-step restrictions, a vital attribute for physical
consistency, especially in scenarios where negative water depth reconstructions
could lead to unrealistic results. In order to introduce the preservation of
general steady equilibria dictated by the underlying balance law, the
high-order reconstruction and numerical flux are blended in a convex fashion
with a well-balanced approximation, which is able to provide exact preservation
of both static and moving equilibria. Through numerical experiments, we
demonstrate the effectiveness and robustness of the proposed approach in
capturing the intricate dynamics of shallow water flows, while preserving key
physical properties essential for flood simulations.
</p>
</div>
</dd>
<dt><a name="item690">[690]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12249" title="Abstract">arXiv:2402.12249</a> [<a href="/pdf/2402.12249" title="Download PDF">pdf</a>, <a href="/format/2402.12249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Levenshtein Transformer&#x27;s Decoder and Its Variants
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+R">Ruiyang Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Levenshtein transformer (LevT) is a non-autoregressive machine translation
model with high decoding efficiency and comparable translation quality in terms
of bleu score, due to its parallel decoding and iterative refinement procedure.
Are there any deficiencies of its translations and what improvements could be
made? In this report, we focus on LevT's decoder and analyse the decoding
results length, subword generation, and deletion module's capability. We hope
to identify weaknesses of the decoder for future improvements.
<br />We also compare translations of the original LevT, knowledge-distilled LevT,
LevT with translation memory, and the KD-LevT with translation memory to see
how KD and translation memory can help.
</p>
</div>
</dd>
<dt><a name="item691">[691]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12250" title="Abstract">arXiv:2402.12250</a> [<a href="/pdf/2402.12250" title="Download PDF">pdf</a>, <a href="/format/2402.12250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Derivative-Free iterative One-Step Reconstruction for Multispectral CT
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Prohaszka%2C+T">Thomas Prohaszka</a>, 
<a href="/search/math?searchtype=author&query=Neumann%2C+L">Lukas Neumann</a>, 
<a href="/search/math?searchtype=author&query=Haltmeier%2C+M">Markus Haltmeier</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">Image reconstruction in Multispectral Computed Tomography (MSCT) requires
solving a challenging nonlinear inverse problem, commonly tackled via iterative
optimization algorithms. Existing methods necessitate computing the derivative
of the forward map and potentially its regularized inverse. In this work, we
present a simple yet highly effective algorithm for MSCT image reconstruction,
utilizing iterative update mechanisms that leverage the full forward model in
the forward step and a derivative-free adjoint problem. Our approach
demonstrates both fast convergence and superior performance compared to
existing algorithms, making it an interesting candidate for future work. We
also discuss further generalizations of our method and its combination with
additional regularization and other data discrepancy terms.
</p>
</div>
</dd>
<dt><a name="item692">[692]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12252" title="Abstract">arXiv:2402.12252</a> [<a href="/pdf/2402.12252" title="Download PDF">pdf</a>, <a href="/format/2402.12252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Interview Study on Third-Party Cyber Threat Hunting Processes in the  U.S. Department of Homeland Security
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maxam%2C+W+P">William P. Maxam III</a>, 
<a href="/search/cs?searchtype=author&query=Davis%2C+J+C">James C. Davis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical report accompanying a paper at USENIX Security 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
<p class="mathjax">Cybersecurity is a major challenge for large organizations. Traditional
cybersecurity defense is reactive. Cybersecurity operations centers keep out
adversaries and incident response teams clean up after break-ins. Recently a
proactive stage has been introduced: Cyber Threat Hunting (TH) looks for
potential compromises missed by other cyber defenses. TH is mandated for
federal executive agencies and government contractors. As threat hunting is a
new cybersecurity discipline, most TH teams operate without a defined process.
The practices and challenges of TH have not yet been documented.
<br />To address this gap, this paper describes the first interview study of threat
hunt practitioners. We obtained access and interviewed 11 threat hunters
associated with the U.S. government's Department of Homeland Security.
Hour-long interviews were conducted. We analyzed the transcripts with process
and thematic coding.We describe the diversity among their processes, show that
their processes differ from the TH processes reported in the literature, and
unify our subjects' descriptions into a single TH process.We enumerate common
TH challenges and solutions according to the subjects. The two most common
challenges were difficulty in assessing a Threat Hunter's expertise, and
developing and maintaining automation. We conclude with recommendations for TH
teams (improve planning, focus on automation, and apprentice new members) and
highlight directions for future work (finding a TH process that balances
flexibility and formalism, and identifying assessments for TH team
performance).
</p>
</div>
</dd>
<dt><a name="item693">[693]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12255" title="Abstract">arXiv:2402.12255</a> [<a href="/pdf/2402.12255" title="Download PDF">pdf</a>, <a href="/format/2402.12255" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shallow Synthesis of Knowledge in GPT-Generated Texts: A Case Study in  Automatic Related Work Composition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Martin-Boyle%2C+A">Anna Martin-Boyle</a>, 
<a href="/search/cs?searchtype=author&query=Tyagi%2C+A">Aahan Tyagi</a>, 
<a href="/search/cs?searchtype=author&query=Hearst%2C+M+A">Marti A. Hearst</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dongyeop Kang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 5 figures, submitted to ACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Numerous AI-assisted scholarly applications have been developed to aid
different stages of the research process. We present an analysis of AI-assisted
scholarly writing generated with ScholaCite, a tool we built that is designed
for organizing literature and composing Related Work sections for academic
papers. Our evaluation method focuses on the analysis of citation graphs to
assess the structural complexity and inter-connectedness of citations in texts
and involves a three-way comparison between (1) original human-written texts,
(2) purely GPT-generated texts, and (3) human-AI collaborative texts. We find
that GPT-4 can generate reasonable coarse-grained citation groupings to support
human users in brainstorming, but fails to perform detailed synthesis of
related works without human intervention. We suggest that future writing
assistant tools should not be used to draft text independently of the human
author.
</p>
</div>
</dd>
<dt><a name="item694">[694]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12259" title="Abstract">arXiv:2402.12259</a> [<a href="/pdf/2402.12259" title="Download PDF">pdf</a>, <a href="/format/2402.12259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with  Queryable Objects and Open-Set Relationships
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koch%2C+S">Sebastian Koch</a>, 
<a href="/search/cs?searchtype=author&query=Vaskevicius%2C+N">Narunas Vaskevicius</a>, 
<a href="/search/cs?searchtype=author&query=Colosi%2C+M">Mirco Colosi</a>, 
<a href="/search/cs?searchtype=author&query=Hermosilla%2C+P">Pedro Hermosilla</a>, 
<a href="/search/cs?searchtype=author&query=Ropinski%2C+T">Timo Ropinski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://kochsebastian.com/open3dsg">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Current approaches for 3D scene graph prediction rely on labeled datasets to
train models for a fixed set of known object classes and relationship
categories. We present Open3DSG, an alternative approach to learn 3D scene
graph prediction in an open world without requiring labeled scene graph data.
We co-embed the features from a 3D scene graph prediction backbone with the
feature space of powerful open world 2D vision language foundation models. This
enables us to predict 3D scene graphs from 3D point clouds in a zero-shot
manner by querying object classes from an open vocabulary and predicting the
inter-object relationships from a grounded LLM with scene graph features and
queried object classes as context. Open3DSG is the first 3D point cloud method
to predict not only explicit open-vocabulary object classes, but also open-set
relationships that are not limited to a predefined label set, making it
possible to express rare as well as specific objects and relationships in the
predicted 3D scene graph. Our experiments show that Open3DSG is effective at
predicting arbitrary object classes as well as their complex inter-object
relationships describing spatial, supportive, semantic and comparative
relationships.
</p>
</div>
</dd>
<dt><a name="item695">[695]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12260" title="Abstract">arXiv:2402.12260</a> [<a href="/pdf/2402.12260" title="Download PDF">pdf</a>, <a href="/format/2402.12260" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Non-orthogonal Age-Optimal Information Dissemination in Vehicular  Networks: A Meta Multi-Objective Reinforcement Learning Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Habob%2C+A+A">A. A. Habob</a>, 
<a href="/search/cs?searchtype=author&query=Tabassum%2C+H">H. Tabassum</a>, 
<a href="/search/cs?searchtype=author&query=Waqar%2C+O">O. Waqar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">This paper considers minimizing the age-of-information (AoI) and transmit
power consumption in a vehicular network, where a roadside unit (RSU) provides
timely updates about a set of physical processes to vehicles. We consider
non-orthogonal multi-modal information dissemination, which is based on
superposed message transmission from RSU and successive interference
cancellation (SIC) at vehicles. The formulated problem is a multi-objective
mixed-integer nonlinear programming problem; thus, a Pareto-optimal front is
very challenging to obtain. First, we leverage the weighted-sum approach to
decompose the multi-objective problem into a set of multiple single-objective
sub-problems corresponding to each predefined objective preference weight.
Then, we develop a hybrid deep Q-network (DQN)-deep deterministic policy
gradient (DDPG) model to solve each optimization sub-problem respective to
predefined objective-preference weight. The DQN optimizes the decoding order,
while the DDPG solves the continuous power allocation. The model needs to be
retrained for each sub-problem. We then present a two-stage
meta-multi-objective reinforcement learning solution to estimate the Pareto
front with a few fine-tuning update steps without retraining the model for each
sub-problem. Simulation results illustrate the efficacy of the proposed
solutions compared to the existing benchmarks and that the meta-multi-objective
reinforcement learning model estimates a high-quality Pareto frontier with
reduced training time.
</p>
</div>
</dd>
<dt><a name="item696">[696]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12261" title="Abstract">arXiv:2402.12261</a> [<a href="/pdf/2402.12261" title="Download PDF">pdf</a>, <a href="/format/2402.12261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NEO-BENCH: Evaluating Robustness of Large Language Models with  Neologisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+J">Jonathan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Ritter%2C+A">Alan Ritter</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The performance of Large Language Models (LLMs) degrades from the temporal
drift between data used for model training and newer text seen during
inference. One understudied avenue of language change causing data drift is the
emergence of neologisms -- new word forms -- over time. We create a diverse
resource of recent English neologisms by using several popular collection
methods. We analyze temporal drift using neologisms by comparing sentences
containing new words with near-identical sentences that replace neologisms with
existing substitute words. Model performance is nearly halved in machine
translation when a single neologism is introduced in a sentence. Motivated by
these results, we construct a benchmark to evaluate LLMs' ability to generalize
to neologisms with various natural language understanding tasks and model
perplexity. Models with later knowledge cutoff dates yield lower perplexities
and perform better in downstream tasks. LLMs are also affected differently
based on the linguistic origins of words, indicating that neologisms are
complex for static LLMs to address. We will release our benchmark and code for
reproducing our experiments.
</p>
</div>
</dd>
<dt><a name="item697">[697]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12263" title="Abstract">arXiv:2402.12263</a> [<a href="/pdf/2402.12263" title="Download PDF">pdf</a>, <a href="/format/2402.12263" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a tailored mixed-precision sub-8bit quantization scheme for  Gated Recurrent Units using Genetic Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miccini%2C+R">Riccardo Miccini</a>, 
<a href="/search/cs?searchtype=author&query=Cerioli%2C+A">Alessandro Cerioli</a>, 
<a href="/search/cs?searchtype=author&query=Laroche%2C+C">Cl&#xe9;ment Laroche</a>, 
<a href="/search/cs?searchtype=author&query=Piechowiak%2C+T">Tobias Piechowiak</a>, 
<a href="/search/cs?searchtype=author&query=Spars%C3%B8%2C+J">Jens Spars&#xf8;</a>, 
<a href="/search/cs?searchtype=author&query=Pezzarossa%2C+L">Luca Pezzarossa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a full paper by the TinyML Research Symposium 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE); Signal Processing (eess.SP)

</div>
<p class="mathjax">Despite the recent advances in model compression techniques for deep neural
networks, deploying such models on ultra-low-power embedded devices still
proves challenging. In particular, quantization schemes for Gated Recurrent
Units (GRU) are difficult to tune due to their dependence on an internal state,
preventing them from fully benefiting from sub-8bit quantization. In this work,
we propose a modular integer quantization scheme for GRUs where the bit width
of each operator can be selected independently. We then employ Genetic
Algorithms (GA) to explore the vast search space of possible bit widths,
simultaneously optimising for model size and accuracy. We evaluate our methods
on four different sequential tasks and demonstrate that mixed-precision
solutions exceed homogeneous-precision ones in terms of Pareto efficiency. In
our results, we achieve a model size reduction between 25% and 55% while
maintaining an accuracy comparable with the 8-bit homogeneous equivalent.
</p>
</div>
</dd>
<dt><a name="item698">[698]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12264" title="Abstract">arXiv:2402.12264</a> [<a href="/pdf/2402.12264" title="Download PDF">pdf</a>, <a href="/format/2402.12264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Uncertainty quantification in fine-tuned LLMs using LoRA ensembles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balabanov%2C+O">Oleksandr Balabanov</a>, 
<a href="/search/cs?searchtype=author&query=Linander%2C+H">Hampus Linander</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (stat.ML)

</div>
<p class="mathjax">Fine-tuning large language models can improve task specific performance,
although a general understanding of what the fine-tuned model has learned,
forgotten and how to trust its predictions is still missing. We derive
principled uncertainty quantification for fine-tuned LLMs with posterior
approximations using computationally efficient low-rank adaptation ensembles.
We analyze three common multiple-choice datasets using low-rank adaptation
ensembles based on Mistral-7b, and draw quantitative and qualitative
conclusions on their perceived complexity and model efficacy on the different
target domains during and after fine-tuning. In particular, backed by the
numerical experiments, we hypothesise about signals from entropic uncertainty
measures for data domains that are inherently difficult for a given
architecture to learn.
</p>
</div>
</dd>
<dt><a name="item699">[699]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12265" title="Abstract">arXiv:2402.12265</a> [<a href="/pdf/2402.12265" title="Download PDF">pdf</a>, <a href="/format/2402.12265" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Byzantine-Resilience of Distillation-Based Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roux%2C+C">Christophe Roux</a>, 
<a href="/search/cs?searchtype=author&query=Zimmer%2C+M">Max Zimmer</a>, 
<a href="/search/cs?searchtype=author&query=Pokutta%2C+S">Sebastian Pokutta</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
<p class="mathjax">Federated Learning (FL) algorithms using Knowledge Distillation (KD) have
received increasing attention due to their favorable properties with respect to
privacy, non-i.i.d. data and communication cost. These methods depart from
transmitting model parameters and, instead, communicate information about a
learning task by sharing predictions on a public dataset. In this work, we
study the performance of such approaches in the byzantine setting, where a
subset of the clients act in an adversarial manner aiming to disrupt the
learning process. We show that KD-based FL algorithms are remarkably resilient
and analyze how byzantine clients can influence the learning process compared
to Federated Averaging. Based on these insights, we introduce two new byzantine
attacks and demonstrate that they are effective against prior
byzantine-resilient methods. Additionally, we propose FilterExp, a novel method
designed to enhance the byzantine resilience of KD-based FL algorithms and
demonstrate its efficacy. Finally, we provide a general method to make attacks
harder to detect, improving their effectiveness.
</p>
</div>
</dd>
<dt><a name="item700">[700]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12267" title="Abstract">arXiv:2402.12267</a> [<a href="/pdf/2402.12267" title="Download PDF">pdf</a>, <a href="/format/2402.12267" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> High-quality Data-to-Text Generation for Severely Under-Resourced  Languages with Out-of-the-box Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lorandi%2C+M">Michela Lorandi</a>, 
<a href="/search/cs?searchtype=author&query=Belz%2C+A">Anya Belz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The performance of NLP methods for severely under-resourced languages cannot
currently hope to match the state of the art in NLP methods for well resourced
languages. We explore the extent to which pretrained large language models
(LLMs) can bridge this gap, via the example of data-to-text generation for
Irish, Welsh, Breton and Maltese. We test LLMs on these under-resourced
languages and English, in a range of scenarios. We find that LLMs easily set
the state of the art for the under-resourced languages by substantial margins,
as measured by both automatic and human evaluations. For all our languages,
human evaluation shows on-a-par performance with humans for our best systems,
but BLEU scores collapse compared to English, casting doubt on the metric's
suitability for evaluating non-task-specific systems. Overall, our results
demonstrate the great potential of LLMs to bridge the performance gap for
under-resourced languages.
</p>
</div>
</dd>
<dt><a name="item701">[701]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12269" title="Abstract">arXiv:2402.12269</a> [<a href="/pdf/2402.12269" title="Download PDF">pdf</a>, <a href="/format/2402.12269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> End-to-end Supervised Prediction of Arbitrary-size Graphs with  Partially-Masked Fused Gromov-Wasserstein Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Krzakala%2C+P">Paul Krzakala</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Junjie Yang</a>, 
<a href="/search/cs?searchtype=author&query=Flamary%2C+R">R&#xe9;mi Flamary</a>, 
<a href="/search/cs?searchtype=author&query=Buc%2C+F+d">Florence d&#x27;Alch&#xe9; Buc</a>, 
<a href="/search/cs?searchtype=author&query=Laclau%2C+C">Charlotte Laclau</a>, 
<a href="/search/cs?searchtype=author&query=Labeau%2C+M">Matthieu Labeau</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
<p class="mathjax">We present a novel end-to-end deep learning-based approach for Supervised
Graph Prediction (SGP). We introduce an original Optimal Transport (OT)-based
loss, the Partially-Masked Fused Gromov-Wasserstein loss (PM-FGW), that allows
to directly leverage graph representations such as adjacency and feature
matrices. PM-FGW exhibits all the desirable properties for SGP: it is node
permutation invariant, sub-differentiable and handles graphs of different sizes
by comparing their padded representations as well as their masking vectors.
Moreover, we present a flexible transformer-based architecture that easily
adapts to different types of input data. In the experimental section, three
different tasks, a novel and challenging synthetic dataset (image2graph) and
two real-world tasks, image2map and fingerprint2molecule - showcase the
efficiency and versatility of the approach compared to competitors.
</p>
</div>
</dd>
<dt><a name="item702">[702]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12271" title="Abstract">arXiv:2402.12271</a> [<a href="/pdf/2402.12271" title="Download PDF">pdf</a>, <a href="/format/2402.12271" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Secure Federated Learning Across Heterogeneous Cloud and  High-Performance Computing Resources -- A Case Study on Federated Fine-tuning  of LLaMA 2
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zilinghan Li</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shilan He</a>, 
<a href="/search/cs?searchtype=author&query=Chaturvedi%2C+P">Pranshu Chaturvedi</a>, 
<a href="/search/cs?searchtype=author&query=Kindratenko%2C+V">Volodymyr Kindratenko</a>, 
<a href="/search/cs?searchtype=author&query=Huerta%2C+E+A">Eliu A Huerta</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K">Kibaek Kim</a>, 
<a href="/search/cs?searchtype=author&query=Madduri%2C+R">Ravi Madduri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Federated learning enables multiple data owners to collaboratively train
robust machine learning models without transferring large or sensitive local
datasets by only sharing the parameters of the locally trained models. In this
paper, we elaborate on the design of our Advanced Privacy-Preserving Federated
Learning (APPFL) framework, which streamlines end-to-end secure and reliable
federated learning experiments across cloud computing facilities and
high-performance computing resources by leveraging Globus Compute, a
distributed function as a service platform, and Amazon Web Services. We further
demonstrate the use case of APPFL in fine-tuning a LLaMA 2 7B model using
several cloud resources and supercomputers.
</p>
</div>
</dd>
<dt><a name="item703">[703]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12272" title="Abstract">arXiv:2402.12272</a> [<a href="/pdf/2402.12272" title="Download PDF">pdf</a>, <a href="/ps/2402.12272" title="Download PostScript">ps</a>, <a href="/format/2402.12272" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Persian News Agencies on Instagram, A Words Co-occurrence  Graph-based Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heydari%2C+M">Mohammad Heydari</a>, 
<a href="/search/cs?searchtype=author&query=Teimourpour%2C+B">Babak Teimourpour</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 13 figures, International Journal of Web Research (IJWR)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">The rise of the Internet and the exponential increase in data have made
manual data summarization and analysis a challenging task. Instagram social
network is a prominent social network widely utilized in Iran for information
sharing and communication across various age groups. The inherent structure of
Instagram, characterized by its text-rich content and graph-like data
representation, enables the utilization of text and graph processing techniques
for data analysis purposes. The degree distributions of these networks exhibit
scale-free characteristics, indicating non-random growth patterns. Recently,
word co-occurrence has gained attention from researchers across multiple
disciplines due to its simplicity and practicality. Keyword extraction is a
crucial task in natural language processing. In this study, we demonstrated
that high-precision extraction of keywords from Instagram posts in the Persian
language can be achieved using unsupervised word co-occurrence methods without
resorting to conventional techniques such as clustering or pre-trained models.
After graph visualization and community detection, it was observed that the top
topics covered by news agencies are represented by these graphs. This approach
is generalizable to new and diverse datasets and can provide acceptable outputs
for new data. To the author's knowledge, this method has not been employed in
the Persian language before on Instagram social network. The new crawled data
has been publicly released on GitHub for exploration by other researchers. By
employing this method, it is possible to use other graph-based algorithms, such
as community detections. The results help us to identify the key role of
different news agencies in information diffusion among the public, identify
hidden communities, and discover latent patterns among a massive amount of
data.
</p>
</div>
</dd>
<dt><a name="item704">[704]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12274" title="Abstract">arXiv:2402.12274</a> [<a href="/pdf/2402.12274" title="Download PDF">pdf</a>, <a href="/format/2402.12274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Designing and Prototyping Extensions to MPI in MPICH
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Raffenetti%2C+K">Ken Raffenetti</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Y">Yanfei Guo</a>, 
<a href="/search/cs?searchtype=author&query=Gillis%2C+T">Thomas Gillis</a>, 
<a href="/search/cs?searchtype=author&query=Latham%2C+R">Robert Latham</a>, 
<a href="/search/cs?searchtype=author&query=Thakur%2C+R">Rajeev Thakur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages. Submitted IJHPCA special issue
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
<p class="mathjax">As HPC system architectures and the applications running on them continue to
evolve, the MPI standard itself must evolve. The trend in current and future
HPC systems toward powerful nodes with multiple CPU cores and multiple GPU
accelerators makes efficient support for hybrid programming critical for
applications to achieve high performance. However, the support for hybrid
programming in the MPI standard has not kept up with recent trends. The MPICH
implementation of MPI provides a platform for implementing and experimenting
with new proposals and extensions to fill this gap and to gain valuable
experience and feedback before the MPI Forum can consider them for
standardization. In this work, we detail six extensions implemented in MPICH to
increase MPI interoperability with other runtimes, with a specific focus on
heterogeneous architectures. First, the extension to MPI generalized requests
lets applications integrate asynchronous tasks into MPI's progress engine.
Second, the iovec extension to datatypes lets applications use MPI datatypes as
a general-purpose data layout API beyond just MPI communications. Third, a new
MPI object, MPIX stream, can be used by applications to identify execution
contexts beyond MPI processes, including threads and GPU streams. MPIX stream
communicators can be created to make existing MPI functions thread-aware and
GPU-aware, thus providing applications with explicit ways to achieve higher
performance. Fourth, MPIX Streams are extended to support the enqueue semantics
for offloading MPI communications onto a GPU stream context. Fifth, thread
communicators allow MPI communicators to be constructed with individual
threads, thus providing a new level of interoperability between MPI and on-node
runtimes such as OpenMP. Lastly, we present an extension to invoke MPI
progress, which lets users spawn progress threads with fine-grained control.
</p>
</div>
</dd>
<dt><a name="item705">[705]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12275" title="Abstract">arXiv:2402.12275</a> [<a href="/pdf/2402.12275" title="Download PDF">pdf</a>, <a href="/format/2402.12275" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WorldCoder, a Model-Based LLM Agent: Building World Models by Writing  Code and Interacting with the Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Hao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Key%2C+D">Darren Key</a>, 
<a href="/search/cs?searchtype=author&query=Ellis%2C+K">Kevin Ellis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
<p class="mathjax">We give a model-based agent that builds a Python program representing its
knowledge of the world based on its interactions with the environment. The
world model tries to explain its interactions, while also being optimistic
about what reward it can achieve. We do this by extending work on program
synthesis via LLMs. We study our agent on gridworlds, finding our approach is
more sample-efficient compared to deep RL, and more compute-efficient compared
to ReAct-style agents.
</p>
</div>
</dd>
<dt><a name="item706">[706]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12276" title="Abstract">arXiv:2402.12276</a> [<a href="/pdf/2402.12276" title="Download PDF">pdf</a>, <a href="/format/2402.12276" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explain then Rank: Scale Calibration of Neural Rankers Using Natural  Language Explanations from Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+P">Puxuan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Cohen%2C+D">Daniel Cohen</a>, 
<a href="/search/cs?searchtype=author&query=Lamba%2C+H">Hemank Lamba</a>, 
<a href="/search/cs?searchtype=author&query=Tetreault%2C+J">Joel Tetreault</a>, 
<a href="/search/cs?searchtype=author&query=Jaimes%2C+A">Alex Jaimes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
<p class="mathjax">The process of scale calibration in ranking systems involves adjusting the
outputs of rankers to correspond with significant qualities like click-through
rates or relevance, crucial for mirroring real-world value and thereby boosting
the system's effectiveness and reliability. Although there has been research on
calibrated ranking losses within learning-to-rank models, the particular issue
of adjusting the scale for neural rankers, which excel in handling textual
information, has not been thoroughly examined. Neural ranking models are adept
at processing text data, yet the application of existing scale calibration
techniques to these models poses significant challenges due to their complexity
and the intensive training they require, often resulting in suboptimal
outcomes.
<br />This study delves into the potential of large language models (LLMs) to
provide uncertainty measurements for a query and document pair that correlate
with the scale-calibrated scores. By employing Monte Carlo sampling to gauge
relevance probabilities from LLMs and incorporating natural language
explanations (NLEs) to articulate this uncertainty, we carry out comprehensive
tests on two major document ranking datasets. Our findings reveal that the
approach leveraging NLEs outperforms existing calibration methods under various
training scenarios, leading to better calibrated neural rankers.
</p>
</div>
</dd>
<dt><a name="item707">[707]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12279" title="Abstract">arXiv:2402.12279</a> [<a href="/pdf/2402.12279" title="Download PDF">pdf</a>, <a href="/format/2402.12279" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Key ingredients for effective zero-shot cross-lingual knowledge transfer  in generative tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chirkova%2C+N">Nadezhda Chirkova</a>, 
<a href="/search/cs?searchtype=author&query=Nikoulina%2C+V">Vassilina Nikoulina</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2310.09917">arXiv:2310.09917</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Zero-shot cross-lingual generation implies finetuning of the multilingual
pretrained language model on a generation task in one language and then using
it to make predictions for this task in other languages. Previous works notice
a frequent problem of generation in a wrong language and propose approaches to
address it, usually using mT5 as a backbone model. In this work we compare
various approaches proposed from the literature in unified settings, also
including alternative backbone models, namely mBART and NLLB-200. We first
underline the importance of tuning learning rate used for finetuning, which
helps to substantially alleviate the problem of generation in the wrong
language. Then, we show that with careful learning rate tuning, the simple full
finetuning of the model acts as a very strong baseline and alternative
approaches bring only marginal improvements. Finally, we find that mBART
performs similarly to mT5 of the same size, and NLLB-200 can be competitive in
some cases. Our final models reach the performance of the approach based on
data translation which is usually considered as an upper baseline for zero-shot
cross-lingual generation.
</p>
</div>
</dd>
<dt><a name="item708">[708]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12280" title="Abstract">arXiv:2402.12280</a> [<a href="/pdf/2402.12280" title="Download PDF">pdf</a>, <a href="/format/2402.12280" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Skeleton Graph Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+S">Shuowei Jin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yongji Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Haizhong Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qingzhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lentz%2C+M">Matthew Lentz</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Z+M">Z. Morley Mao</a>, 
<a href="/search/cs?searchtype=author&query=Prakash%2C+A">Atul Prakash</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+F">Feng Qian</a>, 
<a href="/search/cs?searchtype=author&query=Zhuo%2C+D">Danyang Zhuo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Large language models (LLMs) have seen significant adoption for natural
language tasks, owing their success to massive numbers of model parameters
(e.g., 70B+); however, LLM inference incurs significant computation and memory
costs. Recent approaches propose parallel decoding strategies, such as
Skeleton-of-Thought (SoT), to improve performance by breaking prompts down into
sub-problems that can be decoded in parallel; however, they often suffer from
reduced response quality. Our key insight is that we can request additional
information, specifically dependencies and difficulty, when generating the
sub-problems to improve both response quality and performance. In this paper,
we propose Skeleton Graph Decoding (SGD), which uses dependencies exposed
between sub-problems to support information forwarding between dependent
sub-problems for improved quality while exposing parallelization opportunities
for decoding independent sub-problems. Additionally, we leverage difficulty
estimates for each sub-problem to select an appropriately-sized model,
improving performance without significantly reducing quality. Compared to
standard autoregressive generation and SoT, SGD achieves a 1.69x speedup while
improving quality by up to 51%.
</p>
</div>
</dd>
<dt><a name="item709">[709]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12281" title="Abstract">arXiv:2402.12281</a> [<a href="/pdf/2402.12281" title="Download PDF">pdf</a>, <a href="/ps/2402.12281" title="Download PostScript">ps</a>, <a href="/format/2402.12281" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Challenges and Experiences of Iranian Developers with MLOps at  Enterprise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heydari%2C+M">Mohammad Heydari</a>, 
<a href="/search/cs?searchtype=author&query=Rezvani%2C+Z">Zahra Rezvani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 14 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
<p class="mathjax">Data is becoming more complex, and so are the approaches designed to process
it. Enterprises have access to more data than ever, but many still struggle to
glean the full potential of insights from what they have. This research
explores the challenges and experiences of Iranian developers in implementing
the MLOps paradigm within enterprise settings. MLOps, or Machine Learning
Operations, is a discipline focused on automating the continuous delivery of
machine learning models. In this study, we review the most popular MLOps tools
used by leading technology enterprises. Additionally, we present the results of
a questionnaire answered by over 110 Iranian Machine Learning experts and
Software Developers, shedding light on MLOps tools and the primary obstacles
faced. The findings reveal that data quality problems, a lack of resources, and
difficulties in model deployment are among the primary challenges faced by
practitioners. Collaboration between ML, DevOps, Ops, and Science teams is seen
as a pivotal challenge in implementing MLOps effectively.
</p>
</div>
</dd>
<dt><a name="item710">[710]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12282" title="Abstract">arXiv:2402.12282</a> [<a href="/pdf/2402.12282" title="Download PDF">pdf</a>, <a href="/format/2402.12282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ontology Enhanced Claim Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=H%C3%BCs%C3%BCnbeyi%2C+Z+M">Zehra Melce H&#xfc;s&#xfc;nbeyi</a>, 
<a href="/search/cs?searchtype=author&query=Scheffler%2C+T">Tatjana Scheffler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to defactify workshop at AAAI, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">We propose an ontology enhanced model for sentence based claim detection. We
fused ontology embeddings from a knowledge base with BERT sentence embeddings
to perform claim detection for the ClaimBuster and the NewsClaims datasets. Our
ontology enhanced approach showed the best results with these small-sized
unbalanced datasets, compared to other statistical and neural machine learning
models. The experiments demonstrate that adding domain specific features
(either trained word embeddings or knowledge graph metadata) can improve
traditional ML methods. In addition, adding domain knowledge in the form of
ontology embeddings helps avoid the bias encountered in neural network based
models, for example the pure BERT model bias towards larger classes in our
small corpus.
</p>
</div>
</dd>
<dt><a name="item711">[711]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12284" title="Abstract">arXiv:2402.12284</a> [<a href="/pdf/2402.12284" title="Download PDF">pdf</a>, <a href="/format/2402.12284" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Refining Minimax Regret for Unsupervised Environment Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Beukman%2C+M">Michael Beukman</a>, 
<a href="/search/cs?searchtype=author&query=Coward%2C+S">Samuel Coward</a>, 
<a href="/search/cs?searchtype=author&query=Matthews%2C+M">Michael Matthews</a>, 
<a href="/search/cs?searchtype=author&query=Fellows%2C+M">Mattie Fellows</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Minqi Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Dennis%2C+M">Michael Dennis</a>, 
<a href="/search/cs?searchtype=author&query=Foerster%2C+J">Jakob Foerster</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">In unsupervised environment design, reinforcement learning agents are trained
on environment configurations (levels) generated by an adversary that maximises
some objective. Regret is a commonly used objective that theoretically results
in a minimax regret (MMR) policy with desirable robustness guarantees; in
particular, the agent's maximum regret is bounded. However, once the agent
reaches this regret bound on all levels, the adversary will only sample levels
where regret cannot be further reduced. Although there are possible performance
improvements to be made outside of these regret-maximising levels, learning
stagnates. In this work, we introduce Bayesian level-perfect MMR (BLP), a
refinement of the minimax regret objective that overcomes this limitation. We
formally show that solving for this objective results in a subset of MMR
policies, and that BLP policies act consistently with a Perfect Bayesian policy
over all levels. We further introduce an algorithm, ReMiDi, that results in a
BLP policy at convergence. We empirically demonstrate that training on levels
from a minimax regret adversary causes learning to prematurely stagnate, but
that ReMiDi continues learning.
</p>
</div>
</dd>
<dt><a name="item712">[712]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12285" title="Abstract">arXiv:2402.12285</a> [<a href="/pdf/2402.12285" title="Download PDF">pdf</a>, <a href="/format/2402.12285" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Capturing the Shape of a Point Set with a Line-Segment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=van+Beusekom%2C+N">Nathan van Beusekom</a>, 
<a href="/search/cs?searchtype=author&query=van+Kreveld%2C+M">Marc van Kreveld</a>, 
<a href="/search/cs?searchtype=author&query=van+Mulken%2C+M">Max van Mulken</a>, 
<a href="/search/cs?searchtype=author&query=Roeloffzen%2C+M">Marcel Roeloffzen</a>, 
<a href="/search/cs?searchtype=author&query=Speckmann%2C+B">Bettina Speckmann</a>, 
<a href="/search/cs?searchtype=author&query=Wulms%2C+J">Jules Wulms</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">Detecting location-correlated groups in point sets is an important task in a
wide variety of applications areas. In addition to merely detecting such
groups, the group's shape carries meaning as well. In this paper, we represent
a group's shape using a simple geometric object, a line-segment. Specifically,
given a radius $r$, we say a line-segment is shape-representing of a point set
$P$ if it is within Hausdorff distance $r$ of each point $p \in P$. We aim to
find the shortest such line-segment. This problem is equivalent to stabbing a
set of circles of radius $r$ using the shortest line-segment. We describe an
algorithm to find the shortest shape-representing line-segment in $O(n \log h +
h^2)$ time. The algorithm is loosely based on the rotating calipers algorithm.
</p>
</div>
</dd>
<dt><a name="item713">[713]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12289" title="Abstract">arXiv:2402.12289</a> [<a href="/pdf/2402.12289" title="Download PDF">pdf</a>, <a href="/format/2402.12289" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DriveVLM: The Convergence of Autonomous Driving and Large  Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+X">Xiaoyu Tian</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Junru Gu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bailin Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yicheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+C">Chenxu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+K">Kun Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+P">Peng Jia</a>, 
<a href="/search/cs?searchtype=author&query=Lang%2C+X">Xianpeng Lang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Hang Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://tsinghua-mars-lab.github.io/DriveVLM/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">A primary hurdle of autonomous driving in urban environments is understanding
complex and long-tail scenarios, such as challenging road conditions and
delicate human behaviors. We introduce DriveVLM, an autonomous driving system
leveraging Vision-Language Models (VLMs) for enhanced scene understanding and
planning capabilities. DriveVLM integrates a unique combination of
chain-of-thought (CoT) modules for scene description, scene analysis, and
hierarchical planning. Furthermore, recognizing the limitations of VLMs in
spatial reasoning and heavy computational requirements, we propose
DriveVLM-Dual, a hybrid system that synergizes the strengths of DriveVLM with
the traditional autonomous driving pipeline. DriveVLM-Dual achieves robust
spatial understanding and real-time inference speed. Extensive experiments on
both the nuScenes dataset and our SUP-AD dataset demonstrate the effectiveness
of DriveVLM and the enhanced performance of DriveVLM-Dual, surpassing existing
methods in complex and unpredictable driving conditions.
</p>
</div>
</dd>
<dt><a name="item714">[714]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12291" title="Abstract">arXiv:2402.12291</a> [<a href="/pdf/2402.12291" title="Download PDF">pdf</a>, <a href="/format/2402.12291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KARL: Knowledge-Aware Retrieval and Representations aid Retention and  Learning in Students
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shu%2C+M">Matthew Shu</a>, 
<a href="/search/cs?searchtype=author&query=Balepur%2C+N">Nishant Balepur</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+S">Shi Feng</a>, 
<a href="/search/cs?searchtype=author&query=Boyd-Graber%2C+J">Jordan Boyd-Graber</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In-progress preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Flashcard schedulers are tools that rely on 1) student models to predict the
flashcards a student knows; and 2) teaching policies to schedule cards based on
these predictions. Existing student models, however, only use flashcard-level
features, like the student's past responses, ignoring the semantic ties of
flashcards. Deep Knowledge Tracing (DKT) models can capture semantic relations
with language models, but are inefficient, lack content-rich datasets for
evaluation, and require robust teaching policies. To address these issues, we
design KARL, a DKT-inspired student model that uses retrieval and BERT
embeddings for efficient and accurate student recall predictions. To test KARL,
we collect a new dataset of diverse study history on trivia questions. KARL
bests existing student models in AUC and calibration error. Finally, we propose
a novel teaching policy that exploits the predictive power of DKT models to
deploy KARL online. Based on 27 learners and 32 6-day study trajectories, KARL
shows the ability to enhance medium-term educational learning, proving its
efficacy for scheduling.
</p>
</div>
</dd>
<dt><a name="item715">[715]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12298" title="Abstract">arXiv:2402.12298</a> [<a href="/pdf/2402.12298" title="Download PDF">pdf</a>, <a href="/format/2402.12298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is Open-Source There Yet? A Comparative Study on Commercial and  Open-Source LLMs in Their Ability to Label Chest X-Ray Reports
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dorfner%2C+F+J">Felix J. Dorfner</a>, 
<a href="/search/cs?searchtype=author&query=J%C3%BCrgensen%2C+L">Liv J&#xfc;rgensen</a>, 
<a href="/search/cs?searchtype=author&query=Donle%2C+L">Leonhard Donle</a>, 
<a href="/search/cs?searchtype=author&query=Mohamad%2C+F+A">Fares Al Mohamad</a>, 
<a href="/search/cs?searchtype=author&query=Bodenmann%2C+T+R">Tobias R. Bodenmann</a>, 
<a href="/search/cs?searchtype=author&query=Cleveland%2C+M+C">Mason C. Cleveland</a>, 
<a href="/search/cs?searchtype=author&query=Busch%2C+F">Felix Busch</a>, 
<a href="/search/cs?searchtype=author&query=Adams%2C+L+C">Lisa C. Adams</a>, 
<a href="/search/cs?searchtype=author&query=Sato%2C+J">James Sato</a>, 
<a href="/search/cs?searchtype=author&query=Schultz%2C+T">Thomas Schultz</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+A+E">Albert E. Kim</a>, 
<a href="/search/cs?searchtype=author&query=Merkow%2C+J">Jameson Merkow</a>, 
<a href="/search/cs?searchtype=author&query=Bressem%2C+K+K">Keno K. Bressem</a>, 
<a href="/search/cs?searchtype=author&query=Bridge%2C+C+P">Christopher P. Bridge</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Introduction: With the rapid advances in large language models (LLMs), there
have been numerous new open source as well as commercial models. While recent
publications have explored GPT-4 in its application to extracting information
of interest from radiology reports, there has not been a real-world comparison
of GPT-4 to different leading open-source models.
<br />Materials and Methods: Two different and independent datasets were used. The
first dataset consists of 540 chest x-ray reports that were created at the
Massachusetts General Hospital between July 2019 and July 2021. The second
dataset consists of 500 chest x-ray reports from the ImaGenome dataset. We then
compared the commercial models GPT-3.5 Turbo and GPT-4 from OpenAI to the
open-source models Mistral-7B, Mixtral-8x7B, Llama2-13B, Llama2-70B,
QWEN1.5-72B and CheXbert and CheXpert-labeler in their ability to accurately
label the presence of multiple findings in x-ray text reports using different
prompting techniques.
<br />Results: On the ImaGenome dataset, the best performing open-source model was
Llama2-70B with micro F1-scores of 0.972 and 0.970 for zero- and few-shot
prompts, respectively. GPT-4 achieved micro F1-scores of 0.975 and 0.984,
respectively. On the institutional dataset, the best performing open-source
model was QWEN1.5-72B with micro F1-scores of 0.952 and 0.965 for zero- and
few-shot prompting, respectively. GPT-4 achieved micro F1-scores of 0.975 and
0.973, respectively.
<br />Conclusion: In this paper, we show that while GPT-4 is superior to
open-source models in zero-shot report labeling, the implementation of few-shot
prompting can bring open-source models on par with GPT-4. This shows that
open-source models could be a performant and privacy preserving alternative to
GPT-4 for the task of radiology report classification.
</p>
</div>
</dd>
<dt><a name="item716">[716]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12303" title="Abstract">arXiv:2402.12303</a> [<a href="/pdf/2402.12303" title="Download PDF">pdf</a>, <a href="/format/2402.12303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UncertaintyTrack: Exploiting Detection and Localization Uncertainty in  Multi-Object Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+C+W">Chang Won Lee</a>, 
<a href="/search/cs?searchtype=author&query=Waslander%2C+S+L">Steven L. Waslander</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Robotics (cs.RO)

</div>
<p class="mathjax">Multi-object tracking (MOT) methods have seen a significant boost in
performance recently, due to strong interest from the research community and
steadily improving object detection methods. The majority of tracking methods
follow the tracking-by-detection (TBD) paradigm, blindly trust the incoming
detections with no sense of their associated localization uncertainty. This
lack of uncertainty awareness poses a problem in safety-critical tasks such as
autonomous driving where passengers could be put at risk due to erroneous
detections that have propagated to downstream tasks, including MOT. While there
are existing works in probabilistic object detection that predict the
localization uncertainty around the boxes, no work in 2D MOT for autonomous
driving has studied whether these estimates are meaningful enough to be
leveraged effectively in object tracking. We introduce UncertaintyTrack, a
collection of extensions that can be applied to multiple TBD trackers to
account for localization uncertainty estimates from probabilistic object
detectors. Experiments on the Berkeley Deep Drive MOT dataset show that the
combination of our method and informative uncertainty estimates reduces the
number of ID switches by around 19\% and improves mMOTA by 2-3%. The source
code is available at https://github.com/TRAILab/UncertaintyTrack
</p>
</div>
</dd>
<dt><a name="item717">[717]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12304" title="Abstract">arXiv:2402.12304</a> [<a href="/pdf/2402.12304" title="Download PDF">pdf</a>, <a href="/format/2402.12304" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of the Picard-Newton iteration for the Navier-Stokes equations:  global stability and quadratic convergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Pollock%2C+S">Sara Pollock</a>, 
<a href="/search/math?searchtype=author&query=Rebholz%2C+L">Leo Rebholz</a>, 
<a href="/search/math?searchtype=author&query=Tu%2C+X">Xuemin Tu</a>, 
<a href="/search/math?searchtype=author&query=Xiao%2C+M">Menyging Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
<p class="mathjax">We analyze and test a simple-to-implement two-step iteration for the
incompressible Navier-Stokes equations that consists of first applying the
Picard iteration and then applying the Newton iteration to the Picard output.
We prove that this composition of Picard and Newton converges quadratically,
and our analysis (which covers both the unique solution and non-unique solution
cases) also suggests that this solver has a larger convergence basin than usual
Newton because of the improved stability properties of Picard-Newton over
Newton. Numerical tests show that Picard-Newton dramatically outperforms both
the Picard and Newton iterations, especially as the Reynolds number increases.
We also consider enhancing the Picard step with Anderson acceleration (AA), and
find that the AAPicard-Newton iteration has even better convergence properties
on several benchmark test problems.
</p>
</div>
</dd>
<dt><a name="item718">[718]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12307" title="Abstract">arXiv:2402.12307</a> [<a href="/pdf/2402.12307" title="Download PDF">pdf</a>, <a href="/format/2402.12307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-View Conformal Learning for Heterogeneous Sensor Fusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Garcia-Ceja%2C+E">Enrique Garcia-Ceja</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Being able to assess the confidence of individual predictions in machine
learning models is crucial for decision making scenarios. Specially, in
critical applications such as medical diagnosis, security, and unmanned
vehicles, to name a few. In the last years, complex predictive models have had
great success in solving hard tasks and new methods are being proposed every
day. While the majority of new developments in machine learning models focus on
improving the overall performance, less effort is put on assessing the
trustworthiness of individual predictions, and even to a lesser extent, in the
context of sensor fusion. To this end, we build and test multi-view and
single-view conformal models for heterogeneous sensor fusion. Our models
provide theoretical marginal confidence guarantees since they are based on the
conformal prediction framework. We also propose a multi-view semi-conformal
model based on sets intersection. Through comprehensive experimentation, we
show that multi-view models perform better than single-view models not only in
terms of accuracy-based performance metrics (as it has already been shown in
several previous works) but also in conformal measures that provide uncertainty
estimation. Our results also showed that multi-view models generate prediction
sets with less uncertainty compared to single-view models.
</p>
</div>
</dd>
<dt><a name="item719">[719]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12309" title="Abstract">arXiv:2402.12309</a> [<a href="/pdf/2402.12309" title="Download PDF">pdf</a>, <a href="/format/2402.12309" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TILP: Differentiable Learning of Temporal Logical Rules on Knowledge  Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+S">Siheng Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Fekri%2C+F">Faramarz Fekri</a>, 
<a href="/search/cs?searchtype=author&query=Kerce%2C+J+C">James Clayton Kerce</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2023 poster
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Compared with static knowledge graphs, temporal knowledge graphs (tKG), which
can capture the evolution and change of information over time, are more
realistic and general. However, due to the complexity that the notion of time
introduces to the learning of the rules, an accurate graph reasoning, e.g.,
predicting new links between entities, is still a difficult problem. In this
paper, we propose TILP, a differentiable framework for temporal logical rules
learning. By designing a constrained random walk mechanism and the introduction
of temporal operators, we ensure the efficiency of our model. We present
temporal features modeling in tKG, e.g., recurrence, temporal order, interval
between pair of relations, and duration, and incorporate it into our learning
process. We compare TILP with state-of-the-art methods on two benchmark
datasets. We show that our proposed framework can improve upon the performance
of baseline methods while providing interpretable results. In particular, we
consider various scenarios in which training samples are limited, data is
biased, and the time range between training and inference are different. In all
these cases, TILP works much better than the state-of-the-art methods.
</p>
</div>
</dd>
<dt><a name="item720">[720]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12315" title="Abstract">arXiv:2402.12315</a> [<a href="/pdf/2402.12315" title="Download PDF">pdf</a>, <a href="/ps/2402.12315" title="Download PostScript">ps</a>, <a href="/format/2402.12315" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cosserat Rod Modeling and Validation for a Soft Continuum Robot with  Self-Controllable Variable Curvature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinran Wang</a>, 
<a href="/search/cs?searchtype=author&query=Rojas%2C+N">Nicolas Rojas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for IEEE RoboSoft Conference 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
<p class="mathjax">This paper introduces a Cosserat rod based mathematical model for modeling a
self-controllable variable curvature soft continuum robot. This soft continuum
robot has a hollow inner channel and was developed with the ability to perform
variable curvature utilizing a growing spine. The growing spine is able to grow
and retract while modifies its stiffness through milli-size particle (glass
bubble) granular jamming. This soft continuum robot can then perform continuous
curvature variation, unlike previous approaches whose curvature variation is
discrete and depends on the number of locking mechanisms or manual
configurations. The robot poses an emergent modeling problem due to the
variable stiffness growing spine which is addressed in this paper. We
investigate the property of growing spine stiffness and incorporate it into the
Cosserat rod model by implementing a combined stiffness approach. We conduct
experiments with the soft continuum robot in various configurations and
compared the results with our developed mathematical model. The results show
that the mathematical model based on the adapted Cosserat rod matches the
experimental results with only a 3.3\% error with respect to the length of the
soft continuum robot.
</p>
</div>
</dd>
<dt><a name="item721">[721]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12317" title="Abstract">arXiv:2402.12317</a> [<a href="/pdf/2402.12317" title="Download PDF">pdf</a>, <a href="/format/2402.12317" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ARKS: Active Retrieval in Knowledge Soup for Code Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+H">Hongjin Su</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Shuyang Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Lai%2C+Y">Yuhang Lai</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haoyuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+B">Boao Shi</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Che Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+T">Tao Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Retrieval-augmented code generation
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Recently the retrieval-augmented generation (RAG) paradigm has raised much
attention for its potential in incorporating external knowledge into large
language models (LLMs) without further training. While widely explored in
natural language applications, its utilization in code generation remains
under-explored. In this paper, we introduce Active Retrieval in Knowledge Soup
(ARKS), an advanced strategy for generalizing large language models for code.
In contrast to relying on a single source, we construct a knowledge soup
integrating web search, documentation, execution feedback, and evolved code
snippets. We employ an active retrieval strategy that iteratively refines the
query and updates the knowledge soup. To assess the performance of ARKS, we
compile a new benchmark comprising realistic coding problems associated with
frequently updated libraries and long-tail programming languages. Experimental
results on ChatGPT and CodeLlama demonstrate a substantial improvement in the
average execution accuracy of ARKS on LLMs. The analysis confirms the
effectiveness of our proposed knowledge soup and active retrieval strategies,
offering rich insights into the construction of effective retrieval-augmented
code generation (RACG) pipelines. Our model, code, and data are available at
https://arks-codegen.github.io.
</p>
</div>
</dd>
<dt><a name="item722">[722]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12319" title="Abstract">arXiv:2402.12319</a> [<a href="/pdf/2402.12319" title="Download PDF">pdf</a>, <a href="/format/2402.12319" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Environment Responsive Online Meta-Learning with Fairness  Awareness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+C">Chen Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Mi%2C+F">Feng Mi</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xintao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+K">Kai Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Khan%2C+L">Latifur Khan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+F">Feng Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by TKDD, extended from KDD 2022. arXiv admin note: substantial text overlap with <a href="/abs/2205.11264">arXiv:2205.11264</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
<p class="mathjax">The fairness-aware online learning framework has emerged as a potent tool
within the context of continuous lifelong learning. In this scenario, the
learner's objective is to progressively acquire new tasks as they arrive over
time, while also guaranteeing statistical parity among various protected
sub-populations, such as race and gender, when it comes to the newly introduced
tasks. A significant limitation of current approaches lies in their heavy
reliance on the i.i.d (independent and identically distributed) assumption
concerning data, leading to a static regret analysis of the framework.
Nevertheless, it's crucial to note that achieving low static regret does not
necessarily translate to strong performance in dynamic environments
characterized by tasks sampled from diverse distributions. In this paper, to
tackle the fairness-aware online learning challenge in evolving settings, we
introduce a unique regret measure, FairSAR, by incorporating long-term fairness
constraints into a strongly adapted loss regret framework. Moreover, to
determine an optimal model parameter at each time step, we introduce an
innovative adaptive fairness-aware online meta-learning algorithm, referred to
as FairSAOML. This algorithm possesses the ability to adjust to dynamic
environments by effectively managing bias control and model accuracy. The
problem is framed as a bi-level convex-concave optimization, considering both
the model's primal and dual parameters, which pertain to its accuracy and
fairness attributes, respectively. Theoretical analysis yields sub-linear upper
bounds for both loss regret and the cumulative violation of fairness
constraints. Our experimental evaluation on various real-world datasets in
dynamic environments demonstrates that our proposed FairSAOML algorithm
consistently outperforms alternative approaches rooted in the most advanced
prior online learning methods.
</p>
</div>
</dd>
<dt><a name="item723">[723]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12320" title="Abstract">arXiv:2402.12320</a> [<a href="/pdf/2402.12320" title="Download PDF">pdf</a>, <a href="/format/2402.12320" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Landmark Stereo Dataset for Landmark Recognition and Moving Node  Localization in a Non-GPS Battlefield Environment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sapkota%2C+G">Ganesh Sapkota</a>, 
<a href="/search/cs?searchtype=author&query=Madria%2C+S">Sanjay Madria</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we have proposed a new strategy of using the landmark anchor
node instead of a radio-based anchor node to obtain the virtual coordinates
(landmarkID, DISTANCE) of moving troops or defense forces that will help in
tracking and maneuvering the troops along a safe path within a GPS-denied
battlefield environment. The proposed strategy implements landmark recognition
using the Yolov5 model and landmark distance estimation using an efficient
Stereo Matching Algorithm. We consider that a moving node carrying a low-power
mobile device facilitated with a calibrated stereo vision camera that captures
stereo images of a scene containing landmarks within the battlefield region
whose locations are stored in an offline server residing within the device
itself. We created a custom landmark image dataset called MSTLandmarkv1 with 34
landmark classes and another landmark stereo dataset of those 34 landmark
instances called MSTLandmarkStereov1. We trained the YOLOv5 model with
MSTLandmarkv1 dataset and achieved 0.95 mAP @ 0.5 IoU and 0.767 mAP @ [0.5:
0.95] IoU. We calculated the distance from a node to the landmark utilizing the
bounding box coordinates and the depth map generated by the improved SGM
algorithm using MSTLandmarkStereov1. The tuple of landmark IDs obtained from
the detection result and the distances calculated by the SGM algorithm are
stored as the virtual coordinates of a node. In future work, we will use these
virtual coordinates to obtain the location of a node using an efficient
trilateration algorithm and optimize the node position using the appropriate
optimization method.
</p>
</div>
</dd>
<dt><a name="item724">[724]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12326" title="Abstract">arXiv:2402.12326</a> [<a href="/pdf/2402.12326" title="Download PDF">pdf</a>, <a href="/format/2402.12326" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM Agents for Psychology: A Study on Gamified Assessments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qisen Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zekun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Honghui Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shenzhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pu%2C+Y">Yifan Pu</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenhao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shiji Song</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+G">Gao Huang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
<p class="mathjax">Psychological measurement is essential for mental health, self-understanding,
and personal development. Traditional methods, such as self-report scales and
psychologist interviews, often face challenges with engagement and
accessibility. While game-based and LLM-based tools have been explored to
improve user interest and automate assessment, they struggle to balance
engagement with generalizability. In this work, we propose PsychoGAT
(Psychological Game AgenTs) to achieve a generic gamification of psychological
assessment. The main insight is that powerful LLMs can function both as adept
psychologists and innovative game designers. By incorporating LLM agents into
designated roles and carefully managing their interactions, PsychoGAT can
transform any standardized scales into personalized and engaging interactive
fiction games. To validate the proposed method, we conduct psychometric
evaluations to assess its effectiveness and employ human evaluators to examine
the generated content across various psychological constructs, including
depression, cognitive distortions, and personality traits. Results demonstrate
that PsychoGAT serves as an effective assessment tool, achieving statistically
significant excellence in psychometric metrics such as reliability, convergent
validity, and discriminant validity. Moreover, human evaluations confirm
PsychoGAT's enhancements in content coherence, interactivity, interest,
immersion, and satisfaction.
</p>
</div>
</dd>
<dt><a name="item725">[725]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12327" title="Abstract">arXiv:2402.12327</a> [<a href="/pdf/2402.12327" title="Download PDF">pdf</a>, <a href="/format/2402.12327" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM  Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zengqing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Shuyuan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qianying Liu</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+B+I">Brian Inhyuk Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Onizuka%2C+M">Makoto Onizuka</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+S">Shaojie Tang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+R">Run Peng</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+C">Chuan Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Source codes available at <a href="https://github.com/wuzengqing001225/SABM_ShallWeTalk">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computers and Society (cs.CY); Multiagent Systems (cs.MA); General Economics (econ.GN)

</div>
<p class="mathjax">Recent advancements have shown that agents powered by large language models
(LLMs) possess capabilities to simulate human behaviors and societal dynamics.
However, the potential for LLM agents to spontaneously establish collaborative
relationships in the absence of explicit instructions has not been studied. To
address this gap, we conduct three case studies, revealing that LLM agents are
capable of spontaneously forming collaborations even within competitive
settings. This finding not only demonstrates the capacity of LLM agents to
mimic competition and cooperation in human societies but also validates a
promising vision of computational social science. Specifically, it suggests
that LLM agents could be utilized to model human social interactions, including
those with spontaneous collaborations, thus offering insights into social
phenomena. The source codes for this study are available at
https://github.com/wuzengqing001225/SABM_ShallWeTalk .
</p>
</div>
</dd>
<dt><a name="item726">[726]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12329" title="Abstract">arXiv:2402.12329</a> [<a href="/pdf/2402.12329" title="Download PDF">pdf</a>, <a href="/format/2402.12329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Query-Based Adversarial Prompt Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hayase%2C+J">Jonathan Hayase</a>, 
<a href="/search/cs?searchtype=author&query=Borevkovic%2C+E">Ema Borevkovic</a>, 
<a href="/search/cs?searchtype=author&query=Carlini%2C+N">Nicholas Carlini</a>, 
<a href="/search/cs?searchtype=author&query=Tram%C3%A8r%2C+F">Florian Tram&#xe8;r</a>, 
<a href="/search/cs?searchtype=author&query=Nasr%2C+M">Milad Nasr</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recent work has shown it is possible to construct adversarial examples that
cause an aligned language model to emit harmful strings or perform harmful
behavior. Existing attacks work either in the white-box setting (with full
access to the model weights), or through transferability: the phenomenon that
adversarial examples crafted on one model often remain effective on other
models. We improve on prior work with a query-based attack that leverages API
access to a remote language model to construct adversarial examples that cause
the model to emit harmful strings with (much) higher probability than with
transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety
classifier; we can cause GPT-3.5 to emit harmful strings that current transfer
attacks fail at, and we can evade the safety classifier with nearly 100%
probability.
</p>
</div>
</dd>
<dt><a name="item727">[727]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12331" title="Abstract">arXiv:2402.12331</a> [<a href="/pdf/2402.12331" title="Download PDF">pdf</a>, <a href="/format/2402.12331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generating Survival Interpretable Trajectories and Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Konstantinov%2C+A+V">Andrei V. Konstantinov</a>, 
<a href="/search/cs?searchtype=author&query=Kirpichenko%2C+S+R">Stanislav R. Kirpichenko</a>, 
<a href="/search/cs?searchtype=author&query=Utkin%2C+L+V">Lev V. Utkin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">A new model for generating survival trajectories and data based on applying
an autoencoder of a specific structure is proposed. It solves three tasks.
First, it provides predictions in the form of the expected event time and the
survival function for a new generated feature vector on the basis of the Beran
estimator. Second, the model generates additional data based on a given
training set that would supplement the original dataset. Third, the most
important, it generates a prototype time-dependent trajectory for an object,
which characterizes how features of the object could be changed to achieve a
different time to an event. The trajectory can be viewed as a type of the
counterfactual explanation. The proposed model is robust during training and
inference due to a specific weighting scheme incorporating into the variational
autoencoder. The model also determines the censored indicators of new generated
data by solving a classification task. The paper demonstrates the efficiency
and properties of the proposed model using numerical experiments on synthetic
and real datasets. The code of the algorithm implementing the proposed model is
publicly available.
</p>
</div>
</dd>
<dt><a name="item728">[728]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12332" title="Abstract">arXiv:2402.12332</a> [<a href="/pdf/2402.12332" title="Download PDF">pdf</a>, <a href="/format/2402.12332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Triple-Encoders: Representations That Fire Together, Wire Together
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Erker%2C+J">Justus-Jonas Erker</a>, 
<a href="/search/cs?searchtype=author&query=Mai%2C+F">Florian Mai</a>, 
<a href="/search/cs?searchtype=author&query=Reimers%2C+N">Nils Reimers</a>, 
<a href="/search/cs?searchtype=author&query=Spanakis%2C+G">Gerasimos Spanakis</a>, 
<a href="/search/cs?searchtype=author&query=Gurevych%2C+I">Iryna Gurevych</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> in Review at ACL Rolling Review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Search-based dialog models typically re-encode the dialog history at every
turn, incurring high cost. Curved Contrastive Learning, a representation
learning method that encodes relative distances between utterances into the
embedding space via a bi-encoder, has recently shown promising results for
dialog modeling at far superior efficiency. While high efficiency is achieved
through independently encoding utterances, this ignores the importance of
contextualization. To overcome this issue, this study introduces
triple-encoders, which efficiently compute distributed utterance mixtures from
these independently encoded utterances through a novel hebbian inspired
co-occurrence learning objective without using any weights. Empirically, we
find that triple-encoders lead to a substantial improvement over bi-encoders,
and even to better zero-shot generalization than single-vector representation
models without requiring re-encoding. Our code/model is publicly available.
</p>
</div>
</dd>
<dt><a name="item729">[729]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12336" title="Abstract">arXiv:2402.12336</a> [<a href="/pdf/2402.12336" title="Download PDF">pdf</a>, <a href="/format/2402.12336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings  for Robust Large Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schlarmann%2C+C">Christian Schlarmann</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+N+D">Naman Deep Singh</a>, 
<a href="/search/cs?searchtype=author&query=Croce%2C+F">Francesco Croce</a>, 
<a href="/search/cs?searchtype=author&query=Hein%2C+M">Matthias Hein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are
increasingly used for various real-world tasks. Prior work has shown that these
models are highly vulnerable to adversarial attacks on the vision modality.
These attacks can be leveraged to spread fake information or defraud users, and
thus pose a significant risk, which makes the robustness of large multi-modal
foundation models a pressing problem. The CLIP model, or one of its variants,
is used as a frozen vision encoder in many vision-language models (VLMs), e.g.
LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning
scheme to obtain a robust CLIP vision encoder, which yields robustness on all
vision down-stream tasks (VLMs, zero-shot classification) that rely on CLIP. In
particular, we show that stealth-attacks on users of VLMs by a malicious third
party providing manipulated images are no longer possible once one replaces the
original CLIP model with our robust one. No retraining or fine-tuning of the
VLM is required. The code and robust models are available at
https://github.com/chs20/RobustVLM
</p>
</div>
</dd>
<dt><a name="item730">[730]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12338" title="Abstract">arXiv:2402.12338</a> [<a href="/pdf/2402.12338" title="Download PDF">pdf</a>, <a href="/format/2402.12338" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Adversarial Approach to Evaluating the Robustness of Event  Identification Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bahwal%2C+O">Obai Bahwal</a>, 
<a href="/search/eess?searchtype=author&query=Kosut%2C+O">Oliver Kosut</a>, 
<a href="/search/eess?searchtype=author&query=Sankar%2C+L">Lalitha Sankar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Intelligent machine learning approaches are finding active use for event
detection and identification that allow real-time situational awareness. Yet,
such machine learning algorithms have been shown to be susceptible to
adversarial attacks on the incoming telemetry data. This paper considers a
physics-based modal decomposition method to extract features for event
classification and focuses on interpretable classifiers including logistic
regression and gradient boosting to distinguish two types of events: load loss
and generation loss. The resulting classifiers are then tested against an
adversarial algorithm to evaluate their robustness. The adversarial attack is
tested in two settings: the white box setting, wherein the attacker knows
exactly the classification model; and the gray box setting, wherein the
attacker has access to historical data from the same network as was used to
train the classifier, but does not know the classification model. Thorough
experiments on the synthetic South Carolina 500-bus system highlight that a
relatively simpler model such as logistic regression is more susceptible to
adversarial attacks than gradient boosting.
</p>
</div>
</dd>
<dt><a name="item731">[731]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12340" title="Abstract">arXiv:2402.12340</a> [<a href="/pdf/2402.12340" title="Download PDF">pdf</a>, <a href="/ps/2402.12340" title="Download PostScript">ps</a>, <a href="/format/2402.12340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Simple Mechanisms for Utility Maximization: Approximating Welfare in the  I.I.D. Unit-Demand Setting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Goldner%2C+K">Kira Goldner</a>, 
<a href="/search/cs?searchtype=author&query=Lundy%2C+T">Taylor Lundy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
<p class="mathjax">We investigate the objective of utility maximization from the perspective of
Bayesian mechanism design, initiating this direction, and focus on the
unit-demand setting where values are i.i.d. across both items and buyers. We
take the approach of developing simple, approximately optimal mechanisms,
targeting the simplest benchmark of optimal welfare. We give a
$(1-1/e)$-approximation when there are more items than buyers, and an
$O(\log(n/m))$-approximation when there are more buyers than items, which is
tight up to constant factors. We also characterize complexities in this setting
that defy our intuition from the welfare and revenue literature, and motivate
why coming up with a better benchmark than welfare is a hard problem itself.
</p>
</div>
</dd>
<dt><a name="item732">[732]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12343" title="Abstract">arXiv:2402.12343</a> [<a href="/pdf/2402.12343" title="Download PDF">pdf</a>, <a href="/format/2402.12343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emulated Disalignment: Safety Alignment for Large Language Models May  Backfire!
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zhanhui Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Z">Zhichen Dong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiaheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Large language models (LLMs) need to undergo safety alignment to ensure safe
conversations with humans. However, in this work, we introduce an
inference-time attack framework, demonstrating that safety alignment can also
unintentionally facilitate harmful outcomes under adversarial manipulation.
This framework, named Emulated Disalignment (ED), adversely combines a pair of
open-source pre-trained and safety-aligned language models in the output space
to produce a harmful language model without any training. Our experiments with
ED across three datasets and four model families (Llama-1, Llama-2, Mistral,
and Alpaca) show that ED doubles the harmfulness of pre-trained models and
outperforms strong baselines, achieving the highest harmful rate in 43 out of
48 evaluation subsets by a large margin. Crucially, our findings highlight the
importance of reevaluating the practice of open-sourcing language models even
after safety alignment.
</p>
</div>
</dd>
<dt><a name="item733">[733]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12348" title="Abstract">arXiv:2402.12348</a> [<a href="/pdf/2402.12348" title="Download PDF">pdf</a>, <a href="/format/2402.12348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via  Game-Theoretic Evaluations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Duan%2C+J">Jinhao Duan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Renming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Diffenderfer%2C+J">James Diffenderfer</a>, 
<a href="/search/cs?searchtype=author&query=Kailkhura%2C+B">Bhavya Kailkhura</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lichao Sun</a>, 
<a href="/search/cs?searchtype=author&query=Stengel-Eskin%2C+E">Elias Stengel-Eskin</a>, 
<a href="/search/cs?searchtype=author&query=Bansal%2C+M">Mohit Bansal</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+T">Tianlong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kaidi Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages; the first two authors contributed equally; GTBench HF Leaderboard: <a href="https://huggingface.co/spaces/GTBench/GTBench">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">As Large Language Models (LLMs) are integrated into critical real-world
applications, their strategic and logical reasoning abilities are increasingly
crucial. This paper evaluates LLMs' reasoning abilities in competitive
environments through game-theoretic tasks, e.g., board and card games that
require pure logic and strategic reasoning to compete with opponents. We first
propose GTBench, a language-driven environment composing 10 widely-recognized
tasks, across a comprehensive game taxonomy: complete versus incomplete
information, dynamic versus static, and probabilistic versus deterministic
scenarios. Then, we investigate two key problems: (1) Characterizing
game-theoretic reasoning of LLMs; (2) LLM-vs-LLM competitions as reasoning
evaluation. We observe that (1) LLMs have distinct behaviors regarding various
gaming scenarios; for example, LLMs fail in complete and deterministic games
yet they are competitive in probabilistic gaming scenarios; (2) Open-source
LLMs, e.g., CodeLlama-34b-Instruct, are less competitive than commercial LLMs,
e.g., GPT-4, in complex games. In addition, code-pretraining greatly benefits
strategic reasoning, while advanced reasoning methods such as Chain-of-Thought
(CoT) and Tree-of-Thought (ToT) do not always help. Detailed error profiles are
also provided for a better understanding of LLMs' behavior.
</p>
</div>
</dd>
<dt><a name="item734">[734]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12352" title="Abstract">arXiv:2402.12352</a> [<a href="/pdf/2402.12352" title="Download PDF">pdf</a>, <a href="/format/2402.12352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Delile%2C+J">Julien Delile</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+S">Srayanta Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Van+Pamel%2C+A">Anton Van Pamel</a>, 
<a href="/search/cs?searchtype=author&query=Zhukov%2C+L">Leonid Zhukov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
<p class="mathjax">Large language models (LLMs) are transforming the way information is
retrieved with vast amounts of knowledge being summarized and presented via
natural language conversations. Yet, LLMs are prone to highlight the most
frequently seen pieces of information from the training set and to neglect the
rare ones. In the field of biomedical research, latest discoveries are key to
academic and industrial actors and are obscured by the abundance of an
ever-increasing literature corpus (the information overload problem). Surfacing
new associations between biomedical entities, e.g., drugs, genes, diseases,
with LLMs becomes a challenge of capturing the long-tail knowledge of the
biomedical scientific production. To overcome this challenge, Retrieval
Augmented Generation (RAG) has been proposed to alleviate some of the
shortcomings of LLMs by augmenting the prompts with context retrieved from
external datasets. RAG methods typically select the context via maximum
similarity search over text embeddings. In this study, we show that RAG methods
leave out a significant proportion of relevant information due to clusters of
over-represented concepts in the biomedical literature. We introduce a novel
information-retrieval method that leverages a knowledge graph to downsample
these clusters and mitigate the information overload problem. Its retrieval
performance is about twice better than embedding similarity alternatives on
both precision and recall. Finally, we demonstrate that both embedding
similarity and knowledge graph retrieval methods can be advantageously combined
into a hybrid model that outperforms both, enabling potential improvements to
biomedical question-answering models.
</p>
</div>
</dd>
<dt><a name="item735">[735]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12354" title="Abstract">arXiv:2402.12354</a> [<a href="/pdf/2402.12354" title="Download PDF">pdf</a>, <a href="/format/2402.12354" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LoRA+: Efficient Low Rank Adaptation of Large Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hayou%2C+S">Soufiane Hayou</a>, 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+N">Nikhil Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bin Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (stat.ML)

</div>
<p class="mathjax">In this paper, we show that Low Rank Adaptation (LoRA) as originally
introduced in Hu et al. (2021) leads to suboptimal finetuning of models with
large width (embedding dimension). This is due to the fact that adapter
matrices A and B in LoRA are updated with the same learning rate. Using scaling
arguments for large width networks, we demonstrate that using the same learning
rate for A and B does not allow efficient feature learning. We then show that
this suboptimality of LoRA can be corrected simply by setting different
learning rates for the LoRA adapter matrices A and B with a well-chosen ratio.
We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$
improves performance (1-2 $\%$ improvements) and finetuning speed (up to $\sim$
2X SpeedUp), at the same computational cost as LoRA.
</p>
</div>
</dd>
<dt><a name="item736">[736]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12357" title="Abstract">arXiv:2402.12357</a> [<a href="/pdf/2402.12357" title="Download PDF">pdf</a>, <a href="/format/2402.12357" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Flip Graphs of Pseudo-Triangulations With Face Degree at Most 4
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=L%C3%B6ffler%2C+M">Maarten L&#xf6;ffler</a>, 
<a href="/search/cs?searchtype=author&query=Mchedlidze%2C+T">Tamara Mchedlidze</a>, 
<a href="/search/cs?searchtype=author&query=Orden%2C+D">David Orden</a>, 
<a href="/search/cs?searchtype=author&query=Tkadlec%2C+J">Josef Tkadlec</a>, 
<a href="/search/cs?searchtype=author&query=Wulms%2C+J">Jules Wulms</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">A pseudo-triangle is a simple polygon with exactly three convex vertices, and
all other vertices (if any) are distributed on three concave chains. A
pseudo-triangulation~$\mathcal{T}$ of a point set~$P$ in~$\mathbb{R}^2$ is a
partitioning of the convex hull of~$P$ into pseudo-triangles, such that the
union of the vertices of the pseudo-triangles is exactly~$P$. We call a size-4
pseudo-triangle a dart. For a fixed $k\geq 1$, we study $k$-dart
pseudo-triangulations ($k$-DPTs), that is, pseudo-triangulations in which
exactly $k$ faces are darts and all other faces are triangles. We study the
flip graph for such pseudo-triangulations, in which a flip exchanges the
diagonals of a pseudo-quadrilatral. Our results are as follows. We prove that
the flip graph of $1$-DPTs is generally not connected, and show how to compute
its connected components. Furthermore, for $k$-DPTs on a point configuration
called the double chain we analyze the structure of the flip graph on a more
fine-grained level.
</p>
</div>
</dd>
<dt><a name="item737">[737]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12360" title="Abstract">arXiv:2402.12360</a> [<a href="/pdf/2402.12360" title="Download PDF">pdf</a>, <a href="/ps/2402.12360" title="Download PostScript">ps</a>, <a href="/format/2402.12360" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonlinear Discrete-Time Observers with Physics-Informed Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Alvarez%2C+H+V">Hector Vargas Alvarez</a>, 
<a href="/search/math?searchtype=author&query=Fabiani%2C+G">Gianluca Fabiani</a>, 
<a href="/search/math?searchtype=author&query=Kevrekidis%2C+I+G">Ioannis G. Kevrekidis</a>, 
<a href="/search/math?searchtype=author&query=Kazantzis%2C+N">Nikolaos Kazantzis</a>, 
<a href="/search/math?searchtype=author&query=Siettos%2C+C">Constantinos Siettos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Artificial Intelligence (cs.AI); Dynamical Systems (math.DS)

</div>
<p class="mathjax">We use Physics-Informed Neural Networks (PINNs) to solve the discrete-time
nonlinear observer state estimation problem. Integrated within a single-step
exact observer linearization framework, the proposed PINN approach aims at
learning a nonlinear state transformation map by solving a system of
inhomogeneous functional equations. The performance of the proposed PINN
approach is assessed via two illustrative case studies for which the observer
linearizing transformation map can be derived analytically. We also perform an
uncertainty quantification analysis for the proposed PINN scheme and we compare
it with conventional power-series numerical implementations, which rely on the
computation of a power series solution.
</p>
</div>
</dd>
<dt><a name="item738">[738]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12363" title="Abstract">arXiv:2402.12363</a> [<a href="/pdf/2402.12363" title="Download PDF">pdf</a>, <a href="/format/2402.12363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emergent Word Order Universals from Cognitively-Motivated Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuribayashi%2C+T">Tatsuki Kuribayashi</a>, 
<a href="/search/cs?searchtype=author&query=Ueda%2C+R">Ryo Ueda</a>, 
<a href="/search/cs?searchtype=author&query=Yoshida%2C+R">Ryo Yoshida</a>, 
<a href="/search/cs?searchtype=author&query=Oseki%2C+Y">Yohei Oseki</a>, 
<a href="/search/cs?searchtype=author&query=Briscoe%2C+T">Ted Briscoe</a>, 
<a href="/search/cs?searchtype=author&query=Baldwin%2C+T">Timothy Baldwin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">The world's languages exhibit certain so-called typological or implicational
universals; for example, Subject-Object-Verb (SOV) word order typically employs
postpositions. Explaining the source of such biases is a key goal in
linguistics. We study the word-order universals through a computational
simulation with language models (LMs). Our experiments show that typologically
typical word orders tend to have lower perplexity estimated by LMs with
cognitively plausible biases: syntactic biases, specific parsing strategies,
and memory limitations. This suggests that the interplay of these cognitive
biases and predictability (perplexity) can explain many aspects of word-order
universals. This also showcases the advantage of cognitively-motivated LMs,
which are typically employed in cognitive modeling, in the computational
simulation of language universals.
</p>
</div>
</dd>
<dt><a name="item739">[739]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12364" title="Abstract">arXiv:2402.12364</a> [<a href="/pdf/2402.12364" title="Download PDF">pdf</a>, <a href="/format/2402.12364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Almost-linear time parameterized algorithm for rankwidth via dynamic  rankwidth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Korhonen%2C+T">Tuukka Korhonen</a>, 
<a href="/search/cs?searchtype=author&query=Soko%C5%82owski%2C+M">Marek Soko&#x142;owski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM); Combinatorics (math.CO)

</div>
<p class="mathjax">We give an algorithm that given a graph $G$ with $n$ vertices and $m$ edges
and an integer $k$, in time $O_k(n^{1+o(1)}) + O(m)$ either outputs a rank
decomposition of $G$ of width at most $k$ or determines that the rankwidth of
$G$ is larger than $k$; the $O_k(\cdot)$-notation hides factors depending on
$k$. Our algorithm returns also a $(2^{k+1}-1)$-expression for cliquewidth,
yielding a $(2^{k+1}-1)$-approximation algorithm for cliquewidth with the same
running time. This improves upon the $O_k(n^2)$ time algorithm of Fomin and
Korhonen [STOC 2022].
<br />The main ingredient of our algorithm is a fully dynamic algorithm for
maintaining rank decompositions of bounded width: We give a data structure that
for a dynamic $n$-vertex graph $G$ that is updated by edge insertions and
deletions maintains a rank decomposition of $G$ of width at most $4k$ under the
promise that the rankwidth of $G$ never grows above $k$. The amortized running
time of each update is $O_k(2^{\sqrt{\log n} \log \log n})$. The data structure
furthermore can maintain whether $G$ satisfies some fixed ${\sf CMSO}_1$
property within the same running time. We also give a framework for performing
``dense'' edge updates inside a given set of vertices $X$, where the new edges
inside $X$ are described by a given ${\sf CMSO}_1$ sentence and vertex labels,
in amortized $O_k(|X| \cdot 2^{\sqrt{\log n} \log \log n})$ time. Our dynamic
algorithm generalizes the dynamic treewidth algorithm of Korhonen, Majewski,
Nadara, Pilipczuk, and Soko{\l}owski [FOCS 2023].
</p>
</div>
</dd>
<dt><a name="item740">[740]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12365" title="Abstract">arXiv:2402.12365</a> [<a href="/pdf/2402.12365" title="Download PDF">pdf</a>, <a href="/format/2402.12365" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Universal Physics Transformers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alkin%2C+B">Benedikt Alkin</a>, 
<a href="/search/cs?searchtype=author&query=F%C3%BCrst%2C+A">Andreas F&#xfc;rst</a>, 
<a href="/search/cs?searchtype=author&query=Schmid%2C+S">Simon Schmid</a>, 
<a href="/search/cs?searchtype=author&query=Gruber%2C+L">Lukas Gruber</a>, 
<a href="/search/cs?searchtype=author&query=Holzleitner%2C+M">Markus Holzleitner</a>, 
<a href="/search/cs?searchtype=author&query=Brandstetter%2C+J">Johannes Brandstetter</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Fluid Dynamics (physics.flu-dyn)

</div>
<p class="mathjax">Deep neural network based surrogates for partial differential equations have
recently gained increased interest. However, akin to their numerical
counterparts, different techniques are used across applications, even if the
underlying dynamics of the systems are similar. A prominent example is the
Lagrangian and Eulerian specification in computational fluid dynamics, posing a
challenge for neural networks to effectively model particle- as opposed to
grid-based dynamics. We introduce Universal Physics Transformers (UPTs), a
novel learning paradigm which models a wide range of spatio-temporal problems -
both for Lagrangian and Eulerian discretization schemes. UPTs operate without
grid- or particle-based latent structures, enabling flexibility across meshes
and particles. UPTs efficiently propagate dynamics in the latent space,
emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for
queries of the latent space representation at any point in space-time. We
demonstrate the efficacy of UPTs in mesh-based fluid simulations, steady-state
Reynolds averaged Navier-Stokes simulations, and Lagrangian-based dynamics.
Project page: https://ml-jku.github.io/UPT
</p>
</div>
</dd>
<dt><a name="item741">[741]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12366" title="Abstract">arXiv:2402.12366</a> [<a href="/pdf/2402.12366" title="Download PDF">pdf</a>, <a href="/format/2402.12366" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Critical Evaluation of AI Feedback for Aligning Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Archit Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Keh%2C+S">Sedrick Keh</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+E">Eric Mitchell</a>, 
<a href="/search/cs?searchtype=author&query=Finn%2C+C">Chelsea Finn</a>, 
<a href="/search/cs?searchtype=author&query=Arora%2C+K">Kushal Arora</a>, 
<a href="/search/cs?searchtype=author&query=Kollar%2C+T">Thomas Kollar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">Reinforcement learning with AI feedback (RLAIF) is a popular paradigm for
improving the instruction-following abilities of powerful pre-trained language
models. RLAIF first performs supervised fine-tuning (SFT) using demonstrations
from a teacher model and then further fine-tunes the model with reinforcement
learning (RL), using feedback from a critic model. While recent popular
open-source models have demonstrated substantial improvements in performance
from the RL step, in this paper we question whether the complexity of this RL
step is truly warranted for AI feedback. We show that the improvements of the
RL step are virtually entirely due to the widespread practice of using a weaker
teacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g.,
GPT-4) used for AI feedback generation. Specifically, we show that simple
supervised fine-tuning with GPT-4 as the teacher outperforms existing RLAIF
pipelines. More generally, we find that the gains from RLAIF vary substantially
across base model families, test-time evaluation protocols, and critic models.
Finally, we provide a mechanistic explanation for when SFT may outperform the
full two-step RLAIF pipeline as well as suggestions for making RLAIF maximally
useful in practice.
</p>
</div>
</dd>
<dt><a name="item742">[742]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12368" title="Abstract">arXiv:2402.12368</a> [<a href="/pdf/2402.12368" title="Download PDF">pdf</a>, <a href="/format/2402.12368" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A synthetic data approach for domain generalization of NLI models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hosseini%2C+M+J">Mohammad Javad Hosseini</a>, 
<a href="/search/cs?searchtype=author&query=Petrov%2C+A">Andrey Petrov</a>, 
<a href="/search/cs?searchtype=author&query=Fabrikant%2C+A">Alex Fabrikant</a>, 
<a href="/search/cs?searchtype=author&query=Louis%2C+A">Annie Louis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">Natural Language Inference (NLI) remains an important benchmark task for
LLMs. NLI datasets are a springboard for transfer learning to other semantic
tasks, and NLI models are standard tools for identifying the faithfulness of
model-generated text. There are several large scale NLI datasets today, and
models have improved greatly by hill-climbing on these collections. Yet their
realistic performance on out-of-distribution/domain data is less
well-understood. We present an in-depth exploration of the problem of domain
generalization of NLI models. We demonstrate a new approach for generating
synthetic NLI data in diverse domains and lengths, so far not covered by
existing training sets. The resulting examples have meaningful premises, the
hypotheses are formed in creative ways rather than simple edits to a few
premise tokens, and the labels have high accuracy. We show that models trained
on this data ($685$K synthetic examples) have the best generalization to
completely new downstream test settings. On the TRUE benchmark, a T5-small
model trained with our data improves around $7\%$ on average compared to
training on the best alternative dataset. The improvements are more pronounced
for smaller models, while still meaningful on a T5 XXL model. We also
demonstrate gains on test sets when in-domain training data is augmented with
our domain-general synthetic data.
</p>
</div>
</dd>
<dt><a name="item743">[743]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12370" title="Abstract">arXiv:2402.12370</a> [<a href="/pdf/2402.12370" title="Download PDF">pdf</a>, <a href="/format/2402.12370" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AnaloBench: Benchmarking the Identification of Abstract and Long-context  Analogies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+X">Xiao Ye</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+A">Andrew Wang</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jacob Choi</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Y">Yining Lu</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+S">Shreya Sharma</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+L">Lingfeng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Tiyyala%2C+V">Vijay Tiyyala</a>, 
<a href="/search/cs?searchtype=author&query=Andrews%2C+N">Nicholas Andrews</a>, 
<a href="/search/cs?searchtype=author&query=Khashabi%2C+D">Daniel Khashabi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Humans regularly engage in analogical thinking, relating personal experiences
to current situations ($X$ is analogous to $Y$ because of $Z$). Analogical
thinking allows humans to solve problems in creative ways, grasp difficult
concepts, and articulate ideas more effectively. Can language models (LMs) do
the same? To answer this question, we propose ANALOBENCH, a benchmark to
determine analogical reasoning ability in LMs. Our benchmarking approach
focuses on aspects of this ability that are common among humans: (i) recalling
related experiences from a large amount of information, and (ii) applying
analogical reasoning to complex and lengthy scenarios. We test a broad
collection of proprietary models (e.g., GPT family, Claude V2) and open source
models such as LLaMA2. As in prior results, scaling up LMs results in some
performance boosts. Surprisingly, scale offers minimal gains when, (i)
analogies involve lengthy scenarios, or (ii) recalling relevant scenarios from
a large pool of information, a process analogous to finding a needle in a
haystack. We hope these observations encourage further research in this field.
</p>
</div>
</dd>
<dt><a name="item744">[744]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12371" title="Abstract">arXiv:2402.12371</a> [<a href="/pdf/2402.12371" title="Download PDF">pdf</a>, <a href="/format/2402.12371" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing Enclosing Depth
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=G%C3%A4rtner%2C+B">Bernd G&#xe4;rtner</a>, 
<a href="/search/cs?searchtype=author&query=Rasiti%2C+F">Fatime Rasiti</a>, 
<a href="/search/cs?searchtype=author&query=Schnider%2C+P">Patrick Schnider</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
<p class="mathjax">Enclosing depth is a recently introduced depth measure which gives a lower
bound to many depth measures studied in the literature. So far, enclosing depth
has only been studied from a combinatorial perspective. In this work, we give
the first algorithms to compute the enclosing depth of a query point with
respect to a data point set in any dimension. In the plane we are able to
optimize the algorithm to get a runtime of O(n log n). In constant dimension,
our algorithms still run in polynomial time.
</p>
</div>
</dd>
<dt><a name="item745">[745]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12372" title="Abstract">arXiv:2402.12372</a> [<a href="/pdf/2402.12372" title="Download PDF">pdf</a>, <a href="/format/2402.12372" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HunFlair2 in a cross-corpus evaluation of named entity recognition and  normalization tools
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=S%C3%A4nger%2C+M">Mario S&#xe4;nger</a>, 
<a href="/search/cs?searchtype=author&query=Garda%2C+S">Samuele Garda</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X+D">Xing David Wang</a>, 
<a href="/search/cs?searchtype=author&query=Weber-Genzel%2C+L">Leon Weber-Genzel</a>, 
<a href="/search/cs?searchtype=author&query=Droop%2C+P">Pia Droop</a>, 
<a href="/search/cs?searchtype=author&query=Fuchs%2C+B">Benedikt Fuchs</a>, 
<a href="/search/cs?searchtype=author&query=Akbik%2C+A">Alan Akbik</a>, 
<a href="/search/cs?searchtype=author&query=Leser%2C+U">Ulf Leser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">With the exponential growth of the life science literature, biomedical text
mining (BTM) has become an essential technology for accelerating the extraction
of insights from publications. Identifying named entities (e.g., diseases,
drugs, or genes) in texts and their linkage to reference knowledge bases are
crucial steps in BTM pipelines to enable information aggregation from different
documents. However, tools for these two steps are rarely applied in the same
context in which they were developed. Instead, they are applied in the wild,
i.e., on application-dependent text collections different from those used for
the tools' training, varying, e.g., in focus, genre, style, and text type. This
raises the question of whether the reported performance of BTM tools can be
trusted for downstream applications. Here, we report on the results of a
carefully designed cross-corpus benchmark for named entity extraction, where
tools were applied systematically to corpora not used during their training.
Based on a survey of 28 published systems, we selected five for an in-depth
analysis on three publicly available corpora encompassing four different entity
types. Comparison between tools results in a mixed picture and shows that, in a
cross-corpus setting, the performance is significantly lower than the one
reported in an in-corpus setting. HunFlair2 showed the best performance on
average, being closely followed by PubTator. Our results indicate that users of
BTM tools should expect diminishing performances when applying them in the wild
compared to original publications and show that further research is necessary
to make BTM tools more robust.
</p>
</div>
</dd>
<dt><a name="item746">[746]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12373" title="Abstract">arXiv:2402.12373</a> [<a href="/pdf/2402.12373" title="Download PDF">pdf</a>, <a href="/format/2402.12373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LTL learning on GPUs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Valizadeh%2C+M">Mojtaba Valizadeh</a>, 
<a href="/search/cs?searchtype=author&query=Fijalkow%2C+N">Nathana&#xeb;l Fijalkow</a>, 
<a href="/search/cs?searchtype=author&query=Berger%2C+M">Martin Berger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Programming Languages (cs.PL)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Linear temporal logic (LTL) is widely used in industrial verification. LTL
formulae can be learned from traces. Scaling LTL formula learning is an open
problem. We implement the first GPU-based LTL learner using a novel form of
enumerative program synthesis. The learner is sound and complete. Our
benchmarks indicate that it handles traces at least 2048 times more numerous,
and on average at least 46 times faster than existing state-of-the-art
learners. This is achieved with, among others, novel branch-free LTL semantics
that has $O(\log n)$ time complexity, where $n$ is trace length, while previous
implementations are $O(n^2)$ or worse (assuming bitwise boolean operations and
shifts by powers of 2 have unit costs -- a realistic assumption on modern
processors).
</p>
</div>
</dd>
<dt><a name="item747">[747]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12374" title="Abstract">arXiv:2402.12374</a> [<a href="/pdf/2402.12374" title="Download PDF">pdf</a>, <a href="/format/2402.12374" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuoming Chen</a>, 
<a href="/search/cs?searchtype=author&query=May%2C+A">Avner May</a>, 
<a href="/search/cs?searchtype=author&query=Svirschevski%2C+R">Ruslan Svirschevski</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Y">Yuhsun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Ryabinin%2C+M">Max Ryabinin</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Z">Zhihao Jia</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Beidi Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
<p class="mathjax">As the usage of large language models (LLMs) grows, performing efficient
inference with these models becomes increasingly important. While speculative
decoding has recently emerged as a promising direction for speeding up
inference, existing methods are limited in their ability to scale to larger
speculation budgets, and adapt to different hyperparameters and hardware. This
paper introduces Sequoia, a scalable, robust, and hardware-aware algorithm for
speculative decoding. To attain better scalability, Sequoia introduces a
dynamic programming algorithm to find the optimal tree structure for the
speculated tokens. To achieve robust speculative performance, Sequoia uses a
novel sampling and verification method that outperforms prior work across
different decoding temperatures. Finally, Sequoia introduces a hardware-aware
tree optimizer that maximizes speculative performance by automatically
selecting the token tree size and depth for a given hardware platform.
Evaluation shows that Sequoia improves the decoding speed of Llama2-7B,
Llama2-13B, and Vicuna-33B on an A100 by up to $4.04\times$, $3.84\times$, and
$2.37\times$, and Llama2-70B offloading by up to $10.33\times$ on L40.
</p>
</div>
</dd>
<dt><a name="item748">[748]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12376" title="Abstract">arXiv:2402.12376</a> [<a href="/pdf/2402.12376" title="Download PDF">pdf</a>, <a href="/format/2402.12376" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FiT: Flexible Vision Transformer for Diffusion Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zeyu Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zidong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+D">Di Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+C">Chengyue Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xihui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ouyang%2C+W">Wanli Ouyang</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+L">Lei Bai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">Nature is infinitely resolution-free. In the context of this reality,
existing diffusion models, such as Diffusion Transformers, often face
challenges when processing image resolutions outside of their trained domain.
To overcome this limitation, we present the Flexible Vision Transformer (FiT),
a transformer architecture specifically designed for generating images with
unrestricted resolutions and aspect ratios. Unlike traditional methods that
perceive images as static-resolution grids, FiT conceptualizes images as
sequences of dynamically-sized tokens. This perspective enables a flexible
training strategy that effortlessly adapts to diverse aspect ratios during both
training and inference phases, thus promoting resolution generalization and
eliminating biases induced by image cropping. Enhanced by a meticulously
adjusted network structure and the integration of training-free extrapolation
techniques, FiT exhibits remarkable flexibility in resolution extrapolation
generation. Comprehensive experiments demonstrate the exceptional performance
of FiT across a broad range of resolutions, showcasing its effectiveness both
within and beyond its training resolution distribution. Repository available at
https://github.com/whlzy/FiT.
</p>
</div>
</dd>
<dt><a name="item749">[749]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12377" title="Abstract">arXiv:2402.12377</a> [<a href="/pdf/2402.12377" title="Download PDF">pdf</a>, <a href="/format/2402.12377" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based  View Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Reiser%2C+C">Christian Reiser</a>, 
<a href="/search/cs?searchtype=author&query=Garbin%2C+S">Stephan Garbin</a>, 
<a href="/search/cs?searchtype=author&query=Srinivasan%2C+P+P">Pratul P. Srinivasan</a>, 
<a href="/search/cs?searchtype=author&query=Verbin%2C+D">Dor Verbin</a>, 
<a href="/search/cs?searchtype=author&query=Szeliski%2C+R">Richard Szeliski</a>, 
<a href="/search/cs?searchtype=author&query=Mildenhall%2C+B">Ben Mildenhall</a>, 
<a href="/search/cs?searchtype=author&query=Barron%2C+J+T">Jonathan T. Barron</a>, 
<a href="/search/cs?searchtype=author&query=Hedman%2C+P">Peter Hedman</a>, 
<a href="/search/cs?searchtype=author&query=Geiger%2C+A">Andreas Geiger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page at <a href="https://binary-opacity-grid.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
<p class="mathjax">While surface-based view synthesis algorithms are appealing due to their low
computational requirements, they often struggle to reproduce thin structures.
In contrast, more expensive methods that model the scene's geometry as a
volumetric density field (e.g. NeRF) excel at reconstructing fine geometric
detail. However, density fields often represent geometry in a "fuzzy" manner,
which hinders exact localization of the surface. In this work, we modify
density fields to encourage them to converge towards surfaces, without
compromising their ability to reconstruct thin structures. First, we employ a
discrete opacity grid representation instead of a continuous density field,
which allows opacity values to discontinuously transition from zero to one at
the surface. Second, we anti-alias by casting multiple rays per pixel, which
allows occlusion boundaries and subpixel structures to be modelled without
using semi-transparent voxels. Third, we minimize the binary entropy of the
opacity values, which facilitates the extraction of surface geometry by
encouraging opacity values to binarize towards the end of training. Lastly, we
develop a fusion-based meshing strategy followed by mesh simplification and
appearance model fitting. The compact meshes produced by our model can be
rendered in real-time on mobile devices and achieve significantly higher view
synthesis quality compared to existing mesh-based approaches.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Tue, 20 Feb 24</h3>
<dl>
<dt><a name="item750">[750]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04684" title="Abstract">arXiv:2402.04684</a> (cross-list from math.CO) [<a href="/pdf/2402.04684" title="Download PDF">pdf</a>, <a href="/ps/2402.04684" title="Download PostScript">ps</a>, <a href="/format/2402.04684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Parallel Summation Algorithm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+S">Shaoshi Chen</a>, 
<a href="/search/math?searchtype=author&query=Feng%2C+R">Ruyong Feng</a>, 
<a href="/search/math?searchtype=author&query=Kauers%2C+M">Manuel Kauers</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+X">Xiuyun Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Symbolic Computation (cs.SC)

</div>
<p class="mathjax">We propose a summation analog of the paradigm of parallel integration. Using
this paradigm, we make some first steps towards an indefinite summation
algorithm applicable to summands that rationally depend on the summation index
and a P-recursive sequence and its shifts. Under the assumption that the
corresponding difference field has no unnatural constants, we are able to
compute a bound on the normal part of the denominator of a potential closed
form. We can also handle the numerator. Our algorithm is incomplete so far as
we cannot predict the special part of the denominator. However, we do have some
structural results about special polynomials for the setting under
consideration.
</p>
</div>
</dd>
<dt><a name="item751">[751]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10649" title="Abstract">arXiv:2402.10649</a> (cross-list from math.NA) [<a href="/pdf/2402.10649" title="Download PDF">pdf</a>, <a href="/format/2402.10649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hermite Neural Network Simulation for Solving the 2D Schrodinger  Equation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Parand%2C+K">Kourosh Parand</a>, 
<a href="/search/math?searchtype=author&query=Pakniyat%2C+A">Aida Pakniyat</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">The Schrodinger equation is a mathematical equation describing the wave
function's behavior in a quantum-mechanical system. It is a partial
differential equation that provides valuable insights into the fundamental
principles of quantum mechanics. In this paper, the aim was to solve the
Schrodinger equation with sufficient accuracy by using a mixture of neural
networks with the collocation method base Hermite functions. Initially, the
Hermite functions roots were employed as collocation points, enhancing the
efficiency of the solution. The Schrodinger equation is defined in an infinite
domain, the use of Hermite functions as activation functions resulted in
excellent precision. Finally, the proposed method was simulated using MATLAB's
Simulink tool. The results were then compared with those obtained using
Physics-informed neural networks and the presented method.
</p>
</div>
</dd>
<dt><a name="item752">[752]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10972" title="Abstract">arXiv:2402.10972</a> (cross-list from q-bio.QM) [<a href="/pdf/2402.10972" title="Download PDF">pdf</a>, <a href="/format/2402.10972" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modeling methodology for the accurate and prompt prediction of  symptomatic events in chronic diseases
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Pag%C3%A1n%2C+J">Josu&#xe9; Pag&#xe1;n</a>, 
<a href="/search/q-bio?searchtype=author&query=Risco-Mart%C3%ADn%2C+J+L">Jos&#xe9; L. Risco-Mart&#xed;n</a>, 
<a href="/search/q-bio?searchtype=author&query=Moya%2C+J+M">Jos&#xe9; M. Moya</a>, 
<a href="/search/q-bio?searchtype=author&query=Ayala%2C+J+L">Jos&#xe9; L. Ayala</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Biomedical Informatics, 62, 2016
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Prediction of symptomatic crises in chronic diseases allows to take decisions
before the symptoms occur, such as the intake of drugs to avoid the symptoms or
the activation of medical alarms. The prediction horizon is in this case an
important parameter in order to fulfill the pharmacokinetics of medications, or
the time response of medical services. This paper presents a study about the
prediction limits of a chronic disease with symptomatic crises: the migraine.
For that purpose, this work develops a methodology to build predictive migraine
models and to improve these predictions beyond the limits of the initial
models. The maximum prediction horizon is analyzed, and its dependency on the
selected features is studied. A strategy for model selection is proposed to
tackle the trade off between conservative but robust predictive models, with
respect to less accurate predictions with higher horizons. The obtained results
show a prediction horizon close to 40 minutes, which is in the time range of
the drug pharmacokinetics. Experiments have been performed in a realistic
scenario where input data have been acquired in an ambulatory clinical study by
the deployment of a non-intrusive Wireless Body Sensor Network. Our results
provide an effective methodology for the selection of the future horizon in the
development of prediction algorithms for diseases experiencing symptomatic
crises.
</p>
</div>
</dd>
<dt><a name="item753">[753]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10975" title="Abstract">arXiv:2402.10975</a> (cross-list from math.OC) [<a href="/pdf/2402.10975" title="Download PDF">pdf</a>, <a href="/ps/2402.10975" title="Download PostScript">ps</a>, <a href="/format/2402.10975" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A System-Dynamic Based Simulation and Bayesian Optimization for  Inventory Management
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Maitra%2C+S">Sarit Maitra</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Computational Complexity (cs.CC)

</div>
<p class="mathjax">Inventory management is a fundamental challenge in supply chain management.
The challenge is compounded when the associated products have unpredictable
demands. This study proposes an innovative optimization approach combining
system-dynamic Monte-Carlo simulation and Bayesian optimization. The proposed
algorithm is tested with a real-life, unpredictable demand dataset to find the
optimal stock to meet the business objective. The findings show a considerable
improvement in inventory policy. This information is helpful for supply chain
analytics decision-making, which increases productivity and profitability. This
study further adds sensitivity analysis, considering the variation in demand
and expected output in profit percentage. This paper makes a substantial
contribution by presenting a simple yet robust approach to addressing the
fundamental difficulty of inventory management in a dynamic business
environment.
</p>
</div>
</dd>
<dt><a name="item754">[754]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10980" title="Abstract">arXiv:2402.10980</a> (cross-list from physics.chem-ph) [<a href="/pdf/2402.10980" title="Download PDF">pdf</a>, <a href="/format/2402.10980" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CHEMREASONER: Heuristic Search over a Large Language Model&#x27;s Knowledge  Space using Quantum-Chemical Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Sprueill%2C+H+W">Henry W. Sprueill</a>, 
<a href="/search/physics?searchtype=author&query=Edwards%2C+C">Carl Edwards</a>, 
<a href="/search/physics?searchtype=author&query=Agarwal%2C+K">Khushbu Agarwal</a>, 
<a href="/search/physics?searchtype=author&query=Olarte%2C+M+V">Mariefel V. Olarte</a>, 
<a href="/search/physics?searchtype=author&query=Sanyal%2C+U">Udishnu Sanyal</a>, 
<a href="/search/physics?searchtype=author&query=Johnston%2C+C">Conrad Johnston</a>, 
<a href="/search/physics?searchtype=author&query=Liu%2C+H">Hongbin Liu</a>, 
<a href="/search/physics?searchtype=author&query=Ji%2C+H">Heng Ji</a>, 
<a href="/search/physics?searchtype=author&query=Choudhury%2C+S">Sutanay Choudhury</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
<p class="mathjax">The discovery of new catalysts is essential for the design of new and more
efficient chemical processes in order to transition to a sustainable future. We
introduce an AI-guided computational screening framework unifying linguistic
reasoning with quantum-chemistry based feedback from 3D atomistic
representations. Our approach formulates catalyst discovery as an uncertain
environment where an agent actively searches for highly effective catalysts via
the iterative combination of large language model (LLM)-derived hypotheses and
atomistic graph neural network (GNN)-derived feedback. Identified catalysts in
intermediate search steps undergo structural evaluation based on spatial
orientation, reaction pathways, and stability. Scoring functions based on
adsorption energies and barriers steer the exploration in the LLM's knowledge
space toward energetically favorable, high-efficiency catalysts. We introduce
planning methods that automatically guide the exploration without human input,
providing competitive performance against expert-enumerated chemical
descriptor-based implementations. By integrating language-guided reasoning with
computational chemistry feedback, our work pioneers AI-accelerated, trustworthy
catalyst discovery.
</p>
</div>
</dd>
<dt><a name="item755">[755]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10988" title="Abstract">arXiv:2402.10988</a> (cross-list from quant-ph) [<a href="/pdf/2402.10988" title="Download PDF">pdf</a>, <a href="/ps/2402.10988" title="Download PostScript">ps</a>, <a href="/format/2402.10988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cryptography: Classical versus Post-Quantum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Awasthi%2C+A">Abhinav Awasthi</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chaturvedi%2C+A">Atul Chaturvedi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">The advantages of post-quantum cryptography over classical cryptography are
covered in this survey. We address several post-quantum cryptography
techniques. We conclude that the deployment of quantum-safe cryptographic
systems is anticipated to be the future of secure communication, and that the
development of post-quantum cryptography is essential to guarantee the security
of sensitive information in the post quantum era.
</p>
</div>
</dd>
<dt><a name="item756">[756]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11021" title="Abstract">arXiv:2402.11021</a> (cross-list from quant-ph) [<a href="/pdf/2402.11021" title="Download PDF">pdf</a>, <a href="/format/2402.11021" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TITAN: A Distributed Large-Scale Trapped-Ion NISQ Computer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Chu%2C+C">Cheng Chu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Fu%2C+Z">Zhenxiao Fu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Xu%2C+Y">Yilun Xu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Huang%2C+G">Gang Huang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Muller%2C+H">Hausi Muller</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chen%2C+F">Fan Chen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Jiang%2C+L">Lei Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Trapped-Ion (TI) technology offers potential breakthroughs for Noisy
Intermediate Scale Quantum (NISQ) computing. TI qubits offer extended coherence
times and high gate fidelity, making them appealing for large-scale NISQ
computers. Constructing such computers demands a distributed architecture
connecting Quantum Charge Coupled Devices (QCCDs) via quantum matter-links and
photonic switches. However, current distributed TI NISQ computers face hardware
and system challenges. Entangling qubits across a photonic switch introduces
significant latency, while existing compilers generate suboptimal mappings due
to their unawareness of the interconnection topology. In this paper, we
introduce TITAN, a large-scale distributed TI NISQ computer, which employs an
innovative photonic interconnection design to reduce entanglement latency and
an advanced partitioning and mapping algorithm to optimize matter-link
communications. Our evaluations show that TITAN greatly enhances quantum
application performance by 56.6% and fidelity by 19.7% compared to existing
systems.
</p>
</div>
</dd>
<dt><a name="item757">[757]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11027" title="Abstract">arXiv:2402.11027</a> (cross-list from quant-ph) [<a href="/pdf/2402.11027" title="Download PDF">pdf</a>, <a href="/format/2402.11027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MITS: A Quantum Sorcerer Stone For Designing Surface Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Chatterjee%2C+A">Avimita Chatterjee</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kundu%2C+D">Debarshi Kundu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Ghosh%2C+S">Swaroop Ghosh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">In the evolving landscape of quantum computing, determining the most
efficient parameters for Quantum Error Correction (QEC) is paramount. Various
quantum computers possess varied types and amounts of physical noise.
Traditionally, simulators operate in a forward paradigm, taking parameters such
as distance, rounds, and physical error to output a logical error rate.
However, usage of maximum distance and rounds of the surface code might waste
resources. To bridge this gap, we present MITS, a tool designed to
reverse-engineer the well-known simulator STIM for designing QEC codes. By
curating a comprehensive dataset from STIM using valid physical errors, MITS is
equipped to ascertain the optimal surface code parameters for a given quantum
computer's noise model. MITS accepts the specific noise model of a quantum
computer and a target logical error rate as input and outputs the optimal
surface code rounds and code distances. This guarantees minimal qubit and gate
usage, harmonizing the desired logical error rate with the existing hardware
limitations on qubit numbers and gate fidelity. We explored and compared
multiple heuristics and machine learning models for this task and concluded
that XGBoost and Random Forest regression to be most effective, with Pearson
correlation coefficients of $0.98$ and $0.96$ respectively. MITS employs these
models to reliably and swiftly achieve the target error rates.
</p>
</div>
</dd>
<dt><a name="item758">[758]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11044" title="Abstract">arXiv:2402.11044</a> (cross-list from math.CO) [<a href="/pdf/2402.11044" title="Download PDF">pdf</a>, <a href="/format/2402.11044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Star-Forest Decompositions of Complete Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Anti%C4%87%2C+T">Todor Anti&#x107;</a>, 
<a href="/search/math?searchtype=author&query=Gli%C5%A1i%C4%87%2C+J">Jelena Gli&#x161;i&#x107;</a>, 
<a href="/search/math?searchtype=author&query=Milivoj%C4%8Devi%C4%87%2C+M">Milan Milivoj&#x10d;evi&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Geometry (cs.CG)

</div>
<p class="mathjax">We deal with the problem of decomposing a complete geometric graph into plane
star-forests. In particular, we disprove a recent conjecture by Pach, Saghafian
and Schnider by constructing for each $n$ a complete geometric graph on $n$
vertices which can be decomposed into $\frac{n}{2}+1$ plane star-forests.
Additionally we prove that for even $n$, every decomposition of complete
abstract graph on $n$ vertices into $\frac{n}{2}+1$ star-forests is composed of
a perfect matching and $\frac{n}{2}$ star-forests with two edge-balanced
components, which we call broken double stars.
</p>
</div>
</dd>
<dt><a name="item759">[759]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11071" title="Abstract">arXiv:2402.11071</a> (cross-list from math.CA) [<a href="/pdf/2402.11071" title="Download PDF">pdf</a>, <a href="/format/2402.11071" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fisher-Riemann geometry for nonparametric probability densities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Aimar%2C+H">Hugo Aimar</a>, 
<a href="/search/math?searchtype=author&query=Ruiz%2C+A+C">An&#xed;bal Chicco Ruiz</a>, 
<a href="/search/math?searchtype=author&query=G%C3%B3mez%2C+I">Ivana G&#xf3;mez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Classical Analysis and ODEs (math.CA)</span>; Information Theory (cs.IT); Differential Geometry (math.DG)

</div>
<p class="mathjax">In this article we aim to obtain the Fisher Riemann geodesics for
nonparametric families of probability densities as a weak limit of the
parametric case with increasing number of parameters.
</p>
</div>
</dd>
<dt><a name="item760">[760]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11101" title="Abstract">arXiv:2402.11101</a> (cross-list from cond-mat.mtrl-sci) [<a href="/pdf/2402.11101" title="Download PDF">pdf</a>, <a href="/ps/2402.11101" title="Download PostScript">ps</a>, <a href="/format/2402.11101" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Physics-based material parameters extraction from perovskite experiments  via Bayesian optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Zhan%2C+H">Hualin Zhan</a>, 
<a href="/search/cond-mat?searchtype=author&query=Ahmad%2C+V">Viqar Ahmad</a>, 
<a href="/search/cond-mat?searchtype=author&query=Mayon%2C+A">Azul Mayon</a>, 
<a href="/search/cond-mat?searchtype=author&query=Tabi%2C+G">Grace Tabi</a>, 
<a href="/search/cond-mat?searchtype=author&query=Bui%2C+A+D">Anh Dinh Bui</a>, 
<a href="/search/cond-mat?searchtype=author&query=Li%2C+Z">Zhuofeng Li</a>, 
<a href="/search/cond-mat?searchtype=author&query=Walters%2C+D">Daniel Walters</a>, 
<a href="/search/cond-mat?searchtype=author&query=Nguyen%2C+H">Hieu Nguyen</a>, 
<a href="/search/cond-mat?searchtype=author&query=Weber%2C+K">Klaus Weber</a>, 
<a href="/search/cond-mat?searchtype=author&query=White%2C+T">Thomas White</a>, 
<a href="/search/cond-mat?searchtype=author&query=Catchpole%2C+K">Kylie Catchpole</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
<p class="mathjax">The ability to extract material parameters from quantitative experimental
analysis is essential for rational design and theory advancement. However, the
difficulty of this analysis increases significantly with the complexity of the
theoretical model and the number of material parameters. Here we use Bayesian
optimization to develop an analysis platform that can extract up to 8
fundamental material parameters of an organometallic perovskite semiconductor
from a transient photoluminescence experiment, based on a complex full physics
model that includes drift-diffusion of carriers and dynamic defect occupation.
An example study of thermal degradation reveals that changes in doping
concentration and carrier mobility dominate, while the defect energy level
remains nearly unchanged. This platform can be conveniently applied to other
experiments or to combinations of experiments, accelerating materials discovery
and optimization of semiconductor materials for photovoltaics and other
applications.
</p>
</div>
</dd>
<dt><a name="item761">[761]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11105" title="Abstract">arXiv:2402.11105</a> (cross-list from quant-ph) [<a href="/pdf/2402.11105" title="Download PDF">pdf</a>, <a href="/format/2402.11105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Magic Mirror on the Wall, How to Benchmark Quantum Error Correction  Codes, Overall ?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Chatterjee%2C+A">Avimita Chatterjee</a>, 
<a href="/search/quant-ph?searchtype=author&query=Ghosh%2C+S">Swaroop Ghosh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Quantum Error Correction Codes (QECCs) are fundamental to the advancement of
quantum computing, safeguarding quantum states from the detrimental impact of
noise and errors. The right choice of QECC, tailored to specific scenarios
influenced by noise levels and qubit constraints, is as vital as the
technological advancements themselves. This paper introduces a novel and
comprehensive methodology for benchmarking QECCs, featuring a set of universal
parameters. Utilizing eight distinguished QECCs, we propose a suite of eight
parameters for a thorough analysis. Our work not only establishes a universal
benchmarking methodology but also underscores the nuanced balance inherent in
quantum error correction. The paper highlights that there is no
one-size-fits-all solution; the selection of a QECC is contingent upon the
specific constraints and circumstances of each case.
</p>
</div>
</dd>
<dt><a name="item762">[762]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11111" title="Abstract">arXiv:2402.11111</a> (cross-list from physics.ed-ph) [<a href="/pdf/2402.11111" title="Download PDF">pdf</a>, <a href="/format/2402.11111" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language Models as Science Tutors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Chevalier%2C+A">Alexis Chevalier</a>, 
<a href="/search/physics?searchtype=author&query=Geng%2C+J">Jiayi Geng</a>, 
<a href="/search/physics?searchtype=author&query=Wettig%2C+A">Alexander Wettig</a>, 
<a href="/search/physics?searchtype=author&query=Chen%2C+H">Howard Chen</a>, 
<a href="/search/physics?searchtype=author&query=Mizera%2C+S">Sebastian Mizera</a>, 
<a href="/search/physics?searchtype=author&query=Annala%2C+T">Toni Annala</a>, 
<a href="/search/physics?searchtype=author&query=Aragon%2C+M+J">Max Jameson Aragon</a>, 
<a href="/search/physics?searchtype=author&query=Fanlo%2C+A+R">Arturo Rodr&#xed;guez Fanlo</a>, 
<a href="/search/physics?searchtype=author&query=Frieder%2C+S">Simon Frieder</a>, 
<a href="/search/physics?searchtype=author&query=Machado%2C+S">Simon Machado</a>, 
<a href="/search/physics?searchtype=author&query=Prabhakar%2C+A">Akshara Prabhakar</a>, 
<a href="/search/physics?searchtype=author&query=Thieu%2C+E">Ellie Thieu</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+J+T">Jiachen T. Wang</a>, 
<a href="/search/physics?searchtype=author&query=Wang%2C+Z">Zirui Wang</a>, 
<a href="/search/physics?searchtype=author&query=Wu%2C+X">Xindi Wu</a>, 
<a href="/search/physics?searchtype=author&query=Xia%2C+M">Mengzhou Xia</a>, 
<a href="/search/physics?searchtype=author&query=Jia%2C+W">Wenhan Jia</a>, 
<a href="/search/physics?searchtype=author&query=Yu%2C+J">Jiatong Yu</a>, 
<a href="/search/physics?searchtype=author&query=Zhu%2C+J">Jun-Jie Zhu</a>, 
<a href="/search/physics?searchtype=author&query=Ren%2C+Z+J">Zhiyong Jason Ren</a>, 
<a href="/search/physics?searchtype=author&query=Arora%2C+S">Sanjeev Arora</a>, 
<a href="/search/physics?searchtype=author&query=Chen%2C+D">Danqi Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages without bibliography and appendix, 26 pages total
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Physics Education (physics.ed-ph)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
<p class="mathjax">NLP has recently made exciting progress toward training language models (LMs)
with strong scientific problem-solving skills. However, model development has
not focused on real-life use-cases of LMs for science, including applications
in education that require processing long scientific documents. To address
this, we introduce TutorEval and TutorChat. TutorEval is a diverse
question-answering benchmark consisting of questions about long chapters from
STEM textbooks, written by experts. TutorEval helps measure real-life usability
of LMs as scientific assistants, and it is the first benchmark combining long
contexts, free-form generation, and multi-disciplinary scientific knowledge.
Moreover, we show that fine-tuning base models with existing dialogue datasets
leads to poor performance on TutorEval. Therefore, we create TutorChat, a
dataset of 80,000 long synthetic dialogues about textbooks. We use TutorChat to
fine-tune Llemma models with 7B and 34B parameters. These LM tutors specialized
in math have a 32K-token context window, and they excel at TutorEval while
performing strongly on GSM8K and MATH. Our datasets build on open-source
materials, and we release our models, data, and evaluations.
</p>
</div>
</dd>
<dt><a name="item763">[763]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11127" title="Abstract">arXiv:2402.11127</a> (cross-list from quant-ph) [<a href="/pdf/2402.11127" title="Download PDF">pdf</a>, <a href="/format/2402.11127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Q-Embroidery: A Study of Weaving Quantum Error Correction into the  Fabric of Quantum Classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Chatterjee%2C+A">Avimita Chatterjee</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kundu%2C+D">Debarshi Kundu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Ghosh%2C+S">Swaroop Ghosh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
<p class="mathjax">Quantum computing holds transformative potential for various fields, yet its
practical application is hindered by the susceptibility to errors. This study
makes a pioneering contribution by applying quantum error correction codes
(QECCs) for complex, multi-qubit classification tasks. We implement 1-qubit and
2-qubit quantum classifiers with QECCs, specifically the Steane code, and the
distance 3 &amp; 5 surface codes to analyze 2-dimensional and 4-dimensional
datasets. This research uniquely evaluates the performance of these QECCs in
enhancing the robustness and accuracy of quantum classifiers against various
physical errors, including bit-flip, phase-flip, and depolarizing errors. The
results emphasize that the effectiveness of a QECC in practical scenarios
depends on various factors, including qubit availability, desired accuracy, and
the specific types and levels of physical errors, rather than solely on
theoretical superiority.
</p>
</div>
</dd>
<dt><a name="item764">[764]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11156" title="Abstract">arXiv:2402.11156</a> (cross-list from stat.ML) [<a href="/pdf/2402.11156" title="Download PDF">pdf</a>, <a href="/format/2402.11156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient Low-Rank Matrix Estimation, Experimental Design, and  Arm-Set-Dependent Low-Rank Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Jang%2C+K">Kyoungseok Jang</a>, 
<a href="/search/stat?searchtype=author&query=Zhang%2C+C">Chicheng Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Jun%2C+K">Kwang-Sung Jun</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We study low-rank matrix trace regression and the related problem of low-rank
matrix bandits. Assuming access to the distribution of the covariates, we
propose a novel low-rank matrix estimation method called LowPopArt and provide
its recovery guarantee that depends on a novel quantity denoted by B(Q) that
characterizes the hardness of the problem, where Q is the covariance matrix of
the measurement distribution. We show that our method can provide tighter
recovery guarantees than classical nuclear norm penalized least squares
(Koltchinskii et al., 2011) in several problems. To perform efficient
estimation with a limited number of measurements from an arbitrarily given
measurement set A, we also propose a novel experimental design criterion that
minimizes B(Q) with computational efficiency. We leverage our novel estimator
and design of experiments to derive two low-rank linear bandit algorithms for
general arm sets that enjoy improved regret upper bounds. This improves over
previous works on low-rank bandits, which make somewhat restrictive assumptions
that the arm set is the unit ball or that an efficient exploration distribution
is given. To our knowledge, our experimental design criterion is the first one
tailored to low-rank matrix estimation beyond the naive reduction to linear
regression, which can be of independent interest.
</p>
</div>
</dd>
<dt><a name="item765">[765]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11157" title="Abstract">arXiv:2402.11157</a> (cross-list from econ.TH) [<a href="/pdf/2402.11157" title="Download PDF">pdf</a>, <a href="/ps/2402.11157" title="Download PostScript">ps</a>, <a href="/format/2402.11157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Value of Context: Human versus Black Box Evaluators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Iakovlev%2C+A">Andrei Iakovlev</a>, 
<a href="/search/econ?searchtype=author&query=Liang%2C+A">Annie Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">Evaluations once solely within the domain of human experts (e.g., medical
diagnosis by doctors) can now also be carried out by machine learning
algorithms. This raises a new conceptual question: what is the difference
between being evaluated by humans and algorithms, and when should an individual
prefer one form of evaluation over the other? We propose a theoretical
framework that formalizes one key distinction between the two forms of
evaluation: Machine learning algorithms are standardized, fixing a common set
of covariates by which to assess all individuals, while human evaluators
customize which covariates are acquired to each individual. Our framework
defines and analyzes the advantage of this customization -- the value of
context -- in environments with very high-dimensional data. We show that unless
the agent has precise knowledge about the joint distribution of covariates, the
value of more covariates exceeds the value of context.
</p>
</div>
</dd>
<dt><a name="item766">[766]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11205" title="Abstract">arXiv:2402.11205</a> (cross-list from nucl-th) [<a href="/pdf/2402.11205" title="Download PDF">pdf</a>, <a href="/format/2402.11205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Efficient Quantum Circuit for Block Encoding a Pairing Hamiltonian
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/nucl-th?searchtype=author&query=Liu%2C+D">Diyi Liu</a>, 
<a href="/search/nucl-th?searchtype=author&query=Du%2C+W">Weijie Du</a>, 
<a href="/search/nucl-th?searchtype=author&query=Lin%2C+L">Lin Lin</a>, 
<a href="/search/nucl-th?searchtype=author&query=Vary%2C+J+P">James P.Vary</a>, 
<a href="/search/nucl-th?searchtype=author&query=Yang%2C+C">Chao Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 18 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Nuclear Theory (nucl-th)</span>; Numerical Analysis (math.NA); Quantum Physics (quant-ph)

</div>
<p class="mathjax">We present an efficient quantum circuit for block encoding pairing
Hamiltonians studied in nuclear physics. The new block encoding scheme does not
require mapping the creation and annihilation operators to Pauli operators and
representing the Hamiltonian as a linear combination of unitaries. Instead, we
show how to encode these operators directly using controlled swaps. We analyze
the gate complexity of the block encoding circuit and show that it scales
polynomially with respect to the number of qubits required to represent a
quantum state associated with the pairing Hamiltonian. We also show how the
block encoding circuit can be combined with quantum singular value
transformation to construct an efficient quantum circuit for approximating the
density of state of a pairing Hamiltonian. Athough we focus on block encoding
circuit for pair Hamiltonians in this paper, the techniques presented here can
be extended to encode more general second quantized Hamiltonians.
</p>
</div>
</dd>
<dt><a name="item767">[767]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11211" title="Abstract">arXiv:2402.11211</a> (cross-list from eess.IV) [<a href="/pdf/2402.11211" title="Download PDF">pdf</a>, <a href="/format/2402.11211" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Training-free image style alignment for self-adapting domain shift on  handheld ultrasound devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zeng%2C+H">Hongye Zeng</a>, 
<a href="/search/eess?searchtype=author&query=Zou%2C+K">Ke Zou</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Z">Zhihao Chen</a>, 
<a href="/search/eess?searchtype=author&query=Gao%2C+Y">Yuchong Gao</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+H">Hongbo Chen</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+H">Haibin Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+K">Kang Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+M">Meng Wang</a>, 
<a href="/search/eess?searchtype=author&query=Goh%2C+R+S+M">Rick Siow Mong Goh</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Y">Yong Liu</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+C">Chang Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Zheng%2C+R">Rui Zheng</a>, 
<a href="/search/eess?searchtype=author&query=Fu%2C+H">Huazhu Fu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Handheld ultrasound devices face usage limitations due to user inexperience
and cannot benefit from supervised deep learning without extensive expert
annotations. Moreover, the models trained on standard ultrasound device data
are constrained by training data distribution and perform poorly when directly
applied to handheld device data. In this study, we propose the Training-free
Image Style Alignment (TISA) framework to align the style of handheld device
data to those of standard devices. The proposed TISA can directly infer
handheld device images without extra training and is suited for clinical
applications. We show that TISA performs better and more stably in medical
detection and segmentation tasks for handheld device data. We further validate
TISA as the clinical model for automatic measurements of spinal curvature and
carotid intima-media thickness. The automatic measurements agree well with
manual measurements made by human experts and the measurement errors remain
within clinically acceptable ranges. We demonstrate the potential for TISA to
facilitate automatic diagnosis on handheld ultrasound devices and expedite
their eventual widespread use.
</p>
</div>
</dd>
<dt><a name="item768">[768]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11216" title="Abstract">arXiv:2402.11216</a> (cross-list from eess.AS) [<a href="/pdf/2402.11216" title="Download PDF">pdf</a>, <a href="/format/2402.11216" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feedback Delay Network Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Santo%2C+G+D">Gloria Dal Santo</a>, 
<a href="/search/eess?searchtype=author&query=Prawda%2C+K">Karolina Prawda</a>, 
<a href="/search/eess?searchtype=author&query=Schlecht%2C+S+J">Sebastian J. Schlecht</a>, 
<a href="/search/eess?searchtype=author&query=V%C3%A4lim%C3%A4ki%2C+V">Vesa V&#xe4;lim&#xe4;ki</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">A common bane of artificial reverberation algorithms is spectral coloration,
typically manifesting as metallic ringing, leading to a degradation in the
perceived sound quality. This paper presents an optimization framework where a
differentiable feedback delay network is used to learn a set of parameters to
reduce coloration iteratively. The parameters under optimization include the
feedback matrix, as well as the input and output gains. The optimization
objective is twofold: to maximize spectral flatness through a spectral loss
while maintaining temporal density by penalizing sparseness in the parameter
values. A favorable narrower distribution of modal excitation is achieved while
maintaining the desired impulse response density. In a subjective assessment,
the new method proves effective in reducing perceptual coloration of late
reverberation. The proposed method achieves computational savings compared to
the baseline while preserving its performance. The effectiveness of this work
is demonstrated through two application scenarios where natural-sounding
synthetic impulse responses are obtained via the introduction of attenuation
filters and an optimizable scattering feedback matrix.
</p>
</div>
</dd>
<dt><a name="item769">[769]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11222" title="Abstract">arXiv:2402.11222</a> (cross-list from math.CO) [<a href="/pdf/2402.11222" title="Download PDF">pdf</a>, <a href="/ps/2402.11222" title="Download PostScript">ps</a>, <a href="/format/2402.11222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Treewidth versus clique number. IV. Tree-independence number of graphs  excluding an induced star
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Dallard%2C+C">Cl&#xe9;ment Dallard</a>, 
<a href="/search/math?searchtype=author&query=Krnc%2C+M">Matja&#x17e; Krnc</a>, 
<a href="/search/math?searchtype=author&query=Kwon%2C+O">O-jong Kwon</a>, 
<a href="/search/math?searchtype=author&query=Milani%C4%8D%2C+M">Martin Milani&#x10d;</a>, 
<a href="/search/math?searchtype=author&query=Munaro%2C+A">Andrea Munaro</a>, 
<a href="/search/math?searchtype=author&query=%C5%A0torgel%2C+K">Kenny &#x160;torgel</a>, 
<a href="/search/math?searchtype=author&query=Wiederrecht%2C+S">Sebastian Wiederrecht</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Data Structures and Algorithms (cs.DS)

</div>
<p class="mathjax">Many recent works address the question of characterizing induced obstructions
to bounded treewidth. In 2022, Lozin and Razgon completely answered this
question for graph classes defined by finitely many forbidden induced
subgraphs. Their result also implies a characterization of graph classes
defined by finitely many forbidden induced subgraphs that are
$(tw,\omega)$-bounded, that is, treewidth can only be large due to the presence
of a large clique. This condition is known to be satisfied for any graph class
with bounded tree-independence number, a graph parameter introduced
independently by Yolov in 2018 and by Dallard, Milani\v{c}, and \v{S}torgel in
2024. Dallard et al. conjectured that $(tw,\omega)$-boundedness is actually
equivalent to bounded tree-independence number. We address this conjecture in
the context of graph classes defined by finitely many forbidden induced
subgraphs and prove it for the case of graph classes excluding an induced star.
We also prove it for subclasses of the class of line graphs, determine the
exact values of the tree-independence numbers of line graphs of complete graphs
and line graphs of complete bipartite graphs, and characterize the
tree-independence number of $P_4$-free graphs, which implies a linear-time
algorithm for its computation. Applying the algorithmic framework provided in a
previous paper of the series leads to polynomial-time algorithms for the
Maximum Weight Independent Set problem in an infinite family of graph classes.
</p>
</div>
</dd>
<dt><a name="item770">[770]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11228" title="Abstract">arXiv:2402.11228</a> (cross-list from stat.ML) [<a href="/pdf/2402.11228" title="Download PDF">pdf</a>, <a href="/format/2402.11228" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive Split Balancing for Optimal Random Forest
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zhang%2C+Y">Yuqian Zhang</a>, 
<a href="/search/stat?searchtype=author&query=Ji%2C+W">Weijie Ji</a>, 
<a href="/search/stat?searchtype=author&query=Bradic%2C+J">Jelena Bradic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Statistics Theory (math.ST); Methodology (stat.ME)

</div>
<p class="mathjax">While random forests are commonly used for regression problems, existing
methods often lack adaptability in complex situations or lose optimality under
simple, smooth scenarios. In this study, we introduce the adaptive split
balancing forest (ASBF), capable of learning tree representations from data
while simultaneously achieving minimax optimality under the Lipschitz class. To
exploit higher-order smoothness levels, we further propose a localized version
that attains the minimax rate under the H\"older class $\mathcal{H}^{q,\beta}$
for any $q\in\mathbb{N}$ and $\beta\in(0,1]$. Rather than relying on the
widely-used random feature selection, we consider a balanced modification to
existing approaches. Our results indicate that an over-reliance on auxiliary
randomness may compromise the approximation power of tree models, leading to
suboptimal results. Conversely, a less random, more balanced approach
demonstrates optimality. Additionally, we establish uniform upper bounds and
explore the application of random forests in average treatment effect
estimation problems. Through simulation studies and real-data applications, we
demonstrate the superior empirical performance of the proposed methods over
existing random forests.
</p>
</div>
</dd>
<dt><a name="item771">[771]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11250" title="Abstract">arXiv:2402.11250</a> (cross-list from eess.IV) [<a href="/pdf/2402.11250" title="Download PDF">pdf</a>, <a href="/format/2402.11250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical Prior-based Super Resolution for Point Cloud Geometry  Compression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Li%2C+D">Dingquan Li</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+K">Kede Ma</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jing Wang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+G">Ge Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Multimedia (cs.MM)

</div>
<p class="mathjax">The Geometry-based Point Cloud Compression (G-PCC) has been developed by the
Moving Picture Experts Group to compress point clouds. In its lossy mode, the
reconstructed point cloud by G-PCC often suffers from noticeable distortions
due to the na\"{i}ve geometry quantization (i.e., grid downsampling). This
paper proposes a hierarchical prior-based super resolution method for point
cloud geometry compression. The content-dependent hierarchical prior is
constructed at the encoder side, which enables coarse-to-fine super resolution
of the point cloud geometry at the decoder side. A more accurate prior
generally yields improved reconstruction performance, at the cost of increased
bits required to encode this side information. With a proper balance between
prior accuracy and bit consumption, the proposed method demonstrates
substantial Bjontegaard-delta bitrate savings on the MPEG Cat1A dataset,
surpassing the octree-based and trisoup-based G-PCC v14. We provide our
implementations for reproducible research at
https://github.com/lidq92/mpeg-pcc-tmc13.
</p>
</div>
</dd>
<dt><a name="item772">[772]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11269" title="Abstract">arXiv:2402.11269</a> (cross-list from quant-ph) [<a href="/pdf/2402.11269" title="Download PDF">pdf</a>, <a href="/ps/2402.11269" title="Download PostScript">ps</a>, <a href="/format/2402.11269" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A New Approach to Generic Lower Bounds: Classical/Quantum MDL, Quantum  Factoring, and More
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Hhan%2C+M">Minki Hhan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR)

</div>
<p class="mathjax">This paper studies the limitations of the generic approaches to solving
cryptographic problems in classical and quantum settings in various models.
<br />- In the classical generic group model (GGM), we find simple alternative
proofs for the lower bounds of variants of the discrete logarithm (DL) problem:
the multiple-instance DL and one-more DL problems (and their mixture). We also
re-prove the unknown-order GGM lower bounds, such as the order finding, root
extraction, and repeated squaring.
<br />- In the quantum generic group model (QGGM), we study the complexity of
variants of the discrete logarithm. We prove the logarithm DL lower bound in
the QGGM even for the composite order setting. We also prove an asymptotically
tight lower bound for the multiple-instance DL problem. Both results resolve
the open problems suggested in a recent work by Hhan, Yamakawa, and Yun.
<br />- In the quantum generic ring model we newly suggested, we give the
logarithmic lower bound for the order-finding algorithms, an important step for
Shor's algorithm. We also give a logarithmic lower bound for a certain generic
factoring algorithm outputting relatively small integers, which includes a
modified version of Regev's algorithm.
<br />- Finally, we prove a lower bound for the basic index calculus method for
solving the DL problem in a new idealized group model regarding smooth numbers.
<br />The quantum lower bounds in both models allow certain (different) types of
classical preprocessing. All of the proofs are significantly simpler than the
previous proofs and are through a single tool, the so-called compression lemma,
along with linear algebra tools. Our use of this lemma may be of independent
interest.
</p>
</div>
</dd>
<dt><a name="item773">[773]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11274" title="Abstract">arXiv:2402.11274</a> (cross-list from eess.IV) [<a href="/pdf/2402.11274" title="Download PDF">pdf</a>, <a href="/format/2402.11274" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TC-DiffRecon: Texture coordination MRI reconstruction method based on  diffusion model and modified MF-UNet method
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+C">Chenyan Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Chen%2C+Y">Yifei Chen</a>, 
<a href="/search/eess?searchtype=author&query=Fan%2C+Z">Zhenxiong Fan</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+Y">Yiyu Huang</a>, 
<a href="/search/eess?searchtype=author&query=Weng%2C+W">Wenchao Weng</a>, 
<a href="/search/eess?searchtype=author&query=Ge%2C+R">Ruiquan Ge</a>, 
<a href="/search/eess?searchtype=author&query=Zeng%2C+D">Dong Zeng</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+C">Changmiao Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 2 figures, accept ISBI2024
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recently, diffusion models have gained significant attention as a novel set
of deep learning-based generative methods. These models attempt to sample data
from a Gaussian distribution that adheres to a target distribution, and have
been successfully adapted to the reconstruction of MRI data. However, as an
unconditional generative model, the diffusion model typically disrupts image
coordination because of the consistent projection of data introduced by
conditional bootstrap. This often results in image fragmentation and
incoherence. Furthermore, the inherent limitations of the diffusion model often
lead to excessive smoothing of the generated images. In the same vein, some
deep learning-based models often suffer from poor generalization performance,
meaning their effectiveness is greatly affected by different acceleration
factors. To address these challenges, we propose a novel diffusion model-based
MRI reconstruction method, named TC-DiffRecon, which does not rely on a
specific acceleration factor for training. We also suggest the incorporation of
the MF-UNet module, designed to enhance the quality of MRI images generated by
the model while mitigating the over-smoothing issue to a certain extent. During
the image generation sampling process, we employ a novel TCKG module and a
Coarse-to-Fine sampling scheme. These additions aim to harmonize image texture,
expedite the sampling process, while achieving data consistency. Our source
code is available at https://github.com/JustlfC03/TC-DiffRecon.
</p>
</div>
</dd>
<dt><a name="item774">[774]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11329" title="Abstract">arXiv:2402.11329</a> (cross-list from math.CO) [<a href="/pdf/2402.11329" title="Download PDF">pdf</a>, <a href="/format/2402.11329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On a recent extension of a family of biprojective APN functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=K%C3%B6lsch%2C+L">Lukas K&#xf6;lsch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages. Comments welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">APN functions play a big role as primitives in symmetric cryptography as
building blocks that yield optimal resistance to differential attacks. In this
note, we consider a recent extension of a biprojective APN family by
G\"olo\u{g}lu defined on $\mathbb{F}_{2^{2m}}$. We show that this
generalization yields functions equivalent to G\"olo\u{g}lu's original family
if $3\nmid m$. If $3|m$ we show exactly how many inequivalent APN functions
this new family contains. We also show that the family has the minimal image
set size for an APN function and determine its Walsh spectrum, hereby settling
some open problems. In our proofs, we leverage a group theoretic technique
recently developed by G\"olo\u{g}lu and the author in conjunction with a group
action on the set of projective polynomials.
</p>
</div>
</dd>
<dt><a name="item775">[775]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11330" title="Abstract">arXiv:2402.11330</a> (cross-list from eess.AS) [<a href="/pdf/2402.11330" title="Download PDF">pdf</a>, <a href="/format/2402.11330" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffuse Sound Field Synthesis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zotter%2C+F">Franz Zotter</a>, 
<a href="/search/eess?searchtype=author&query=Riedel%2C+S">Stefan Riedel</a>, 
<a href="/search/eess?searchtype=author&query=G%C3%B6lles%2C+L">Lukas G&#xf6;lles</a>, 
<a href="/search/eess?searchtype=author&query=Frank%2C+M">Matthias Frank</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 27 pages, 17 figures, submitted to acta acustica nov 20th 2023, including jan/feb 2024 upgrades while awaiting the reviews
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">Can uncorrelated surrounding sound sources be used to generate extended
diffuse sound fields? By definition, targets are a constant sound pressure
level, a vanishing average sound intensity, uncorrelated sound waves arriving
isotropically from all directions. Does this require specific sources and
geometries for surrounding 2D and 3D source layouts?
<br />As methods, we employ numeric simulations and undertake a series of
calculations with uncorrelated circular/spherical source layouts, or such with
infinite excess dimensions, and we point out relations to potential theory.
Using a radial decay 1/r^b modified by the exponent b, the representation of
the resulting fields with hypergeometric functions, Gegenbauer polynomials, and
circular as well as spherical harmonics yields fruitful insights.
<br />In circular layouts, waves decaying by the exponent b=1/2 synthesize ideally
extended, diffuse sound fields; spherical layouts do so with b=1. None of the
layouts synthesizes a perfectly constant expected sound pressure level but its
flatness is acceptable.
<br />Spherical t-designs describe optimal source layouts with well-described area
of high diffuseness, and non-spherical, convex layouts can be improved by
restoring isotropy or by mode matching for a maximally diffuse synthesis.
<br />Theory and simulation offer a basis for loudspeaker-based synthesis of
diffuse sound fields and contribute physical reasons to recent psychoacoustic
findings in spatial audio.
</p>
</div>
</dd>
<dt><a name="item776">[776]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11345" title="Abstract">arXiv:2402.11345</a> (cross-list from stat.ML) [<a href="/pdf/2402.11345" title="Download PDF">pdf</a>, <a href="/format/2402.11345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variational Entropy Search for Adjusting Expected Improvement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cheng%2C+N">Nuojin Cheng</a>, 
<a href="/search/stat?searchtype=author&query=Becker%2C+S">Stephen Becker</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">Bayesian optimization is a widely used technique for optimizing black-box
functions, with Expected Improvement (EI) being the most commonly utilized
acquisition function in this domain. While EI is often viewed as distinct from
other information-theoretic acquisition functions, such as entropy search (ES)
and max-value entropy search (MES), our work reveals that EI can be considered
a special case of MES when approached through variational inference (VI). In
this context, we have developed the Variational Entropy Search (VES)
methodology and the VES-Gamma algorithm, which adapts EI by incorporating
principles from information-theoretic concepts. The efficacy of VES-Gamma is
demonstrated across a variety of test functions and read datasets, highlighting
its theoretical and practical utilities in Bayesian optimization scenarios.
</p>
</div>
</dd>
<dt><a name="item777">[777]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11363" title="Abstract">arXiv:2402.11363</a> (cross-list from q-bio.QM) [<a href="/pdf/2402.11363" title="Download PDF">pdf</a>, <a href="/format/2402.11363" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformer-based de novo peptide sequencing for data-independent  acquisition mass spectrometry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Ebrahimi%2C+S">Shiva Ebrahimi</a>, 
<a href="/search/q-bio?searchtype=author&query=Guo%2C+X">Xuan Guo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Ebrahimi S., Guo X. Transformer-based de novo peptide sequencing for data-independent acquisition mass spectrometry. In 2023 IEEE 23rd International Conference on Bioinformatics and Bioengineering (BIBE) 2022 Dec 6 (pp. 17-22). IEEE
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Tandem mass spectrometry (MS/MS) stands as the predominant high-throughput
technique for comprehensively analyzing protein content within biological
samples. This methodology is a cornerstone driving the advancement of
proteomics. In recent years, substantial strides have been made in
Data-Independent Acquisition (DIA) strategies, facilitating impartial and
non-targeted fragmentation of precursor ions. The DIA-generated MS/MS spectra
present a formidable obstacle due to their inherent high multiplexing nature.
Each spectrum encapsulates fragmented product ions originating from multiple
precursor peptides. This intricacy poses a particularly acute challenge in de
novo peptide/protein sequencing, where current methods are ill-equipped to
address the multiplexing conundrum. In this paper, we introduce Casanovo-DIA, a
deep-learning model based on transformer architecture. It deciphers peptide
sequences from DIA mass spectrometry data. Our results show significant
improvements over existing STOA methods, including DeepNovo-DIA and PepNet.
Casanovo-DIA enhances precision by 15.14% to 34.8%, recall by 11.62% to 31.94%
at the amino acid level, and boosts precision by 59% to 81.36% at the peptide
level. Integrating DIA data and our Casanovo-DIA model holds considerable
promise to uncover novel peptides and more comprehensive profiling of
biological samples. Casanovo-DIA is freely available under the GNU GPL license
at https://github.com/Biocomputing-Research-Group/Casanovo-DIA.
</p>
</div>
</dd>
<dt><a name="item778">[778]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11425" title="Abstract">arXiv:2402.11425</a> (cross-list from stat.ME) [<a href="/pdf/2402.11425" title="Download PDF">pdf</a>, <a href="/ps/2402.11425" title="Download PostScript">ps</a>, <a href="/format/2402.11425" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Local False Discovery Rate Control: A Resource Allocation  Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ao%2C+R">Ruicheng Ao</a>, 
<a href="/search/stat?searchtype=author&query=Chen%2C+H">Hongyu Chen</a>, 
<a href="/search/stat?searchtype=author&query=Simchi-Levi%2C+D">David Simchi-Levi</a>, 
<a href="/search/stat?searchtype=author&query=Zhu%2C+F">Feng Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC); Probability (math.PR)

</div>
<p class="mathjax">We consider the problem of online local false discovery rate (FDR) control
where multiple tests are conducted sequentially, with the goal of maximizing
the total expected number of discoveries. We formulate the problem as an online
resource allocation problem with accept/reject decisions, which from a high
level can be viewed as an online knapsack problem, with the additional
uncertainty of random budget replenishment. We start with general arrival
distributions and propose a simple policy that achieves a $O(\sqrt{T})$ regret.
We complement the result by showing that such regret rate is in general not
improvable. We then shift our focus to discrete arrival distributions. We find
that many existing re-solving heuristics in the online resource allocation
literature, albeit achieve bounded loss in canonical settings, may incur a
$\Omega(\sqrt{T})$ or even a $\Omega(T)$ regret. With the observation that
canonical policies tend to be too optimistic and over accept arrivals, we
propose a novel policy that incorporates budget buffers. We show that small
additional logarithmic buffers suffice to reduce the regret from
$\Omega(\sqrt{T})$ or even $\Omega(T)$ to $O(\ln^2 T)$. Numerical experiments
are conducted to validate our theoretical findings. Our formulation may have
wider applications beyond the problem considered in this paper, and our results
emphasize how effective policies should be designed to reach a balance between
circumventing wrong accept and reducing wrong reject in online resource
allocation problems with uncertain budgets.
</p>
</div>
</dd>
<dt><a name="item779">[779]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11459" title="Abstract">arXiv:2402.11459</a> (cross-list from q-bio.BM) [<a href="/pdf/2402.11459" title="Download PDF">pdf</a>, <a href="/format/2402.11459" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion  Bridge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Huang%2C+Y">Yufei Huang</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+O">Odin Zhang</a>, 
<a href="/search/q-bio?searchtype=author&query=Wu%2C+L">Lirong Wu</a>, 
<a href="/search/q-bio?searchtype=author&query=Tan%2C+C">Cheng Tan</a>, 
<a href="/search/q-bio?searchtype=author&query=Lin%2C+H">Haitao Lin</a>, 
<a href="/search/q-bio?searchtype=author&query=Gao%2C+Z">Zhangyang Gao</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+S">Siyuan Li</a>, 
<a href="/search/q-bio?searchtype=author&query=Li%2C+S+Z">Stan.Z. Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Chemical Physics (physics.chem-ph)

</div>
<p class="mathjax">Accurate prediction of protein-ligand binding structures, a task known as
molecular docking is crucial for drug design but remains challenging. While
deep learning has shown promise, existing methods often depend on holo-protein
structures (docked, and not accessible in realistic tasks) or neglect pocket
sidechain conformations, leading to limited practical utility and unrealistic
conformation predictions. To fill these gaps, we introduce an under-explored
task, named flexible docking to predict poses of ligand and pocket sidechains
simultaneously and introduce Re-Dock, a novel diffusion bridge generative model
extended to geometric manifolds. Specifically, we propose energy-to-geometry
mapping inspired by the Newton-Euler equation to co-model the binding energy
and conformations for reflecting the energy-constrained docking generative
process. Comprehensive experiments on designed benchmark datasets including
apo-dock and cross-dock demonstrate our model's superior effectiveness and
efficiency over current methods.
</p>
</div>
</dd>
<dt><a name="item780">[780]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11464" title="Abstract">arXiv:2402.11464</a> (cross-list from econ.TH) [<a href="/pdf/2402.11464" title="Download PDF">pdf</a>, <a href="/ps/2402.11464" title="Download PostScript">ps</a>, <a href="/format/2402.11464" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Weighted Myerson value for Network games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Kakoty%2C+N">Niharika Kakoty</a>, 
<a href="/search/econ?searchtype=author&query=Borkotokey%2C+S">Surajit Borkotokey</a>, 
<a href="/search/econ?searchtype=author&query=Kumar%2C+R">Rajnish Kumar</a>, 
<a href="/search/econ?searchtype=author&query=Bora%2C+A">Abhijit Bora</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT)

</div>
<p class="mathjax">We study the weighted Myerson value for Network games extending a similar
concept for communication situations. Network games, unlike communication
situations, treat direct and indirect links among players differently and
distinguish their effects in both worth generation and allocation processes.
The weighted Myerson value is an allocation rule for Network games that
generalizes the Myerson value of Network games. Here, the players are assumed
to have some weights measuring their capacity to form links with other players.
Two characterization of the weighted Myerson value are provided. Finally, we
propose a bidding mechanism to show that the weighted Myerson value is a
subgame-perfect Nash equilibrium under a non-cooperative framework.
</p>
</div>
</dd>
<dt><a name="item781">[781]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11472" title="Abstract">arXiv:2402.11472</a> (cross-list from q-bio.BM) [<a href="/pdf/2402.11472" title="Download PDF">pdf</a>, <a href="/format/2402.11472" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt  Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Wang%2C+Y">Yingying Wang</a>, 
<a href="/search/q-bio?searchtype=author&query=Xiong%2C+Y">Yun Xiong</a>, 
<a href="/search/q-bio?searchtype=author&query=Wu%2C+X">Xixi Wu</a>, 
<a href="/search/q-bio?searchtype=author&query=Sun%2C+X">Xiangguo Sun</a>, 
<a href="/search/q-bio?searchtype=author&query=Zhang%2C+J">Jiawei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Recently, Graph Neural Networks have become increasingly prevalent in
predicting adverse drug-drug interactions (DDI) due to their proficiency in
modeling the intricate associations between atoms and functional groups within
and across drug molecules. However, they are still hindered by two significant
challenges: (1) the issue of highly imbalanced event distribution, which is a
common but critical problem in medical datasets where certain interactions are
vastly underrepresented. This imbalance poses a substantial barrier to
achieving accurate and reliable DDI predictions. (2) the scarcity of labeled
data for rare events, which is a pervasive issue in the medical field where
rare yet potentially critical interactions are often overlooked or
under-studied due to limited available data. In response, we offer DDIPrompt,
an innovative panacea inspired by the recent advancements in graph prompting.
Our framework aims to address these issues by leveraging the intrinsic
knowledge from pre-trained models, which can be efficiently deployed with
minimal downstream data. Specifically, to solve the first challenge, DDIPrompt
employs augmented links between drugs, considering both structural and
interactive proximity. It features a hierarchical pre-training strategy that
comprehends intra-molecular structures and inter-molecular interactions,
fostering a comprehensive and unbiased understanding of drug properties. For
the second challenge, we implement a prototype-enhanced prompting mechanism
during inference. This mechanism, refined by few-shot examples from each
category, effectively harnesses the rich pre-training knowledge to enhance
prediction accuracy, particularly for these rare but crucial interactions.
Comprehensive evaluations on two benchmark datasets demonstrate the superiority
of DDIPrompt, particularly in predicting rare DDI events.
</p>
</div>
</dd>
<dt><a name="item782">[782]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11510" title="Abstract">arXiv:2402.11510</a> (cross-list from eess.IV) [<a href="/pdf/2402.11510" title="Download PDF">pdf</a>, <a href="/format/2402.11510" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Underestimation of lung regions on chest X-ray segmentation masks  assessed by comparison with total lung volume evaluated on computed  tomography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Bombi%C5%84ski%2C+P">Przemys&#x142;aw Bombi&#x144;ski</a>, 
<a href="/search/eess?searchtype=author&query=Szatkowski%2C+P">Patryk Szatkowski</a>, 
<a href="/search/eess?searchtype=author&query=Sobieski%2C+B">Bart&#x142;omiej Sobieski</a>, 
<a href="/search/eess?searchtype=author&query=Kwieci%C5%84ski%2C+T">Tymoteusz Kwieci&#x144;ski</a>, 
<a href="/search/eess?searchtype=author&query=P%C5%82otka%2C+S">Szymon P&#x142;otka</a>, 
<a href="/search/eess?searchtype=author&query=Adamek%2C+M">Mariusz Adamek</a>, 
<a href="/search/eess?searchtype=author&query=Banasiuk%2C+M">Marcin Banasiuk</a>, 
<a href="/search/eess?searchtype=author&query=Furmanek%2C+M+I">Mariusz I. Furmanek</a>, 
<a href="/search/eess?searchtype=author&query=Biecek%2C+P">Przemys&#x142;aw Biecek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint to Elsevier
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Lung mask creation lacks well-defined criteria and standardized guidelines,
leading to a high degree of subjectivity between annotators. In this study, we
assess the underestimation of lung regions on chest X-ray segmentation masks
created according to the current state-of-the-art method, by comparison with
total lung volume evaluated on computed tomography (CT). We show, that lung
X-ray masks created by following the contours of the heart, mediastinum, and
diaphragm significantly underestimate lung regions and exclude substantial
portions of the lungs from further assessment, which may result in numerous
clinical errors.
</p>
</div>
</dd>
<dt><a name="item783">[783]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11538" title="Abstract">arXiv:2402.11538</a> (cross-list from hep-ph) [<a href="/pdf/2402.11538" title="Download PDF">pdf</a>, <a href="/format/2402.11538" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PASCL: Supervised Contrastive Learning with Perturbative Augmentation  for Particle Decay Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/hep-ph?searchtype=author&query=Lu%2C+J">Junjian Lu</a>, 
<a href="/search/hep-ph?searchtype=author&query=Liu%2C+S">Siwei Liu</a>, 
<a href="/search/hep-ph?searchtype=author&query=Kobylianski%2C+D">Dmitrii Kobylianski</a>, 
<a href="/search/hep-ph?searchtype=author&query=Dreyer%2C+E">Etienne Dreyer</a>, 
<a href="/search/hep-ph?searchtype=author&query=Gross%2C+E">Eilam Gross</a>, 
<a href="/search/hep-ph?searchtype=author&query=Liang%2C+S">Shangsong Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">High Energy Physics - Phenomenology (hep-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In high-energy physics, particles produced in collision events decay in a
format of a hierarchical tree structure, where only the final decay products
can be observed using detectors. However, the large combinatorial space of
possible tree structures makes it challenging to recover the actual decay
process given a set of final particles. To better analyse the hierarchical tree
structure, we propose a graph-based deep learning model to infer the tree
structure to reconstruct collision events. In particular, we use a compact
matrix representation termed as lowest common ancestor generations (LCAG)
matrix, to encode the particle decay tree structure. Then, we introduce a
perturbative augmentation technique applied to node features, aiming to mimic
experimental uncertainties and increase data diversity. We further propose a
supervised graph contrastive learning algorithm to utilize the information of
inter-particle relations from multiple decay processes. Extensive experiments
show that our proposed supervised graph contrastive learning with perturbative
augmentation (PASCL) method outperforms state-of-the-art baseline models on an
existing physics-based dataset, significantly improving the reconstruction
accuracy. This method provides a more effective training strategy for models
with the same parameters and makes way for more accurate and efficient
high-energy particle physics data analysis.
</p>
</div>
</dd>
<dt><a name="item784">[784]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11552" title="Abstract">arXiv:2402.11552</a> (cross-list from stat.ML) [<a href="/pdf/2402.11552" title="Download PDF">pdf</a>, <a href="/format/2402.11552" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empirical Density Estimation based on Spline Quasi-Interpolation with  applications to Copulas clustering modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Tamborrino%2C+C">Cristiano Tamborrino</a>, 
<a href="/search/stat?searchtype=author&query=Falini%2C+A">Antonella Falini</a>, 
<a href="/search/stat?searchtype=author&query=Mazzia%2C+F">Francesca Mazzia</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Numerical Analysis (math.NA)

</div>
<p class="mathjax">Density estimation is a fundamental technique employed in various fields to
model and to understand the underlying distribution of data. The primary
objective of density estimation is to estimate the probability density function
of a random variable. This process is particularly valuable when dealing with
univariate or multivariate data and is essential for tasks such as clustering,
anomaly detection, and generative modeling. In this paper we propose the
mono-variate approximation of the density using spline quasi interpolation and
we applied it in the context of clustering modeling. The clustering technique
used is based on the construction of suitable multivariate distributions which
rely on the estimation of the monovariate empirical densities (marginals). Such
an approximation is achieved by using the proposed spline quasi-interpolation,
while the joint distributions to model the sought clustering partition is
constructed with the use of copulas functions. In particular, since copulas can
capture the dependence between the features of the data independently from the
marginal distributions, a finite mixture copula model is proposed. The
presented algorithm is validated on artificial and real datasets.
</p>
</div>
</dd>
<dt><a name="item785">[785]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11557" title="Abstract">arXiv:2402.11557</a> (cross-list from eess.IV) [<a href="/pdf/2402.11557" title="Download PDF">pdf</a>, <a href="/format/2402.11557" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Adversarial Robustness of Low dose CT Recovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Gandikota%2C+K+V">Kanchana Vaishnavi Gandikota</a>, 
<a href="/search/eess?searchtype=author&query=Chandramouli%2C+P">Paramanand Chandramouli</a>, 
<a href="/search/eess?searchtype=author&query=Droege%2C+H">Hannah Droege</a>, 
<a href="/search/eess?searchtype=author&query=Moeller%2C+M">Michael Moeller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> MIDL 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Low dose computed tomography (CT) acquisition using reduced radiation or
sparse angle measurements is recommended to decrease the harmful effects of
X-ray radiation. Recent works successfully apply deep networks to the problem
of low dose CT recovery on bench-mark datasets. However, their robustness needs
a thorough evaluation before use in clinical settings. In this work, we
evaluate the robustness of different deep learning approaches and classical
methods for CT recovery. We show that deep networks, including model-based
networks encouraging data consistency, are more susceptible to untargeted
attacks. Surprisingly, we observe that data consistency is not heavily affected
even for these poor quality reconstructions, motivating the need for better
regularization for the networks. We demonstrate the feasibility of universal
attacks and study attack transferability across different methods. We analyze
robustness to attacks causing localized changes in clinically relevant regions.
Both classical approaches and deep networks are affected by such attacks
leading to changes in the visual appearance of localized lesions, for extremely
small perturbations. As the resulting reconstructions have high data
consistency with the original measurements, these localized attacks can be used
to explore the solution space of the CT recovery problem.
</p>
</div>
</dd>
<dt><a name="item786">[786]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11567" title="Abstract">arXiv:2402.11567</a> (cross-list from quant-ph) [<a href="/pdf/2402.11567" title="Download PDF">pdf</a>, <a href="/ps/2402.11567" title="Download PostScript">ps</a>, <a href="/format/2402.11567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Saturability of the Quantum Cram&#xe9;r-Rao Bound in Multiparameter  Quantum Estimation at the Single-Copy Level
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Nurdin%2C+H+I">Hendra I. Nurdin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, no figures. Comments are welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Systems and Control (eess.SY)

</div>
<p class="mathjax">The quantum Cram\'{e}r-Rao bound (QCRB) as the ultimate lower bound for
precision in quantum parameter estimation is only known to be saturable in the
multiparameter setting in special cases and under conditions such as full or
average commutavity of the symmetric logarithmic derivatives (SLDs) associated
with the parameters. Moreover, for general mixed states, collective
measurements over infinitely many identical copies of the quantum state are
generally required to attain the QCRB. In the important and experimentally
relevant single-copy scenario, a necessary condition for saturating the QCRB in
the multiparameter setting for general mixed states is the so-called partial
commutativity condition on the SLDs. However, it is not known if this condition
is also sufficient. This paper derives new necessary conditions that imply
partial commutativity and are almost sufficient. It is shown that together with
another condition they become sufficient for saturability of the QCRB in the
multiparameter single-copy case. Moreover, when the sufficient conditions are
satisfied an optimal measurement saturating the QCRB can be chosen to be
projective and explicitly characterized. An example is developed to illustrate
the case of a multiparameter quantum state where the conditions derived herein
are satisfied and can be explicitly verified.
</p>
</div>
</dd>
<dt><a name="item787">[787]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11630" title="Abstract">arXiv:2402.11630</a> (cross-list from eess.SP) [<a href="/pdf/2402.11630" title="Download PDF">pdf</a>, <a href="/format/2402.11630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Distributed and Intelligent Integrated Sensing and  Communications for 6G Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Strinati%2C+E+C">Emilio Calvanese Strinati</a>, 
<a href="/search/eess?searchtype=author&query=Alexandropoulos%2C+G+C">George C. Alexandropoulos</a>, 
<a href="/search/eess?searchtype=author&query=Amani%2C+N">Navid Amani</a>, 
<a href="/search/eess?searchtype=author&query=Crozzoli%2C+M">Maurizio Crozzoli</a>, 
<a href="/search/eess?searchtype=author&query=Madhusudan%2C+G">Giyyarpuram Madhusudan</a>, 
<a href="/search/eess?searchtype=author&query=Mekki%2C+S">Sami Mekki</a>, 
<a href="/search/eess?searchtype=author&query=Rivet%2C+F">Francois Rivet</a>, 
<a href="/search/eess?searchtype=author&query=Sciancalepore%2C+V">Vincenzo Sciancalepore</a>, 
<a href="/search/eess?searchtype=author&query=Sehier%2C+P">Philippe Sehier</a>, 
<a href="/search/eess?searchtype=author&query=Stark%2C+M">Maximilian Stark</a>, 
<a href="/search/eess?searchtype=author&query=Wymeersch%2C+H">Henk Wymeersch</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review for possible publication in an IEEE Magazine
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">This paper introduces the distributed and intelligent integrated sensing and
communications (DISAC) concept, a transformative approach for 6G wireless
networks that extends the emerging concept of integrated sensing and
communications (ISAC). DISAC addresses the limitations of the existing ISAC
models and, to overcome them, it introduces two novel foundational
functionalities for both sensing and communications: a distributed architecture
and a semantic and goal-oriented framework. The distributed architecture
enables large-scale and energy-efficient tracking of connected users and
objects, leveraging the fusion of heterogeneous sensors. The semantic and
goal-oriented intelligent and parsimonious framework, enables the transition
from classical data fusion to the composition of semantically selected
information, offering new paradigms for the optimization of resource
utilization and exceptional multi-modal sensing performance across various use
cases. This paper details DISAC's principles, architecture, and potential
applications.
</p>
</div>
</dd>
<dt><a name="item788">[788]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11645" title="Abstract">arXiv:2402.11645</a> (cross-list from quant-ph) [<a href="/pdf/2402.11645" title="Download PDF">pdf</a>, <a href="/ps/2402.11645" title="Download PostScript">ps</a>, <a href="/format/2402.11645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum Image Denoising with Machine Learning: A Novel Approach to  Improve Quantum Image Processing Quality and Reliability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Wonga%2C+Y+K">Yew Kee Wonga</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zhou%2C+Y">Yifan Zhou</a>, 
<a href="/search/quant-ph?searchtype=author&query=Liang%2C+Y+S">Yan Shing Liang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 3 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Artificial Intelligence (cs.AI)

</div>
<p class="mathjax">Quantum Image Processing (QIP) is a field that aims to utilize the benefits
of quantum computing for manipulating and analyzing images. However, QIP faces
two challenges: the limitation of qubits and the presence of noise in a quantum
machine. In this research we propose a novel approach to address the issue of
noise in QIP. By training and employing a machine learning model that
identifies and corrects the noise in quantum processed images, we can
compensate for the noisiness caused by the machine and retrieve a processing
result similar to that performed by a classical computer with higher
efficiency. The model is trained by learning a dataset consisting of both
existing processed images and quantum processed images from open access
datasets. This model will be capable of providing us with the confidence level
for each pixel and its potential original value. To assess the model's accuracy
in compensating for loss and decoherence in QIP, we evaluate it using three
metrics: Peak Signal to Noise Ratio (PSNR), Structural Similarity Index (SSIM),
and Mean Opinion Score (MOS). Additionally, we discuss the applicability of our
model across domains well as its cost effectiveness compared to alternative
methods.
</p>
</div>
</dd>
<dt><a name="item789">[789]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11652" title="Abstract">arXiv:2402.11652</a> (cross-list from econ.EM) [<a href="/pdf/2402.11652" title="Download PDF">pdf</a>, <a href="/format/2402.11652" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Doubly Robust Inference in Causal Latent Factor Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Abadie%2C+A">Alberto Abadie</a>, 
<a href="/search/econ?searchtype=author&query=Agarwal%2C+A">Anish Agarwal</a>, 
<a href="/search/econ?searchtype=author&query=Dwivedi%2C+R">Raaz Dwivedi</a>, 
<a href="/search/econ?searchtype=author&query=Shah%2C+A">Abhin Shah</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Econometrics (econ.EM)</span>; Machine Learning (cs.LG); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">This article introduces a new framework for estimating average treatment
effects under unobserved confounding in modern data-rich environments featuring
large numbers of units and outcomes. The proposed estimator is doubly robust,
combining outcome imputation, inverse probability weighting, and a novel
cross-fitting procedure for matrix completion. We derive finite-sample and
asymptotic guarantees, and show that the error of the new estimator converges
to a mean-zero Gaussian distribution at a parametric rate. Simulation results
demonstrate the practical relevance of the formal properties of the estimators
analyzed in this article.
</p>
</div>
</dd>
<dt><a name="item790">[790]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11654" title="Abstract">arXiv:2402.11654</a> (cross-list from math.OC) [<a href="/pdf/2402.11654" title="Download PDF">pdf</a>, <a href="/format/2402.11654" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model-Free $&#x3bc;$-Synthesis: A Nonsmooth Optimization Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Keivan%2C+D">Darioush Keivan</a>, 
<a href="/search/math?searchtype=author&query=Guo%2C+X">Xingang Guo</a>, 
<a href="/search/math?searchtype=author&query=Seiler%2C+P">Peter Seiler</a>, 
<a href="/search/math?searchtype=author&query=Dullerud%2C+G">Geir Dullerud</a>, 
<a href="/search/math?searchtype=author&query=Hu%2C+B">Bin Hu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to L4DC 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this paper, we revisit model-free policy search on an important robust
control benchmark, namely $\mu$-synthesis. In the general output-feedback
setting, there do not exist convex formulations for this problem, and hence
global optimality guarantees are not expected. Apkarian (2011) presented a
nonconvex nonsmooth policy optimization approach for this problem, and achieved
state-of-the-art design results via using subgradient-based policy search
algorithms which generate update directions in a model-based manner. Despite
the lack of convexity and global optimality guarantees, these subgradient-based
policy search methods have led to impressive numerical results in practice.
Built upon such a policy optimization persepctive, our paper extends these
subgradient-based search methods to a model-free setting. Specifically, we
examine the effectiveness of two model-free policy optimization strategies: the
model-free non-derivative sampling method and the zeroth-order policy search
with uniform smoothing. We performed an extensive numerical study to
demonstrate that both methods consistently replicate the design outcomes
achieved by their model-based counterparts. Additionally, we provide some
theoretical justifications showing that convergence guarantees to stationary
points can be established for our model-free $\mu$-synthesis under some
assumptions related to the coerciveness of the cost function. Overall, our
results demonstrate that derivative-free policy optimization offers a
competitive and viable approach for solving general output-feedback
$\mu$-synthesis problems in the model-free setting.
</p>
</div>
</dd>
<dt><a name="item791">[791]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11687" title="Abstract">arXiv:2402.11687</a> (cross-list from quant-ph) [<a href="/pdf/2402.11687" title="Download PDF">pdf</a>, <a href="/format/2402.11687" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating Efficacy of Model Stealing Attacks and Defenses on Quantum  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Kundu%2C+S">Satwik Kundu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Kundu%2C+D">Debarshi Kundu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Ghosh%2C+S">Swaroop Ghosh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
<p class="mathjax">Cloud hosting of quantum machine learning (QML) models exposes them to a
range of vulnerabilities, the most significant of which is the model stealing
attack. In this study, we assess the efficacy of such attacks in the realm of
quantum computing. We conducted comprehensive experiments on various datasets
with multiple QML model architectures. Our findings revealed that model
stealing attacks can produce clone models achieving up to $0.9\times$ and
$0.99\times$ clone test accuracy when trained using Top-$1$ and Top-$k$ labels,
respectively ($k:$ num\_classes). To defend against these attacks, we leverage
the unique properties of current noisy hardware and perturb the victim model
outputs and hinder the attacker's training process. In particular, we propose:
1) hardware variation-induced perturbation (HVIP) and 2) hardware and
architecture variation-induced perturbation (HAVIP). Although noise and
architectural variability can provide up to $\sim16\%$ output obfuscation, our
comprehensive analysis revealed that models cloned under noisy conditions tend
to be resilient, suffering little to no performance degradation due to such
obfuscations. Despite limited success with our defense techniques, this outcome
has led to an important discovery: QML models trained on noisy hardwares are
naturally resistant to perturbation or obfuscation-based defenses or attacks.
</p>
</div>
</dd>
<dt><a name="item792">[792]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11701" title="Abstract">arXiv:2402.11701</a> (cross-list from cond-mat.dis-nn) [<a href="/pdf/2402.11701" title="Download PDF">pdf</a>, <a href="/ps/2402.11701" title="Download PostScript">ps</a>, <a href="/format/2402.11701" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explaining the Machine Learning Solution of the Ising Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Alamino%2C+R+C">Roberto C. Alamino</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Disordered Systems and Neural Networks (cond-mat.dis-nn)</span>; Machine Learning (cs.LG); Computational Physics (physics.comp-ph)

</div>
<p class="mathjax">As powerful as machine learning (ML) techniques are in solving problems
involving data with large dimensionality, explaining the results from the
fitted parameters remains a challenging task of utmost importance, especially
in physics applications. Here it is shown how this can be accomplished for the
ferromagnetic Ising model, the target of many ML studies in the last years. By
using a neural network (NN) without any hidden layers and the symmetry of the
Hamiltonian to find the critical temperature for the continuous phase
transition of the model, an explanation of its strategy is found. This allows
the prediction of the minimal extension of the NN to solve the problem when the
symmetry is not known, which is also explainable.
</p>
</div>
</dd>
<dt><a name="item793">[793]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11705" title="Abstract">arXiv:2402.11705</a> (cross-list from stat.ML) [<a href="/pdf/2402.11705" title="Download PDF">pdf</a>, <a href="/format/2402.11705" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Memory Kernels in Generalized Langevin Equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lang%2C+Q">Quanjun Lang</a>, 
<a href="/search/stat?searchtype=author&query=Lu%2C+J">Jianfeng Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">We introduce a novel approach for learning memory kernels in Generalized
Langevin Equations. This approach initially utilizes a regularized Prony method
to estimate correlation functions from trajectory data, followed by regression
over a Sobolev norm-based loss function with RKHS regularization. Our approach
guarantees improved performance within an exponentially weighted $L^2$ space,
with the kernel estimation error controlled by the error in estimated
correlation functions. We demonstrate the superiority of our estimator compared
to other regression estimators that rely on $L^2$ loss functions and also an
estimator derived from the inverse Laplace transform, using numerical examples
that highlight its consistent advantage across various weight parameter
selections. Additionally, we provide examples that include the application of
force and drift terms in the equation.
</p>
</div>
</dd>
<dt><a name="item794">[794]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11747" title="Abstract">arXiv:2402.11747</a> (cross-list from eess.AS) [<a href="/pdf/2402.11747" title="Download PDF">pdf</a>, <a href="/format/2402.11747" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Parameter Efficient Finetuning for Speech Emotion Recognition and Domain  Adaptation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Lashkarashvili%2C+N">Nineli Lashkarashvili</a>, 
<a href="/search/eess?searchtype=author&query=Wu%2C+W">Wen Wu</a>, 
<a href="/search/eess?searchtype=author&query=Sun%2C+G">Guangzhi Sun</a>, 
<a href="/search/eess?searchtype=author&query=Woodland%2C+P+C">Philip C. Woodland</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Sound (cs.SD)

</div>
<p class="mathjax">Foundation models have shown superior performance for speech emotion
recognition (SER). However, given the limited data in emotion corpora,
finetuning all parameters of large pre-trained models for SER can be both
resource-intensive and susceptible to overfitting. This paper investigates
parameter-efficient finetuning (PEFT) for SER. Various PEFT adaptors are
systematically studied for both classification of discrete emotion categories
and prediction of dimensional emotional attributes. The results demonstrate
that the combination of PEFT methods surpasses full finetuning with a
significant reduction in the number of trainable parameters. Furthermore, a
two-stage adaptation strategy is proposed to adapt models trained on acted
emotion data, which is more readily available, to make the model more adept at
capturing natural emotional expressions. Both intra- and cross-corpus
experiments validate the efficacy of the proposed approach in enhancing the
performance on both the source and target domains.
</p>
</div>
</dd>
<dt><a name="item795">[795]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11758" title="Abstract">arXiv:2402.11758</a> (cross-list from math.CO) [<a href="/pdf/2402.11758" title="Download PDF">pdf</a>, <a href="/format/2402.11758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Conformally rigid graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Steinerberger%2C+S">Stefan Steinerberger</a>, 
<a href="/search/math?searchtype=author&query=Thomas%2C+R+R">Rekha R. Thomas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Optimization and Control (math.OC); Spectral Theory (math.SP)

</div>
<p class="mathjax">Given a finite, simple, connected graph $G=(V,E)$ with $|V|=n$, we consider
the associated graph Laplacian matrix $L = D - A$ with eigenvalues $0 =
\lambda_1 &lt; \lambda_2 \leq \dots \leq \lambda_n$. One can also consider the
same graph equipped with positive edge weights $w:E \rightarrow \mathbb{R}_{&gt;
0}$ normalized to $\sum_{e \in E} w_e = |E|$ and the associated weighted
Laplacian matrix $L_w$. We say that $G$ is conformally rigid if constant
edge-weights maximize the second eigenvalue $\lambda_2(w)$ of $L_w$ over all
$w$, and minimize $\lambda_n(w')$ of $L_{w'}$ over all $w'$, i.e., for all
$w,w'$, $$ \lambda_2(w) \leq \lambda_2(1) \leq \lambda_n(1) \leq
\lambda_n(w').$$ Conformal rigidity requires an extraordinary amount of
symmetry in $G$. Every edge-transitive graph is conformally rigid. We prove
that every distance-regular graph, and hence every strongly-regular graph, is
conformally rigid. Certain special graph embeddings can be used to characterize
conformal rigidity. Cayley graphs can be conformally rigid but need not be, we
prove a sufficient criterion. We also find a small set of conformally rigid
graphs that do not belong into any of the above categories; these include the
Hoffman graph, the crossing number graph 6B and others. Conformal rigidity can
be certified via semidefinite programming, we provide explicit examples.
</p>
</div>
</dd>
<dt><a name="item796">[796]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11775" title="Abstract">arXiv:2402.11775</a> (cross-list from eess.IV) [<a href="/pdf/2402.11775" title="Download PDF">pdf</a>, <a href="/format/2402.11775" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> FOD-Swin-Net: angular super resolution of fiber orientation distribution  using a transformer-based deep model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=da+Silva%2C+M+O">Mateus Oliveira da Silva</a>, 
<a href="/search/eess?searchtype=author&query=Santana%2C+C+P">Caio Pinheiro Santana</a>, 
<a href="/search/eess?searchtype=author&query=Carmo%2C+D+S+d">Diedre Santos do Carmo</a>, 
<a href="/search/eess?searchtype=author&query=Rittner%2C+L">Let&#xed;cia Rittner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at ISBI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
<p class="mathjax">Identifying and characterizing brain fiber bundles can help to understand
many diseases and conditions. An important step in this process is the
estimation of fiber orientations using Diffusion-Weighted Magnetic Resonance
Imaging (DW-MRI). However, obtaining robust orientation estimates demands
high-resolution data, leading to lengthy acquisitions that are not always
clinically available. In this work, we explore the use of automated angular
super resolution from faster acquisitions to overcome this challenge. Using the
publicly available Human Connectome Project (HCP) DW-MRI data, we trained a
transformer-based deep learning architecture to achieve angular super
resolution in fiber orientation distribution (FOD). Our patch-based
methodology, FOD-Swin-Net, is able to bring a single-shell reconstruction
driven from 32 directions to be comparable to a multi-shell 288 direction FOD
reconstruction, greatly reducing the number of required directions on initial
acquisition. Evaluations of the reconstructed FOD with Angular Correlation
Coefficient and qualitative visualizations reveal superior performance than the
state-of-the-art in HCP testing data. Open source code for reproducibility is
available at https://github.com/MICLab-Unicamp/FOD-Swin-Net.
</p>
</div>
</dd>
<dt><a name="item797">[797]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11789" title="Abstract">arXiv:2402.11789</a> (cross-list from stat.ML) [<a href="/pdf/2402.11789" title="Download PDF">pdf</a>, <a href="/format/2402.11789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Test for Generated Hypotheses by Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Katsuoka%2C+T">Teruyuki Katsuoka</a>, 
<a href="/search/stat?searchtype=author&query=Shiraishi%2C+T">Tomohiro Shiraishi</a>, 
<a href="/search/stat?searchtype=author&query=Miwa%2C+D">Daiki Miwa</a>, 
<a href="/search/stat?searchtype=author&query=Duy%2C+V+N+L">Vo Nguyen Le Duy</a>, 
<a href="/search/stat?searchtype=author&query=Takeuchi%2C+I">Ichiro Takeuchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32pages, 6figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">The enhanced performance of AI has accelerated its integration into
scientific research. In particular, the use of generative AI to create
scientific hypotheses is promising and is increasingly being applied across
various fields. However, when employing AI-generated hypotheses for critical
decisions, such as medical diagnoses, verifying their reliability is crucial.
In this study, we consider a medical diagnostic task using generated images by
diffusion models, and propose a statistical test to quantify its reliability.
The basic idea behind the proposed statistical test is to employ a selective
inference framework, where we consider a statistical test conditional on the
fact that the generated images are produced by a trained diffusion model. Using
the proposed method, the statistical reliability of medical image diagnostic
results can be quantified in the form of a p-value, allowing for
decision-making with a controlled error rate. We show the theoretical validity
of the proposed statistical test and its effectiveness through numerical
experiments on synthetic and brain image datasets.
</p>
</div>
</dd>
<dt><a name="item798">[798]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11806" title="Abstract">arXiv:2402.11806</a> (cross-list from quant-ph) [<a href="/pdf/2402.11806" title="Download PDF">pdf</a>, <a href="/format/2402.11806" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Building a Hierarchical Architecture and Communication Model for the  Quantum Internet
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=He%2C+B">Binjie He</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zhang%2C+D">Dong Zhang</a>, 
<a href="/search/quant-ph?searchtype=author&query=Loke%2C+S+W">Seng W. Loke</a>, 
<a href="/search/quant-ph?searchtype=author&query=Lin%2C+S">Shengrui Lin</a>, 
<a href="/search/quant-ph?searchtype=author&query=Lu%2C+L">Luke Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Networking and Internet Architecture (cs.NI)

</div>
<p class="mathjax">The research of architecture has tremendous significance in realizing quantum
Internet. Although there is not yet a standard quantum Internet architecture,
the distributed architecture is one of the possible solutions, which utilizes
quantum repeaters or dedicated entanglement sources in a flat structure for
entanglement preparation &amp; distribution. In this paper, we analyze the
distributed architecture in detail and demonstrate that it has three
limitations: 1) possible high maintenance overhead, 2) possible low-performance
entanglement distribution, and 3) unable to support optimal entanglement
routing. We design a hierarchical quantum Internet architecture and a
communication model to solve the problems above. We also present a W-state
Based Centralized Entanglement Preparation &amp; Distribution (W-state Based CEPD)
scheme and a Centralized Entanglement Routing (CER) algorithm within our
hierarchical architecture and perform an experimental comparison with other
entanglement preparation &amp; distribution schemes and entanglement routing
algorithms within the distributed architecture. The evaluation results show
that the entanglement distribution efficiency of hierarchical architecture is
11.5% higher than that of distributed architecture on average (minimum 3.3%,
maximum 37.3%), and the entanglement routing performance of hierarchical
architecture is much better than that of a distributed architecture according
to the fidelity and throughput.
</p>
</div>
</dd>
<dt><a name="item799">[799]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11830" title="Abstract">arXiv:2402.11830</a> (cross-list from quant-ph) [<a href="/pdf/2402.11830" title="Download PDF">pdf</a>, <a href="/format/2402.11830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Maximum Likelihood Quantum Error Mitigation for Algorithms with a Single  Correct Output
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Baron%2C+D">Dror Baron</a>, 
<a href="/search/quant-ph?searchtype=author&query=Patil%2C+H+P">Hrushikesh Pramod Patil</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zhou%2C+H">Huiyang Zhou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT)

</div>
<p class="mathjax">Quantum error mitigation is an important technique to reduce the impact of
noise in quantum computers. With more and more qubits being supported on
quantum computers, there are two emerging fundamental challenges. First, the
number of shots required for quantum algorithms with large numbers of qubits
needs to increase in order to obtain a meaningful distribution or expected
value of an observable. Second, although steady progress has been made in
improving the fidelity of each qubit, circuits with a large number of qubits
are likely to produce erroneous results. This low-shot, high-noise regime calls
for highly scalable error mitigation techniques. In this paper, we propose a
simple and effective mitigation scheme, qubit-wise majority vote, for quantum
algorithms with a single correct output. We show that our scheme produces the
maximum likelihood (ML) estimate under certain assumptions, and bound the
number of shots required. Our experimental results on real quantum devices
confirm that our proposed approach requires fewer shots than existing ones, and
can sometimes recover the correct answers even when they are not observed from
the measurement results.
</p>
</div>
</dd>
<dt><a name="item800">[800]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11858" title="Abstract">arXiv:2402.11858</a> (cross-list from stat.ML) [<a href="/pdf/2402.11858" title="Download PDF">pdf</a>, <a href="/ps/2402.11858" title="Download PostScript">ps</a>, <a href="/format/2402.11858" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stochastic Hessian Fitting on Lie Group
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Li%2C+X">Xi-Lin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 3 tables, 4 figures (with code for reproducing)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
<p class="mathjax">This paper studies the fitting of Hessian or its inverse with stochastic
Hessian-vector products. A Hessian fitting criterion, which can be used to
derive most of the commonly used methods, e.g., BFGS, Gaussian-Newton, AdaGrad,
etc., is used for the analysis. Our studies reveal different convergence rates
for different Hessian fitting methods, e.g., sublinear rates for gradient
descent in the Euclidean space and a commonly used closed-form solution, linear
rates for gradient descent on the manifold of symmetric positive definite (SPL)
matrices and certain Lie groups. The Hessian fitting problem is further shown
to be strongly convex under mild conditions on a specific yet general enough
Lie group. To confirm our analysis, these methods are tested under different
settings like noisy Hessian-vector products, time varying Hessians, and low
precision arithmetic. These findings are useful for stochastic second order
optimizations that rely on fast, robust and accurate Hessian estimations.
</p>
</div>
</dd>
<dt><a name="item801">[801]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11868" title="Abstract">arXiv:2402.11868</a> (cross-list from physics.comp-ph) [<a href="/pdf/2402.11868" title="Download PDF">pdf</a>, <a href="/ps/2402.11868" title="Download PostScript">ps</a>, <a href="/format/2402.11868" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recent Extensions of the ZKCM Library for Parallel and Accurate MPS  Simulation of Quantum Circuits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=SaiToh%2C+A">Akira SaiToh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 6 pages, 2 figures, under review in the post-conference Proc. CCP2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Mathematical Software (cs.MS); Quantum Physics (quant-ph)

</div>
<p class="mathjax">A C++ library ZKCM and its extension library ZKCM_QC have been developed
since 2011 for multiple-precision matrix computation and accurate
matrix-product-state (MPS) quantum circuit simulation, respectively. In this
report, a recent progress in the extensions of these libraries is described,
which are mainly for parallel processing with the OpenMP and CUDA frameworks.
</p>
</div>
</dd>
<dt><a name="item802">[802]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.11950" title="Abstract">arXiv:2402.11950</a> (cross-list from q-bio.BM) [<a href="/pdf/2402.11950" title="Download PDF">pdf</a>, <a href="/ps/2402.11950" title="Download PostScript">ps</a>, <a href="/format/2402.11950" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A novel molecule generative model of VAE combined with Transformer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Yoshikai%2C+Y">Yasuhiro Yoshikai</a>, 
<a href="/search/q-bio?searchtype=author&query=Mizuno%2C+T">Tadahaya Mizuno</a>, 
<a href="/search/q-bio?searchtype=author&query=Nemoto%2C+S">Shumpei Nemoto</a>, 
<a href="/search/q-bio?searchtype=author&query=Kusuhara%2C+H">Hiroyuki Kusuhara</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Biomolecules (q-bio.BM)</span>; Machine Learning (cs.LG); Chemical Physics (physics.chem-ph)

</div>
<p class="mathjax">Recently, molecule generation using deep learning has been actively
investigated in drug discovery. In this field, Transformer and VAE are widely
used as powerful models, but they are rarely used in combination due to
structural and performance mismatch of them. This study proposes a model that
combines these two models through structural and parameter optimization in
handling diverse molecules. The proposed model shows comparable performance to
existing models in generating molecules, and showed by far superior performance
in generating molecules with unseen structures. In addition, the proposed model
successfully predicted molecular properties using the latent representation of
VAE. Ablation studies suggested the advantage of VAE over other generative
models like language model in generating novel molecules, and that the
molecules can be described by ~32 dimensional variables, much smaller than
existing descriptors and models. This study is expected to provide a virtual
chemical library containing a wide variety of compounds for virtual screening
and to enable efficient screening.
</p>
</div>
</dd>
<dt><a name="item803">[803]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12034" title="Abstract">arXiv:2402.12034</a> (cross-list from stat.ML) [<a href="/pdf/2402.12034" title="Download PDF">pdf</a>, <a href="/format/2402.12034" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Do Off-Policy and On-Policy Policy Gradient Methods Align?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Mambelli%2C+D">Davide Mambelli</a>, 
<a href="/search/stat?searchtype=author&query=Bongers%2C+S">Stephan Bongers</a>, 
<a href="/search/stat?searchtype=author&query=Zoeter%2C+O">Onno Zoeter</a>, 
<a href="/search/stat?searchtype=author&query=Spaan%2C+M+T+J">Matthijs T.J. Spaan</a>, 
<a href="/search/stat?searchtype=author&query=Oliehoek%2C+F+A">Frans A. Oliehoek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Policy gradient methods are widely adopted reinforcement learning algorithms
for tasks with continuous action spaces. These methods succeeded in many
application domains, however, because of their notorious sample inefficiency
their use remains limited to problems where fast and accurate simulations are
available. A common way to improve sample efficiency is to modify their
objective function to be computable from off-policy samples without importance
sampling. A well-established off-policy objective is the excursion objective.
This work studies the difference between the excursion objective and the
traditional on-policy objective, which we refer to as the on-off gap. We
provide the first theoretical analysis showing conditions to reduce the on-off
gap while establishing empirical evidence of shortfalls arising when these
conditions are not met.
</p>
</div>
</dd>
<dt><a name="item804">[804]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12051" title="Abstract">arXiv:2402.12051</a> (cross-list from math.CT) [<a href="/pdf/2402.12051" title="Download PDF">pdf</a>, <a href="/ps/2402.12051" title="Download PostScript">ps</a>, <a href="/format/2402.12051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From higher-order rewriting systems to higher-order categorial algebras  and higher-order Curry-Howard isomorphisms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Vidal%2C+J+C">Juan Climent Vidal</a>, 
<a href="/search/math?searchtype=author&query=Ll%C3%B3pez%2C+E+C">Enric Cosme Ll&#xf3;pez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 814 pages, three parts and an appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Category Theory (math.CT)</span>; Formal Languages and Automata Theory (cs.FL)

</div>
<p class="mathjax">This ongoing project aims to define and investigate, from the standpoint of
category theory, order theory and universal algebra, the notions of
higher-order many-sorted rewriting system and of higher-order many-sorted
categorial algebra and their relationships, via the higher-order Curry-Howard
isomorphisms.
<br />The ultimate goal, to be developed in future versions of this work, is to
define and investigate the category of towers, whose objects will consist of
families, indexed by $\mathbb{N}$, of higher-order many-sorted rewriting
systems and of higher-order many-sorted categorial algebras, including
higher-order Curry-Howard type results for the latter, together with an
additional structure that intertwines such $\mathbb{N}$-families; and whose
morphism from a tower to another will be families, indexed by $\mathbb{N}$, of
morphisms between its higher-order many-sorted rewriting systems and of
higher-order many-sorted categorial algebras compatible with their structures.
All feedback is appreciated.
</p>
</div>
</dd>
<dt><a name="item805">[805]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12072" title="Abstract">arXiv:2402.12072</a> (cross-list from eess.IV) [<a href="/pdf/2402.12072" title="Download PDF">pdf</a>, <a href="/format/2402.12072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robustness and Exploration of Variational and Machine Learning  Approaches to Inverse Problems: An Overview
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Auras%2C+A">Alexander Auras</a>, 
<a href="/search/eess?searchtype=author&query=Gandikota%2C+K+V">Kanchana Vaishnavi Gandikota</a>, 
<a href="/search/eess?searchtype=author&query=Droege%2C+H">Hannah Droege</a>, 
<a href="/search/eess?searchtype=author&query=Moeller%2C+M">Michael Moeller</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Numerical Analysis (math.NA)

</div>
<p class="mathjax">This paper attempts to provide an overview of current approaches for solving
inverse problems in imaging using variational methods and machine learning. A
special focus lies on point estimators and their robustness against adversarial
perturbations. In this context results of numerical experiments for a
one-dimensional toy problem are provided, showing the robustness of different
approaches and empirically verifying theoretical guarantees. Another focus of
this review is the exploration of the subspace of data consistent solutions
through explicit guidance to satisfy specific semantic or textural properties.
</p>
</div>
</dd>
<dt><a name="item806">[806]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12094" title="Abstract">arXiv:2402.12094</a> (cross-list from eess.AS) [<a href="/pdf/2402.12094" title="Download PDF">pdf</a>, <a href="/format/2402.12094" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the relationship between speech and hearing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Umesh%2C+S">Srinivasan Umesh</a>, 
<a href="/search/eess?searchtype=author&query=Cohen%2C+L">Leon Cohen</a>, 
<a href="/search/eess?searchtype=author&query=Nelson%2C+D">Douglas Nelson</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">We present a framework for experimentally linking speech production and
hearing. Using this approach, we describe experimental results, that lead to
the concept that sounds made by different individuals and perceived to be the
same can be transformed into each other by a "speech scale". The speech scale
is empirically determined using only speech data. We show the similarity of the
speech scale to the MEL scale of Stevens and Volkmann, which was derived only
from hearing experiments. We thus experimentally link speech production and
hearing.
</p>
</div>
</dd>
<dt><a name="item807">[807]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12114" title="Abstract">arXiv:2402.12114</a> (cross-list from eess.IV) [<a href="/pdf/2402.12114" title="Download PDF">pdf</a>, <a href="/format/2402.12114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Spatiotemporal Illumination Model for 3D Image Fusion in Optical  Coherence Tomography
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ploner%2C+S">Stefan Ploner</a>, 
<a href="/search/eess?searchtype=author&query=Won%2C+J">Jungeun Won</a>, 
<a href="/search/eess?searchtype=author&query=Schottenhamml%2C+J">Julia Schottenhamml</a>, 
<a href="/search/eess?searchtype=author&query=Girgis%2C+J">Jessica Girgis</a>, 
<a href="/search/eess?searchtype=author&query=Lam%2C+K">Kenneth Lam</a>, 
<a href="/search/eess?searchtype=author&query=Waheed%2C+N">Nadia Waheed</a>, 
<a href="/search/eess?searchtype=author&query=Fujimoto%2C+J">James Fujimoto</a>, 
<a href="/search/eess?searchtype=author&query=Maier%2C+A">Andreas Maier</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Presented orally &amp; as poster on 20th April 2023 at the IEEE International Symposium on Biomedical Imaging (ISBI) in Cartagena, Colombia. 6 pages, 3 figures. You can find the official version with broken equations and bad contrast figures under <a href="https://ieeexplore.ieee.org/document/10230526">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
<p class="mathjax">Optical coherence tomography (OCT) is a non-invasive, micrometer-scale
imaging modality that has become a clinical standard in ophthalmology. By
raster-scanning the retina, sequential cross-sectional image slices are
acquired to generate volumetric data. In-vivo imaging suffers from
discontinuities between slices that show up as motion and illumination
artifacts. We present a new illumination model that exploits continuity in
orthogonally raster-scanned volume data. Our novel spatiotemporal
parametrization adheres to illumination continuity both temporally, along the
imaged slices, as well as spatially, in the transverse directions. Yet, our
formulation does not make inter-slice assumptions, which could have
discontinuities. This is the first optimization of a 3D inverse model in an
image reconstruction context in OCT. Evaluation in 68 volumes from eyes with
pathology showed reduction of illumination artifacts in 88\% of the data, and
only 6\% showed moderate residual illumination artifacts. The method enables
the use of forward-warped motion corrected data, which is more accurate, and
enables supersampling and advanced 3D image reconstruction in OCT.
</p>
</div>
</dd>
<dt><a name="item808">[808]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12134" title="Abstract">arXiv:2402.12134</a> (cross-list from physics.chem-ph) [<a href="/pdf/2402.12134" title="Download PDF">pdf</a>, <a href="/ps/2402.12134" title="Download PostScript">ps</a>, <a href="/format/2402.12134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Molecule Generation and Optimization for Efficient Fragrance Creation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Rodrigues%2C+B+C+L">Bruno C. L. Rodrigues</a>, 
<a href="/search/physics?searchtype=author&query=Santana%2C+V+V">Vinicius V. Santana</a>, 
<a href="/search/physics?searchtype=author&query=Murins%2C+S">Sandris Murins</a>, 
<a href="/search/physics?searchtype=author&query=Nogueira%2C+I+B+R">Idelfonso B. R. Nogueira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Chemical Physics (physics.chem-ph)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">This research introduces a Machine Learning-centric approach to replicate
olfactory experiences, validated through experimental quantification of perfume
perception. Key contributions encompass a hybrid model connecting perfume
molecular structure to human olfactory perception. This model includes an
AI-driven molecule generator (utilizing Graph and Generative Neural Networks),
quantification and prediction of odor intensity, and refinery of optimal
solvent and molecule combinations for desired fragrances. Additionally, a
thermodynamic-based model establishes a link between olfactory perception and
liquid-phase concentrations. The methodology employs Transfer Learning and
selects the most suitable molecules based on vapor pressure and fragrance
notes. Ultimately, a mathematical optimization problem is formulated to
minimize discrepancies between new and target olfactory experiences. The
methodology is validated by reproducing two distinct olfactory experiences
using available experimental data.
</p>
</div>
</dd>
<dt><a name="item809">[809]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12182" title="Abstract">arXiv:2402.12182</a> (cross-list from math.OC) [<a href="/pdf/2402.12182" title="Download PDF">pdf</a>, <a href="/ps/2402.12182" title="Download PostScript">ps</a>, <a href="/format/2402.12182" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Riemannian rank-adaptive method for higher-order tensor completion in  the tensor-train format
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Vermeylen%2C+C">Charlotte Vermeylen</a>, 
<a href="/search/math?searchtype=author&query=Van+Barel%2C+M">Marc Van Barel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
<p class="mathjax">In this paper a new Riemannian rank adaptive method (RRAM) is proposed for
the low-rank tensor completion problem (LRTCP) formulated as a least-squares
optimization problem on the algebraic variety of tensors of bounded
tensor-train (TT) rank. The method iteratively optimizes over fixed-rank smooth
manifolds using a Riemannian conjugate gradient algorithm from Steinlechner
(2016) and gradually increases the rank by computing a descent direction in the
tangent cone to the variety. Additionally, a numerical method to estimate the
amount of rank increase is proposed based on a theoretical result for the
stationary points of the low-rank tensor approximation problem and a definition
of an estimated TT-rank. Furthermore, when the iterate comes close to a
lower-rank set, the RRAM decreases the rank based on the TT-rounding algorithm
from Oseledets (2011) and a definition of a numerical rank. We prove that the
TT-rounding algorithm can be considered as an approximate projection onto the
lower-rank set which satisfies a certain angle condition to ensure that the
image is sufficiently close to that of an exact projection. Several numerical
experiments are given to illustrate the use of the RRAM and its subroutines in
{\Matlab}. Furthermore, in all experiments the proposed RRAM outperforms the
state-of-the-art RRAM for tensor completion in the TT format from Steinlechner
(2016) in terms of computation time.
</p>
</div>
</dd>
<dt><a name="item810">[810]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12188" title="Abstract">arXiv:2402.12188</a> (cross-list from q-bio.NC) [<a href="/pdf/2402.12188" title="Download PDF">pdf</a>, <a href="/format/2402.12188" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure of activity in multiregion recurrent neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Clark%2C+D+G">David G. Clark</a>, 
<a href="/search/q-bio?searchtype=author&query=Beiran%2C+M">Manuel Beiran</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Neural and Evolutionary Computing (cs.NE)

</div>
<p class="mathjax">Neural circuits are composed of multiple regions, each with rich dynamics and
engaging in communication with other regions. The combination of local,
within-region dynamics and global, network-level dynamics is thought to provide
computational flexibility. However, the nature of such multiregion dynamics and
the underlying synaptic connectivity patterns remain poorly understood. Here,
we study the dynamics of recurrent neural networks with multiple interconnected
regions. Within each region, neurons have a combination of random and
structured recurrent connections. Motivated by experimental evidence of
communication subspaces between cortical areas, these networks have low-rank
connectivity between regions, enabling selective routing of activity. These
networks exhibit two interacting forms of dynamics: high-dimensional
fluctuations within regions and low-dimensional signal transmission between
regions. To characterize this interaction, we develop a dynamical mean-field
theory to analyze such networks in the limit where each region contains
infinitely many neurons, with cross-region currents as key order parameters.
Regions can act as both generators and transmitters of activity, roles that we
show are in conflict. Specifically, taming the complexity of activity within a
region is necessary for it to route signals to and from other regions. Unlike
previous models of routing in neural circuits, which suppressed the activities
of neuronal groups to control signal flow, routing in our model is achieved by
exciting different high-dimensional activity patterns through a combination of
connectivity structure and nonlinear recurrent dynamics. This theory provides
insight into the interpretation of both multiregion neural data and trained
neural networks.
</p>
</div>
</dd>
<dt><a name="item811">[811]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12190" title="Abstract">arXiv:2402.12190</a> (cross-list from stat.ML) [<a href="/pdf/2402.12190" title="Download PDF">pdf</a>, <a href="/format/2402.12190" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards AI-Based Precision Oncology: A Machine Learning Framework for  Personalized Counterfactual Treatment Suggestions based on Multi-Omics Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Sch%C3%BCrch%2C+M">Manuel Sch&#xfc;rch</a>, 
<a href="/search/stat?searchtype=author&query=Boos%2C+L">Laura Boos</a>, 
<a href="/search/stat?searchtype=author&query=Heinzelmann-Schwarz%2C+V">Viola Heinzelmann-Schwarz</a>, 
<a href="/search/stat?searchtype=author&query=Gut%2C+G">Gabriele Gut</a>, 
<a href="/search/stat?searchtype=author&query=Krauthammer%2C+M">Michael Krauthammer</a>, 
<a href="/search/stat?searchtype=author&query=Wicki%2C+A">Andreas Wicki</a>, 
<a href="/search/stat?searchtype=author&query=Consortium%2C+T+P">Tumor Profiler Consortium</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Quantitative Methods (q-bio.QM)

</div>
<p class="mathjax">AI-driven precision oncology has the transformative potential to reshape
cancer treatment by leveraging the power of AI models to analyze the
interaction between complex patient characteristics and their corresponding
treatment outcomes. New technological platforms have facilitated the timely
acquisition of multimodal data on tumor biology at an unprecedented resolution,
such as single-cell multi-omics data, making this quality and quantity of data
available for data-driven improved clinical decision-making. In this work, we
propose a modular machine learning framework designed for personalized
counterfactual cancer treatment suggestions based on an ensemble of machine
learning experts trained on diverse multi-omics technologies. These specialized
counterfactual experts per technology are consistently aggregated into a more
powerful expert with superior performance and can provide both confidence and
an explanation of its decision. The framework is tailored to address critical
challenges inherent in data-driven cancer research, including the
high-dimensional nature of the data, and the presence of treatment assignment
bias in the retrospective observational data. The framework is showcased
through comprehensive demonstrations using data from in-vitro and in-vivo
treatment responses from a cohort of patients with ovarian cancer. Our method
aims to empower clinicians with a reality-centric decision-support tool
including probabilistic treatment suggestions with calibrated confidence and
personalized explanations for tailoring treatment strategies to multi-omics
characteristics of individual cancer patients.
</p>
</div>
</dd>
<dt><a name="item812">[812]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12208" title="Abstract">arXiv:2402.12208</a> (cross-list from eess.AS) [<a href="/pdf/2402.12208" title="Download PDF">pdf</a>, <a href="/format/2402.12208" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Language-Codec: Reducing the Gaps Between Discrete Codec Representation  and Speech Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ji%2C+S">Shengpeng Ji</a>, 
<a href="/search/eess?searchtype=author&query=Fang%2C+M">Minghui Fang</a>, 
<a href="/search/eess?searchtype=author&query=Jiang%2C+Z">Ziyue Jiang</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+R">Rongjie Huang</a>, 
<a href="/search/eess?searchtype=author&query=Zuo%2C+J">Jialung Zuo</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+S">Shulei Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zhao%2C+Z">Zhou Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Sound (cs.SD)

</div>
<p class="mathjax">In recent years, large language models have achieved significant success in
generative tasks (e.g., speech cloning and audio generation) related to speech,
audio, music, and other signal domains. A crucial element of these models is
the discrete acoustic codecs, which serves as an intermediate representation
replacing the mel-spectrogram. However, there exist several gaps between
discrete codecs and downstream speech language models. Specifically, 1) most
codec models are trained on only 1,000 hours of data, whereas most speech
language models are trained on 60,000 hours; 2) Achieving good reconstruction
performance requires the utilization of numerous codebooks, which increases the
burden on downstream speech language models; 3) The initial channel of the
codebooks contains excessive information, making it challenging to directly
generate acoustic tokens from weakly supervised signals such as text in
downstream tasks. Consequently, leveraging the characteristics of speech
language models, we propose Language-Codec. In the Language-Codec, we introduce
a Mask Channel Residual Vector Quantization (MCRVQ) mechanism along with
improved Fourier transform structures and larger training datasets to address
the aforementioned gaps. We compare our method with competing audio compression
algorithms and observe significant outperformance across extensive evaluations.
Furthermore, we also validate the efficiency of the Language-Codec on
downstream speech language models. The source code and pre-trained models can
be accessed at https://github.com/speechnovateur/languagecodec_tmp .
</p>
</div>
</dd>
<dt><a name="item813">[813]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12220" title="Abstract">arXiv:2402.12220</a> (cross-list from eess.AS) [<a href="/pdf/2402.12220" title="Download PDF">pdf</a>, <a href="/ps/2402.12220" title="Download PostScript">ps</a>, <a href="/format/2402.12220" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic  Forgetting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+H">Haolin Chen</a>, 
<a href="/search/eess?searchtype=author&query=Garner%2C+P+N">Philip N. Garner</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Although motivated by the adaptation of text-to-speech synthesis models, we
argue that more generic parameter-efficient fine-tuning (PEFT) is an
appropriate framework to do such adaptation. However, catastrophic forgetting
remains an issue with PEFT, damaging the pre-trained model's inherent
capabilities. We demonstrate that existing Bayesian learning techniques can be
applied to PEFT to prevent catastrophic forgetting as long as the parameter
shift of the fine-tuned layers can be calculated differentiably. In a
principled series of experiments on language modeling and speech synthesis
tasks, we utilize established Laplace approximations, including diagonal and
Kronecker factored approaches, to regularize PEFT with the low-rank adaptation
(LoRA) and compare their performance in pre-training knowledge preservation.
Our results demonstrate that catastrophic forgetting can be overcome by our
methods without degrading the fine-tuning performance, and using the Kronecker
factored approximations produces a better preservation of the pre-training
knowledge than the diagonal ones.
</p>
</div>
</dd>
<dt><a name="item814">[814]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12223" title="Abstract">arXiv:2402.12223</a> (cross-list from physics.comp-ph) [<a href="/pdf/2402.12223" title="Download PDF">pdf</a>, <a href="/format/2402.12223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Second Order Meanfield Approximation for calculating Dynamics in  Au-Nanoparticle Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Wonisch%2C+E">Evan Wonisch</a>, 
<a href="/search/physics?searchtype=author&query=Mensing%2C+J">Jonas Mensing</a>, 
<a href="/search/physics?searchtype=author&query=Heuer%2C+A">Andreas Heuer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Emerging Technologies (cs.ET); Numerical Analysis (math.NA); Data Analysis, Statistics and Probability (physics.data-an)

</div>
<p class="mathjax">Exploiting physical processes for fast and energy-efficient computation bears
great potential in the advancement of modern hardware components. This paper
explores non-linear charge tunneling in nanoparticle networks, controlled by
external voltages. The dynamics are described by a master equation, which
describes the development of a distribution function over the set of charge
occupation numbers. The driving force behind this evolution are charge
tunneling events among nanoparticles and their associated rates. In this paper,
we introduce two meanfield approximations to this master equation. By
parametrization of the distribution function using its first- and second-order
statistical moments, and a subsequent projection of the dynamics onto the
resulting moment manifold, one can deterministically calculate expected charges
and currents. Unlike a kinetic Monte Carlo approach, which extracts samples
from the distribution function, this meanfield approach avoids any random
elements. A comparison of results between the meanfield approximation and an
already available kinetic Monte Carlo simulation demonstrates great accuracy.
Our analysis also reveals that transitioning from a first-order to a
second-order approximation significantly enhances the accuracy. Furthermore, we
demonstrate the applicability of our approach to time-dependent simulations,
using eulerian time-integration schemes.
</p>
</div>
</dd>
<dt><a name="item815">[815]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12232" title="Abstract">arXiv:2402.12232</a> (cross-list from stat.ML) [<a href="/pdf/2402.12232" title="Download PDF">pdf</a>, <a href="/format/2402.12232" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kernel KMeans clustering splits for end-to-end unsupervised decision  trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Ohl%2C+L">Louis Ohl</a>, 
<a href="/search/stat?searchtype=author&query=Mattei%2C+P">Pierre-Alexandre Mattei</a>, 
<a href="/search/stat?searchtype=author&query=Leclercq%2C+M">Micka&#xeb;l Leclercq</a>, 
<a href="/search/stat?searchtype=author&query=Droit%2C+A">Arnaud Droit</a>, 
<a href="/search/stat?searchtype=author&query=Precioso%2C+F">Fr&#xe9;d&#xe9;ric Precioso</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
<p class="mathjax">Trees are convenient models for obtaining explainable predictions on
relatively small datasets. Although there are many proposals for the end-to-end
construction of such trees in supervised learning, learning a tree end-to-end
for clustering without labels remains an open challenge. As most works focus on
interpreting with trees the result of another clustering algorithm, we present
here a novel end-to-end trained unsupervised binary tree for clustering: Kauri.
This method performs a greedy maximisation of the kernel KMeans objective
without requiring the definition of centroids. We compare this model on
multiple datasets with recent unsupervised trees and show that Kauri performs
identically when using a linear kernel. For other kernels, Kauri often
outperforms the concatenation of kernel KMeans and a CART decision tree.
</p>
</div>
</dd>
<dt><a name="item816">[816]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12239" title="Abstract">arXiv:2402.12239</a> (cross-list from eess.SP) [<a href="/pdf/2402.12239" title="Download PDF">pdf</a>, <a href="/format/2402.12239" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Significance of Chirp MFCC as a Feature in Speech and Audio Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Joysingh%2C+S+J">S. Johanan Joysingh</a>, 
<a href="/search/eess?searchtype=author&query=Vijayalakshmi%2C+P">P. Vijayalakshmi</a>, 
<a href="/search/eess?searchtype=author&query=Nagarajan%2C+T">T. Nagarajan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)

</div>
<p class="mathjax">A novel feature, based on the chirp z-transform, that offers an improved
representation of the underlying true spectrum is proposed. This feature, the
chirp MFCC, is derived by computing the Mel frequency cepstral coefficients
from the chirp magnitude spectrum, instead of the Fourier transform magnitude
spectrum. The theoretical foundations for the proposal, and the experimental
validation using product of likelihood Gaussians, to show the improved class
separation offered by the proposed chirp MFCC, when compared with vanilla MFCC
are discussed. Further, real world evaluation of the feature is performed using
three diverse tasks, namely, speech-music classification, speaker
identification, and speech commands recognition. It is shown in all three tasks
that the proposed chirp MFCC offers considerable improvements.
</p>
</div>
</dd>
<dt><a name="item817">[817]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12268" title="Abstract">arXiv:2402.12268</a> (cross-list from math.CO) [<a href="/pdf/2402.12268" title="Download PDF">pdf</a>, <a href="/ps/2402.12268" title="Download PostScript">ps</a>, <a href="/format/2402.12268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Quantitative Fractional Helly theorem
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Frankl%2C+N">N&#xf3;ra Frankl</a>, 
<a href="/search/math?searchtype=author&query=Jung%2C+A">Attila Jung</a>, 
<a href="/search/math?searchtype=author&query=Tomon%2C+I">Istv&#xe1;n Tomon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Geometry (cs.CG); Metric Geometry (math.MG)

</div>
<p class="mathjax">Two celebrated extensions of Helly's theorem are the Fractional Helly theorem
of Katchalski and Liu (1979) and the Quantitative Volume theorem of B\'ar\'any,
Katchalski, and Pach (1982). Improving on several recent works, we prove an
optimal combination of these two results. We show that given a family
$\mathcal{F}$ of $n$ convex sets in $\mathbb{R}^d$ such that at least $\alpha
\binom{n}{d+1}$ of the $(d+1)$-tuples of $\mathcal{F}$ have an intersection of
volume at least 1, then one can select $\Omega_{d,\alpha}(n)$ members of
$\mathcal{F}$ whose intersection has volume at least $\Omega_d(1)$.
<br />Furthermore, with the help of this theorem, we establish a quantitative
version of the $(p,q)$ theorem of Alon and Kleitman. Let $p\geq q\geq d+1$ and
let $\mathcal{F}$ be a finite family of convex sets in $\mathbb{R}^d$ such that
among any $p$ elements of $\mathcal{F}$, there are $q$ that have an
intersection of volume at least $1$. Then, we prove that there exists a family
$T$ of $O_{p,q}(1)$ ellipsoids of volume $\Omega_d(1)$ such that every member
of $\mathcal{F}$ contains at least one element of $T$.
</p>
</div>
</dd>
<dt><a name="item818">[818]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12292" title="Abstract">arXiv:2402.12292</a> (cross-list from stat.ML) [<a href="/pdf/2402.12292" title="Download PDF">pdf</a>, <a href="/format/2402.12292" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regularization by denoising: Bayesian model and Langevin-within-split  Gibbs sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Faye%2C+E+C">Elhadji C. Faye</a>, 
<a href="/search/stat?searchtype=author&query=Fall%2C+M+D">Mame Diarra Fall</a>, 
<a href="/search/stat?searchtype=author&query=Dobigeon%2C+N">Nicolas Dobigeon</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
<p class="mathjax">This paper introduces a Bayesian framework for image inversion by deriving a
probabilistic counterpart to the regularization-by-denoising (RED) paradigm. It
additionally implements a Monte Carlo algorithm specifically tailored for
sampling from the resulting posterior distribution, based on an asymptotically
exact data augmentation (AXDA). The proposed algorithm is an approximate
instance of split Gibbs sampling (SGS) which embeds one Langevin Monte Carlo
step. The proposed method is applied to common imaging tasks such as
deblurring, inpainting and super-resolution, demonstrating its efficacy through
extensive numerical experiments. These contributions advance Bayesian inference
in imaging by leveraging data-driven regularization strategies within a
probabilistic framework.
</p>
</div>
</dd>
<dt><a name="item819">[819]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12302" title="Abstract">arXiv:2402.12302</a> (cross-list from stat.ML) [<a href="/pdf/2402.12302" title="Download PDF">pdf</a>, <a href="/format/2402.12302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asymptotic Gaussian Fluctuations of Eigenvectors in Spectral Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lebeau%2C+H">Hugo Lebeau</a>, 
<a href="/search/stat?searchtype=author&query=Chatelain%2C+F">Florent Chatelain</a>, 
<a href="/search/stat?searchtype=author&query=Couillet%2C+R">Romain Couillet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Probability (math.PR)

</div>
<p class="mathjax">The performance of spectral clustering relies on the fluctuations of the
entries of the eigenvectors of a similarity matrix, which has been left
uncharacterized until now. In this letter, it is shown that the signal $+$
noise structure of a general spike random matrix model is transferred to the
eigenvectors of the corresponding Gram kernel matrix and the fluctuations of
their entries are Gaussian in the large-dimensional regime. This CLT-like
result was the last missing piece to precisely predict the classification
performance of spectral clustering. The proposed proof is very general and
relies solely on the rotational invariance of the noise. Numerical experiments
on synthetic and real data illustrate the universality of this phenomenon.
</p>
</div>
</dd>
<dt><a name="item820">[820]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.12369" title="Abstract">arXiv:2402.12369</a> (cross-list from astro-ph.SR) [<a href="/pdf/2402.12369" title="Download PDF">pdf</a>, <a href="/format/2402.12369" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Short-Period Variables in TESS Full-Frame Image Light Curves Identified  via Convolutional Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/astro-ph?searchtype=author&query=Olmschenk%2C+G">Greg Olmschenk</a>, 
<a href="/search/astro-ph?searchtype=author&query=Barry%2C+R+K">Richard K. Barry</a>, 
<a href="/search/astro-ph?searchtype=author&query=Silva%2C+S+I">Stela Ishitani Silva</a>, 
<a href="/search/astro-ph?searchtype=author&query=Powell%2C+B+P">Brian P. Powell</a>, 
<a href="/search/astro-ph?searchtype=author&query=Kruse%2C+E">Ethan Kruse</a>, 
<a href="/search/astro-ph?searchtype=author&query=Schnittman%2C+J+D">Jeremy D. Schnittman</a>, 
<a href="/search/astro-ph?searchtype=author&query=Cieplak%2C+A+M">Agnieszka M. Cieplak</a>, 
<a href="/search/astro-ph?searchtype=author&query=Barclay%2C+T">Thomas Barclay</a>, 
<a href="/search/astro-ph?searchtype=author&query=Solanki%2C+S">Siddhant Solanki</a>, 
<a href="/search/astro-ph?searchtype=author&query=Ortega%2C+B">Bianca Ortega</a>, 
<a href="/search/astro-ph?searchtype=author&query=Baker%2C+J">John Baker</a>, 
<a href="/search/astro-ph?searchtype=author&query=Mamani%2C+Y+H+S">Yesenia Helem Salinas Mamani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Solar and Stellar Astrophysics (astro-ph.SR)</span>; Earth and Planetary Astrophysics (astro-ph.EP); Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (cs.LG); Image and Video Processing (eess.IV)

</div>
<p class="mathjax">The Transiting Exoplanet Survey Satellite (TESS) mission measured light from
stars in ~85% of the sky throughout its two-year primary mission, resulting in
millions of TESS 30-minute cadence light curves to analyze in the search for
transiting exoplanets. To search this vast dataset, we aim to provide an
approach that is both computationally efficient, produces highly performant
predictions, and minimizes the required human search effort. We present a
convolutional neural network that we train to identify short period variables.
To make a prediction for a given light curve, our network requires no prior
target parameters identified using other methods. Our network performs
inference on a TESS 30-minute cadence light curve in ~5ms on a single GPU,
enabling large scale archival searches. We present a collection of 14156
short-period variables identified by our network. The majority of our
identified variables fall into two prominent populations, one of short-period
main sequence binaries and another of Delta Scuti stars. Our neural network
model and related code is additionally provided as open-source code for public
use and extension.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Tue, 20 Feb 24</h3>
<dl>
<dt><a name="item821">[821]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1212.0494" title="Abstract">arXiv:1212.0494</a> (replaced) [<a href="/pdf/1212.0494" title="Download PDF">pdf</a>, <a href="/ps/1212.0494" title="Download PostScript">ps</a>, <a href="/format/1212.0494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identification Via Quantum Channels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Winter%2C+A">Andreas Winter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Review article for the LNCS volume in memory of Rudolf Ahlswede. 17 pages, requires Springer llncs class. V2 corrects a few annoying little errors and reports on recent progress regarding simultaneous vs non-simultaneous ID
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Lecture Notes in Computer Science, vol. 7777 (2013), pp. 217-233
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT); Mathematical Physics (math-ph)

</div>
</div>
</dd>
<dt><a name="item822">[822]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1606.03137" title="Abstract">arXiv:1606.03137</a> (replaced) [<a href="/pdf/1606.03137" title="Download PDF">pdf</a>, <a href="/format/1606.03137" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cooperative Inverse Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hadfield-Menell%2C+D">Dylan Hadfield-Menell</a>, 
<a href="/search/cs?searchtype=author&query=Dragan%2C+A">Anca Dragan</a>, 
<a href="/search/cs?searchtype=author&query=Abbeel%2C+P">Pieter Abbeel</a>, 
<a href="/search/cs?searchtype=author&query=Russell%2C+S">Stuart Russell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item823">[823]</a>&nbsp;  <span class="list-identifier"><a href="/abs/1910.08597" title="Abstract">arXiv:1910.08597</a> (replaced) [<a href="/pdf/1910.08597" title="Download PDF">pdf</a>, <a href="/format/1910.08597" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Learning Rate Selection for Stochastic Optimization via Splitting  Diagnostic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Sordello%2C+M">Matteo Sordello</a>, 
<a href="/search/stat?searchtype=author&query=Dalmasso%2C+N">Niccol&#xf2; Dalmasso</a>, 
<a href="/search/stat?searchtype=author&query=He%2C+H">Hangfeng He</a>, 
<a href="/search/stat?searchtype=author&query=Su%2C+W">Weijie Su</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item824">[824]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2008.05966" title="Abstract">arXiv:2008.05966</a> (replaced) [<a href="/pdf/2008.05966" title="Download PDF">pdf</a>, <a href="/format/2008.05966" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep-Lock: Secure Authorization for Deep Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alam%2C+M">Manaar Alam</a>, 
<a href="/search/cs?searchtype=author&query=Saha%2C+S">Sayandeep Saha</a>, 
<a href="/search/cs?searchtype=author&query=Mukhopadhyay%2C+D">Debdeep Mukhopadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Kundu%2C+S">Sandip Kundu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item825">[825]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.02493" title="Abstract">arXiv:2104.02493</a> (replaced) [<a href="/pdf/2104.02493" title="Download PDF">pdf</a>, <a href="/format/2104.02493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RadarScenes: A Real-World Radar Point Cloud Data Set for Automotive  Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schumann%2C+O">Ole Schumann</a>, 
<a href="/search/cs?searchtype=author&query=Hahn%2C+M">Markus Hahn</a>, 
<a href="/search/cs?searchtype=author&query=Scheiner%2C+N">Nicolas Scheiner</a>, 
<a href="/search/cs?searchtype=author&query=Weishaupt%2C+F">Fabio Weishaupt</a>, 
<a href="/search/cs?searchtype=author&query=Tilly%2C+J+F">Julius F. Tilly</a>, 
<a href="/search/cs?searchtype=author&query=Dickmann%2C+J">J&#xfc;rgen Dickmann</a>, 
<a href="/search/cs?searchtype=author&query=W%C3%B6hler%2C+C">Christian W&#xf6;hler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item826">[826]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2104.08928" title="Abstract">arXiv:2104.08928</a> (replaced) [<a href="/pdf/2104.08928" title="Download PDF">pdf</a>, <a href="/format/2104.08928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Group-Sparse Matrix Factorization for Transfer Learning of Word  Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Xu%2C+K">Kan Xu</a>, 
<a href="/search/stat?searchtype=author&query=Zhao%2C+X">Xuanyi Zhao</a>, 
<a href="/search/stat?searchtype=author&query=Bastani%2C+H">Hamsa Bastani</a>, 
<a href="/search/stat?searchtype=author&query=Bastani%2C+O">Osbert Bastani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item827">[827]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.02743" title="Abstract">arXiv:2107.02743</a> (replaced) [<a href="/pdf/2107.02743" title="Download PDF">pdf</a>, <a href="/ps/2107.02743" title="Download PostScript">ps</a>, <a href="/format/2107.02743" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Submodular Order Functions and Assortment Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Udwani%2C+R">Rajan Udwani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in Management Science
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item828">[828]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2107.13429" title="Abstract">arXiv:2107.13429</a> (replaced) [<a href="/pdf/2107.13429" title="Download PDF">pdf</a>, <a href="/format/2107.13429" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Task-Specific Normalization for Continual Learning of Blind Image  Quality Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weixia Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+K">Kede Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+G">Guangtao Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xiaokang Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE T-IP
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item829">[829]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2108.00588" title="Abstract">arXiv:2108.00588</a> (replaced) [<a href="/pdf/2108.00588" title="Download PDF">pdf</a>, <a href="/format/2108.00588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Measuring User Experience Inclusivity in Human-AI Interaction via Five  User Problem-Solving Styles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Anderson%2C+A">Andrew Anderson</a>, 
<a href="/search/cs?searchtype=author&query=Guevara%2C+J+N">Jimena Noa Guevara</a>, 
<a href="/search/cs?searchtype=author&query=Moussaoui%2C+F">Fatima Moussaoui</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Vorvoreanu%2C+M">Mihaela Vorvoreanu</a>, 
<a href="/search/cs?searchtype=author&query=Burnett%2C+M">Margaret Burnett</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item830">[830]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2108.04786" title="Abstract">arXiv:2108.04786</a> (replaced) [<a href="/pdf/2108.04786" title="Download PDF">pdf</a>, <a href="/ps/2108.04786" title="Download PostScript">ps</a>, <a href="/format/2108.04786" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tangled Paths: A Random Graph Model from Mallows Permutations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Enright%2C+J">Jessica Enright</a>, 
<a href="/search/math?searchtype=author&query=Meeks%2C+K">Kitty Meeks</a>, 
<a href="/search/math?searchtype=author&query=Pettersson%2C+W">William Pettersson</a>, 
<a href="/search/math?searchtype=author&query=Sylvester%2C+J">John Sylvester</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 36 pages, 7 figures. Strengthened Theorems 1.1 &amp; 1.4
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item831">[831]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2108.06663" title="Abstract">arXiv:2108.06663</a> (replaced) [<a href="/pdf/2108.06663" title="Download PDF">pdf</a>, <a href="/format/2108.06663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> HCR-Net: A deep learning based script independent handwritten character  recognition network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chauhan%2C+V+K">Vinod Kumar Chauhan</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Sukhdeep Singh</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Anuj Sharma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted to Multimedia Tools and Applications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item832">[832]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.08902" title="Abstract">arXiv:2110.08902</a> (replaced) [<a href="/pdf/2110.08902" title="Download PDF">pdf</a>, <a href="/format/2110.08902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variance Reduction Based Experience Replay for Policy Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Hua Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+W">Wei Xie</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+M+B">M. Ben Feng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 57 pages; Previously this version appeared as <a href="/abs/2208.12341">arXiv:2208.12341</a> which was submitted as a new work by accident
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item833">[833]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2110.12616" title="Abstract">arXiv:2110.12616</a> (replaced) [<a href="/pdf/2110.12616" title="Download PDF">pdf</a>, <a href="/format/2110.12616" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On query complexity measures and their relations for symmetric functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Mittal%2C+R">Rajat Mittal</a>, 
<a href="/search/quant-ph?searchtype=author&query=Nair%2C+S+S">Sanjay S Nair</a>, 
<a href="/search/quant-ph?searchtype=author&query=Patro%2C+S">Sunayana Patro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computational Complexity (cs.CC)

</div>
</div>
</dd>
<dt><a name="item834">[834]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2201.11653" title="Abstract">arXiv:2201.11653</a> (replaced) [<a href="/pdf/2201.11653" title="Download PDF">pdf</a>, <a href="/format/2201.11653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Representations learnt by SGD and Adaptive learning rules: Conditions  that vary sparsity and selectivity in neural network
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Park%2C+J+H">Jin Hyun Park</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item835">[835]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.00789" title="Abstract">arXiv:2202.00789</a> (replaced) [<a href="/pdf/2202.00789" title="Download PDF">pdf</a>, <a href="/format/2202.00789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Team Belief DAG: Generalizing the Sequence Form to Team Games for Fast  Computation of Correlated Team Max-Min Equilibria via Regret Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B+H">Brian Hu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Farina%2C+G">Gabriele Farina</a>, 
<a href="/search/cs?searchtype=author&query=Sandholm%2C+T">Tuomas Sandholm</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item836">[836]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2202.07108" title="Abstract">arXiv:2202.07108</a> (replaced) [<a href="/pdf/2202.07108" title="Download PDF">pdf</a>, <a href="/ps/2202.07108" title="Download PostScript">ps</a>, <a href="/format/2202.07108" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic optical contrast imaging for real-time delineation of tumor  resection margins using head and neck cancer as a model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Hu%2C+Y">Yong Hu</a>, 
<a href="/search/eess?searchtype=author&query=Huang%2C+S">Shan Huang</a>, 
<a href="/search/eess?searchtype=author&query=Han%2C+A+Y">Albert Y. Han</a>, 
<a href="/search/eess?searchtype=author&query=Moon%2C+S">Seong Moon</a>, 
<a href="/search/eess?searchtype=author&query=Krane%2C+J+F">Jeffrey F. Krane</a>, 
<a href="/search/eess?searchtype=author&query=Stafsudd%2C+O">Oscar Stafsudd</a>, 
<a href="/search/eess?searchtype=author&query=Grundfest%2C+W">Warren Grundfest</a>, 
<a href="/search/eess?searchtype=author&query=John%2C+M+A+S">Maie A. St. John</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 21 pages, 7 figures and 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Systems and Control (eess.SY); Biological Physics (physics.bio-ph); Medical Physics (physics.med-ph)

</div>
</div>
</dd>
<dt><a name="item837">[837]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.02511" title="Abstract">arXiv:2203.02511</a> (replaced) [<a href="/pdf/2203.02511" title="Download PDF">pdf</a>, <a href="/format/2203.02511" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Supervised Learning for Joint Pushing and Grasping Policies in  Highly Cluttered Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mokhtar%2C+K">Kamal Mokhtar</a>, 
<a href="/search/cs?searchtype=author&query=Heemskerk%2C+C">Cock Heemskerk</a>, 
<a href="/search/cs?searchtype=author&query=Kasaei%2C+H">Hamidreza Kasaei</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted for publication at the ICRA2024 conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item838">[838]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.07012" title="Abstract">arXiv:2203.07012</a> (replaced) [<a href="/pdf/2203.07012" title="Download PDF">pdf</a>, <a href="/format/2203.07012" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Extended Finite Elements for 3D-1D coupled problems via a  PDE-constrained optimization approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Grappein%2C+D">Denise Grappein</a>, 
<a href="/search/math?searchtype=author&query=Scial%C3%B3%2C+S">Stefano Scial&#xf3;</a>, 
<a href="/search/math?searchtype=author&query=Vicini%2C+F">Fabio Vicini</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item839">[839]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2203.12446" title="Abstract">arXiv:2203.12446</a> (replaced) [<a href="/pdf/2203.12446" title="Download PDF">pdf</a>, <a href="/format/2203.12446" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SMEMO: Social Memory for Trajectory Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marchetti%2C+F">Francesco Marchetti</a>, 
<a href="/search/cs?searchtype=author&query=Becattini%2C+F">Federico Becattini</a>, 
<a href="/search/cs?searchtype=author&query=Seidenari%2C+L">Lorenzo Seidenari</a>, 
<a href="/search/cs?searchtype=author&query=Del+Bimbo%2C+A">Alberto Del Bimbo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in IEEE Transaction on Pattern Analysis and Machine Intelligence (PAMI)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item840">[840]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2204.02352" title="Abstract">arXiv:2204.02352</a> (replaced) [<a href="/pdf/2204.02352" title="Download PDF">pdf</a>, <a href="/format/2204.02352" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Asynchronous Load Balancing and Auto-scaling: Mean-Field Limit and  Optimal Design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Anselmi%2C+J">Jonatha Anselmi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Performance (cs.PF); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item841">[841]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.02645" title="Abstract">arXiv:2205.02645</a> (replaced) [<a href="/pdf/2205.02645" title="Download PDF">pdf</a>, <a href="/format/2205.02645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Discovering stochastic dynamical equations from biological time series  data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Nabeel%2C+A">Arshed Nabeel</a>, 
<a href="/search/q-bio?searchtype=author&query=Karichannavar%2C+A">Ashwin Karichannavar</a>, 
<a href="/search/q-bio?searchtype=author&query=Palathingal%2C+S">Shuaib Palathingal</a>, 
<a href="/search/q-bio?searchtype=author&query=Jhawar%2C+J">Jitesh Jhawar</a>, 
<a href="/search/q-bio?searchtype=author&query=Br%C3%BCckner%2C+D+B">David B. Br&#xfc;ckner</a>, 
<a href="/search/q-bio?searchtype=author&query=M.%2C+D+R">Danny Raj M.</a>, 
<a href="/search/q-bio?searchtype=author&query=Guttal%2C+V">Vishwesha Guttal</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updates: v3: Significantly reorganized the paper and added a section analysis of a cell migration dataset. v4: Update arXiv title to match the updated title of the manuscript. v5: Added sections detailing the limitations of the approach
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantitative Methods (q-bio.QM)</span>; Machine Learning (cs.LG); Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item842">[842]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.05795" title="Abstract">arXiv:2205.05795</a> (replaced) [<a href="/pdf/2205.05795" title="Download PDF">pdf</a>, <a href="/format/2205.05795" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Algebraic Machine Learning with an Application to Chemistry
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Sai%2C+E+E">Ezzeddine El Sai</a>, 
<a href="/search/math?searchtype=author&query=Gara%2C+P">Parker Gara</a>, 
<a href="/search/math?searchtype=author&query=Pflaum%2C+M+J">Markus J. Pflaum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> to appear in "Foundations of Data Science"
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Algebraic Geometry (math.AG)</span>; Computational Geometry (cs.CG); Machine Learning (cs.LG); Mathematical Physics (math-ph)

</div>
</div>
</dd>
<dt><a name="item843">[843]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.06063" title="Abstract">arXiv:2205.06063</a> (replaced) [<a href="/pdf/2205.06063" title="Download PDF">pdf</a>, <a href="/ps/2205.06063" title="Download PostScript">ps</a>, <a href="/format/2205.06063" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Outage Analysis of Aerial Semi-Grant-Free NOMA Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lei%2C+H">Hongjiang Lei</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chen Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+K">Ki-Hong Park</a>, 
<a href="/search/cs?searchtype=author&query=Ansari%2C+I+S">Imran Shafique Ansari</a>, 
<a href="/search/cs?searchtype=author&query=Lei%2C+W">Weijia Lei</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Hong Tang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+K+J">Kyeong Jin Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by Digital Communications and Networks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item844">[844]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.10016" title="Abstract">arXiv:2205.10016</a> (replaced) [<a href="/pdf/2205.10016" title="Download PDF">pdf</a>, <a href="/format/2205.10016" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Progress Driven Multi-Agent Curriculum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wenshuai Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Pajarinen%2C+J">Joni Pajarinen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item845">[845]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2205.11119" title="Abstract">arXiv:2205.11119</a> (replaced) [<a href="/pdf/2205.11119" title="Download PDF">pdf</a>, <a href="/format/2205.11119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> NPGA: A Unified Algorithmic Framework for Decentralized  Constraint-Coupled Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Li%2C+J">Jingwang Li</a>, 
<a href="/search/math?searchtype=author&query=Su%2C+H">Housheng Su</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item846">[846]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.01900" title="Abstract">arXiv:2206.01900</a> (replaced) [<a href="/pdf/2206.01900" title="Download PDF">pdf</a>, <a href="/format/2206.01900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating counterfactual treatment outcomes over time in complex  multiagent scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fujii%2C+K">Keisuke Fujii</a>, 
<a href="/search/cs?searchtype=author&query=Takeuchi%2C+K">Koh Takeuchi</a>, 
<a href="/search/cs?searchtype=author&query=Kuribayashi%2C+A">Atsushi Kuribayashi</a>, 
<a href="/search/cs?searchtype=author&query=Takeishi%2C+N">Naoya Takeishi</a>, 
<a href="/search/cs?searchtype=author&query=Kawahara%2C+Y">Yoshinobu Kawahara</a>, 
<a href="/search/cs?searchtype=author&query=Takeda%2C+K">Kazuya Takeda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 11 figures. Accepted in IEEE Transactions on Neural Networks and Learning Systems
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG); Multiagent Systems (cs.MA); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item847">[847]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.08051" title="Abstract">arXiv:2206.08051</a> (replaced) [<a href="/pdf/2206.08051" title="Download PDF">pdf</a>, <a href="/format/2206.08051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Voronoi Density Estimator for High-Dimensional Data: Computation,  Compactification and Convergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Polianskii%2C+V">Vladislav Polianskii</a>, 
<a href="/search/stat?searchtype=author&query=Marchetti%2C+G+L">Giovanni Luca Marchetti</a>, 
<a href="/search/stat?searchtype=author&query=Kravberg%2C+A">Alexander Kravberg</a>, 
<a href="/search/stat?searchtype=author&query=Varava%2C+A">Anastasiia Varava</a>, 
<a href="/search/stat?searchtype=author&query=Pokorny%2C+F+T">Florian T. Pokorny</a>, 
<a href="/search/stat?searchtype=author&query=Kragic%2C+D">Danica Kragic</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the Conference on Uncertainty in Artificial Intelligence (UAI) 2022. This version contains erratas of the published material
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Computational Geometry (cs.CG)

</div>
</div>
</dd>
<dt><a name="item848">[848]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2206.12943" title="Abstract">arXiv:2206.12943</a> (replaced) [<a href="/pdf/2206.12943" title="Download PDF">pdf</a>, <a href="/format/2206.12943" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-view Feature Augmentation with Adaptive Class Activation Mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xiang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yingjie Tian</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Z">Zhiquan Qi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> An arxiv version of the paper published in Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI-21). See <a href="https://www.ijcai.org/proceedings/2021/94">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the Thirtieth International Joint Conference on
  Artificial Intelligence. Main Track. 2021. Pages 678-684
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item849">[849]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.00391" title="Abstract">arXiv:2207.00391</a> (replaced) [<a href="/pdf/2207.00391" title="Download PDF">pdf</a>, <a href="/format/2207.00391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Theoretical Analysis of the Learning Dynamics under Class Imbalance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Francazi%2C+E">Emanuele Francazi</a>, 
<a href="/search/stat?searchtype=author&query=Baity-Jesi%2C+M">Marco Baity-Jesi</a>, 
<a href="/search/stat?searchtype=author&query=Lucchi%2C+A">Aurelien Lucchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In the latest update of our paper, we've refined the formulations of the theorems and their proofs in the appendix to improve clarity
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Conference on Machine Learning 2023, (PMLR)
  10285-10322
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item850">[850]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.03565" title="Abstract">arXiv:2207.03565</a> (replaced) [<a href="/pdf/2207.03565" title="Download PDF">pdf</a>, <a href="/format/2207.03565" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> With a little help from my friends: essentiality vs opportunity in group  criticality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Aleandri%2C+M">Michele Aleandri</a>, 
<a href="/search/econ?searchtype=author&query=Dall%27Aglio%2C+M">Marco Dall&#x27;Aglio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages, 5 tables, 5 figures. First presented at the 12th Conference on Economic Design, Padova, June 9, 2022 and at EURO 2022, Espoo, July 6, 2022
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item851">[851]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.04250" title="Abstract">arXiv:2207.04250</a> (replaced) [<a href="/pdf/2207.04250" title="Download PDF">pdf</a>, <a href="/format/2207.04250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving saliency models&#x27; predictions of the next fixation with humans&#x27;  intrinsic cost of gaze shifts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kadner%2C+F">Florian Kadner</a>, 
<a href="/search/cs?searchtype=author&query=Thomas%2C+T">Tobias Thomas</a>, 
<a href="/search/cs?searchtype=author&query=Hoppe%2C+D">David Hoppe</a>, 
<a href="/search/cs?searchtype=author&query=Rothkopf%2C+C+A">Constantin A. Rothkopf</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item852">[852]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2207.12638" title="Abstract">arXiv:2207.12638</a> (replaced) [<a href="/pdf/2207.12638" title="Download PDF">pdf</a>, <a href="/format/2207.12638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Variance estimation in graphs with the fused lasso
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Padilla%2C+O+H+M">Oscar Hernan Madrid Padilla</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item853">[853]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.01113" title="Abstract">arXiv:2208.01113</a> (replaced) [<a href="/pdf/2208.01113" title="Download PDF">pdf</a>, <a href="/format/2208.01113" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Evaluation of User Privacy in Deep Neural Networks using Timing  Side Channel
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shukla%2C+S">Shubhi Shukla</a>, 
<a href="/search/cs?searchtype=author&query=Alam%2C+M">Manaar Alam</a>, 
<a href="/search/cs?searchtype=author&query=Bhattacharya%2C+S">Sarani Bhattacharya</a>, 
<a href="/search/cs?searchtype=author&query=Mukhopadhyay%2C+D">Debdeep Mukhopadhyay</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+P">Pabitra Mitra</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 20 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item854">[854]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2208.01587" title="Abstract">arXiv:2208.01587</a> (replaced) [<a href="/pdf/2208.01587" title="Download PDF">pdf</a>, <a href="/format/2208.01587" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Incorporate Texture Saliency Adaptive Attention to Image  Cartoonization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xiang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yingjie Tian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the 39th International Conference on Machine Learning (ICML), PMLR 162:7183-7207, 2022
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> International Conference on Machine Learning (ICML). PMLR 162,
  2022, 7183-7207
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item855">[855]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.07148" title="Abstract">arXiv:2209.07148</a> (replaced) [<a href="/pdf/2209.07148" title="Download PDF">pdf</a>, <a href="/ps/2209.07148" title="Download PostScript">ps</a>, <a href="/format/2209.07148" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-supervised Batch Learning From Logged Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aminian%2C+G">Gholamali Aminian</a>, 
<a href="/search/cs?searchtype=author&query=Behnamnia%2C+A">Armin Behnamnia</a>, 
<a href="/search/cs?searchtype=author&query=Vega%2C+R">Roberto Vega</a>, 
<a href="/search/cs?searchtype=author&query=Toni%2C+L">Laura Toni</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chengchun Shi</a>, 
<a href="/search/cs?searchtype=author&query=Rabiee%2C+H+R">Hamid R. Rabiee</a>, 
<a href="/search/cs?searchtype=author&query=Rivasplata%2C+O">Omar Rivasplata</a>, 
<a href="/search/cs?searchtype=author&query=Rodrigues%2C+M+R+D">Miguel R. D. Rodrigues</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 46 pages,
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item856">[856]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.10020" title="Abstract">arXiv:2209.10020</a> (replaced) [<a href="/pdf/2209.10020" title="Download PDF">pdf</a>, <a href="/format/2209.10020" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards 3D VR-Sketch to 3D Shape Retrieval
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Luo%2C+L">Ling Luo</a>, 
<a href="/search/cs?searchtype=author&query=Gryaditskaya%2C+Y">Yulia Gryaditskaya</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yongxin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+T">Tao Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+Y">Yi-Zhe Song</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2020 International Conference on 3D Vision (3DV), pp. 81-90. IEEE,
  2020
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item857">[857]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.10134" title="Abstract">arXiv:2209.10134</a> (replaced) [<a href="/pdf/2209.10134" title="Download PDF">pdf</a>, <a href="/format/2209.10134" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recipe Generation from Unsegmented Cooking Videos
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Nishimura%2C+T">Taichi Nishimura</a>, 
<a href="/search/cs?searchtype=author&query=Hashimoto%2C+A">Atsushi Hashimoto</a>, 
<a href="/search/cs?searchtype=author&query=Ushiku%2C+Y">Yoshitaka Ushiku</a>, 
<a href="/search/cs?searchtype=author&query=Kameko%2C+H">Hirotaka Kameko</a>, 
<a href="/search/cs?searchtype=author&query=Mori%2C+S">Shinsuke Mori</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ACM TOMM; ACM Transactions on Multimedia Computing, Communications, and Applications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item858">[858]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.11461" title="Abstract">arXiv:2209.11461</a> (replaced) [<a href="/pdf/2209.11461" title="Download PDF">pdf</a>, <a href="/format/2209.11461" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spatio-Temporal Contrastive Learning Enhanced GNNs for Session-based  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+Z">Zhongwei Wan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Benyou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+J">Jiezhong Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Boyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+T">Ting Guo</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guangyong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ACM Transaction on Information Systems (ACM TOIS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item859">[859]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.11860" title="Abstract">arXiv:2209.11860</a> (replaced) [<a href="/pdf/2209.11860" title="Download PDF">pdf</a>, <a href="/format/2209.11860" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On star-$k$-PCGs: Exploring class boundaries for small $k$ values
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Monti%2C+A">Angelo Monti</a>, 
<a href="/search/math?searchtype=author&query=Sinaimeri%2C+B">Blerina Sinaimeri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item860">[860]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2209.12519" title="Abstract">arXiv:2209.12519</a> (replaced) [<a href="/pdf/2209.12519" title="Download PDF">pdf</a>, <a href="/format/2209.12519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Parameterized Intractability of Determinant Maximization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ohsaka%2C+N">Naoto Ohsaka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages. Accepted to Algorithmica. A preliminary version appeared in Proc. 33rd Int. Symp. on Algorithms and Computation (ISAAC), 2022
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the 33rd International Symposium on Algorithms and
  Computation (ISAAC), pp. 46:1--46:16, 2022; Algorithmica, 1--33, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item861">[861]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.00571" title="Abstract">arXiv:2210.00571</a> (replaced) [<a href="/pdf/2210.00571" title="Download PDF">pdf</a>, <a href="/ps/2210.00571" title="Download PostScript">ps</a>, <a href="/format/2210.00571" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond the Existential Theory of the Reals
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Schaefer%2C+M">Marcus Schaefer</a>, 
<a href="/search/cs?searchtype=author&query=Stefankovic%2C+D">Daniel Stefankovic</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item862">[862]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.04047" title="Abstract">arXiv:2210.04047</a> (replaced) [<a href="/pdf/2210.04047" title="Download PDF">pdf</a>, <a href="/format/2210.04047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Motion Planning on Visual Manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramaiah%2C+M+S">M Seetha Ramaiah</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A PhD thesis submitted to and accepted by the Department of Computer Science &amp; Engineering at the Indian Institute of Technology Kanpur
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item863">[863]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.04856" title="Abstract">arXiv:2210.04856</a> (replaced) [<a href="/pdf/2210.04856" title="Download PDF">pdf</a>, <a href="/format/2210.04856" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Posteriori Error Estimate and Adaptivity for QM/MM Models of  Crystalline Defects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Wang%2C+Y">Yangshuai Wang</a>, 
<a href="/search/physics?searchtype=author&query=Kermode%2C+J+R">James R. Kermode</a>, 
<a href="/search/physics?searchtype=author&query=Ortner%2C+C">Christoph Ortner</a>, 
<a href="/search/physics?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item864">[864]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.07793" title="Abstract">arXiv:2210.07793</a> (replaced) [<a href="/pdf/2210.07793" title="Download PDF">pdf</a>, <a href="/format/2210.07793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Greedy Transaction Fee Mechanisms for (Non-)myopic Miners
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gafni%2C+Y">Yotam Gafni</a>, 
<a href="/search/cs?searchtype=author&query=Yaish%2C+A">Aviv Yaish</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 40 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Theoretical Economics (econ.TH)

</div>
</div>
</dd>
<dt><a name="item865">[865]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.11993" title="Abstract">arXiv:2210.11993</a> (replaced) [<a href="/pdf/2210.11993" title="Download PDF">pdf</a>, <a href="/format/2210.11993" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bisparse Blind Deconvolution through Hierarchical Sparse Recovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Flinth%2C+A">Axel Flinth</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+I">Ingo Roth</a>, 
<a href="/search/cs?searchtype=author&query=Wunder%2C+G">Gerhard Wunder</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> V2: Completely revised version, entirely different proof, resulting in the recovery guarantee improved by a factor s
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item866">[866]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2210.15373" title="Abstract">arXiv:2210.15373</a> (replaced) [<a href="/pdf/2210.15373" title="Download PDF">pdf</a>, <a href="/format/2210.15373" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-consistent Reasoning For Solving Math Word Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xiong%2C+J">Jing Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Z">Zhongwei Wan</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xiping Hu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Min Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chengming Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE ICASSP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item867">[867]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.01987" title="Abstract">arXiv:2211.01987</a> (replaced) [<a href="/pdf/2211.01987" title="Download PDF">pdf</a>, <a href="/format/2211.01987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exact calculation of quantizer constants for arbitrary lattices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pook-Kolb%2C+D">Daniel Pook-Kolb</a>, 
<a href="/search/cs?searchtype=author&query=Allen%2C+B">Bruce Allen</a>, 
<a href="/search/cs?searchtype=author&query=Agrell%2C+E">Erik Agrell</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Metric Geometry (math.MG)

</div>
</div>
</dd>
<dt><a name="item868">[868]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.03595" title="Abstract">arXiv:2211.03595</a> (replaced) [<a href="/pdf/2211.03595" title="Download PDF">pdf</a>, <a href="/format/2211.03595" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Denoising Diffusions to Denoising Markov Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Benton%2C+J">Joe Benton</a>, 
<a href="/search/stat?searchtype=author&query=Shi%2C+Y">Yuyang Shi</a>, 
<a href="/search/stat?searchtype=author&query=De+Bortoli%2C+V">Valentin De Bortoli</a>, 
<a href="/search/stat?searchtype=author&query=Deligiannidis%2C+G">George Deligiannidis</a>, 
<a href="/search/stat?searchtype=author&query=Doucet%2C+A">Arnaud Doucet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item869">[869]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.05737" title="Abstract">arXiv:2211.05737</a> (replaced) [<a href="/pdf/2211.05737" title="Download PDF">pdf</a>, <a href="/format/2211.05737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IK-Geo: Unified Robot Inverse Kinematics Using Subproblem Decomposition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Elias%2C+A+J">Alexander J. Elias</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J+T">John T. Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 8 figures. Update 2: New polynomial IK method, 1D search for nonconsecutive intersecting axes. Update 1: New subproblem solution methods and timing results
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item870">[870]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.06745" title="Abstract">arXiv:2211.06745</a> (replaced) [<a href="/pdf/2211.06745" title="Download PDF">pdf</a>, <a href="/ps/2211.06745" title="Download PostScript">ps</a>, <a href="/format/2211.06745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quadrature Control-Bounded ADCs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Malmberg%2C+H">Hampus Malmberg</a>, 
<a href="/search/eess?searchtype=author&query=Feyling%2C+F">Fredrik Feyling</a>, 
<a href="/search/eess?searchtype=author&query=de+la+Rosa%2C+J+M">Jose M de la Rosa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 6 figures, submitted to ISCAS 2023
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2023 IEEE 66th International Midwest Symposium on Circuits and
  Systems (MWSCAS), Tempe, AZ, USA, 2023, pp. 380-384
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item871">[871]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.10968" title="Abstract">arXiv:2211.10968</a> (replaced) [<a href="/pdf/2211.10968" title="Download PDF">pdf</a>, <a href="/ps/2211.10968" title="Download PostScript">ps</a>, <a href="/format/2211.10968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Optimality of Divide and Conquer Kernel-based Functional  Linear Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Liu%2C+J">Jiading Liu</a>, 
<a href="/search/stat?searchtype=author&query=Shi%2C+L">Lei Shi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item872">[872]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.14115" title="Abstract">arXiv:2211.14115</a> (replaced) [<a href="/pdf/2211.14115" title="Download PDF">pdf</a>, <a href="/format/2211.14115" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Inverse Feasibility in Over-the-Air Federated Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Piotrowski%2C+T">Tomasz Piotrowski</a>, 
<a href="/search/stat?searchtype=author&query=Ismayilov%2C+R">Rafail Ismayilov</a>, 
<a href="/search/stat?searchtype=author&query=Frey%2C+M">Matthias Frey</a>, 
<a href="/search/stat?searchtype=author&query=Cavalcante%2C+R+L+G">Renato L.G. Cavalcante</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item873">[873]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2211.16335" title="Abstract">arXiv:2211.16335</a> (replaced) [<a href="/pdf/2211.16335" title="Download PDF">pdf</a>, <a href="/format/2211.16335" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> X-ICP: Localizability-Aware LiDAR Registration for Robust Localization  in Extreme Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tuna%2C+T">Turcan Tuna</a>, 
<a href="/search/cs?searchtype=author&query=Nubert%2C+J">Julian Nubert</a>, 
<a href="/search/cs?searchtype=author&query=Nava%2C+Y">Yoshua Nava</a>, 
<a href="/search/cs?searchtype=author&query=Khattak%2C+S">Shehryar Khattak</a>, 
<a href="/search/cs?searchtype=author&query=Hutter%2C+M">Marco Hutter</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 Pages, 20 Figures Submitted to IEEE Transactions On Robotics. Supplementary Video: <a href="https://youtu.be/SviLl7q69aA">this https URL</a> Project Website: <a href="https://sites.google.com/leggedrobotics.com/x-icp">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item874">[874]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.03613" title="Abstract">arXiv:2212.03613</a> (replaced) [<a href="/pdf/2212.03613" title="Download PDF">pdf</a>, <a href="/format/2212.03613" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> G-MAP: General Memory-Augmented Pre-trained Language Model for Domain  Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+Z">Zhongwei Wan</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+Y">Yichun Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+J">Jiaxin Shi</a>, 
<a href="/search/cs?searchtype=author&query=Shang%2C+L">Lifeng Shang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guangyong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+X">Xin Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qun Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2022,Long paper,Main conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item875">[875]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.04548" title="Abstract">arXiv:2212.04548</a> (replaced) [<a href="/pdf/2212.04548" title="Download PDF">pdf</a>, <a href="/format/2212.04548" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> STLGRU: Spatio-Temporal Lightweight Graph GRU for Traffic Flow  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bhaumik%2C+K+K">Kishor Kumar Bhaumik</a>, 
<a href="/search/cs?searchtype=author&query=Niloy%2C+F+F">Fahim Faisal Niloy</a>, 
<a href="/search/cs?searchtype=author&query=Mahmud%2C+S">Saif Mahmud</a>, 
<a href="/search/cs?searchtype=author&query=Woo%2C+S">Simon Woo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> PAKDD 2024 (Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item876">[876]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.05861" title="Abstract">arXiv:2212.05861</a> (replaced) [<a href="/pdf/2212.05861" title="Download PDF">pdf</a>, <a href="/format/2212.05861" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Joint Counting, Detection and Re-Identification for Multi-Object  Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+W">Weihong Ren</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Denglu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+H">Hui Cao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi&#x27;ai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zhi Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Honghai Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item877">[877]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.08762" title="Abstract">arXiv:2212.08762</a> (replaced) [<a href="/pdf/2212.08762" title="Download PDF">pdf</a>, <a href="/format/2212.08762" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Iterative RNDOP-Optimal Anchor Placement for Beyond Convex Hull  ToA-based Localization: Performance Bounds and Heuristic Algorithms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rao%2C+R+M">Raghunandan M. Rao</a>, 
<a href="/search/cs?searchtype=author&query=Emenonye%2C+D">Don-Roberts Emenonye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages. To appear in a future issue of the IEEE Transactions on Vehicular Technology
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>; Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item878">[878]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.10469" title="Abstract">arXiv:2212.10469</a> (replaced) [<a href="/pdf/2212.10469" title="Download PDF">pdf</a>, <a href="/format/2212.10469" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> BMX: Boosting Natural Language Generation Metrics with Explainability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Leiter%2C+C">Christoph Leiter</a>, 
<a href="/search/cs?searchtype=author&query=Nguyen%2C+H">Hoa Nguyen</a>, 
<a href="/search/cs?searchtype=author&query=Eger%2C+S">Steffen Eger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to Findings of EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item879">[879]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2212.14464" title="Abstract">arXiv:2212.14464</a> (replaced) [<a href="/pdf/2212.14464" title="Download PDF">pdf</a>, <a href="/format/2212.14464" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Result Diversification in Search and Recommendation: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haolun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yansen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chen Ma</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+F">Fuyuan Lyu</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+B">Bowei He</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+B">Bhaskar Mitra</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xue Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item880">[880]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.02817" title="Abstract">arXiv:2301.02817</a> (replaced) [<a href="/pdf/2301.02817" title="Download PDF">pdf</a>, <a href="/ps/2301.02817" title="Download PostScript">ps</a>, <a href="/format/2301.02817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cost-optimal Seeding Strategy During a Botanical Pandemic in  Domesticated Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lazebnik%2C+T">Teddy Lazebnik</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Dynamical Systems (math.DS)

</div>
</div>
</dd>
<dt><a name="item881">[881]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.02876" title="Abstract">arXiv:2301.02876</a> (replaced) [<a href="/pdf/2301.02876" title="Download PDF">pdf</a>, <a href="/format/2301.02876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Assigning Agents to Increase Network-Based Neighborhood Diversity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Z">Zirou Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+A">Andrew Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Marathe%2C+M+V">Madhav V. Marathe</a>, 
<a href="/search/cs?searchtype=author&query=Ravi%2C+S+S">S. S. Ravi</a>, 
<a href="/search/cs?searchtype=author&query=Rosenkrantz%2C+D+J">Daniel J. Rosenkrantz</a>, 
<a href="/search/cs?searchtype=author&query=Stearns%2C+R+E">Richard E. Stearns</a>, 
<a href="/search/cs?searchtype=author&query=Vullikanti%2C+A">Anil Vullikanti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAMAS-23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item882">[882]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.02889" title="Abstract">arXiv:2301.02889</a> (replaced) [<a href="/pdf/2301.02889" title="Download PDF">pdf</a>, <a href="/format/2301.02889" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Networked Anti-Coordination Games Meet Graphical Dynamical Systems:  Equilibria and Convergence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Z">Zirou Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Marathe%2C+M+V">Madhav V. Marathe</a>, 
<a href="/search/cs?searchtype=author&query=Ravi%2C+S+S">S. S. Ravi</a>, 
<a href="/search/cs?searchtype=author&query=Rosenkrantz%2C+D+J">Daniel J. Rosenkrantz</a>, 
<a href="/search/cs?searchtype=author&query=Stearns%2C+R+E">Richard E. Stearns</a>, 
<a href="/search/cs?searchtype=author&query=Vullikanti%2C+A">Anil Vullikanti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAAI-23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item883">[883]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.04090" title="Abstract">arXiv:2301.04090</a> (replaced) [<a href="/pdf/2301.04090" title="Download PDF">pdf</a>, <a href="/format/2301.04090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Finding Nontrivial Minimum Fixed Points in Discrete Dynamical Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+Z">Zirou Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+C">Chen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Marathe%2C+M+V">Madhav V. Marathe</a>, 
<a href="/search/cs?searchtype=author&query=Ravi%2C+S+S">S. S. Ravi</a>, 
<a href="/search/cs?searchtype=author&query=Rosenkrantz%2C+D+J">Daniel J. Rosenkrantz</a>, 
<a href="/search/cs?searchtype=author&query=Stearns%2C+R+E">Richard E. Stearns</a>, 
<a href="/search/cs?searchtype=author&query=Vullikanti%2C+A">Anil Vullikanti</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAAI-22
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item884">[884]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.07260" title="Abstract">arXiv:2301.07260</a> (replaced) [<a href="/pdf/2301.07260" title="Download PDF">pdf</a>, <a href="/format/2301.07260" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Additive Schwarz methods for fourth-order variational inequalities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Park%2C+J">Jongho Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item885">[885]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2301.11237" title="Abstract">arXiv:2301.11237</a> (replaced) [<a href="/pdf/2301.11237" title="Download PDF">pdf</a>, <a href="/format/2301.11237" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Hazards and Benefits of Condescension in Social Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Arieli%2C+I">Itai Arieli</a>, 
<a href="/search/econ?searchtype=author&query=Babichenko%2C+Y">Yakov Babichenko</a>, 
<a href="/search/econ?searchtype=author&query=M%C3%BCller%2C+S">Stephan M&#xfc;ller</a>, 
<a href="/search/econ?searchtype=author&query=Pourbabaee%2C+F">Farzad Pourbabaee</a>, 
<a href="/search/econ?searchtype=author&query=Tamuz%2C+O">Omer Tamuz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item886">[886]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.00058" title="Abstract">arXiv:2302.00058</a> (replaced) [<a href="/pdf/2302.00058" title="Download PDF">pdf</a>, <a href="/format/2302.00058" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-based Time-Series Anomaly Detection: A Survey and Outlook
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ho%2C+T+K+K">Thi Kieu Khanh Ho</a>, 
<a href="/search/cs?searchtype=author&query=Karami%2C+A">Ali Karami</a>, 
<a href="/search/cs?searchtype=author&query=Armanfard%2C+N">Narges Armanfard</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 4 figures, 2 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item887">[887]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.01250" title="Abstract">arXiv:2302.01250</a> (replaced) [<a href="/pdf/2302.01250" title="Download PDF">pdf</a>, <a href="/format/2302.01250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifying regions of importance in wall-bounded turbulence through  explainable deep learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Cremades%2C+A">Andres Cremades</a>, 
<a href="/search/physics?searchtype=author&query=Hoyas%2C+S">Sergio Hoyas</a>, 
<a href="/search/physics?searchtype=author&query=Deshpande%2C+R">Rahul Deshpande</a>, 
<a href="/search/physics?searchtype=author&query=Quintero%2C+P">Pedro Quintero</a>, 
<a href="/search/physics?searchtype=author&query=Lellep%2C+M">Martin Lellep</a>, 
<a href="/search/physics?searchtype=author&query=Lee%2C+W+J">Will Junghoon Lee</a>, 
<a href="/search/physics?searchtype=author&query=Monty%2C+J">Jason Monty</a>, 
<a href="/search/physics?searchtype=author&query=Hutchins%2C+N">Nicholas Hutchins</a>, 
<a href="/search/physics?searchtype=author&query=Linkmann%2C+M">Moritz Linkmann</a>, 
<a href="/search/physics?searchtype=author&query=Marusic%2C+I">Ivan Marusic</a>, 
<a href="/search/physics?searchtype=author&query=Vinuesa%2C+R">Ricardo Vinuesa</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Fluid Dynamics (physics.flu-dyn)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item888">[888]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.04763" title="Abstract">arXiv:2302.04763</a> (replaced) [<a href="/pdf/2302.04763" title="Download PDF">pdf</a>, <a href="/format/2302.04763" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Sampling with Approximate Transport Maps
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Grenioux%2C+L">Louis Grenioux</a>, 
<a href="/search/stat?searchtype=author&query=Durmus%2C+A">Alain Durmus</a>, 
<a href="/search/stat?searchtype=author&query=Moulines%2C+%C3%89">&#xc9;ric Moulines</a>, 
<a href="/search/stat?searchtype=author&query=Gabri%C3%A9%2C+M">Marylou Gabri&#xe9;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item889">[889]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.05793" title="Abstract">arXiv:2302.05793</a> (replaced) [<a href="/pdf/2302.05793" title="Download PDF">pdf</a>, <a href="/format/2302.05793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributional GFlowNets with Quantile Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dinghuai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+L">Ling Pan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R+T+Q">Ricky T. Q. Chen</a>, 
<a href="/search/cs?searchtype=author&query=Courville%2C+A">Aaron Courville</a>, 
<a href="/search/cs?searchtype=author&query=Bengio%2C+Y">Yoshua Bengio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by TMLR
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation (stat.CO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item890">[890]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.06727" title="Abstract">arXiv:2302.06727</a> (replaced) [<a href="/pdf/2302.06727" title="Download PDF">pdf</a>, <a href="/format/2302.06727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Learning Predicts Prevalent and Incident Parkinson&#x27;s Disease From  UK Biobank Fundus Imaging
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tran%2C+C">Charlie Tran</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+K">Kai Shen</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ashok%2C+A">Akshay Ashok</a>, 
<a href="/search/cs?searchtype=author&query=Ramirez-Zamora%2C+A">Adolfo Ramirez-Zamora</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jinghua Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yulin Li</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+R">Ruogu Fang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 17 pages, 4 figures, 2 tables, 4 supplementary tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item891">[891]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.07558" title="Abstract">arXiv:2302.07558</a> (replaced) [<a href="/pdf/2302.07558" title="Download PDF">pdf</a>, <a href="/format/2302.07558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Initialisation from lattice Boltzmann to multi-step Finite Difference  methods: modified equations and discrete observability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bellotti%2C+T">Thomas Bellotti</a> (CMAP)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item892">[892]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08621" title="Abstract">arXiv:2302.08621</a> (replaced) [<a href="/pdf/2302.08621" title="Download PDF">pdf</a>, <a href="/format/2302.08621" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distances for Markov Chains, and Their Differentiation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brug%C3%A8re%2C+T">Tristan Brug&#xe8;re</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Z">Zhengchao Wan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yusu Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages + 42 pages appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item893">[893]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.08766" title="Abstract">arXiv:2302.08766</a> (replaced) [<a href="/pdf/2302.08766" title="Download PDF">pdf</a>, <a href="/format/2302.08766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Lower Bound and a Near-Optimal Algorithm for Bilevel Empirical Risk  Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Dagr%C3%A9ou%2C+M">Mathieu Dagr&#xe9;ou</a>, 
<a href="/search/stat?searchtype=author&query=Moreau%2C+T">Thomas Moreau</a>, 
<a href="/search/stat?searchtype=author&query=Vaiter%2C+S">Samuel Vaiter</a>, 
<a href="/search/stat?searchtype=author&query=Ablin%2C+P">Pierre Ablin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item894">[894]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.10764" title="Abstract">arXiv:2302.10764</a> (replaced) [<a href="/pdf/2302.10764" title="Download PDF">pdf</a>, <a href="/format/2302.10764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On The Coherence of Quantitative Evaluation of Visual Explanations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vandersmissen%2C+B">Benjamin Vandersmissen</a>, 
<a href="/search/cs?searchtype=author&query=Oramas%2C+J">Jose Oramas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at CVIU
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item895">[895]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2302.13961" title="Abstract">arXiv:2302.13961</a> (replaced) [<a href="/pdf/2302.13961" title="Download PDF">pdf</a>, <a href="/format/2302.13961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Soft labelling for semantic segmentation: Bringing coherence to label  down-sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alcover-Couso%2C+R">Roberto Alcover-Couso</a>, 
<a href="/search/cs?searchtype=author&query=Escudero-Vinolo%2C+M">Marcos Escudero-Vinolo</a>, 
<a href="/search/cs?searchtype=author&query=SanMiguel%2C+J+C">Juan C. SanMiguel</a>, 
<a href="/search/cs?searchtype=author&query=Martinez%2C+J+M">Jose M. Martinez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item896">[896]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.00402" title="Abstract">arXiv:2303.00402</a> (replaced) [<a href="/pdf/2303.00402" title="Download PDF">pdf</a>, <a href="/format/2303.00402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On discrete ground states of rotating Bose-Einstein condensates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Henning%2C+P">Patrick Henning</a>, 
<a href="/search/math?searchtype=author&query=Yadav%2C+M">Mahima Yadav</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item897">[897]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.00492" title="Abstract">arXiv:2303.00492</a> (replaced) [<a href="/pdf/2303.00492" title="Download PDF">pdf</a>, <a href="/format/2303.00492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lumos: Heterogeneity-aware Federated Graph Learning over Decentralized  Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+Q">Qiying Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yifei Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+L">Lingyang Chu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 7 figures, published in the Proceedings of the 39th IEEE International Conference on Data Engineering (ICDE 2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item898">[898]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.01385" title="Abstract">arXiv:2303.01385</a> (replaced) [<a href="/pdf/2303.01385" title="Download PDF">pdf</a>, <a href="/format/2303.01385" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hyperlink communities in higher-order networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lotito%2C+Q+F">Quintino Francesco Lotito</a>, 
<a href="/search/cs?searchtype=author&query=Musciotto%2C+F">Federico Musciotto</a>, 
<a href="/search/cs?searchtype=author&query=Montresor%2C+A">Alberto Montresor</a>, 
<a href="/search/cs?searchtype=author&query=Battiston%2C+F">Federico Battiston</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in the Journal of Complex Networks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Physics and Society (physics.soc-ph); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item899">[899]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.02586" title="Abstract">arXiv:2303.02586</a> (replaced) [<a href="/pdf/2303.02586" title="Download PDF">pdf</a>, <a href="/format/2303.02586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Complex Quasi-Newton Proximal Method for Image Reconstruction in  Compressed Sensing MRI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hong%2C+T">Tao Hong</a>, 
<a href="/search/math?searchtype=author&query=Hernandez-Garcia%2C+L">Luis Hernandez-Garcia</a>, 
<a href="/search/math?searchtype=author&query=Fessler%2C+J+A">Jeffrey A. Fessler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 pages, 26 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Image and Video Processing (eess.IV); Signal Processing (eess.SP); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item900">[900]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.05754" title="Abstract">arXiv:2303.05754</a> (replaced) [<a href="/pdf/2303.05754" title="Download PDF">pdf</a>, <a href="/format/2303.05754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decomposed Diffusion Sampler for Accelerating Large-Scale Inverse  Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chung%2C+H">Hyungjin Chung</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Suhyeon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+J+C">Jong Chul Ye</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024; 28 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item901">[901]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.05983" title="Abstract">arXiv:2303.05983</a> (replaced) [<a href="/pdf/2303.05983" title="Download PDF">pdf</a>, <a href="/format/2303.05983" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accountable Textual-Visual Chat Learns to Reject Human Instructions in  Image Re-creation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuliang Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> TMLR, Survey Certification
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item902">[902]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.08103" title="Abstract">arXiv:2303.08103</a> (replaced) [<a href="/pdf/2303.08103" title="Download PDF">pdf</a>, <a href="/format/2303.08103" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-task Meta Label Correction for Time Series Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Luxuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+T">Ting Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+W">Wei Wei</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+M">Min Dai</a>, 
<a href="/search/cs?searchtype=author&query=Fang%2C+C">Cheng Fang</a>, 
<a href="/search/cs?searchtype=author&query=Duan%2C+J">Jinqiao Duan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
</div>
</dd>
<dt><a name="item903">[903]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.09265" title="Abstract">arXiv:2303.09265</a> (replaced) [<a href="/pdf/2303.09265" title="Download PDF">pdf</a>, <a href="/ps/2303.09265" title="Download PostScript">ps</a>, <a href="/format/2303.09265" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterizations of a Class of Planar Functions over Finite Fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Chen%2C+R">Ruikai Chen</a>, 
<a href="/search/math?searchtype=author&query=Mesnager%2C+S">Sihem Mesnager</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Number Theory (math.NT)</span>; Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item904">[904]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10257" title="Abstract">arXiv:2303.10257</a> (replaced) [<a href="/pdf/2303.10257" title="Download PDF">pdf</a>, <a href="/format/2303.10257" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recent Developments in Machine Learning Methods for Stochastic Control  and Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hu%2C+R">Ruimeng Hu</a>, 
<a href="/search/math?searchtype=author&query=Lauri%C3%A8re%2C+M">Mathieu Lauri&#xe8;re</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item905">[905]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.10526" title="Abstract">arXiv:2303.10526</a> (replaced) [<a href="/pdf/2303.10526" title="Download PDF">pdf</a>, <a href="/format/2303.10526" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient deadlock avoidance for 2D mesh NoCs that use OQ or VOQ routers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Papaphilippou%2C+P">Philippos Papaphilippou</a>, 
<a href="/search/cs?searchtype=author&query=Van+Chu%2C+T">Thiem Van Chu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in IEEE Transactions on Computers
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item906">[906]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.11156" title="Abstract">arXiv:2303.11156</a> (replaced) [<a href="/pdf/2303.11156" title="Download PDF">pdf</a>, <a href="/format/2303.11156" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can AI-Generated Text be Reliably Detected?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sadasivan%2C+V+S">Vinu Sankar Sadasivan</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+A">Aounon Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Balasubramanian%2C+S">Sriram Balasubramanian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenxiao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Feizi%2C+S">Soheil Feizi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item907">[907]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.11779" title="Abstract">arXiv:2303.11779</a> (replaced) [<a href="/pdf/2303.11779" title="Download PDF">pdf</a>, <a href="/format/2303.11779" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Hitting of Unit Balls and Hypercubes in $\mathbb{R}^d$ using  Points from $\mathbb{Z}^d$
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=De%2C+M">Minati De</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Satyam Singh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 5 figures. Accepted in TCS
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Theoretical Computer Science 992C (2024) 114452
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>

</div>
</div>
</dd>
<dt><a name="item908">[908]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.13961" title="Abstract">arXiv:2303.13961</a> (replaced) [<a href="/pdf/2303.13961" title="Download PDF">pdf</a>, <a href="/format/2303.13961" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Error bounds for discrete minimizers of the Ginzburg-Landau energy in  the high-$&#x3ba;$ regime
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=D%C3%B6rich%2C+B">Benjamin D&#xf6;rich</a>, 
<a href="/search/math?searchtype=author&query=Henning%2C+P">Patrick Henning</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item909">[909]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15041" title="Abstract">arXiv:2303.15041</a> (replaced) [<a href="/pdf/2303.15041" title="Download PDF">pdf</a>, <a href="/format/2303.15041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards black-box parameter estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Lenzi%2C+A">Amanda Lenzi</a>, 
<a href="/search/stat?searchtype=author&query=Rue%2C+H">Haavard Rue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item910">[910]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2303.15422" title="Abstract">arXiv:2303.15422</a> (replaced) [<a href="/pdf/2303.15422" title="Download PDF">pdf</a>, <a href="/format/2303.15422" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KPEval: Towards Fine-Grained Semantic-Based Keyphrase Evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+D">Di Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Da Yin</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item911">[911]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.00199" title="Abstract">arXiv:2304.00199</a> (replaced) [<a href="/pdf/2304.00199" title="Download PDF">pdf</a>, <a href="/format/2304.00199" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Applications of No-Collision Transportation Maps in Manifold Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Negrini%2C+E">Elisa Negrini</a>, 
<a href="/search/cs?searchtype=author&query=Nurbekyan%2C+L">Levon Nurbekyan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item912">[912]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.02259" title="Abstract">arXiv:2304.02259</a> (replaced) [<a href="/pdf/2304.02259" title="Download PDF">pdf</a>, <a href="/ps/2304.02259" title="Download PostScript">ps</a>, <a href="/format/2304.02259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On a finite-volume approximation of a diffusion-convection equation with  a multiplicative stochastic force
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Bauzet%2C+C">Caroline Bauzet</a>, 
<a href="/search/math?searchtype=author&query=Schmitz%2C+K">Kerstin Schmitz</a>, 
<a href="/search/math?searchtype=author&query=Zimmermann%2C+A">Aleksandra Zimmermann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2203.09851">arXiv:2203.09851</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Analysis of PDEs (math.AP)

</div>
</div>
</dd>
<dt><a name="item913">[913]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03343" title="Abstract">arXiv:2304.03343</a> (replaced) [<a href="/pdf/2304.03343" title="Download PDF">pdf</a>, <a href="/ps/2304.03343" title="Download PostScript">ps</a>, <a href="/format/2304.03343" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Spintronic Physical Reservoir for Autonomous Prediction and Long-Term  Household Energy Load Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Misba%2C+W+A">Walid Al Misba</a>, 
<a href="/search/cs?searchtype=author&query=Mavikumbure%2C+H+S">Harindra S. Mavikumbure</a>, 
<a href="/search/cs?searchtype=author&query=Rajib%2C+M+M">Md Mahadi Rajib</a>, 
<a href="/search/cs?searchtype=author&query=Marino%2C+D+L">Daniel L. Marino</a>, 
<a href="/search/cs?searchtype=author&query=Cobilean%2C+V">Victor Cobilean</a>, 
<a href="/search/cs?searchtype=author&query=Manic%2C+M">Milos Manic</a>, 
<a href="/search/cs?searchtype=author&query=Atulasimha%2C+J">Jayasimha Atulasimha</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Access, Vol. 11, pp. 124725 - 124737 (2023)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item914">[914]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.03834" title="Abstract">arXiv:2304.03834</a> (replaced) [<a href="/pdf/2304.03834" title="Download PDF">pdf</a>, <a href="/format/2304.03834" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> WOMD-LiDAR: Raw Sensor Dataset Benchmark for Motion Forecasting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ge%2C+R">Runzhou Ge</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+H">Hang Qiu</a>, 
<a href="/search/cs?searchtype=author&query=AI-Rfou%2C+R">Rami AI-Rfou</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+C+R">Charles R. Qi</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xuanyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zoey Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ettinger%2C+S">Scott Ettinger</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+P">Pei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Leng%2C+Z">Zhaoqi Leng</a>, 
<a href="/search/cs?searchtype=author&query=Baniodeh%2C+M">Mustafa Baniodeh</a>, 
<a href="/search/cs?searchtype=author&query=Bogun%2C+I">Ivan Bogun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Weiyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+M">Mingxing Tan</a>, 
<a href="/search/cs?searchtype=author&query=Anguelov%2C+D">Dragomir Anguelov</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICRA 2024 camera ready version. Dataset website: <a href="https://waymo.com/open/data/motion/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item915">[915]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.04339" title="Abstract">arXiv:2304.04339</a> (replaced) [<a href="/pdf/2304.04339" title="Download PDF">pdf</a>, <a href="/format/2304.04339" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zengzhi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qiming Xie</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yi Feng</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+Z">Zixiang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zinong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+R">Rui Xia</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report; 21 pages, add more evaluation results (e.g., comparative opinion mining, cot, and self-consistency)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item916">[916]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.10164" title="Abstract">arXiv:2304.10164</a> (replaced) [<a href="/pdf/2304.10164" title="Download PDF">pdf</a>, <a href="/ps/2304.10164" title="Download PostScript">ps</a>, <a href="/format/2304.10164" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analyzing FOMC Minutes: Accuracy and Constraints of Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+W">Wonseong Kim</a>, 
<a href="/search/cs?searchtype=author&query=Sp%C3%B6rer%2C+J+F">Jan Frederic Sp&#xf6;rer</a>, 
<a href="/search/cs?searchtype=author&query=Handschuh%2C+S">Siegfried Handschuh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15pages, 4 figures, 5 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item917">[917]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2304.14979" title="Abstract">arXiv:2304.14979</a> (replaced) [<a href="/pdf/2304.14979" title="Download PDF">pdf</a>, <a href="/format/2304.14979" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MLCopilot: Unleashing the Power of Large Language Models in Solving  Machine Learning Tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Lei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuge Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+K">Kan Ren</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongsheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuqing Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as a conference paper at EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item918">[918]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.00402" title="Abstract">arXiv:2305.00402</a> (replaced) [<a href="/pdf/2305.00402" title="Download PDF">pdf</a>, <a href="/format/2305.00402" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sliced Wasserstein Estimation with Control Variates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Nguyen%2C+K">Khai Nguyen</a>, 
<a href="/search/stat?searchtype=author&query=Ho%2C+N">Nhat Ho</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR2024, 20 pages, 7 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computer Vision and Pattern Recognition (cs.CV); Graphics (cs.GR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item919">[919]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.03938" title="Abstract">arXiv:2305.03938</a> (replaced) [<a href="/pdf/2305.03938" title="Download PDF">pdf</a>, <a href="/format/2305.03938" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adam-family Methods for Nonsmooth Optimization with Convergence  Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Xiao%2C+N">Nachuan Xiao</a>, 
<a href="/search/math?searchtype=author&query=Hu%2C+X">Xiaoyin Hu</a>, 
<a href="/search/math?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/math?searchtype=author&query=Toh%2C+K">Kim-Chuan Toh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 53 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item920">[920]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.04449" title="Abstract">arXiv:2305.04449</a> (replaced) [<a href="/pdf/2305.04449" title="Download PDF">pdf</a>, <a href="/format/2305.04449" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DeformerNet: Learning Bimanual Manipulation of 3D Deformable Objects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Thach%2C+B">Bao Thach</a>, 
<a href="/search/cs?searchtype=author&query=Cho%2C+B+Y">Brian Y. Cho</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+S">Shing-Hei Ho</a>, 
<a href="/search/cs?searchtype=author&query=Hermans%2C+T">Tucker Hermans</a>, 
<a href="/search/cs?searchtype=author&query=Kuntz%2C+A">Alan Kuntz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to IEEE Transactions on Robotics (T-RO). 20 pages, 27 figures. arXiv admin note: text overlap with <a href="/abs/2110.04685">arXiv:2110.04685</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item921">[921]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05055" title="Abstract">arXiv:2305.05055</a> (replaced) [<a href="/pdf/2305.05055" title="Download PDF">pdf</a>, <a href="/format/2305.05055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CPMA: An Efficient Batch-Parallel Compressed Set Without Pointers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wheatman%2C+B">Brian Wheatman</a>, 
<a href="/search/cs?searchtype=author&query=Burns%2C+R">Randal Burns</a>, 
<a href="/search/cs?searchtype=author&query=Bulu%C3%A7%2C+A">Ayd&#x131;n Bulu&#xe7;</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Helen Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Performance (cs.PF)

</div>
</div>
</dd>
<dt><a name="item922">[922]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05143" title="Abstract">arXiv:2305.05143</a> (replaced) [<a href="/pdf/2305.05143" title="Download PDF">pdf</a>, <a href="/format/2305.05143" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fundamental Limits of Distributed Linearly Separable Computation under  Cyclic Assignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Wenbo Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+K">Kai Wan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hua Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+M">Mingyue Ji</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+R+C">Robert Caiming Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Caire%2C+G">Giuseppe Caire</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A short version has been accepted in ISIT2023. 9pages, 2figures, conference; The Journal version has been submitted to TIT, 44papges, 8figueres
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item923">[923]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.05875" title="Abstract">arXiv:2305.05875</a> (replaced) [<a href="/pdf/2305.05875" title="Download PDF">pdf</a>, <a href="/format/2305.05875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantization Aware Attack: Enhancing Transferable Adversarial Attacks by  Model Quantization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yulong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenhao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qian Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Zhengyu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+H">Haoran Fan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+D">Dawei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+N">Nannan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tongliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+C">Chao Shen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE Transactions on Information Forensics and Security in 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item924">[924]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.08850" title="Abstract">arXiv:2305.08850</a> (replaced) [<a href="/pdf/2305.08850" title="Download PDF">pdf</a>, <a href="/format/2305.08850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Make-A-Protagonist: Generic Video Editing with An Ensemble of Experts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yuyang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+E">Enze Xie</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lanqing Hong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+G+H">Gim Hee Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://make-a-protagonist.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item925">[925]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10620" title="Abstract">arXiv:2305.10620</a> (replaced) [<a href="/pdf/2305.10620" title="Download PDF">pdf</a>, <a href="/format/2305.10620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Soft-Minimum and Soft-Maximum Barrier Functions for Safety with  Actuation Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Rabiee%2C+P">Pedram Rabiee</a>, 
<a href="/search/eess?searchtype=author&query=Hoagg%2C+J+B">Jesse B. Hoagg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Preprint submitted to Automatica. arXiv admin note: text overlap with <a href="/abs/2304.00693">arXiv:2304.00693</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item926">[926]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.10892" title="Abstract">arXiv:2305.10892</a> (replaced) [<a href="/pdf/2305.10892" title="Download PDF">pdf</a>, <a href="/format/2305.10892" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EventNet-ITA: Italian Frame Parsing for Events
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rovera%2C+M">Marco Rovera</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted paper at the 8th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature (LaTeCH-CLfL 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item927">[927]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.11854" title="Abstract">arXiv:2305.11854</a> (replaced) [<a href="/pdf/2305.11854" title="Download PDF">pdf</a>, <a href="/format/2305.11854" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Web Navigation with Instruction-Finetuned Foundation Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Furuta%2C+H">Hiroki Furuta</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K">Kuang-Huei Lee</a>, 
<a href="/search/cs?searchtype=author&query=Nachum%2C+O">Ofir Nachum</a>, 
<a href="/search/cs?searchtype=author&query=Matsuo%2C+Y">Yutaka Matsuo</a>, 
<a href="/search/cs?searchtype=author&query=Faust%2C+A">Aleksandra Faust</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+S+S">Shixiang Shane Gu</a>, 
<a href="/search/cs?searchtype=author&query=Gur%2C+I">Izzeddin Gur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Website: <a href="https://sites.google.com/view/mm-webnav/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item928">[928]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.12050" title="Abstract">arXiv:2305.12050</a> (replaced) [<a href="/pdf/2305.12050" title="Download PDF">pdf</a>, <a href="/format/2305.12050" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI-assisted Code Authoring at Scale: Fine-tuning, deploying, and mixed  methods evaluation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Murali%2C+V">Vijayaraghavan Murali</a>, 
<a href="/search/cs?searchtype=author&query=Maddila%2C+C">Chandra Maddila</a>, 
<a href="/search/cs?searchtype=author&query=Ahmad%2C+I">Imad Ahmad</a>, 
<a href="/search/cs?searchtype=author&query=Bolin%2C+M">Michael Bolin</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+D">Daniel Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Ghorbani%2C+N">Negar Ghorbani</a>, 
<a href="/search/cs?searchtype=author&query=Fernandez%2C+R">Renuka Fernandez</a>, 
<a href="/search/cs?searchtype=author&query=Nagappan%2C+N">Nachiappan Nagappan</a>, 
<a href="/search/cs?searchtype=author&query=Rigby%2C+P+C">Peter C. Rigby</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item929">[929]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13062" title="Abstract">arXiv:2305.13062</a> (replaced) [<a href="/pdf/2305.13062" title="Download PDF">pdf</a>, <a href="/format/2305.13062" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Table Meets LLM: Can Large Language Models Understand Structured Table  Data? A Benchmark and Empirical Study
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sui%2C+Y">Yuan Sui</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mengyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mingjie Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Shi Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dongmei Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted as a full paper at WSDM 2024. The code will be released at <a href="https://github.com/microsoft/TableProvider">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item930">[930]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.13718" title="Abstract">arXiv:2305.13718</a> (replaced) [<a href="/pdf/2305.13718" title="Download PDF">pdf</a>, <a href="/format/2305.13718" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiao%2C+F">Fangkai Jiao</a>, 
<a href="/search/cs?searchtype=author&query=Teng%2C+Z">Zhiyang Teng</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+B">Bosheng Ding</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+N+F">Nancy F. Chen</a>, 
<a href="/search/cs?searchtype=author&query=Joty%2C+S">Shafiq Joty</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item931">[931]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.14259" title="Abstract">arXiv:2305.14259</a> (replaced) [<a href="/pdf/2305.14259" title="Download PDF">pdf</a>, <a href="/format/2305.14259" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SciMON: Scientific Inspiration Machines Optimized for Novelty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qingyun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Downey%2C+D">Doug Downey</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Hope%2C+T">Tom Hope</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages. Code and resource is available at <a href="https://github.com/EagleW/CLBD">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item932">[932]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.15072" title="Abstract">arXiv:2305.15072</a> (replaced) [<a href="/pdf/2305.15072" title="Download PDF">pdf</a>, <a href="/format/2305.15072" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PathAsst: A Generative Foundation AI Assistant Towards Artificial  General Intelligence of Pathology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yuxuan Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chenglu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Sunyi Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kai Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+L">Lin Sun</a>, 
<a href="/search/cs?searchtype=author&query=Shui%2C+Z">Zhongyi Shui</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yunlong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Honglin Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Lin Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 7 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item933">[933]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16794" title="Abstract">arXiv:2305.16794</a> (replaced) [<a href="/pdf/2305.16794" title="Download PDF">pdf</a>, <a href="/format/2305.16794" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Secure Vertical Federated Learning Under Unreliable Connectivity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+X">Xinchi Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+H">Heng Pan</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+W">Wanru Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yan Gao</a>, 
<a href="/search/cs?searchtype=author&query=Gusmao%2C+P+P+B">Pedro P.B. Gusmao</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+W+F">William F. Shen</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chenyang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Lane%2C+N+D">Nicholas D. Lane</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Generalised extension from our previous work: <a href="/abs/2305.11236">arXiv:2305.11236</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item934">[934]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16830" title="Abstract">arXiv:2305.16830</a> (replaced) [<a href="/pdf/2305.16830" title="Download PDF">pdf</a>, <a href="/format/2305.16830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leaving the Nest: Going Beyond Local Loss Functions for  Predict-Then-Optimize
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shah%2C+S">Sanket Shah</a>, 
<a href="/search/cs?searchtype=author&query=Perrault%2C+A">Andrew Perrault</a>, 
<a href="/search/cs?searchtype=author&query=Wilder%2C+B">Bryan Wilder</a>, 
<a href="/search/cs?searchtype=author&query=Tambe%2C+M">Milind Tambe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item935">[935]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.16871" title="Abstract">arXiv:2305.16871</a> (replaced) [<a href="/pdf/2305.16871" title="Download PDF">pdf</a>, <a href="/format/2305.16871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Modelling, Analysis and Control of OmniMorph: an Omnidirectional  Morphing Multi-rotor UAV
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Aboudorra%2C+Y">Youssef Aboudorra</a>, 
<a href="/search/cs?searchtype=author&query=Gabellieri%2C+C">Chiara Gabellieri</a>, 
<a href="/search/cs?searchtype=author&query=Brantjes%2C+R">Ralph Brantjes</a>, 
<a href="/search/cs?searchtype=author&query=Sabl%C3%A9%2C+Q">Quentin Sabl&#xe9;</a>, 
<a href="/search/cs?searchtype=author&query=Franchi%2C+A">Antonio Franchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This version of the article has been accepted for publication, after peer review. But is not the Version of Record and does not reflect post-acceptance improvements, or any corrections. The Version of Record of this article is published in Journal of Intelligent &amp; Robotic Systems and is available online at <a href="https://doi.org/10.1007/s10846-024-02054-x">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Journal of Intelligent &amp; Robotic Systems. Volume 110, article
  number 21, (2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item936">[936]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18475" title="Abstract">arXiv:2305.18475</a> (replaced) [<a href="/pdf/2305.18475" title="Download PDF">pdf</a>, <a href="/format/2305.18475" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximation Rate of the Transformer Architecture for Sequence Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+H">Haotian Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qianxiao Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item937">[937]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18485" title="Abstract">arXiv:2305.18485</a> (replaced) [<a href="/pdf/2305.18485" title="Download PDF">pdf</a>, <a href="/format/2305.18485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Autoencoding Conditional Neural Processes for Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Prokhorov%2C+V">Victor Prokhorov</a>, 
<a href="/search/cs?searchtype=author&query=Titov%2C+I">Ivan Titov</a>, 
<a href="/search/cs?searchtype=author&query=Siddharth%2C+N">N. Siddharth</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item938">[938]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.18712" title="Abstract">arXiv:2305.18712</a> (replaced) [<a href="/pdf/2305.18712" title="Download PDF">pdf</a>, <a href="/format/2305.18712" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can We Evaluate Domain Adaptation Models Without Target-Domain Labels?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jianfei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Qian%2C+H">Hanjie Qian</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuecong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kai Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+L">Lihua Xie</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published at ICLR 2024, update formula and appendix, project and code available at <a href="https://sleepyseal.github.io/TransferScoreWeb/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item939">[939]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19196" title="Abstract">arXiv:2305.19196</a> (replaced) [<a href="/pdf/2305.19196" title="Download PDF">pdf</a>, <a href="/format/2305.19196" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Responsible Composition and Optimization of Integration Processes under  Correctness Preserving Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ritter%2C+D">Daniel Ritter</a>, 
<a href="/search/cs?searchtype=author&query=Forsberg%2C+F+N">Fredrik Nordvall Forsberg</a>, 
<a href="/search/cs?searchtype=author&query=Rinderle-Ma%2C+S">Stefanie Rinderle-Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>

</div>
</div>
</dd>
<dt><a name="item940">[940]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19306" title="Abstract">arXiv:2305.19306</a> (replaced) [<a href="/pdf/2305.19306" title="Download PDF">pdf</a>, <a href="/format/2305.19306" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets  Spiking Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jintang Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Huizhe Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+R">Ruofan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Z">Zulun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Baokun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+C">Changhua Meng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zibin Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Liang Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICLR 2024; Code is available at <a href="https://github.com/EdisonLeeeee/SpikeGCL">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item941">[941]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19612" title="Abstract">arXiv:2305.19612</a> (replaced) [<a href="/pdf/2305.19612" title="Download PDF">pdf</a>, <a href="/format/2305.19612" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Underwater-Art: Expanding Information Perspectives With Text Templates  For Underwater Acoustic Target Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yuan Xie</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+J">Jiawei Ren</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Ji Xu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> The Journal of the Acoustical Society of America, 2022, 152(5):
  2641-2651
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item942">[942]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2305.19894" title="Abstract">arXiv:2305.19894</a> (replaced) [<a href="/pdf/2305.19894" title="Download PDF">pdf</a>, <a href="/format/2305.19894" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Med-UniC: Unifying Cross-Lingual Medical Vision-Language Pre-Training by  Diminishing Bias
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wan%2C+Z">Zhongwei Wan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Che Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jie Fu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Benyou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+S">Sibo Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+L">Lei Ma</a>, 
<a href="/search/cs?searchtype=author&query=Quilodr%C3%A1n-Casas%2C+C">C&#xe9;sar Quilodr&#xe1;n-Casas</a>, 
<a href="/search/cs?searchtype=author&query=Arcucci%2C+R">Rossella Arcucci</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> NeurIPS 2023 Main track
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item943">[943]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00575" title="Abstract">arXiv:2306.00575</a> (replaced) [<a href="/pdf/2306.00575" title="Download PDF">pdf</a>, <a href="/format/2306.00575" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting Temporal Aspects of Movement for Predictive Replication in  Fog Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balitzki%2C+E">Emil Balitzki</a>, 
<a href="/search/cs?searchtype=author&query=Pfandzelter%2C+T">Tobias Pfandzelter</a>, 
<a href="/search/cs?searchtype=author&query=Bermbach%2C+D">David Bermbach</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item944">[944]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.00684" title="Abstract">arXiv:2306.00684</a> (replaced) [<a href="/pdf/2306.00684" title="Download PDF">pdf</a>, <a href="/format/2306.00684" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Balanced Training of Energy-Based Models with Adaptive Flow Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Grenioux%2C+L">Louis Grenioux</a>, 
<a href="/search/cs?searchtype=author&query=Moulines%2C+%C3%89">&#xc9;ric Moulines</a>, 
<a href="/search/cs?searchtype=author&query=Gabri%C3%A9%2C+M">Marylou Gabri&#xe9;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item945">[945]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01002" title="Abstract">arXiv:2306.01002</a> (replaced) [<a href="/pdf/2306.01002" title="Download PDF">pdf</a>, <a href="/format/2306.01002" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adaptive ship-radiated noise recognition with learnable fine-grained  wavelet transform
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Xie%2C+Y">Yuan Xie</a>, 
<a href="/search/eess?searchtype=author&query=Ren%2C+J">Jiawei Ren</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+J">Ji Xu</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Ocean Engineering 265 (2022): 112626
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Audio and Speech Processing (eess.AS)</span>; Machine Learning (cs.LG); Sound (cs.SD)

</div>
</div>
</dd>
<dt><a name="item946">[946]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.01195" title="Abstract">arXiv:2306.01195</a> (replaced) [<a href="/pdf/2306.01195" title="Download PDF">pdf</a>, <a href="/format/2306.01195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Consistency-guided Prompt Learning for Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Roy%2C+S">Shuvendu Roy</a>, 
<a href="/search/cs?searchtype=author&query=Etemad%2C+A">Ali Etemad</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item947">[947]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.02865" title="Abstract">arXiv:2306.02865</a> (replaced) [<a href="/pdf/2306.02865" title="Download PDF">pdf</a>, <a href="/format/2306.02865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy  Actor-Critic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ji%2C+T">Tianying Ji</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yu Luo</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+F">Fuchun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhan%2C+X">Xianyuan Zhan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jianwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Huazhe Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item948">[948]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04242" title="Abstract">arXiv:2306.04242</a> (replaced) [<a href="/pdf/2306.04242" title="Download PDF">pdf</a>, <a href="/format/2306.04242" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 4D Millimeter-Wave Radar in Autonomous Driving: A Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Han%2C+Z">Zeyu Han</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jiahao Wang</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+Z">Zikun Xu</a>, 
<a href="/search/eess?searchtype=author&query=Yang%2C+S">Shuocheng Yang</a>, 
<a href="/search/eess?searchtype=author&query=He%2C+L">Lei He</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+S">Shaobing Xu</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jianqiang Wang</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+K">Keqiang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item949">[949]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.04325" title="Abstract">arXiv:2306.04325</a> (replaced) [<a href="/pdf/2306.04325" title="Download PDF">pdf</a>, <a href="/format/2306.04325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Study on Chinese Social Perspective regarding ChatGPT for Education  and Beyond
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tian%2C+Y">Yao Tian</a>, 
<a href="/search/cs?searchtype=author&query=Tong%2C+C">Chengwei Tong</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+L">Lik-Hang Lee</a>, 
<a href="/search/cs?searchtype=author&query=Mogavi%2C+R+H">Reza Hadi Mogavi</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Y">Yong Liao</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Pengyuan Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item950">[950]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.05221" title="Abstract">arXiv:2306.05221</a> (replaced) [<a href="/pdf/2306.05221" title="Download PDF">pdf</a>, <a href="/format/2306.05221" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Steering No-Regret Learners to a Desired Equilibrium
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B+H">Brian Hu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Farina%2C+G">Gabriele Farina</a>, 
<a href="/search/cs?searchtype=author&query=Anagnostides%2C+I">Ioannis Anagnostides</a>, 
<a href="/search/cs?searchtype=author&query=Cacciamani%2C+F">Federico Cacciamani</a>, 
<a href="/search/cs?searchtype=author&query=McAleer%2C+S+M">Stephen Marcus McAleer</a>, 
<a href="/search/cs?searchtype=author&query=Haupt%2C+A+A">Andreas Alexander Haupt</a>, 
<a href="/search/cs?searchtype=author&query=Celli%2C+A">Andrea Celli</a>, 
<a href="/search/cs?searchtype=author&query=Gatti%2C+N">Nicola Gatti</a>, 
<a href="/search/cs?searchtype=author&query=Conitzer%2C+V">Vincent Conitzer</a>, 
<a href="/search/cs?searchtype=author&query=Sandholm%2C+T">Tuomas Sandholm</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item951">[951]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08353" title="Abstract">arXiv:2306.08353</a> (replaced) [<a href="/pdf/2306.08353" title="Download PDF">pdf</a>, <a href="/ps/2306.08353" title="Download PostScript">ps</a>, <a href="/format/2306.08353" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Characterizing First Arrival Position Channels: Noise Distribution and  Capacity Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yen-Chi Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lo%2C+Y">Yun-Feng Lo</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jen-Ming Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+M">Min-Hsiu Hsieh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 9 figures, 1 table; this manuscript (v3) is submitted to IEEE Transactions on Communications and currently is under minor revision
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Emerging Technologies (cs.ET)

</div>
</div>
</dd>
<dt><a name="item952">[952]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08651" title="Abstract">arXiv:2306.08651</a> (replaced) [<a href="/pdf/2306.08651" title="Download PDF">pdf</a>, <a href="/format/2306.08651" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Grounded Commonsense Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kwon%2C+M">Minae Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+H">Hengyuan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Myers%2C+V">Vivek Myers</a>, 
<a href="/search/cs?searchtype=author&query=Karamcheti%2C+S">Siddharth Karamcheti</a>, 
<a href="/search/cs?searchtype=author&query=Dragan%2C+A">Anca Dragan</a>, 
<a href="/search/cs?searchtype=author&query=Sadigh%2C+D">Dorsa Sadigh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IEEE International Conference on Robotics and Automation 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item953">[953]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.08848" title="Abstract">arXiv:2306.08848</a> (replaced) [<a href="/pdf/2306.08848" title="Download PDF">pdf</a>, <a href="/format/2306.08848" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Datasheets for Machine Learning Sensors: Towards Transparency,  Auditability, and Responsibility for Intelligent Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stewart%2C+M">Matthew Stewart</a>, 
<a href="/search/cs?searchtype=author&query=Warden%2C+P">Pete Warden</a>, 
<a href="/search/cs?searchtype=author&query=Omri%2C+Y">Yasmine Omri</a>, 
<a href="/search/cs?searchtype=author&query=Prakash%2C+S">Shvetank Prakash</a>, 
<a href="/search/cs?searchtype=author&query=Santos%2C+J">Joao Santos</a>, 
<a href="/search/cs?searchtype=author&query=Hymel%2C+S">Shawn Hymel</a>, 
<a href="/search/cs?searchtype=author&query=Brown%2C+B">Benjamin Brown</a>, 
<a href="/search/cs?searchtype=author&query=MacArthur%2C+J">Jim MacArthur</a>, 
<a href="/search/cs?searchtype=author&query=Jeffries%2C+N">Nat Jeffries</a>, 
<a href="/search/cs?searchtype=author&query=Katti%2C+S">Sachin Katti</a>, 
<a href="/search/cs?searchtype=author&query=Plancher%2C+B">Brian Plancher</a>, 
<a href="/search/cs?searchtype=author&query=Reddi%2C+V+J">Vijay Janapa Reddi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item954">[954]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.09432" title="Abstract">arXiv:2306.09432</a> (replaced) [<a href="/pdf/2306.09432" title="Download PDF">pdf</a>, <a href="/format/2306.09432" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quantum State Tomography for Matrix Product Density Operators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Qin%2C+Z">Zhen Qin</a>, 
<a href="/search/quant-ph?searchtype=author&query=Jameson%2C+C">Casey Jameson</a>, 
<a href="/search/quant-ph?searchtype=author&query=Gong%2C+Z">Zhexuan Gong</a>, 
<a href="/search/quant-ph?searchtype=author&query=Wakin%2C+M+B">Michael B. Wakin</a>, 
<a href="/search/quant-ph?searchtype=author&query=Zhu%2C+Z">Zhihui Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Information Theory (cs.IT); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item955">[955]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.10001" title="Abstract">arXiv:2306.10001</a> (replaced) [<a href="/pdf/2306.10001" title="Download PDF">pdf</a>, <a href="/format/2306.10001" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Group Orthogonalization Regularization For Vision Models Adaptation and  Robustness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kurtz%2C+Y">Yoav Kurtz</a>, 
<a href="/search/cs?searchtype=author&query=Bar%2C+N">Noga Bar</a>, 
<a href="/search/cs?searchtype=author&query=Giryes%2C+R">Raja Giryes</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> BMVC 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item956">[956]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11011" title="Abstract">arXiv:2306.11011</a> (replaced) [<a href="/pdf/2306.11011" title="Download PDF">pdf</a>, <a href="/format/2306.11011" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> virtCCA: Virtualized Arm Confidential Compute Architecture with  TrustZone
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiangyi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenhao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yongzheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chenyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Huifeng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+H">Haocheng Ma</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+Z">Zhennan Min</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+Z">Zixuan Pang</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+R">Rui Hou</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Y">Yier Jin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item957">[957]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11157" title="Abstract">arXiv:2306.11157</a> (replaced) [<a href="/pdf/2306.11157" title="Download PDF">pdf</a>, <a href="/format/2306.11157" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human Limits in Machine Learning: Prediction of Plant Phenotypes Using  Soil Microbiome Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Aghdam%2C+R">Rosa Aghdam</a>, 
<a href="/search/stat?searchtype=author&query=Tang%2C+X">Xudong Tang</a>, 
<a href="/search/stat?searchtype=author&query=Shan%2C+S">Shan Shan</a>, 
<a href="/search/stat?searchtype=author&query=Lankau%2C+R">Richard Lankau</a>, 
<a href="/search/stat?searchtype=author&query=Sol%C3%ADs-Lemus%2C+C">Claudia Sol&#xed;s-Lemus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Applications (stat.AP)

</div>
</div>
</dd>
<dt><a name="item958">[958]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.11282" title="Abstract">arXiv:2306.11282</a> (replaced) [<a href="/pdf/2306.11282" title="Download PDF">pdf</a>, <a href="/format/2306.11282" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Phase Repair for Time-Domain Convolutional Neural Networks in Music  Super-Resolution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yenan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Kolkman%2C+G">Guilly Kolkman</a>, 
<a href="/search/cs?searchtype=author&query=Watanabe%2C+H">Hiroshi Watanabe</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item959">[959]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.12106" title="Abstract">arXiv:2306.12106</a> (replaced) [<a href="/pdf/2306.12106" title="Download PDF">pdf</a>, <a href="/format/2306.12106" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ViTEraser: Harnessing the Power of Vision Transformers for Scene Text  Removal with SegMIM Pretraining
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+D">Dezhi Peng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chongyu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yuliang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+L">Lianwen Jin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 2024; Full Version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item960">[960]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13637" title="Abstract">arXiv:2306.13637</a> (replaced) [<a href="/pdf/2306.13637" title="Download PDF">pdf</a>, <a href="/format/2306.13637" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Constrained optimization of sensor placement for nuclear digital twins
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Karnik%2C+N">Niharika Karnik</a>, 
<a href="/search/math?searchtype=author&query=Abdo%2C+M+G">Mohammad G. Abdo</a>, 
<a href="/search/math?searchtype=author&query=Perez%2C+C+E+E">Carlos E. Estrada Perez</a>, 
<a href="/search/math?searchtype=author&query=Yoo%2C+J+S">Jun Soo Yoo</a>, 
<a href="/search/math?searchtype=author&query=Cogliati%2C+J+J">Joshua J. Cogliati</a>, 
<a href="/search/math?searchtype=author&query=Skifton%2C+R+S">Richard S. Skifton</a>, 
<a href="/search/math?searchtype=author&query=Calderoni%2C+P">Pattrick Calderoni</a>, 
<a href="/search/math?searchtype=author&query=Brunton%2C+S+L">Steven L. Brunton</a>, 
<a href="/search/math?searchtype=author&query=Manohar%2C+K">Krithika Manohar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item961">[961]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.13941" title="Abstract">arXiv:2306.13941</a> (replaced) [<a href="/pdf/2306.13941" title="Download PDF">pdf</a>, <a href="/format/2306.13941" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Grassroots Social Networking: Where People have Agency over their  Personal Information and Social Graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shapiro%2C+E">Ehud Shapiro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>; Computers and Society (cs.CY); Multiagent Systems (cs.MA); Networking and Internet Architecture (cs.NI); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item962">[962]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16060" title="Abstract">arXiv:2306.16060</a> (replaced) [<a href="/pdf/2306.16060" title="Download PDF">pdf</a>, <a href="/format/2306.16060" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Path-Controllable Deep Unfolding Network for Compressive Sensing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+J">Jiechong Song</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jian Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> TIP, 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Image and Video Processing (eess.IV)

</div>
</div>
</dd>
<dt><a name="item963">[963]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.16261" title="Abstract">arXiv:2306.16261</a> (replaced) [<a href="/pdf/2306.16261" title="Download PDF">pdf</a>, <a href="/format/2306.16261" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SE-PQA: Personalized Community Question Answering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kasela%2C+P">Pranav Kasela</a>, 
<a href="/search/cs?searchtype=author&query=Braga%2C+M">Marco Braga</a>, 
<a href="/search/cs?searchtype=author&query=Pasi%2C+G">Gabriella Pasi</a>, 
<a href="/search/cs?searchtype=author&query=Perego%2C+R">Raffaele Perego</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item964">[964]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.17256" title="Abstract">arXiv:2306.17256</a> (replaced) [<a href="/pdf/2306.17256" title="Download PDF">pdf</a>, <a href="/format/2306.17256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Could Small Language Models Serve as Recommenders? Towards Data-centric  Cold-start Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xuansheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Huachi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yucheng Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+W">Wenlin Yao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Ninghao Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item965">[965]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2306.17828" title="Abstract">arXiv:2306.17828</a> (replaced) [<a href="/pdf/2306.17828" title="Download PDF">pdf</a>, <a href="/format/2306.17828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Cause of Unfairness: A Training Sample Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuanshun Yao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item966">[966]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.01649" title="Abstract">arXiv:2307.01649</a> (replaced) [<a href="/pdf/2307.01649" title="Download PDF">pdf</a>, <a href="/format/2307.01649" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nonparametric Classification on Low Dimensional Manifolds using  Overparameterized Convolutional Residual Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+K">Kaiqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zixuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+M">Minshuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Takeda%2C+Y">Yuma Takeda</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mengdi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tuo Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yu-Xiang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item967">[967]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02485" title="Abstract">arXiv:2307.02485</a> (replaced) [<a href="/pdf/2307.02485" title="Download PDF">pdf</a>, <a href="/format/2307.02485" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Building Cooperative Embodied Agents Modularly with Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hongxin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+W">Weihua Du</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+J">Jiaming Shan</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Q">Qinhong Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yilun Du</a>, 
<a href="/search/cs?searchtype=author&query=Tenenbaum%2C+J+B">Joshua B. Tenenbaum</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+T">Tianmin Shu</a>, 
<a href="/search/cs?searchtype=author&query=Gan%2C+C">Chuang Gan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR24. The first two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item968">[968]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.02564" title="Abstract">arXiv:2307.02564</a> (replaced) [<a href="/pdf/2307.02564" title="Download PDF">pdf</a>, <a href="/format/2307.02564" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Secure-by-Construction Synthesis for Control Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhong%2C+B">Bingzhuo Zhong</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+S">Siyuan Liu</a>, 
<a href="/search/eess?searchtype=author&query=Caccamo%2C+M">Marco Caccamo</a>, 
<a href="/search/eess?searchtype=author&query=Zamani%2C+M">Majid Zamani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item969">[969]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.03177" title="Abstract">arXiv:2307.03177</a> (replaced) [<a href="/pdf/2307.03177" title="Download PDF">pdf</a>, <a href="/format/2307.03177" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PanoDiffusion: 360-degree Panorama Outpainting via Diffusion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+T">Tianhao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+C">Chuanxia Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Cham%2C+T">Tat-Jen Cham</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> <a href="https://sm0kywu.github.io/panodiffusion/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item970">[970]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.04866" title="Abstract">arXiv:2307.04866</a> (replaced) [<a href="/pdf/2307.04866" title="Download PDF">pdf</a>, <a href="/format/2307.04866" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gait Event Detection and Travel Distance Using Waist-Worn Accelerometers  across a Range of Speeds: Automated Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Ramli%2C+A+A">Albara Ah Ramli</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+X">Xin Liu</a>, 
<a href="/search/eess?searchtype=author&query=Berndt%2C+K">Kelly Berndt</a>, 
<a href="/search/eess?searchtype=author&query=Chuah%2C+C">Chen-Nee Chuah</a>, 
<a href="/search/eess?searchtype=author&query=Goude%2C+E">Erica Goude</a>, 
<a href="/search/eess?searchtype=author&query=Kaethler%2C+L+B">Lynea B. Kaethler</a>, 
<a href="/search/eess?searchtype=author&query=Lopez%2C+A">Amanda Lopez</a>, 
<a href="/search/eess?searchtype=author&query=Nicorici%2C+A">Alina Nicorici</a>, 
<a href="/search/eess?searchtype=author&query=Owens%2C+C">Corey Owens</a>, 
<a href="/search/eess?searchtype=author&query=Rodriguez%2C+D">David Rodriguez</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jane Wang</a>, 
<a href="/search/eess?searchtype=author&query=Aranki%2C+D">Daniel Aranki</a>, 
<a href="/search/eess?searchtype=author&query=McDonald%2C+C+M">Craig M. McDonald</a>, 
<a href="/search/eess?searchtype=author&query=Henricson%2C+E+K">Erik K. Henricson</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Sensors. 2024; 24(4):1155
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item971">[971]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.05209" title="Abstract">arXiv:2307.05209</a> (replaced) [<a href="/pdf/2307.05209" title="Download PDF">pdf</a>, <a href="/format/2307.05209" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contextual Pre-Planning on Reward Machine Abstractions for Enhanced  Transfer in Deep Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Azran%2C+G">Guy Azran</a>, 
<a href="/search/cs?searchtype=author&query=Danesh%2C+M+H">Mohamad H. Danesh</a>, 
<a href="/search/cs?searchtype=author&query=Albrecht%2C+S+V">Stefano V. Albrecht</a>, 
<a href="/search/cs?searchtype=author&query=Keren%2C+S">Sarah Keren</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Proceedings of the 38th AAAI Conference on Artificial Intelligence (AAAI), 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item972">[972]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06865" title="Abstract">arXiv:2307.06865</a> (replaced) [<a href="/pdf/2307.06865" title="Download PDF">pdf</a>, <a href="/format/2307.06865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective Prompt Extraction from Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yiming Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Carlini%2C+N">Nicholas Carlini</a>, 
<a href="/search/cs?searchtype=author&query=Ippolito%2C+D">Daphne Ippolito</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item973">[973]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.06976" title="Abstract">arXiv:2307.06976</a> (replaced) [<a href="/pdf/2307.06976" title="Download PDF">pdf</a>, <a href="/ps/2307.06976" title="Download PostScript">ps</a>, <a href="/format/2307.06976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Complexity of Target Set Selection in Simple Geometric Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dvo%C5%99%C3%A1k%2C+M">Michal Dvo&#x159;&#xe1;k</a>, 
<a href="/search/cs?searchtype=author&query=Knop%2C+D">Du&#x161;an Knop</a>, 
<a href="/search/cs?searchtype=author&query=Schierreich%2C+%C5%A0">&#x160;imon Schierreich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This is an extended and revised version of a preliminary conference report that was presented in WAW 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item974">[974]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07462" title="Abstract">arXiv:2307.07462</a> (replaced) [<a href="/pdf/2307.07462" title="Download PDF">pdf</a>, <a href="/format/2307.07462" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Computing Zigzag Vineyard Efficiently Including Expansions and  Contractions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dey%2C+T+K">Tamal K. Dey</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+T">Tao Hou</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated funding information for one co-author
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Algebraic Topology (math.AT)

</div>
</div>
</dd>
<dt><a name="item975">[975]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07494" title="Abstract">arXiv:2307.07494</a> (replaced) [<a href="/pdf/2307.07494" title="Download PDF">pdf</a>, <a href="/format/2307.07494" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TALL: Thumbnail Layout for Deepfake Video Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuting Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jian Liang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+G">Gengyun Jia</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Ziming Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yanhao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+R">Ran He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICCV 2023; We revised the first paragraph of section 3
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item976">[976]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.07727" title="Abstract">arXiv:2307.07727</a> (replaced) [<a href="/pdf/2307.07727" title="Download PDF">pdf</a>, <a href="/ps/2307.07727" title="Download PostScript">ps</a>, <a href="/format/2307.07727" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Mixing via Tensorization for Random Independent Sets on  Arbitrary Trees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Efthymiou%2C+C">Charilaos Efthymiou</a>, 
<a href="/search/cs?searchtype=author&query=Hayes%2C+T+P">Thomas P. Hayes</a>, 
<a href="/search/cs?searchtype=author&query=Stefankovic%2C+D">Daniel Stefankovic</a>, 
<a href="/search/cs?searchtype=author&query=Vigoda%2C+E">Eric Vigoda</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The optimum mixing result (Theorem 1.2) of version 1 of the manuscript has been removed due to an error
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item977">[977]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.08433" title="Abstract">arXiv:2307.08433</a> (replaced) [<a href="/pdf/2307.08433" title="Download PDF">pdf</a>, <a href="/format/2307.08433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From random-walks to graph-sprints: a low-latency node embedding  framework on continuous-time dynamic graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eddin%2C+A+N">Ahmad Naser Eddin</a>, 
<a href="/search/cs?searchtype=author&query=Bono%2C+J">Jacopo Bono</a>, 
<a href="/search/cs?searchtype=author&query=Apar%C3%ADcio%2C+D">David Apar&#xed;cio</a>, 
<a href="/search/cs?searchtype=author&query=Ferreira%2C+H">Hugo Ferreira</a>, 
<a href="/search/cs?searchtype=author&query=Ascens%C3%A3o%2C+J">Jo&#xe3;o Ascens&#xe3;o</a>, 
<a href="/search/cs?searchtype=author&query=Ribeiro%2C+P">Pedro Ribeiro</a>, 
<a href="/search/cs?searchtype=author&query=Bizarro%2C+P">Pedro Bizarro</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 5 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item978">[978]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09110" title="Abstract">arXiv:2307.09110</a> (replaced) [<a href="/pdf/2307.09110" title="Download PDF">pdf</a>, <a href="/ps/2307.09110" title="Download PostScript">ps</a>, <a href="/format/2307.09110" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Cut Sparsification and Succinct Representation of Submodular Hypergraphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kenneth%2C+Y">Yotam Kenneth</a>, 
<a href="/search/cs?searchtype=author&query=Krauthgamer%2C+R">Robert Krauthgamer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item979">[979]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.09529" title="Abstract">arXiv:2307.09529</a> (replaced) [<a href="/pdf/2307.09529" title="Download PDF">pdf</a>, <a href="/format/2307.09529" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> QDoor: Exploiting Approximate Synthesis for Backdoor Attacks in Quantum  Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/quant-ph?searchtype=author&query=Chu%2C+C">Cheng Chu</a>, 
<a href="/search/quant-ph?searchtype=author&query=Chen%2C+F">Fan Chen</a>, 
<a href="/search/quant-ph?searchtype=author&query=Richerme%2C+P">Philip Richerme</a>, 
<a href="/search/quant-ph?searchtype=author&query=Jiang%2C+L">Lei Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Quantum Physics (quant-ph)</span>; Emerging Technologies (cs.ET)

</div>
</div>
</dd>
<dt><a name="item980">[980]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10332" title="Abstract">arXiv:2307.10332</a> (replaced) [<a href="/pdf/2307.10332" title="Download PDF">pdf</a>, <a href="/format/2307.10332" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Labeling Methods for Partially Ordered Paths
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Euler%2C+R">Ricardo Euler</a>, 
<a href="/search/cs?searchtype=author&query=de+las+Casas%2C+P+M">Pedro Maristany de las Casas</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>; Optimization and Control (math.OC)

</div>
</div>
</dd>
<dt><a name="item981">[981]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.10655" title="Abstract">arXiv:2307.10655</a> (replaced) [<a href="/pdf/2307.10655" title="Download PDF">pdf</a>, <a href="/format/2307.10655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey of What to Share in Federated Learning: Perspectives on Model  Utility, Privacy Leakage, and Communication Efficiency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shao%2C+J">Jiawei Shao</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zijian Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+W">Wenqiang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+T">Tailin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Y">Yuchang Sun</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Lumin Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Z">Zehong Lin</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+Y">Yuyi Mao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jun Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item982">[982]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11114" title="Abstract">arXiv:2307.11114</a> (replaced) [<a href="/pdf/2307.11114" title="Download PDF">pdf</a>, <a href="/ps/2307.11114" title="Download PostScript">ps</a>, <a href="/format/2307.11114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Nature of Intelligence
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=You%2C+B+J">Barco Jie You</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neurons and Cognition (q-bio.NC)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item983">[983]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11471" title="Abstract">arXiv:2307.11471</a> (replaced) [<a href="/pdf/2307.11471" title="Download PDF">pdf</a>, <a href="/format/2307.11471" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Visual Question Answering: Datasets, Methods, and Future  Challenges
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ma%2C+J">Jie Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Pinghui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+D">Dechen Kong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zewei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Pei%2C+H">Hongbin Pei</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Junzhou Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by IEEE TPAMI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item984">[984]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11655" title="Abstract">arXiv:2307.11655</a> (replaced) [<a href="/pdf/2307.11655" title="Download PDF">pdf</a>, <a href="/format/2307.11655" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preferences Evolve And So Should Your Bandits: Bandits with Evolving  States for Online Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Khosravi%2C+K">Khashayar Khosravi</a>, 
<a href="/search/cs?searchtype=author&query=Leme%2C+R+P">Renato Paes Leme</a>, 
<a href="/search/cs?searchtype=author&query=Podimata%2C+C">Chara Podimata</a>, 
<a href="/search/cs?searchtype=author&query=Tsorvantzis%2C+A">Apostolis Tsorvantzis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item985">[985]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.11729" title="Abstract">arXiv:2307.11729</a> (replaced) [<a href="/pdf/2307.11729" title="Download PDF">pdf</a>, <a href="/format/2307.11729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OUTFOX: LLM-Generated Essay Detection Through In-Context Learning with  Adversarially Generated Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Koike%2C+R">Ryuto Koike</a>, 
<a href="/search/cs?searchtype=author&query=Kaneko%2C+M">Masahiro Kaneko</a>, 
<a href="/search/cs?searchtype=author&query=Okazaki%2C+N">Naoaki Okazaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 2024 camera ready. Code and dataset available at <a href="https://github.com/ryuryukke/OUTFOX">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item986">[986]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.12441" title="Abstract">arXiv:2307.12441</a> (replaced) [<a href="/pdf/2307.12441" title="Download PDF">pdf</a>, <a href="/format/2307.12441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Swarm-based optimization with random descent
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Tadmor%2C+E">Eitan Tadmor</a>, 
<a href="/search/math?searchtype=author&query=Zenginoglu%2C+A">Anil Zenginoglu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item987">[987]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.15663" title="Abstract">arXiv:2307.15663</a> (replaced) [<a href="/pdf/2307.15663" title="Download PDF">pdf</a>, <a href="/format/2307.15663" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoRe Optimizer: An All-in-One Solution for Machine Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Eckhoff%2C+M">Marco Eckhoff</a>, 
<a href="/search/cs?searchtype=author&query=Reiher%2C+M">Markus Reiher</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 5 figures, 1 table
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Mach. Learn.: Sci. Technol. 5 (2024) 015018
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Computational Physics (physics.comp-ph)

</div>
</div>
</dd>
<dt><a name="item988">[988]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2307.16847" title="Abstract">arXiv:2307.16847</a> (replaced) [<a href="/pdf/2307.16847" title="Download PDF">pdf</a>, <a href="/format/2307.16847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CroSSL: Cross-modal Self-Supervised Learning for Time-series through  Latent Masking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deldari%2C+S">Shohreh Deldari</a>, 
<a href="/search/cs?searchtype=author&query=Spathis%2C+D">Dimitris Spathis</a>, 
<a href="/search/cs?searchtype=author&query=Malekzadeh%2C+M">Mohammad Malekzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Kawsar%2C+F">Fahim Kawsar</a>, 
<a href="/search/cs?searchtype=author&query=Salim%2C+F">Flora Salim</a>, 
<a href="/search/cs?searchtype=author&query=Mathur%2C+A">Akhil Mathur</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted in WSDM24. Short version presented in ML4MHD @ICML23
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item989">[989]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.01123" title="Abstract">arXiv:2308.01123</a> (replaced) [<a href="/pdf/2308.01123" title="Download PDF">pdf</a>, <a href="/format/2308.01123" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Planar Friction Modelling with LuGre Dynamics and Limit Surfaces
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Waltersson%2C+G+A">Gabriel Arslan Waltersson</a>, 
<a href="/search/eess?searchtype=author&query=Karayiannidis%2C+Y">Yiannis Karayiannidis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Revised version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item990">[990]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02345" title="Abstract">arXiv:2308.02345</a> (replaced) [<a href="/pdf/2308.02345" title="Download PDF">pdf</a>, <a href="/format/2308.02345" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Communication-Efficient Decentralized Multi-Agent Reinforcement Learning  for Cooperative Adaptive Cruise Control
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+D">Dong Chen</a>, 
<a href="/search/eess?searchtype=author&query=Zhang%2C+K">Kaixiang Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yongqiang Wang</a>, 
<a href="/search/eess?searchtype=author&query=Yin%2C+X">Xunyuan Yin</a>, 
<a href="/search/eess?searchtype=author&query=Li%2C+Z">Zhaojian Li</a>, 
<a href="/search/eess?searchtype=author&query=Filev%2C+D">Dimitar Filev</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 11 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item991">[991]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.02802" title="Abstract">arXiv:2308.02802</a> (replaced) [<a href="/pdf/2308.02802" title="Download PDF">pdf</a>, <a href="/format/2308.02802" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning physics-based reduced-order models from data using nonlinear  manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Geelen%2C+R">Rudy Geelen</a>, 
<a href="/search/math?searchtype=author&query=Balzano%2C+L">Laura Balzano</a>, 
<a href="/search/math?searchtype=author&query=Wright%2C+S">Stephen Wright</a>, 
<a href="/search/math?searchtype=author&query=Willcox%2C+K">Karen Willcox</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item992">[992]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.03321" title="Abstract">arXiv:2308.03321</a> (replaced) [<a href="/pdf/2308.03321" title="Download PDF">pdf</a>, <a href="/format/2308.03321" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AFN: Adaptive Fusion Normalization via an Encoder-Decoder Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Z">Zikai Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shuo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Ziruo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huanran Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: text overlap with <a href="/abs/2106.01899">arXiv:2106.01899</a> by other authors
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item993">[993]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04247" title="Abstract">arXiv:2308.04247</a> (replaced) [<a href="/pdf/2308.04247" title="Download PDF">pdf</a>, <a href="/format/2308.04247" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UniRecSys: A Unified Framework for Personalized, Group, Package, and  Package-to-Group Recommendations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shyam%2C+A">Adamya Shyam</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+V">Vikas Kumar</a>, 
<a href="/search/cs?searchtype=author&query=Kagita%2C+V+R">Venkateswara Rao Kagita</a>, 
<a href="/search/cs?searchtype=author&query=Pujari%2C+A+K">Arun K Pujari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 29 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item994">[994]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.04882" title="Abstract">arXiv:2308.04882</a> (replaced) [<a href="/pdf/2308.04882" title="Download PDF">pdf</a>, <a href="/format/2308.04882" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multipacking and broadcast domination on cactus graph and its impact on  hyperbolic graph
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+S">Sandip Das</a>, 
<a href="/search/cs?searchtype=author&query=Islam%2C+S+S">Sk Samim Islam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Discrete Mathematics (cs.DM)</span>

</div>
</div>
</dd>
<dt><a name="item995">[995]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05724" title="Abstract">arXiv:2308.05724</a> (replaced) [<a href="/pdf/2308.05724" title="Download PDF">pdf</a>, <a href="/format/2308.05724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimizing Performance of Feedforward and Convolutional Neural Networks  through Dynamic Activation Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Rane%2C+C">Chinmay Rane</a>, 
<a href="/search/cs?searchtype=author&query=Tyagi%2C+K">Kanishka Tyagi</a>, 
<a href="/search/cs?searchtype=author&query=Manry%2C+M">Michael Manry</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Tiny ICLR Approved
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item996">[996]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.05809" title="Abstract">arXiv:2308.05809</a> (replaced) [<a href="/pdf/2308.05809" title="Download PDF">pdf</a>, <a href="/format/2308.05809" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toward Process Controlled Medical Robotic System
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yihao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kheradmand%2C+A">Amir Kheradmand</a>, 
<a href="/search/cs?searchtype=author&query=Armand%2C+M">Mehran Armand</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item997">[997]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07124" title="Abstract">arXiv:2308.07124</a> (replaced) [<a href="/pdf/2308.07124" title="Download PDF">pdf</a>, <a href="/format/2308.07124" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OctoPack: Instruction Tuning Code Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Muennighoff%2C+N">Niklas Muennighoff</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zebaze%2C+A">Armel Zebaze</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Q">Qinkai Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Hui%2C+B">Binyuan Hui</a>, 
<a href="/search/cs?searchtype=author&query=Zhuo%2C+T+Y">Terry Yue Zhuo</a>, 
<a href="/search/cs?searchtype=author&query=Singh%2C+S">Swayam Singh</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiangru Tang</a>, 
<a href="/search/cs?searchtype=author&query=von+Werra%2C+L">Leandro von Werra</a>, 
<a href="/search/cs?searchtype=author&query=Longpre%2C+S">Shayne Longpre</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 60 pages (9 main), 40 figures, 19 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item998">[998]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07480" title="Abstract">arXiv:2308.07480</a> (replaced) [<a href="/pdf/2308.07480" title="Download PDF">pdf</a>, <a href="/format/2308.07480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Order-based Structure Learning with Normalizing Flows
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamkari%2C+H">Hamidreza Kamkari</a>, 
<a href="/search/cs?searchtype=author&query=Balazadeh%2C+V">Vahid Balazadeh</a>, 
<a href="/search/cs?searchtype=author&query=Zehtab%2C+V">Vahid Zehtab</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+R+G">Rahul G. Krishnan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item999">[999]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.07758" title="Abstract">arXiv:2308.07758</a> (replaced) [<a href="/pdf/2308.07758" title="Download PDF">pdf</a>, <a href="/format/2308.07758" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forward-Backward Reasoning in Large Language Models for Mathematical  Verification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Weisen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+H">Han Shi</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Longhui Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhengying Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Kwok%2C+J+T">James T. Kwok</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Technical Report
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1000">[1000]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.09862" title="Abstract">arXiv:2308.09862</a> (replaced) [<a href="/pdf/2308.09862" title="Download PDF">pdf</a>, <a href="/format/2308.09862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Breaking Language Barriers: A Question Answering Dataset for Hindi and  Marathi
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sabane%2C+M">Maithili Sabane</a>, 
<a href="/search/cs?searchtype=author&query=Litake%2C+O">Onkar Litake</a>, 
<a href="/search/cs?searchtype=author&query=Chadha%2C+A">Aman Chadha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1001">[1001]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10204" title="Abstract">arXiv:2308.10204</a> (replaced) [<a href="/pdf/2308.10204" title="Download PDF">pdf</a>, <a href="/format/2308.10204" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatEDA: A Large Language Model Powered Autonomous Agent for EDA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhuolun He</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haoyuan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xinyun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+X">Xufeng Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+S">Su Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Haisheng Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Bei Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1002">[1002]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.10873" title="Abstract">arXiv:2308.10873</a> (replaced) [<a href="/pdf/2308.10873" title="Download PDF">pdf</a>, <a href="/format/2308.10873" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SpikingBERT: Distilling BERT to Train Spiking Language Models Using  Implicit Differentiation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bal%2C+M">Malyaban Bal</a>, 
<a href="/search/cs?searchtype=author&query=Sengupta%2C+A">Abhronil Sengupta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item1003">[1003]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11131" title="Abstract">arXiv:2308.11131</a> (replaced) [<a href="/pdf/2308.11131" title="Download PDF">pdf</a>, <a href="/format/2308.11131" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential  Behavior Comprehension in Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jianghao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+R">Rong Shan</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Chenxu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+K">Kounianhua Du</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+B">Bo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Quan%2C+S">Shigang Quan</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+R">Ruiming Tang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Weinan Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW 2024. Full and More Readable Version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1004">[1004]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.11804" title="Abstract">arXiv:2308.11804</a> (replaced) [<a href="/pdf/2308.11804" title="Download PDF">pdf</a>, <a href="/format/2308.11804" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Illusions in Multi-Modal Embeddings
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tingwei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+R">Rishi Jha</a>, 
<a href="/search/cs?searchtype=author&query=Bagdasaryan%2C+E">Eugene Bagdasaryan</a>, 
<a href="/search/cs?searchtype=author&query=Shmatikov%2C+V">Vitaly Shmatikov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1005">[1005]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13250" title="Abstract">arXiv:2308.13250</a> (replaced) [<a href="/pdf/2308.13250" title="Download PDF">pdf</a>, <a href="/format/2308.13250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TC-LIF: A Two-Compartment Spiking Neuron Model for Long-Term Sequential  Modelling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shimin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Q">Qu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chenxiang Ma</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jibin Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haizhou Li</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+K+C">Kay Chen Tan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> arXiv admin note: substantial text overlap with <a href="/abs/2307.07231">arXiv:2307.07231</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item1006">[1006]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.13916" title="Abstract">arXiv:2308.13916</a> (replaced) [<a href="/pdf/2308.13916" title="Download PDF">pdf</a>, <a href="/format/2308.13916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring Large Language Models for Knowledge Graph Completion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+L">Liang Yao</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+J">Jiazhen Peng</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+C">Chengsheng Mao</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yuan Luo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1007">[1007]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.14778" title="Abstract">arXiv:2308.14778</a> (replaced) [<a href="/pdf/2308.14778" title="Download PDF">pdf</a>, <a href="/ps/2308.14778" title="Download PostScript">ps</a>, <a href="/format/2308.14778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A precise condition for independent transversals in bipartite covers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Cambie%2C+S">Stijn Cambie</a>, 
<a href="/search/math?searchtype=author&query=Haxell%2C+P">Penny Haxell</a>, 
<a href="/search/math?searchtype=author&query=Kang%2C+R+J">Ross J. Kang</a>, 
<a href="/search/math?searchtype=author&query=Wdowinski%2C+R">Ronen Wdowinski</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 2 figures; v2 is AAM accepted to SIDMA after incorporating minor corrections
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item1008">[1008]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15022" title="Abstract">arXiv:2308.15022</a> (replaced) [<a href="/pdf/2308.15022" title="Download PDF">pdf</a>, <a href="/format/2308.15022" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recursively Summarizing Enables Long-Term Dialogue Memory in Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qingyue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+L">Liang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yanan Cao</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+Z">Zhiliang Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Tao%2C+D">Dacheng Tao</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+L">Li Guo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1009">[1009]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.15307" title="Abstract">arXiv:2308.15307</a> (replaced) [<a href="/pdf/2308.15307" title="Download PDF">pdf</a>, <a href="/format/2308.15307" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compositional maps for registration in complex geometries
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Taddei%2C+T">Tommaso Taddei</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1010">[1010]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16017" title="Abstract">arXiv:2308.16017</a> (replaced) [<a href="/pdf/2308.16017" title="Download PDF">pdf</a>, <a href="/ps/2308.16017" title="Download PostScript">ps</a>, <a href="/format/2308.16017" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hidden-Role Games: Equilibrium Concepts and Computation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Carminati%2C+L">Luca Carminati</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B+H">Brian Hu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Farina%2C+G">Gabriele Farina</a>, 
<a href="/search/cs?searchtype=author&query=Gatti%2C+N">Nicola Gatti</a>, 
<a href="/search/cs?searchtype=author&query=Sandholm%2C+T">Tuomas Sandholm</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item1011">[1011]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2308.16215" title="Abstract">arXiv:2308.16215</a> (replaced) [<a href="/pdf/2308.16215" title="Download PDF">pdf</a>, <a href="/format/2308.16215" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Deep Video Codec Control for Vision Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Reich%2C+C">Christoph Reich</a>, 
<a href="/search/eess?searchtype=author&query=Debnath%2C+B">Biplob Debnath</a>, 
<a href="/search/eess?searchtype=author&query=Patel%2C+D">Deep Patel</a>, 
<a href="/search/eess?searchtype=author&query=Prangemeier%2C+T">Tim Prangemeier</a>, 
<a href="/search/eess?searchtype=author&query=Cremers%2C+D">Daniel Cremers</a>, 
<a href="/search/eess?searchtype=author&query=Chakradhar%2C+S">Srimat Chakradhar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages, 26 figures, 7 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Multimedia (cs.MM)

</div>
</div>
</dd>
<dt><a name="item1012">[1012]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00300" title="Abstract">arXiv:2309.00300</a> (replaced) [<a href="/pdf/2309.00300" title="Download PDF">pdf</a>, <a href="/format/2309.00300" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards the Identifiability and Explainability for Personalized Learner  Modeling: An Inductive Paradigm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiatong Li</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Q">Qi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+F">Fei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiayu Liu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zhenya Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+F">Fangzhou Yao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Linbo Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+Y">Yu Su</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the ACM Web Conference 2024 (WWW '24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1013">[1013]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.00440" title="Abstract">arXiv:2309.00440</a> (replaced) [<a href="/pdf/2309.00440" title="Download PDF">pdf</a>, <a href="/format/2309.00440" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Yet another Improvement of Plantard Arithmetic for Faster Kyber on  Low-end 32-bit IoT Devices
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Junhao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+H">Haosong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jipeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+W">Wangchen Dai</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+L">Lu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Cheung%2C+R+C+C">Ray C.C. Cheung</a>, 
<a href="/search/cs?searchtype=author&query=Koc%2C+C+K">Cetin Kaya Koc</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Donglong Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item1014">[1014]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.05173" title="Abstract">arXiv:2309.05173</a> (replaced) [<a href="/pdf/2309.05173" title="Download PDF">pdf</a>, <a href="/format/2309.05173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zhengxiang Shi</a>, 
<a href="/search/cs?searchtype=author&query=Lipani%2C+A">Aldo Lipani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024. Code is available at <a href="https://github.com/ZhengxiangShi/DePT">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1015">[1015]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.06924" title="Abstract">arXiv:2309.06924</a> (replaced) [<a href="/pdf/2309.06924" title="Download PDF">pdf</a>, <a href="/format/2309.06924" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contrast-Phys+: Unsupervised and Weakly-supervised Video-based Remote  Physiological Measurement via Spatiotemporal Contrast
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+Z">Zhaodong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaobai Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by TPAMI
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1016">[1016]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.07759" title="Abstract">arXiv:2309.07759</a> (replaced) [<a href="/pdf/2309.07759" title="Download PDF">pdf</a>, <a href="/format/2309.07759" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PROGrasp: Pragmatic Human-Robot Communication for Object Grasping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kang%2C+G">Gi-Cheon Kang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Junghyun Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Jaein Kim</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Byoung-Tak Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1017">[1017]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08227" title="Abstract">arXiv:2309.08227</a> (replaced) [<a href="/pdf/2309.08227" title="Download PDF">pdf</a>, <a href="/format/2309.08227" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> VERSE: Virtual-Gradient Aware Streaming Lifelong Learning with Anytime  Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Banerjee%2C+S">Soumya Banerjee</a>, 
<a href="/search/cs?searchtype=author&query=Verma%2C+V+K">Vinay K. Verma</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+A">Avideep Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+D">Deepak Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Namboodiri%2C+V+P">Vinay P. Namboodiri</a>, 
<a href="/search/cs?searchtype=author&query=Rai%2C+P">Piyush Rai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1018">[1018]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08638" title="Abstract">arXiv:2309.08638</a> (replaced) [<a href="/pdf/2309.08638" title="Download PDF">pdf</a>, <a href="/format/2309.08638" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anchor Points: Benchmarking Models with Much Fewer Examples
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vivek%2C+R">Rajan Vivek</a>, 
<a href="/search/cs?searchtype=author&query=Ethayarajh%2C+K">Kawin Ethayarajh</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Diyi Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kiela%2C+D">Douwe Kiela</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024 Main Conference. Code will be released at: <a href="https://github.com/rvivek3/AnchorPoints">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1019">[1019]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.08902" title="Abstract">arXiv:2309.08902</a> (replaced) [<a href="/pdf/2309.08902" title="Download PDF">pdf</a>, <a href="/format/2309.08902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and  Nationality Bias in Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kamruzzaman%2C+M">Mahammed Kamruzzaman</a>, 
<a href="/search/cs?searchtype=author&query=Shovon%2C+M+M+I">Md. Minul Islam Shovon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+G+L">Gene Louis Kim</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1020">[1020]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.09737" title="Abstract">arXiv:2309.09737</a> (replaced) [<a href="/pdf/2309.09737" title="Download PDF">pdf</a>, <a href="/format/2309.09737" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pan%2C+Z">Zhijun Pan</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+F">Fangqiang Ding</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+H">Hantao Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+C+X">Chris Xiaoxuan Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICRA 2024. 8 pages, 4 figures. Co-first authorship for Zhijun Pan, Fangqiang Ding and Hantao Zhong, listed randomly
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1021">[1021]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10302" title="Abstract">arXiv:2309.10302</a> (replaced) [<a href="/pdf/2309.10302" title="Download PDF">pdf</a>, <a href="/format/2309.10302" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decoupled Training: Return of Frustratingly Easy Multi-Domain Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Ximei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+J">Junwei Pan</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+X">Xingzhuo Guo</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+D">Dapeng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+J">Jie Jiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item1022">[1022]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.10479" title="Abstract">arXiv:2309.10479</a> (replaced) [<a href="/pdf/2309.10479" title="Download PDF">pdf</a>, <a href="/format/2309.10479" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RECALL+: Adversarial Web-based Replay for Continual Learning in Semantic  Segmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chang Liu</a>, 
<a href="/search/cs?searchtype=author&query=Rizzoli%2C+G">Giulia Rizzoli</a>, 
<a href="/search/cs?searchtype=author&query=Barbato%2C+F">Francesco Barbato</a>, 
<a href="/search/cs?searchtype=author&query=Maracani%2C+A">Andrea Maracani</a>, 
<a href="/search/cs?searchtype=author&query=Toldo%2C+M">Marco Toldo</a>, 
<a href="/search/cs?searchtype=author&query=Michieli%2C+U">Umberto Michieli</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+Y">Yi Niu</a>, 
<a href="/search/cs?searchtype=author&query=Zanuttigh%2C+P">Pietro Zanuttigh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1023">[1023]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11015" title="Abstract">arXiv:2309.11015</a> (replaced) [<a href="/e-print/2309.11015" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> 3D-U-SAM Network For Few-shot Tooth Segmentation in CBCT Images
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Zhang%2C+Y">Yifu Zhang</a>, 
<a href="/search/eess?searchtype=author&query=Liu%2C+Z">Zuozhu Liu</a>, 
<a href="/search/eess?searchtype=author&query=Feng%2C+Y">Yang Feng</a>, 
<a href="/search/eess?searchtype=author&query=Xu%2C+R">Renjing Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper is rejected
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1024">[1024]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11268" title="Abstract">arXiv:2309.11268</a> (replaced) [<a href="/pdf/2309.11268" title="Download PDF">pdf</a>, <a href="/format/2309.11268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> StructChart: Perception, Structuring, Reasoning for Visual Chart  Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+R">Renqiu Xia</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B">Bo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+H">Haoyang Peng</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+H">Hancheng Ye</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+X">Xiangchao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+P">Peng Ye</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+B">Botian Shi</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Junchi Yan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> SimChart9K is available for downloading at: <a href="https://github.com/UniModal4Reasoning/SimChart9K">this https URL</a> 26 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1025">[1025]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.11754" title="Abstract">arXiv:2309.11754</a> (replaced) [<a href="/pdf/2309.11754" title="Download PDF">pdf</a>, <a href="/format/2309.11754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Vision-Centric Approach for Static Map Element Annotation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaxin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Shiyuan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+H">Haoran Yin</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+R">Ruohong Mei</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Cong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+W">Wei Sui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at 2024 IEEE International Conference on Robotics and Automation (ICRA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1026">[1026]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12244" title="Abstract">arXiv:2309.12244</a> (replaced) [<a href="/pdf/2309.12244" title="Download PDF">pdf</a>, <a href="/format/2309.12244" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChaCha: Leveraging Large Language Models to Prompt Children to Share  Their Emotions about Personal Events
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Seo%2C+W">Woosuk Seo</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+C">Chanmo Yang</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+Y">Young-Ho Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 5 figures, 2 tables; Accepted at ACM CHI 2024. More details at <a href="https://naver-ai.github.io/chacha/">this https URL</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> In Proceedings of the CHI Conference on Human Factors in Computing
  Systems (CHI '24), May 11-16, 2024, Honolulu, HI, USA. ACM, New York, NY, USA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1027">[1027]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12897" title="Abstract">arXiv:2309.12897</a> (replaced) [<a href="/pdf/2309.12897" title="Download PDF">pdf</a>, <a href="/format/2309.12897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Distributed Optimisation with Linear Equality and Inequality Constraints  using PDMM
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heusdens%2C+R">Richard Heusdens</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G">Guoqiang Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> acceped by IEEE Trans. Signal and Information Processing over Networks
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Distributed, Parallel, and Cluster Computing (cs.DC)</span>

</div>
</div>
</dd>
<dt><a name="item1028">[1028]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12916" title="Abstract">arXiv:2309.12916</a> (replaced) [<a href="/pdf/2309.12916" title="Download PDF">pdf</a>, <a href="/format/2309.12916" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meso-scale size effects of material heterogeneities on crack propagation  in brittle solids: Perspectives from phase-field simulations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cond-mat?searchtype=author&query=Li%2C+L">Liuchi Li</a>, 
<a href="/search/cond-mat?searchtype=author&query=Rao%2C+J">Jack Rao</a>, 
<a href="/search/cond-mat?searchtype=author&query=Hufnagel%2C+T">Todd Hufnagel</a>, 
<a href="/search/cond-mat?searchtype=author&query=Ramesh%2C+K">KT Ramesh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Materials Science (cond-mat.mtrl-sci)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item1029">[1029]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.12968" title="Abstract">arXiv:2309.12968</a> (replaced) [<a href="/pdf/2309.12968" title="Download PDF">pdf</a>, <a href="/format/2309.12968" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PassViz: A Visualisation System for Analysing Leaked Passwords
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Parker%2C+S">Sam Parker</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Haiyue Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shujun Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Please cite this paper as follows: Sam Parker, Haiyue Yuan and Shujun Li (2023) PassViz: An Interactive Visualisation System for Analysing Leaked Passwords. Proceedings of the 2023 20th IEEE Symposium on Visualization for Cyber Security (VizSec 2023), pp.33-42, IEEE, doi: 10.1109/VizSec60606.<a href="/abs/2023.00011">2023.00011</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item1030">[1030]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13443" title="Abstract">arXiv:2309.13443</a> (replaced) [<a href="/pdf/2309.13443" title="Download PDF">pdf</a>, <a href="/format/2309.13443" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Early-Exit with Class Exclusion for Efficient Inference of Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingcun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bing Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G+L">Grace Li Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1031">[1031]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13598" title="Abstract">arXiv:2309.13598</a> (replaced) [<a href="/pdf/2309.13598" title="Download PDF">pdf</a>, <a href="/format/2309.13598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Posterior Distribution in Denoising: Application to Uncertainty  Quantification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manor%2C+H">Hila Manor</a>, 
<a href="/search/cs?searchtype=author&query=Michaeli%2C+T">Tomer Michaeli</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for ICLR 2024. Code and examples are available on the project webpage in <a href="https://hilamanor.github.io/GaussianDenoisingPosterior/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1032">[1032]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.13893" title="Abstract">arXiv:2309.13893</a> (replaced) [<a href="/pdf/2309.13893" title="Download PDF">pdf</a>, <a href="/format/2309.13893" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Scene Informer: Anchor-based Occlusion Inference and Trajectory  Prediction in Partially Observable Environments
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lange%2C+B">Bernard Lange</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiachen Li</a>, 
<a href="/search/cs?searchtype=author&query=Kochenderfer%2C+M+J">Mykel J. Kochenderfer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to 2024 IEEE International Conference on Robotics and Automation (ICRA)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1033">[1033]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.14277" title="Abstract">arXiv:2309.14277</a> (replaced) [<a href="/pdf/2309.14277" title="Download PDF">pdf</a>, <a href="/format/2309.14277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SINCERE: Supervised Information Noise-Contrastive Estimation REvisited
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feeney%2C+P">Patrick Feeney</a>, 
<a href="/search/cs?searchtype=author&query=Hughes%2C+M+C">Michael C. Hughes</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1034">[1034]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.16808" title="Abstract">arXiv:2309.16808</a> (replaced) [<a href="/pdf/2309.16808" title="Download PDF">pdf</a>, <a href="/format/2309.16808" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Granularity at Scale: Estimating Neighborhood Socioeconomic Indicators  from High-Resolution Orthographic Imagery and Hybrid Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brewer%2C+E">Ethan Brewer</a>, 
<a href="/search/cs?searchtype=author&query=Valdrighi%2C+G">Giovani Valdrighi</a>, 
<a href="/search/cs?searchtype=author&query=Solunke%2C+P">Parikshit Solunke</a>, 
<a href="/search/cs?searchtype=author&query=Rulff%2C+J">Joao Rulff</a>, 
<a href="/search/cs?searchtype=author&query=Piadyk%2C+Y">Yurii Piadyk</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+Z">Zhonghui Lv</a>, 
<a href="/search/cs?searchtype=author&query=Poco%2C+J">Jorge Poco</a>, 
<a href="/search/cs?searchtype=author&query=Silva%2C+C">Claudio Silva</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Updated after acceptance to IEEE J-STARS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1035">[1035]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2309.17055" title="Abstract">arXiv:2309.17055</a> (replaced) [<a href="/pdf/2309.17055" title="Download PDF">pdf</a>, <a href="/format/2309.17055" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Application Driven Method for Assembling Numerical Schemes for the  Solution of Complex Multiphysics Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Zimbrod%2C+P">Patrick Zimbrod</a>, 
<a href="/search/math?searchtype=author&query=Fleck%2C+M">Michael Fleck</a>, 
<a href="/search/math?searchtype=author&query=Schilp%2C+J">Johannes Schilp</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Computational Engineering, Finance, and Science (cs.CE)

</div>
</div>
</dd>
<dt><a name="item1036">[1036]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00492" title="Abstract">arXiv:2310.00492</a> (replaced) [<a href="/pdf/2310.00492" title="Download PDF">pdf</a>, <a href="/format/2310.00492" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Language Modeling to Instruction Following: Understanding the  Behavior Shift in LLMs after Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xuansheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+W">Wenlin Yao</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianshu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xiaoman Pan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xiaoyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Ninghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dong Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 28 pages, 14 figures, 13 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1037">[1037]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00558" title="Abstract">arXiv:2310.00558</a> (replaced) [<a href="/pdf/2310.00558" title="Download PDF">pdf</a>, <a href="/format/2310.00558" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diving into the Depths of Spotting Text in Multi-Domain Noisy Scenes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Das%2C+A">Alloy Das</a>, 
<a href="/search/cs?searchtype=author&query=Biswas%2C+S">Sanket Biswas</a>, 
<a href="/search/cs?searchtype=author&query=Pal%2C+U">Umapada Pal</a>, 
<a href="/search/cs?searchtype=author&query=Llad%C3%B3s%2C+J">Josep Llad&#xf3;s</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1038">[1038]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00566" title="Abstract">arXiv:2310.00566</a> (replaced) [<a href="/pdf/2310.00566" title="Download PDF">pdf</a>, <a href="/format/2310.00566" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Empowering Many, Biasing a Few: Generalist Credit Scoring through Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+D">Duanyu Feng</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yongfu Dai</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jimin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qianqian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+W">Weiguang Han</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhengyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lopez-Lira%2C+A">Alejandro Lopez-Lira</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1039">[1039]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.00797" title="Abstract">arXiv:2310.00797</a> (replaced) [<a href="/pdf/2310.00797" title="Download PDF">pdf</a>, <a href="/format/2310.00797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Going Beyond Familiar Features for Deep Anomaly Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sivaprasad%2C+S">Sarath Sivaprasad</a>, 
<a href="/search/cs?searchtype=author&query=Fritz%2C+M">Mario Fritz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1040">[1040]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01407" title="Abstract">arXiv:2310.01407</a> (replaced) [<a href="/pdf/2310.01407" title="Download PDF">pdf</a>, <a href="/format/2310.01407" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CoDi: Conditional Diffusion Distillation for Higher-Fidelity and Faster  Image Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mei%2C+K">Kangfu Mei</a>, 
<a href="/search/cs?searchtype=author&query=Delbracio%2C+M">Mauricio Delbracio</a>, 
<a href="/search/cs?searchtype=author&query=Talebi%2C+H">Hossein Talebi</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Z">Zhengzhong Tu</a>, 
<a href="/search/cs?searchtype=author&query=Patel%2C+V+M">Vishal M. Patel</a>, 
<a href="/search/cs?searchtype=author&query=Milanfar%2C+P">Peyman Milanfar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1041">[1041]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.01545" title="Abstract">arXiv:2310.01545</a> (replaced) [<a href="/pdf/2310.01545" title="Download PDF">pdf</a>, <a href="/format/2310.01545" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RF-ULM: Ultrasound Localization Microscopy Learned from Radio-Frequency  Wavefronts
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hahne%2C+C">Christopher Hahne</a>, 
<a href="/search/cs?searchtype=author&query=Chabouh%2C+G">Georges Chabouh</a>, 
<a href="/search/cs?searchtype=author&query=Chavignon%2C+A">Arthur Chavignon</a>, 
<a href="/search/cs?searchtype=author&query=Couture%2C+O">Olivier Couture</a>, 
<a href="/search/cs?searchtype=author&query=Sznitman%2C+R">Raphael Sznitman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Geometry (cs.CG)</span>; Computer Vision and Pattern Recognition (cs.CV); Medical Physics (physics.med-ph)

</div>
</div>
</dd>
<dt><a name="item1042">[1042]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02250" title="Abstract">arXiv:2310.02250</a> (replaced) [<a href="/pdf/2310.02250" title="Download PDF">pdf</a>, <a href="/format/2310.02250" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Why should autoencoders work?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kvalheim%2C+M+D">Matthew D. Kvalheim</a>, 
<a href="/search/cs?searchtype=author&query=Sontag%2C+E+D">Eduardo D. Sontag</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 9 figures; version 3 is accepted for publication in Transactions on Machine Learning Research (TMLR)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1043">[1043]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02277" title="Abstract">arXiv:2310.02277</a> (replaced) [<a href="/pdf/2310.02277" title="Download PDF">pdf</a>, <a href="/format/2310.02277" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Pruning Small Pre-Trained Weights Irreversibly and Monotonically Impairs  &quot;Difficult&quot; Downstream Tasks in LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+L">Lu Yin</a>, 
<a href="/search/cs?searchtype=author&query=Jaiswal%2C+A">Ajay Jaiswal</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shiwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Kundu%2C+S">Souvik Kundu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1044">[1044]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.02953" title="Abstract">arXiv:2310.02953</a> (replaced) [<a href="/pdf/2310.02953" title="Download PDF">pdf</a>, <a href="/format/2310.02953" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> JsonTuning: Towards Generalizable, Robust, and Controllable Instruction  Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gao%2C+C">Chang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenxuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guizhen Chen</a>, 
<a href="/search/cs?searchtype=author&query=Lam%2C+W">Wai Lam</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1045">[1045]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03252" title="Abstract">arXiv:2310.03252</a> (replaced) [<a href="/pdf/2310.03252" title="Download PDF">pdf</a>, <a href="/ps/2310.03252" title="Download PostScript">ps</a>, <a href="/format/2310.03252" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Exploring age-related patterns in internet access: Insights from a  secondary analysis of New Zealand survey data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pacheco%2C+E">Edgar Pacheco</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 5 tables
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Media Peripheries 2024 volume 18 issue 1 pages 38-56
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item1046">[1046]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.03976" title="Abstract">arXiv:2310.03976</a> (replaced) [<a href="/pdf/2310.03976" title="Download PDF">pdf</a>, <a href="/ps/2310.03976" title="Download PostScript">ps</a>, <a href="/format/2310.03976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> From Text to Self: Users&#x27; Perceptions of Potential of AI on  Interpersonal Communication and Self
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+Y">Yue Fu</a>, 
<a href="/search/cs?searchtype=author&query=Foell%2C+S">Sami Foell</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xuhai Xu</a>, 
<a href="/search/cs?searchtype=author&query=Hiniker%2C+A">Alexis Hiniker</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of the CHI Conference on Human Factors in Computing
  Systems (CHI 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>

</div>
</div>
</dd>
<dt><a name="item1047">[1047]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.04910" title="Abstract">arXiv:2310.04910</a> (replaced) [<a href="/pdf/2310.04910" title="Download PDF">pdf</a>, <a href="/format/2310.04910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Faithful Knowledge Graph Explanations for Commonsense Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhai%2C+W">Weihe Zhai</a>, 
<a href="/search/cs?searchtype=author&query=Zubiaga%2C+A">Arkaitz Zubiaga</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bingquan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+C">Chengjie Sun</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yalong Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1048">[1048]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05167" title="Abstract">arXiv:2310.05167</a> (replaced) [<a href="/pdf/2310.05167" title="Download PDF">pdf</a>, <a href="/format/2310.05167" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hieros: Hierarchical Imagination on Structured State Space Sequence  World Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mattes%2C+P">Paul Mattes</a>, 
<a href="/search/cs?searchtype=author&query=Schlosser%2C+R">Rainer Schlosser</a>, 
<a href="/search/cs?searchtype=author&query=Herbrich%2C+R">Ralf Herbrich</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to ICML 2024, 23 pages, 11 figures, code available at: <a href="https://github.com/Snagnar/Hieros">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1049">[1049]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05175" title="Abstract">arXiv:2310.05175</a> (replaced) [<a href="/pdf/2310.05175" title="Download PDF">pdf</a>, <a href="/format/2310.05175" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for  Pruning LLMs to High Sparsity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+L">Lu Yin</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">You Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhenyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+C">Cheng-Yu Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yaqing Wang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+Y">Yiling Jia</a>, 
<a href="/search/cs?searchtype=author&query=Pechenizkiy%2C+M">Mykola Pechenizkiy</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+Y">Yi Liang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Shiwei Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1050">[1050]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05401" title="Abstract">arXiv:2310.05401</a> (replaced) [<a href="/pdf/2310.05401" title="Download PDF">pdf</a>, <a href="/format/2310.05401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Entropy-MCMC: Sampling from Flat Basins with Ease
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+B">Bolian Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruqi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1051">[1051]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05518" title="Abstract">arXiv:2310.05518</a> (replaced) [<a href="/pdf/2310.05518" title="Download PDF">pdf</a>, <a href="/format/2310.05518" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Double Descent in Reinforcement Learning with LSTD and Random  Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Brellmann%2C+D">David Brellmann</a>, 
<a href="/search/cs?searchtype=author&query=Berthier%2C+E">Elo&#xef;se Berthier</a>, 
<a href="/search/cs?searchtype=author&query=Filliat%2C+D">David Filliat</a>, 
<a href="/search/cs?searchtype=author&query=Frehse%2C+G">Goran Frehse</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1052">[1052]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.05620" title="Abstract">arXiv:2310.05620</a> (replaced) [<a href="/pdf/2310.05620" title="Download PDF">pdf</a>, <a href="/format/2310.05620" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LAiW: A Chinese Legal Large Language Models Benchmark
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yongfu Dai</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+D">Duanyu Feng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jimin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jia%2C+H">Haochen Jia</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Q">Qianqian Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yifang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+W">Weiguang Han</a>, 
<a href="/search/cs?searchtype=author&query=Tian%2C+W">Wei Tian</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Hao Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1053">[1053]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06452" title="Abstract">arXiv:2310.06452</a> (replaced) [<a href="/pdf/2310.06452" title="Download PDF">pdf</a>, <a href="/format/2310.06452" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding the Effects of RLHF on LLM Generalisation and Diversity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kirk%2C+R">Robert Kirk</a>, 
<a href="/search/cs?searchtype=author&query=Mediratta%2C+I">Ishita Mediratta</a>, 
<a href="/search/cs?searchtype=author&query=Nalmpantis%2C+C">Christoforos Nalmpantis</a>, 
<a href="/search/cs?searchtype=author&query=Luketina%2C+J">Jelena Luketina</a>, 
<a href="/search/cs?searchtype=author&query=Hambro%2C+E">Eric Hambro</a>, 
<a href="/search/cs?searchtype=author&query=Grefenstette%2C+E">Edward Grefenstette</a>, 
<a href="/search/cs?searchtype=author&query=Raileanu%2C+R">Roberta Raileanu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Code available here: <a href="https://github.com/facebookresearch/rlfh-gen-div">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1054">[1054]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.06549" title="Abstract">arXiv:2310.06549</a> (replaced) [<a href="/pdf/2310.06549" title="Download PDF">pdf</a>, <a href="/format/2310.06549" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield  but Also a Catalyst for Model Inversion Attacks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Struppek%2C+L">Lukas Struppek</a>, 
<a href="/search/cs?searchtype=author&query=Hintersdorf%2C+D">Dominik Hintersdorf</a>, 
<a href="/search/cs?searchtype=author&query=Kersting%2C+K">Kristian Kersting</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1055">[1055]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.07592" title="Abstract">arXiv:2310.07592</a> (replaced) [<a href="/pdf/2310.07592" title="Download PDF">pdf</a>, <a href="/format/2310.07592" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transformers for Green Semantic Communication: Less Energy, More  Semantics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+S">Shubhabrata Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Beard%2C+C">Cory Beard</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Sejun Song</a> (School of Science and Engineering, University of Missouri-Kansas City, Kansas City, MO, USA)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First revision, Version 2
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item1056">[1056]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.09336" title="Abstract">arXiv:2310.09336</a> (replaced) [<a href="/pdf/2310.09336" title="Download PDF">pdf</a>, <a href="/format/2310.09336" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Compositional Abilities Emerge Multiplicatively: Exploring Diffusion  Models on a Synthetic Task
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Okawa%2C+M">Maya Okawa</a>, 
<a href="/search/cs?searchtype=author&query=Lubana%2C+E+S">Ekdeep Singh Lubana</a>, 
<a href="/search/cs?searchtype=author&query=Dick%2C+R+P">Robert P. Dick</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+H">Hidenori Tanaka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 37th Conference on Neural Information Processing Systems (NeurIPS)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1057">[1057]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.10683" title="Abstract">arXiv:2310.10683</a> (replaced) [<a href="/pdf/2310.10683" title="Download PDF">pdf</a>, <a href="/format/2310.10683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large Language Model Unlearning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yao%2C+Y">Yuanshun Yao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+X">Xiaojun Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yang Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1058">[1058]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11122" title="Abstract">arXiv:2310.11122</a> (replaced) [<a href="/pdf/2310.11122" title="Download PDF">pdf</a>, <a href="/format/2310.11122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sensitivity-Aware Amortized Bayesian Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Elsem%C3%BCller%2C+L">Lasse Elsem&#xfc;ller</a>, 
<a href="/search/stat?searchtype=author&query=Olischl%C3%A4ger%2C+H">Hans Olischl&#xe4;ger</a>, 
<a href="/search/stat?searchtype=author&query=Schmitt%2C+M">Marvin Schmitt</a>, 
<a href="/search/stat?searchtype=author&query=B%C3%BCrkner%2C+P">Paul-Christian B&#xfc;rkner</a>, 
<a href="/search/stat?searchtype=author&query=K%C3%B6the%2C+U">Ullrich K&#xf6;the</a>, 
<a href="/search/stat?searchtype=author&query=Radev%2C+S+T">Stefan T. Radev</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item1059">[1059]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11730" title="Abstract">arXiv:2310.11730</a> (replaced) [<a href="/pdf/2310.11730" title="Download PDF">pdf</a>, <a href="/format/2310.11730" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Federated Heterogeneous Graph Neural Network for Privacy-preserving  Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+B">Bo Yan</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y">Yang Cao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wenchuan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+J">Junping Du</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+C">Chuan Shi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Distributed, Parallel, and Cluster Computing (cs.DC)

</div>
</div>
</dd>
<dt><a name="item1060">[1060]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11840" title="Abstract">arXiv:2310.11840</a> (replaced) [<a href="/pdf/2310.11840" title="Download PDF">pdf</a>, <a href="/format/2310.11840" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On The Expressivity of Objective-Specification Formalisms in  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Subramani%2C+R">Rohan Subramani</a>, 
<a href="/search/cs?searchtype=author&query=Williams%2C+M">Marcus Williams</a>, 
<a href="/search/cs?searchtype=author&query=Heitmann%2C+M">Max Heitmann</a>, 
<a href="/search/cs?searchtype=author&query=Holm%2C+H">Halfdan Holm</a>, 
<a href="/search/cs?searchtype=author&query=Griffin%2C+C">Charlie Griffin</a>, 
<a href="/search/cs?searchtype=author&query=Skalse%2C+J">Joar Skalse</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Published as a conference paper at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1061">[1061]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.11897" title="Abstract">arXiv:2310.11897</a> (replaced) [<a href="/pdf/2310.11897" title="Download PDF">pdf</a>, <a href="/format/2310.11897" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerated Policy Gradient: On the Convergence Rates of the Nesterov  Momentum for Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yen-Ju Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+N">Nai-Chieh Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+P">Ping-Chun Hsieh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 54 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1062">[1062]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12081" title="Abstract">arXiv:2310.12081</a> (replaced) [<a href="/pdf/2310.12081" title="Download PDF">pdf</a>, <a href="/format/2310.12081" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Graph Matching Using An Unbalanced Hierarchical Optimal Transport  Framework
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Haoran Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+D">Dixin Luo</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongteng Xu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1063">[1063]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.12508" title="Abstract">arXiv:2310.12508</a> (replaced) [<a href="/pdf/2310.12508" title="Download PDF">pdf</a>, <a href="/format/2310.12508" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency  in Both Image Classification and Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+C">Chongyu Fan</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiancheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yihua Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+D">Dennis Wei</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+E">Eric Wong</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Sijia Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICLR 2024 as a Spotlight paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1064">[1064]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13505" title="Abstract">arXiv:2310.13505</a> (replaced) [<a href="/pdf/2310.13505" title="Download PDF">pdf</a>, <a href="/format/2310.13505" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Training for Conversational Question Answering Models with  Reinforced Reformulation Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kaiser%2C+M">Magdalena Kaiser</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+R+S">Rishiraj Saha Roy</a>, 
<a href="/search/cs?searchtype=author&query=Weikum%2C+G">Gerhard Weikum</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WSDM 2024 Research Paper, 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item1065">[1065]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.13828" title="Abstract">arXiv:2310.13828</a> (replaced) [<a href="/pdf/2310.13828" title="Download PDF">pdf</a>, <a href="/format/2310.13828" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shan%2C+S">Shawn Shan</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+W">Wenxin Ding</a>, 
<a href="/search/cs?searchtype=author&query=Passananti%2C+J">Josephine Passananti</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+S">Stanley Wu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+H">Haitao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B+Y">Ben Y. Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1066">[1066]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.14550" title="Abstract">arXiv:2310.14550</a> (replaced) [<a href="/pdf/2310.14550" title="Download PDF">pdf</a>, <a href="/format/2310.14550" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Corruption-Robust Offline Reinforcement Learning with General Function  Approximation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+C">Chenlu Ye</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Rui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+Q">Quanquan Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+T">Tong Zhang</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Neurips 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1067">[1067]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15458" title="Abstract">arXiv:2310.15458</a> (replaced) [<a href="/pdf/2310.15458" title="Download PDF">pdf</a>, <a href="/format/2310.15458" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An O(N) distributed-memory parallel direct solver for planar integral  equations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Liang%2C+T">Tianyu Liang</a>, 
<a href="/search/math?searchtype=author&query=Chen%2C+C">Chao Chen</a>, 
<a href="/search/math?searchtype=author&query=Martinsson%2C+P">Per-Gunnar Martinsson</a>, 
<a href="/search/math?searchtype=author&query=Biros%2C+G">George Biros</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1068">[1068]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.15801" title="Abstract">arXiv:2310.15801</a> (replaced) [<a href="/pdf/2310.15801" title="Download PDF">pdf</a>, <a href="/format/2310.15801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Generalized Adjusted Min-Sum Decoder for 5G LDPC Codes: Algorithm and  Implementation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+Y">Yuqing Ren</a>, 
<a href="/search/cs?searchtype=author&query=Harb%2C+H">Hassan Harb</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yifei Shen</a>, 
<a href="/search/cs?searchtype=author&query=Balatsoukas-Stimming%2C+A">Alexios Balatsoukas-Stimming</a>, 
<a href="/search/cs?searchtype=author&query=Burg%2C+A">Andreas Burg</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 15 figures, accepted by IEEE Transactions on Circuits and Systems I: Regular Paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>; Hardware Architecture (cs.AR); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1069">[1069]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17953" title="Abstract">arXiv:2310.17953</a> (replaced) [<a href="/pdf/2310.17953" title="Download PDF">pdf</a>, <a href="/format/2310.17953" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MCE: Mixed Cantonese and English Audio Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+P">Peng Xie</a>, 
<a href="/search/cs?searchtype=author&query=Xin%2C+Z">Zihao Xin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shengjun Huang</a>, 
<a href="/search/cs?searchtype=author&query=Chan%2C+T+W">Tsz Wai Chan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kani Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Computation and Language (cs.CL); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1070">[1070]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.17976" title="Abstract">arXiv:2310.17976</a> (replaced) [<a href="/pdf/2310.17976" title="Download PDF">pdf</a>, <a href="/format/2310.17976" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InCharacter: Evaluating Personality Fidelity in Role-Playing Agents  through Psychological Interviews
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xintao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yunze Xiao</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jen-tse Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Siyu Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R">Rui Xu</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+H">Haoran Guo</a>, 
<a href="/search/cs?searchtype=author&query=Tu%2C+Q">Quan Tu</a>, 
<a href="/search/cs?searchtype=author&query=Fei%2C+Y">Yaying Fei</a>, 
<a href="/search/cs?searchtype=author&query=Leng%2C+Z">Ziang Leng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiangjie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Cheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+Y">Yanghua Xiao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1071">[1071]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.19796" title="Abstract">arXiv:2310.19796</a> (replaced) [<a href="/pdf/2310.19796" title="Download PDF">pdf</a>, <a href="/format/2310.19796" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Re-evaluating Retrosynthesis Algorithms with Syntheseus
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Maziarz%2C+K">Krzysztof Maziarz</a>, 
<a href="/search/cs?searchtype=author&query=Tripp%2C+A">Austin Tripp</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Guoqing Liu</a>, 
<a href="/search/cs?searchtype=author&query=Stanley%2C+M">Megan Stanley</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+S">Shufang Xie</a>, 
<a href="/search/cs?searchtype=author&query=Gai%C5%84ski%2C+P">Piotr Gai&#x144;ski</a>, 
<a href="/search/cs?searchtype=author&query=Seidl%2C+P">Philipp Seidl</a>, 
<a href="/search/cs?searchtype=author&query=Segler%2C+M">Marwin Segler</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item1072">[1072]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2310.20266" title="Abstract">arXiv:2310.20266</a> (replaced) [<a href="/pdf/2310.20266" title="Download PDF">pdf</a>, <a href="/format/2310.20266" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Average Return in Markov Decision Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Marthe%2C+A">Alexandre Marthe</a> (ENS de Lyon, UMPA-ENSL), 
<a href="/search/cs?searchtype=author&query=Garivier%2C+A">Aur&#xe9;lien Garivier</a> (UMPA-ENSL (MC2)), 
<a href="/search/cs?searchtype=author&query=Vernade%2C+C">Claire Vernade</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Neurips 2023, Dec 2023, New Orleans, United States
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Optimization and Control (math.OC); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item1073">[1073]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.00233" title="Abstract">arXiv:2311.00233</a> (replaced) [<a href="/pdf/2311.00233" title="Download PDF">pdf</a>, <a href="/format/2311.00233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instructive Decoding: Instruction-Tuned Large Language Models are  Self-Refiner from Noisy Instructions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kim%2C+T">Taehyeon Kim</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+J">Joonkee Kim</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+G">Gihun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Yun%2C+S">Se-Young Yun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICLR 2024 (Spotlight)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1074">[1074]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01450" title="Abstract">arXiv:2311.01450</a> (replaced) [<a href="/pdf/2311.01450" title="Download PDF">pdf</a>, <a href="/format/2311.01450" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DreamSmooth: Improving Model-based Reinforcement Learning via Reward  Smoothing
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+V">Vint Lee</a>, 
<a href="/search/cs?searchtype=author&query=Abbeel%2C+P">Pieter Abbeel</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Youngwoon Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> For code and website, see <a href="https://vint-1.github.io/dreamsmooth/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1075">[1075]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01602" title="Abstract">arXiv:2311.01602</a> (replaced) [<a href="/pdf/2311.01602" title="Download PDF">pdf</a>, <a href="/format/2311.01602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DRNet: A Decision-Making Method for Autonomous Lane Changingwith Deep  Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+K">Kunpeng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+L">Lifei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shengrui Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1076">[1076]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.01740" title="Abstract">arXiv:2311.01740</a> (replaced) [<a href="/pdf/2311.01740" title="Download PDF">pdf</a>, <a href="/format/2311.01740" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAC3: Reliable Hallucination Detection in Black-Box Language Models via  Semantic-aware Cross-check Consistency
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jiaxin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhuohang Li</a>, 
<a href="/search/cs?searchtype=author&query=Das%2C+K">Kamalika Das</a>, 
<a href="/search/cs?searchtype=author&query=Malin%2C+B+A">Bradley A. Malin</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sricharan Kumar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> EMNLP 2023
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1077">[1077]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02010" title="Abstract">arXiv:2311.02010</a> (replaced) [<a href="/pdf/2311.02010" title="Download PDF">pdf</a>, <a href="/format/2311.02010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A cast of thousands: How the IDEAS Productivity project has advanced  software productivity and sustainability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=McInnes%2C+L+C">Lois Curfman McInnes</a>, 
<a href="/search/cs?searchtype=author&query=Heroux%2C+M">Michael Heroux</a>, 
<a href="/search/cs?searchtype=author&query=Bernholdt%2C+D+E">David E. Bernholdt</a>, 
<a href="/search/cs?searchtype=author&query=Dubey%2C+A">Anshu Dubey</a>, 
<a href="/search/cs?searchtype=author&query=Gonsiorowski%2C+E">Elsa Gonsiorowski</a>, 
<a href="/search/cs?searchtype=author&query=Gupta%2C+R">Rinku Gupta</a>, 
<a href="/search/cs?searchtype=author&query=Marques%2C+O">Osni Marques</a>, 
<a href="/search/cs?searchtype=author&query=Moulton%2C+J+D">J. David Moulton</a>, 
<a href="/search/cs?searchtype=author&query=Nam%2C+H+A">Hai Ah Nam</a>, 
<a href="/search/cs?searchtype=author&query=Norris%2C+B">Boyana Norris</a>, 
<a href="/search/cs?searchtype=author&query=Raybourn%2C+E+M">Elaine M. Raybourn</a>, 
<a href="/search/cs?searchtype=author&query=Willenbring%2C+J">Jim Willenbring</a>, 
<a href="/search/cs?searchtype=author&query=Almgren%2C+A">Ann Almgren</a>, 
<a href="/search/cs?searchtype=author&query=Bartlett%2C+R">Ross Bartlett</a>, 
<a href="/search/cs?searchtype=author&query=Cranfill%2C+K">Kita Cranfill</a>, 
<a href="/search/cs?searchtype=author&query=Fickas%2C+S">Stephen Fickas</a>, 
<a href="/search/cs?searchtype=author&query=Frederick%2C+D">Don Frederick</a>, 
<a href="/search/cs?searchtype=author&query=Godoy%2C+W">William Godoy</a>, 
<a href="/search/cs?searchtype=author&query=Grubel%2C+P">Patricia Grubel</a>, 
<a href="/search/cs?searchtype=author&query=Hartman-Baker%2C+R">Rebecca Hartman-Baker</a>, 
<a href="/search/cs?searchtype=author&query=Huebl%2C+A">Axel Huebl</a>, 
<a href="/search/cs?searchtype=author&query=Lynch%2C+R">Rose Lynch</a>, 
<a href="/search/cs?searchtype=author&query=Thakur%2C+A+M">Addi Malviya Thakur</a>, 
<a href="/search/cs?searchtype=author&query=Milewicz%2C+R">Reed Milewicz</a>, 
<a href="/search/cs?searchtype=author&query=Miller%2C+M+C">Mark C. Miller</a>, 
<a href="/search/cs?searchtype=author&query=Mundt%2C+M">Miranda Mundt</a>, 
<a href="/search/cs?searchtype=author&query=Palmer%2C+E">Erik Palmer</a>, 
<a href="/search/cs?searchtype=author&query=Parete-Koon%2C+S">Suzanne Parete-Koon</a>, 
<a href="/search/cs?searchtype=author&query=Phinney%2C+M">Megan Phinney</a>, 
<a href="/search/cs?searchtype=author&query=Riley%2C+K">Katherine Riley</a>, 
<a href="/search/cs?searchtype=author&query=Rogers%2C+D+M">David M. Rogers</a>, 
<a href="/search/cs?searchtype=author&query=Sims%2C+B">Ben Sims</a>, 
<a href="/search/cs?searchtype=author&query=Stevens%2C+D">Deborah Stevens</a>, 
<a href="/search/cs?searchtype=author&query=Watson%2C+G+R">Gregory R. Watson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item1078">[1078]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.02847" title="Abstract">arXiv:2311.02847</a> (replaced) [<a href="/pdf/2311.02847" title="Download PDF">pdf</a>, <a href="/format/2311.02847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kinematic-aware Prompting for Generalizable Articulated Object  Manipulation with LLMs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xia%2C+W">Wenke Xia</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Dong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+X">Xincheng Pang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhigang Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+B">Bin Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+D">Di Hu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuelong Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1079">[1079]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.04256" title="Abstract">arXiv:2311.04256</a> (replaced) [<a href="/pdf/2311.04256" title="Download PDF">pdf</a>, <a href="/ps/2311.04256" title="Download PostScript">ps</a>, <a href="/format/2311.04256" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Foundational theories of hesitant fuzzy sets and hesitant fuzzy  information systems and their applications for multi-strength intelligent  classifiers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lu%2C+S">Shizhan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zeshui Xu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+Z">Zhu Fu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 25 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Information Theory (cs.IT); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1080">[1080]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06318" title="Abstract">arXiv:2311.06318</a> (replaced) [<a href="/pdf/2311.06318" title="Download PDF">pdf</a>, <a href="/format/2311.06318" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge-Augmented Large Language Models for Personalized Contextual  Query Suggestion
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baek%2C+J">Jinheon Baek</a>, 
<a href="/search/cs?searchtype=author&query=Chandrasekaran%2C+N">Nirupama Chandrasekaran</a>, 
<a href="/search/cs?searchtype=author&query=Cucerzan%2C+S">Silviu Cucerzan</a>, 
<a href="/search/cs?searchtype=author&query=herring%2C+A">Allen herring</a>, 
<a href="/search/cs?searchtype=author&query=Jauhar%2C+S+K">Sujay Kumar Jauhar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The Web Conference (WWW) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1081">[1081]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.06835" title="Abstract">arXiv:2311.06835</a> (replaced) [<a href="/pdf/2311.06835" title="Download PDF">pdf</a>, <a href="/format/2311.06835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open-Set Graph Anomaly Detection via Normal Structure Regularisation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Q">Qizhou Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pang%2C+G">Guansong Pang</a>, 
<a href="/search/cs?searchtype=author&query=Salehi%2C+M">Mahsa Salehi</a>, 
<a href="/search/cs?searchtype=author&query=Leckie%2C+C">Christopher Leckie</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item1082">[1082]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07532" title="Abstract">arXiv:2311.07532</a> (replaced) [<a href="/pdf/2311.07532" title="Download PDF">pdf</a>, <a href="/format/2311.07532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> It&#x27;s Not Easy Being Wrong: Large Language Models Struggle with Process  of Elimination Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Balepur%2C+N">Nishant Balepur</a>, 
<a href="/search/cs?searchtype=author&query=Palta%2C+S">Shramay Palta</a>, 
<a href="/search/cs?searchtype=author&query=Rudinger%2C+R">Rachel Rudinger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In progress preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1083">[1083]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07568" title="Abstract">arXiv:2311.07568</a> (replaced) [<a href="/pdf/2311.07568" title="Download PDF">pdf</a>, <a href="/format/2311.07568" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Feature emergence via margin maximization: case studies in algebraic  tasks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Morwani%2C+D">Depen Morwani</a>, 
<a href="/search/cs?searchtype=author&query=Edelman%2C+B+L">Benjamin L. Edelman</a>, 
<a href="/search/cs?searchtype=author&query=Oncescu%2C+C">Costin-Andrei Oncescu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Rosie Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Kakade%2C+S">Sham Kakade</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted as Spotlight at ICLR 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1084">[1084]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07632" title="Abstract">arXiv:2311.07632</a> (replaced) [<a href="/pdf/2311.07632" title="Download PDF">pdf</a>, <a href="/format/2311.07632" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ResMGCN: Residual Message Graph Convolution Network for Fast Biomedical  Interactions Discovering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yin%2C+Z">Zecheng Yin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Molecular Networks (q-bio.MN)

</div>
</div>
</dd>
<dt><a name="item1085">[1085]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.07879" title="Abstract">arXiv:2311.07879</a> (replaced) [<a href="/pdf/2311.07879" title="Download PDF">pdf</a>, <a href="/format/2311.07879" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting  Volunteer Content Moderators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+Y+T">Yang Trista Cao</a>, 
<a href="/search/cs?searchtype=author&query=Domingo%2C+L">Lovely-Frances Domingo</a>, 
<a href="/search/cs?searchtype=author&query=Gilbert%2C+S+A">Sarah Ann Gilbert</a>, 
<a href="/search/cs?searchtype=author&query=Mazurek%2C+M">Michelle Mazurek</a>, 
<a href="/search/cs?searchtype=author&query=Shilton%2C+K">Katie Shilton</a>, 
<a href="/search/cs?searchtype=author&query=Daum%C3%A9%2C+H">Hal Daum&#xe9; III</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1086">[1086]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08045" title="Abstract">arXiv:2311.08045</a> (replaced) [<a href="/pdf/2311.08045" title="Download PDF">pdf</a>, <a href="/format/2311.08045" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Preference Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+P">Pengyu Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yifan Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jian Li</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yong Dai</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+T">Tianhao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+P">Peixin Cao</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+N">Nan Du</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In process
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1087">[1087]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08154" title="Abstract">arXiv:2311.08154</a> (replaced) [<a href="/pdf/2311.08154" title="Download PDF">pdf</a>, <a href="/format/2311.08154" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Ask One More Time: Self-Agreement Improves Reasoning of Language Models  in (Almost) All Scenarios
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+L">Lei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+J">Jiayi Fu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Pengli Liu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Q">Qingyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yan Gong</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+J">Junchen Wan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+F">Fuzheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhongyuan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Di Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gai%2C+K">Kun Gai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1088">[1088]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08268" title="Abstract">arXiv:2311.08268</a> (replaced) [<a href="/pdf/2311.08268" title="Download PDF">pdf</a>, <a href="/format/2311.08268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Wolf in Sheep&#x27;s Clothing: Generalized Nested Jailbreak Prompts can  Fool Large Language Models Easily
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ding%2C+P">Peng Ding</a>, 
<a href="/search/cs?searchtype=author&query=Kuang%2C+J">Jun Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+D">Dan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+X">Xuezhi Cao</a>, 
<a href="/search/cs?searchtype=author&query=Xian%2C+Y">Yunsen Xian</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiajun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shujian Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Pre-print, code is available at <a href="https://github.com/NJUNLP/ReNeLLM">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1089">[1089]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08598" title="Abstract">arXiv:2311.08598</a> (replaced) [<a href="/pdf/2311.08598" title="Download PDF">pdf</a>, <a href="/format/2311.08598" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DALA: A Distribution-Aware LoRA-Based Adversarial Attack against  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yibo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xiangjue Dong</a>, 
<a href="/search/cs?searchtype=author&query=Caverlee%2C+J">James Caverlee</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+P+S">Philip S. Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> First two authors contribute equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1090">[1090]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.08685" title="Abstract">arXiv:2311.08685</a> (replaced) [<a href="/pdf/2311.08685" title="Download PDF">pdf</a>, <a href="/format/2311.08685" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Safer-Instruct: Aligning Language Models with Automated Preference Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shi%2C+T">Taiwei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+K">Kai Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jieyu Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1091">[1091]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09031" title="Abstract">arXiv:2311.09031</a> (replaced) [<a href="/pdf/2311.09031" title="Download PDF">pdf</a>, <a href="/format/2311.09031" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Integrating Sensing, Communication, and Power Transfer: From Theory to  Practice
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaoyang Li</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zidong Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+G">Guangxu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yuanming Shi</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yi Gong</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qinyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kaibin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Letaief%2C+K+B">Khaled B. Letaief</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> This paper has been accepted by IEEE Communications Magazine
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item1092">[1092]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09090" title="Abstract">arXiv:2311.09090</a> (replaced) [<a href="/pdf/2311.09090" title="Download PDF">pdf</a>, <a href="/format/2311.09090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Social Bias Probing: Fairness Benchmarking for Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Manerba%2C+M+M">Marta Marchiori Manerba</a>, 
<a href="/search/cs?searchtype=author&query=Sta%C5%84czak%2C+K">Karolina Sta&#x144;czak</a>, 
<a href="/search/cs?searchtype=author&query=Guidotti%2C+R">Riccardo Guidotti</a>, 
<a href="/search/cs?searchtype=author&query=Augenstein%2C+I">Isabelle Augenstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1093">[1093]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09136" title="Abstract">arXiv:2311.09136</a> (replaced) [<a href="/pdf/2311.09136" title="Download PDF">pdf</a>, <a href="/format/2311.09136" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rescue: Ranking LLM Responses with Partial Ordering to Improve Response  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yikun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+R">Rui Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haoming Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Gui%2C+T">Tao Gui</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fei Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1094">[1094]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09195" title="Abstract">arXiv:2311.09195</a> (replaced) [<a href="/pdf/2311.09195" title="Download PDF">pdf</a>, <a href="/format/2311.09195" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Supervised Curriculum Generation for Autonomous Reinforcement  Learning without Task-Specific Knowledge
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+S">Sang-Hyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+S">Seung-Woo Seo</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1095">[1095]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09278" title="Abstract">arXiv:2311.09278</a> (replaced) [<a href="/pdf/2311.09278" title="Download PDF">pdf</a>, <a href="/format/2311.09278" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Symbol-LLM: Towards Foundational Symbol-centric Interface For Large  Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+F">Fangzhi Xu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhiyong Wu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Q">Qiushi Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Siyu Ren</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+F">Fei Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Shuai Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Q">Qika Lin</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jun Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 13 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1096">[1096]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09562" title="Abstract">arXiv:2311.09562</a> (replaced) [<a href="/pdf/2311.09562" title="Download PDF">pdf</a>, <a href="/format/2311.09562" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TextEE: Benchmark, Reevaluation, Reflections, and Future Challenges in  Event Extraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+K">Kuan-Hao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hsu%2C+I">I-Hung Hsu</a>, 
<a href="/search/cs?searchtype=author&query=Parekh%2C+T">Tanmay Parekh</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+Z">Zhiyu Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zixuan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Natarajan%2C+P">Premkumar Natarajan</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+K">Kai-Wei Chang</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+N">Nanyun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+H">Heng Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1097">[1097]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09581" title="Abstract">arXiv:2311.09581</a> (replaced) [<a href="/pdf/2311.09581" title="Download PDF">pdf</a>, <a href="/format/2311.09581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DocLens: Multi-aspect Fine-grained Evaluation for Medical Text  Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xie%2C+Y">Yiqing Xie</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+H">Hao Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Pengfei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Gero%2C+Z">Zelalem Gero</a>, 
<a href="/search/cs?searchtype=author&query=Wong%2C+C">Cliff Wong</a>, 
<a href="/search/cs?searchtype=author&query=Naumann%2C+T">Tristan Naumann</a>, 
<a href="/search/cs?searchtype=author&query=Poon%2C+H">Hoifung Poon</a>, 
<a href="/search/cs?searchtype=author&query=Rose%2C+C">Carolyn Rose</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1098">[1098]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09603" title="Abstract">arXiv:2311.09603</a> (replaced) [<a href="/pdf/2311.09603" title="Download PDF">pdf</a>, <a href="/format/2311.09603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Self-Contradictory Reasoning Evaluation and Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Ziyi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+I">Isabelle Lee</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+Y">Yongkang Du</a>, 
<a href="/search/cs?searchtype=author&query=Sanyal%2C+S">Soumya Sanyal</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jieyu Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1099">[1099]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09766" title="Abstract">arXiv:2311.09766</a> (replaced) [<a href="/pdf/2311.09766" title="Download PDF">pdf</a>, <a href="/format/2311.09766" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yiqi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Moosavi%2C+N+S">Nafise Sadat Moosavi</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+C">Chenghua Lin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1100">[1100]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.09799" title="Abstract">arXiv:2311.09799</a> (replaced) [<a href="/pdf/2311.09799" title="Download PDF">pdf</a>, <a href="/format/2311.09799" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Far Can We Extract Diverse Perspectives from Large Language Models?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hayati%2C+S+A">Shirley Anugrah Hayati</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Minhwa Lee</a>, 
<a href="/search/cs?searchtype=author&query=Rajagopal%2C+D">Dheeraj Rajagopal</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+D">Dongyeop Kang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1101">[1101]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10291" title="Abstract">arXiv:2311.10291</a> (replaced) [<a href="/pdf/2311.10291" title="Download PDF">pdf</a>, <a href="/format/2311.10291" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Function Space Aggregation for Federated Learning at Scale
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Dhawan%2C+N">Nikita Dhawan</a>, 
<a href="/search/cs?searchtype=author&query=Mitchell%2C+N">Nicole Mitchell</a>, 
<a href="/search/cs?searchtype=author&query=Charles%2C+Z">Zachary Charles</a>, 
<a href="/search/cs?searchtype=author&query=Garrett%2C+Z">Zachary Garrett</a>, 
<a href="/search/cs?searchtype=author&query=Dziugaite%2C+G+K">Gintare Karolina Dziugaite</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 10 figures. Transactions on Machine Learning Research, 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1102">[1102]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10537" title="Abstract">arXiv:2311.10537</a> (replaced) [<a href="/pdf/2311.10537" title="Download PDF">pdf</a>, <a href="/format/2311.10537" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MedAgents: Large Language Models as Collaborators for Zero-shot Medical  Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiangru Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+A">Anni Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuosheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Ziming Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yilun Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xingyao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Cohan%2C+A">Arman Cohan</a>, 
<a href="/search/cs?searchtype=author&query=Gerstein%2C+M">Mark Gerstein</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1103">[1103]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10801" title="Abstract">arXiv:2311.10801</a> (replaced) [<a href="/pdf/2311.10801" title="Download PDF">pdf</a>, <a href="/format/2311.10801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Learning with Maskable Stock Representation for Portfolio  Management in Customizable Stock Pools
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Zhang%2C+W">Wentao Zhang</a>, 
<a href="/search/q-fin?searchtype=author&query=Zhao%2C+Y">Yilei Zhao</a>, 
<a href="/search/q-fin?searchtype=author&query=Sun%2C+S">Shuo Sun</a>, 
<a href="/search/q-fin?searchtype=author&query=Ying%2C+J">Jie Ying</a>, 
<a href="/search/q-fin?searchtype=author&query=Xie%2C+Y">Yonggang Xie</a>, 
<a href="/search/q-fin?searchtype=author&query=Song%2C+Z">Zitao Song</a>, 
<a href="/search/q-fin?searchtype=author&query=Wang%2C+X">Xinrun Wang</a>, 
<a href="/search/q-fin?searchtype=author&query=An%2C+B">Bo An</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Portfolio Management (q-fin.PM)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1104">[1104]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10902" title="Abstract">arXiv:2311.10902</a> (replaced) [<a href="/pdf/2311.10902" title="Download PDF">pdf</a>, <a href="/format/2311.10902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> OCT2Confocal: 3D CycleGAN based Translation of Retinal OCT Images to  Confocal Microscopy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tian%2C+X">Xin Tian</a>, 
<a href="/search/eess?searchtype=author&query=Anantrasirichai%2C+N">Nantheera Anantrasirichai</a>, 
<a href="/search/eess?searchtype=author&query=Nicholson%2C+L">Lindsay Nicholson</a>, 
<a href="/search/eess?searchtype=author&query=Achim%2C+A">Alin Achim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 4pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1105">[1105]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.10959" title="Abstract">arXiv:2311.10959</a> (replaced) [<a href="/pdf/2311.10959" title="Download PDF">pdf</a>, <a href="/format/2311.10959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structure-Aware Sparse-View X-ray 3D Reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Cai%2C+Y">Yuanhao Cai</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+J">Jiahao Wang</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+Z">Zongwei Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+A">Angtian Wang</a>, 
<a href="/search/eess?searchtype=author&query=Yuille%2C+A">Alan Yuille</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first Transformer-based method for X-ray neural rendering
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1106">[1106]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.11509" title="Abstract">arXiv:2311.11509</a> (replaced) [<a href="/pdf/2311.11509" title="Download PDF">pdf</a>, <a href="/format/2311.11509" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Token-Level Adversarial Prompt Detection Based on Perplexity Measures  and Contextual Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhengmian Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+G">Gang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Mitra%2C+S">Saayan Mitra</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Ruiyi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+T">Tong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Swaminathan%2C+V">Viswanathan Swaminathan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1107">[1107]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.11904" title="Abstract">arXiv:2311.11904</a> (replaced) [<a href="/pdf/2311.11904" title="Download PDF">pdf</a>, <a href="/format/2311.11904" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMs as Visual Explainers: Advancing Image Classification with Evolving  Visual Descriptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Songhao Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhuo%2C+L">Le Zhuo</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+Y">Yue Liao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+S">Si Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1108">[1108]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12713" title="Abstract">arXiv:2311.12713</a> (replaced) [<a href="/pdf/2311.12713" title="Download PDF">pdf</a>, <a href="/format/2311.12713" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Alpha Zero for Physics: Application of Symbolic Regression with Alpha  Zero to find the analytical methods in physics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Michishita%2C+Y">Yoshihiro Michishita</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 4 figures and a table in the main text. The source codes are uploaded on Git Hub
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1109">[1109]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.12803" title="Abstract">arXiv:2311.12803</a> (replaced) [<a href="/pdf/2311.12803" title="Download PDF">pdf</a>, <a href="/format/2311.12803" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Copyright Risks of Text-to-Image Diffusion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Tzun%2C+T+T">Teoh Tze Tzun</a>, 
<a href="/search/cs?searchtype=author&query=Hern%2C+L+W">Lim Wei Hern</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haonan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kawaguchi%2C+K">Kenji Kawaguchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages including appendix
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Multimedia (cs.MM)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item1110">[1110]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.13010" title="Abstract">arXiv:2311.13010</a> (replaced) [<a href="/pdf/2311.13010" title="Download PDF">pdf</a>, <a href="/format/2311.13010" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Catoni: Sharper Rates for Heavy-Tailed and Robust Mean Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Gupta%2C+S">Shivam Gupta</a>, 
<a href="/search/math?searchtype=author&query=Hopkins%2C+S+B">Samuel B. Hopkins</a>, 
<a href="/search/math?searchtype=author&query=Price%2C+E">Eric Price</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Data Structures and Algorithms (cs.DS); Information Theory (cs.IT)

</div>
</div>
</dd>
<dt><a name="item1111">[1111]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.13155" title="Abstract">arXiv:2311.13155</a> (replaced) [<a href="/pdf/2311.13155" title="Download PDF">pdf</a>, <a href="/format/2311.13155" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A threshold-type algorithm to the gradient flow of the Canham-Helfrich  functional
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ishii%2C+K">Katsuyuki Ishii</a>, 
<a href="/search/math?searchtype=author&query=Kohsaka%2C+Y">Yoshihito Kohsaka</a>, 
<a href="/search/math?searchtype=author&query=Miyake%2C+N">Nobuhito Miyake</a>, 
<a href="/search/math?searchtype=author&query=Sakakibara%2C+K">Koya Sakakibara</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Analysis of PDEs (math.AP)</span>; Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item1112">[1112]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.13584" title="Abstract">arXiv:2311.13584</a> (replaced) [<a href="/pdf/2311.13584" title="Download PDF">pdf</a>, <a href="/ps/2311.13584" title="Download PostScript">ps</a>, <a href="/format/2311.13584" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On diffusion-based generative models and their error bounds: The  log-concave case with full convergence estimates
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bruno%2C+S">Stefano Bruno</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Ying Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+D">Dong-Young Lim</a>, 
<a href="/search/cs?searchtype=author&query=Akyildiz%2C+%C3%96+D">&#xd6;mer Deniz Akyildiz</a>, 
<a href="/search/cs?searchtype=author&query=Sabanis%2C+S">Sotirios Sabanis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Probability (math.PR); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1113">[1113]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.13871" title="Abstract">arXiv:2311.13871</a> (replaced) [<a href="/pdf/2311.13871" title="Download PDF">pdf</a>, <a href="/format/2311.13871" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Legal Requirements Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Abualhaija%2C+S">Sallam Abualhaija</a>, 
<a href="/search/cs?searchtype=author&query=Ceci%2C+M">Marcello Ceci</a>, 
<a href="/search/cs?searchtype=author&query=Briand%2C+L">Lionel Briand</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1114">[1114]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.14603" title="Abstract">arXiv:2311.14603</a> (replaced) [<a href="/pdf/2311.14603" title="Download PDF">pdf</a>, <a href="/format/2311.14603" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Animate124: Animating One Image to 4D Dynamic Scene
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Y">Yuyang Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Z">Zhiwen Yan</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+E">Enze Xie</a>, 
<a href="/search/cs?searchtype=author&query=Hong%2C+L">Lanqing Hong</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhenguo Li</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+G+H">Gim Hee Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://animate124.github.io">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1115">[1115]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.14645" title="Abstract">arXiv:2311.14645</a> (replaced) [<a href="/pdf/2311.14645" title="Download PDF">pdf</a>, <a href="/format/2311.14645" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A General Framework for User-Guided Bayesian Optimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hvarfner%2C+C">Carl Hvarfner</a>, 
<a href="/search/cs?searchtype=author&query=Hutter%2C+F">Frank Hutter</a>, 
<a href="/search/cs?searchtype=author&query=Nardi%2C+L">Luigi Nardi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 11 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 12:th International Conference on Learning Representations (ICLR
  2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1116">[1116]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.14778" title="Abstract">arXiv:2311.14778</a> (replaced) [<a href="/pdf/2311.14778" title="Download PDF">pdf</a>, <a href="/format/2311.14778" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Anomaly detection in cross-country money transfer temporal networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vilella%2C+S">Salvatore Vilella</a>, 
<a href="/search/cs?searchtype=author&query=Lupi%2C+A+T+E+C">Arthur Thomas Edward Capozzi Lupi</a>, 
<a href="/search/cs?searchtype=author&query=Fornasiero%2C+M">Marco Fornasiero</a>, 
<a href="/search/cs?searchtype=author&query=Moncalvo%2C+D">Dario Moncalvo</a>, 
<a href="/search/cs?searchtype=author&query=Ricci%2C+V">Valeria Ricci</a>, 
<a href="/search/cs?searchtype=author&query=Ronchiadin%2C+S">Silvia Ronchiadin</a>, 
<a href="/search/cs?searchtype=author&query=Ruffo%2C+G">Giancarlo Ruffo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computational Engineering, Finance, and Science (cs.CE); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item1117">[1117]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.14901" title="Abstract">arXiv:2311.14901</a> (replaced) [<a href="/pdf/2311.14901" title="Download PDF">pdf</a>, <a href="/format/2311.14901" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Code Search Debiasing:Improve Search Results beyond Overall Ranking  Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Sheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Hui Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanlin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wei%2C+Z">Zhao Wei</a>, 
<a href="/search/cs?searchtype=author&query=Xiu%2C+Y">Yong Xiu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Juhong Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+R">Rongong Ji</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1118">[1118]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.15296" title="Abstract">arXiv:2311.15296</a> (replaced) [<a href="/pdf/2311.15296" title="Download PDF">pdf</a>, <a href="/format/2311.15296" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UHGEval: Benchmarking the Hallucination of Chinese Large Language Models  via Unconstrained Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+X">Xun Liang</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+S">Shichao Song</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+S">Simin Niu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+F">Feiyu Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+B">Bo Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wy%2C+Z">Zhaohui Wy</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+D">Dawei He</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+P">Peng Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhonghao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+H">Haiying Deng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1119">[1119]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.16121" title="Abstract">arXiv:2311.16121</a> (replaced) [<a href="/pdf/2311.16121" title="Download PDF">pdf</a>, <a href="/format/2311.16121" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Real-Time Neural Materials using Block-Compressed Features
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Weinreich%2C+C">Cl&#xe9;ment Weinreich</a>, 
<a href="/search/cs?searchtype=author&query=de+Oliveira%2C+L">Louis de Oliveira</a>, 
<a href="/search/cs?searchtype=author&query=Houdard%2C+A">Antoine Houdard</a>, 
<a href="/search/cs?searchtype=author&query=Nader%2C+G">Georges Nader</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Eurographics 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR)

</div>
</div>
</dd>
<dt><a name="item1120">[1120]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.16609" title="Abstract">arXiv:2311.16609</a> (replaced) [<a href="/pdf/2311.16609" title="Download PDF">pdf</a>, <a href="/format/2311.16609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Eigenmatrix for unstructured sparse recovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ying%2C+L">Lexing Ying</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Information Theory (cs.IT); Machine Learning (cs.LG); Signal Processing (eess.SP)

</div>
</div>
</dd>
<dt><a name="item1121">[1121]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.16716" title="Abstract">arXiv:2311.16716</a> (replaced) [<a href="/pdf/2311.16716" title="Download PDF">pdf</a>, <a href="/format/2311.16716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GraphPro: Graph Pre-training and Prompt Learning for Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yuhao Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+L">Lianghao Xia</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+D">Da Luo</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+K">Kangyi Lin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+C">Chao Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by WWW'2024, full paper
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1122">[1122]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.17085" title="Abstract">arXiv:2311.17085</a> (replaced) [<a href="/pdf/2311.17085" title="Download PDF">pdf</a>, <a href="/format/2311.17085" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Visual Cues: Synchronously Exploring Target-Centric Semantics for  Vision-Language Tracking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ge%2C+J">Jiawei Ge</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiangmei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+J">Jiuxin Cao</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xuelin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bo Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1123">[1123]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2311.18703" title="Abstract">arXiv:2311.18703</a> (replaced) [<a href="/pdf/2311.18703" title="Download PDF">pdf</a>, <a href="/format/2311.18703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predictable Reinforcement Learning Dynamics through Entropy Rate  Minimization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ornia%2C+D+J">Daniel Jarne Ornia</a>, 
<a href="/search/cs?searchtype=author&query=Delimpaltadakis%2C+G">Giannis Delimpaltadakis</a>, 
<a href="/search/cs?searchtype=author&query=Kober%2C+J">Jens Kober</a>, 
<a href="/search/cs?searchtype=author&query=Alonso-Mora%2C+J">Javier Alonso-Mora</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1124">[1124]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.00024" title="Abstract">arXiv:2312.00024</a> (replaced) [<a href="/pdf/2312.00024" title="Download PDF">pdf</a>, <a href="/format/2312.00024" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Can LLMs Patch Security Issues?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Alrashedy%2C+K">Kamel Alrashedy</a>, 
<a href="/search/cs?searchtype=author&query=Aljasser%2C+A">Abdullah Aljasser</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1125">[1125]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.00078" title="Abstract">arXiv:2312.00078</a> (replaced) [<a href="/pdf/2312.00078" title="Download PDF">pdf</a>, <a href="/format/2312.00078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing Cross-domain Click-Through Rate Prediction via Explicit  Feature Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+Z">Zida Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+J">Jiangchao Yao</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+C">Chen Ju</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+W">Weilin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+J">Jinsong Lan</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+X">Xiaoyi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Xiao%2C+S">Shuai Xiao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by WWW 2024. arXiv admin note: substantial text overlap with <a href="/abs/2305.03953">arXiv:2305.03953</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item1126">[1126]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.00163" title="Abstract">arXiv:2312.00163</a> (replaced) [<a href="/pdf/2312.00163" title="Download PDF">pdf</a>, <a href="/format/2312.00163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Just add WATER: WebAssembly-based Circumvention Transports
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chi%2C+E">Erik Chi</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Gaukas Wang</a>, 
<a href="/search/cs?searchtype=author&query=Halderman%2C+J+A">J. Alex Halderman</a>, 
<a href="/search/cs?searchtype=author&query=Wustrow%2C+E">Eric Wustrow</a>, 
<a href="/search/cs?searchtype=author&query=Wampler%2C+J">Jack Wampler</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> FOCI 2024
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> FOCI issue 1 (2024) 22-28
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item1127">[1127]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.00233" title="Abstract">arXiv:2312.00233</a> (replaced) [<a href="/pdf/2312.00233" title="Download PDF">pdf</a>, <a href="/format/2312.00233" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The role of interface design on prompt-mediated creativity in Generative  AI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Torricelli%2C+M">Maddalena Torricelli</a>, 
<a href="/search/cs?searchtype=author&query=Martino%2C+M">Mauro Martino</a>, 
<a href="/search/cs?searchtype=author&query=Baronchelli%2C+A">Andrea Baronchelli</a>, 
<a href="/search/cs?searchtype=author&query=Aiello%2C+L+M">Luca Maria Aiello</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication at WebSci'24. 6 pages, 8 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Human-Computer Interaction (cs.HC); Physics and Society (physics.soc-ph)

</div>
</div>
</dd>
<dt><a name="item1128">[1128]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.00246" title="Abstract">arXiv:2312.00246</a> (replaced) [<a href="/pdf/2312.00246" title="Download PDF">pdf</a>, <a href="/format/2312.00246" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Directions of Curvature as an Explanation for Loss of Plasticity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lewandowski%2C+A">Alex Lewandowski</a>, 
<a href="/search/cs?searchtype=author&query=Tanaka%2C+H">Haruto Tanaka</a>, 
<a href="/search/cs?searchtype=author&query=Schuurmans%2C+D">Dale Schuurmans</a>, 
<a href="/search/cs?searchtype=author&query=Machado%2C+M+C">Marlos C. Machado</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1129">[1129]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.00519" title="Abstract">arXiv:2312.00519</a> (replaced) [<a href="/pdf/2312.00519" title="Download PDF">pdf</a>, <a href="/ps/2312.00519" title="Download PostScript">ps</a>, <a href="/format/2312.00519" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Impact of Privacy and Security Attitudes and Concerns of Travellers  on Their Willingness to Use Mobility-as-a-Service Systems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heering%2C+M+S">Maria Sophia Heering</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Haiyue Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+S">Shujun Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Please cite this paper as follows: Maria Sophia Heering, Haiyue Yuan and Shujun Li (2023) The Impact of Privacy and Security Attitudes and Concerns of Travellers on Their Willingness to Use Mobility-as-a-Service Systems. Proceedings of the 2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC 2023), pp.5573-5578, IEEE, doi: 10.1109/ITSC57777.2023.10422468
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1130">[1130]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.01619" title="Abstract">arXiv:2312.01619</a> (replaced) [<a href="/pdf/2312.01619" title="Download PDF">pdf</a>, <a href="/format/2312.01619" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How Many Validation Labels Do You Need? Exploring the Design Space of  Label-Efficient Model Ranking
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+Z">Zhengyu Hu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jieyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yue Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+Y">Yuchen Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+H">Hui Xiong</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1131">[1131]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.02658" title="Abstract">arXiv:2312.02658</a> (replaced) [<a href="/pdf/2312.02658" title="Download PDF">pdf</a>, <a href="/ps/2312.02658" title="Download PostScript">ps</a>, <a href="/format/2312.02658" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do AI models produce better weather forecasts than physics-based models?  A quantitative evaluation case study of Storm Ciar&#xe1;n
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Charlton-Perez%2C+A+J">Andrew J. Charlton-Perez</a>, 
<a href="/search/cs?searchtype=author&query=Dacre%2C+H+F">Helen F. Dacre</a>, 
<a href="/search/cs?searchtype=author&query=Driscoll%2C+S">Simon Driscoll</a>, 
<a href="/search/cs?searchtype=author&query=Gray%2C+S+L">Suzanne L. Gray</a>, 
<a href="/search/cs?searchtype=author&query=Harvey%2C+B">Ben Harvey</a>, 
<a href="/search/cs?searchtype=author&query=Harvey%2C+N+J">Natalie J. Harvey</a>, 
<a href="/search/cs?searchtype=author&query=Hunt%2C+K+M+R">Kieran M. R. Hunt</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+R+W">Robert W. Lee</a>, 
<a href="/search/cs?searchtype=author&query=Swaminathan%2C+R">Ranjini Swaminathan</a>, 
<a href="/search/cs?searchtype=author&query=Vandaele%2C+R">Remy Vandaele</a>, 
<a href="/search/cs?searchtype=author&query=Volont%C3%A9%2C+A">Ambrogio Volont&#xe9;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Atmospheric and Oceanic Physics (physics.ao-ph)

</div>
</div>
</dd>
<dt><a name="item1132">[1132]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.03748" title="Abstract">arXiv:2312.03748</a> (replaced) [<a href="/pdf/2312.03748" title="Download PDF">pdf</a>, <a href="/ps/2312.03748" title="Download PostScript">ps</a>, <a href="/format/2312.03748" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Applying Large Language Models and Chain-of-Thought for Automatic  Scoring
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+G">Gyeong-Geon Lee</a>, 
<a href="/search/cs?searchtype=author&query=Latif%2C+E">Ehsan Latif</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xuansheng Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+N">Ninghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhai%2C+X">Xiaoming Zhai</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1133">[1133]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.05583" title="Abstract">arXiv:2312.05583</a> (replaced) [<a href="/pdf/2312.05583" title="Download PDF">pdf</a>, <a href="/format/2312.05583" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Better Neural PDE Solvers Through Data-Free Mesh Movers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+P">Peiyan Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yue Wang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+Z">Zhi-Ming Ma</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Numerical Analysis (math.NA)

</div>
</div>
</dd>
<dt><a name="item1134">[1134]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.05875" title="Abstract">arXiv:2312.05875</a> (replaced) [<a href="/pdf/2312.05875" title="Download PDF">pdf</a>, <a href="/format/2312.05875" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Class-Aware Pruning for Efficient Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jiang%2C+M">Mengnan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jingcun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Eldebiky%2C+A">Amro Eldebiky</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+X">Xunzhao Yin</a>, 
<a href="/search/cs?searchtype=author&query=Zhuo%2C+C">Cheng Zhuo</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+I">Ing-Chao Lin</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+G+L">Grace Li Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Design Automation and Test in Europe (DATE) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1135">[1135]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.06149" title="Abstract">arXiv:2312.06149</a> (replaced) [<a href="/pdf/2312.06149" title="Download PDF">pdf</a>, <a href="/format/2312.06149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking Anticipatory Text Generation: A Constrained Approach for Large  Language Models Decoding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tu%2C+L">Lifu Tu</a>, 
<a href="/search/cs?searchtype=author&query=Yavuz%2C+S">Semih Yavuz</a>, 
<a href="/search/cs?searchtype=author&query=Qu%2C+J">Jin Qu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jiacheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+R">Rui Meng</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+C">Caiming Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+Y">Yingbo Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1136">[1136]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.06185" title="Abstract">arXiv:2312.06185</a> (replaced) [<a href="/pdf/2312.06185" title="Download PDF">pdf</a>, <a href="/format/2312.06185" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> KnowGPT: Black-Box Knowledge Injection for Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qinggang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Junnan Dong</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Hao Chen</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xiao Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zha%2C+D">Daochen Zha</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Z">Zailiang Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1137">[1137]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07205" title="Abstract">arXiv:2312.07205</a> (replaced) [<a href="/pdf/2312.07205" title="Download PDF">pdf</a>, <a href="/format/2312.07205" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Construction and application of an algebraic dual basis and the  Fine-Scale Greens&#x27; Function for computing projections and reconstructing  unresolved scales
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shrestha%2C+S">Suyash Shrestha</a>, 
<a href="/search/math?searchtype=author&query=Dekker%2C+J">Joey Dekker</a>, 
<a href="/search/math?searchtype=author&query=Gerritsma%2C+M">Marc Gerritsma</a>, 
<a href="/search/math?searchtype=author&query=Hulshoff%2C+S">Steven Hulshoff</a>, 
<a href="/search/math?searchtype=author&query=Akkerman%2C+I">Ido Akkerman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Mathematical Physics (math-ph)

</div>
</div>
</dd>
<dt><a name="item1138">[1138]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.07401" title="Abstract">arXiv:2312.07401</a> (replaced) [<a href="/pdf/2312.07401" title="Download PDF">pdf</a>, <a href="/format/2312.07401" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Diversified Preferences of Large Language Model Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zeng%2C+D">Dun Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Yong Dai</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+P">Pengyu Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+T">Tianhao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+W">Wanshun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+N">Nan Du</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Z">Zenglin Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> preprint
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1139">[1139]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08207" title="Abstract">arXiv:2312.08207</a> (replaced) [<a href="/pdf/2312.08207" title="Download PDF">pdf</a>, <a href="/format/2312.08207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Black-box Membership Inference Attacks against Fine-tuned Diffusion  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pang%2C+Y">Yan Pang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+T">Tianhao Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item1140">[1140]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.08935" title="Abstract">arXiv:2312.08935</a> (replaced) [<a href="/pdf/2312.08935" title="Download PDF">pdf</a>, <a href="/format/2312.08935" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human  Annotations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+P">Peiyi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lei Li</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Z">Zhihong Shao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+R+X">R.X. Xu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+D">Damai Dai</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yifei Li</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+D">Deli Chen</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Y.Wu</a>, 
<a href="/search/cs?searchtype=author&query=Sui%2C+Z">Zhifang Sui</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Add Step-by-Step reinforcement learning results
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1141">[1141]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.09039" title="Abstract">arXiv:2312.09039</a> (replaced) [<a href="/pdf/2312.09039" title="Download PDF">pdf</a>, <a href="/format/2312.09039" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TAP4LLM: Table Provider on Sampling, Augmenting, and Packing  Semi-structured Data for Large Language Model Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sui%2C+Y">Yuan Sui</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+J">Jiaru Zou</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mengyu Zhou</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+X">Xinyi He</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+L">Lun Du</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+S">Shi Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dongmei Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1142">[1142]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.09236" title="Abstract">arXiv:2312.09236</a> (replaced) [<a href="/pdf/2312.09236" title="Download PDF">pdf</a>, <a href="/format/2312.09236" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A framework for conditional diffusion modelling with applications in  motif scaffolding for protein design
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Didi%2C+K">Kieran Didi</a>, 
<a href="/search/cs?searchtype=author&query=Vargas%2C+F">Francisco Vargas</a>, 
<a href="/search/cs?searchtype=author&query=Mathis%2C+S+V">Simon V Mathis</a>, 
<a href="/search/cs?searchtype=author&query=Dutordoir%2C+V">Vincent Dutordoir</a>, 
<a href="/search/cs?searchtype=author&query=Mathieu%2C+E">Emile Mathieu</a>, 
<a href="/search/cs?searchtype=author&query=Komorowska%2C+U+J">Urszula J Komorowska</a>, 
<a href="/search/cs?searchtype=author&query=Lio%2C+P">Pietro Lio</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Biomolecules (q-bio.BM)

</div>
</div>
</dd>
<dt><a name="item1143">[1143]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.10040" title="Abstract">arXiv:2312.10040</a> (replaced) [<a href="/pdf/2312.10040" title="Download PDF">pdf</a>, <a href="/format/2312.10040" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Robust Errant Beam Prognostics with Conditional Modeling for Particle  Accelerators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/physics?searchtype=author&query=Rajput%2C+K">Kishansingh Rajput</a>, 
<a href="/search/physics?searchtype=author&query=Schram%2C+M">Malachi Schram</a>, 
<a href="/search/physics?searchtype=author&query=Blokland%2C+W">Willem Blokland</a>, 
<a href="/search/physics?searchtype=author&query=Alanazi%2C+Y">Yasir Alanazi</a>, 
<a href="/search/physics?searchtype=author&query=Ramuhalli%2C+P">Pradeep Ramuhalli</a>, 
<a href="/search/physics?searchtype=author&query=Zhukov%2C+A">Alexander Zhukov</a>, 
<a href="/search/physics?searchtype=author&query=Peters%2C+C">Charles Peters</a>, 
<a href="/search/physics?searchtype=author&query=Vilalta%2C+R">Ricardo Vilalta</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review at Machine Learning: Science and Technology Journal
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Accelerator Physics (physics.acc-ph)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1144">[1144]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.10325" title="Abstract">arXiv:2312.10325</a> (replaced) [<a href="/pdf/2312.10325" title="Download PDF">pdf</a>, <a href="/format/2312.10325" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Attentive Inductive Bias for Sequential Recommendation beyond the  Self-Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Shin%2C+Y">Yehjin Shin</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jeongwhan Choi</a>, 
<a href="/search/cs?searchtype=author&query=Wi%2C+H">Hyowon Wi</a>, 
<a href="/search/cs?searchtype=author&query=Park%2C+N">Noseong Park</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by AAAI 2024. Yehjin Shin and Jeongwhan Choi are co-first authors with equal contribution
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item1145">[1145]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.10793" title="Abstract">arXiv:2312.10793</a> (replaced) [<a href="/pdf/2312.10793" title="Download PDF">pdf</a>, <a href="/format/2312.10793" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Demystifying Instruction Mixing for Fine-tuning Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Renxi Wang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Haonan Li</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Minghao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuxia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xudong Han</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chiyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Baldwin%2C+T">Timothy Baldwin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Instruction Tuning, Large Language Model, Alignment
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1146">[1146]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.11364" title="Abstract">arXiv:2312.11364</a> (replaced) [<a href="/pdf/2312.11364" title="Download PDF">pdf</a>, <a href="/format/2312.11364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Counting Reward Automata: Sample Efficient Reinforcement Learning  Through the Exploitation of Reward Function Structure
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Bester%2C+T">Tristan Bester</a>, 
<a href="/search/cs?searchtype=author&query=Rosman%2C+B">Benjamin Rosman</a>, 
<a href="/search/cs?searchtype=author&query=James%2C+S">Steven James</a>, 
<a href="/search/cs?searchtype=author&query=Tasse%2C+G+N">Geraud Nangue Tasse</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 14 pages, 11 Figures, Published in AAAI W25: Neuro-Symbolic Learning and Reasoning in the era of Large Language Models (NuCLeaR)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1147">[1147]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12065" title="Abstract">arXiv:2312.12065</a> (replaced) [<a href="/pdf/2312.12065" title="Download PDF">pdf</a>, <a href="/format/2312.12065" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PPO-Clip Attains Global Optimality: Towards Deeper Understandings of  Clipping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+N">Nai-Chieh Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hsieh%2C+P">Ping-Chun Hsieh</a>, 
<a href="/search/cs?searchtype=author&query=Ho%2C+K">Kuo-Hao Ho</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+I">I-Chen Wu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1148">[1148]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12341" title="Abstract">arXiv:2312.12341</a> (replaced) [<a href="/pdf/2312.12341" title="Download PDF">pdf</a>, <a href="/format/2312.12341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Engineering an Exact Pseudo-Boolean Model Counter
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+S">Suwei Yang</a>, 
<a href="/search/cs?searchtype=author&query=Meel%2C+K+S">Kuldeep S. Meel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 8 figures. To appear in AAAI24
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1149">[1149]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.12905" title="Abstract">arXiv:2312.12905</a> (replaced) [<a href="/pdf/2312.12905" title="Download PDF">pdf</a>, <a href="/format/2312.12905" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the distance to low-rank matrices in the maximum norm
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Budzinskiy%2C+S">Stanislav Budzinskiy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted version
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item1150">[1150]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.13027" title="Abstract">arXiv:2312.13027</a> (replaced) [<a href="/pdf/2312.13027" title="Download PDF">pdf</a>, <a href="/format/2312.13027" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Doubly Perturbed Task Free Continual Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+B+H">Byung Hyun Lee</a>, 
<a href="/search/cs?searchtype=author&query=Oh%2C+M">Min-hwan Oh</a>, 
<a href="/search/cs?searchtype=author&query=Chun%2C+S+Y">Se Young Chun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AAAI 2024 (Oral)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1151">[1151]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.13521" title="Abstract">arXiv:2312.13521</a> (replaced) [<a href="/pdf/2312.13521" title="Download PDF">pdf</a>, <a href="/ps/2312.13521" title="Download PostScript">ps</a>, <a href="/format/2312.13521" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Preparing to Integrate Generative Pretrained Transformer Series 4 models  into Genetic Variant Assessment Workflows: Assessing Performance, Drift, and  Nondeterminism Characteristics Relative to Classifying Functional Evidence in  Literature
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-bio?searchtype=author&query=Aronson%2C+S+J">Samuel J. Aronson</a> (1,2), 
<a href="/search/q-bio?searchtype=author&query=Machini%2C+K">Kalotina Machini</a> (1,3), 
<a href="/search/q-bio?searchtype=author&query=Shin%2C+J">Jiyeon Shin</a> (2), 
<a href="/search/q-bio?searchtype=author&query=Sriraman%2C+P">Pranav Sriraman</a> (1), 
<a href="/search/q-bio?searchtype=author&query=Hamill%2C+S">Sean Hamill</a> (4), 
<a href="/search/q-bio?searchtype=author&query=Henricks%2C+E+R">Emma R. Henricks</a> (1), 
<a href="/search/q-bio?searchtype=author&query=Mailly%2C+C">Charlotte Mailly</a> (1,2), 
<a href="/search/q-bio?searchtype=author&query=Nottage%2C+A+J">Angie J. Nottage</a> (1), 
<a href="/search/q-bio?searchtype=author&query=Amr%2C+S+S">Sami S. Amr</a> (1,3), 
<a href="/search/q-bio?searchtype=author&query=Oates%2C+M">Michael Oates</a> (1,2), 
<a href="/search/q-bio?searchtype=author&query=Lebo%2C+M+S">Matthew S. Lebo</a> (1,3) ((1) Mass General Brigham Personalized Medicine, (2) Accelerator for Clinical Transformation, Mass General Brigham, (3) Department of Pathology, Brigham and Women&#x27;s Hospital, (4) Microsoft Corporation)
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 1 table, 4 figures, 2 supplementary tables, 1 supplementary figure. These authors contributed equally: Samuel J. Aronson, Kalotina Machini, and Jiyeon Shin Corresponding author: Samuel J. Aronson
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Genomics (q-bio.GN)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1152">[1152]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.13729" title="Abstract">arXiv:2312.13729</a> (replaced) [<a href="/pdf/2312.13729" title="Download PDF">pdf</a>, <a href="/format/2312.13729" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gaussian Splatting with NeRF-based Color and Opacity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Malarz%2C+D">Dawid Malarz</a>, 
<a href="/search/cs?searchtype=author&query=Smolak%2C+W">Weronika Smolak</a>, 
<a href="/search/cs?searchtype=author&query=Tabor%2C+J">Jacek Tabor</a>, 
<a href="/search/cs?searchtype=author&query=Tadeja%2C+S">S&#x142;awomir Tadeja</a>, 
<a href="/search/cs?searchtype=author&query=Spurek%2C+P">Przemys&#x142;aw Spurek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1153">[1153]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.14590" title="Abstract">arXiv:2312.14590</a> (replaced) [<a href="/pdf/2312.14590" title="Download PDF">pdf</a>, <a href="/format/2312.14590" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SIG: Speaker Identification in Literature via Prompt-Based Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Su%2C+Z">Zhenlin Su</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Liyan Xu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jin Xu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jiangnan Li</a>, 
<a href="/search/cs?searchtype=author&query=Huangfu%2C+M">Mingdu Huangfu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1154">[1154]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.14922" title="Abstract">arXiv:2312.14922</a> (replaced) [<a href="/pdf/2312.14922" title="Download PDF">pdf</a>, <a href="/format/2312.14922" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning from higher-order statistics, efficiently: hypothesis tests,  random features, and neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Sz%C3%A9kely%2C+E">Eszter Sz&#xe9;kely</a>, 
<a href="/search/stat?searchtype=author&query=Bardone%2C+L">Lorenzo Bardone</a>, 
<a href="/search/stat?searchtype=author&query=Gerace%2C+F">Federica Gerace</a>, 
<a href="/search/stat?searchtype=author&query=Goldt%2C+S">Sebastian Goldt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Statistical Mechanics (cond-mat.stat-mech); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1155">[1155]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15112" title="Abstract">arXiv:2312.15112</a> (replaced) [<a href="/pdf/2312.15112" title="Download PDF">pdf</a>, <a href="/format/2312.15112" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Less or More From Teacher: Exploiting Trilateral Geometry For Knowledge  Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hu%2C+C">Chengming Hu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Haolun Wu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chen Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+J">Jun Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+B">Boyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xue Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1156">[1156]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15433" title="Abstract">arXiv:2312.15433</a> (replaced) [<a href="/pdf/2312.15433" title="Download PDF">pdf</a>, <a href="/ps/2312.15433" title="Download PostScript">ps</a>, <a href="/format/2312.15433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Best-of-Both-Worlds Algorithms for Linear Contextual Bandits
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuroki%2C+Y">Yuko Kuroki</a>, 
<a href="/search/cs?searchtype=author&query=Rumi%2C+A">Alberto Rumi</a>, 
<a href="/search/cs?searchtype=author&query=Tsuchiya%2C+T">Taira Tsuchiya</a>, 
<a href="/search/cs?searchtype=author&query=Vitale%2C+F">Fabio Vitale</a>, 
<a href="/search/cs?searchtype=author&query=Cesa-Bianchi%2C+N">Nicol&#xf2; Cesa-Bianchi</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AISTATS2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1157">[1157]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15874" title="Abstract">arXiv:2312.15874</a> (replaced) [<a href="/pdf/2312.15874" title="Download PDF">pdf</a>, <a href="/ps/2312.15874" title="Download PostScript">ps</a>, <a href="/format/2312.15874" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lower Bounds for Set-Multilinear Branching Programs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chatterjee%2C+P">Prerona Chatterjee</a>, 
<a href="/search/cs?searchtype=author&query=Kush%2C+D">Deepanshu Kush</a>, 
<a href="/search/cs?searchtype=author&query=Saraf%2C+S">Shubhangi Saraf</a>, 
<a href="/search/cs?searchtype=author&query=Shpilka%2C+A">Amir Shpilka</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item1158">[1158]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.15910" title="Abstract">arXiv:2312.15910</a> (replaced) [<a href="/pdf/2312.15910" title="Download PDF">pdf</a>, <a href="/format/2312.15910" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Reinforcement Unlearning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ye%2C+D">Dayong Ye</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+T">Tianqing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+C">Congcong Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+D">Derui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Z">Zewei Shi</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+S">Sheng Shen</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+W">Wanlei Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Xue%2C+M">Minhui Xue</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1159">[1159]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2312.16819" title="Abstract">arXiv:2312.16819</a> (replaced) [<a href="/pdf/2312.16819" title="Download PDF">pdf</a>, <a href="/format/2312.16819" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hidden Minima in Two-Layer ReLU Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arjevani%2C+Y">Yossi Arjevani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1160">[1160]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.00850" title="Abstract">arXiv:2401.00850</a> (replaced) [<a href="/pdf/2401.00850" title="Download PDF">pdf</a>, <a href="/format/2401.00850" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Refining Pre-Trained Motion Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sun%2C+X">Xinglong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Harley%2C+A+W">Adam W. Harley</a>, 
<a href="/search/cs?searchtype=author&query=Guibas%2C+L+J">Leonidas J. Guibas</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1161">[1161]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.02739" title="Abstract">arXiv:2401.02739</a> (replaced) [<a href="/pdf/2401.02739" title="Download PDF">pdf</a>, <a href="/format/2401.02739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Denoising Diffusion Variational Inference: Diffusion Models as  Expressive Variational Posteriors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piriyakulkij%2C+T">Top Piriyakulkij</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yingheng Wang</a>, 
<a href="/search/cs?searchtype=author&query=Kuleshov%2C+V">Volodymyr Kuleshov</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Quantitative Methods (q-bio.QM); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1162">[1162]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.03160" title="Abstract">arXiv:2401.03160</a> (replaced) [<a href="/pdf/2401.03160" title="Download PDF">pdf</a>, <a href="/format/2401.03160" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning  for Safe and Efficient Autonomous Driving
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+Z">Zilin Huang</a>, 
<a href="/search/cs?searchtype=author&query=Sheng%2C+Z">Zihao Sheng</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+C">Chengyuan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+S">Sikai Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by Communications in Transportation Research
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1163">[1163]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05018" title="Abstract">arXiv:2401.05018</a> (replaced) [<a href="/pdf/2401.05018" title="Download PDF">pdf</a>, <a href="/format/2401.05018" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AdvMT: Adversarial Motion Transformer for Long-term Human Motion  Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Idrees%2C+S">Sarmad Idrees</a>, 
<a href="/search/cs?searchtype=author&query=Choi%2C+J">Jongeun Choi</a>, 
<a href="/search/cs?searchtype=author&query=Sohn%2C+S">Seokman Sohn</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The paper is under consideration at Pattern Recognition Letters
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1164">[1164]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.05394" title="Abstract">arXiv:2401.05394</a> (replaced) [<a href="/pdf/2401.05394" title="Download PDF">pdf</a>, <a href="/format/2401.05394" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Iterative Regularization with k-support Norm: An Important Complement to  Sparse Recovery
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=de+Vazelhes%2C+W">William de Vazelhes</a>, 
<a href="/search/eess?searchtype=author&query=Mukhoty%2C+B">Bhaskar Mukhoty</a>, 
<a href="/search/eess?searchtype=author&query=Yuan%2C+X">Xiao-Tong Yuan</a>, 
<a href="/search/eess?searchtype=author&query=Gu%2C+B">Bin Gu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at AAAI 2024. Code at <a href="https://github.com/wdevazelhes/IRKSN_AAAI2024">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG); Optimization and Control (math.OC); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1165">[1165]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06201" title="Abstract">arXiv:2401.06201</a> (replaced) [<a href="/pdf/2401.06201" title="Download PDF">pdf</a>, <a href="/format/2401.06201" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+S">Siyu Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Song%2C+K">Kaitao Song</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiangjie Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tan%2C+X">Xu Tan</a>, 
<a href="/search/cs?searchtype=author&query=Shen%2C+Y">Yongliang Shen</a>, 
<a href="/search/cs?searchtype=author&query=Kan%2C+R">Ren Kan</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+D">Dongsheng Li</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+D">Deqing Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1166">[1166]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06356" title="Abstract">arXiv:2401.06356</a> (replaced) [<a href="/pdf/2401.06356" title="Download PDF">pdf</a>, <a href="/format/2401.06356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Investigation into the Effect of Parameter Choices in  Knowledge Distillation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sultan%2C+M+A">Md Arafat Sultan</a>, 
<a href="/search/cs?searchtype=author&query=Trivedi%2C+A">Aashka Trivedi</a>, 
<a href="/search/cs?searchtype=author&query=Awasthy%2C+P">Parul Awasthy</a>, 
<a href="/search/cs?searchtype=author&query=Sil%2C+A">Avirup Sil</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1167">[1167]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06532" title="Abstract">arXiv:2401.06532</a> (replaced) [<a href="/pdf/2401.06532" title="Download PDF">pdf</a>, <a href="/format/2401.06532" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> INTERS: Unlocking the Power of Large Language Models in Search with  Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yutao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+P">Peitian Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chenghao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Y">Yifei Chen</a>, 
<a href="/search/cs?searchtype=author&query=Xie%2C+B">Binyu Xie</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Z">Zhicheng Dou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zheng Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+J">Ji-Rong Wen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> repo: <a href="https://github.com/DaoD/INTERS">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item1168">[1168]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06801" title="Abstract">arXiv:2401.06801</a> (replaced) [<a href="/pdf/2401.06801" title="Download PDF">pdf</a>, <a href="/ps/2401.06801" title="Download PostScript">ps</a>, <a href="/format/2401.06801" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph-of-Thought: Utilizing Large Language Models to Solve Complex and  Dynamic Business Problems
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Ye Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Keywords: Graph-of-Thought (GoT), Workflow Automation, Large Language Models (LLMs), Task Execution, Data-Driven Decision Making, Complexity Management
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1169">[1169]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.06824" title="Abstract">arXiv:2401.06824</a> (replaced) [<a href="/pdf/2401.06824" title="Download PDF">pdf</a>, <a href="/format/2401.06824" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Open the Pandora&#x27;s Box of LLMs: Jailbreaking LLMs through Representation  Engineering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+T">Tianlong Li</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+S">Shihan Dou</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+W">Wenhao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+M">Muling Wu</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+C">Changze Lv</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+X">Xiaoqing Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+X">Xuanjing Huang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 13 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1170">[1170]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07105" title="Abstract">arXiv:2401.07105</a> (replaced) [<a href="/pdf/2401.07105" title="Download PDF">pdf</a>, <a href="/format/2401.07105" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Plenz%2C+M">Moritz Plenz</a>, 
<a href="/search/cs?searchtype=author&query=Frank%2C+A">Anette Frank</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 10 figures, 9 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1171">[1171]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07348" title="Abstract">arXiv:2401.07348</a> (replaced) [<a href="/pdf/2401.07348" title="Download PDF">pdf</a>, <a href="/ps/2401.07348" title="Download PostScript">ps</a>, <a href="/format/2401.07348" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generative AI in EU Law: Liability, Privacy, Intellectual Property, and  Cybersecurity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Novelli%2C+C">Claudio Novelli</a>, 
<a href="/search/cs?searchtype=author&query=Casolari%2C+F">Federico Casolari</a>, 
<a href="/search/cs?searchtype=author&query=Hacker%2C+P">Philipp Hacker</a>, 
<a href="/search/cs?searchtype=author&query=Spedicato%2C+G">Giorgio Spedicato</a>, 
<a href="/search/cs?searchtype=author&query=Floridi%2C+L">Luciano Floridi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1172">[1172]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07382" title="Abstract">arXiv:2401.07382</a> (replaced) [<a href="/pdf/2401.07382" title="Download PDF">pdf</a>, <a href="/format/2401.07382" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Sparse Rewards: Enhancing Reinforcement Learning with Language  Model Critique in Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cao%2C+M">Meng Cao</a>, 
<a href="/search/cs?searchtype=author&query=Shu%2C+L">Lei Shu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+L">Lei Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Wichers%2C+N">Nevan Wichers</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Y">Yinxiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Meng%2C+L">Lei Meng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1173">[1173]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07467" title="Abstract">arXiv:2401.07467</a> (replaced) [<a href="/pdf/2401.07467" title="Download PDF">pdf</a>, <a href="/format/2401.07467" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Selection Improvements for the Parallel Iterative Algorithm for Stable  Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wynn%2C+S">Scott Wynn</a>, 
<a href="/search/cs?searchtype=author&query=Kyritsis%2C+A">Alec Kyritsis</a>, 
<a href="/search/cs?searchtype=author&query=Alberi%2C+S">Stephora Alberi</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+E">Enyue Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Data Structures and Algorithms (cs.DS)</span>

</div>
</div>
</dd>
<dt><a name="item1174">[1174]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07744" title="Abstract">arXiv:2401.07744</a> (replaced) [<a href="/pdf/2401.07744" title="Download PDF">pdf</a>, <a href="/format/2401.07744" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combining Machine Learning and Ontology: A Systematic Literature Review
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghidalia%2C+S">Sarah Ghidalia</a>, 
<a href="/search/cs?searchtype=author&query=Narsis%2C+O+L">Ouassila Labbani Narsis</a>, 
<a href="/search/cs?searchtype=author&query=Bertaux%2C+A">Aur&#xe9;lie Bertaux</a>, 
<a href="/search/cs?searchtype=author&query=Nicolle%2C+C">Christophe Nicolle</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1175">[1175]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07764" title="Abstract">arXiv:2401.07764</a> (replaced) [<a href="/pdf/2401.07764" title="Download PDF">pdf</a>, <a href="/format/2401.07764" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> When Large Language Model Agents Meet 6G Networks: Perception,  Grounding, and Alignment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+M">Minrui Xu</a>, 
<a href="/search/cs?searchtype=author&query=Niyato%2C+D">Dusit Niyato</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+J">Jiawen Kang</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+Z">Zehui Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+S">Shiwen Mao</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+Z">Zhu Han</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+D+I">Dong In Kim</a>, 
<a href="/search/cs?searchtype=author&query=Letaief%2C+K+B">Khaled B. Letaief</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Networking and Internet Architecture (cs.NI)

</div>
</div>
</dd>
<dt><a name="item1176">[1176]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.07817" title="Abstract">arXiv:2401.07817</a> (replaced) [<a href="/pdf/2401.07817" title="Download PDF">pdf</a>, <a href="/format/2401.07817" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Question Translation Training for Better Multilingual Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+W">Wenhao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+S">Shujian Huang</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+F">Fei Yuan</a>, 
<a href="/search/cs?searchtype=author&query=She%2C+S">Shuaijie She</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jiajun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Birch%2C+A">Alexandra Birch</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1177">[1177]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08189" title="Abstract">arXiv:2401.08189</a> (replaced) [<a href="/pdf/2401.08189" title="Download PDF">pdf</a>, <a href="/format/2401.08189" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PRewrite: Prompt Rewriting with Reinforcement Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kong%2C+W">Weize Kong</a>, 
<a href="/search/cs?searchtype=author&query=Hombaiah%2C+S+A">Spurthi Amba Hombaiah</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mingyang Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Mei%2C+Q">Qiaozhu Mei</a>, 
<a href="/search/cs?searchtype=author&query=Bendersky%2C+M">Michael Bendersky</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1178">[1178]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08392" title="Abstract">arXiv:2401.08392</a> (replaced) [<a href="/pdf/2401.08392" title="Download PDF">pdf</a>, <a href="/format/2401.08392" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language  Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zongxin Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+G">Guikun Chen</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiaodi Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenguan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1179">[1179]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08699" title="Abstract">arXiv:2401.08699</a> (replaced) [<a href="/pdf/2401.08699" title="Download PDF">pdf</a>, <a href="/ps/2401.08699" title="Download PostScript">ps</a>, <a href="/format/2401.08699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Image Search in Histopathology
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Tizhoosh%2C+H+R">H.R. Tizhoosh</a>, 
<a href="/search/eess?searchtype=author&query=Pantanowitz%2C+L">Liron Pantanowitz</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Information Retrieval (cs.IR); Quantitative Methods (q-bio.QM)

</div>
</div>
</dd>
<dt><a name="item1180">[1180]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.08876" title="Abstract">arXiv:2401.08876</a> (replaced) [<a href="/pdf/2401.08876" title="Download PDF">pdf</a>, <a href="/format/2401.08876" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image  Labeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dongping Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chatzimparmpas%2C+A">Angelos Chatzimparmpas</a>, 
<a href="/search/cs?searchtype=author&query=Kamali%2C+N">Negar Kamali</a>, 
<a href="/search/cs?searchtype=author&query=Hullman%2C+J">Jessica Hullman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 11 figures, 9 tables. Accepted by ACM CHI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1181">[1181]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.09198" title="Abstract">arXiv:2401.09198</a> (replaced) [<a href="/pdf/2401.09198" title="Download PDF">pdf</a>, <a href="/format/2401.09198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Space and Time Continuous Physics Simulation From Partial Observations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Steeven%2C+J">Janny Steeven</a>, 
<a href="/search/cs?searchtype=author&query=Madiha%2C+N">Nadri Madiha</a>, 
<a href="/search/cs?searchtype=author&query=Julie%2C+D">Digne Julie</a>, 
<a href="/search/cs?searchtype=author&query=Christian%2C+W">Wolf Christian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project Page: <a href="https://continuous-pde.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1182">[1182]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.09395" title="Abstract">arXiv:2401.09395</a> (replaced) [<a href="/pdf/2401.09395" title="Download PDF">pdf</a>, <a href="/format/2401.09395" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Caught in the Quicksand of Reasoning, Far from AGI Summit: Evaluating  LLMs&#x27; Mathematical and Coding Competency through Ontology-guided  Interventions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hong%2C+P">Pengfei Hong</a>, 
<a href="/search/cs?searchtype=author&query=Ghosal%2C+D">Deepanway Ghosal</a>, 
<a href="/search/cs?searchtype=author&query=Majumder%2C+N">Navonil Majumder</a>, 
<a href="/search/cs?searchtype=author&query=Aditya%2C+S">Somak Aditya</a>, 
<a href="/search/cs?searchtype=author&query=Mihalcea%2C+R">Rada Mihalcea</a>, 
<a href="/search/cs?searchtype=author&query=Poria%2C+S">Soujanya Poria</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1183">[1183]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.09498" title="Abstract">arXiv:2401.09498</a> (replaced) [<a href="/pdf/2401.09498" title="Download PDF">pdf</a>, <a href="/format/2401.09498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Technical Report: On the Convergence of Gossip Learning in the Presence  of Node Inaccessibility
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+T">Tian Liu</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+Y">Yue Cui</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+X">Xueyang Hu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yecheng Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+B">Bo Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1184">[1184]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.09627" title="Abstract">arXiv:2401.09627</a> (replaced) [<a href="/pdf/2401.09627" title="Download PDF">pdf</a>, <a href="/ps/2401.09627" title="Download PostScript">ps</a>, <a href="/format/2401.09627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SymTC: A Symbiotic Transformer-CNN Net for Instance Segmentation of  Lumbar Spine MRI
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Chen%2C+J">Jiasong Chen</a>, 
<a href="/search/eess?searchtype=author&query=Qian%2C+L">Linchen Qian</a>, 
<a href="/search/eess?searchtype=author&query=Ma%2C+L">Linhai Ma</a>, 
<a href="/search/eess?searchtype=author&query=Urakov%2C+T">Timur Urakov</a>, 
<a href="/search/eess?searchtype=author&query=Gu%2C+W">Weiyong Gu</a>, 
<a href="/search/eess?searchtype=author&query=Liang%2C+L">Liang Liang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1185">[1185]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10019" title="Abstract">arXiv:2401.10019</a> (replaced) [<a href="/pdf/2401.10019" title="Download PDF">pdf</a>, <a href="/format/2401.10019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> R-Judge: Benchmarking Safety Risk Awareness for LLM Agents
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+T">Tongxin Yuan</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+Z">Zhiwei He</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+L">Lingzhong Dong</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yiming Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+R">Ruijie Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+T">Tian Xia</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+L">Lizhen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+B">Binglin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+F">Fangqi Li</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhuosheng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+R">Rui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+G">Gongshen Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1186">[1186]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10186" title="Abstract">arXiv:2401.10186</a> (replaced) [<a href="/pdf/2401.10186" title="Download PDF">pdf</a>, <a href="/format/2401.10186" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on  Data-to-Text Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kasner%2C+Z">Zden&#x11b;k Kasner</a>, 
<a href="/search/cs?searchtype=author&query=Du%C5%A1ek%2C+O">Ond&#x159;ej Du&#x161;ek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1187">[1187]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.10417" title="Abstract">arXiv:2401.10417</a> (replaced) [<a href="/pdf/2401.10417" title="Download PDF">pdf</a>, <a href="/format/2401.10417" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SSR: Spatial Sequential Hybrid Architecture for Latency Throughput  Tradeoff in Transformer Acceleration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhuang%2C+J">Jinming Zhuang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhuoping Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+S">Shixin Ji</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+H">Heng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Jones%2C+A+K">Alex K. Jones</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+J">Jingtong Hu</a>, 
<a href="/search/cs?searchtype=author&query=Shi%2C+Y">Yiyu Shi</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+P">Peipei Zhou</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2024 ACM/SIGDA International Symposium on Field Programmable Gate
  Arrays (FPGA '24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Programming Languages (cs.PL)

</div>
</div>
</dd>
<dt><a name="item1188">[1188]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11610" title="Abstract">arXiv:2401.11610</a> (replaced) [<a href="/pdf/2401.11610" title="Download PDF">pdf</a>, <a href="/ps/2401.11610" title="Download PostScript">ps</a>, <a href="/format/2401.11610" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Note on k-Planar and Min-k-Planar Drawings of Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Hlin%C4%9Bn%C3%BD%2C+P">Petr Hlin&#x11b;n&#xfd;</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item1189">[1189]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11665" title="Abstract">arXiv:2401.11665</a> (replaced) [<a href="/pdf/2401.11665" title="Download PDF">pdf</a>, <a href="/format/2401.11665" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Accelerating Approximate Thompson Sampling with Underdamped Langevin  Monte Carlo
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Zheng%2C+H">Haoyang Zheng</a>, 
<a href="/search/stat?searchtype=author&query=Deng%2C+W">Wei Deng</a>, 
<a href="/search/stat?searchtype=author&query=Moya%2C+C">Christian Moya</a>, 
<a href="/search/stat?searchtype=author&query=Lin%2C+G">Guang Lin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 52 pages, 2 figures, to appear in AISTATS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1190">[1190]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11772" title="Abstract">arXiv:2401.11772</a> (replaced) [<a href="/pdf/2401.11772" title="Download PDF">pdf</a>, <a href="/format/2401.11772" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LightDiC: A Simple yet Effective Approach for Large-scale Digraph  Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xunkai Li</a>, 
<a href="/search/cs?searchtype=author&query=Liao%2C+M">Meihao Liao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Z">Zhengyu Wu</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+D">Daohan Su</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wentao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+R">Rong-Hua Li</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+G">Guoren Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by VLDB 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Social and Information Networks (cs.SI)

</div>
</div>
</dd>
<dt><a name="item1191">[1191]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11880" title="Abstract">arXiv:2401.11880</a> (replaced) [<a href="/pdf/2401.11880" title="Download PDF">pdf</a>, <a href="/format/2401.11880" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PsySafe: A Comprehensive Framework for Psychological-based Attack,  Defense, and Evaluation of Multi-agent System Safety
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zaibin Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongting Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Lijun Li</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+H">Hongzhi Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+L">Lijun Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+H">Huchuan Lu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+F">Feng Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+Y">Yu Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+J">Jing Shao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item1192">[1192]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11911" title="Abstract">arXiv:2401.11911</a> (replaced) [<a href="/pdf/2401.11911" title="Download PDF">pdf</a>, <a href="/format/2401.11911" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Blinded by Generated Contexts: How Language Models Merge Generated and  Retrieved Contexts for Open-Domain QA?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tan%2C+H">Hexiang Tan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+F">Fei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wanli Yang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yuanzhuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cao%2C+Q">Qi Cao</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1193">[1193]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.11963" title="Abstract">arXiv:2401.11963</a> (replaced) [<a href="/pdf/2401.11963" title="Download PDF">pdf</a>, <a href="/format/2401.11963" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging Evolutionary Algorithms and Reinforcement Learning: A  Comprehensive Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+P">Pengyi Li</a>, 
<a href="/search/cs?searchtype=author&query=Hao%2C+J">Jianye Hao</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Hongyao Tang</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+X">Xian Fu</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Y">Yan Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+K">Ke Tang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1194">[1194]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.12097" title="Abstract">arXiv:2401.12097</a> (replaced) [<a href="/pdf/2401.12097" title="Download PDF">pdf</a>, <a href="/format/2401.12097" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Empirical Study of In-context Learning in LLMs for Machine  Translation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chitale%2C+P+A">Pranjal A. Chitale</a>, 
<a href="/search/cs?searchtype=author&query=Gala%2C+J">Jay Gala</a>, 
<a href="/search/cs?searchtype=author&query=Dabre%2C+R">Raj Dabre</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1195">[1195]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13480" title="Abstract">arXiv:2401.13480</a> (replaced) [<a href="/pdf/2401.13480" title="Download PDF">pdf</a>, <a href="/format/2401.13480" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Dynamics of (Not) Unfollowing Misinformation Spreaders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ashkinaze%2C+J">Joshua Ashkinaze</a>, 
<a href="/search/cs?searchtype=author&query=Gilbert%2C+E">Eric Gilbert</a>, 
<a href="/search/cs?searchtype=author&query=Budak%2C+C">Ceren Budak</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WWW 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY); Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item1196">[1196]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13586" title="Abstract">arXiv:2401.13586</a> (replaced) [<a href="/pdf/2401.13586" title="Download PDF">pdf</a>, <a href="/format/2401.13586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Instruction Fine-Tuning: Does Prompt Loss Matter?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huerta-Enochian%2C+M">Mathew Huerta-Enochian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages of content. 12 pages supporting. 30 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1197">[1197]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13601" title="Abstract">arXiv:2401.13601</a> (replaced) [<a href="/pdf/2401.13601" title="Download PDF">pdf</a>, <a href="/format/2401.13601" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MM-LLMs: Recent Advances in MultiModal Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Duzhen Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Y">Yahan Yu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+C">Chenxing Li</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+J">Jiahua Dong</a>, 
<a href="/search/cs?searchtype=author&query=Su%2C+D">Dan Su</a>, 
<a href="/search/cs?searchtype=author&query=Chu%2C+C">Chenhui Chu</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dong Yu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1198">[1198]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13789" title="Abstract">arXiv:2401.13789</a> (replaced) [<a href="/pdf/2401.13789" title="Download PDF">pdf</a>, <a href="/format/2401.13789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Unified Approach to Emotion Detection and Task-Oriented Dialogue  Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stricker%2C+A">Armand Stricker</a>, 
<a href="/search/cs?searchtype=author&query=Paroubek%2C+P">Patrick Paroubek</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted @ IWSDS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1199">[1199]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.13812" title="Abstract">arXiv:2401.13812</a> (replaced) [<a href="/pdf/2401.13812" title="Download PDF">pdf</a>, <a href="/ps/2401.13812" title="Download PostScript">ps</a>, <a href="/format/2401.13812" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal Queueing Regimes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Scarsini%2C+M">Marco Scarsini</a>, 
<a href="/search/econ?searchtype=author&query=Shmaya%2C+E">Eran Shmaya</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item1200">[1200]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.14579" title="Abstract">arXiv:2401.14579</a> (replaced) [<a href="/pdf/2401.14579" title="Download PDF">pdf</a>, <a href="/ps/2401.14579" title="Download PostScript">ps</a>, <a href="/format/2401.14579" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recognizing Multiple Ingredients in Food Images Using a  Single-Ingredient Classification Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fu%2C+K">Kun Fu</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+Y">Ying Dai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 9 pages, 21 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1201">[1201]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15118" title="Abstract">arXiv:2401.15118</a> (replaced) [<a href="/pdf/2401.15118" title="Download PDF">pdf</a>, <a href="/ps/2401.15118" title="Download PostScript">ps</a>, <a href="/format/2401.15118" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GeoDecoder: Empowering Multimodal Map Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qi%2C+F">Feng Qi</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+M">Mian Dai</a>, 
<a href="/search/cs?searchtype=author&query=Zheng%2C+Z">Zixian Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+C">Chao Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1202">[1202]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15356" title="Abstract">arXiv:2401.15356</a> (replaced) [<a href="/pdf/2401.15356" title="Download PDF">pdf</a>, <a href="/format/2401.15356" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Statistical Framework for Measuring AI Reliance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Ziyang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+Y">Yifan Wu</a>, 
<a href="/search/cs?searchtype=author&query=Hartline%2C+J">Jason Hartline</a>, 
<a href="/search/cs?searchtype=author&query=Hullman%2C+J">Jessica Hullman</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item1203">[1203]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15456" title="Abstract">arXiv:2401.15456</a> (replaced) [<a href="/pdf/2401.15456" title="Download PDF">pdf</a>, <a href="/ps/2401.15456" title="Download PostScript">ps</a>, <a href="/format/2401.15456" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Product Mixing in Compact Lie Groups
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ellis%2C+D">David Ellis</a>, 
<a href="/search/math?searchtype=author&query=Kindler%2C+G">Guy Kindler</a>, 
<a href="/search/math?searchtype=author&query=Lifshitz%2C+N">Noam Lifshitz</a>, 
<a href="/search/math?searchtype=author&query=Minzer%2C+D">Dor Minzer</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Further typos corrected
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Combinatorics (math.CO)</span>; Computational Complexity (cs.CC); Group Theory (math.GR); Probability (math.PR)

</div>
</div>
</dd>
<dt><a name="item1204">[1204]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15498" title="Abstract">arXiv:2401.15498</a> (replaced) [<a href="/pdf/2401.15498" title="Download PDF">pdf</a>, <a href="/format/2401.15498" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Do We Need Language-Specific Fact-Checking Models? The Case of Chinese
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Caiqi Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Guo%2C+Z">Zhijiang Guo</a>, 
<a href="/search/cs?searchtype=author&query=Vlachos%2C+A">Andreas Vlachos</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1205">[1205]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15635" title="Abstract">arXiv:2401.15635</a> (replaced) [<a href="/pdf/2401.15635" title="Download PDF">pdf</a>, <a href="/format/2401.15635" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> RecDCL: Dual Contrastive Learning for Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dan Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Geng%2C+Y">Yangliao Geng</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+W">Wenwen Gong</a>, 
<a href="/search/cs?searchtype=author&query=Qi%2C+Z">Zhongang Qi</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhiyu Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xing Tang</a>, 
<a href="/search/cs?searchtype=author&query=Shan%2C+Y">Ying Shan</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+Y">Yuxiao Dong</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+J">Jie Tang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to WWW 2024
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proceedings of TheWebConf 2024 (WWW '24), May 13--17, 2024,
  Singapore
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1206">[1206]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15847" title="Abstract">arXiv:2401.15847</a> (replaced) [<a href="/pdf/2401.15847" title="Download PDF">pdf</a>, <a href="/format/2401.15847" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Muffin or Chihuahua? Challenging Large Vision-Language Models with  Multipanel VQA
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fan%2C+Y">Yue Fan</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jing Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+K">Kaiwen Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Q">Qianqi Yan</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+S">Shan Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Kuo%2C+C">Ching-Chen Kuo</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+X">Xinze Guan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X+E">Xin Eric Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1207">[1207]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15884" title="Abstract">arXiv:2401.15884</a> (replaced) [<a href="/pdf/2401.15884" title="Download PDF">pdf</a>, <a href="/format/2401.15884" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Corrective Retrieval Augmented Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yan%2C+S">Shi-Qi Yan</a>, 
<a href="/search/cs?searchtype=author&query=Gu%2C+J">Jia-Chen Gu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Y">Yun Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Ling%2C+Z">Zhen-Hua Ling</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1208">[1208]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.15947" title="Abstract">arXiv:2401.15947</a> (replaced) [<a href="/pdf/2401.15947" title="Download PDF">pdf</a>, <a href="/format/2401.15947" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MoE-LLaVA: Mixture of Experts for Large Vision-Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lin%2C+B">Bin Lin</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+Z">Zhenyu Tang</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+Y">Yang Ye</a>, 
<a href="/search/cs?searchtype=author&query=Cui%2C+J">Jiaxi Cui</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+B">Bin Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+P">Peng Jin</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+J">Jinfa Huang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Junwu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+M">Munan Ning</a>, 
<a href="/search/cs?searchtype=author&query=Yuan%2C+L">Li Yuan</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> update table 5
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1209">[1209]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16268" title="Abstract">arXiv:2401.16268</a> (replaced) [<a href="/pdf/2401.16268" title="Download PDF">pdf</a>, <a href="/ps/2401.16268" title="Download PostScript">ps</a>, <a href="/format/2401.16268" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A.I. In All The Wrong Places
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=B%C3%B6hlen%2C+M">Marc B&#xf6;hlen</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Ruolin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+X">Xiaoxu Dong</a>, 
<a href="/search/cs?searchtype=author&query=Gopaladinne%2C+S">Srikar Gopaladinne</a>, 
<a href="/search/cs?searchtype=author&query=Gorla%2C+H">Hemanth Gorla</a>, 
<a href="/search/cs?searchtype=author&query=Kandukuri%2C+D">Divya Kandukuri</a>, 
<a href="/search/cs?searchtype=author&query=Mansfield%2C+S">Sean Mansfield</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 3 tables, 4 images
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item1210">[1210]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16342" title="Abstract">arXiv:2401.16342</a> (replaced) [<a href="/pdf/2401.16342" title="Download PDF">pdf</a>, <a href="/format/2401.16342" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On Achievable Rates for the Shotgun Sequencing Channel with Erasures
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Narayanan%2C+H">Hrishi Narayanan</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+P">Prasad Krishnan</a>, 
<a href="/search/cs?searchtype=author&query=Parekh%2C+N">Nita Parekh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item1211">[1211]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16457" title="Abstract">arXiv:2401.16457</a> (replaced) [<a href="/pdf/2401.16457" title="Download PDF">pdf</a>, <a href="/format/2401.16457" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Effective Controllable Bias Mitigation for Classification and Retrieval  using Gate Adapters
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Masoudian%2C+S">Shahed Masoudian</a>, 
<a href="/search/cs?searchtype=author&query=Volaucnik%2C+C">Cornelia Volaucnik</a>, 
<a href="/search/cs?searchtype=author&query=Schedl%2C+M">Markus Schedl</a>, 
<a href="/search/cs?searchtype=author&query=Rekabsaz%2C+N">Navid Rekabsaz</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Paper is accepted to main proceedings of EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1212">[1212]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16578" title="Abstract">arXiv:2401.16578</a> (replaced) [<a href="/pdf/2401.16578" title="Download PDF">pdf</a>, <a href="/format/2401.16578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Leveraging Professional Radiologists&#x27; Expertise to Enhance LLMs&#x27;  Evaluation for Radiology Reports
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhu%2C+Q">Qingqing Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiuying Chen</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Q">Qiao Jin</a>, 
<a href="/search/cs?searchtype=author&query=Hou%2C+B">Benjamin Hou</a>, 
<a href="/search/cs?searchtype=author&query=Mathai%2C+T+S">Tejas Sudharshan Mathai</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+P">Pritam Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xin Gao</a>, 
<a href="/search/cs?searchtype=author&query=Summers%2C+R+M">Ronald M Summers</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zhiyong Lu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1213">[1213]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16699" title="Abstract">arXiv:2401.16699</a> (replaced) [<a href="/pdf/2401.16699" title="Download PDF">pdf</a>, <a href="/format/2401.16699" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards Unified Interactive Visual Grounding in The Wild
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Xu%2C+J">Jie Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+H">Hanbo Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Si%2C+Q">Qingyi Si</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yifeng Li</a>, 
<a href="/search/cs?searchtype=author&query=Lan%2C+X">Xuguang Lan</a>, 
<a href="/search/cs?searchtype=author&query=Kong%2C+T">Tao Kong</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to ICRA 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>

</div>
</div>
</dd>
<dt><a name="item1214">[1214]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.16754" title="Abstract">arXiv:2401.16754</a> (replaced) [<a href="/pdf/2401.16754" title="Download PDF">pdf</a>, <a href="/format/2401.16754" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> AI Oversight and Human Mistakes: Evidence from Centre Court
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Almog%2C+D">David Almog</a>, 
<a href="/search/cs?searchtype=author&query=Gauriot%2C+R">Romain Gauriot</a>, 
<a href="/search/cs?searchtype=author&query=Page%2C+L">Lionel Page</a>, 
<a href="/search/cs?searchtype=author&query=Martin%2C+D">Daniel Martin</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computers and Society (cs.CY); General Economics (econ.GN)

</div>
</div>
</dd>
<dt><a name="item1215">[1215]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17043" title="Abstract">arXiv:2401.17043</a> (replaced) [<a href="/pdf/2401.17043" title="Download PDF">pdf</a>, <a href="/format/2401.17043" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented  Generation of Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lyu%2C+Y">Yuanjie Lyu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zhiyu Li</a>, 
<a href="/search/cs?searchtype=author&query=Niu%2C+S">Simin Niu</a>, 
<a href="/search/cs?searchtype=author&query=Xiong%2C+F">Feiyu Xiong</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+B">Bo Tang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenjin Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+H">Hao Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+H">Huanyong Liu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+T">Tong Xu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+E">Enhong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yi Luo</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+P">Peng Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+H">Haiying Deng</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhonghao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+Z">Zijia Lu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 26 Pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1216">[1216]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17543" title="Abstract">arXiv:2401.17543</a> (replaced) [<a href="/pdf/2401.17543" title="Download PDF">pdf</a>, <a href="/format/2401.17543" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fr&#xe9;chet Distance for Offline Evaluation of Information Retrieval  Systems with Sparse Labels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Arabzadeh%2C+N">Negar Arabzadeh</a>, 
<a href="/search/cs?searchtype=author&query=Clarke%2C+C+L+A">Charles L. A. Clarke</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item1217">[1217]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2401.17609" title="Abstract">arXiv:2401.17609</a> (replaced) [<a href="/pdf/2401.17609" title="Download PDF">pdf</a>, <a href="/format/2401.17609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LaneGraph2Seq: Lane Topology Extraction with Language Model via  Vertex-Edge Encoding and Connectivity Enhancement
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Peng%2C+R">Renyuan Peng</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+X">Xinyue Cai</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Lu%2C+J">Jiachen Lu</a>, 
<a href="/search/cs?searchtype=author&query=Wen%2C+F">Feng Wen</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+L">Li Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> AAAI 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1218">[1218]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.00627" title="Abstract">arXiv:2402.00627</a> (replaced) [<a href="/pdf/2402.00627" title="Download PDF">pdf</a>, <a href="/format/2402.00627" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> CapHuman: Capture Your Moments in Parallel Universes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+C">Chao Liang</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+F">Fan Ma</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+L">Linchao Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+Y">Yingying Deng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yi Yang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Project page: <a href="https://caphuman.github.io/">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1219">[1219]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01109" title="Abstract">arXiv:2402.01109</a> (replaced) [<a href="/pdf/2402.01109" title="Download PDF">pdf</a>, <a href="/format/2402.01109" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Vaccine: Perturbation-aware Alignment for Large Language Model
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huang%2C+T">Tiansheng Huang</a>, 
<a href="/search/cs?searchtype=author&query=Hu%2C+S">Sihao Hu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+L">Ling Liu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1220">[1220]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01341" title="Abstract">arXiv:2402.01341</a> (replaced) [<a href="/pdf/2402.01341" title="Download PDF">pdf</a>, <a href="/ps/2402.01341" title="Download PostScript">ps</a>, <a href="/format/2402.01341" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fundamental Properties of Causal Entropy and Information Gain
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Simoes%2C+F+N+F+Q">Francisco N. F. Q. Simoes</a>, 
<a href="/search/cs?searchtype=author&query=Dastani%2C+M">Mehdi Dastani</a>, 
<a href="/search/cs?searchtype=author&query=van+Ommen%2C+T">Thijs van Ommen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> In Proceedings of the conference CLeaR (Causal Learning and Reasoning) 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Theory (cs.IT); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1221">[1221]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01586" title="Abstract">arXiv:2402.01586</a> (replaced) [<a href="/pdf/2402.01586" title="Download PDF">pdf</a>, <a href="/format/2402.01586" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent  Constitution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hua%2C+W">Wenyue Hua</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+X">Xianjun Yang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+Z">Zelong Li</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wei Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongfeng Zhang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 figures, 5 tables, comments and suggestions are welcome
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item1222">[1222]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01644" title="Abstract">arXiv:2402.01644</a> (replaced) [<a href="/pdf/2402.01644" title="Download PDF">pdf</a>, <a href="/format/2402.01644" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Holistic Approach for Equity-aware Carbon Reduction of Ridesharing  Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sahebdel%2C+M">Mahsa Sahebdel</a>, 
<a href="/search/cs?searchtype=author&query=Zeynali%2C+A">Ali Zeynali</a>, 
<a href="/search/cs?searchtype=author&query=Bashir%2C+N">Noman Bashir</a>, 
<a href="/search/cs?searchtype=author&query=Shenoy%2C+P">Prashant Shenoy</a>, 
<a href="/search/cs?searchtype=author&query=Hajiesmaili%2C+M">Mohammad Hajiesmaili</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Systems and Control (eess.SY)</span>

</div>
</div>
</dd>
<dt><a name="item1223">[1223]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01741" title="Abstract">arXiv:2402.01741</a> (replaced) [<a href="/pdf/2402.01741" title="Download PDF">pdf</a>, <a href="/ps/2402.01741" title="Download PostScript">ps</a>, <a href="/format/2402.01741" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Development and Testing of a Novel Large Language Model-Based Clinical  Decision Support Systems for Medication Safety in 12 Clinical Specialties
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ong%2C+J+C+L">Jasmine Chiat Ling Ong</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+L">Liyuan Jin</a>, 
<a href="/search/cs?searchtype=author&query=Elangovan%2C+K">Kabilan Elangovan</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+G+Y+S">Gilbert Yong San Lim</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+D+Y+Z">Daniel Yan Zheng Lim</a>, 
<a href="/search/cs?searchtype=author&query=Sng%2C+G+G+R">Gerald Gui Ren Sng</a>, 
<a href="/search/cs?searchtype=author&query=Ke%2C+Y">Yuhe Ke</a>, 
<a href="/search/cs?searchtype=author&query=Tung%2C+J+Y+M">Joshua Yi Min Tung</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+R+J">Ryan Jian Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Koh%2C+C+M+Y">Christopher Ming Yao Koh</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+K+Z+H">Keane Zhi Hao Lee</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+X">Xiang Chen</a>, 
<a href="/search/cs?searchtype=author&query=Chng%2C+J+K">Jack Kian Chng</a>, 
<a href="/search/cs?searchtype=author&query=Than%2C+A">Aung Than</a>, 
<a href="/search/cs?searchtype=author&query=Goh%2C+K+J">Ken Junyang Goh</a>, 
<a href="/search/cs?searchtype=author&query=Ting%2C+D+S+W">Daniel Shu Wei Ting</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1224">[1224]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01944" title="Abstract">arXiv:2402.01944</a> (replaced) [<a href="/pdf/2402.01944" title="Download PDF">pdf</a>, <a href="/format/2402.01944" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Guarantees in Software Security
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=B%C3%B6hme%2C+M">Marcel B&#xf6;hme</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages. Pre-print (under submission; single-blind). Feedback appreciated
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Software Engineering (cs.SE)

</div>
</div>
</dd>
<dt><a name="item1225">[1225]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.01967" title="Abstract">arXiv:2402.01967</a> (replaced) [<a href="/pdf/2402.01967" title="Download PDF">pdf</a>, <a href="/format/2402.01967" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> MasonPerplexity at Multimodal Hate Speech Event Detection 2024: Hate  Speech and Target Detection Using Transformer Ensembles
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ganguly%2C+A">Amrita Ganguly</a>, 
<a href="/search/cs?searchtype=author&query=Emran%2C+A+N+B">Al Nahian Bin Emran</a>, 
<a href="/search/cs?searchtype=author&query=Puspo%2C+S+S+C">Sadiya Sayara Chowdhury Puspo</a>, 
<a href="/search/cs?searchtype=author&query=Raihan%2C+M+N">Md Nishat Raihan</a>, 
<a href="/search/cs?searchtype=author&query=Goswami%2C+D">Dhiman Goswami</a>, 
<a href="/search/cs?searchtype=author&query=Zampieri%2C+M">Marcos Zampieri</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1226">[1226]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02041" title="Abstract">arXiv:2402.02041</a> (replaced) [<a href="/pdf/2402.02041" title="Download PDF">pdf</a>, <a href="/ps/2402.02041" title="Download PostScript">ps</a>, <a href="/format/2402.02041" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> $&#x3b1;$-Divergence Loss Function for Neural Density Ratio Estimation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kitazawa%2C+Y">Yoshiaki Kitazawa</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> $\mathcal{T}_{\text{Lip}}$ in Theorem 7.1 (Theorem B.15.) was changed to the set of all locally Lipschitz continuous functions. In the previous version, $\mathcal{T}_{\text{Lip}}$ was defined as the set of all Lipschitz continuous functions, which is unsuitable for the statement of case (ii) in the theorem
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1227">[1227]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02047" title="Abstract">arXiv:2402.02047</a> (replaced) [<a href="/pdf/2402.02047" title="Download PDF">pdf</a>, <a href="/format/2402.02047" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Calibration and Correctness of Language Models for Code
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Spiess%2C+C">Claudio Spiess</a>, 
<a href="/search/cs?searchtype=author&query=Gros%2C+D">David Gros</a>, 
<a href="/search/cs?searchtype=author&query=Pai%2C+K+S">Kunal Suresh Pai</a>, 
<a href="/search/cs?searchtype=author&query=Pradel%2C+M">Michael Pradel</a>, 
<a href="/search/cs?searchtype=author&query=Rabin%2C+M+R+I">Md Rafiqul Islam Rabin</a>, 
<a href="/search/cs?searchtype=author&query=Alipour%2C+A">Amin Alipour</a>, 
<a href="/search/cs?searchtype=author&query=Jha%2C+S">Susmit Jha</a>, 
<a href="/search/cs?searchtype=author&query=Devanbu%2C+P">Prem Devanbu</a>, 
<a href="/search/cs?searchtype=author&query=Ahmed%2C+T">Toufique Ahmed</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1228">[1228]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02130" title="Abstract">arXiv:2402.02130</a> (replaced) [<a href="/pdf/2402.02130" title="Download PDF">pdf</a>, <a href="/format/2402.02130" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rendering Graphs for Graph Reasoning in Multimodal Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wei%2C+Y">Yanbin Wei</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+S">Shuai Fu</a>, 
<a href="/search/cs?searchtype=author&query=Jiang%2C+W">Weisen Jiang</a>, 
<a href="/search/cs?searchtype=author&query=Kwok%2C+J+T">James T. Kwok</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yu Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1229">[1229]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02171" title="Abstract">arXiv:2402.02171</a> (replaced) [<a href="/pdf/2402.02171" title="Download PDF">pdf</a>, <a href="/format/2402.02171" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Off-Policy Evaluation of Slate Bandit Policies via Optimizing  Abstraction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Kiyohara%2C+H">Haruka Kiyohara</a>, 
<a href="/search/stat?searchtype=author&query=Nomura%2C+M">Masahiro Nomura</a>, 
<a href="/search/stat?searchtype=author&query=Saito%2C+Y">Yuta Saito</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> WWW2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1230">[1230]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02441" title="Abstract">arXiv:2402.02441</a> (replaced) [<a href="/pdf/2402.02441" title="Download PDF">pdf</a>, <a href="/format/2402.02441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TopoX: A Suite of Python Packages for Machine Learning on Topological  Domains
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hajij%2C+M">Mustafa Hajij</a>, 
<a href="/search/cs?searchtype=author&query=Papillon%2C+M">Mathilde Papillon</a>, 
<a href="/search/cs?searchtype=author&query=Frantzen%2C+F">Florian Frantzen</a>, 
<a href="/search/cs?searchtype=author&query=Agerberg%2C+J">Jens Agerberg</a>, 
<a href="/search/cs?searchtype=author&query=AlJabea%2C+I">Ibrahem AlJabea</a>, 
<a href="/search/cs?searchtype=author&query=Ballester%2C+R">Ruben Ballester</a>, 
<a href="/search/cs?searchtype=author&query=Battiloro%2C+C">Claudio Battiloro</a>, 
<a href="/search/cs?searchtype=author&query=Bern%C3%A1rdez%2C+G">Guillermo Bern&#xe1;rdez</a>, 
<a href="/search/cs?searchtype=author&query=Birdal%2C+T">Tolga Birdal</a>, 
<a href="/search/cs?searchtype=author&query=Brent%2C+A">Aiden Brent</a>, 
<a href="/search/cs?searchtype=author&query=Chin%2C+P">Peter Chin</a>, 
<a href="/search/cs?searchtype=author&query=Escalera%2C+S">Sergio Escalera</a>, 
<a href="/search/cs?searchtype=author&query=Fiorellino%2C+S">Simone Fiorellino</a>, 
<a href="/search/cs?searchtype=author&query=Gardaa%2C+O+H">Odin Hoff Gardaa</a>, 
<a href="/search/cs?searchtype=author&query=Gopalakrishnan%2C+G">Gurusankar Gopalakrishnan</a>, 
<a href="/search/cs?searchtype=author&query=Govil%2C+D">Devendra Govil</a>, 
<a href="/search/cs?searchtype=author&query=Hoppe%2C+J">Josef Hoppe</a>, 
<a href="/search/cs?searchtype=author&query=Karri%2C+M+R">Maneel Reddy Karri</a>, 
<a href="/search/cs?searchtype=author&query=Khouja%2C+J">Jude Khouja</a>, 
<a href="/search/cs?searchtype=author&query=Lecha%2C+M">Manuel Lecha</a>, 
<a href="/search/cs?searchtype=author&query=Livesay%2C+N">Neal Livesay</a>, 
<a href="/search/cs?searchtype=author&query=Mei%C3%9Fner%2C+J">Jan Mei&#xdf;ner</a>, 
<a href="/search/cs?searchtype=author&query=Mukherjee%2C+S">Soham Mukherjee</a>, 
<a href="/search/cs?searchtype=author&query=Nikitin%2C+A">Alexander Nikitin</a>, 
<a href="/search/cs?searchtype=author&query=Papamarkou%2C+T">Theodore Papamarkou</a>, 
<a href="/search/cs?searchtype=author&query=Pr%C3%ADlepok%2C+J">Jaro Pr&#xed;lepok</a>, 
<a href="/search/cs?searchtype=author&query=Ramamurthy%2C+K+N">Karthikeyan Natesan Ramamurthy</a>, 
<a href="/search/cs?searchtype=author&query=Rosen%2C+P">Paul Rosen</a>, 
<a href="/search/cs?searchtype=author&query=Guzm%C3%A1n-S%C3%A1enz%2C+A">Aldo Guzm&#xe1;n-S&#xe1;enz</a>, 
<a href="/search/cs?searchtype=author&query=Salatiello%2C+A">Alessandro Salatiello</a>, 
<a href="/search/cs?searchtype=author&query=Samaga%2C+S+N">Shreyas N. Samaga</a>, 
<a href="/search/cs?searchtype=author&query=Scardapane%2C+S">Simone Scardapane</a>, 
<a href="/search/cs?searchtype=author&query=Schaub%2C+M+T">Michael T. Schaub</a>, 
<a href="/search/cs?searchtype=author&query=Scofano%2C+L">Luca Scofano</a>, 
<a href="/search/cs?searchtype=author&query=Spinelli%2C+I">Indro Spinelli</a>, 
<a href="/search/cs?searchtype=author&query=Telyatnikov%2C+L">Lev Telyatnikov</a>, 
<a href="/search/cs?searchtype=author&query=Truong%2C+Q">Quang Truong</a>, 
<a href="/search/cs?searchtype=author&query=Walters%2C+R">Robin Walters</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+M">Maosheng Yang</a>, 
<a href="/search/cs?searchtype=author&query=Zaghen%2C+O">Olga Zaghen</a>, 
<a href="/search/cs?searchtype=author&query=Zamzmi%2C+G">Ghada Zamzmi</a>, 
<a href="/search/cs?searchtype=author&query=Zia%2C+A">Ali Zia</a>, 
<a href="/search/cs?searchtype=author&query=Miolane%2C+N">Nina Miolane</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Mathematical Software (cs.MS); Computation (stat.CO)

</div>
</div>
</dd>
<dt><a name="item1231">[1231]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.02996" title="Abstract">arXiv:2402.02996</a> (replaced) [<a href="/pdf/2402.02996" title="Download PDF">pdf</a>, <a href="/format/2402.02996" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Text-Guided Image Clustering
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Stephan%2C+A">Andreas Stephan</a>, 
<a href="/search/cs?searchtype=author&query=Miklautz%2C+L">Lukas Miklautz</a>, 
<a href="/search/cs?searchtype=author&query=Sidak%2C+K">Kevin Sidak</a>, 
<a href="/search/cs?searchtype=author&query=Wahle%2C+J+P">Jan Philip Wahle</a>, 
<a href="/search/cs?searchtype=author&query=Gipp%2C+B">Bela Gipp</a>, 
<a href="/search/cs?searchtype=author&query=Plant%2C+C">Claudia Plant</a>, 
<a href="/search/cs?searchtype=author&query=Roth%2C+B">Benjamin Roth</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted to EACL 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1232">[1232]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03074" title="Abstract">arXiv:2402.03074</a> (replaced) [<a href="/pdf/2402.03074" title="Download PDF">pdf</a>, <a href="/format/2402.03074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A moment-based Hermite WENO scheme with unified stencils for hyperbolic  conservation laws
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Fan%2C+C">Chuan Fan</a>, 
<a href="/search/math?searchtype=author&query=Qiu%2C+J">Jianxian Qiu</a>, 
<a href="/search/math?searchtype=author&query=Zhao%2C+Z">Zhuang Zhao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 pages, 14 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>

</div>
</div>
</dd>
<dt><a name="item1233">[1233]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03149" title="Abstract">arXiv:2402.03149</a> (replaced) [<a href="/pdf/2402.03149" title="Download PDF">pdf</a>, <a href="/format/2402.03149" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comparative Analysis of Microrings Based Incoherent Photonic GEMM  Accelerators
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Vatsavai%2C+S+S">Sairam Sri Vatsavai</a>, 
<a href="/search/cs?searchtype=author&query=Karempudi%2C+V+S+P">Venkata Sai Praneeth Karempudi</a>, 
<a href="/search/cs?searchtype=author&query=Alo%2C+O+A">Oluwaseun Adewunmi Alo</a>, 
<a href="/search/cs?searchtype=author&query=Thakkar%2C+I">Ishan Thakkar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To Appear at ISQED 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Hardware Architecture (cs.AR)</span>; Emerging Technologies (cs.ET); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE)

</div>
</div>
</dd>
<dt><a name="item1234">[1234]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03358" title="Abstract">arXiv:2402.03358</a> (replaced) [<a href="/pdf/2402.03358" title="Download PDF">pdf</a>, <a href="/format/2402.03358" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening,  and Condensation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Hashemi%2C+M">Mohammad Hashemi</a>, 
<a href="/search/cs?searchtype=author&query=Gong%2C+S">Shengbo Gong</a>, 
<a href="/search/cs?searchtype=author&query=Ni%2C+J">Juntong Ni</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+W">Wenqi Fan</a>, 
<a href="/search/cs?searchtype=author&query=Prakash%2C+B+A">B. Aditya Prakash</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+W">Wei Jin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 3 tables, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Artificial Intelligence (cs.AI); Data Structures and Algorithms (cs.DS); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1235">[1235]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.03630" title="Abstract">arXiv:2402.03630</a> (replaced) [<a href="/pdf/2402.03630" title="Download PDF">pdf</a>, <a href="/format/2402.03630" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Enhancing LLM-Based Coding Tools through Native Integration of  IDE-Derived Static Context
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+Y">Yichen Li</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+Y">Yun Peng</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+Y">Yintong Huo</a>, 
<a href="/search/cs?searchtype=author&query=Lyu%2C+M+R">Michael R. Lyu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1236">[1236]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04051" title="Abstract">arXiv:2402.04051</a> (replaced) [<a href="/pdf/2402.04051" title="Download PDF">pdf</a>, <a href="/format/2402.04051" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Analysis of Linear Mode Connectivity via Permutation-Based Weight  Matching
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ito%2C+A">Akira Ito</a>, 
<a href="/search/cs?searchtype=author&query=Yamada%2C+M">Masanori Yamada</a>, 
<a href="/search/cs?searchtype=author&query=Kumagai%2C+A">Atsutoshi Kumagai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1237">[1237]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04296" title="Abstract">arXiv:2402.04296</a> (replaced) [<a href="/e-print/2402.04296" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LightHGNN: Distilling Hypergraph Neural Networks into MLPs for  $100\times$ Faster Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+Y">Yifan Feng</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+Y">Yihe Luo</a>, 
<a href="/search/cs?searchtype=author&query=Ying%2C+S">Shihui Ying</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+Y">Yue Gao</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Some details are missing. The method of this paper is not complete
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1238">[1238]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04588" title="Abstract">arXiv:2402.04588</a> (replaced) [<a href="/pdf/2402.04588" title="Download PDF">pdf</a>, <a href="/format/2402.04588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised  Fine-tuning Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wang%2C+H">Haoyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+S">Shuo Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+Y">Yukun Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xujia Wang</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Z">Zhiyu Yang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+Y">Yuzhuang Xu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhenghao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+L">Liner Yang</a>, 
<a href="/search/cs?searchtype=author&query=Ding%2C+N">Ning Ding</a>, 
<a href="/search/cs?searchtype=author&query=Han%2C+X">Xu Han</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+Z">Zhiyuan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+M">Maosong Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in Progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1239">[1239]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04602" title="Abstract">arXiv:2402.04602</a> (replaced) [<a href="/pdf/2402.04602" title="Download PDF">pdf</a>, <a href="/format/2402.04602" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Quantile Regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Shen%2C+Y">Yinan Shen</a>, 
<a href="/search/math?searchtype=author&query=Xia%2C+D">Dong Xia</a>, 
<a href="/search/math?searchtype=author&query=Zhou%2C+W">Wen-Xin Zhou</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Information Theory (cs.IT); Methodology (stat.ME)

</div>
</div>
</dd>
<dt><a name="item1240">[1240]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04615" title="Abstract">arXiv:2402.04615</a> (replaced) [<a href="/pdf/2402.04615" title="Download PDF">pdf</a>, <a href="/format/2402.04615" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ScreenAI: A Vision-Language Model for UI and Infographics Understanding
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Baechler%2C+G">Gilles Baechler</a>, 
<a href="/search/cs?searchtype=author&query=Sunkara%2C+S">Srinivas Sunkara</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Maria Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zubach%2C+F">Fedir Zubach</a>, 
<a href="/search/cs?searchtype=author&query=Mansoor%2C+H">Hassan Mansoor</a>, 
<a href="/search/cs?searchtype=author&query=Etter%2C+V">Vincent Etter</a>, 
<a href="/search/cs?searchtype=author&query=C%C4%83rbune%2C+V">Victor C&#x103;rbune</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+J">Jason Lin</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jindong Chen</a>, 
<a href="/search/cs?searchtype=author&query=Sharma%2C+A">Abhanshu Sharma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Revision notes: 1) In Appendix I, added dataset location for ScreenQA Short in Appendix I. 2) In Table 4, updated evaluation numbers for Screen Annotation and Complex Screen QA benchmarks as the datasets are updated. 3) Updated Figure 4 to reflect the changes in evaluation numbers described in 2). 4) Minor revisions in other places
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1241">[1241]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04821" title="Abstract">arXiv:2402.04821</a> (replaced) [<a href="/pdf/2402.04821" title="Download PDF">pdf</a>, <a href="/format/2402.04821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> E(3)-Equivariant Mesh Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Trang%2C+T">Thuan Trang</a>, 
<a href="/search/cs?searchtype=author&query=Ngo%2C+N+K">Nhat Khang Ngo</a>, 
<a href="/search/cs?searchtype=author&query=Levy%2C+D">Daniel Levy</a>, 
<a href="/search/cs?searchtype=author&query=Vo%2C+T+N">Thieu N. Vo</a>, 
<a href="/search/cs?searchtype=author&query=Ravanbakhsh%2C+S">Siamak Ravanbakhsh</a>, 
<a href="/search/cs?searchtype=author&query=Hy%2C+T+S">Truong Son Hy</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1242">[1242]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.04999" title="Abstract">arXiv:2402.04999</a> (replaced) [<a href="/pdf/2402.04999" title="Download PDF">pdf</a>, <a href="/format/2402.04999" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Longitudinal Study of Italian and French Reddit Conversations Around  the Russian Invasion of Ukraine
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Corso%2C+F">Francesco Corso</a>, 
<a href="/search/cs?searchtype=author&query=Russo%2C+G">Giuseppe Russo</a>, 
<a href="/search/cs?searchtype=author&query=Pierri%2C+F">Francesco Pierri</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages, 10 figures, Accepted at ACM WEBSCI'24 - Update: Added a reference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Social and Information Networks (cs.SI)</span>; Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1243">[1243]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05119" title="Abstract">arXiv:2402.05119</a> (replaced) [<a href="/pdf/2402.05119" title="Download PDF">pdf</a>, <a href="/format/2402.05119" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Closer Look at the Limitations of Instruction Tuning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ghosh%2C+S">Sreyan Ghosh</a>, 
<a href="/search/cs?searchtype=author&query=Evuru%2C+C+K+R">Chandra Kiran Reddy Evuru</a>, 
<a href="/search/cs?searchtype=author&query=Kumar%2C+S">Sonal Kumar</a>, 
<a href="/search/cs?searchtype=author&query=S%2C+R">Ramaneswaran S</a>, 
<a href="/search/cs?searchtype=author&query=Aneja%2C+D">Deepali Aneja</a>, 
<a href="/search/cs?searchtype=author&query=Jin%2C+Z">Zeyu Jin</a>, 
<a href="/search/cs?searchtype=author&query=Duraiswami%2C+R">Ramani Duraiswami</a>, 
<a href="/search/cs?searchtype=author&query=Manocha%2C+D">Dinesh Manocha</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1244">[1244]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05396" title="Abstract">arXiv:2402.05396</a> (replaced) [<a href="/pdf/2402.05396" title="Download PDF">pdf</a>, <a href="/format/2402.05396" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph  Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Deng%2C+G">Gangda Deng</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+H">Hongkuan Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+H">Hanqing Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Xia%2C+Y">Yinglong Xia</a>, 
<a href="/search/cs?searchtype=author&query=Leung%2C+C">Christopher Leung</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianbo Li</a>, 
<a href="/search/cs?searchtype=author&query=Kannan%2C+R">Rajgopal Kannan</a>, 
<a href="/search/cs?searchtype=author&query=Prasanna%2C+V">Viktor Prasanna</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IPDPS 2024
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1245">[1245]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.05493" title="Abstract">arXiv:2402.05493</a> (replaced) [<a href="/pdf/2402.05493" title="Download PDF">pdf</a>, <a href="/format/2402.05493" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Investigating White-Box Attacks for On-Device Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhou%2C+M">Mingyi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Gao%2C+X">Xiang Gao</a>, 
<a href="/search/cs?searchtype=author&query=Wu%2C+J">Jing Wu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kui Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Hailong Sun</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Li Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The International Conference on Software Engineering 2024 (ICSE'24)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item1246">[1246]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06025" title="Abstract">arXiv:2402.06025</a> (replaced) [<a href="/pdf/2402.06025" title="Download PDF">pdf</a>, <a href="/format/2402.06025" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Doing Experiments and Revising Rules with Natural Language and  Probabilistic Reasoning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Piriyakulkij%2C+T">Top Piriyakulkij</a>, 
<a href="/search/cs?searchtype=author&query=Ellis%2C+K">Kevin Ellis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1247">[1247]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06122" title="Abstract">arXiv:2402.06122</a> (replaced) [<a href="/pdf/2402.06122" title="Download PDF">pdf</a>, <a href="/format/2402.06122" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Peeking with PEAK: Sequential, Nonparametric Composite Hypothesis Tests  for Means of Multiple Data Streams
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/stat?searchtype=author&query=Cho%2C+B">Brian Cho</a>, 
<a href="/search/stat?searchtype=author&query=Gan%2C+K">Kyra Gan</a>, 
<a href="/search/stat?searchtype=author&query=Kallus%2C+N">Nathan Kallus</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1248">[1248]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06262" title="Abstract">arXiv:2402.06262</a> (replaced) [<a href="/pdf/2402.06262" title="Download PDF">pdf</a>, <a href="/format/2402.06262" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Efficacy of Eviction Policy for Key-Value Constrained Generative  Language Model Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ren%2C+S">Siyu Ren</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+K+Q">Kenny Q. Zhu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1249">[1249]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06430" title="Abstract">arXiv:2402.06430</a> (replaced) [<a href="/pdf/2402.06430" title="Download PDF">pdf</a>, <a href="/format/2402.06430" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Polyarc bounded complex interval arithmetic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Ger%C3%A9b%2C+G">G&#xe1;bor Ger&#xe9;b</a>, 
<a href="/search/math?searchtype=author&query=S%C3%A1ndor%2C+A">Andr&#xe1;s S&#xe1;ndor</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 38 pages (plus 48 pages of supplemetary material), 8 figures (plus 21 supplementary)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Numerical Analysis (math.NA)</span>; Algebraic Geometry (math.AG)

</div>
</div>
</dd>
<dt><a name="item1250">[1250]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06512" title="Abstract">arXiv:2402.06512</a> (replaced) [<a href="/pdf/2402.06512" title="Download PDF">pdf</a>, <a href="/format/2402.06512" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Clinical Trial Outcome Prediction with Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zheng%2C+W">Wenhao Zheng</a>, 
<a href="/search/cs?searchtype=author&query=Peng%2C+D">Dongsheng Peng</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+H">Hongxia Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+H">Hongtu Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+T">Tianfan Fu</a>, 
<a href="/search/cs?searchtype=author&query=Yao%2C+H">Huaxiu Yao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1251">[1251]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06529" title="Abstract">arXiv:2402.06529</a> (replaced) [<a href="/pdf/2402.06529" title="Download PDF">pdf</a>, <a href="/format/2402.06529" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Introspective Planning: Guiding Language-Enabled Agents to Refine Their  Own Uncertainty
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+K">Kaiqu Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zixu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Fisac%2C+J+F">Jaime Fern&#xe1;ndez Fisac</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 22 pages, 15 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1252">[1252]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06609" title="Abstract">arXiv:2402.06609</a> (replaced) [<a href="/pdf/2402.06609" title="Download PDF">pdf</a>, <a href="/format/2402.06609" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> You Still See Me: How Data Protection Supports the Architecture of ML  Surveillance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yew%2C+R">Rui-Jie Yew</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+L">Lucy Qin</a>, 
<a href="/search/cs?searchtype=author&query=Venkatasubramanian%2C+S">Suresh Venkatasubramanian</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> A version of this work was accepted at the 2023 NeurIPS Workshop on Regulatable ML
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Cryptography and Security (cs.CR)

</div>
</div>
</dd>
<dt><a name="item1253">[1253]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06716" title="Abstract">arXiv:2402.06716</a> (replaced) [<a href="/pdf/2402.06716" title="Download PDF">pdf</a>, <a href="/format/2402.06716" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Graph Information Bottleneck
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yuan%2C+H">Haonan Yuan</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+Q">Qingyun Sun</a>, 
<a href="/search/cs?searchtype=author&query=Fu%2C+X">Xingcheng Fu</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+C">Cheng Ji</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jianxin Li</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted by the research tracks of The Web Conference 2024 (WWW 2024)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1254">[1254]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.06821" title="Abstract">arXiv:2402.06821</a> (replaced) [<a href="/pdf/2402.06821" title="Download PDF">pdf</a>, <a href="/format/2402.06821" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Complexity of Promise Constraint Satisfaction Problem Seen from the  Other Side
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Asimi%2C+K">Kristina Asimi</a>, 
<a href="/search/cs?searchtype=author&query=Barto%2C+L">Libor Barto</a>, 
<a href="/search/cs?searchtype=author&query=Dalmau%2C+V">Victor Dalmau</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>

</div>
</div>
</dd>
<dt><a name="item1255">[1255]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07092" title="Abstract">arXiv:2402.07092</a> (replaced) [<a href="/pdf/2402.07092" title="Download PDF">pdf</a>, <a href="/format/2402.07092" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizing Conversational Dense Retrieval via LLM-Cognition Data  Augmentation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Haonan Chen</a>, 
<a href="/search/cs?searchtype=author&query=Dou%2C+Z">Zhicheng Dou</a>, 
<a href="/search/cs?searchtype=author&query=Mao%2C+K">Kelong Mao</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+J">Jiongnan Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+Z">Ziliang Zhao</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR)

</div>
</div>
</dd>
<dt><a name="item1256">[1256]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07249" title="Abstract">arXiv:2402.07249</a> (replaced) [<a href="/pdf/2402.07249" title="Download PDF">pdf</a>, <a href="/format/2402.07249" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Impact of Domain Knowledge and Multi-Modality on Intelligent  Molecular Property Prediction: A Systematic Survey
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Kuang%2C+T">Taojie Kuang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+P">Pengfei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhixiang Ren</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computational Engineering, Finance, and Science (cs.CE); Biomolecules (q-bio.BM)

</div>
</div>
</dd>
<dt><a name="item1257">[1257]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07331" title="Abstract">arXiv:2402.07331</a> (replaced) [<a href="/pdf/2402.07331" title="Download PDF">pdf</a>, <a href="/format/2402.07331" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fundamental Problems on Bounded-Treewidth Graphs: The Real Source of  Hardness
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Esmer%2C+B+C">Bar&#x131;&#x15f; Can Esmer</a>, 
<a href="/search/cs?searchtype=author&query=Focke%2C+J">Jacob Focke</a>, 
<a href="/search/cs?searchtype=author&query=Marx%2C+D">D&#xe1;niel Marx</a>, 
<a href="/search/cs?searchtype=author&query=Rz%C4%85%C5%BCewski%2C+P">Pawe&#x142; Rz&#x105;&#x17c;ewski</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Complexity (cs.CC)</span>; Data Structures and Algorithms (cs.DS)

</div>
</div>
</dd>
<dt><a name="item1258">[1258]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07355" title="Abstract">arXiv:2402.07355</a> (replaced) [<a href="/pdf/2402.07355" title="Download PDF">pdf</a>, <a href="/ps/2402.07355" title="Download PostScript">ps</a>, <a href="/format/2402.07355" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sampling from the Mean-Field Stationary Distribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Kook%2C+Y">Yunbum Kook</a>, 
<a href="/search/math?searchtype=author&query=Zhang%2C+M+S">Matthew S. Zhang</a>, 
<a href="/search/math?searchtype=author&query=Chewi%2C+S">Sinho Chewi</a>, 
<a href="/search/math?searchtype=author&query=Erdogdu%2C+M+A">Murat A. Erdogdu</a>, 
<a href="/search/math?searchtype=author&query=Li%2C+M+B">Mufan Bill Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1259">[1259]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07570" title="Abstract">arXiv:2402.07570</a> (replaced) [<a href="/pdf/2402.07570" title="Download PDF">pdf</a>, <a href="/format/2402.07570" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Only the Curve Shape Matters: Training Foundation Models for Zero-Shot  Multivariate Time Series Forecasting through Next Curve Shape Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Feng%2C+C">Cheng Feng</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Long Huang</a>, 
<a href="/search/cs?searchtype=author&query=Krompass%2C+D">Denis Krompass</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1260">[1260]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07696" title="Abstract">arXiv:2402.07696</a> (replaced) [<a href="/pdf/2402.07696" title="Download PDF">pdf</a>, <a href="/format/2402.07696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Synthesizing Strongly Equivalent Logic Programs: Beth Definability for  Answer Set Programs via Craig Interpolation in First-Order Logic
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Heuer%2C+J">Jan Heuer</a>, 
<a href="/search/cs?searchtype=author&query=Wernhard%2C+C">Christoph Wernhard</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Logic in Computer Science (cs.LO)</span>

</div>
</div>
</dd>
<dt><a name="item1261">[1261]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.07939" title="Abstract">arXiv:2402.07939</a> (replaced) [<a href="/pdf/2402.07939" title="Download PDF">pdf</a>, <a href="/format/2402.07939" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UFO: A UI-Focused Agent for Windows OS Interaction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chaoyun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+L">Liqun Li</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+S">Shilin He</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+X">Xu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Qiao%2C+B">Bo Qiao</a>, 
<a href="/search/cs?searchtype=author&query=Qin%2C+S">Si Qin</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+M">Minghua Ma</a>, 
<a href="/search/cs?searchtype=author&query=Kang%2C+Y">Yu Kang</a>, 
<a href="/search/cs?searchtype=author&query=Lin%2C+Q">Qingwei Lin</a>, 
<a href="/search/cs?searchtype=author&query=Rajmohan%2C+S">Saravan Rajmohan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+D">Dongmei Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Q">Qi Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1262">[1262]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08126" title="Abstract">arXiv:2402.08126</a> (replaced) [<a href="/pdf/2402.08126" title="Download PDF">pdf</a>, <a href="/ps/2402.08126" title="Download PostScript">ps</a>, <a href="/format/2402.08126" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Contextual Multinomial Logit Bandits with General Value Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+M">Mengxiao Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+H">Haipeng Luo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1263">[1263]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08170" title="Abstract">arXiv:2402.08170</a> (replaced) [<a href="/pdf/2402.08170" title="Download PDF">pdf</a>, <a href="/format/2402.08170" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLaGA: Large Language and Graph Assistant
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chen%2C+R">Runjin Chen</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+T">Tong Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Jaiswal%2C+A">Ajay Jaiswal</a>, 
<a href="/search/cs?searchtype=author&query=Shah%2C+N">Neil Shah</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhangyang Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1264">[1264]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08223" title="Abstract">arXiv:2402.08223</a> (replaced) [<a href="/pdf/2402.08223" title="Download PDF">pdf</a>, <a href="/ps/2402.08223" title="Download PostScript">ps</a>, <a href="/format/2402.08223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Limits of Price Discrimination Under Privacy Constraints
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/econ?searchtype=author&query=Fallah%2C+A">Alireza Fallah</a>, 
<a href="/search/econ?searchtype=author&query=Jordan%2C+M+I">Michael I. Jordan</a>, 
<a href="/search/econ?searchtype=author&query=Makhdoumi%2C+A">Ali Makhdoumi</a>, 
<a href="/search/econ?searchtype=author&query=Malekian%2C+A">Azarakhsh Malekian</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Theoretical Economics (econ.TH)</span>; Computer Science and Game Theory (cs.GT)

</div>
</div>
</dd>
<dt><a name="item1265">[1265]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08303" title="Abstract">arXiv:2402.08303</a> (replaced) [<a href="/e-print/2402.08303" title="Download source">src</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> ChatCell: Facilitating Single-Cell Analysis with Natural Language
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Fang%2C+Y">Yin Fang</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+K">Kangwei Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+N">Ningyu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+X">Xinle Deng</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+P">Penghui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Zhuo Chen</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+X">Xiangru Tang</a>, 
<a href="/search/cs?searchtype=author&query=Gerstein%2C+M">Mark Gerstein</a>, 
<a href="/search/cs?searchtype=author&query=Fan%2C+X">Xiaohui Fan</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+H">Huajun Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> I have decided to temporarily withdraw this draft as I am in the process of making further revisions to improve its content. Code: <a href="https://github.com/zjunlp/ChatCell">this https URL</a> Dataset: <a href="https://huggingface.co/datasets/zjunlp/ChatCell-Instructions">this https URL</a> Demo: <a href="https://chat.openai.com/g/g-vUwj222gQ-chatcell">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computational Engineering, Finance, and Science (cs.CE); Human-Computer Interaction (cs.HC); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1266">[1266]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08322" title="Abstract">arXiv:2402.08322</a> (replaced) [<a href="/pdf/2402.08322" title="Download PDF">pdf</a>, <a href="/format/2402.08322" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> zk-IoT: Securing the Internet of Things with Zero-Knowledge Proofs on  Blockchain Platforms
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramezan%2C+G">Gholamreza Ramezan</a>, 
<a href="/search/cs?searchtype=author&query=Meamari%2C+E">Ehsan Meamari</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>

</div>
</div>
</dd>
<dt><a name="item1267">[1267]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08547" title="Abstract">arXiv:2402.08547</a> (replaced) [<a href="/pdf/2402.08547" title="Download PDF">pdf</a>, <a href="/format/2402.08547" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dueling Over Dessert, Mastering the Art of Repeated Cake Cutting
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Br%C3%A2nzei%2C+S">Simina Br&#xe2;nzei</a>, 
<a href="/search/cs?searchtype=author&query=Hajiaghayi%2C+M">MohammadTaghi Hajiaghayi</a>, 
<a href="/search/cs?searchtype=author&query=Phillips%2C+R">Reed Phillips</a>, 
<a href="/search/cs?searchtype=author&query=Shin%2C+S">Suho Shin</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kun Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Artificial Intelligence (cs.AI); Theoretical Economics (econ.TH)

</div>
</div>
</dd>
<dt><a name="item1268">[1268]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08631" title="Abstract">arXiv:2402.08631</a> (replaced) [<a href="/pdf/2402.08631" title="Download PDF">pdf</a>, <a href="/format/2402.08631" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Knowledge Editing on Black-box Large Language Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Song%2C+X">Xiaoshuai Song</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zhengyang Wang</a>, 
<a href="/search/cs?searchtype=author&query=He%2C+K">Keqing He</a>, 
<a href="/search/cs?searchtype=author&query=Dong%2C+G">Guanting Dong</a>, 
<a href="/search/cs?searchtype=author&query=Mou%2C+Y">Yutao Mou</a>, 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jinxu Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Weiran Xu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1269">[1269]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08653" title="Abstract">arXiv:2402.08653</a> (replaced) [<a href="/pdf/2402.08653" title="Download PDF">pdf</a>, <a href="/format/2402.08653" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Cheng%2C+W">Wuxinlin Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Deng%2C+C">Chenhui Deng</a>, 
<a href="/search/cs?searchtype=author&query=Aghdaei%2C+A">Ali Aghdaei</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Z">Zhiru Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Feng%2C+Z">Zhuo Feng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1270">[1270]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08662" title="Abstract">arXiv:2402.08662</a> (replaced) [<a href="/pdf/2402.08662" title="Download PDF">pdf</a>, <a href="/format/2402.08662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Emergent Gaits with Decentralized Phase Oscillators: on the  role of Observations, Rewards, and Feedback
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+J">Jenny Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Heim%2C+S">Steve Heim</a>, 
<a href="/search/cs?searchtype=author&query=Jeon%2C+S+H">Se Hwan Jeon</a>, 
<a href="/search/cs?searchtype=author&query=Kim%2C+S">Sangbae Kim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> ICRA 2024, 8 pages 7 Figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1271">[1271]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08678" title="Abstract">arXiv:2402.08678</a> (replaced) [<a href="/pdf/2402.08678" title="Download PDF">pdf</a>, <a href="/format/2402.08678" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Graph Mamba: Towards Learning on Graphs with State Space Models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Behrouz%2C+A">Ali Behrouz</a>, 
<a href="/search/cs?searchtype=author&query=Hashemi%2C+F">Farnoosh Hashemi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1272">[1272]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08902" title="Abstract">arXiv:2402.08902</a> (replaced) [<a href="/pdf/2402.08902" title="Download PDF">pdf</a>, <a href="/format/2402.08902" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Auto-Encoding Bayesian Inverse Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xinjie Liu</a>, 
<a href="/search/cs?searchtype=author&query=Peters%2C+L">Lasse Peters</a>, 
<a href="/search/cs?searchtype=author&query=Alonso-Mora%2C+J">Javier Alonso-Mora</a>, 
<a href="/search/cs?searchtype=author&query=Topcu%2C+U">Ufuk Topcu</a>, 
<a href="/search/cs?searchtype=author&query=Fridovich-Keil%2C+D">David Fridovich-Keil</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Computer Science and Game Theory (cs.GT); Machine Learning (cs.LG); Multiagent Systems (cs.MA); Systems and Control (eess.SY)

</div>
</div>
</dd>
<dt><a name="item1273">[1273]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08923" title="Abstract">arXiv:2402.08923</a> (replaced) [<a href="/pdf/2402.08923" title="Download PDF">pdf</a>, <a href="/format/2402.08923" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human  Pose Estimation with Transformer Architecture
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ramani%2C+V">Varun Ramani</a>, 
<a href="/search/cs?searchtype=author&query=Khayami%2C+H">Hossein Khayami</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+Y">Yang Bai</a>, 
<a href="/search/cs?searchtype=author&query=Garg%2C+N">Nakul Garg</a>, 
<a href="/search/cs?searchtype=author&query=Roy%2C+N">Nirupam Roy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 16 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1274">[1274]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08971" title="Abstract">arXiv:2402.08971</a> (replaced) [<a href="/pdf/2402.08971" title="Download PDF">pdf</a>, <a href="/format/2402.08971" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Structured Language Generation Model for Robust Structure Prediction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Lee%2C+M">Minho Lee</a>, 
<a href="/search/cs?searchtype=author&query=Min%2C+J">Junghyun Min</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+W">Woochul Lee</a>, 
<a href="/search/cs?searchtype=author&query=Lee%2C+Y">Yeonsoo Lee</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures, 5 tables, 7 pages of appendix with 9 additional tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
<dt><a name="item1275">[1275]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.08987" title="Abstract">arXiv:2402.08987</a> (replaced) [<a href="/pdf/2402.08987" title="Download PDF">pdf</a>, <a href="/format/2402.08987" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multi-modality transrectal ultrasound video classification for  identification of clinically significant prostate cancer
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/eess?searchtype=author&query=Wu%2C+H">Hong Wu</a>, 
<a href="/search/eess?searchtype=author&query=Fu%2C+J">Juan Fu</a>, 
<a href="/search/eess?searchtype=author&query=Ye%2C+H">Hongsheng Ye</a>, 
<a href="/search/eess?searchtype=author&query=Zhong%2C+Y">Yuming Zhong</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+X">Xuebin Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Zhou%2C+J">Jianhua Zhou</a>, 
<a href="/search/eess?searchtype=author&query=Wang%2C+Y">Yi Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Image and Video Processing (eess.IV)</span>; Computer Vision and Pattern Recognition (cs.CV)

</div>
</div>
</dd>
<dt><a name="item1276">[1276]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09264" title="Abstract">arXiv:2402.09264</a> (replaced) [<a href="/pdf/2402.09264" title="Download PDF">pdf</a>, <a href="/format/2402.09264" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jia%2C+H">Hong Jia</a>, 
<a href="/search/cs?searchtype=author&query=Kwon%2C+Y+D">Young D. Kwon</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+D">Dong Ma</a>, 
<a href="/search/cs?searchtype=author&query=Pham%2C+N">Nhat Pham</a>, 
<a href="/search/cs?searchtype=author&query=Qendro%2C+L">Lorena Qendro</a>, 
<a href="/search/cs?searchtype=author&query=Vu%2C+T">Tam Vu</a>, 
<a href="/search/cs?searchtype=author&query=Mascolo%2C+C">Cecilia Mascolo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Human-Computer Interaction (cs.HC)

</div>
</div>
</dd>
<dt><a name="item1277">[1277]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09303" title="Abstract">arXiv:2402.09303</a> (replaced) [<a href="/pdf/2402.09303" title="Download PDF">pdf</a>, <a href="/format/2402.09303" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Immediate generalisation in humans but a generalisation lag in deep  neural networks -- evidence for representational divergence?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Huber%2C+L+S">Lukas S. Huber</a>, 
<a href="/search/cs?searchtype=author&query=Mast%2C+F+W">Fred W. Mast</a>, 
<a href="/search/cs?searchtype=author&query=Wichmann%2C+F+A">Felix A. Wichmann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Under review at the ICLR 2024 Workshop on Representational Alignment (Re-Align)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item1278">[1278]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09329" title="Abstract">arXiv:2402.09329</a> (replaced) [<a href="/pdf/2402.09329" title="Download PDF">pdf</a>, <a href="/ps/2402.09329" title="Download PostScript">ps</a>, <a href="/format/2402.09329" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> YOLOv8-AM: YOLOv8 with Attention Mechanisms for Pediatric Wrist Fracture  Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Chien%2C+C">Chun-Tse Chien</a>, 
<a href="/search/cs?searchtype=author&query=Ju%2C+R">Rui-Yang Ju</a>, 
<a href="/search/cs?searchtype=author&query=Chou%2C+K">Kuang-Yi Chou</a>, 
<a href="/search/cs?searchtype=author&query=Chiang%2C+J">Jen-Shiun Chiang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1279">[1279]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09364" title="Abstract">arXiv:2402.09364</a> (replaced) [<a href="/pdf/2402.09364" title="Download PDF">pdf</a>, <a href="/format/2402.09364" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Performance-Complexity-Latency Trade-offs of Concatenated RS-BCH Codes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Sukmadji%2C+A+Y">Alvin Y. Sukmadji</a>, 
<a href="/search/cs?searchtype=author&query=Kschischang%2C+F+R">Frank R. Kschischang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in the IEEE Transactions on Communications
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Theory (cs.IT)</span>

</div>
</div>
</dd>
<dt><a name="item1280">[1280]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09391" title="Abstract">arXiv:2402.09391</a> (replaced) [<a href="/pdf/2402.09391" title="Download PDF">pdf</a>, <a href="/format/2402.09391" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LlaSMol: Advancing Large Language Models for Chemistry with a  Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+B">Botao Yu</a>, 
<a href="/search/cs?searchtype=author&query=Baker%2C+F+N">Frazier N. Baker</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+Z">Ziqi Chen</a>, 
<a href="/search/cs?searchtype=author&query=Ning%2C+X">Xia Ning</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+H">Huan Sun</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 23 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computational Engineering, Finance, and Science (cs.CE); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1281">[1281]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09495" title="Abstract">arXiv:2402.09495</a> (replaced) [<a href="/pdf/2402.09495" title="Download PDF">pdf</a>, <a href="/format/2402.09495" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Potential of Network-Based Features for Fraud Detection
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/q-fin?searchtype=author&query=Azarm%2C+C">Catayoun Azarm</a>, 
<a href="/search/q-fin?searchtype=author&query=Acar%2C+E">Erman Acar</a>, 
<a href="/search/q-fin?searchtype=author&query=van+Zeelt%2C+M">Mickey van Zeelt</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Risk Management (q-fin.RM)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1282">[1282]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09581" title="Abstract">arXiv:2402.09581</a> (replaced) [<a href="/pdf/2402.09581" title="Download PDF">pdf</a>, <a href="/format/2402.09581" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Combatting deepfakes: Policies to address national security threats and  rights violations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Miotti%2C+A">Andrea Miotti</a>, 
<a href="/search/cs?searchtype=author&query=Wasil%2C+A">Akash Wasil</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)

</div>
</div>
</dd>
<dt><a name="item1283">[1283]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09588" title="Abstract">arXiv:2402.09588</a> (replaced) [<a href="/pdf/2402.09588" title="Download PDF">pdf</a>, <a href="/format/2402.09588" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Emerging Opportunities of Using Large Language Models for Translation  Between Drug Molecules and Indications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Oniani%2C+D">David Oniani</a>, 
<a href="/search/cs?searchtype=author&query=Hilsman%2C+J">Jordan Hilsman</a>, 
<a href="/search/cs?searchtype=author&query=Zang%2C+C">Chengxi Zang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Junmei Wang</a>, 
<a href="/search/cs?searchtype=author&query=Cai%2C+L">Lianjin Cai</a>, 
<a href="/search/cs?searchtype=author&query=Zawala%2C+J">Jan Zawala</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yanshan Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1284">[1284]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09656" title="Abstract">arXiv:2402.09656</a> (replaced) [<a href="/pdf/2402.09656" title="Download PDF">pdf</a>, <a href="/format/2402.09656" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Butterfly Effect of Model Editing: Few Edits Can Trigger Large  Language Models Collapse
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+W">Wanli Yang</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+F">Fei Sun</a>, 
<a href="/search/cs?searchtype=author&query=Ma%2C+X">Xinyu Ma</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+X">Xun Liu</a>, 
<a href="/search/cs?searchtype=author&query=Yin%2C+D">Dawei Yin</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+X">Xueqi Cheng</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Artificial Intelligence (cs.AI)</span>

</div>
</div>
</dd>
<dt><a name="item1285">[1285]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09662" title="Abstract">arXiv:2402.09662</a> (replaced) [<a href="/pdf/2402.09662" title="Download PDF">pdf</a>, <a href="/format/2402.09662" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> GeoBotsVR: A Robotics Learning Game for Beginners with Hands-on Learning  Simulation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mubarrat%2C+S+T">Syed Tanzim Mubarrat</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To be published in Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA 2024). 6 pages, 6 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Robotics (cs.RO)

</div>
</div>
</dd>
<dt><a name="item1286">[1286]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09670" title="Abstract">arXiv:2402.09670</a> (replaced) [<a href="/pdf/2402.09670" title="Download PDF">pdf</a>, <a href="/ps/2402.09670" title="Download PostScript">ps</a>, <a href="/format/2402.09670" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Efficient $&#x3a6;$-Regret Minimization with Low-Degree Swap Deviations in  Extensive-Form Games
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+B+H">Brian Hu Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Anagnostides%2C+I">Ioannis Anagnostides</a>, 
<a href="/search/cs?searchtype=author&query=Farina%2C+G">Gabriele Farina</a>, 
<a href="/search/cs?searchtype=author&query=Sandholm%2C+T">Tuomas Sandholm</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>

</div>
</div>
</dd>
<dt><a name="item1287">[1287]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09714" title="Abstract">arXiv:2402.09714</a> (replaced) [<a href="/pdf/2402.09714" title="Download PDF">pdf</a>, <a href="/format/2402.09714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> An Accelerated Distributed Stochastic Gradient Method with Momentum
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/math?searchtype=author&query=Huang%2C+K">Kun Huang</a>, 
<a href="/search/math?searchtype=author&query=Pu%2C+S">Shi Pu</a>, 
<a href="/search/math?searchtype=author&query=Nedi%C4%87%2C+A">Angelia Nedi&#x107;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 32 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Optimization and Control (math.OC)</span>; Distributed, Parallel, and Cluster Computing (cs.DC); Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item1288">[1288]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09724" title="Abstract">arXiv:2402.09724</a> (replaced) [<a href="/pdf/2402.09724" title="Download PDF">pdf</a>, <a href="/format/2402.09724" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Region Feature Descriptor Adapted to High Affine Transformations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhang%2C+S">Shaojie Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Y">Yinghui Wang</a>, 
<a href="/search/cs?searchtype=author&query=Nan%2C+B">Bin Nan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+J">Jinlong Yang</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+T">Tao Yan</a>, 
<a href="/search/cs?searchtype=author&query=Huang%2C+L">Liangyi Huang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+M">Mingfeng Wang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1289">[1289]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09776" title="Abstract">arXiv:2402.09776</a> (replaced) [<a href="/pdf/2402.09776" title="Download PDF">pdf</a>, <a href="/format/2402.09776" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Strategic Vote Timing in Online Elections With Public Tallies
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yaish%2C+A">Aviv Yaish</a>, 
<a href="/search/cs?searchtype=author&query=Abramova%2C+S">Svetlana Abramova</a>, 
<a href="/search/cs?searchtype=author&query=B%C3%B6hme%2C+R">Rainer B&#xf6;hme</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 44 pages, 4 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Science and Game Theory (cs.GT)</span>; Multiagent Systems (cs.MA)

</div>
</div>
</dd>
<dt><a name="item1290">[1290]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09784" title="Abstract">arXiv:2402.09784</a> (replaced) [<a href="/pdf/2402.09784" title="Download PDF">pdf</a>, <a href="/format/2402.09784" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sequential Recommendation on Temporal Proximities with Contrastive  Learning and Self-Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jung%2C+H">Hansol Jung</a>, 
<a href="/search/cs?searchtype=author&query=Seo%2C+H">Hyunwoo Seo</a>, 
<a href="/search/cs?searchtype=author&query=Lim%2C+C">Chiehyeon Lim</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 9 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1291">[1291]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09820" title="Abstract">arXiv:2402.09820</a> (replaced) [<a href="/pdf/2402.09820" title="Download PDF">pdf</a>, <a href="/ps/2402.09820" title="Download PostScript">ps</a>, <a href="/format/2402.09820" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Utilizing Deep Learning for Enhancing Network Resilience in Finance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Gong%2C+Y">Yulu Gong</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+M">Mengran Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Huo%2C+S">Shuning Huo</a>, 
<a href="/search/cs?searchtype=author&query=Xiang%2C+Y">Yafei Xiang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+H">Hanyi Yu</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); General Finance (q-fin.GN)

</div>
</div>
</dd>
<dt><a name="item1292">[1292]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09954" title="Abstract">arXiv:2402.09954</a> (replaced) [<a href="/pdf/2402.09954" title="Download PDF">pdf</a>, <a href="/format/2402.09954" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of  In-Context Learning for Persona-based Dialogue Generation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Pu%2C+J">Jiashu Pu</a>, 
<a href="/search/cs?searchtype=author&query=Wan%2C+Y">Yajing Wan</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yuru Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jing Chen</a>, 
<a href="/search/cs?searchtype=author&query=Cheng%2C+L">Ling Cheng</a>, 
<a href="/search/cs?searchtype=author&query=Shao%2C+Q">Qian Shao</a>, 
<a href="/search/cs?searchtype=author&query=Chang%2C+Y">Yongzhu Chang</a>, 
<a href="/search/cs?searchtype=author&query=Lv%2C+T">Tangjie Lv</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+R">Rongsheng Zhang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item1293">[1293]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09959" title="Abstract">arXiv:2402.09959</a> (replaced) [<a href="/pdf/2402.09959" title="Download PDF">pdf</a>, <a href="/format/2402.09959" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLM-based Federated Recommendation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Zhao%2C+J">Jujia Zhao</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+W">Wenjie Wang</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+C">Chen Xu</a>, 
<a href="/search/cs?searchtype=author&query=Ren%2C+Z">Zhaochun Ren</a>, 
<a href="/search/cs?searchtype=author&query=Ng%2C+S">See-Kiong Ng</a>, 
<a href="/search/cs?searchtype=author&query=Chua%2C+T">Tat-Seng Chua</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>

</div>
</div>
</dd>
<dt><a name="item1294">[1294]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.09989" title="Abstract">arXiv:2402.09989</a> (replaced) [<a href="/pdf/2402.09989" title="Download PDF">pdf</a>, <a href="/format/2402.09989" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> LLMs as Bridges: Reformulating Grounded Multimodal Named Entity  Recognition
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Li%2C+J">Jinyuan Li</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+H">Han Li</a>, 
<a href="/search/cs?searchtype=author&query=Sun%2C+D">Di Sun</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+J">Jiahao Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+W">Wenkun Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+Z">Zan Wang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+G">Gang Pan</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1295">[1295]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10074" title="Abstract">arXiv:2402.10074</a> (replaced) [<a href="/pdf/2402.10074" title="Download PDF">pdf</a>, <a href="/format/2402.10074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Class-Balanced and Reinforced Active Learning on Graphs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yu%2C+C">Chengcheng Yu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+J">Jiapeng Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Li%2C+X">Xiang Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>

</div>
</div>
</dd>
<dt><a name="item1296">[1296]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10100" title="Abstract">arXiv:2402.10100</a> (replaced) [<a href="/pdf/2402.10100" title="Download PDF">pdf</a>, <a href="/format/2402.10100" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Tuning In: Analysis of Audio Classifier Performance in Clinical Settings  with Limited Data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Mahdi%2C+H">Hamza Mahdi</a>, 
<a href="/search/cs?searchtype=author&query=Nashnoush%2C+E">Eptehal Nashnoush</a>, 
<a href="/search/cs?searchtype=author&query=Saab%2C+R">Rami Saab</a>, 
<a href="/search/cs?searchtype=author&query=Balachandar%2C+A">Arjun Balachandar</a>, 
<a href="/search/cs?searchtype=author&query=Dagli%2C+R">Rishit Dagli</a>, 
<a href="/search/cs?searchtype=author&query=Perri%2C+L+X">Lucas X. Perri</a>, 
<a href="/search/cs?searchtype=author&query=Khosravani%2C+H">Houman Khosravani</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Audio and Speech Processing (eess.AS)

</div>
</div>
</dd>
<dt><a name="item1297">[1297]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10163" title="Abstract">arXiv:2402.10163</a> (replaced) [<a href="/pdf/2402.10163" title="Download PDF">pdf</a>, <a href="/format/2402.10163" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hidden Traveling Waves bind Working Memory Variables in Recurrent Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Karuvally%2C+A">Arjun Karuvally</a>, 
<a href="/search/cs?searchtype=author&query=Sejnowski%2C+T+J">Terrence J. Sejnowski</a>, 
<a href="/search/cs?searchtype=author&query=Siegelmann%2C+H+T">Hava T. Siegelmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Neural and Evolutionary Computing (cs.NE)</span>

</div>
</div>
</dd>
<dt><a name="item1298">[1298]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10184" title="Abstract">arXiv:2402.10184</a> (replaced) [<a href="/pdf/2402.10184" title="Download PDF">pdf</a>, <a href="/format/2402.10184" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rethinking Information Structures in RLHF: Reward Generalization from a  Graph Theory Perspective
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Qiu%2C+T">Tianyi Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Zeng%2C+F">Fanzhi Zeng</a>, 
<a href="/search/cs?searchtype=author&query=Ji%2C+J">Jiaming Ji</a>, 
<a href="/search/cs?searchtype=author&query=Yan%2C+D">Dong Yan</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+K">Kaile Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+J">Jiayi Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+H">Han Yang</a>, 
<a href="/search/cs?searchtype=author&query=Dai%2C+J">Josef Dai</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xuehai Pan</a>, 
<a href="/search/cs?searchtype=author&query=Yang%2C+Y">Yaodong Yang</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Discrete Mathematics (cs.DM)

</div>
</div>
</dd>
<dt><a name="item1299">[1299]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10198" title="Abstract">arXiv:2402.10198</a> (replaced) [<a href="/pdf/2402.10198" title="Download PDF">pdf</a>, <a href="/format/2402.10198" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Unlocking the Potential of Transformers in Time Series Forecasting with  Sharpness-Aware Minimization and Channel-Wise Attention
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Ilbert%2C+R">Romain Ilbert</a>, 
<a href="/search/cs?searchtype=author&query=Odonnat%2C+A">Ambroise Odonnat</a>, 
<a href="/search/cs?searchtype=author&query=Feofanov%2C+V">Vasilii Feofanov</a>, 
<a href="/search/cs?searchtype=author&query=Virmaux%2C+A">Aladin Virmaux</a>, 
<a href="/search/cs?searchtype=author&query=Paolo%2C+G">Giuseppe Paolo</a>, 
<a href="/search/cs?searchtype=author&query=Palpanas%2C+T">Themis Palpanas</a>, 
<a href="/search/cs?searchtype=author&query=Redko%2C+I">Ievgen Redko</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The first two authors contributed equally
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item1300">[1300]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10207" title="Abstract">arXiv:2402.10207</a> (replaced) [<a href="/pdf/2402.10207" title="Download PDF">pdf</a>, <a href="/format/2402.10207" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Rewards-in-Context: Multi-objective Alignment of Foundation Models with  Dynamic Preference Adjustment
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Yang%2C+R">Rui Yang</a>, 
<a href="/search/cs?searchtype=author&query=Pan%2C+X">Xiaoman Pan</a>, 
<a href="/search/cs?searchtype=author&query=Luo%2C+F">Feng Luo</a>, 
<a href="/search/cs?searchtype=author&query=Qiu%2C+S">Shuang Qiu</a>, 
<a href="/search/cs?searchtype=author&query=Zhong%2C+H">Han Zhong</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+D">Dong Yu</a>, 
<a href="/search/cs?searchtype=author&query=Chen%2C+J">Jianshu Chen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 20 pages, 12 figures, 6 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

</div>
</div>
</dd>
<dt><a name="item1301">[1301]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10340" title="Abstract">arXiv:2402.10340</a> (replaced) [<a href="/pdf/2402.10340" title="Download PDF">pdf</a>, <a href="/format/2402.10340" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting  the Risks and Vulnerabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Wu%2C+X">Xiyang Wu</a>, 
<a href="/search/cs?searchtype=author&query=Xian%2C+R">Ruiqi Xian</a>, 
<a href="/search/cs?searchtype=author&query=Guan%2C+T">Tianrui Guan</a>, 
<a href="/search/cs?searchtype=author&query=Liang%2C+J">Jing Liang</a>, 
<a href="/search/cs?searchtype=author&query=Chakraborty%2C+S">Souradip Chakraborty</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+F">Fuxiao Liu</a>, 
<a href="/search/cs?searchtype=author&query=Sadler%2C+B">Brian Sadler</a>, 
<a href="/search/cs?searchtype=author&query=Manocha%2C+D">Dinesh Manocha</a>, 
<a href="/search/cs?searchtype=author&query=Bedi%2C+A+S">Amrit Singh Bedi</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Robotics (cs.RO)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1302">[1302]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10513" title="Abstract">arXiv:2402.10513</a> (replaced) [<a href="/pdf/2402.10513" title="Download PDF">pdf</a>, <a href="/format/2402.10513" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Understanding Delays in AF\_XDP-based Applications
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Perron%2C+K+C+d">Killian Castillon du Perron</a>, 
<a href="/search/cs?searchtype=author&query=Pacheco%2C+D+L">Dino Lopez Pacheco</a>, 
<a href="/search/cs?searchtype=author&query=Huet%2C+F">Fabrice Huet</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> 2024 IEEE International Conference on Communications (ICC):
  Next-Generation Networking and Internet Symposium (IEEE ICC'24 - NGNI
  Symposium), IEEE, Jun 2024, Denver, United States
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Networking and Internet Architecture (cs.NI)</span>

</div>
</div>
</dd>
<dt><a name="item1303">[1303]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10567" title="Abstract">arXiv:2402.10567</a> (replaced) [<a href="/pdf/2402.10567" title="Download PDF">pdf</a>, <a href="/format/2402.10567" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs  ready for the Indian Legal Domain?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Tripathi%2C+Y">Yogesh Tripathi</a>, 
<a href="/search/cs?searchtype=author&query=Donakanti%2C+R">Raghav Donakanti</a>, 
<a href="/search/cs?searchtype=author&query=Girhepuje%2C+S">Sahil Girhepuje</a>, 
<a href="/search/cs?searchtype=author&query=Kavathekar%2C+I">Ishan Kavathekar</a>, 
<a href="/search/cs?searchtype=author&query=Vedula%2C+B+H">Bhaskara Hanuma Vedula</a>, 
<a href="/search/cs?searchtype=author&query=Krishnan%2C+G+S">Gokul S Krishnan</a>, 
<a href="/search/cs?searchtype=author&query=Goyal%2C+S">Shreya Goyal</a>, 
<a href="/search/cs?searchtype=author&query=Goel%2C+A">Anmol Goel</a>, 
<a href="/search/cs?searchtype=author&query=Ravindran%2C+B">Balaraman Ravindran</a>, 
<a href="/search/cs?searchtype=author&query=Kumaraguru%2C+P">Ponnurangam Kumaraguru</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)

</div>
</div>
</dd>
<dt><a name="item1304">[1304]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10739" title="Abstract">arXiv:2402.10739</a> (replaced) [<a href="/pdf/2402.10739" title="Download PDF">pdf</a>, <a href="/format/2402.10739" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PointMamba: A Simple State Space Model for Point Cloud Analysis
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Liang%2C+D">Dingkang Liang</a>, 
<a href="/search/cs?searchtype=author&query=Zhou%2C+X">Xin Zhou</a>, 
<a href="/search/cs?searchtype=author&query=Wang%2C+X">Xinyu Wang</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+X">Xingkui Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Xu%2C+W">Wei Xu</a>, 
<a href="/search/cs?searchtype=author&query=Zou%2C+Z">Zhikang Zou</a>, 
<a href="/search/cs?searchtype=author&query=Ye%2C+X">Xiaoqing Ye</a>, 
<a href="/search/cs?searchtype=author&query=Bai%2C+X">Xiang Bai</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> The code is available at <a href="https://github.com/LMD0311/PointMamba">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>

</div>
</div>
</dd>
<dt><a name="item1305">[1305]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10830" title="Abstract">arXiv:2402.10830</a> (replaced) [<a href="/pdf/2402.10830" title="Download PDF">pdf</a>, <a href="/format/2402.10830" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Towards a Consensual Definition for Smart Tourism and Smart Tourism  Tools
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Galv%C3%A3o%2C+A">Ant&#xf3;nio Galv&#xe3;o</a>, 
<a href="/search/cs?searchtype=author&query=Abreu%2C+F+B+e">Fernando Brito e Abreu</a>, 
<a href="/search/cs?searchtype=author&query=de+Melo%2C+J+J">Jo&#xe3;o Joanaz de Melo</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>

</div>
</div>
</dd>
<dt><a name="item1306">[1306]</a>&nbsp;  <span class="list-identifier"><a href="/abs/2402.10835" title="Abstract">arXiv:2402.10835</a> (replaced) [<a href="/pdf/2402.10835" title="Download PDF">pdf</a>, <a href="/format/2402.10835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time Series Forecasting with LLMs: Understanding and Enhancing Model  Capabilities
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="/search/cs?searchtype=author&query=Jin%2C+M">Mingyu Jin</a>, 
<a href="/search/cs?searchtype=author&query=Tang%2C+H">Hua Tang</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+C">Chong Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Yu%2C+Q">Qinkai Yu</a>, 
<a href="/search/cs?searchtype=author&query=Liu%2C+C">Chengzhi Liu</a>, 
<a href="/search/cs?searchtype=author&query=Zhu%2C+S">Suiyuan Zhu</a>, 
<a href="/search/cs?searchtype=author&query=Zhang%2C+Y">Yongfeng Zhang</a>, 
<a href="/search/cs?searchtype=author&query=Du%2C+M">Mengnan Du</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation and Language (cs.CL)</span>

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="/list/cs/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item750">Cross-lists</a></li>
<li><a href="#item821">Replacements</a></li>
</ul>
<small>[ total of 1306 entries:  <b>1-1306</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="/list/cs/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="/help/mathjax">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="/" accesskey="a">arXiv</a>,
<a href="/form/cs">form interface</a>,
<a href="/find/cs">find</a>,
<a href="/archive/cs">cs</a>, <a href="/list/cs/recent">recent</a>, <a href="/list/cs/2402">2402</a>,
<a href="/help/contact">contact</a>,
<a href="/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
 <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/about">About</a></li>
                <li><a href="https://arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://arxiv.org/help/contact"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/license">Copyright</a></li>
                <li><a href="https://arxiv.org/help/policies/privacy_policy">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://arxiv.org/help/web_accessibility">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                    Get status notifications via
                    <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
                    or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
</body>
</html>
